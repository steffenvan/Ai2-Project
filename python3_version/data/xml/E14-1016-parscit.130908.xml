<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000445">
<title confidence="0.99111">
Leveraging Verb-Argument Structures to Infer Semantic Relations
</title>
<author confidence="0.984868">
Eduardo Blanco and Dan Moldovan
</author>
<affiliation confidence="0.960093">
Lymba Corporation
</affiliation>
<address confidence="0.794783">
Richardson, TX 75080 USA
</address>
<email confidence="0.991634">
{eduardo,moldovan}@lymba.com
</email>
<sectionHeader confidence="0.993742" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957">
This paper presents a methodology to in-
fer implicit semantic relations from verb-
argument structures. An annotation effort
shows implicit relations boost the amount
of meaning explicitly encoded for verbs.
Experimental results with automatically
obtained parse trees and verb-argument
structures demonstrate that inferring im-
plicit relations is a doable task.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999768533333333">
Automatic extraction of semantic relations is an
important step towards capturing the meaning of
text. Semantic relations explicitly encode links be-
tween concepts. For example, in The accident left
him a changed man, the ‘accident’ is the CAUSE
of the man undergoing some ‘change’. A question
answering system would benefit from detecting
this relation when answering Why did he change?
Extracting all semantic relations from text is a
monumental task and is at the core of language
understanding. In recent years, approaches that
aim at extracting a subset of all relations have
achieved great success. In particular, previous re-
search (Carreras and M`arquez, 2005; Punyakanok
et al., 2008; Che et al., 2010; Zapirain et al., 2010)
focused on verb-argument structures, i.e., relations
between a verb and its syntactic arguments. Prop-
Bank (Palmer et al., 2005) is the corpus of refer-
ence for verb-argument relations. However, rela-
tions between a verb and its syntactic arguments
are only a fraction of the relations present in texts.
Consider the statement [Mr. Brown]NP1 suc-
ceeds [Joseph W. Hibben, who retired last
August]NP2 and its parse tree (Figure 1). Verb-
argument relations encode that NP1 is the AGENT
and NP2 is the THEME of verb ‘succeeds’ (Prop-
Bank uses labels ARG0 and ARG1). Any se-
mantic relation between ‘succeeds’ and concepts
dominated in the parse tree by one of its syntac-
tic arguments NP1 or NP2, e.g., ‘succeeds’ oc-
</bodyText>
<figure confidence="0.769772">
S
</figure>
<figureCaption confidence="0.8012">
Figure 1: Example of parse tree and verb-
argument structures (solid arrows). The relation
between ‘succeeds’ and ‘last August’ is missing,
but a TIME-AFTER holds (dashed arrow).
</figureCaption>
<bodyText confidence="0.99965125">
curred after ‘last August’, are missing. Note that
in this example, verb-argument structures encode
that ‘retired’ has TIME ‘last August’, and this
knowledge could be exploited to infer the miss-
ing relation. The work presented here stems from
two observations: (1) verbs are semantically con-
nected with concepts that are not direct syntac-
tic arguments (henceforth, implicit relations); and
(2) verb-argument structures can be leveraged to
infer implicit relations.
This paper goes beyond verb-argument struc-
tures and targets implicit relations like the one
depicted above. TIME, LOCATION, MANNER,
PURPOSE and CAUSE are inferred without im-
posing syntactic restrictions between their argu-
ments: systems trained over PropBank do not at-
tempt to extract these relations. An annotation ef-
fort demonstrates implicit relations reveal as much
as 30% of meaning on top of verb-argument struc-
tures. The main contributions are: (1) empirical
study of verb-argument structures and implicit re-
lations in PropBank; (2) annotations of implicit re-
lations on top of PropBank; (3) novel features ex-
tracted from verb-argument structures; and (4) ex-
perimental results with features derived from gold
and automatically obtained linguistic information,
showing implicit relations can be extracted in a re-
alistic environment.
</bodyText>
<figure confidence="0.995409571428571">
TIME-AFTER
NP1 VP
AGENT
Mr. Brown VBZ THEME NP2
[Joseph W. Hibben, who]AGENT
[retired]v [last August]TIME
succeeds
</figure>
<page confidence="0.980566">
145
</page>
<note confidence="0.997323">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 145–154,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997378" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99990362">
Several systems to extract verb-argument struc-
tures from plain text have been proposed (Johans-
son and Nugues, 2008; Che et al., 2010). The
work presented here complements them with ad-
ditional semantic relations. The TimeBank corpus
(Pustejovsky et al., 2003) and TempEval compe-
titions (UzZaman et al., 2013) target events and
detailed temporal information; this work also tar-
gets LOCATION, MANNER, PURPOSE and CAUSE.
Extracting missing relations is not a new prob-
lem. Early work focused on a very limited domain
(Palmer et al., 1986; Tetreault, 2002) or did not
attempt to automate the task (Whittemore et al.,
1991). This section focuses on more recent work.
Gerber and Chai (2010) augment NomBank an-
notations (Meyers et al., 2004) of 10 predicates
with additional core arguments. Their supervised
systems obtain F-measures of 42.3 and 50.3 (Ger-
ber and Chai, 2012). Laparra and Rigau (2013a)
present a deterministic algorithm and obtain an F-
measure of 45.3. In contrast, our approach does
not focus on a few selected predicates or core argu-
ments. It targets all predicates and argument mod-
ifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose
meaning is shared across verbs.
The SemEval-2010 Task 10: Linking Events
and their Participants in Discourse (Ruppenhofer
et al., 2009) targeted cross-sentence missing core
arguments in both PropBank and FrameNet (Baker
et al., 1998). Ruppenhofer et al. (2013) detail
the annotations and results. The task proved ex-
tremely difficult, participants (Chen et al., 2010;
Tonelli and Delmonte, 2010) reported overall F-
measures around 2 (out of 100). Posterior work
(Silberer and Frank, 2012; Laparra and Rigau,
2013b) reported F-measures below 20 for the same
task. The work presented here does not target
missing core arguments but modifiers within the
same sentence. Furthermore, results show our pro-
posal is useful in a real environment.
Finally, our previous work (Blanco and
Moldovan, 2011; Blanco and Moldovan, 2014)
proposed composing new relations out of chains
of previously extracted relations. This approach
is unsupervised and accurate (88% with gold an-
notations), but inferences are made only between
the ends of chains of existing relations. Our cur-
rent proposal also leverages relations previously
extracted, but productivity is higher and results
with automatic annotations are presented.
</bodyText>
<tableCaption confidence="0.857757444444444">
[But]MDIS [the surprisingly durable seven-year economic
expansion]ARG0 has [made]v [mincemeat]ARG1 [of more
than one forecast]ARG2.
Also, financial planners advising on insurance say that
to their knowledge there has not yet been [a tax
ruling]ARG0 [exempting]v [these advance payments]ARG1
[from taxes]ARG2.
Table 1: Examples of verb-argument structures
from PropBank.
</tableCaption>
<sectionHeader confidence="0.554858" genericHeader="method">
3 Verb-Argument Structures and
Implicit Relations
</sectionHeader>
<bodyText confidence="0.999856037037037">
Throughout this paper, R(x, y) denotes a seman-
tic relation R holding between x and y. R(x,
y) is interpreted “x has R y”, e.g., AGENT(took,
Bill) could be read “took has AGENT Bill”. Verb-
argument structures, or semantic roles, account for
semantic relations between a verb and its syntactic
arguments. In other words, R(x, y) is a semantic
role if ‘x’ is a verb and ‘y’ a syntactic argument
of ‘x’, and all semantic roles with ‘x’ as first ar-
gument form the verb-argument structure of verb
‘x’. Implicit relations are relations R(x, y) where
x is a verb and y is not a syntactic argument of x.
The work presented in this paper aims at com-
plementing verb-argument structures with implicit
semantic relations. We follow a practical approach
by inferring implicit relations from PropBank’s
verb-argument structures. We believe this is an
advantage since PropBank is well-known in the
field and several tools to predict PropBank annota-
tions are documented and publicly available.1 The
work presented here could be incorporated in any
NLP pipeline after role labeling without modifica-
tions to other components. Furthermore, working
on top of PropBank allows us to quantify the im-
pact of features derived from gold and automati-
cally extracted linguistic information when infer-
ring implicit relations (Section 6).
</bodyText>
<subsectionHeader confidence="0.999925">
3.1 Verb-Argument structures in PropBank
</subsectionHeader>
<bodyText confidence="0.999896142857143">
PropBank (Palmer et al., 2005) annotates verb-
argument structures on top of the syntactic trees
of the Penn TreeBank (Marcus et al., 1994). It
uses a set of numbered arguments2 (ARG0, ARG1,
ARG2, etc.) and modifiers (AM-TMP, AM-MNR,
etc.). Numbered arguments do not share a com-
mon meaning across verbs, they are defined on a
</bodyText>
<footnote confidence="0.99888675">
1E.g., Illinois SRL, http://cogcomp.cs.illinois.edu/
page/software; SENNA,http://ml.nec-labs.com/senna/;
SwiRL, http://www.surdeanu.info/mihai/swirl/
2Numbered arguments are also referred to as core.
</footnote>
<page confidence="0.987254">
146
</page>
<figure confidence="0.714700428571429">
S
NP1 VP1
THEME
NP2 VP2 VBD VP3
THEME
The first hybrid
corn seeds
VBN MANNER S-ADV were VBD TIME PP
produced
using this mechanical
approach
TIME
introduced
in the 1930s
</figure>
<figureCaption confidence="0.996476">
Figure 2: Verb-argument structures (solid arrows) and inferred implicit semantic relation (dashed arrow).
</figureCaption>
<table confidence="0.9996595">
AM-LOC: location AM-CAU: cause
AM-EXT: extent AM-TMP: time
AM-DIS: discourse connective AM-PNC: purpose
AM-ADV: general-purpose AM-MNR: manner
AM-NEG: negation marker AM-DIR: direction
AM-MOD: modal verb
</table>
<tableCaption confidence="0.976708">
Table 2: Argument modifiers in PropBank.
</tableCaption>
<table confidence="0.999792555555556">
Label # predicates % predicates
ARG0 79,334 70.26%
ARG1 106,331 94.17%
ARG2 24,560 21.75%
AM-TMP 19,756 17.50%
AM-MNR 7,833 6.94%
AM-LOC 7,198 6.37%
AM-PNC 2,784 2.47%
AM-CAU 1,563 1.38%
</table>
<tableCaption confidence="0.770963">
Table 3: Counts of selected PropBank semantic
roles. Total number of predicates is 112,917.
</tableCaption>
<bodyText confidence="0.999348294117647">
verb by verb basis in each frameset. For exam-
ple, ARG2 is used to indicate “created-from, thing
changed” with verb make and “entity exempted
from” with verb exempt (Table 1).
Unlike numbered arguments, modifiers share a
common meaning across verbs (Table 2). Some
modifiers are arguably not a semantic relation
and are not present in most relation invento-
ries (Tratz and Hovy, 2010; Hendrickx et al.,
2009). For example, AM-NEG and AM-MOD sig-
nal the presence of negation and modals, e.g.,
[woJAM-MOD[n’tJAM-NEG [goJv. For more informa-
tion about PropBank annotations and examples,
refer to the annotation guidelines.3
Inspecting PropBank annotations one can eas-
ily conclude that numbered arguments dominate
the annotations and only a few modifiers are an-
</bodyText>
<footnote confidence="0.775299">
3http://verbs.colorado.edu/˜mpalmer/projects/ace/
PBguidelines.pdf
</footnote>
<bodyText confidence="0.999981363636364">
notated (Table 3). ARG0 and ARG1 are present
in most verb-argument structures, other numbered
arguments are often not defined in the correspond-
ing frameset and are thus not annotated.
Examining PropBank one can also conclude
that information regarding TIME, LOCATION,
MANNER, CAUSE and PURPOSE for a given verb
is often present, yet not annotated because the text
encoding this knowledge is not a direct syntactic
argument of the verb (Section 4.3). Because of this
fact, we decided to focus on these five relations.
</bodyText>
<subsectionHeader confidence="0.999841">
3.2 Implicit relations in PropBank
</subsectionHeader>
<bodyText confidence="0.99998588">
Two scenarios are possible when inferring an im-
plicit relation R(x, y): (1) a semantic role R′(x′, y)
exists; or (2) such a semantic role does not exists.
In (1), y is a syntactic argument of some verb x′,
where x 7� x′ and in (2) that is not the case. Infer-
ences under scenario (1) can be further classified
into (1a) when a semantic role R′′(x, y′) such that
y′ contains y exists; or (1b) when such a semantic
roles does not exist. The remainder of this section
exemplifies the three scenarios.
The example in Figure 1 falls under scenario
(1a). Semantic roles encode, among others, ‘re-
tired’ has TIME ‘last August’, and ‘succeeds’ has
AGENT ‘Mr. Brown’ and THEME ‘Joseph W. Hi-
bben, who retired last August’. The second argu-
ment of implicit relation TIME-AFTER(succeeds,
last August) is a semantic role of ‘retired’ and is
contained in the THEME of ‘succeeds’.
Figure 2 shows a statement in which implicit re-
lation TIME(produced, in the 1930s) could be in-
ferred under scenario (1b). Semantic roles of ‘pro-
duced’ only indicate that NP2 is the THEME and
S-ADV the MANNER; roles of ‘introduced’ indi-
cate that NP1 is the THEME and ‘[in the 1930s]PP’
the TIME. In this case, there is no connection be-
</bodyText>
<page confidence="0.915498">
147
</page>
<bodyText confidence="0.988025">
rs = {TIME, LOCATION, MANNER, CAUSE, PURPOSE};
foreach semantic role R′(x′, y) such that R′ ∈ rs do
foreach verb x in the same sentence do
generate potential implicit relation R(x, y);
Algorithm 1. Procedure to generate all potential
implicit relations in scenario (1) (Section 3.2).
tween ‘produced’ and ‘[in the 1930s]PP’ or any
other node subsuming this PP in the parse tree.
Scenario (2) occurs whenever the second argu-
ment of implicit relation R(x, y) is not a syntac-
tic argument of a verb. If it were, a semantic
role R′(x′, y) would exist and it would fall un-
der scenario (1). For example, in [I]AGENT [gave]v
[her]RECIPIENT [a bookfrom 1945]THEME, we could
infer the implicit semantic relation “gave occurred
after 1945”.
</bodyText>
<sectionHeader confidence="0.989876" genericHeader="method">
4 Annotating Implicit Relations
</sectionHeader>
<bodyText confidence="0.99989075">
Inferring all implicit semantic relations is a chal-
lenging task. This paper targets implicit relations
that can be inferred under scenarios (1a, 1b); sce-
nario (2) is reserved for future work. All poten-
tial implicit relations under scenario (1) are gen-
erated using Algorithm 1. A manual annotation
effort discards potential implicit relations that do
not hold in order to create a gold standard.
</bodyText>
<subsectionHeader confidence="0.999012">
4.1 Annotation Guidelines
</subsectionHeader>
<bodyText confidence="0.9988489">
Annotators are faced with the task of deciding
whether a potential implicit relation R(x, y) holds.
If it does, they mark it with YES, otherwise with
NO. Annotators were initially trained with the
original PropBank annotation guidelines4 as this
task is very similar to annotating PropBank se-
mantic roles. Indeed, the only difference is that
‘y’ is not a syntactic argument of ‘x’.
After some preliminary annotations, we found
it useful to account for three subtypes of TIME.
This way, richer semantic connections are in-
ferred. When the task is to decide whether im-
plicit relation TIME(x, y) holds, annotators have
four labels to choose from: (1) TIME-BEFORE: x
occurred before y; (2) TIME-AFTER: x occurred
after y; (3) TIME-SAME x occurred at/duringy; and
(4) NO: y does not describe temporal information
of x. If more than one label is valid, annotators
choose the one encoding the temporal context y
of x starting the earliest. Namely, TIME-BEFORE
</bodyText>
<footnote confidence="0.83046">
4http://verbs.colorado.edu/˜mpalmer/projects/ace/
PBguidelines.pdf
</footnote>
<bodyText confidence="0.9996495625">
has the highest priority, followed by TIME-SAME,
TIME-AFTER and finally NO.
Annotation examples are detailed in Section
4.2, the more complex annotations involving TIME
are illustrated below. Consider the following state-
ment and PropBank annotations:
[The government’s decision]ARG2, v1
[reflects]v1 [their true desires before
[the next election]ARG1, v2, [expected]v2
[in late 1991]TIME, v2 ]ARG1, v1.
When annotating potential implicit semantic re-
lation R(reflects, in late 1991), annotators may
select TIME-BEFORE, TIME-SAME and TIME-
AFTER. However, they select TIME-BEFORE be-
cause it indicates the temporal context of ‘reflects’
that starts the earliest.
</bodyText>
<subsectionHeader confidence="0.997238">
4.2 Annotation Examples
</subsectionHeader>
<bodyText confidence="0.999300484848485">
Several annotations examples are shown in Ta-
ble 4. Semantic roles for statement (1) in-
clude TIME(remain, in 1990), MANNER(remain,
at about 1,200 cars) and no other TIME or MAN-
NER. Implicit relations reveal two extra seman-
tic connections: TIME-BEFORE(said, in 1990) and
TIME-BEFORE(expects, in 1990), i.e., ‘said’ and
‘expects’ occurred before ‘1990’. The potential
implicit relations MANNER(said, at about 1,200
cars) and MANNER(expects, at about 1,200 cars)
do not hold and are annotated N.
Interpreting statement (2) one can see that ‘this
past summer’ is not only indicating the TIME of
‘proposed’; events encoded by verbs ‘make’ and
‘exempt’ occurred after ‘this past summer’. In
this example, two implicit semantic relations are
inferred from a single semantic role.
Statement (3) shows that two potential implicit
relations R(x, y) and R(x′, y) sharing the sec-
ond argument ‘y’ may be assigned different la-
bels. Regarding time, semantic roles only in-
clude TIME(report, in December). Implicit rela-
tions add TIME-BEFORE(proposed, in December)
and TIME-SAME(allow, in December).
Two implicit LOCATION relations are inferred
in statement (4): ‘discovered’ and ‘preserving’
occurred ‘in the test-tube experiments’. The po-
tential implicit relation LOCATION(said, in the
test-tube experiments) is discarded (annotated N).
Statement (5) shows two potential implicit MAN-
NER that can be inferred. The ‘program’ was
‘aired’ and ‘seen by 12 million viewers’ in the fol-
lowing manner: ‘With Mr. Vila as host’.
</bodyText>
<page confidence="0.979447">
148
</page>
<table confidence="0.999805594594595">
Statement TMP LOC MNR PRP CAU
B A S N Y N Y N Y N Y N
1: Rolls-Royce said it expects [its U.S. sales]ARG1 to [remain]v [steady]ARG3 [at about 1,200 cars]MANNER [in 1990]TIME.
– said, [in 1990]TIME ✓ - - -
– expects, [in 1990]TIME ✓ - - -
– said, [at about 1,200 cars]MANNER - ✓
– expects, [at about 1,200 cars]MANNER - ✓
2: They make the argument in letters to the agency about [rule changes]ARG1 [proposed]v [this past summer]TIME that, among
other things, exempt many middle-management executives from government supervision.
– make, [this past summer]TIME - ✓ - -
– exempt, [this past summer]TIME - ✓ - -
3: The proposed changes also allow [executives]ARG0 to [report]v [exercises of options]ARG1 [in December]TIME.
– proposed, [in December]TIME ✓ - - -
– allow, [in December]TIME - - ✓ -
4: Two Japanese scientists said they discovered [an antibody that]ARG0, [in laboratory test-tube experiments]LOCATION, [kills]v
[AIDS-infected cells]ARG1 [while preserving healthy cells]TIME.
– said, [in laboratory test-tube experiments]LOCATION - ✓
– discovered, [in laboratory test-tube experiments]LOCATION ✓ -
– preserving, [in laboratory test-tube experiments]LOCATION ✓ -
5: [With Mr. Vila as host]MANNER, “[This Old House]ARG1” [became]v [one of the Public Broadcasting Service’s top 10
programs]ARG2 , [airing weekly on about 300 of the network ’s stations and seen by an average of 12 million viewers]AM-ADV.
– airing, [With Mr. Vila as host]MANNER ✓ -
– seen, [With Mr. Vila as host]MANNER ✓ -
[6: It]ARG0 [raised]v [financing of 300 billion lire]ARG1 [for the purchase this summer by another Agnelli-related group of
the food concern Galbani S.p.A.]PURPOSE, [by selling a chunk of its IFI shares to Mediobanca S.p.A.]MANNER
– selling, [for the purchase this summer by another ... ]PURPOSE ✓ -
7: [Greece and Turkey]ARG0 , for example, are suspected of [overstating]v [their arsenals]ARG1 [in hopes that they can emerge
from the arms-reduction treaty with large remaining forces to deter each other]PURPOSE.
– suspected, [in hopes that they can emerge from the ... ]PURPOSE - ✓
8: ...the rationalization that [given the country’s lack of natural resources]CAUSE, [they]ARG0 [must]AM-MOD [work]v
[hard]MANNER [to create value through exports]ARG1 and buy food with the surplus.
– create, [given the country’s lack of natural resources]CAUSE ✓ -
– buy, [given the country’s lack of natural resources]CAUSE ✓ -
9: Its third-quarter earnings were lower than analysts had forecast, and the company said [it]ARG0 had [lowered]v [its
projections for earnings growth through the end of 1990]ARG1 [because ofplanned price cuts]CAUSE.
– forecast, [because of planned price cuts]CAUSE - ✓
– said, [because of planned price cuts]CAUSE - ✓
</table>
<tableCaption confidence="0.986513">
Table 4: Examples of potential implicit relations and their annotations. All of them but the ones annotated
</tableCaption>
<bodyText confidence="0.949818642857143">
with N can be inferred. B stands for BEFORE, A for AFTER, S for SAME, N for NO and Y for YES.
PropBank semantic roles from which implicit relations are generated are indicated between brackets.
Statement (6, 7) exemplify potential implicit
PURPOSE relations. While the ‘selling’ event in
statement (6) has as its purpose ‘the purchase
[... ]’ (label Y), the ‘suspected’ event in statement
(7) is clearly not done so that ‘they (Greece and
Turkey) can emerge from the [... ]’ (label N).
Finally, statements (8, 9) exemplify potential
implicit CAUSE relations. In (8), both ‘create’ and
‘buy’ are done due to the ‘country’s lack ofnatural
resources’. However, in (9), the analysts ‘forecast-
ing’ and the company ‘saying’ do not have as their
cause ‘planned price cuts’.
</bodyText>
<subsectionHeader confidence="0.999585">
4.3 Annotation Analysis
</subsectionHeader>
<bodyText confidence="0.999750136363636">
Table 5 shows counts for all potential implicit re-
lations annotated. All labels except N indicate a
valid implicit relation. 94.1% of potential implicit
relations generated from a TIME semantic role can
be inferred. Other roles yield less inferences in
relative terms, but substantial additional mean-
ing: LOCATION 39.4%, MANNER 16.7%, PUR-
POSE 29.4%, and CAUSE 30.2%.
Two annotators performed the annotations. A
simple script generated all potential implicit rela-
tions and prompted for a label: BEFORE, AFTER,
SAME or NO if the potential implicit relation was
generated from a TIME semantic role; YES or NO
otherwise. Annotators are not concerned with ar-
gument identification, as arguments of implicit re-
lations are retrieved from the verb-argument struc-
tures in PropBank (Algorithm 1). This makes the
annotation process easier and faster.
Annotation quality was calculated with two
agreement coefficients: observed agreement (raw
percentage of equal annotations) and Cohen’s κ
(Artstein and Poesio, 2008). The actual num-
</bodyText>
<page confidence="0.98913">
149
</page>
<table confidence="0.999877157894737">
Source No. Name Description
basic x 1,2 word, POS tag x’s surface form and part-of-speech tag
3 voice whether x is in active or passive voice
y 4,5 first word, POS tag first word and part of speech tag in y
6,7 last word, POS tag last word and part-of-speech tag in y
8,9 head, POS tag head of y and its part-of-speech tag
10–12 node, left and right sibling syntactic nodes of y, and its left and right siblings
13 subcategory concatenation of y’s children nodes
x, y 14 direction whether x occurs before or after y
15 subsumer common syntactic node between x and y
16 path syntactic path between x and y
pred structures x ps 17–31 verb semantic roles flags indicating presence of semantic roles in x ps
y ps 32,33 verb, POS tag verb in y ps and its part-of-speech tag
34 arg label semantic role between verb in y ps and y
35–49 arg semantic roles flags indicating presence of semantic roles in y ps
x ps, 50 overlapping semantic role role R″linking x and y′, where y′ contains y
y ps
51 overlapping head head of y′ in semantic role detected in feature 50
52 overlapping direct whether feature 51 is the verb in y ps
</table>
<tableCaption confidence="0.996021">
Table 6: Complete feature set to determine whether a potential implicit semantic relation R(x, y) should
be inferred. Second column indicates the source: first or second argument (x, y), or their respective
predicate structures (x ps, y ps). Features in bold are novel and specially designed for our task.
</tableCaption>
<table confidence="0.999939222222223">
Label # instances % instances
TIME B 3,033 38.4%
A 2,886 36.5%
S 1,514 19.2%
N 463 5.9%
All 7,896 100.0%
LOCATION Y 3,345 39.4%
N 5,151 60.6%
All 8,496 100.0%
MANNER Y 1,600 16.7%
N 7,987 83.3%
All 9,587 100.0%
PURPOSE Y 821 29.4%
N 1,971 70.6%
All 2,792 100.0%
CAUSE Y 404 30.2%
N 909 69.2%
All 1,313 100.0%
</table>
<tableCaption confidence="0.955745">
Table 5: Number of potential implicit relations (in-
</tableCaption>
<bodyText confidence="0.994695785714286">
stances) annotated and counts for each label. Total
number of instances is 30,084.
bers are: 78.16% (observed) / 0.687 (κ) for TIME,
86.63% / 0.733 for LOCATION, 93.02% / 0.782
for MANNER, 88.60% / 0.734 for PURPOSE, and
90.91% / 0.810 for CAUSE. These agreements
are either comparable or superior to similar pre-
vious annotation efforts. Girju et al. (2007) re-
ported observed agreements between 47.8% and
86.1% when annotating 7 semantic relations be-
tween nominals, and Bethard et al. (2008) ob-
served agreements of 81.2% and 77.8% (Kappa:
0.715 and 0.556) when annotating temporal and
causal relations between event pairs.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="method">
5 Inferring Implicit Relations
</sectionHeader>
<bodyText confidence="0.999972777777778">
Inferring implicit relations is reduced to (1) gener-
ating potential implicit relations (Algorithm 1) and
(2) labeling them. The second task determines if
potential implicit relations should be discarded or
inferred, all labels but N indicate potential implicit
relations that should be inferred. We follow a stan-
dard supervised machine learning approach where
each potential implicit relation is an instance.
Instances were divided into training (70%) and
test (30%). The feature set (Section 5.1) and
model parameters were tuned using 10-fold strat-
ified cross-validation over the training split, and
results (Section 6) are reported using the test split.
More features than the ones presented were tried
and discarded because they did not improve per-
formance, e.g., syntactic path between verbs in the
verb-argument structures of x and y, depth of both
structures, number of tokens in y.
</bodyText>
<subsectionHeader confidence="0.989433">
5.1 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999927090909091">
The full set of features to determine whether a po-
tential implicit relation R(x, y) can be inferred is
summarized in Table 6. Features are classified
into basic and predicate structures. The former
are commonly used by semantic role labelers. The
latter exploit the output of role labelers, i.e., verb-
argument structures, and, to our knowledge, are
novel. Results show predicate structures features
improve performance (Section 6.2).
Basic features are derived from lexical and syn-
tactic information. We do not elaborate more on
</bodyText>
<page confidence="0.996331">
150
</page>
<table confidence="0.999663555555556">
Feat Io. Value
1,2 succeeds, VBZ
3 active
4,5 last, JJ
6,7 August, NNP
8,9 August, NNP
10–12 NP, VBD, nil
13 JJ-NNP
14 after
15 VP
16 VBZ+VP-NP-SBAR-S-VP-NP
17–31 ARG0 and ARG1 true, rest false
32,33 retired, VBD
34 AM-TMP
35-49 ARG0 and AM-TMP true, rest false
50 ARG1
51 Hibben
52 false
</table>
<tableCaption confidence="0.695874333333333">
Table 7: Feature values when deciding if
R(succeeds, last summer) can be inferred from the
verb-argument structures in Figure 1.
</tableCaption>
<bodyText confidence="0.999253321428572">
these features, detailed descriptions and examples
are provided by Gildea and Jurafsky (2002).
Features (17–52) are derived from the predicate
structures of x and y and specially defined to infer
implicit semantic relations. Features (17–31, 35–
49) are flags indicating the presence of semantic
roles in the predicate structures of x and y.
Features (32–34) characterize the semantic role
R′(x′, y) from which the potential implicit relation
was generated. They specify verb x′, its part-of-
speech, and label R′. Note that x′ is not present in
the potential implicit relation R(x, y), but incorpo-
rating this information helps determining whether
a relation actually holds as well as label R (TIME-
BEFORE, TIME-AFTER, TIME-SAME, etc.).
Finally, features 50–52 apply to inferences un-
der scenario (1a) (Section 3.2). Feature (50) indi-
cates the semantic role R′′(x, y′), if any, such that
y′ contains y. Feature (51) indicates the head of ar-
gument y′ found in feature (50). Feature (52) cap-
tures whether the head calculated in feature (51) is
the verb in the predicate structure of y.
Table 7 exemplifies all features when deciding
whether TIME-AFTER(succeeds, last August) can
be inferred from the verb-argument structures in
Mr. Brown succeeds Joseph W. Hibben, who re-
tired last August (Figure 1). Table 8 provides an
additional example for features 50–52.
</bodyText>
<sectionHeader confidence="0.994759" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9658995">
Experiments were carried out using Support Vec-
tor Machines with RBF kernel as implemented in
</bodyText>
<table confidence="0.9962869">
Mr. Corr resigned to pursue other interests, the airline said.
ARG0(resigned, Mr. Corr)
AM-PNC(resigned, to pursue other interests)
ARG0(pursue, Mr. Corr)
ARG1(pursue, other interests)
ARG0(said, the airline)
ARG1(said, Mr. Corr resigned to pursue other interests)
feature 50, overlapping sem rel ARG1
feature 51, overlapping head resigned
feature 52, overlapping direct true
</table>
<tableCaption confidence="0.78892">
Table 8: PropBank roles and values for features
(50–52) when predicting potential implicit relation
R(said, to pursue other interests), labeled N.
</tableCaption>
<bodyText confidence="0.738051083333333">
LIBSVM (Chang and Lin, 2011). Parameters α
and γ were tuned by grid search using 10-fold
cross validation over training instances.
Results are reported using features extracted
from gold and automatic annotations. Gold anno-
tations are taken directly from the Penn TreeBank
and PropBank. Automatic annotations are ob-
tained with Polaris (Moldovan and Blanco, 2012),
a semantic parser that among others is trained with
PropBank. Results using gold (automatic) annota-
tions are obtained with a model trained with gold
(automatic) annotations.
</bodyText>
<subsectionHeader confidence="0.99877">
6.1 Detailed Results
</subsectionHeader>
<bodyText confidence="0.99997824">
Table 9 presents per-relation and overall results. In
general terms, there is a decrease in performance
when using automatic annotations. The difference
is most noticeable in recall and it is due to missing
semantic roles, which in turn are often due to syn-
tactic parsing errors. This is not surprising as in
order for an implicit relation R(x, y) to be gener-
ated as potential and fed to the learning algorithm
for classification, a semantic role R′(x′, y) must be
extracted first (Algorithm 1). However, using au-
tomatic annotations brings very little decrease in
precision. This leads to the conclusion that as long
as ‘y’ is identified as a semantic role of some verb,
even if it is mislabeled, one can still infer the right
implicit relations. Since results obtained with au-
tomatic parse trees and semantic roles are a realis-
tic estimation of performance, the remainder of the
discussion focuses on those. Results with gold an-
notations are provided for informational purposes.
Overall results for inferring implicit semantic
relations are encouraging: precision 0.66, recall
0.58 and F-measure 0.616. Direct comparison
with previous work is not possible because the
implicit relations we aim at inferring have not
been considered before. However, we note the top
</bodyText>
<page confidence="0.99389">
151
</page>
<table confidence="0.999214818181818">
gold automatic
basic basic + ps basic basic + ps
P R F P R F P R F P R F
TIME B .66 .72 .689 .72 .74 ∗.730 .64 .65 .643 .68 .67 .677
A .63 .74 .681 .67 .75 .708 .61 .68 .642 .66 .72 .687
S .57 .41 .477 .54 .45 .491 .55 .36 .437 .55 .38 .450
LOCATION Y .71 .61 .656 .70 .64 .669 .71 .56 .624 .71 .58 .635
MANNER Y .65 .38 .480 .60 .45 .514 .54 .45 .489 .64 .41 .500
PURPOSE Y .65 .58 .613 .69 .60 .642 .56 .49 .525 .68 .49 .572
CAUSE Y .71 .60 .650 .74 .62 .675 .69 .65 .670 .71 .63 .669
All .66 .61 .625 .67 .64 ∗.651 .63 .57 .591 .66 .58 ∗.616
</table>
<tableCaption confidence="0.739509666666667">
Table 9: Results obtained with the test split using features extracted from gold and automatic annotations,
and using basic and predicate structures (ps) features. Statistical significance between F-measures using
basic and basic + predicate structures features is indicated with ∗ (confidence 95%).
</tableCaption>
<bodyText confidence="0.997536266666667">
performer (Koomen et al., 2005) at CoNLL-2005
Shared Task on role labeling obtained the follow-
ing F-measures when extracting the same relations
between a verb and its syntactic arguments: 0.774
(TIME), 0.6033 (LOCATION), 0.5922 (MANNER),
0.4541 (PURPOSE) and 0.5397 (CAUSE).
The most difficult relations are TIME-SAME and
MANNER, F-measures are 0.450 and 0.500 re-
spectively. Even when using gold annotations
these two relations are challenging: F-measures
are 0.491 for TIME-SAME, an increase of 9.1%,
and 0.514 for MANNER, an increase of 2.8%. Re-
sults show that other relations can be inferred with
F-measures between 0.635 and 0.687, the only ex-
ception is PURPOSE with an F-measure of 0.572.
</bodyText>
<subsectionHeader confidence="0.999097">
6.2 Feature Ablation
</subsectionHeader>
<bodyText confidence="0.999952727272727">
Results in Table 9 suggest that while implicit rela-
tions can be inferred using basic features, it is ben-
eficial to complement them with the novel features
derived from predicate structures. This is true for
all relations except CAUSE when using automatic
annotations with a negligible difference of 0.001.
When considering all implicit relations, the differ-
ence in performance is 0.616 − 0.591 = 0.025,
an increase of 4.2% that is statistically significant
(Z-test, confidence 95%).
The positive impact of features derived from
predicate structures is most noticeable when infer-
ring PURPOSE, with an increase of 8.9% (0.572 −
0.525 = 0.047). TIME-BEFORE and TIME-AFTER
also benefit, with increases of 5.3% (0.677 −
0.643 = 0.034) and 7.0% (0.687−0.642 = 0.045)
respectively. The improvement predicate struc-
tures features bring is statistically significant when
taking into account all relations (confidence 95%).
However, due to the lower number of instances,
differences in performance when considering in-
dividual relations is not statistically significant.
</bodyText>
<sectionHeader confidence="0.994668" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999952172413793">
Verb-argument structures, or semantic roles, com-
prise semantic relations between a verb and its
syntactic arguments. The work presented in this
paper leverages verb-argument structures to infer
implicit semantic relations. A relation R(x, y) is
implicit if x is a verb and y is not a syntactic ar-
gument of x. The method could be incorporated
into any NLP pipeline after role labeling without
modifications to other components.
An analysis of verb-argument structures and im-
plicit relations in PropBank has been presented.
Out of all potential implicit relations R(x, y), this
paper targets those that can be generated from a
semantic role R′(x′, y), where x =� x′. A man-
ual annotation effort demonstrates implicit rela-
tions yield substantial additional meaning. Most
of the time (94.1%) a semantic role TIME(x′, y)
is present, we can infer temporal information for
other verbs within the same sentence. Productiv-
ity is lower but substantial with other roles: 39.4%
(LOCATION), 30.2% (CAUSE), 29.4% (PURPOSE)
and 16.7% (MANNER).
Experimental results show that implicit rela-
tions can be inferred using automatically obtained
parse trees and verb-argument structures. Stan-
dard machine learning is used to decide whether a
potential implicit relation should be inferred, and
novel features characterizing the verb-argument
structures we infer from have been proposed.
</bodyText>
<page confidence="0.996927">
152
</page>
<sectionHeader confidence="0.990072" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999747477477478">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596, December.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ’98, pages 86–90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Steven Bethard, William Corvey, Sara Klingenstein,
and James H. Martin. 2008. Building a Corpus of
Temporal-Causal Structure. In Proceedings of the
Sixth International Language Resources and Evalu-
ation (LREC’08), pages 908–915, Marrakech, Mo-
rocco. European Language Resources Association
(ELRA).
Eduardo Blanco and Dan Moldovan. 2011. Un-
supervised learning of semantic relation composi-
tion. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 1456–1465,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Eduardo Blanco and Dan Moldovan. 2014. Compo-
sition of semantic relations: Theoretical framework
and case study. ACM Trans. Speech Lang. Process.,
10(4):17:1–17:36, January.
Xavier Carreras and Lluis M`arquez. 2005. Intro-
duction to the CoNLL-2005 shared task: semantic
role labeling. In CONLL ’05: Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning, pages 152–164, Morristown, NJ,
USA. Association for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.
Wanxiang Che, Ting Liu, and Yongqiang Li. 2010. Im-
proving Semantic Role Labeling with Word Sense.
In The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 246–249, Los Angeles, Califor-
nia, June. Association for Computational Linguis-
tics.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. Semafor: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, pages 264–267, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A Study of Implicit Arguments for Nomi-
nal Predicates. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1583–1592, Uppsala, Sweden, July.
Association for Computational Linguistics.
Matthew Gerber and Joyce Chai. 2012. Seman-
tic role labeling of implicit arguments for nominal
predicates. Computational Linguistics, 38:755–798,
2012.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling Of Semantic Roles. Computational Lin-
guistics, 28:245–288.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 13–18, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Iris Hendrickx, Su N. Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid, Sebastian Pad´o, Marco Pennac-
chiotti, Lorenza Romano, and Stan Szpakowicz.
2009. SemEval-2010 Task 8: Multi-Way Classifica-
tion of Semantic Relations Between Pairs of Nom-
inals. In Proceedings of the Workshop on Seman-
tic Evaluations: Recent Achievements and Future
Directions (SEW-2009), pages 94–99, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of prop-
bank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’08, pages 69–78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen T. Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005), pages
181–184, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Egoitz Laparra and German Rigau. 2013a. Impar: A
deterministic algorithm for implicit semantic role la-
belling. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 1180–1189,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Egoitz Laparra and German Rigau. 2013b. Sources of
evidence for implicit argument resolution. In Pro-
ceedings of the 10th International Conference on
Computational Semantics (IWCS 2013) – Long Pa-
pers, pages 155–166, Potsdam, Germany, March.
Association for Computational Linguistics.
Mitchel Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
</reference>
<page confidence="0.989705">
153
</page>
<reference confidence="0.999925637362637">
corpus of English: The Penn Treebank. Computa-
tional linguistics, 19(2):313–330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
Annotating Noun Argument Structure for Nom-
Bank. In Proceedings of LREC-2004, pages 803–
806, Lisbon, Portugal.
Dan Moldovan and Eduardo Blanco. 2012. Polaris:
Lymba’s semantic parser. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation (LREC-2012), pages 66–
72, Istanbul, Turkey, May. European Language Re-
sources Association (ELRA).
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and
John Dowding. 1986. Recovering implicit infor-
mation. In Proceedings of the 24th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 10–19, New York, New York, USA, July.
Association for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71–106.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287, June.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003. The timebank corpus. Corpus
linguistics, 2003:40.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2009.
SemEval-2010 Task 10: Linking Events and Their
Participants in Discourse. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009),
pages 106–111, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Josef Ruppenhofer, Russell Lee-Goldman, Caroline
Sporleder, and Roser Morante. 2013. Beyond
sentence-level semantic role labeling: linking argu-
ment structures in discourse. Language Resources
and Evaluation, 47(3):695–721.
Carina Silberer and Anette Frank. 2012. Casting im-
plicit role linking as an anaphora resolution task.
In *SEM 2012: The First Joint Conference on
Lexical and Computational Semantics, pages 1–10,
Montr´eal, Canada, 7-8 June. Association for Com-
putational Linguistics.
Joel R Tetreault. 2002. Implicit role reference. In In-
ternational Symposium on Reference Resolution for
Natural Language Processing, pages 109–115.
Sara Tonelli and Rodolfo Delmonte. 2010. Venses++:
Adapting a deep semantic processing system to the
identification of null instantiations. In Proceedings
ofthe 5th International Workshop on Semantic Eval-
uation, pages 296–299, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2010. A Taxonomy,
Dataset, and Classifier for Automatic Noun Com-
pound Interpretation. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 678–687, Uppsala, Sweden, July.
Association for Computational Linguistics.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval2013), pages 1–9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Greg Whittemore, Melissa Macpherson, and Greg
Carlson. 1991. Event-building through role-filling
and anaphora resolution. In Proceedings of the 29th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 17–24, Berkeley, Califor-
nia, USA, June. Association for Computational Lin-
guistics.
Be N. Zapirain, Eneko Agirre, Lluis M`arquez, and Mi-
hai Surdeanu. 2010. Improving Semantic Role
Classification with Selectional Preferences. In The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 373–376, Los Angeles, California,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.999772">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865911">
<title confidence="0.998317">Leveraging Verb-Argument Structures to Infer Semantic Relations</title>
<author confidence="0.906842">Blanco</author>
<affiliation confidence="0.998494">Lymba Corporation</affiliation>
<address confidence="0.973816">Richardson, TX 75080 USA</address>
<email confidence="0.999853">eduardo@lymba.com</email>
<email confidence="0.999853">moldovan@lymba.com</email>
<abstract confidence="0.9981718">This paper presents a methodology to infer implicit semantic relations from verbargument structures. An annotation effort shows implicit relations boost the amount of meaning explicitly encoded for verbs. Experimental results with automatically obtained parse trees and verb-argument structures demonstrate that inferring implicit relations is a doable task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="20837" citStr="Artstein and Poesio, 2008" startWordPosition="3232" endWordPosition="3235">ors performed the annotations. A simple script generated all potential implicit relations and prompted for a label: BEFORE, AFTER, SAME or NO if the potential implicit relation was generated from a TIME semantic role; YES or NO otherwise. Annotators are not concerned with argument identification, as arguments of implicit relations are retrieved from the verb-argument structures in PropBank (Algorithm 1). This makes the annotation process easier and faster. Annotation quality was calculated with two agreement coefficients: observed agreement (raw percentage of equal annotations) and Cohen’s κ (Artstein and Poesio, 2008). The actual num149 Source No. Name Description basic x 1,2 word, POS tag x’s surface form and part-of-speech tag 3 voice whether x is in active or passive voice y 4,5 first word, POS tag first word and part of speech tag in y 6,7 last word, POS tag last word and part-of-speech tag in y 8,9 head, POS tag head of y and its part-of-speech tag 10–12 node, left and right sibling syntactic nodes of y, and its left and right siblings 13 subcategory concatenation of y’s children nodes x, y 14 direction whether x occurs before or after y 15 subsumer common syntactic node between x and y 16 path syntac</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics -Volume 1, ACL ’98,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5181" citStr="Baker et al., 1998" startWordPosition="789" endWordPosition="792"> with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et al., 2010; Tonelli and Delmonte, 2010) reported overall Fmeasures around 2 (out of 100). Posterior work (Silberer and Frank, 2012; Laparra and Rigau, 2013b) reported F-measures below 20 for the same task. The work presented here does not target missing core arguments but modifiers within the same sentence. Furthermore, results show our proposal is useful in a real environment. Finally, our previous work (Blanco and Moldovan, 2011; Blanco and Moldovan, 2014) proposed compos</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics -Volume 1, ACL ’98, pages 86–90, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>William Corvey</author>
<author>Sara Klingenstein</author>
<author>James H Martin</author>
</authors>
<title>Building a Corpus of Temporal-Causal Structure.</title>
<date>2008</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<pages>908--915</pages>
<location>Marrakech,</location>
<contexts>
<context position="23131" citStr="Bethard et al. (2008)" startWordPosition="3648" endWordPosition="3651"> N 1,971 70.6% All 2,792 100.0% CAUSE Y 404 30.2% N 909 69.2% All 1,313 100.0% Table 5: Number of potential implicit relations (instances) annotated and counts for each label. Total number of instances is 30,084. bers are: 78.16% (observed) / 0.687 (κ) for TIME, 86.63% / 0.733 for LOCATION, 93.02% / 0.782 for MANNER, 88.60% / 0.734 for PURPOSE, and 90.91% / 0.810 for CAUSE. These agreements are either comparable or superior to similar previous annotation efforts. Girju et al. (2007) reported observed agreements between 47.8% and 86.1% when annotating 7 semantic relations between nominals, and Bethard et al. (2008) observed agreements of 81.2% and 77.8% (Kappa: 0.715 and 0.556) when annotating temporal and causal relations between event pairs. 5 Inferring Implicit Relations Inferring implicit relations is reduced to (1) generating potential implicit relations (Algorithm 1) and (2) labeling them. The second task determines if potential implicit relations should be discarded or inferred, all labels but N indicate potential implicit relations that should be inferred. We follow a standard supervised machine learning approach where each potential implicit relation is an instance. Instances were divided into </context>
</contexts>
<marker>Bethard, Corvey, Klingenstein, Martin, 2008</marker>
<rawString>Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin. 2008. Building a Corpus of Temporal-Causal Structure. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), pages 908–915, Marrakech, Morocco. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduardo Blanco</author>
<author>Dan Moldovan</author>
</authors>
<title>Unsupervised learning of semantic relation composition.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1456--1465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="5737" citStr="Blanco and Moldovan, 2011" startWordPosition="876" endWordPosition="879">ing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et al., 2010; Tonelli and Delmonte, 2010) reported overall Fmeasures around 2 (out of 100). Posterior work (Silberer and Frank, 2012; Laparra and Rigau, 2013b) reported F-measures below 20 for the same task. The work presented here does not target missing core arguments but modifiers within the same sentence. Furthermore, results show our proposal is useful in a real environment. Finally, our previous work (Blanco and Moldovan, 2011; Blanco and Moldovan, 2014) proposed composing new relations out of chains of previously extracted relations. This approach is unsupervised and accurate (88% with gold annotations), but inferences are made only between the ends of chains of existing relations. Our current proposal also leverages relations previously extracted, but productivity is higher and results with automatic annotations are presented. [But]MDIS [the surprisingly durable seven-year economic expansion]ARG0 has [made]v [mincemeat]ARG1 [of more than one forecast]ARG2. Also, financial planners advising on insurance say that t</context>
</contexts>
<marker>Blanco, Moldovan, 2011</marker>
<rawString>Eduardo Blanco and Dan Moldovan. 2011. Unsupervised learning of semantic relation composition. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1456–1465, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduardo Blanco</author>
<author>Dan Moldovan</author>
</authors>
<title>Composition of semantic relations: Theoretical framework and case study.</title>
<date>2014</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="5765" citStr="Blanco and Moldovan, 2014" startWordPosition="880" endWordPosition="883">PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et al., 2010; Tonelli and Delmonte, 2010) reported overall Fmeasures around 2 (out of 100). Posterior work (Silberer and Frank, 2012; Laparra and Rigau, 2013b) reported F-measures below 20 for the same task. The work presented here does not target missing core arguments but modifiers within the same sentence. Furthermore, results show our proposal is useful in a real environment. Finally, our previous work (Blanco and Moldovan, 2011; Blanco and Moldovan, 2014) proposed composing new relations out of chains of previously extracted relations. This approach is unsupervised and accurate (88% with gold annotations), but inferences are made only between the ends of chains of existing relations. Our current proposal also leverages relations previously extracted, but productivity is higher and results with automatic annotations are presented. [But]MDIS [the surprisingly durable seven-year economic expansion]ARG0 has [made]v [mincemeat]ARG1 [of more than one forecast]ARG2. Also, financial planners advising on insurance say that to their knowledge there has </context>
</contexts>
<marker>Blanco, Moldovan, 2014</marker>
<rawString>Eduardo Blanco and Dan Moldovan. 2014. Composition of semantic relations: Theoretical framework and case study. ACM Trans. Speech Lang. Process., 10(4):17:1–17:36, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: semantic role labeling.</title>
<date>2005</date>
<booktitle>In CONLL ’05: Proceedings of the Ninth Conference on Computational Natural Language Learning,</booktitle>
<pages>152--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Lluis M`arquez. 2005. Introduction to the CoNLL-2005 shared task: semantic role labeling. In CONLL ’05: Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 152–164, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="27180" citStr="Chang and Lin, 2011" startWordPosition="4282" endWordPosition="4285"> out using Support Vector Machines with RBF kernel as implemented in Mr. Corr resigned to pursue other interests, the airline said. ARG0(resigned, Mr. Corr) AM-PNC(resigned, to pursue other interests) ARG0(pursue, Mr. Corr) ARG1(pursue, other interests) ARG0(said, the airline) ARG1(said, Mr. Corr resigned to pursue other interests) feature 50, overlapping sem rel ARG1 feature 51, overlapping head resigned feature 52, overlapping direct true Table 8: PropBank roles and values for features (50–52) when predicting potential implicit relation R(said, to pursue other interests), labeled N. LIBSVM (Chang and Lin, 2011). Parameters α and γ were tuned by grid search using 10-fold cross validation over training instances. Results are reported using features extracted from gold and automatic annotations. Gold annotations are taken directly from the Penn TreeBank and PropBank. Automatic annotations are obtained with Polaris (Moldovan and Blanco, 2012), a semantic parser that among others is trained with PropBank. Results using gold (automatic) annotations are obtained with a model trained with gold (automatic) annotations. 6.1 Detailed Results Table 9 presents per-relation and overall results. In general terms, </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Yongqiang Li</author>
</authors>
<title>Improving Semantic Role Labeling with Word Sense.</title>
<date>2010</date>
<booktitle>In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>246--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="1259" citStr="Che et al., 2010" startWordPosition="180" endWordPosition="183">g of text. Semantic relations explicitly encode links between concepts. For example, in The accident left him a changed man, the ‘accident’ is the CAUSE of the man undergoing some ‘change’. A question answering system would benefit from detecting this relation when answering Why did he change? Extracting all semantic relations from text is a monumental task and is at the core of language understanding. In recent years, approaches that aim at extracting a subset of all relations have achieved great success. In particular, previous research (Carreras and M`arquez, 2005; Punyakanok et al., 2008; Che et al., 2010; Zapirain et al., 2010) focused on verb-argument structures, i.e., relations between a verb and its syntactic arguments. PropBank (Palmer et al., 2005) is the corpus of reference for verb-argument relations. However, relations between a verb and its syntactic arguments are only a fraction of the relations present in texts. Consider the statement [Mr. Brown]NP1 succeeds [Joseph W. Hibben, who retired last August]NP2 and its parse tree (Figure 1). Verbargument relations encode that NP1 is the AGENT and NP2 is the THEME of verb ‘succeeds’ (PropBank uses labels ARG0 and ARG1). Any semantic relati</context>
<context position="3946" citStr="Che et al., 2010" startWordPosition="594" endWordPosition="597">rived from gold and automatically obtained linguistic information, showing implicit relations can be extracted in a realistic environment. TIME-AFTER NP1 VP AGENT Mr. Brown VBZ THEME NP2 [Joseph W. Hibben, who]AGENT [retired]v [last August]TIME succeeds 145 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 145–154, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Several systems to extract verb-argument structures from plain text have been proposed (Johansson and Nugues, 2008; Che et al., 2010). The work presented here complements them with additional semantic relations. The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) </context>
</contexts>
<marker>Che, Liu, Li, 2010</marker>
<rawString>Wanxiang Che, Ting Liu, and Yongqiang Li. 2010. Improving Semantic Role Labeling with Word Sense. In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 246–249, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desai Chen</author>
<author>Nathan Schneider</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Semafor: Frame argument resolution with log-linear models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>264--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5313" citStr="Chen et al., 2010" startWordPosition="809" endWordPosition="812">u (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et al., 2010; Tonelli and Delmonte, 2010) reported overall Fmeasures around 2 (out of 100). Posterior work (Silberer and Frank, 2012; Laparra and Rigau, 2013b) reported F-measures below 20 for the same task. The work presented here does not target missing core arguments but modifiers within the same sentence. Furthermore, results show our proposal is useful in a real environment. Finally, our previous work (Blanco and Moldovan, 2011; Blanco and Moldovan, 2014) proposed composing new relations out of chains of previously extracted relations. This approach is unsupervised and accurate (88% with gold annotat</context>
</contexts>
<marker>Chen, Schneider, Das, Smith, 2010</marker>
<rawString>Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A. Smith. 2010. Semafor: Frame argument resolution with log-linear models. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 264–267, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Chai</author>
</authors>
<title>Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1583--1592</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="4495" citStr="Gerber and Chai (2010)" startWordPosition="682" endWordPosition="685">in text have been proposed (Johansson and Nugues, 2008; Che et al., 2010). The work presented here complements them with additional semantic relations. The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cr</context>
</contexts>
<marker>Gerber, Chai, 2010</marker>
<rawString>Matthew Gerber and Joyce Chai. 2010. Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583–1592, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Chai</author>
</authors>
<title>Semantic role labeling of implicit arguments for nominal predicates.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<contexts>
<context position="4678" citStr="Gerber and Chai, 2012" startWordPosition="710" endWordPosition="714">y et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficu</context>
</contexts>
<marker>Gerber, Chai, 2012</marker>
<rawString>Matthew Gerber and Joyce Chai. 2012. Semantic role labeling of implicit arguments for nominal predicates. Computational Linguistics, 38:755–798, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling Of Semantic Roles. Computational Linguistics,</title>
<date>2002</date>
<pages>28--245</pages>
<contexts>
<context position="25249" citStr="Gildea and Jurafsky (2002)" startWordPosition="3983" endWordPosition="3986"> (Section 6.2). Basic features are derived from lexical and syntactic information. We do not elaborate more on 150 Feat Io. Value 1,2 succeeds, VBZ 3 active 4,5 last, JJ 6,7 August, NNP 8,9 August, NNP 10–12 NP, VBD, nil 13 JJ-NNP 14 after 15 VP 16 VBZ+VP-NP-SBAR-S-VP-NP 17–31 ARG0 and ARG1 true, rest false 32,33 retired, VBD 34 AM-TMP 35-49 ARG0 and AM-TMP true, rest false 50 ARG1 51 Hibben 52 false Table 7: Feature values when deciding if R(succeeds, last summer) can be inferred from the verb-argument structures in Figure 1. these features, detailed descriptions and examples are provided by Gildea and Jurafsky (2002). Features (17–52) are derived from the predicate structures of x and y and specially defined to infer implicit semantic relations. Features (17–31, 35– 49) are flags indicating the presence of semantic roles in the predicate structures of x and y. Features (32–34) characterize the semantic role R′(x′, y) from which the potential implicit relation was generated. They specify verb x′, its part-ofspeech, and label R′. Note that x′ is not present in the potential implicit relation R(x, y), but incorporating this information helps determining whether a relation actually holds as well as label R (T</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling Of Semantic Roles. Computational Linguistics, 28:245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>SemEval-2007 Task 04: Classification of Semantic Relations between Nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>13--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="22997" citStr="Girju et al. (2007)" startWordPosition="3627" endWordPosition="3630">100.0% LOCATION Y 3,345 39.4% N 5,151 60.6% All 8,496 100.0% MANNER Y 1,600 16.7% N 7,987 83.3% All 9,587 100.0% PURPOSE Y 821 29.4% N 1,971 70.6% All 2,792 100.0% CAUSE Y 404 30.2% N 909 69.2% All 1,313 100.0% Table 5: Number of potential implicit relations (instances) annotated and counts for each label. Total number of instances is 30,084. bers are: 78.16% (observed) / 0.687 (κ) for TIME, 86.63% / 0.733 for LOCATION, 93.02% / 0.782 for MANNER, 88.60% / 0.734 for PURPOSE, and 90.91% / 0.810 for CAUSE. These agreements are either comparable or superior to similar previous annotation efforts. Girju et al. (2007) reported observed agreements between 47.8% and 86.1% when annotating 7 semantic relations between nominals, and Bethard et al. (2008) observed agreements of 81.2% and 77.8% (Kappa: 0.715 and 0.556) when annotating temporal and causal relations between event pairs. 5 Inferring Implicit Relations Inferring implicit relations is reduced to (1) generating potential implicit relations (Algorithm 1) and (2) labeling them. The second task determines if potential implicit relations should be discarded or inferred, all labels but N indicate potential implicit relations that should be inferred. We foll</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. SemEval-2007 Task 04: Classification of Semantic Relations between Nominals. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 13–18, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Iris Hendrickx</author>
<author>Su N Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sebastian Pad´o Diarmuid</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009),</booktitle>
<pages>94--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="9649" citStr="Hendrickx et al., 2009" startWordPosition="1466" endWordPosition="1469">1 94.17% ARG2 24,560 21.75% AM-TMP 19,756 17.50% AM-MNR 7,833 6.94% AM-LOC 7,198 6.37% AM-PNC 2,784 2.47% AM-CAU 1,563 1.38% Table 3: Counts of selected PropBank semantic roles. Total number of predicates is 112,917. verb by verb basis in each frameset. For example, ARG2 is used to indicate “created-from, thing changed” with verb make and “entity exempted from” with verb exempt (Table 1). Unlike numbered arguments, modifiers share a common meaning across verbs (Table 2). Some modifiers are arguably not a semantic relation and are not present in most relation inventories (Tratz and Hovy, 2010; Hendrickx et al., 2009). For example, AM-NEG and AM-MOD signal the presence of negation and modals, e.g., [woJAM-MOD[n’tJAM-NEG [goJv. For more information about PropBank annotations and examples, refer to the annotation guidelines.3 Inspecting PropBank annotations one can easily conclude that numbered arguments dominate the annotations and only a few modifiers are an3http://verbs.colorado.edu/˜mpalmer/projects/ace/ PBguidelines.pdf notated (Table 3). ARG0 and ARG1 are present in most verb-argument structures, other numbered arguments are often not defined in the corresponding frameset and are thus not annotated. Ex</context>
</contexts>
<marker>Hendrickx, Kim, Kozareva, Nakov, Diarmuid, Pennacchiotti, Romano, Szpakowicz, 2009</marker>
<rawString>Iris Hendrickx, Su N. Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2009. SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), pages 94–99, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based semantic role labeling of propbank.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>69--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3927" citStr="Johansson and Nugues, 2008" startWordPosition="589" endWordPosition="593">tal results with features derived from gold and automatically obtained linguistic information, showing implicit relations can be extracted in a realistic environment. TIME-AFTER NP1 VP AGENT Mr. Brown VBZ THEME NP2 [Joseph W. Hibben, who]AGENT [retired]v [last August]TIME succeeds 145 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 145–154, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Several systems to extract verb-argument structures from plain text have been proposed (Johansson and Nugues, 2008; Che et al., 2010). The work presented here complements them with additional semantic relations. The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Me</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based semantic role labeling of propbank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 69–78, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Koomen</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen T Yih</author>
</authors>
<title>Generalized Inference with Multiple Semantic Role Labeling Systems.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>181--184</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="29857" citStr="Koomen et al., 2005" startWordPosition="4745" endWordPosition="4748">.61 .656 .70 .64 .669 .71 .56 .624 .71 .58 .635 MANNER Y .65 .38 .480 .60 .45 .514 .54 .45 .489 .64 .41 .500 PURPOSE Y .65 .58 .613 .69 .60 .642 .56 .49 .525 .68 .49 .572 CAUSE Y .71 .60 .650 .74 .62 .675 .69 .65 .670 .71 .63 .669 All .66 .61 .625 .67 .64 ∗.651 .63 .57 .591 .66 .58 ∗.616 Table 9: Results obtained with the test split using features extracted from gold and automatic annotations, and using basic and predicate structures (ps) features. Statistical significance between F-measures using basic and basic + predicate structures features is indicated with ∗ (confidence 95%). performer (Koomen et al., 2005) at CoNLL-2005 Shared Task on role labeling obtained the following F-measures when extracting the same relations between a verb and its syntactic arguments: 0.774 (TIME), 0.6033 (LOCATION), 0.5922 (MANNER), 0.4541 (PURPOSE) and 0.5397 (CAUSE). The most difficult relations are TIME-SAME and MANNER, F-measures are 0.450 and 0.500 respectively. Even when using gold annotations these two relations are challenging: F-measures are 0.491 for TIME-SAME, an increase of 9.1%, and 0.514 for MANNER, an increase of 2.8%. Results show that other relations can be inferred with F-measures between 0.635 and 0.</context>
</contexts>
<marker>Koomen, Punyakanok, Roth, Yih, 2005</marker>
<rawString>Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2005. Generalized Inference with Multiple Semantic Role Labeling Systems. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 181–184, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Impar: A deterministic algorithm for implicit semantic role labelling.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1180--1189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4703" citStr="Laparra and Rigau (2013" startWordPosition="715" endWordPosition="718">Eval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et</context>
</contexts>
<marker>Laparra, Rigau, 2013</marker>
<rawString>Egoitz Laparra and German Rigau. 2013a. Impar: A deterministic algorithm for implicit semantic role labelling. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1180–1189, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Sources of evidence for implicit argument resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers,</booktitle>
<pages>155--166</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Potsdam, Germany,</location>
<contexts>
<context position="4703" citStr="Laparra and Rigau (2013" startWordPosition="715" endWordPosition="718">Eval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et</context>
</contexts>
<marker>Laparra, Rigau, 2013</marker>
<rawString>Egoitz Laparra and German Rigau. 2013b. Sources of evidence for implicit argument resolution. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers, pages 155–166, Potsdam, Germany, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchel Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1994</date>
<contexts>
<context position="8053" citStr="Marcus et al., 1994" startWordPosition="1232" endWordPosition="1235">ll-known in the field and several tools to predict PropBank annotations are documented and publicly available.1 The work presented here could be incorporated in any NLP pipeline after role labeling without modifications to other components. Furthermore, working on top of PropBank allows us to quantify the impact of features derived from gold and automatically extracted linguistic information when inferring implicit relations (Section 6). 3.1 Verb-Argument structures in PropBank PropBank (Palmer et al., 2005) annotates verbargument structures on top of the syntactic trees of the Penn TreeBank (Marcus et al., 1994). It uses a set of numbered arguments2 (ARG0, ARG1, ARG2, etc.) and modifiers (AM-TMP, AM-MNR, etc.). Numbered arguments do not share a common meaning across verbs, they are defined on a 1E.g., Illinois SRL, http://cogcomp.cs.illinois.edu/ page/software; SENNA,http://ml.nec-labs.com/senna/; SwiRL, http://www.surdeanu.info/mihai/swirl/ 2Numbered arguments are also referred to as core. 146 S NP1 VP1 THEME NP2 VP2 VBD VP3 THEME The first hybrid corn seeds VBN MANNER S-ADV were VBD TIME PP produced using this mechanical approach TIME introduced in the 1930s Figure 2: Verb-argument structures (soli</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchel Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>Annotating Noun Argument Structure for NomBank.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC-2004,</booktitle>
<pages>803--806</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="4545" citStr="Meyers et al., 2004" startWordPosition="690" endWordPosition="693">08; Che et al., 2010). The work presented here complements them with additional semantic relations. The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBa</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. Annotating Noun Argument Structure for NomBank. In Proceedings of LREC-2004, pages 803– 806, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Eduardo Blanco</author>
</authors>
<title>Polaris: Lymba’s semantic parser.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),</booktitle>
<pages>66--72</pages>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="27514" citStr="Moldovan and Blanco, 2012" startWordPosition="4332" endWordPosition="4335">ests) feature 50, overlapping sem rel ARG1 feature 51, overlapping head resigned feature 52, overlapping direct true Table 8: PropBank roles and values for features (50–52) when predicting potential implicit relation R(said, to pursue other interests), labeled N. LIBSVM (Chang and Lin, 2011). Parameters α and γ were tuned by grid search using 10-fold cross validation over training instances. Results are reported using features extracted from gold and automatic annotations. Gold annotations are taken directly from the Penn TreeBank and PropBank. Automatic annotations are obtained with Polaris (Moldovan and Blanco, 2012), a semantic parser that among others is trained with PropBank. Results using gold (automatic) annotations are obtained with a model trained with gold (automatic) annotations. 6.1 Detailed Results Table 9 presents per-relation and overall results. In general terms, there is a decrease in performance when using automatic annotations. The difference is most noticeable in recall and it is due to missing semantic roles, which in turn are often due to syntactic parsing errors. This is not surprising as in order for an implicit relation R(x, y) to be generated as potential and fed to the learning al</context>
</contexts>
<marker>Moldovan, Blanco, 2012</marker>
<rawString>Dan Moldovan and Eduardo Blanco. 2012. Polaris: Lymba’s semantic parser. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012), pages 66– 72, Istanbul, Turkey, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha S Palmer</author>
<author>Deborah A Dahl</author>
<author>Rebecca J Schiffman</author>
<author>Lynette Hirschman</author>
<author>Marcia Linebarger</author>
<author>John Dowding</author>
</authors>
<title>Recovering implicit information.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>10--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York, New York, USA,</location>
<contexts>
<context position="4345" citStr="Palmer et al., 1986" startWordPosition="657" endWordPosition="660">den, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Several systems to extract verb-argument structures from plain text have been proposed (Johansson and Nugues, 2008; Che et al., 2010). The work presented here complements them with additional semantic relations. The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), who</context>
</contexts>
<marker>Palmer, Dahl, Schiffman, Hirschman, Linebarger, Dowding, 1986</marker>
<rawString>Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiffman, Lynette Hirschman, Marcia Linebarger, and John Dowding. 1986. Recovering implicit information. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pages 10–19, New York, New York, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1411" citStr="Palmer et al., 2005" startWordPosition="203" endWordPosition="206">AUSE of the man undergoing some ‘change’. A question answering system would benefit from detecting this relation when answering Why did he change? Extracting all semantic relations from text is a monumental task and is at the core of language understanding. In recent years, approaches that aim at extracting a subset of all relations have achieved great success. In particular, previous research (Carreras and M`arquez, 2005; Punyakanok et al., 2008; Che et al., 2010; Zapirain et al., 2010) focused on verb-argument structures, i.e., relations between a verb and its syntactic arguments. PropBank (Palmer et al., 2005) is the corpus of reference for verb-argument relations. However, relations between a verb and its syntactic arguments are only a fraction of the relations present in texts. Consider the statement [Mr. Brown]NP1 succeeds [Joseph W. Hibben, who retired last August]NP2 and its parse tree (Figure 1). Verbargument relations encode that NP1 is the AGENT and NP2 is the THEME of verb ‘succeeds’ (PropBank uses labels ARG0 and ARG1). Any semantic relation between ‘succeeds’ and concepts dominated in the parse tree by one of its syntactic arguments NP1 or NP2, e.g., ‘succeeds’ ocS Figure 1: Example of p</context>
<context position="7946" citStr="Palmer et al., 2005" startWordPosition="1214" endWordPosition="1217">it relations from PropBank’s verb-argument structures. We believe this is an advantage since PropBank is well-known in the field and several tools to predict PropBank annotations are documented and publicly available.1 The work presented here could be incorporated in any NLP pipeline after role labeling without modifications to other components. Furthermore, working on top of PropBank allows us to quantify the impact of features derived from gold and automatically extracted linguistic information when inferring implicit relations (Section 6). 3.1 Verb-Argument structures in PropBank PropBank (Palmer et al., 2005) annotates verbargument structures on top of the syntactic trees of the Penn TreeBank (Marcus et al., 1994). It uses a set of numbered arguments2 (ARG0, ARG1, ARG2, etc.) and modifiers (AM-TMP, AM-MNR, etc.). Numbered arguments do not share a common meaning across verbs, they are defined on a 1E.g., Illinois SRL, http://cogcomp.cs.illinois.edu/ page/software; SENNA,http://ml.nec-labs.com/senna/; SwiRL, http://www.surdeanu.info/mihai/swirl/ 2Numbered arguments are also referred to as core. 146 S NP1 VP1 THEME NP2 VP2 VBD VP3 THEME The first hybrid corn seeds VBN MANNER S-ADV were VBD TIME PP pr</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen T Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="1241" citStr="Punyakanok et al., 2008" startWordPosition="176" endWordPosition="179">ards capturing the meaning of text. Semantic relations explicitly encode links between concepts. For example, in The accident left him a changed man, the ‘accident’ is the CAUSE of the man undergoing some ‘change’. A question answering system would benefit from detecting this relation when answering Why did he change? Extracting all semantic relations from text is a monumental task and is at the core of language understanding. In recent years, approaches that aim at extracting a subset of all relations have achieved great success. In particular, previous research (Carreras and M`arquez, 2005; Punyakanok et al., 2008; Che et al., 2010; Zapirain et al., 2010) focused on verb-argument structures, i.e., relations between a verb and its syntactic arguments. PropBank (Palmer et al., 2005) is the corpus of reference for verb-argument relations. However, relations between a verb and its syntactic arguments are only a fraction of the relations present in texts. Consider the statement [Mr. Brown]NP1 succeeds [Joseph W. Hibben, who retired last August]NP2 and its parse tree (Figure 1). Verbargument relations encode that NP1 is the AGENT and NP2 is the THEME of verb ‘succeeds’ (PropBank uses labels ARG0 and ARG1). A</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
</authors>
<title>The timebank corpus. Corpus linguistics,</title>
<date>2003</date>
<pages>2003--40</pages>
<contexts>
<context position="4071" citStr="Pustejovsky et al., 2003" startWordPosition="612" endWordPosition="615">ealistic environment. TIME-AFTER NP1 VP AGENT Mr. Brown VBZ THEME NP2 [Joseph W. Hibben, who]AGENT [retired]v [last August]TIME succeeds 145 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 145–154, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Several systems to extract verb-argument structures from plain text have been proposed (Johansson and Nugues, 2008; Che et al., 2010). The work presented here complements them with additional semantic relations. The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, et al. 2003. The timebank corpus. Corpus linguistics, 2003:40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<title>SemEval-2010 Task 10: Linking Events and Their Participants in Discourse.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009),</booktitle>
<pages>106--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5083" citStr="Ruppenhofer et al., 2009" startWordPosition="775" endWordPosition="778">e recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et al., 2010; Tonelli and Delmonte, 2010) reported overall Fmeasures around 2 (out of 100). Posterior work (Silberer and Frank, 2012; Laparra and Rigau, 2013b) reported F-measures below 20 for the same task. The work presented here does not target missing core arguments but modifiers within the same sentence. Furthermore, results show our proposal is useful in a real environment.</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2009</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2009. SemEval-2010 Task 10: Linking Events and Their Participants in Discourse. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), pages 106–111, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Russell Lee-Goldman</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
</authors>
<title>Beyond sentence-level semantic role labeling: linking argument structures in discourse.</title>
<date>2013</date>
<journal>Language Resources and Evaluation,</journal>
<volume>47</volume>
<issue>3</issue>
<contexts>
<context position="5208" citStr="Ruppenhofer et al. (2013)" startWordPosition="793" endWordPosition="796"> arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et al., 2010; Tonelli and Delmonte, 2010) reported overall Fmeasures around 2 (out of 100). Posterior work (Silberer and Frank, 2012; Laparra and Rigau, 2013b) reported F-measures below 20 for the same task. The work presented here does not target missing core arguments but modifiers within the same sentence. Furthermore, results show our proposal is useful in a real environment. Finally, our previous work (Blanco and Moldovan, 2011; Blanco and Moldovan, 2014) proposed composing new relations out of ch</context>
</contexts>
<marker>Ruppenhofer, Lee-Goldman, Sporleder, Morante, 2013</marker>
<rawString>Josef Ruppenhofer, Russell Lee-Goldman, Caroline Sporleder, and Roser Morante. 2013. Beyond sentence-level semantic role labeling: linking argument structures in discourse. Language Resources and Evaluation, 47(3):695–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Anette Frank</author>
</authors>
<title>Casting implicit role linking as an anaphora resolution task.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="5433" citStr="Silberer and Frank, 2012" startWordPosition="828" endWordPosition="831">cus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et al., 2010; Tonelli and Delmonte, 2010) reported overall Fmeasures around 2 (out of 100). Posterior work (Silberer and Frank, 2012; Laparra and Rigau, 2013b) reported F-measures below 20 for the same task. The work presented here does not target missing core arguments but modifiers within the same sentence. Furthermore, results show our proposal is useful in a real environment. Finally, our previous work (Blanco and Moldovan, 2011; Blanco and Moldovan, 2014) proposed composing new relations out of chains of previously extracted relations. This approach is unsupervised and accurate (88% with gold annotations), but inferences are made only between the ends of chains of existing relations. Our current proposal also leverage</context>
</contexts>
<marker>Silberer, Frank, 2012</marker>
<rawString>Carina Silberer and Anette Frank. 2012. Casting implicit role linking as an anaphora resolution task. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics, pages 1–10, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
</authors>
<title>Implicit role reference.</title>
<date>2002</date>
<booktitle>In International Symposium on Reference Resolution for Natural Language Processing,</booktitle>
<pages>109--115</pages>
<contexts>
<context position="4363" citStr="Tetreault, 2002" startWordPosition="661" endWordPosition="662">. c�2014 Association for Computational Linguistics 2 Related Work Several systems to extract verb-argument structures from plain text have been proposed (Johansson and Nugues, 2008; Che et al., 2010). The work presented here complements them with additional semantic relations. The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shar</context>
</contexts>
<marker>Tetreault, 2002</marker>
<rawString>Joel R Tetreault. 2002. Implicit role reference. In International Symposium on Reference Resolution for Natural Language Processing, pages 109–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Tonelli</author>
<author>Rodolfo Delmonte</author>
</authors>
<title>Venses++: Adapting a deep semantic processing system to the identification of null instantiations.</title>
<date>2010</date>
<booktitle>In Proceedings ofthe 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>296--299</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5342" citStr="Tonelli and Delmonte, 2010" startWordPosition="813" endWordPosition="816"> deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and their Participants in Discourse (Ruppenhofer et al., 2009) targeted cross-sentence missing core arguments in both PropBank and FrameNet (Baker et al., 1998). Ruppenhofer et al. (2013) detail the annotations and results. The task proved extremely difficult, participants (Chen et al., 2010; Tonelli and Delmonte, 2010) reported overall Fmeasures around 2 (out of 100). Posterior work (Silberer and Frank, 2012; Laparra and Rigau, 2013b) reported F-measures below 20 for the same task. The work presented here does not target missing core arguments but modifiers within the same sentence. Furthermore, results show our proposal is useful in a real environment. Finally, our previous work (Blanco and Moldovan, 2011; Blanco and Moldovan, 2014) proposed composing new relations out of chains of previously extracted relations. This approach is unsupervised and accurate (88% with gold annotations), but inferences are mad</context>
</contexts>
<marker>Tonelli, Delmonte, 2010</marker>
<rawString>Sara Tonelli and Rodolfo Delmonte. 2010. Venses++: Adapting a deep semantic processing system to the identification of null instantiations. In Proceedings ofthe 5th International Workshop on Semantic Evaluation, pages 296–299, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>678--687</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9624" citStr="Tratz and Hovy, 2010" startWordPosition="1462" endWordPosition="1465">334 70.26% ARG1 106,331 94.17% ARG2 24,560 21.75% AM-TMP 19,756 17.50% AM-MNR 7,833 6.94% AM-LOC 7,198 6.37% AM-PNC 2,784 2.47% AM-CAU 1,563 1.38% Table 3: Counts of selected PropBank semantic roles. Total number of predicates is 112,917. verb by verb basis in each frameset. For example, ARG2 is used to indicate “created-from, thing changed” with verb make and “entity exempted from” with verb exempt (Table 1). Unlike numbered arguments, modifiers share a common meaning across verbs (Table 2). Some modifiers are arguably not a semantic relation and are not present in most relation inventories (Tratz and Hovy, 2010; Hendrickx et al., 2009). For example, AM-NEG and AM-MOD signal the presence of negation and modals, e.g., [woJAM-MOD[n’tJAM-NEG [goJv. For more information about PropBank annotations and examples, refer to the annotation guidelines.3 Inspecting PropBank annotations one can easily conclude that numbered arguments dominate the annotations and only a few modifiers are an3http://verbs.colorado.edu/˜mpalmer/projects/ace/ PBguidelines.pdf notated (Table 3). ARG0 and ARG1 are present in most verb-argument structures, other numbered arguments are often not defined in the corresponding frameset and a</context>
</contexts>
<marker>Tratz, Hovy, 2010</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2010. A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 678–687, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naushad UzZaman</author>
<author>Hector Llorens</author>
<author>Leon Derczynski</author>
<author>James Allen</author>
<author>Marc Verhagen</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval2013),</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="4120" citStr="UzZaman et al., 2013" startWordPosition="620" endWordPosition="623">wn VBZ THEME NP2 [Joseph W. Hibben, who]AGENT [retired]v [last August]TIME succeeds 145 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 145–154, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Several systems to extract verb-argument structures from plain text have been proposed (Johansson and Nugues, 2008; Che et al., 2010). The work presented here complements them with additional semantic relations. The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a dete</context>
</contexts>
<marker>UzZaman, Llorens, Derczynski, Allen, Verhagen, Pustejovsky, 2013</marker>
<rawString>Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. 2013. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval2013), pages 1–9, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Whittemore</author>
<author>Melissa Macpherson</author>
<author>Greg Carlson</author>
</authors>
<title>Event-building through role-filling and anaphora resolution.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Berkeley, California, USA,</location>
<contexts>
<context position="4429" citStr="Whittemore et al., 1991" startWordPosition="671" endWordPosition="674">ed Work Several systems to extract verb-argument structures from plain text have been proposed (Johansson and Nugues, 2008; Che et al., 2010). The work presented here complements them with additional semantic relations. The TimeBank corpus (Pustejovsky et al., 2003) and TempEval competitions (UzZaman et al., 2013) target events and detailed temporal information; this work also targets LOCATION, MANNER, PURPOSE and CAUSE. Extracting missing relations is not a new problem. Early work focused on a very limited domain (Palmer et al., 1986; Tetreault, 2002) or did not attempt to automate the task (Whittemore et al., 1991). This section focuses on more recent work. Gerber and Chai (2010) augment NomBank annotations (Meyers et al., 2004) of 10 predicates with additional core arguments. Their supervised systems obtain F-measures of 42.3 and 50.3 (Gerber and Chai, 2012). Laparra and Rigau (2013a) present a deterministic algorithm and obtain an Fmeasure of 45.3. In contrast, our approach does not focus on a few selected predicates or core arguments. It targets all predicates and argument modifiers (AM-TMP, AM-MNR, AM-LOC, etc.), whose meaning is shared across verbs. The SemEval-2010 Task 10: Linking Events and thei</context>
</contexts>
<marker>Whittemore, Macpherson, Carlson, 1991</marker>
<rawString>Greg Whittemore, Melissa Macpherson, and Greg Carlson. 1991. Event-building through role-filling and anaphora resolution. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 17–24, Berkeley, California, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Be N Zapirain</author>
<author>Eneko Agirre</author>
<author>Lluis M`arquez</author>
<author>Mihai Surdeanu</author>
</authors>
<title>Improving Semantic Role Classification with Selectional Preferences.</title>
<date>2010</date>
<booktitle>In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>373--376</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>Zapirain, Agirre, M`arquez, Surdeanu, 2010</marker>
<rawString>Be N. Zapirain, Eneko Agirre, Lluis M`arquez, and Mihai Surdeanu. 2010. Improving Semantic Role Classification with Selectional Preferences. In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 373–376, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>