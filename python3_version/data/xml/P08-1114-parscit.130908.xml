<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000153">
<title confidence="0.995044">
Soft Syntactic Constraints for Hierarchical Phrased-Based Translation
</title>
<author confidence="0.987515">
Yuval Marton and Philip Resnik
</author>
<affiliation confidence="0.994495">
Department of Linguistics
and the Laboratory for Computational Linguistics and Information Processing (CLIP)
at the Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742-7505, USA
</affiliation>
<email confidence="0.784514">
{ymarton, resnik} @t umiacs.umd.edu
</email>
<sectionHeader confidence="0.993292" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999558529411765">
In adding syntax to statistical MT, there is
a tradeoff between taking advantage of lin-
guistic analysis, versus allowing the model
to exploit linguistically unmotivated mappings
learned from parallel training data. A num-
ber of previous efforts have tackled this trade-
off by starting with a commitment to linguisti-
cally motivated analyses and then finding ap-
propriate ways to soften that commitment. We
present an approach that explores the trade-
off from the other direction, starting with a
context-free translation model learned directly
from aligned parallel text, and then adding soft
constituent-level constraints based on parses
of the source language. We obtain substantial
improvements in performance for translation
from Chinese and Arabic to English.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989155319149">
The statistical revolution in machine translation, be-
ginning with (Brown et al., 1993) in the early 1990s,
replaced an earlier era of detailed language analy-
sis with automatic learning of shallow source-target
mappings from large parallel corpora. Over the last
several years, however, the pendulum has begun to
swing back in the other direction, with researchers
exploring a variety of statistical models that take ad-
vantage of source- and particularly target-language
syntactic analysis (e.g. (Cowan et al., 2006; Zoll-
mann and Venugopal, 2006; Marcu et al., 2006; Gal-
ley et al., 2006) and numerous others).
Chiang (2005) distinguishes statistical MT ap-
proaches that are “syntactic” in a formal sense, go-
ing beyond the finite-state underpinnings of phrase-
based models, from approaches that are syntactic
in a linguistic sense, i.e. taking advantage of a
priori language knowledge in the form of annota-
tions derived from human linguistic analysis or tree-
banking.&apos; The two forms of syntactic modeling are
doubly dissociable: current research frameworks in-
clude systems that are finite state but informed by
linguistic annotation prior to training (e.g., (Koehn
and Hoang, 2007; Birch et al., 2007; Hassan et al.,
2007)), and also include systems employing context-
free models trained on parallel text without benefit
of any prior linguistic analysis (e.g. (Chiang, 2005;
Chiang, 2007; Wu, 1997)). Over time, however,
there has been increasing movement in the direction
of systems that are syntactic in both the formal and
linguistic senses.
In any such system, there is a natural tension be-
tween taking advantage of the linguistic analysis,
versus allowing the model to use linguistically un-
motivated mappings learned from parallel training
data. The tradeoff often involves starting with a sys-
tem that exploits rich linguistic representations and
relaxing some part of it. For example, DeNeefe et
al. (2007) begin with a tree-to-string model, using
treebank-based target language analysis, and find it
useful to modify it in order to accommodate useful
“phrasal” chunks that are present in parallel train-
ing data but not licensed by linguistically motivated
parses of the target language. Similarly, Cowan et al.
(2006) focus on using syntactically rich representa-
tions of source and target parse trees, but they re-
sort to phrase-based translation for modifiers within
</bodyText>
<note confidence="0.567929">
&apos;See (Lopez, to appear) for a comprehensive survey.
</note>
<page confidence="0.822174">
1003
</page>
<note confidence="0.725719">
Proceedings of ACL-08: HLT, pages 1003–1011,
</note>
<page confidence="0.539183">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.99979334375">
clauses. Finding the right way to balance linguistic
analysis with unconstrained data-driven modeling is
clearly a key challenge.
In this paper we address this challenge from a less
explored direction. Rather than starting with a sys-
tem based on linguistically motivated parse trees, we
begin with a model that is syntactic only in the for-
mal sense. We then introduce soft constraints that
take source-language parses into account to a lim-
ited extent. Introducing syntactic constraints in this
restricted way allows us to take maximal advantage
of what can be learned from parallel training data,
while effectively factoring in key aspects of linguis-
tically motivated analysis. As a result, we obtain
substantial improvements in performance for both
Chinese-English and Arabic-English translation.
In Section 2, we briefly review the Hiero statis-
tical MT framework (Chiang, 2005, 2007), upon
which this work builds, and we discuss Chiang’s ini-
tial effort to incorporate soft source-language con-
stituency constraints for Chinese-English transla-
tion. In Section 3, we suggest that an insufficiently
fine-grained view of constituency constraints was re-
sponsible for Chiang’s lack of strong results, and
introduce finer grained constraints into the model.
Section 4 demonstrates the the value of these con-
straints via substantial improvements in Chinese-
English translation performance, and extends the ap-
proach to Arabic-English. Section 5 discusses the
results, and Section 6 considers related work. Fi-
nally we conclude in Section 7 with a summary and
potential directions for future work.
</bodyText>
<sectionHeader confidence="0.83948" genericHeader="method">
2 Hierarchical Phrase-based Translation
</sectionHeader>
<subsectionHeader confidence="0.966224">
2.1 Hiero
</subsectionHeader>
<bodyText confidence="0.999884965517242">
Hiero (Chiang, 2005; Chiang, 2007) is a hi-
erarchical phrase-based statistical MT framework
that generalizes phrase-based models by permit-
ting phrases with gaps. Formally, Hiero’s trans-
lation model is a weighted synchronous context-
free grammar. Hiero employs a generalization of
the standard non-hierarchical phrase extraction ap-
proach in order to acquire the synchronous rules
of the grammar directly from word-aligned paral-
lel text Rules have the form X → he, 1i, where
e and f are phrases containing terminal symbols
(words) and possibly co-indexed instances of the
nonterminal symbol X.2 Associated with each rule
is a set of translation model features, Oi(�f, e); for
example, one intuitively natural feature of a rule is
the phrase translation (log-)probability O( f, e) _
log p(e |f) , directly analogous to the corresponding
feature in non-hierarchical phrase-based models like
Pharaoh (Koehn et al., 2003). In addition to this
phrase translation probability feature, Hiero’s fea-
ture set includes the inverse phrase translation prob-
ability log p(�f|e), lexical weights lexwt(�f|e) and
lexwt(e |�f), which are estimates of translation qual-
ity based on word-level correspondences (Koehn et
al., 2003), and a rule penalty allowing the model to
learn a preference for longer or shorter derivations;
see (Chiang, 2007) for details.
These features are combined using a log-linear
model, with each synchronous rule contributing
</bodyText>
<equation confidence="0.9583845">
� AiOi(f, e) (1)
i
</equation>
<bodyText confidence="0.99971375">
to the total log-probability of a derived hypothesis.
Each Ai is a weight associated with feature Oi, and
these weights are typically optimized using mini-
mum error rate training (Och, 2003).
</bodyText>
<subsectionHeader confidence="0.99986">
2.2 Soft Syntactic Constraints
</subsectionHeader>
<bodyText confidence="0.999651">
When looking at Hiero rules, which are acquired au-
tomatically by the model from parallel text, it is easy
to find many cases that seem to respect linguistically
motivated boundaries. For example,
</bodyText>
<equation confidence="0.733494">
X → hjingtian X1, X1 this yeari,
</equation>
<bodyText confidence="0.999747375">
seems to capture the use of jingtian/this year as
a temporal modifier when building linguistic con-
stituents such as noun phrases (the election this
year) or verb phrases (voted in the primary this
year). However, it is important to observe that noth-
ing in the Hiero framework actually requires nonter-
minal symbols to cover linguistically sensible con-
stituents, and in practice they frequently do not.3
</bodyText>
<footnote confidence="0.672081333333333">
2This is slightly simplified: Chiang’s original formulation
of Hiero, which we use, has two nonterminal symbols, X and
S. The latter is used only in two special “glue” rules that permit
complete trees to be constructed via concatenation of subtrees
when there is no better way to combine them.
3For example, this rule could just as well be applied with Xl
covering the “phrase” submitted and to produce non-constituent
substring submitted and this year in a hypothesis like The bud-
get was submitted and this year cuts are likely.
</footnote>
<page confidence="0.997217">
1004
</page>
<bodyText confidence="0.999897285714286">
Chiang (2005) conjectured that there might be
value in allowing the Hiero model to favor hy-
potheses for which the synchronous derivation re-
spects linguistically motivated source-language con-
stituency boundaries, as identified using a parser.
He tested this conjecture by adding a soft constraint
in the form of a “constituency feature”: if a syn-
chronous rule X —* (e, f) is used in a derivation,
and the span of f is a constituent in the source-
language parse, then a term a, is added to the model
score in expression (1).4 Unlike a hard constraint,
which would simply prevent the application of rules
violating syntactic boundaries, using the feature to
introduce a soft constraint allows the model to boost
the “goodness” for a rule if it is constitent with the
source language constituency analysis, and to leave
its score unchanged otherwise. The weight a,, like
all other aZ, is set via minimum error rate train-
ing, and that optimization process determines em-
pirically the extent to which the constituency feature
should be trusted.
Figure 1 illustrates the way the constituency fea-
ture worked, treating English as the source language
for the sake of readability. In this example, a, would
be added to the hypothesis score for any rule used in
the hypothesis whose source side spanned the minis-
ter, a speech, yesterday, gave a speech yesterday, or
the minister gave a speech yesterday. A rule trans-
lating, say, minister gave a as a unit would receive
no such boost.
Chiang tested the constituency feature for
Chinese-English translation, and obtained no signif-
icant improvement on the test set. The idea then
seems essentially to have been abandoned; it does
not appear in later discussions (Chiang, 2007).
</bodyText>
<sectionHeader confidence="0.958388" genericHeader="method">
3 Soft Syntactic Constraints, Revisited
</sectionHeader>
<bodyText confidence="0.999742">
On the face of it, there are any number of possi-
ble reasons Chiang’s (2005) soft constraint did not
work – including, for example, practical issues like
the quality of the Chinese parses.5 However, we fo-
cus here on two conceptual issues underlying his use
of source language syntactic constituents.
</bodyText>
<footnote confidence="0.647231">
4Formally, O,(�f, e) is defined as a binary feature, with
value 1 if f spans a source constituent and 0 otherwise. In the
latter case a,O,(f, e) = 0 and the score in expression (1) is
unaffected.
5In fact, this turns out not to be the issue; see Section 4.
</footnote>
<figureCaption confidence="0.993131">
Figure 1: Illustration of Chiang’s (2005) syntactic con-
stituency feature, which does not distinguish among con-
stituent types.
</figureCaption>
<bodyText confidence="0.996010545454545">
First, the constituency feature treats all syntac-
tic constituent types equally, making no distinction
among them. For any given language pair, however,
there might be some source constituents that tend to
map naturally to the target language as units, and
others that do not (Fox, 2002; Eisner, 2003). More-
over, a parser may tend to be more accurate for some
constituents than for others.
Second, the Chiang (2005) constituency feature
gives a rule additional credit when the rule’s source
side overlaps exactly with a source-side syntactic
constituent. Logically, however, it might make sense
not just to give a rule X —* (e, f) extra credit when
f matches a constituent, but to incur a cost when f
violates a constituent boundary. Using the example
in Figure 1, we might want to penalize hypotheses
containing rules where f is the minister gave a (and
other cases, such as minister gave, minister gave a,
and so forth).6
These observations suggest a finer-grained ap-
proach to the constituency feature idea, retaining the
idea of soft constraints, but applying them using var-
ious soft-constraint constituency features. Our first
observation argues for distinguishing among con-
stituent types (NP, VP, etc.). Our second observa-
tion argues for distinguishing the benefit of match-
6This accomplishes coverage of the logically complete set of
possibilities, which include not only f matching a constituent
exactly or crossing its boundaries, but also f being properly
contained within the constituent span, properly containing it,
or being outside it entirely. Whenever these latter possibilities
occur, f will exactly match or cross the boundaries of some
other constituent.
</bodyText>
<page confidence="0.953119">
1005
</page>
<bodyText confidence="0.953290208333334">
ing constituents from the cost of crossing constituent
boundaries. We therefore define a space of new fea-
tures as the cross product
{CP, IP, NP, VP, ...} x {_, +}.
where = and + signify matching and crossing bound-
aries, respectively. For example, ONP= would de-
note a binary feature that matches whenever the span
of f exactly covers an NP in the source-side parse
tree, resulting in ANP= being added to the hypoth-
esis score (expression (1)). Similarly, oVP+ would
denote a binary feature that matches whenever the
span of f crosses a VP boundary in the parse tree,
resulting in AVP+ being subtracted from the hypoth-
esis score.7 For readability from this point forward,
we will omit 0 from the notation and refer to features
such as NP= (which one could read as “NP match”),
VP+ (which one could read as “VP crossing”), etc.
In addition to these individual features, we define
three more variants:
• For each constituent type, e.g. NP, we define
a feature NP_ that ties the weights of NP= and
NP+. If NP= matches a rule, the model score is
incremented by ANP_, and if NP+ matches, the
model score is decremented by the same quan-
tity.
• For each constituent type, e.g. NP, we define a
version of the model, NP2, in which NP= and
NP+ are both included as features, with sepa-
rate weights ANP= and ANP+.
• We define a set of “standard” linguistic labels
containing {CP, IP, NP, VP, PP, ADJP, ADVP,
QP, LCP, DNP} and excluding other labels such
as PRN (parentheses), FRAG (fragment), etc.8
We define feature XP= as the disjunction of
{CP=, IP=, ..., DNP=}; i.e. its value equals 1
for a rule if the span of f exactly covers a con-
stituent having any of the standard labels. The
7Formally, AVP+ simply contributes to the sum in expres-
sion (1), as with all features in the model, but weight optimiza-
tion using minimum error rate training should, and does, auto-
matically assign this feature a negative weight.
8We map SBAR and S labels in Arabic parses to CP and IP,
respectively, consistent with the Chinese parses. We map Chi-
nese DP labels to NP. DNP and LCP appear only in Chinese. We
ran no ADJP experiment in Chinese, because this label virtually
aways spans only one token in the Chinese parses.
definitions of XP+, XP_, and XP2 are analo-
gous.
</bodyText>
<listItem confidence="0.677274">
• Similarly, since Chiang’s original constituency
</listItem>
<bodyText confidence="0.958341666666667">
feature can be viewed as a disjunctive “all-
labels=” feature, we also defined “all-labels+”,
“all-labels2”, and “all-labels_” analogously.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999913344827586">
We carried out MT experiments for translation
from Chinese to English and from Arabic to En-
glish, using a descendant of Chiang’s Hiero sys-
tem. Language models were built using the SRI
Language Modeling Toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). Word-level alignments were obtained
using GIZA++ (Och and Ney, 2000). The base-
line model in both languages used the feature set
described in Section 2; for the Chinese baseline we
also included a rule-based number translation fea-
ture (Chiang, 2007).
In order to compute syntactic features, we an-
alyzed source sentences using state of the art,
tree-bank trained constituency parsers ((Huang et
al., 2008) for Chinese, and the Stanford parser
v.2007-08-19 for Arabic (Klein and Manning,
2003a; Klein and Manning, 2003b)). In addition
to the baseline condition, and baseline plus Chi-
ang’s (2005) original constituency feature, experi-
mental conditions augmented the baseline with ad-
ditional features as described in Section 3.
All models were optimized and tested using the
BLEU metric (Papineni et al., 2002) with the NIST-
implemented (“shortest”) effective reference length,
on lowercased, tokenized outputs/references. Sta-
tistical significance of difference from the baseline
BLEU score was measured by using paired boot-
strap re-sampling (Koehn, 2004).9
</bodyText>
<subsectionHeader confidence="0.998048">
4.1 Chinese-English
</subsectionHeader>
<bodyText confidence="0.999074166666667">
For the Chinese-English translation experiments, we
trained the translation model on the corpora in Ta-
ble 1, totalling approximately 2.1 million sentence
pairs after GIZA++ filtering for length ratio. Chi-
nese text was segmented using the Stanford seg-
menter (Tseng et al., 2005).
</bodyText>
<footnote confidence="0.7725775">
9Whenever we use the word “significant”, we mean “statis-
tically significant” (at p &lt; .05 unless specified otherwise).
</footnote>
<page confidence="0.689747">
1006
</page>
<table confidence="0.948121428571429">
LDC ID Description
LDC2002E18 Xinhua Ch/Eng Par News V1 beta
LDC2003E07 Ch/En Treebank Par Corpus
LDC2005T10 Ch/En News Mag Par Txt (Sinorama)
LDC2003E14 FBIS Multilanguage Txts
LDC2005T06 Ch News Translation Txt Pt 1
LDC2004T08 HK Par Text (only HKNews)
</table>
<tableCaption confidence="0.99996">
Table 1: Training corpora for Chinese-English translation
</tableCaption>
<bodyText confidence="0.999971411764706">
We trained a 5-gram language model using the
English (target) side of the training set, pruning 4-
gram and 5-gram singletons. For minimum error
rate training and development we used the NIST
MTeval MT03 set.
Table 2 presents our results. We first evaluated
translation performance using the NIST MT06 (nist-
text) set. Like Chiang (2005), we find that the orig-
inal, undifferentiated constituency feature (Chiang-
05) introduces a negligible, statistically insignificant
improvement over the baseline. However, we find
that several of the finer-grained constraints (IP=,
VP=, VP+, QP+, and NP=) achieve statistically
significant improvements over baseline (up to .74
BLEU), and the latter three also improve signifi-
cantly on the undifferentiated constituency feature.
By combining multiple finer-grained syntactic fea-
tures, we obtain significant improvements of up to
1.65 BLEU points (NP_, VP2, IP2, all-labels_, and
XP+).
We also obtained further gains using combina-
tions of features that had performed well; e.g., con-
dition IP2.VP2.NP_ augments the baseline features
with IP2 and VP2 (i.e. IP=, IP+, VP= and VP+),
and NP_ (tying weights of NP= and NP+; see Sec-
tion 3). Since component features in those combi-
nations were informed by individual-feature perfor-
mance on the test set, we tested the best perform-
ing conditions from MT06 on a new test set, NIST
MT08. NP= and VP+ yielded significant improve-
ments of up to 1.53 BLEU. Combination conditions
replicated the pattern of results from MT06, includ-
ing the same increasing order of gains, with im-
provements up to 1.11 BLEU.
</bodyText>
<subsectionHeader confidence="0.964914">
4.2 Arabic-English
</subsectionHeader>
<bodyText confidence="0.6225765">
For Arabic-English translation, we used the train-
ing corpora in Table 3, approximately 100,000 sen-
</bodyText>
<table confidence="0.998482051282051">
Chinese MT06 MT08
Baseline .2624 .2064
Chiang-05 .2634 .2065
PP= .2607
DNP+ .2621
CP+ .2622
AP+ .2633
AP= .2634
DNP= .2640
IP+ .2643
PP+ .2644
LCP= .2649
LCP+ .2654
CP= .2657
NP+ .2662
QP= .2674^+ .2071
IP= .2680*+ .2061
VP= .2683* .2072
VP+ .2693**++ .2109*+
QP+ .2694**++ .2091
NP= .2698**++ .2217**++
Multiple / conflated features:
QP2 .2614
NP2 .2621
XP= .2630
XP2 .2633
all-labels+ .2633
VP_ .2637
QP_ .2641
NP.VP.IP=.QP.VP+ .2646
IP_ .2647
IP2+VP2 .2649
all-labels2 .2673*- .2070
NP_ .2690**++ .2101^+
IP2.VP2.NP_ .2699**++ .2105*+
VP2 .2722**++ .2123**++
all-labels_ .2731**++ .2125*++
IP2 .2750**++ .2132**+
XP+ .2789**++ .2175**++
</table>
<tableCaption confidence="0.994023">
Table 2: Chinese-English results. *: Significantly better
</tableCaption>
<construct confidence="0.377194666666667">
than baseline (p &lt; .05). **: Significantly better than
baseline (p &lt; .01). ^: Almost significantly better than
baseline (p &lt; .075). +: Significantly better than Chiang-
05 (p &lt; .05). ++: Significantly better than Chiang-05
(p &lt; .01). -: Almost significantly better than Chiang-05
(p &lt; .075).
</construct>
<page confidence="0.901717">
1007
</page>
<table confidence="0.7835374">
LDC ID Description
LDC2004T17 Ar News Trans Txt Pt 1
LDC2004T18 Ar/En Par News Pt 1
LDC2005E46 Ar/En Treebank En Translation
LDC2004E72 eTIRR Ar/En News Txt
</table>
<tableCaption confidence="0.999897">
Table 3: Training corpora for Arabic-English translation
</tableCaption>
<bodyText confidence="0.999353525">
tence pairs after GIZA++ length-ratio filtering. We
trained a trigram language model using the English
side of this training set, plus the English Gigaword
v2 AFP and Gigaword v1 Xinhua corpora. Devel-
opment and minimum error rate training were done
using the NIST MT02 set.
Table 4 presents our results. We first tested on
on the NIST MT03 and MT06 (nist-text) sets. On
MT03, the original, undifferentiated constituency
feature did not improve over baseline. Two individ-
ual finer-grained features (PP+ and AdvP=) yielded
statistically significant gains up to .42 BLEU points,
and feature combinations AP2, XP2 and all-labels2
yielded significant gains up to 1.03 BLEU points.
XP2 and all-labels2 also improved significantly on
the undifferentiated constituency feature, by .72 and
1.11 BLEU points, respectively.
For MT06, Chiang’s original feature improved the
baseline significantly — this is a new result using
his feature, since he did not experiment with Ara-
bic — as did our our IP=, PP=, and VP= condi-
tions. Adding individual features PP+ and AdvP=
yielded significant improvements up to 1.4 BLEU
points over baseline, and in fact the improvement for
individual feature AdvP= over Chiang’s undifferen-
tiated constituency feature approaches significance
(p &lt; .075).
More important, several conditions combining
features achieved statistically significant improve-
ments over baseline of up 1.94 BLEU points: XP2,
IP2, IP, VP=.PP+.AdvP=, AP2, PP+.AdvP=, and
AdvP2. Of these, AdvP2 is also a significant im-
provement over the undifferentiated constituency
feature (Chiang-05), with p &lt; .01. As we did
for Chinese, we tested the best-performing models
on a new test set, NIST MT08. Consistent patterns
reappeared: improvements over the baseline up to
1.69 BLEU (p &lt; .01), with AdvP2 again in the
lead (also outperforming the undifferentiated con-
stituency feature, p &lt; .05).
</bodyText>
<table confidence="0.999947941176471">
Arabic MT03 MT06 MT08
Baseline .4795 .3571 .3571
Chiang-05 .4787 .3679** .3678**
VP+ .4802 .3481
AP+ .4856 .3495
IP+ .4818 .3516
CP= .4815 .3523
NP= .4847 .3537
NP+ .4800 .3548
AP= .4797 .3569
AdvP+ .4852 .3572
CP+ .4758 .3578
IP= .4811 .3636** .3647**
PP= .4801 .3651** .3662**
VP= .4803 .3655** .3694**
PP+ .4837** .3707** .3700**
AdvP= .4823** .3711**- .3717**
Multiple / conflated features:
XP+ .4771 .3522
all-labels2 .4898**+ .3536 .3572
all-labels_ .4828 .3548
VP2 .4826 .3552
NP2 .4832 .3561
navr�vr�rr�M= .4826 .3571
VP_ .4825 .3604
all-labels+ .4825 .3600
XP2 .4859**+ .3605^ .3613**
IP2 .4793 .3611* .3593
IP_ .4791 .3635* .3648**
XP= .4808 .3659** .3704**+
vr=rr+aavr= .4833** .3677** .3718**
AP2 .4840** .3692** .3719**
PP+.AdvP= .4777 .3708** .3680**
AdvP2 .4803 .3765**++ .3740**+
</table>
<tableCaption confidence="0.9803615">
Table 4: Arabic-English Experiments. Results are
sorted by MT06 BLEU score. *: Better than baseline
</tableCaption>
<bodyText confidence="0.88984075">
(p &lt; .05). **: Better than baseline (p &lt; .01). +: Bet-
ter than Chiang-05 (p &lt; .05). ++: Better than Chiang-05
(p &lt; .01). -: Almost significantly better than Chiang-05
(p &lt; .075)
</bodyText>
<page confidence="0.993866">
1008
</page>
<sectionHeader confidence="0.997561" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999966805555556">
The results in Section 4 demonstrate, to our knowl-
edge for the first time, that significant and sometimes
substantial gains over baseline can be obtained by
incorporating soft syntactic constraints into Hiero’s
translation model. Within language, we also see
considerable consistency across multiple test sets, in
terms of which constraints tend to help most.
Furthermore, our results provide some insight into
why the original approach may have failed to yield a
positive outcome. For Chinese, we found that when
we defined finer-grained versions of the exact-match
features, there was value for some constituency
types in biasing the model to favor matching the
source language parse. Moreover, we found that
there was significant value in allowing the model
to be sensitive to violations (crossing boundaries)
of source parses. These results confirm that parser
quality was not the limitation in the original work
(or at least not the only limitation), since in our ex-
periments the parser was held constant.
Looking at combinations of new features, some
“double-feature” combinations (VP2, IP2) achieved
large gains, although note that more is not neces-
sarily better: combinations of more features did not
yield better scores, and some did not yield any gain
at all. No conflated feature reached significance, but
it is not the case that all conflated features are worse
than their same-constituent “double-feature” coun-
terparts. We found no simple correlation between
finer-grained feature scores (and/or boundary con-
dition type) and combination or conflation scores.
Since some combinations seem to cancel individ-
ual contributions, we can conclude that the higher
the number of participant features (of the kinds de-
scribed here), the more likely a cancellation effect is;
therefore, a “double-feature” combination is more
likely to yield higher gains than a combination con-
taining more features.
We also investigated whether non-canonical lin-
guistic constituency labels such as PRN, FRAG,
UCP and VSB introduce “noise”, by means of the
XP features — the XP= feature is, in fact, simply the
undifferentiated constituency feature, but sensitive
only to “standard” XPs. Although performance of
XP=, XP2 and all-labels+ were similar to that of the
undifferentiated constituency feature, XP+ achieved
the highest gain. Intuitively, this seems plausible:
the feature says, at least for Chinese, that a transla-
tion hypothesis should incur a penalty if it is trans-
lating a substring as a unit when that substring is not
a canonical source constituent.
Having obtained positive results with Chinese, we
explored the extent to which the approach might
improve translation using a very different source
language. The approach on Arabic-English trans-
lation yielded large BLEU gains over baseline, as
well as significant improvements over the undiffer-
entiated constituency feature. Comparing the two
sets of experiments, we see that there are definitely
language-specific variations in the value of syntactic
constraints; for example, AdvP, the top performer in
Arabic, cannot possibly perform well for Chinese,
since in our parses the AdvP constituents rarely in-
clude more than a single word. At the same time,
some IP and VP variants seem to do generally well
in both languages. This makes sense, since — at
least for these language pairs and perhaps more gen-
erally — clauses and verb phrases seem to corre-
spond often on the source and target side. We found
it more surprising that no NP variant yielded much
gain in Arabic; this question will be taken up in fu-
ture work.
</bodyText>
<sectionHeader confidence="0.999863" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999682736842105">
Space limitations preclude a thorough review of
work attempting to navigate the tradeoff between us-
ing language analyzers and exploiting unconstrained
data-driven modeling, although the recent literature
is full of variety and promising approaches. We limit
ourselves here to several approaches that seem most
closely related.
Among approaches using parser-based syntactic
models, several researchers have attempted to re-
duce the strictness of syntactic constraints in or-
der to better exploit shallow correspondences in
parallel training data. Our introduction has al-
ready briefly noted Cowan et al. (2006), who relax
parse-tree-based alignment to permit alignment of
non-constituent subphrases on the source side, and
translate modifiers using a separate phrase-based
model, and DeNeefe et al. (2007), who modify
syntax-based extraction and binarize trees (follow-
ing (Wang et al., 2007b)) to improve phrasal cov-
</bodyText>
<page confidence="0.987976">
1009
</page>
<bodyText confidence="0.999471585365853">
erage. Similarly, Marcu et al. (2006) relax their
syntax-based system by rewriting target-side parse
trees on the fly in order to avoid the loss of “non-
syntactifiable” phrase pairs. Setiawan et al. (2007)
employ a “function-word centered syntax-based ap-
proach”, with synchronous CFG and extended ITG
models for reordering phrases, and relax syntac-
tic constraints by only using a small number func-
tion words (approximated by high-frequency words)
to guide the phrase-order inversion. Zollman and
Venugopal (2006) start with a target language parser
and use it to provide constraints on the extraction of
hierarchical phrase pairs. Unlike Hiero, their trans-
lation model uses a full range of named nonterminal
symbols in the synchronous grammar. As an alter-
native way to relax strict parser-based constituency
requirements, they explore the use of phrases span-
ning generalized, categorial-style constituents in the
parse tree, e.g. type NP/NN denotes a phrase like
the great that lacks only a head noun (say, wall) in
order to comprise an NP.
In addition, various researchers have explored the
use of hard linguistic constraints on the source side,
e.g. via “chunking” noun phrases and translating
them separately (Owczarzak et al., 2006), or by per-
forming hard reorderings of source parse trees in
order to more closely approximate target-language
word order (Wang et al., 2007a; Collins et al., 2005).
Finally, another soft-constraint approach that can
also be viewed as coming from the data-driven side,
adding syntax, is taken by Riezler and Maxwell
(2006). They use LFG dependency trees on both
source and target sides, and relax syntactic con-
straints by adding a “fragment grammar” for un-
parsable chunks. They decode using Pharaoh, aug-
mented with their own log-linear features (such as
p(esnippet|fsnippet) and its converse), side by side to
“traditional” lexical weights. Riezler and Maxwell
(2006) do not achieve higher BLEU scores, but
do score better according to human grammaticality
judgments for in-coverage cases.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999881454545455">
When hierarchical phrase-based translation was in-
troduced by Chiang (2005), it represented a new and
successful way to incorporate syntax into statistical
MT, allowing the model to exploit non-local depen-
dencies and lexically sensitive reordering without
requiring linguistically motivated parsing of either
the source or target language. An approach to incor-
porating parser-based constituents in the model was
explored briefly, treating syntactic constituency as a
soft constraint, with negative results.
In this paper, we returned to the idea of linguis-
tically motivated soft constraints, and we demon-
strated that they can, in fact, lead to substantial
improvements in translation performance when in-
tegrated into the Hiero framework. We accom-
plished this using constraints that not only dis-
tinguish among constituent types, but which also
distinguish between the benefit of matching the
source parse bracketing, versus the cost of us-
ing phrases that cross relevant bracketing bound-
aries. We demonstrated improvements for Chinese-
English translation, and succeed in obtaining sub-
stantial gains for Arabic-English translation, as well.
Our results contribute to a growing body of work
on combining monolingually based, linguistically
motivated syntactic analysis with translation mod-
els that are closely tied to observable parallel train-
ing data. Consistent with other researchers, we find
that “syntactic constituency” may be too coarse a no-
tion by itself; rather, there is value in taking a finer-
grained approach, and in allowing the model to de-
cide how far to trust each element of the syntactic
analysis as part of the system’s optimization process.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998986666666667">
This work was supported in part by DARPA prime
agreement HR0011-06-2-0001. The authors would
like to thank David Chiang and Adam Lopez for
making their source code available; the Stanford
Parser team and Mary Harper for making their
parsers available; David Chiang, Amy Weinberg,
and CLIP Laboratory colleagues, particularly Chris
Dyer, Adam Lopez, and Smaranda Muresan, for dis-
cussion and invaluable assistance.
</bodyText>
<page confidence="0.990857">
1010
</page>
<sectionHeader confidence="0.983514" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99963533018868">
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the ACL Workshop on
Statistical Machine Translation 2007.
P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263–313.
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech.
Report TR-10-98, Comp. Sci. Group, Harvard U.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263–270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings ofACL-05.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. EMNLP.
S DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based MT learn from phrase-based
MT? In Proceedings ofEMNLP-CoNLL.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In ACL Companion Vol.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. EMNLP 2002.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL-06.
H. Hassan, K. Sima’an, and A. Way. 2007. Integrating
supertags into phrase-based statistical machine trans-
lation. In Proc. ACL-07, pages 288–295.
Zhongqiang Huang, Denis Filimonov, and Mary Harper.
2008. Accuracy enhancements for mandarin parsing.
Tech. report, University of Maryland.
Dan Klein and Christopher D. Manning. 2003a. Accu-
rate unlexicalized parsing. In Proceedings ofACL-03,
pages 423–430.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. Advances in Neural Information Pro-
cessing Systems, 15(NIPS 2002):3–10.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proc. EMNLP+CoNLL, pages 868–
876, Prague.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings ofHLT-NAACL, pages 127–133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Adam Lopez. (to appear). Statistical machine transla-
tion. ACM Computing Surveys. Earlier version: A
Survey of Statistical Machine Translation. U. of Mary-
land, UMIACS tech. report 2006-47. Apr 2007.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language phrases. In
Proc. EMNLP, pages 44–52.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the ACL, pages 440–447. GIZA++.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160–167.
K. Owczarzak, B. Mellebeek, D. Groves, J. Van Gen-
abith, and A. Way. 2006. Wrapper syntax for
example-based machine translation. In Proceedings
of the 7th Conference of the Association for Machine
Translation in the Americas, pages 148–155.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002. Corpusbased
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Pro-
ceedings of the Human Language Technology Confer-
ence (ACL’2002), pages 124–127, San Diego, CA.
Stefan Riezler and John Maxwell. 2006. Grammatical
machine translation. In Proc. HLT-NAACL, New York,
NY.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 712–719.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901–904.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.
Chao Wang, Michael Collins, and Phillip Koehn. 2007a.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007b. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In Proc. EMNLP+CoNLL 2007.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377–404.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the SMT Workshop, HLT-NAACL.
</reference>
<page confidence="0.993222">
1011
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.663153">
<title confidence="0.999799">Soft Syntactic Constraints for Hierarchical Phrased-Based Translation</title>
<author confidence="0.999738">Yuval Marton</author>
<author confidence="0.999738">Philip Resnik</author>
<affiliation confidence="0.890574333333333">Department of Linguistics and the Laboratory for Computational Linguistics and Information Processing (CLIP) at the Institute for Advanced Computer Studies (UMIACS)</affiliation>
<address confidence="0.925377">University of Maryland, College Park, MD 20742-7505, USA</address>
<email confidence="0.996277">ymarton@tumiacs.umd.edu</email>
<email confidence="0.996277">resnik@tumiacs.umd.edu</email>
<abstract confidence="0.993709833333333">In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data. A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>CCG supertags in factored statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation</booktitle>
<contexts>
<context position="2338" citStr="Birch et al., 2007" startWordPosition="344" endWordPosition="347">l., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.&apos; The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2007</marker>
<rawString>Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007. CCG supertags in factored statistical machine translation. In Proceedings of the ACL Workshop on Statistical Machine Translation 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1229" citStr="Brown et al., 1993" startWordPosition="171" endWordPosition="174">s have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment. We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language. We obtain substantial improvements in performance for translation from Chinese and Arabic to English. 1 Introduction The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation. Computational Linguistics, 19(2):263–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Tech. Report TR-10-98,</tech>
<institution>Comp. Sci. Group, Harvard U.</institution>
<contexts>
<context position="15002" citStr="Chen and Goodman, 1998" startWordPosition="2397" endWordPosition="2401">Chinese, because this label virtually aways spans only one token in the Chinese parses. definitions of XP+, XP_, and XP2 are analogous. • Similarly, since Chiang’s original constituency feature can be viewed as a disjunctive “alllabels=” feature, we also defined “all-labels+”, “all-labels2”, and “all-labels_” analogously. 4 Experiments We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang’s Hiero system. Language models were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) origi</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. F. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Tech. Report TR-10-98, Comp. Sci. Group, Harvard U.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1765" citStr="Chiang (2005)" startWordPosition="257" endWordPosition="258">stical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.&apos; The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), an</context>
<context position="4572" citStr="Chiang, 2005" startWordPosition="688" endWordPosition="689">ated parse trees, we begin with a model that is syntactic only in the formal sense. We then introduce soft constraints that take source-language parses into account to a limited extent. Introducing syntactic constraints in this restricted way allows us to take maximal advantage of what can be learned from parallel training data, while effectively factoring in key aspects of linguistically motivated analysis. As a result, we obtain substantial improvements in performance for both Chinese-English and Arabic-English translation. In Section 2, we briefly review the Hiero statistical MT framework (Chiang, 2005, 2007), upon which this work builds, and we discuss Chiang’s initial effort to incorporate soft source-language constituency constraints for Chinese-English translation. In Section 3, we suggest that an insufficiently fine-grained view of constituency constraints was responsible for Chiang’s lack of strong results, and introduce finer grained constraints into the model. Section 4 demonstrates the the value of these constraints via substantial improvements in ChineseEnglish translation performance, and extends the approach to Arabic-English. Section 5 discusses the results, and Section 6 consi</context>
<context position="8179" citStr="Chiang (2005)" startWordPosition="1253" endWordPosition="1254"> sensible constituents, and in practice they frequently do not.3 2This is slightly simplified: Chiang’s original formulation of Hiero, which we use, has two nonterminal symbols, X and S. The latter is used only in two special “glue” rules that permit complete trees to be constructed via concatenation of subtrees when there is no better way to combine them. 3For example, this rule could just as well be applied with Xl covering the “phrase” submitted and to produce non-constituent substring submitted and this year in a hypothesis like The budget was submitted and this year cuts are likely. 1004 Chiang (2005) conjectured that there might be value in allowing the Hiero model to favor hypotheses for which the synchronous derivation respects linguistically motivated source-language constituency boundaries, as identified using a parser. He tested this conjecture by adding a soft constraint in the form of a “constituency feature”: if a synchronous rule X —* (e, f) is used in a derivation, and the span of f is a constituent in the sourcelanguage parse, then a term a, is added to the model score in expression (1).4 Unlike a hard constraint, which would simply prevent the application of rules violating sy</context>
<context position="11013" citStr="Chiang (2005)" startWordPosition="1727" endWordPosition="1728">unaffected. 5In fact, this turns out not to be the issue; see Section 4. Figure 1: Illustration of Chiang’s (2005) syntactic constituency feature, which does not distinguish among constituent types. First, the constituency feature treats all syntactic constituent types equally, making no distinction among them. For any given language pair, however, there might be some source constituents that tend to map naturally to the target language as units, and others that do not (Fox, 2002; Eisner, 2003). Moreover, a parser may tend to be more accurate for some constituents than for others. Second, the Chiang (2005) constituency feature gives a rule additional credit when the rule’s source side overlaps exactly with a source-side syntactic constituent. Logically, however, it might make sense not just to give a rule X —* (e, f) extra credit when f matches a constituent, but to incur a cost when f violates a constituent boundary. Using the example in Figure 1, we might want to penalize hypotheses containing rules where f is the minister gave a (and other cases, such as minister gave, minister gave a, and so forth).6 These observations suggest a finer-grained approach to the constituency feature idea, retai</context>
<context position="17127" citStr="Chiang (2005)" startWordPosition="2722" endWordPosition="2723"> Ch/Eng Par News V1 beta LDC2003E07 Ch/En Treebank Par Corpus LDC2005T10 Ch/En News Mag Par Txt (Sinorama) LDC2003E14 FBIS Multilanguage Txts LDC2005T06 Ch News Translation Txt Pt 1 LDC2004T08 HK Par Text (only HKNews) Table 1: Training corpora for Chinese-English translation We trained a 5-gram language model using the English (target) side of the training set, pruning 4- gram and 5-gram singletons. For minimum error rate training and development we used the NIST MTeval MT03 set. Table 2 presents our results. We first evaluated translation performance using the NIST MT06 (nisttext) set. Like Chiang (2005), we find that the original, undifferentiated constituency feature (Chiang05) introduces a negligible, statistically insignificant improvement over the baseline. However, we find that several of the finer-grained constraints (IP=, VP=, VP+, QP+, and NP=) achieve statistically significant improvements over baseline (up to .74 BLEU), and the latter three also improve significantly on the undifferentiated constituency feature. By combining multiple finer-grained syntactic features, we obtain significant improvements of up to 1.65 BLEU points (NP_, VP2, IP2, all-labels_, and XP+). We also obtained</context>
<context position="29263" citStr="Chiang (2005)" startWordPosition="4602" endWordPosition="4603">ide, adding syntax, is taken by Riezler and Maxwell (2006). They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a “fragment grammar” for unparsable chunks. They decode using Pharaoh, augmented with their own log-linear features (such as p(esnippet|fsnippet) and its converse), side by side to “traditional” lexical weights. Riezler and Maxwell (2006) do not achieve higher BLEU scores, but do score better according to human grammaticality judgments for in-coverage cases. 7 Conclusion When hierarchical phrase-based translation was introduced by Chiang (2005), it represented a new and successful way to incorporate syntax into statistical MT, allowing the model to exploit non-local dependencies and lexically sensitive reordering without requiring linguistically motivated parsing of either the source or target language. An approach to incorporating parser-based constituents in the model was explored briefly, treating syntactic constituency as a soft constraint, with negative results. In this paper, we returned to the idea of linguistically motivated soft constraints, and we demonstrated that they can, in fact, lead to substantial improvements in tra</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL-05, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2524" citStr="Chiang, 2007" startWordPosition="374" endWordPosition="375"> from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.&apos; The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system that exploits rich linguistic representations and relaxing some part of it. For example, DeNeefe et al. (2007) begin with a tree-to-string model, using treebank-based target language an</context>
<context position="5365" citStr="Chiang, 2007" startWordPosition="805" endWordPosition="806">n 3, we suggest that an insufficiently fine-grained view of constituency constraints was responsible for Chiang’s lack of strong results, and introduce finer grained constraints into the model. Section 4 demonstrates the the value of these constraints via substantial improvements in ChineseEnglish translation performance, and extends the approach to Arabic-English. Section 5 discusses the results, and Section 6 considers related work. Finally we conclude in Section 7 with a summary and potential directions for future work. 2 Hierarchical Phrase-based Translation 2.1 Hiero Hiero (Chiang, 2005; Chiang, 2007) is a hierarchical phrase-based statistical MT framework that generalizes phrase-based models by permitting phrases with gaps. Formally, Hiero’s translation model is a weighted synchronous contextfree grammar. Hiero employs a generalization of the standard non-hierarchical phrase extraction approach in order to acquire the synchronous rules of the grammar directly from word-aligned parallel text Rules have the form X → he, 1i, where e and f are phrases containing terminal symbols (words) and possibly co-indexed instances of the nonterminal symbol X.2 Associated with each rule is a set of trans</context>
<context position="6651" citStr="Chiang, 2007" startWordPosition="1002" endWordPosition="1003">ature of a rule is the phrase translation (log-)probability O( f, e) _ log p(e |f) , directly analogous to the corresponding feature in non-hierarchical phrase-based models like Pharaoh (Koehn et al., 2003). In addition to this phrase translation probability feature, Hiero’s feature set includes the inverse phrase translation probability log p(�f|e), lexical weights lexwt(�f|e) and lexwt(e |�f), which are estimates of translation quality based on word-level correspondences (Koehn et al., 2003), and a rule penalty allowing the model to learn a preference for longer or shorter derivations; see (Chiang, 2007) for details. These features are combined using a log-linear model, with each synchronous rule contributing � AiOi(f, e) (1) i to the total log-probability of a derived hypothesis. Each Ai is a weight associated with feature Oi, and these weights are typically optimized using minimum error rate training (Och, 2003). 2.2 Soft Syntactic Constraints When looking at Hiero rules, which are acquired automatically by the model from parallel text, it is easy to find many cases that seem to respect linguistically motivated boundaries. For example, X → hjingtian X1, X1 this yeari, seems to capture the u</context>
<context position="9875" citStr="Chiang, 2007" startWordPosition="1536" endWordPosition="1537">d, treating English as the source language for the sake of readability. In this example, a, would be added to the hypothesis score for any rule used in the hypothesis whose source side spanned the minister, a speech, yesterday, gave a speech yesterday, or the minister gave a speech yesterday. A rule translating, say, minister gave a as a unit would receive no such boost. Chiang tested the constituency feature for Chinese-English translation, and obtained no significant improvement on the test set. The idea then seems essentially to have been abandoned; it does not appear in later discussions (Chiang, 2007). 3 Soft Syntactic Constraints, Revisited On the face of it, there are any number of possible reasons Chiang’s (2005) soft constraint did not work – including, for example, practical issues like the quality of the Chinese parses.5 However, we focus here on two conceptual issues underlying his use of source language syntactic constituents. 4Formally, O,(�f, e) is defined as a binary feature, with value 1 if f spans a source constituent and 0 otherwise. In the latter case a,O,(f, e) = 0 and the score in expression (1) is unaffected. 5In fact, this turns out not to be the issue; see Section 4. Fi</context>
<context position="15252" citStr="Chiang, 2007" startWordPosition="2440" endWordPosition="2441">ed “all-labels+”, “all-labels2”, and “all-labels_” analogously. 4 Experiments We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang’s Hiero system. Language models were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3. All models were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effe</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-05.</booktitle>
<contexts>
<context position="28550" citStr="Collins et al., 2005" startWordPosition="4493" endWordPosition="4496">ct parser-based constituency requirements, they explore the use of phrases spanning generalized, categorial-style constituents in the parse tree, e.g. type NP/NN denotes a phrase like the great that lacks only a head noun (say, wall) in order to comprise an NP. In addition, various researchers have explored the use of hard linguistic constraints on the source side, e.g. via “chunking” noun phrases and translating them separately (Owczarzak et al., 2006), or by performing hard reorderings of source parse trees in order to more closely approximate target-language word order (Wang et al., 2007a; Collins et al., 2005). Finally, another soft-constraint approach that can also be viewed as coming from the data-driven side, adding syntax, is taken by Riezler and Maxwell (2006). They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a “fragment grammar” for unparsable chunks. They decode using Pharaoh, augmented with their own log-linear features (such as p(esnippet|fsnippet) and its converse), side by side to “traditional” lexical weights. Riezler and Maxwell (2006) do not achieve higher BLEU scores, but do score better according to human grammaticality judgmen</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings ofACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Ivona Kucerova</author>
<author>Michael Collins</author>
</authors>
<title>A discriminative model for tree-to-tree translation. In</title>
<date>2006</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="1657" citStr="Cowan et al., 2006" startWordPosition="236" endWordPosition="239">stantial improvements in performance for translation from Chinese and Arabic to English. 1 Introduction The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.&apos; The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguist</context>
<context position="3361" citStr="Cowan et al. (2006)" startWordPosition="503" endWordPosition="506">g advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system that exploits rich linguistic representations and relaxing some part of it. For example, DeNeefe et al. (2007) begin with a tree-to-string model, using treebank-based target language analysis, and find it useful to modify it in order to accommodate useful “phrasal” chunks that are present in parallel training data but not licensed by linguistically motivated parses of the target language. Similarly, Cowan et al. (2006) focus on using syntactically rich representations of source and target parse trees, but they resort to phrase-based translation for modifiers within &apos;See (Lopez, to appear) for a comprehensive survey. 1003 Proceedings of ACL-08: HLT, pages 1003–1011, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics clauses. Finding the right way to balance linguistic analysis with unconstrained data-driven modeling is clearly a key challenge. In this paper we address this challenge from a less explored direction. Rather than starting with a system based on linguistically motiva</context>
<context position="26840" citStr="Cowan et al. (2006)" startWordPosition="4229" endWordPosition="4232">Related Work Space limitations preclude a thorough review of work attempting to navigate the tradeoff between using language analyzers and exploiting unconstrained data-driven modeling, although the recent literature is full of variety and promising approaches. We limit ourselves here to several approaches that seem most closely related. Among approaches using parser-based syntactic models, several researchers have attempted to reduce the strictness of syntactic constraints in order to better exploit shallow correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal cov1009 erage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and ex</context>
</contexts>
<marker>Cowan, Kucerova, Collins, 2006</marker>
<rawString>Brooke Cowan, Ivona Kucerova, and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DeNeefe</author>
<author>K Knight</author>
<author>W Wang</author>
<author>D Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT?</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL.</booktitle>
<contexts>
<context position="3049" citStr="DeNeefe et al. (2007)" startWordPosition="455" endWordPosition="458">parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system that exploits rich linguistic representations and relaxing some part of it. For example, DeNeefe et al. (2007) begin with a tree-to-string model, using treebank-based target language analysis, and find it useful to modify it in order to accommodate useful “phrasal” chunks that are present in parallel training data but not licensed by linguistically motivated parses of the target language. Similarly, Cowan et al. (2006) focus on using syntactically rich representations of source and target parse trees, but they resort to phrase-based translation for modifiers within &apos;See (Lopez, to appear) for a comprehensive survey. 1003 Proceedings of ACL-08: HLT, pages 1003–1011, Columbus, Ohio, USA, June 2008. c�20</context>
<context position="27035" citStr="DeNeefe et al. (2007)" startWordPosition="4256" endWordPosition="4259">h the recent literature is full of variety and promising approaches. We limit ourselves here to several approaches that seem most closely related. Among approaches using parser-based syntactic models, several researchers have attempted to reduce the strictness of syntactic constraints in order to better exploit shallow correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal cov1009 erage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion. Z</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>S DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proceedings ofEMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<journal>In ACL Companion</journal>
<volume>Vol.</volume>
<contexts>
<context position="10899" citStr="Eisner, 2003" startWordPosition="1707" endWordPosition="1708"> spans a source constituent and 0 otherwise. In the latter case a,O,(f, e) = 0 and the score in expression (1) is unaffected. 5In fact, this turns out not to be the issue; see Section 4. Figure 1: Illustration of Chiang’s (2005) syntactic constituency feature, which does not distinguish among constituent types. First, the constituency feature treats all syntactic constituent types equally, making no distinction among them. For any given language pair, however, there might be some source constituents that tend to map naturally to the target language as units, and others that do not (Fox, 2002; Eisner, 2003). Moreover, a parser may tend to be more accurate for some constituents than for others. Second, the Chiang (2005) constituency feature gives a rule additional credit when the rule’s source side overlaps exactly with a source-side syntactic constituent. Logically, however, it might make sense not just to give a rule X —* (e, f) extra credit when f matches a constituent, but to incur a cost when f violates a constituent boundary. Using the example in Figure 1, we might want to penalize hypotheses containing rules where f is the minister gave a (and other cases, such as minister gave, minister g</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>J. Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In ACL Companion Vol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="10884" citStr="Fox, 2002" startWordPosition="1705" endWordPosition="1706">alue 1 if f spans a source constituent and 0 otherwise. In the latter case a,O,(f, e) = 0 and the score in expression (1) is unaffected. 5In fact, this turns out not to be the issue; see Section 4. Figure 1: Illustration of Chiang’s (2005) syntactic constituency feature, which does not distinguish among constituent types. First, the constituency feature treats all syntactic constituent types equally, making no distinction among them. For any given language pair, however, there might be some source constituents that tend to map naturally to the target language as units, and others that do not (Fox, 2002; Eisner, 2003). Moreover, a parser may tend to be more accurate for some constituents than for others. Second, the Chiang (2005) constituency feature gives a rule additional credit when the rule’s source side overlaps exactly with a source-side syntactic constituent. Logically, however, it might make sense not just to give a rule X —* (e, f) extra credit when f matches a constituent, but to incur a cost when f violates a constituent boundary. Using the example in Figure 1, we might want to penalize hypotheses containing rules where f is the minister gave a (and other cases, such as minister g</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi Fox. 2002. Phrasal cohesion and statistical machine translation. In Proc. EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06.</booktitle>
<contexts>
<context position="1729" citStr="Galley et al., 2006" startWordPosition="249" endWordPosition="253">Arabic to English. 1 Introduction The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.&apos; The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING/ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hassan</author>
<author>K Sima’an</author>
<author>A Way</author>
</authors>
<title>Integrating supertags into phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL-07,</booktitle>
<pages>288--295</pages>
<marker>Hassan, Sima’an, Way, 2007</marker>
<rawString>H. Hassan, K. Sima’an, and A. Way. 2007. Integrating supertags into phrase-based statistical machine translation. In Proc. ACL-07, pages 288–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
</authors>
<title>Accuracy enhancements for mandarin parsing.</title>
<date>2008</date>
<tech>Tech. report,</tech>
<institution>University of Maryland.</institution>
<contexts>
<context position="15407" citStr="Huang et al., 2008" startWordPosition="2462" endWordPosition="2465">nd from Arabic to English, using a descendant of Chiang’s Hiero system. Language models were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3. All models were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effective reference length, on lowercased, tokenized outputs/references. Statistical significance of difference from the baseline BLEU score was measured by us</context>
</contexts>
<marker>Huang, Filimonov, Harper, 2008</marker>
<rawString>Zhongqiang Huang, Denis Filimonov, and Mary Harper. 2008. Accuracy enhancements for mandarin parsing. Tech. report, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL-03,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="15493" citStr="Klein and Manning, 2003" startWordPosition="2475" endWordPosition="2478">odels were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3. All models were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effective reference length, on lowercased, tokenized outputs/references. Statistical significance of difference from the baseline BLEU score was measured by using paired bootstrap re-sampling (Koehn, 2004).9 4.1 Chinese-English For the Chinese-E</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. Accurate unlexicalized parsing. In Proceedings ofACL-03, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems, 15(NIPS</booktitle>
<pages>2002--3</pages>
<contexts>
<context position="15493" citStr="Klein and Manning, 2003" startWordPosition="2475" endWordPosition="2478">odels were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3. All models were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effective reference length, on lowercased, tokenized outputs/references. Statistical significance of difference from the baseline BLEU score was measured by using paired bootstrap re-sampling (Koehn, 2004).9 4.1 Chinese-English For the Chinese-E</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Fast exact inference with a factored model for natural language parsing. Advances in Neural Information Processing Systems, 15(NIPS 2002):3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP+CoNLL,</booktitle>
<pages>868--876</pages>
<location>Prague.</location>
<contexts>
<context position="2318" citStr="Koehn and Hoang, 2007" startWordPosition="340" endWordPosition="343"> al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.&apos; The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves st</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proc. EMNLP+CoNLL, pages 868– 876, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="6244" citStr="Koehn et al., 2003" startWordPosition="939" endWordPosition="942">-hierarchical phrase extraction approach in order to acquire the synchronous rules of the grammar directly from word-aligned parallel text Rules have the form X → he, 1i, where e and f are phrases containing terminal symbols (words) and possibly co-indexed instances of the nonterminal symbol X.2 Associated with each rule is a set of translation model features, Oi(�f, e); for example, one intuitively natural feature of a rule is the phrase translation (log-)probability O( f, e) _ log p(e |f) , directly analogous to the corresponding feature in non-hierarchical phrase-based models like Pharaoh (Koehn et al., 2003). In addition to this phrase translation probability feature, Hiero’s feature set includes the inverse phrase translation probability log p(�f|e), lexical weights lexwt(�f|e) and lexwt(e |�f), which are estimates of translation quality based on word-level correspondences (Koehn et al., 2003), and a rule penalty allowing the model to learn a preference for longer or shorter derivations; see (Chiang, 2007) for details. These features are combined using a log-linear model, with each synchronous rule contributing � AiOi(f, e) (1) i to the total log-probability of a derived hypothesis. Each Ai is a</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT-NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="16053" citStr="Koehn, 2004" startWordPosition="2557" endWordPosition="2558">ser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3. All models were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effective reference length, on lowercased, tokenized outputs/references. Statistical significance of difference from the baseline BLEU score was measured by using paired bootstrap re-sampling (Koehn, 2004).9 4.1 Chinese-English For the Chinese-English translation experiments, we trained the translation model on the corpora in Table 1, totalling approximately 2.1 million sentence pairs after GIZA++ filtering for length ratio. Chinese text was segmented using the Stanford segmenter (Tseng et al., 2005). 9Whenever we use the word “significant”, we mean “statistically significant” (at p &lt; .05 unless specified otherwise). 1006 LDC ID Description LDC2002E18 Xinhua Ch/Eng Par News V1 beta LDC2003E07 Ch/En Treebank Par Corpus LDC2005T10 Ch/En News Mag Par Txt (Sinorama) LDC2003E14 FBIS Multilanguage Tx</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>(to appear). Statistical machine translation. ACM Computing Surveys. Earlier version: A Survey of</title>
<date>2007</date>
<booktitle>Statistical Machine Translation. U. of Maryland, UMIACS tech. report</booktitle>
<pages>2006--47</pages>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. (to appear). Statistical machine translation. ACM Computing Surveys. Earlier version: A Survey of Statistical Machine Translation. U. of Maryland, UMIACS tech. report 2006-47. Apr 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="1707" citStr="Marcu et al., 2006" startWordPosition="245" endWordPosition="248">on from Chinese and Arabic to English. 1 Introduction The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.&apos; The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and </context>
<context position="27188" citStr="Marcu et al. (2006)" startWordPosition="4280" endWordPosition="4283">proaches using parser-based syntactic models, several researchers have attempted to reduce the strictness of syntactic constraints in order to better exploit shallow correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal cov1009 erage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion. Zollman and Venugopal (2006) start with a target language parser and use it to provide constraints on the extraction of hierarchical phrase pairs. Unlike </context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. EMNLP, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL,</booktitle>
<pages>440--447</pages>
<publisher>GIZA++.</publisher>
<contexts>
<context position="15072" citStr="Och and Ney, 2000" startWordPosition="2408" endWordPosition="2411">nese parses. definitions of XP+, XP_, and XP2 are analogous. • Similarly, since Chiang’s original constituency feature can be viewed as a disjunctive “alllabels=” feature, we also defined “all-labels+”, “all-labels2”, and “all-labels_” analogously. 4 Experiments We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang’s Hiero system. Language models were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseli</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the ACL, pages 440–447. GIZA++.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="6967" citStr="Och, 2003" startWordPosition="1053" endWordPosition="1054">nslation probability log p(�f|e), lexical weights lexwt(�f|e) and lexwt(e |�f), which are estimates of translation quality based on word-level correspondences (Koehn et al., 2003), and a rule penalty allowing the model to learn a preference for longer or shorter derivations; see (Chiang, 2007) for details. These features are combined using a log-linear model, with each synchronous rule contributing � AiOi(f, e) (1) i to the total log-probability of a derived hypothesis. Each Ai is a weight associated with feature Oi, and these weights are typically optimized using minimum error rate training (Och, 2003). 2.2 Soft Syntactic Constraints When looking at Hiero rules, which are acquired automatically by the model from parallel text, it is easy to find many cases that seem to respect linguistically motivated boundaries. For example, X → hjingtian X1, X1 this yeari, seems to capture the use of jingtian/this year as a temporal modifier when building linguistic constituents such as noun phrases (the election this year) or verb phrases (voted in the primary this year). However, it is important to observe that nothing in the Hiero framework actually requires nonterminal symbols to cover linguistically </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Owczarzak</author>
<author>B Mellebeek</author>
<author>D Groves</author>
<author>J Van Genabith</author>
<author>A Way</author>
</authors>
<title>Wrapper syntax for example-based machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>148--155</pages>
<marker>Owczarzak, Mellebeek, Groves, Van Genabith, Way, 2006</marker>
<rawString>K. Owczarzak, B. Mellebeek, D. Groves, J. Van Genabith, and A. Way. 2006. Wrapper syntax for example-based machine translation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>John Henderson</author>
<author>Florence Reeder</author>
</authors>
<title>Corpusbased comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference (ACL’2002),</booktitle>
<pages>124--127</pages>
<location>San Diego, CA.</location>
<contexts>
<context position="15809" citStr="Papineni et al., 2002" startWordPosition="2523" endWordPosition="2526">o included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3. All models were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effective reference length, on lowercased, tokenized outputs/references. Statistical significance of difference from the baseline BLEU score was measured by using paired bootstrap re-sampling (Koehn, 2004).9 4.1 Chinese-English For the Chinese-English translation experiments, we trained the translation model on the corpora in Table 1, totalling approximately 2.1 million sentence pairs after GIZA++ filtering for length ratio. Chinese text was segmented using the Stanford segmenter (Tseng et al., 2005). 9Whenever we use the word “significant”, we mean “stat</context>
</contexts>
<marker>Papineni, Roukos, Ward, Henderson, Reeder, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, John Henderson, and Florence Reeder. 2002. Corpusbased comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results. In Proceedings of the Human Language Technology Conference (ACL’2002), pages 124–127, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John Maxwell</author>
</authors>
<title>Grammatical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="28708" citStr="Riezler and Maxwell (2006)" startWordPosition="4517" endWordPosition="4520">pe NP/NN denotes a phrase like the great that lacks only a head noun (say, wall) in order to comprise an NP. In addition, various researchers have explored the use of hard linguistic constraints on the source side, e.g. via “chunking” noun phrases and translating them separately (Owczarzak et al., 2006), or by performing hard reorderings of source parse trees in order to more closely approximate target-language word order (Wang et al., 2007a; Collins et al., 2005). Finally, another soft-constraint approach that can also be viewed as coming from the data-driven side, adding syntax, is taken by Riezler and Maxwell (2006). They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a “fragment grammar” for unparsable chunks. They decode using Pharaoh, augmented with their own log-linear features (such as p(esnippet|fsnippet) and its converse), side by side to “traditional” lexical weights. Riezler and Maxwell (2006) do not achieve higher BLEU scores, but do score better according to human grammaticality judgments for in-coverage cases. 7 Conclusion When hierarchical phrase-based translation was introduced by Chiang (2005), it represented a new and successful way to </context>
</contexts>
<marker>Riezler, Maxwell, 2006</marker>
<rawString>Stefan Riezler and John Maxwell. 2006. Grammatical machine translation. In Proc. HLT-NAACL, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Min-Yen Kan</author>
<author>Haizhou Li</author>
</authors>
<title>Ordering phrases with function words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>712--719</pages>
<contexts>
<context position="27355" citStr="Setiawan et al. (2007)" startWordPosition="4307" endWordPosition="4310">w correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal cov1009 erage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion. Zollman and Venugopal (2006) start with a target language parser and use it to provide constraints on the extraction of hierarchical phrase pairs. Unlike Hiero, their translation model uses a full range of named nonterminal symbols in the synchronous grammar. As an alternative way to relax strict parser-based constituen</context>
</contexts>
<marker>Setiawan, Kan, Li, 2007</marker>
<rawString>Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007. Ordering phrases with function words. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 712–719.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="14942" citStr="Stolcke, 2002" startWordPosition="2391" endWordPosition="2392">pear only in Chinese. We ran no ADJP experiment in Chinese, because this label virtually aways spans only one token in the Chinese parses. definitions of XP+, XP_, and XP2 are analogous. • Similarly, since Chiang’s original constituency feature can be viewed as a disjunctive “alllabels=” feature, we also defined “all-labels+”, “all-labels2”, and “all-labels_” analogously. 4 Experiments We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang’s Hiero system. Language models were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="16353" citStr="Tseng et al., 2005" startWordPosition="2600" endWordPosition="2603">els were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effective reference length, on lowercased, tokenized outputs/references. Statistical significance of difference from the baseline BLEU score was measured by using paired bootstrap re-sampling (Koehn, 2004).9 4.1 Chinese-English For the Chinese-English translation experiments, we trained the translation model on the corpora in Table 1, totalling approximately 2.1 million sentence pairs after GIZA++ filtering for length ratio. Chinese text was segmented using the Stanford segmenter (Tseng et al., 2005). 9Whenever we use the word “significant”, we mean “statistically significant” (at p &lt; .05 unless specified otherwise). 1006 LDC ID Description LDC2002E18 Xinhua Ch/Eng Par News V1 beta LDC2003E07 Ch/En Treebank Par Corpus LDC2005T10 Ch/En News Mag Par Txt (Sinorama) LDC2003E14 FBIS Multilanguage Txts LDC2005T06 Ch News Translation Txt Pt 1 LDC2004T08 HK Par Text (only HKNews) Table 1: Training corpora for Chinese-English translation We trained a 5-gram language model using the English (target) side of the training set, pruning 4- gram and 5-gram singletons. For minimum error rate training and</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Phillip Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="27120" citStr="Wang et al., 2007" startWordPosition="4269" endWordPosition="4272">ere to several approaches that seem most closely related. Among approaches using parser-based syntactic models, several researchers have attempted to reduce the strictness of syntactic constraints in order to better exploit shallow correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal cov1009 erage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion. Zollman and Venugopal (2006) start with a target language parser and use it to provide</context>
<context position="28526" citStr="Wang et al., 2007" startWordPosition="4489" endWordPosition="4492">ve way to relax strict parser-based constituency requirements, they explore the use of phrases spanning generalized, categorial-style constituents in the parse tree, e.g. type NP/NN denotes a phrase like the great that lacks only a head noun (say, wall) in order to comprise an NP. In addition, various researchers have explored the use of hard linguistic constraints on the source side, e.g. via “chunking” noun phrases and translating them separately (Owczarzak et al., 2006), or by performing hard reorderings of source parse trees in order to more closely approximate target-language word order (Wang et al., 2007a; Collins et al., 2005). Finally, another soft-constraint approach that can also be viewed as coming from the data-driven side, adding syntax, is taken by Riezler and Maxwell (2006). They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a “fragment grammar” for unparsable chunks. They decode using Pharaoh, augmented with their own log-linear features (such as p(esnippet|fsnippet) and its converse), side by side to “traditional” lexical weights. Riezler and Maxwell (2006) do not achieve higher BLEU scores, but do score better according to huma</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Phillip Koehn. 2007a. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP+CoNLL</booktitle>
<contexts>
<context position="27120" citStr="Wang et al., 2007" startWordPosition="4269" endWordPosition="4272">ere to several approaches that seem most closely related. Among approaches using parser-based syntactic models, several researchers have attempted to reduce the strictness of syntactic constraints in order to better exploit shallow correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal cov1009 erage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion. Zollman and Venugopal (2006) start with a target language parser and use it to provide</context>
<context position="28526" citStr="Wang et al., 2007" startWordPosition="4489" endWordPosition="4492">ve way to relax strict parser-based constituency requirements, they explore the use of phrases spanning generalized, categorial-style constituents in the parse tree, e.g. type NP/NN denotes a phrase like the great that lacks only a head noun (say, wall) in order to comprise an NP. In addition, various researchers have explored the use of hard linguistic constraints on the source side, e.g. via “chunking” noun phrases and translating them separately (Owczarzak et al., 2006), or by performing hard reorderings of source parse trees in order to more closely approximate target-language word order (Wang et al., 2007a; Collins et al., 2005). Finally, another soft-constraint approach that can also be viewed as coming from the data-driven side, adding syntax, is taken by Riezler and Maxwell (2006). They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a “fragment grammar” for unparsable chunks. They decode using Pharaoh, augmented with their own log-linear features (such as p(esnippet|fsnippet) and its converse), side by side to “traditional” lexical weights. Riezler and Maxwell (2006) do not achieve higher BLEU scores, but do score better according to huma</context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007b. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proc. EMNLP+CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="2535" citStr="Wu, 1997" startWordPosition="376" endWordPosition="377">es that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.&apos; The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system that exploits rich linguistic representations and relaxing some part of it. For example, DeNeefe et al. (2007) begin with a tree-to-string model, using treebank-based target language analysis, and</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the SMT Workshop, HLT-NAACL.</booktitle>
<contexts>
<context position="1687" citStr="Zollmann and Venugopal, 2006" startWordPosition="240" endWordPosition="244">s in performance for translation from Chinese and Arabic to English. 1 Introduction The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.&apos; The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to trainin</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the SMT Workshop, HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>