<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997909">
Power-Law Distributions for Paraphrases Extracted from Bilingual
Corpora
</title>
<author confidence="0.979045">
Spyros Martzoukos Christof Monz
</author>
<affiliation confidence="0.995399">
Informatics Institute, University of Amsterdam
</affiliation>
<address confidence="0.923862">
Science Park 904, 1098 XH Amsterdam, The Netherlands
</address>
<email confidence="0.998172">
{s.martzoukos, c.monz}@uva.nl
</email>
<sectionHeader confidence="0.99386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889352941176">
We describe a novel method that extracts
paraphrases from a bitext, for both the
source and target languages. In order
to reduce the search space, we decom-
pose the phrase-table into sub-phrase-tables
and construct separate clusters for source
and target phrases. We convert the clus-
ters into graphs, add smoothing/syntactic-
information-carrier vertices, and compute
the similarity between phrases with a ran-
dom walk-based measure, the commute
time. The resulting phrase-paraphrase
probabilities are built upon the conversion
of the commute times into artificial co-
occurrence counts with a novel technique.
The co-occurrence count distribution be-
longs to the power-law family.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937509090909">
Paraphrase extraction has emerged as an impor-
tant problem in NLP. Currently, there exists an
abundance of methods for extracting paraphrases
from monolingual, comparable and bilingual cor-
pora (Madnani and Dorr, 2010; Androutsopou-
los and Malakasiotis, 2010); we focus on the lat-
ter and specifically on the phrase-table that is ex-
tracted from a bitext during the training stage of
Statistical Machine Translation (SMT). Bannard
and Callison-Burch (2005) introduced the pivot-
ing approach, which relies on a 2-step transition
from a phrase, via its translations, to a paraphrase
candidate. By incorporating the syntactic struc-
ture of phrases (Callison-Burch, 2005), the qual-
ity of the paraphrases extracted with pivoting can
be improved. Kok and Brockett (2010) (hence-
forth KB) used a random walk framework to de-
termine the similarity between phrases, which
was shown to outperform pivoting with syntac-
tic information, when multiple phrase-tables are
used. In SMT, extracted paraphrases with asso-
ciated pivot-based (Callison-Burch et al., 2006;
Onishi et al., 2010) and cluster-based (Kuhn et
al., 2010) probabilities have been found to im-
prove the quality of translation. Pivoting has also
been employed in the extraction of syntactic para-
phrases, which are a mixture of phrases and non-
terminals (Zhao et al., 2008; Ganitkevitch et al.,
2011).
We develop a method for extracting para-
phrases from a bitext for both the source and tar-
get languages. Emphasis is placed on the qual-
ity of the phrase-paraphrase probabilities as well
as on providing a stepping stone for extracting
syntactic paraphrases with equally reliable prob-
abilities. In line with previous work, our method
depends on the connectivity of the phrase-table,
but the resulting construction treats each side sep-
arately, which can potentially be benefited from
additional monolingual data.
The initial problem in harvesting paraphrases
from a phrase-table is the identification of the
search space. Previous work has relied on breadth
first search from the query phrase with a depth
of 2 (pivoting) and 6 (KB). The former can be
too restrictive and the latter can lead to excessive
noise contamination when taking shallow syntac-
tic information features into account. Instead, we
choose to cluster the phrase-table into separate
source and target clusters and in order to make this
task computationally feasible, we decompose the
phrase-table into sub-phrase-tables. We propose
a novel heuristic algorithm for the decomposition
of the phrase-table (Section 2.1), and use a well-
established co-clustering algorithm for clustering
</bodyText>
<page confidence="0.951376">
2
</page>
<note confidence="0.976624">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 2–11,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.997894266666667">
each sub-phrase-table (Section 2.2).
The underlying connectivity of the source
and target clusters gives rise to a natural graph
representation for each cluster (Section 3.1).
The vertices of the graphs consist of phrases
and features with a dual smoothing/syntactic-
information-carrier role. The latter allow (a) re-
distribution of the mass for phrases with no appro-
priate paraphrases and (b) the extraction of syn-
tactic paraphrases. The proximity among vertices
of a graph is measured by means of a random walk
distance measure, the commute time (Aldous and
Fill, 2001). This measure is known to perform
well in identifying similar words on the graph of
WordNet (Rao et al., 2008) and a related measure,
the hitting time is known to perform well in har-
vesting paraphrases on a graph constructed from
multiple phrase-tables (KB).
Generally in NLP, power-law distributions are
typically encountered in the collection of counts
during the training stage. The distances of Sec-
tion 3.1 are converted into artificial co-occurrence
counts with a novel technique (Section 3.2). Al-
though they need not be integers, the main chal-
lenge is the type of the underlying distributions;
it should ideally emulate the resulting count dis-
tributions from the phrase extraction stage of a
monolingual parallel corpus (Dolan et al., 2004).
These counts give rise to the desired probability
distributions by means of relative frequencies.
</bodyText>
<sectionHeader confidence="0.972792" genericHeader="method">
2 Sub-phrase-tables &amp; Clustering
</sectionHeader>
<subsectionHeader confidence="0.959005">
2.1 Extracting Connected Components
</subsectionHeader>
<bodyText confidence="0.9999565">
For the decomposition of the phrase-table into
sub-phrase-tables it is convenient to view the
phrase-table as an undirected, unweighted graph
P with the vertex set being the source and target
phrases and the edge set being the phrase-table en-
tries. For the rest of this section, we do not distin-
guish between source and target phrases, i.e. both
types are treated equally as vertices of P. When
referring to the size of a graph, we mean the num-
ber of vertices it contains.
A trivial initial decomposition of P is achieved
by identifying all its connected components (com-
ponents for brevity), i.e. the mutually disjoint
connected subgraphs, {P0, P1, ..., Pn}. It turns
out (see Section 4.1) that the largest component,
say P0, is of significant size. We call P0 giant
and it needs to be further decomposed. This is
done by identifying all vertices such that, upon
removal, the component becomes disconnected.
Such vertices are called articulation points or cut-
vertices. Cut-vertices of high connectivity degree
are removed from the giant component (see Sec-
tion 4.1). For the remaining vertices of the giant
component, new components are identified and
we proceed iteratively, while keeping track of the
cut-vertices that are removed at each iteration, un-
til the size of the largest component is less than a
certain threshold 0 (see Section 4.1).
Note that at each iteration, when removing cut-
vertices from a giant component, the resulting col-
lection of components may include graphs con-
sisting of a single vertex. We refer to such ver-
tices as residues. They are excluded from the re-
sulting collection and are considered for separate
treatment, as explained later in this section.
The cut-vertices need to be inserted appropri-
ately back to the components: Starting from the
last iteration step, the respective cut-vertices are
added to all the components of P0 which they
used to ‘glue’ together; this process is performed
iteratively, until there are no more cut-vertices to
add. By ‘addition’ of a cut-vertex to a component,
we mean the re-establishment of edges between
the former and other vertices of the latter. The
result is a collection of components whose total
number of unique vertices is less than the number
of vertices of the initial giant component P0.
These remaining vertices are the residues. We
then construct the graph R which consists of
the residues together with all their translations
(even those that are included in components of
the above collection) and then identify its compo-
nents {R0,..., R�}. It turns out, that the largest
component, say R0, is giant and we repeat the de-
composition process that was performed on P0.
This results in a new collection of components
as well as new residues: The components need
to be pruned (see Section 4.1) and the residues
give rise to a new graph R&apos; which is constructed
in the same way as R. We proceed iteratively until
the number of residues stops changing. For each
remaining residue u, we identify its translations,
and for each translation v we identify the largest
component of which v is a member and add u to
that component.
The final result is a collection C = D U F,
where D is the collection of components emerg-
ing from the entire iterative decomposition of P0
</bodyText>
<page confidence="0.997068">
3
</page>
<bodyText confidence="0.931934727272727">
and R, and F = {P1, ..., P,,}. Figure 1 shows
the decomposition of a connected graph G0; for
simplicity we assume that only one cut-vertex is
removed at each iteration and ties are resolved ar-
bitrarily. In Figure 2 the residue graph is con-
structed and its two components are identified.
The iterative insertion of the cut vertices is also
depicted. The resulting two components together
with those from R form the collection D for G0.
The addition of cut-vertices into multiple com-
ponents, as well as the construction method of the
residue-based graph R, can yield the occurrences
of a vertex in multiple components in D. We ex-
ploit this property in two ways:
(a) In order to mitigate the risk of excessive de-
composition (which implies greater risk of good
paraphrases being in different components), as
well as to reduce the size of D, a conserva-
tive merging algorithm of components is em-
ployed. Suppose that the elements of D are
ranked according to size in ascending order as
D = {D1, ..., Dk, Dk+1, ..., D|D|}, where |Di |≤
S, for i = 1, ..., k, and some threshold S (see Sec-
tion 4.1). Each component Di with i ∈ {1, ..., k}
is examined as follows: For each vertex of Di the
number of its occurrences in D is inspected; this is
done in order to identify an appropriate vertex b to
act as a bridge between Di and other components
of which b is a member. Note that translations of
a vertex b with smaller number of occurrences in
D are less likely to capture their full spectrum of
paraphrases. We thus choose a vertex b from Di
with the smallest number of occurrences in D ,
resolving ties arbitrarily, and proceed with merg-
ing Di with the largest component, say Dj with
j ∈ {1, ..., |D |− 1}, of which b is also a member.
The resulting merged component Dj, contains all
vertices and edges of Di and Dj and new edges,
which are formed according to the rule: if u is a
vertex of Di and v is a vertex of Dj and (u, v) is
a phrase-table entry, then (u, v) is an edge in Dj,.
As long as no connected component has identi-
fied Di as the component with which it should be
merged, then Di is deleted from the collection D.
</bodyText>
<listItem confidence="0.836133666666667">
(b) We define an idf-inspired measure for each
phrase pair (x, x&apos;) of the same type (source or tar-
get) as
</listItem>
<bodyText confidence="0.970553166666667">
idf (x, x&apos;) = log |D |log ( c( �) ,+)ID) I, (1)
where c(x, x&apos;) is the number of components in
which the phrases x and x&apos; co-occur, and equiv-
alently for c(·). The purpose of this measure is
for pruning paraphrase candidates and its use is
explained in Section 3.1. Note that idf(x, x&apos;) ∈
[0, 1].
The merging process and the idf measure are
irrelevant for phrases belonging to the compo-
nents of F, since the vertex set of each compo-
nent of F is mutually disjoint with the vertex set
of any other component in C.
</bodyText>
<figureCaption confidence="0.9355396">
Figure 1: The decomposition of G0 with vertices
si and tj: The cut-vertex of the ith iteration is de-
noted by ci, and r collects the residues after each
iteration. The task is completed in Figure 2.
Figure 2: Top: Residue graph with its components
</figureCaption>
<bodyText confidence="0.9368325">
(no further decomposition is required). Bottom:
Adding cut-vertices back to their components.
</bodyText>
<subsectionHeader confidence="0.999585">
2.2 Clustering Connected Components
</subsectionHeader>
<bodyText confidence="0.999885428571428">
The aim of this subsection is to generate sep-
arate clusters for the source and target phrases
of each sub-phrase-table (component) C ∈ C.
For this purpose the Information-Theoretic Co-
Clustering (ITC) algorithm (Dhillon et al., 2003)
is employed, which is a general principled cluster-
ing algorithm that generates hard clusters (i.e. ev-
</bodyText>
<equation confidence="0.694952673469388">
t1
s1
s1
s2
s3
s4
t1
t2
t3
t4
s3 t3
s4
t4
r={t2}
G0
G11
G12
c0={s2}
s3
s4
t4
t3
c1={t3}
s3 t4
r &lt;--r∪{s4}
G12
G21
s2 t2
s2 t2
s4 t3
R
s4 t3
s3
t3
t4
t1
s1
s2
s2
s3
t4
t3
+c1
s3 t4
s1 t1
+c0
+c0
t3
s3 t4
</equation>
<page confidence="0.920322">
4
</page>
<bodyText confidence="0.9999898">
ery element belongs to exactly one cluster) of two
interdependent quantities and is known to per-
form well on high-dimensional and sparse data.
In our case, the interdependent quantities are the
source and target phrases and the sparse data is
the phrase-table.
ITC is a search algorithm similar to K-means,
in the sense that a cost function, is minimized at
each iteration step and the number of clusters for
both quantities are meta-parameters. The number
of clusters is set to the most conservative initial-
ization for both source and target phrases, namely
to as many clusters as there are phrases. At each
iteration, new clusters are constructed based on
the identification of the argmin of the cost func-
tion for each phrase, which gradually reduces the
number of clusters.
We observe that conservative choices for the
meta-parameters often result in good paraphrases
being in different clusters. To overcome this prob-
lem, the hard clusters are converted into soft (i.e.
an element may belong to several clusters): One
step before the stopping criterion is met, we mod-
ify the algorithm so that instead of assigning a
phrase to the cluster with the smallest cost we se-
lect the bottom-X clusters ranked by cost. Addi-
tionally, only a certain number of phrases is cho-
sen for soft clustering. Both selections are done
conservatively with criteria based on the proper-
ties of the cost functions.
The formation of clusters leads to a natural re-
finement of the idf measure defined in eqn. (1):
The quantity c(x, x&apos;) is redefined as the number
of components in which the phrases x and x&apos; co-
occur in at least one cluster.
</bodyText>
<sectionHeader confidence="0.985946" genericHeader="method">
3 Monolingual Graphs &amp; Counts
</sectionHeader>
<bodyText confidence="0.999968">
We proceed with converting the clusters into di-
rected, weighted graphs and then extract para-
phrases for both the source and target side. For
brevity we explain the process restricted to the
source clusters of a sub-phrase-table, but the same
method applies for the target side and for all sub-
phrase-tables in the collection C.
</bodyText>
<subsectionHeader confidence="0.999237">
3.1 Monolingual graphs
</subsectionHeader>
<bodyText confidence="0.999944666666667">
Each source cluster is converted into a graph G as
follows: The vertex set consists of the phrases of
the cluster and an edge between s and s&apos; exists, if
(a) s and s&apos; have at least one translation from the
same target cluster, and (b) idf(s, s&apos;) is greater
than some threshold Q (see Section 4.1). If two
phrases that satisfy condition (b) and have trans-
lations in more than one common target cluster,
a distinct such edge is established. All edges are
bi-directional with distinct weights for both direc-
tions.
Figure 3 depicts an example of such a construc-
tion; a link between a phrase sz and a target cluster
implies the existence of at least one translation for
sz in that cluster. We are not interested in the tar-
get phrases and they are thus not shown. For sim-
plicity we assume that condition (b) is always sat-
isfied and the extracted graph contains the maxi-
mum possible edges. Observe that phrases s3 and
s4 have two edges connecting them, (due to tar-
get clusters T, and Td) and that the target cluster
Ta is irrelevant to the construction of the graph,
since s1 is the only phrase with translations in it.
This conversion of a source cluster into a graph G
</bodyText>
<figureCaption confidence="0.97264325">
Figure 3: Top: A source cluster containing
phrases s1,..., s8 and the associated target clusters
Ta,..., Tf. Bottom: The extracted graph from the
source cluster. All edges are bi-directional.
</figureCaption>
<bodyText confidence="0.8953764">
results in the formation of subgraphs in G, where
each subgraph is generated by a target cluster. In
general, if condition (b) is not always satisfied,
then G need not be connected and each connected
component is treated as a distinct graph.
Analogous to KB, we introduce feature vertices
to G: For each phrase vertex s, its part-of-speech
(POS) tag sequence and stem sequence are iden-
tified and inserted into G as new vertices with
bi-directional weighted edges connected to s. If
phrase vertices s and s&apos; have the same POS tag se-
quence, then they are connected to the same POS
tag feature vertex. Similarly for stem feature ver-
tices. See Figure 4 for an example. Note that we
do not allow edges between POS tag and stem fea-
</bodyText>
<figure confidence="0.97873">
Ta Tb Tc Td TTf
e
s1 s2 s3 s4 s5 s6 s7 s8
s1
s2
s3
s4
s5
s6
s7
s8
5
</figure>
<figureCaption confidence="0.774433">
Figure 4: Adding feature vertices to the extracted
graph (has) - (owns) - (i have) I (i had).
</figureCaption>
<bodyText confidence="0.991386066666667">
Phrase, POS tag feature and stem feature ver-
tices are drawn in circles, dotted rectangles and
solid rectangles respectively. All edges are bi-
directional.
ture vertices. The purpose of the feature vertices,
unlike KB, is primarily for smoothing and secon-
darily for identifying paraphrases with the same
syntactic information and this will become clear
in the description of the computation of weights.
The set of all phrase vertices that are adja-
cent to s is written as F(s), and referred to
as the neighborhood of s. Let n(s, t) denote
the co-occurrence count of a phrase-table entry
(s, t) (Koehn, 2009). We define the strength of
s in the subgraph generated by cluster T as
</bodyText>
<equation confidence="0.991014">
n(s; T) = � n(s, t), (2)
tET
</equation>
<bodyText confidence="0.999672">
which is simply a partial occurrence count for s.
We proceed with computing weights for all edges
of G:
Phrase&amp;quot;phrase weights: Inspired by the
notion of preferential attachment (Yule, 1925),
which is known to produce power-law weight dis-
tributions for evolving weighted networks (Barrat
et al., 2004), we set the weight of a directed
edge from s to s&apos; to be proportional to the
strengths of s&apos; in all subgraphs in which both
s and s&apos; are members. Thus, in the random
walk framework, s is more likely to visit
a stronger (more reliable) neighbor. If Tss&apos; =
{T |s and s&apos; coexist in subgraph generated by T},
then the weight w(s → s&apos;) of the directed edge
from s to s&apos; is given by
</bodyText>
<equation confidence="0.950196333333333">
�w(s → s&apos;) = n(s&apos;; T), (3)
T ETs�s&apos;
if s&apos; ∈ F(s) and 0 otherwise.
</equation>
<bodyText confidence="0.999954076923077">
Phrase feature weights: As mentioned
above, feature vertices have the dual role of car-
rying syntactic information and smoothing. From
eqn. (3) it can be deduced that, if for a phrase
s, the amount of its outgoing weights is close to
the amount of its incoming weights, then this is
an indication that at least a significant part of its
neighborhood is reliable; the larger the strengths,
the more certain the indication. Otherwise, either
s or a significant part of its neighborhood is
unreliable. The amount of weight from s to its
feature vertices should depend on this observation
and we thus let
</bodyText>
<equation confidence="0.483528">
(w(s → s&apos;) − w(s&apos; → s)) ������ + E,
(4)
</equation>
<bodyText confidence="0.856039666666667">
where E prevents net(s) from becoming 0 (see
Section 4.1). The net weight of a phrase vertex
s is distributed over its feature vertices as
</bodyText>
<equation confidence="0.96571">
w(s → fX) =&lt; w(s → s&apos;) &gt; +net(s), (5)
</equation>
<bodyText confidence="0.999913923076923">
where the first summand is the average weight
from s to its neighboring phrase vertices and
X = POS, STEM. If s has multiple POS tag
sequences, we distribute the weight of eqn. (5)
relatively to the co-occurrences of s with the re-
spective POS tag feature vertices. The quantity
&lt; w(s → s&apos;) &gt; accounts for the basic smoothing
and is augmented by a value net(s) that measures
the reliability of s’s neighborhood; the more unre-
liable the neighborhood, the larger the net weight
and thus larger the overall weights to the feature
vertices.
The choice for the opposite direction is trivial:
</bodyText>
<equation confidence="0.9717855">
= 1
w(fX → s){s&apos; : (fX, s&apos;) is an edge }|, (6)
</equation>
<bodyText confidence="0.999412545454545">
where X = POS, STEM. Note the effect of
eqns. (4)–(6) in the case where the neighborhood
of s has unreliable strengths: In a random walk
the feature vertices of s will be preferred and the
resulting similarities between s and other phrase
vertices will be small, as desired. Nonetheless,
if the syntactic information is the same with any
other phrase vertex in G, then the paraphrases will
be captured.
The transition probability from any vertex u to
any other vertex v in G, i.e., the probability of
</bodyText>
<figure confidence="0.989650727272727">
OWNHAVE
has owns
VBZ
OWN
PRP VBP
I HAVE
i have
PRP VBD
i had
net(s) = ������ �
s&apos;Er(s)
</figure>
<page confidence="0.977435">
6
</page>
<bodyText confidence="0.896741">
hopping from u to v in one step, is given by
</bodyText>
<equation confidence="0.9340725">
w(u → v)
p(u → v) = Pv0 w(u → v0), (7)
</equation>
<bodyText confidence="0.999989777777778">
where we sum over all vertices adjacent to u in G.
We can thus compute the similarity between any
two vertices u and v in G by their commute time,
i.e., the expected number of steps in a round trip,
in a random walk from u to v and then back to u,
which is denoted by κ(u, v) (see Section 4.1 for
the method of computation of κ). Since κ(u, v) is
a distance measure, the smaller its value, the more
similar u and v are.
</bodyText>
<subsectionHeader confidence="0.993331">
3.2 Counts
</subsectionHeader>
<bodyText confidence="0.9999772">
We convert the distance κ(u, v) of a vertex pair
u, v in a graph G into a co-occurrence count
nG(u, v) with a novel technique: In order to as-
sess the quality of the pair u, v with respect to G
we compare κ(u, v) with κ(u, x) and κ(v, x) for
all other vertices x in G. We thus consider the av-
erage distance of u with the other vertices of G
other than v, and similarly for v. This quantity is
denoted by κ(u; v) and κ(v; u) respectively, and
by definition it is given by
</bodyText>
<equation confidence="0.984368666666667">
κ(i; j) = X κ(i, x)pG(x|i) (8)
x∈G
x6=j
</equation>
<bodyText confidence="0.99991">
where pG(x|i) ≡ p(x|G, i) is a yet unknown
probability distribution with respect to G. The
quantity (κ(u; v)+κ(v; u))/2 can then be viewed
as the average distance of the pair u, v to the rest
of the graph G. The co-occurrence count of u and
v in G is thus defined by
</bodyText>
<equation confidence="0.985525">
κ(u; v) + κ(v; u)
</equation>
<bodyText confidence="0.90587875">
nG(u, v) = 2κ(u, v) . (9)
In order to calculate the probabilities pG(·|·) we
employ the following heuristic: Starting with a
uniform distribution p(0)
</bodyText>
<equation confidence="0.999697625">
G (·|·) at timestep t = 0,
we iterate
κ(t)(i;j) = X κ(i, x)p(t)
G (x|i) (10)
x∈G
x6=j
κ(t)(u; v) + κ(t)(v; u)
nG (u, v) =
(t) (11)
2κ(u,v)
nG (u, v)
(t)
pG (v|u) =
(t+1) P (12)
x∈G n(t)
G (u, v)
</equation>
<bodyText confidence="0.999973222222222">
for all pairs of vertices u, v in G until conver-
gence. Experimentally, we find that convergence
is always achieved. After the execution of this it-
erative process we divide each count by the small-
est count in order to achieve a lower bound of 1.
A pair u, v may appear in multiple graphs in the
same sub-phrase-table C. The total co-occurrence
count of u and v in C and the associated condi-
tional probabilities are thus given by
</bodyText>
<equation confidence="0.9993965">
XnC(u, v) = nG(u, v) (13)
G∈C
nC(u, v)
pC(v|u) = Px∈C nC(u, x). (14)
</equation>
<bodyText confidence="0.99970325">
A pair u, v may appear in multiple sub-phrase-
tables and for the calculation of the final count
n(u, v) we need to average over the associated
counts from all sub-phrase-tables. Moreover, we
have to take into account the type of the vertices:
For the simplest case where both u and v repre-
sent phrase vertices, their expected count is, by
definition, given by
</bodyText>
<equation confidence="0.9954605">
Xn(s, s0) = nC(s, s0)p(C|s, s0). (15)
C
</equation>
<bodyText confidence="0.999948153846154">
On the other hand, if at least one of u or v is
a feature vertex, then we have to consider the
phrase vertex that generates this feature: Suppose
that u is the phrase vertex s=‘acquire’ and v the
POS tag vertex f=‘NN’ and they co-occur in two
sub-phrase-tables C and C0 with positive counts
nC(s, f) and nC0(s, f) respectively; the feature
vertex f is generated by the phrase vertices ‘own-
ership’ in C and by ‘possession’ in C0. In that
case, an interpolation of the counts nC(s, f) and
nC0(s, f) as in eqn. (15) would be incorrect and
a direct sum nC(s, f) + nC0(s, f) would provide
the true count. As a result we have
</bodyText>
<equation confidence="0.802596333333333">
n(s, f) = X X nC(s, f(s0))p(C|s, f(s0)),
s0 C
(16)
</equation>
<bodyText confidence="0.999586333333333">
where the first summation is over all phrase ver-
tices s0 such that f(s0) = f. With a similar argu-
ment we can write
</bodyText>
<equation confidence="0.986915333333333">
n(f, f0) = X X nC(f(s), f(s0))×
s,s0 C
× p(C|f(s), f(s0)). (17)
</equation>
<page confidence="0.972978">
7
</page>
<bodyText confidence="0.9753325">
For the interpolants, from standard probability we
find
</bodyText>
<equation confidence="0.987084">
p(C|u, v) = pC(v|u)p(C|u) (18)
EC, pC, (v|u)p(C&apos;|u),
</equation>
<bodyText confidence="0.9999775">
where the probabilities p(C|u) can be computed
by considering the likelihood function
</bodyText>
<equation confidence="0.983942333333333">
N N
f(u) = p(xi|u) = � pC(xi|u)p(C|u)
i=1 i=1 C
</equation>
<bodyText confidence="0.994128666666667">
and by maximizing the average log-likelihood
N 1 log f(u), where N is the total number of ver-
tices with which u co-occurs with positive counts
in all sub-phrase-tables.
Finally, the desired probability distributions are
given by the relative frequencies
</bodyText>
<equation confidence="0.994707333333333">
n(u, v)
p(v|u) = (19)
Ex n(u, x),
</equation>
<bodyText confidence="0.959752">
for all pairs of vertices u, v.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.991019">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999809461538462">
The data for building the phrase-table P
is drawn from DE-EN bitexts crawled from
www.project-syndicate.org, which is
a standard resource provider for the WMT
campaigns (News Commentary bitexts, see,
e.g. (Callison-Burch et al., 2007) ). The filtered
bitext consists of 125K sentences; word align-
ment was performed running GIZA++ in both di-
rections and generating the symmetric alignments
using the ‘grow-diag-final-and’ heuristics. The
resulting P has 7.7M entries, 30% of which are
‘1-1’, i.e. entries (s, t) that satisfy p(s|t) =
p(t|s) = 1. These entries are irrelevant for para-
phrase harvesting for both the baseline and our
method, and are thus excluded from the process.
The initial giant component P0 contains 1.7M
vertices (Figure 5), of which 30% become
residues and are used to construct R. At each it-
eration of the decomposition of a giant compo-
nent, we remove the top 0.5% · size cut-vertices
ranked by degree of connectivity, where size is
the number of vertices of the giant component and
set 0 = 2500 as the stopping criterion. The latter
choice is appropriate for the subsequent step of
co-clustering the components, for both time com-
plexity and performance of the ITC algorithm.
</bodyText>
<figureCaption confidence="0.7685712">
Figure 5: Log-log plot of ranked components ac-
cording to their size (number of source and target
phrases) for: (a) Components extracted from P.
‘1-1’ components are not shown. (b) Components
extracted from the decomposition of P0.
</figureCaption>
<bodyText confidence="0.999848290322581">
In the components emerging from the decompo-
sition of R0, we observe an excessive number
of cut-vertices. Note that vertices that consist
these components can be of two types: i) for-
mer residues, i.e., residues that emerged from the
decomposition of P0, and ii) other vertices of
P0. Cut-vertices can be of either type. For each
component, we remove cut-vertices that are not
translations of the former residues of that com-
ponent. Following this pruning strategy, the de-
generacy of excessive cut-vertices does not reap-
pear in the subsequent iterations of decompos-
ing components generated by new residues, but
the emergence of two giant components was ob-
served: One consisting mostly of source type ver-
tices and one of target type vertices. Without go-
ing into further details, the algorithm can extend
to multiple giant components straightforwardly.
For the merging process of the collection D we
set 6 = 5000, to avoid the emergence of a giant
component. The sizes of the resulting sub-phrase-
tables are shown in Figure 6. For the ITC algo-
rithm we use the smoothing technique discussed
in (Dhillon and Guan, 2003) with a = 106.
For the monolingual graphs, we set a = 0.65
and discard graphs with more than 20 phrase ver-
tices, as they contain mostly noise. Thus, the sizes
of the graphs allow us to use analytical methods
to compute the commute times: For a graph G,
we form the transition matrix P, whose entries
P(u, v) are given by eqn. (7), and the fundamen-
</bodyText>
<figure confidence="0.9613178">
107
106
105
105
P0
100
100 102 104 106
102
101
100
100 102 104 106
rank
size
104
103
8
106
105
104
103
102
101
100
100 102 104 106
rank
</figure>
<figureCaption confidence="0.730449">
Figure 6: Log-log plot of ranked sub-phrase-
tables according to their size (number of source
and target phrases).
</figureCaption>
<bodyText confidence="0.994148785714286">
the graph. Figure 8 depicts the new graph, where
the lengths of the edges represent the magnitude
of commute times. Observe that the quality of
the probabilities is preserved but the counts are
inflated, as required.
In general, if a source phrase vertex s has at
least one translation t such that n(s, t) &gt; 3, then a
triplet (is, f(is), g(is)) is added to the graph as in
Figure 8. The inflation vertex is establishes edges
with all other phrase and inflation vertices in the
graph and weights are computed as in Section 3.1.
The pipeline remains the same up to eqn. (13),
where all counts that include inflation vertices are
ignored.
</bodyText>
<figure confidence="0.980109333333333">
size
before merging
after merging
a b
f (a)
g(a)
f (b)
g(b)
n(a ,b) =2.0
n(a , f (a)) =2.6
n(a,g(a)) =2.6
n(a , f (b)) =1.3
n(a,g(b)) =1.3
p(bja) =.20
p( f (a)ja) =.27
p(g(a)ja) =.27
p( f (b)ja) =.13
p(g(b)ja) =.13
</figure>
<bodyText confidence="0.996523111111111">
tal matrix (Grinstead and Snell, 2006; Boley et al.,
2011) Z = (I −P +17rT)−1, where I is the iden-
tity matrix, 1 denotes the vector of all ones and 7r
is the vector of stationary probabilities (Aldous
and Fill, 2001) which is such that 7rT P = 7rT
and 7rT 1 = 1 and can be computed as in (Hunter,
2000). The commute time between any vertices u
and v in G is then given by (Grinstead and Snell,
2006)
</bodyText>
<equation confidence="0.9778495">
r.(u, v) = (Z(v, v) − Z(u, v))/7r(v) +
+ (Z(u, u) − Z(v, u))/7r(u). (20)
</equation>
<bodyText confidence="0.977409538461538">
For the parameter of eqn. (4), an appropriate
choice is c = |F(s) |+ 1; for reliable neighbor-
hoods, this quantity is insignificant. POS tags and
lemmata are generated with TreeTagger1.
Figure 7 depicts the most basic type of graph
that can be extracted from a cluster; it includes
two source phrase vertices a, b, of different syn-
tactic information. Suppose that both a and
b are highly reliable with strengths n(a; T) =
n(b; T) = 40, for some target cluster T. The re-
sulting conditional probabilities adequately repre-
sent the proximity of the involved vertices. On
the other hand, the range of the co-occurrence
counts is not compatible with that of the strengths.
This is because i) there are no phrase vertices with
small strength in the graph, and ii) eqn. (9) is es-
sentially a comparison between a pair of vertices
and the rest of the graph. To overcome this prob-
lem inflation vertices is and ib of strength 1 with
accompanying feature vertices are introduced to
Figure 7: Top: A graph with source phrase ver-
tices a and b, both of strength 40, with accom-
panying distinct POS sequence vertices f(·) and
stem sequence vertices g(·). Bottom: The result-
ing co-occurrence counts and conditional proba-
bilities for a.
</bodyText>
<construct confidence="0.7533282">
n(a ,b) =11.3 p(bla) =.22
n(a , f (a)) =13.5 p( f (a)la) =.26
n(a,g(a)) =13.5 p(g(a)la) =.26
n(a , f (b)) =6.7 p( f (b)la) =.13
n(a,g(b)) =6.7 p(g(b)la) =.13
</construct>
<figureCaption confidence="0.999773">
Figure 8: The inflated version of Figure 7.
</figureCaption>
<figure confidence="0.99967125">
f (a)
f (b)
a
b
g(a)
g(b)
g (ia)
g (ib)
ia
ib
f (ia)
f (ib)
</figure>
<footnote confidence="0.945918">
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
</footnote>
<page confidence="0.994071">
9
</page>
<sectionHeader confidence="0.884191" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999983">
Our method generates conditional probabilities
for any pair chosen from {phrase, POS sequence,
stem sequence}, but for this evaluation we restrict
ourselves to phrase pairs. For a phrase s, the qual-
ity of a paraphrase s0 is assessed by
</bodyText>
<equation confidence="0.502792">
P(s0|s) a p(s0|s) + p(f1(s0)|s) + p(f2(s0)|s),
(21)
</equation>
<bodyText confidence="0.9938114">
where f1(s0) and f2(s0) denote the POS tag se-
quence and stem sequence of s0 respectively. All
three summands of eqn. (21) are computed from
eqn. (19). The baseline is given by pivoting (Ban-
nard and Callison-Burch, 2005),
</bodyText>
<equation confidence="0.9968765">
P(s0|s) = � p(t|s)p(s0|t), (22)
t
</equation>
<bodyText confidence="0.970879558823529">
where p(t|s) and p(s0|t) are the phrase-based rel-
ative frequencies of the translation model.
We select 150 phrases (an equal number for
unigrams, bigrams and trigrams), for which we
expect to see paraphrases, and keep the top-10
paraphrases for each phrase, ranked by the above
measures. We follow (Kok and Brockett, 2010;
Metzler et al., 2011) in the evaluation of the ex-
tracted paraphrases: Each phrase-paraphrase pair
is manually annotated with the following options:
0) Different meaning; 1) (i) Same meaning, but
potential replacement of the phrase with the para-
phrase in a sentence ruins the grammatical struc-
ture of the sentence. (ii) Tokens of the paraphrase
are morphological inflections of the phrase’s to-
kens. 2) Same meaning. Although useful for SMT
purposes, ‘super/substrings of’ are annotated with
0 to achieve an objective evaluation.
Both methods are evaluated in terms of the
Mean Expected Precision (MEP) at k; the Ex-
pected Precision for each selected phrase s at
rank k is computed by Es[p@k] = 1 �k i�1 pi,
k
where pi is the proportion of positive annotations
for item i. The desired metric is thus given by
MEP@k = 150 Es Es[p@k]. The contribution
to pi can be restricted to perfect paraphrases only,
which leads to a strict strategy for harvesting para-
phrases. Table 1 summarizes the results of our
evaluation and
we deduce that our method can lead to improve-
ments over the baseline.
An important accomplishment of our method
is that the distribution of counts n(u, v), (as given
</bodyText>
<table confidence="0.998447">
Method Lenient MEP Strict MEP
@1 @5 @10 @1 @5 @10
Baseline .58 .47 .41 .43 .33 .28
Graphs .72 .61 .52 .53 .40 .33
</table>
<tableCaption confidence="0.999279">
Table 1: Mean Expected Precision (MEP) at k un-
</tableCaption>
<bodyText confidence="0.9553677">
der lenient and strict evaluation criteria.
by eqns. (15)–(17)) for all vertices u and v, be-
longs to the power-law family (Figure 9). This is
evidence that the monolingual graphs can simu-
late the phrase extraction process of a monolin-
gual parallel corpus. Intuitively, we may think of
the German side of the DE–EN parallel corpus as
the ‘English’ approximation to a ‘EN’–EN par-
allel corpus, and the monolingual graphs as the
word alignment process.
</bodyText>
<figure confidence="0.9557345">
105
104
103
102
101
100
100 102 104 106 108
rank
</figure>
<figureCaption confidence="0.9703535">
Figure 9: Log-log plot of ranked pairs of English
vertices according to their counts
</figureCaption>
<sectionHeader confidence="0.998374" genericHeader="conclusions">
5 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.9773845">
We have described a new method that harvests
paraphrases from a bitext, generates artificial
co-occurrence counts for any pair chosen from
{phrase, POS sequence, stem sequence}, and po-
tentially identifies patterns for the syntactic infor-
mation of the phrases. The quality of the para-
phrases’ ranked lists outperforms that of a stan-
dard baseline. The quality of the resulting condi-
tional probabilities is promising and will be eval-
uated implicitly via an application to SMT.
This research was funded by the European
Commission through the CoSyne project FP7-
ICT- 4-248531.
co−occurrence count
</bodyText>
<page confidence="0.991929">
10
</page>
<sectionHeader confidence="0.995236" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905070588236">
David Aldous and James A. Fill. 2001. Reversible
Markov Chains and Random Walks on Graphs.
http://www.stat.berkeley.edu/∼aldous/RWG/
book.html
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence
Research, 38:135–187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proc.
ACL, pp. 597–604.
Alain Barrat, Marc Barthlemy, and Alessandro Vespig-
nani. 2004. Modeling the Evolution of Weighted
Networks. Phys. Rev. Lett., 92.
Daniel Boley, Gyan Ranjan, and Zhi-Li Zhang. 2011.
Commute Times for a Directed Graph using an
Asymmetric Laplacian. Linear Algebra and its Ap-
plications, Issue 2, pp. 224–242.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora.
Proc. EMNLP, pp. 196–205.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007
(Meta-) Evaluation of Machine Translation. Proc.
Workshop on Statistical Machine Translation, pp.
136–158.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006 Improved statistical machine trans-
lation using paraphrases. Proc. HLT/NAACL, pp.
17–24.
Inderjit S. Dhillon and Yuqiang Guan. 2003. Informa-
tion Theoretic Clustering of Sparse Co-Occurrence
Data. Proc. IEEE Int’l Conf. Data Mining, pp. 517–
520.
Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-
mendra S. Modha. 2003. Information-Theoretic
Coclustering. Proc. ACM SIGKDD Int’l Conf.
Knowledge Discovery and Data Mining, pp. 89–98.
William Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised construction of large para-
phrase corpora: Exploiting massively parallel news
sources. Proc. COLING, pp. 350-356.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme 2011. Learn-
ing Sentential Paraphrases from Bilingual Paral-
lel Corpora for Text-to-Text Generation. Proc.
EMNLP, pp. 1168–1179.
Charles Grinstead and Laurie Snell. 2006. Introduc-
tion to Probability. Second ed., American Mathe-
matical Society.
Jeffrey J. Hunter. 2000. A Survey of Generalized In-
verses and their Use in Stochastic Modelling. Res.
Lett. Inf. Math. Sci., Vol. 1, pp. 25–36.
Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press, Cambridge, UK.
Stanley Kok and Chris Brockett. 2010. Hitting the
Right Paraphrases in Good Time. Proc. NAACL,
pp.145–153.
Roland Kuhn, Boxing Chen, George Foster, and Evan
Stratford. 2010. Phrase Clustering for Smoothing
TM Probabilities: or, how to Extract Paraphrases
from Phrase Tables. Proc. COLING, pp.608–616.
Nitin Madnani and Bonnie Dorr. 2010. Generating
Phrasal and Sentential Paraphrases: A Survey of
Data-Driven Methods. Computational Linguistics,
36(3):341–388.
Donald Metzler, Eduard Hovy, and Chunliang
Zhang. 2011. An Empirical Evaluation of Data-
Driven Paraphrase Generation Techniques. Proc.
ACL:Short Papers, pp. 546–551.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase Lattice for Statistical Machine
Translation. Proc. ACL:Short Papers, pp. 1–5.
Delip Rao, David Yarowsky, and Chris Callison-
Burch. 2008. Affinity Measures based on the Graph
Laplacian. Proc. Textgraphs Workshop on Graph-
based Algorithms for NLP at COLING, pp. 41–48.
George U. Yule. 1925. A Mathematical Theory of
Evolution, based on the Conclusions of Dr. J. C.
Willis, F.R.S. Philos. Trans. R. Soc. London, B 213,
pp. 21–87.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. Proc. ACL, pp.
780–788.
</reference>
<page confidence="0.999484">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.183975">
<title confidence="0.686085666666667">Power-Law Distributions for Paraphrases Extracted from Bilingual Corpora Spyros Martzoukos Christof</title>
<note confidence="0.609449">Informatics Institute, University of Science Park 904, 1098 XH Amsterdam, The</note>
<abstract confidence="0.987573">We describe a novel method that extracts paraphrases from a bitext, for both the source and target languages. In order to reduce the search space, we decompose the phrase-table into sub-phrase-tables and construct separate clusters for source and target phrases. We convert the clusters into graphs, add smoothing/syntacticinformation-carrier vertices, and compute the similarity between phrases with a ranwalk-based measure, the The resulting probabilities are built upon the conversion of the commute times into artificial cooccurrence counts with a novel technique. The co-occurrence count distribution belongs to the power-law family.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Aldous</author>
<author>James A Fill</author>
</authors>
<title>Reversible Markov Chains and Random Walks on Graphs.</title>
<date>2001</date>
<note>http://www.stat.berkeley.edu/∼aldous/RWG/ book.html</note>
<contexts>
<context position="4298" citStr="Aldous and Fill, 2001" startWordPosition="642" endWordPosition="645">12. c�2012 Association for Computational Linguistics each sub-phrase-table (Section 2.2). The underlying connectivity of the source and target clusters gives rise to a natural graph representation for each cluster (Section 3.1). The vertices of the graphs consist of phrases and features with a dual smoothing/syntacticinformation-carrier role. The latter allow (a) redistribution of the mass for phrases with no appropriate paraphrases and (b) the extraction of syntactic paraphrases. The proximity among vertices of a graph is measured by means of a random walk distance measure, the commute time (Aldous and Fill, 2001). This measure is known to perform well in identifying similar words on the graph of WordNet (Rao et al., 2008) and a related measure, the hitting time is known to perform well in harvesting paraphrases on a graph constructed from multiple phrase-tables (KB). Generally in NLP, power-law distributions are typically encountered in the collection of counts during the training stage. The distances of Section 3.1 are converted into artificial co-occurrence counts with a novel technique (Section 3.2). Although they need not be integers, the main challenge is the type of the underlying distributions;</context>
<context position="28024" citStr="Aldous and Fill, 2001" startWordPosition="4941" endWordPosition="4944">e and inflation vertices in the graph and weights are computed as in Section 3.1. The pipeline remains the same up to eqn. (13), where all counts that include inflation vertices are ignored. size before merging after merging a b f (a) g(a) f (b) g(b) n(a ,b) =2.0 n(a , f (a)) =2.6 n(a,g(a)) =2.6 n(a , f (b)) =1.3 n(a,g(b)) =1.3 p(bja) =.20 p( f (a)ja) =.27 p(g(a)ja) =.27 p( f (b)ja) =.13 p(g(b)ja) =.13 tal matrix (Grinstead and Snell, 2006; Boley et al., 2011) Z = (I −P +17rT)−1, where I is the identity matrix, 1 denotes the vector of all ones and 7r is the vector of stationary probabilities (Aldous and Fill, 2001) which is such that 7rT P = 7rT and 7rT 1 = 1 and can be computed as in (Hunter, 2000). The commute time between any vertices u and v in G is then given by (Grinstead and Snell, 2006) r.(u, v) = (Z(v, v) − Z(u, v))/7r(v) + + (Z(u, u) − Z(v, u))/7r(u). (20) For the parameter of eqn. (4), an appropriate choice is c = |F(s) |+ 1; for reliable neighborhoods, this quantity is insignificant. POS tags and lemmata are generated with TreeTagger1. Figure 7 depicts the most basic type of graph that can be extracted from a cluster; it includes two source phrase vertices a, b, of different syntactic inform</context>
</contexts>
<marker>Aldous, Fill, 2001</marker>
<rawString>David Aldous and James A. Fill. 2001. Reversible Markov Chains and Random Walks on Graphs. http://www.stat.berkeley.edu/∼aldous/RWG/ book.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A Survey of Paraphrasing and Textual Entailment Methods.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>38--135</pages>
<contexts>
<context position="1190" citStr="Androutsopoulos and Malakasiotis, 2010" startWordPosition="161" endWordPosition="165">othing/syntacticinformation-carrier vertices, and compute the similarity between phrases with a random walk-based measure, the commute time. The resulting phrase-paraphrase probabilities are built upon the conversion of the commute times into artificial cooccurrence counts with a novel technique. The co-occurrence count distribution belongs to the power-law family. 1 Introduction Paraphrase extraction has emerged as an important problem in NLP. Currently, there exists an abundance of methods for extracting paraphrases from monolingual, comparable and bilingual corpora (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010); we focus on the latter and specifically on the phrase-table that is extracted from a bitext during the training stage of Statistical Machine Translation (SMT). Bannard and Callison-Burch (2005) introduced the pivoting approach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate. By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine the similarity between phrases, which wa</context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A Survey of Paraphrasing and Textual Entailment Methods. Journal of Artificial Intelligence Research, 38:135–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<date>2005</date>
<booktitle>Paraphrasing with Bilingual Parallel Corpora. Proc. ACL,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="1385" citStr="Bannard and Callison-Burch (2005)" startWordPosition="193" endWordPosition="196"> the conversion of the commute times into artificial cooccurrence counts with a novel technique. The co-occurrence count distribution belongs to the power-law family. 1 Introduction Paraphrase extraction has emerged as an important problem in NLP. Currently, there exists an abundance of methods for extracting paraphrases from monolingual, comparable and bilingual corpora (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010); we focus on the latter and specifically on the phrase-table that is extracted from a bitext during the training stage of Statistical Machine Translation (SMT). Bannard and Callison-Burch (2005) introduced the pivoting approach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate. By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables are used. In SMT, extracted paraphrases with associated pivot-based (Callison-Burch et al., 2006; Onishi et </context>
<context position="30342" citStr="Bannard and Callison-Burch, 2005" startWordPosition="5354" endWordPosition="5358">) a b g(a) g(b) g (ia) g (ib) ia ib f (ia) f (ib) 1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ 9 4.2 Results Our method generates conditional probabilities for any pair chosen from {phrase, POS sequence, stem sequence}, but for this evaluation we restrict ourselves to phrase pairs. For a phrase s, the quality of a paraphrase s0 is assessed by P(s0|s) a p(s0|s) + p(f1(s0)|s) + p(f2(s0)|s), (21) where f1(s0) and f2(s0) denote the POS tag sequence and stem sequence of s0 respectively. All three summands of eqn. (21) are computed from eqn. (19). The baseline is given by pivoting (Bannard and Callison-Burch, 2005), P(s0|s) = � p(t|s)p(s0|t), (22) t where p(t|s) and p(s0|t) are the phrase-based relative frequencies of the translation model. We select 150 phrases (an equal number for unigrams, bigrams and trigrams), for which we expect to see paraphrases, and keep the top-10 paraphrases for each phrase, ranked by the above measures. We follow (Kok and Brockett, 2010; Metzler et al., 2011) in the evaluation of the extracted paraphrases: Each phrase-paraphrase pair is manually annotated with the following options: 0) Different meaning; 1) (i) Same meaning, but potential replacement of the phrase with the p</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. Proc. ACL, pp. 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alain Barrat</author>
<author>Marc Barthlemy</author>
<author>Alessandro Vespignani</author>
</authors>
<title>Modeling the Evolution of Weighted Networks.</title>
<date>2004</date>
<journal>Phys. Rev. Lett.,</journal>
<volume>92</volume>
<contexts>
<context position="17347" citStr="Barrat et al., 2004" startWordPosition="2955" endWordPosition="2958">ion of weights. The set of all phrase vertices that are adjacent to s is written as F(s), and referred to as the neighborhood of s. Let n(s, t) denote the co-occurrence count of a phrase-table entry (s, t) (Koehn, 2009). We define the strength of s in the subgraph generated by cluster T as n(s; T) = � n(s, t), (2) tET which is simply a partial occurrence count for s. We proceed with computing weights for all edges of G: Phrase&amp;quot;phrase weights: Inspired by the notion of preferential attachment (Yule, 1925), which is known to produce power-law weight distributions for evolving weighted networks (Barrat et al., 2004), we set the weight of a directed edge from s to s&apos; to be proportional to the strengths of s&apos; in all subgraphs in which both s and s&apos; are members. Thus, in the random walk framework, s is more likely to visit a stronger (more reliable) neighbor. If Tss&apos; = {T |s and s&apos; coexist in subgraph generated by T}, then the weight w(s → s&apos;) of the directed edge from s to s&apos; is given by �w(s → s&apos;) = n(s&apos;; T), (3) T ETs�s&apos; if s&apos; ∈ F(s) and 0 otherwise. Phrase feature weights: As mentioned above, feature vertices have the dual role of carrying syntactic information and smoothing. From eqn. (3) it can be ded</context>
</contexts>
<marker>Barrat, Barthlemy, Vespignani, 2004</marker>
<rawString>Alain Barrat, Marc Barthlemy, and Alessandro Vespignani. 2004. Modeling the Evolution of Weighted Networks. Phys. Rev. Lett., 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Boley</author>
<author>Gyan Ranjan</author>
<author>Zhi-Li Zhang</author>
</authors>
<title>Commute Times for a Directed Graph using an Asymmetric Laplacian. Linear Algebra and its</title>
<date>2011</date>
<journal>Applications, Issue</journal>
<volume>2</volume>
<pages>224--242</pages>
<contexts>
<context position="27866" citStr="Boley et al., 2011" startWordPosition="4910" endWordPosition="4913">ch that n(s, t) &gt; 3, then a triplet (is, f(is), g(is)) is added to the graph as in Figure 8. The inflation vertex is establishes edges with all other phrase and inflation vertices in the graph and weights are computed as in Section 3.1. The pipeline remains the same up to eqn. (13), where all counts that include inflation vertices are ignored. size before merging after merging a b f (a) g(a) f (b) g(b) n(a ,b) =2.0 n(a , f (a)) =2.6 n(a,g(a)) =2.6 n(a , f (b)) =1.3 n(a,g(b)) =1.3 p(bja) =.20 p( f (a)ja) =.27 p(g(a)ja) =.27 p( f (b)ja) =.13 p(g(b)ja) =.13 tal matrix (Grinstead and Snell, 2006; Boley et al., 2011) Z = (I −P +17rT)−1, where I is the identity matrix, 1 denotes the vector of all ones and 7r is the vector of stationary probabilities (Aldous and Fill, 2001) which is such that 7rT P = 7rT and 7rT 1 = 1 and can be computed as in (Hunter, 2000). The commute time between any vertices u and v in G is then given by (Grinstead and Snell, 2006) r.(u, v) = (Z(v, v) − Z(u, v))/7r(v) + + (Z(u, u) − Z(v, u))/7r(u). (20) For the parameter of eqn. (4), an appropriate choice is c = |F(s) |+ 1; for reliable neighborhoods, this quantity is insignificant. POS tags and lemmata are generated with TreeTagger1. </context>
</contexts>
<marker>Boley, Ranjan, Zhang, 2011</marker>
<rawString>Daniel Boley, Gyan Ranjan, and Zhi-Li Zhang. 2011. Commute Times for a Directed Graph using an Asymmetric Laplacian. Linear Algebra and its Applications, Issue 2, pp. 224–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic Constraints on Paraphrases Extracted from Parallel Corpora.</title>
<date>2008</date>
<booktitle>Proc. EMNLP,</booktitle>
<pages>196--205</pages>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic Constraints on Paraphrases Extracted from Parallel Corpora. Proc. EMNLP, pp. 196–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) Evaluation of Machine Translation.</title>
<date>2007</date>
<booktitle>Proc. Workshop on Statistical Machine Translation,</booktitle>
<pages>136--158</pages>
<contexts>
<context position="24056" citStr="Callison-Burch et al., 2007" startWordPosition="4239" endWordPosition="4242">n N N f(u) = p(xi|u) = � pC(xi|u)p(C|u) i=1 i=1 C and by maximizing the average log-likelihood N 1 log f(u), where N is the total number of vertices with which u co-occurs with positive counts in all sub-phrase-tables. Finally, the desired probability distributions are given by the relative frequencies n(u, v) p(v|u) = (19) Ex n(u, x), for all pairs of vertices u, v. 4 Experiments 4.1 Setup The data for building the phrase-table P is drawn from DE-EN bitexts crawled from www.project-syndicate.org, which is a standard resource provider for the WMT campaigns (News Commentary bitexts, see, e.g. (Callison-Burch et al., 2007) ). The filtered bitext consists of 125K sentences; word alignment was performed running GIZA++ in both directions and generating the symmetric alignments using the ‘grow-diag-final-and’ heuristics. The resulting P has 7.7M entries, 30% of which are ‘1-1’, i.e. entries (s, t) that satisfy p(s|t) = p(t|s) = 1. These entries are irrelevant for paraphrase harvesting for both the baseline and our method, and are thus excluded from the process. The initial giant component P0 contains 1.7M vertices (Figure 5), of which 30% become residues and are used to construct R. At each iteration of the decompo</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007 (Meta-) Evaluation of Machine Translation. Proc. Workshop on Statistical Machine Translation, pp. 136–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>Proc. HLT/NAACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1973" citStr="Callison-Burch et al., 2006" startWordPosition="282" endWordPosition="285">SMT). Bannard and Callison-Burch (2005) introduced the pivoting approach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate. By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables are used. In SMT, extracted paraphrases with associated pivot-based (Callison-Burch et al., 2006; Onishi et al., 2010) and cluster-based (Kuhn et al., 2010) probabilities have been found to improve the quality of translation. Pivoting has also been employed in the extraction of syntactic paraphrases, which are a mixture of phrases and nonterminals (Zhao et al., 2008; Ganitkevitch et al., 2011). We develop a method for extracting paraphrases from a bitext for both the source and target languages. Emphasis is placed on the quality of the phrase-paraphrase probabilities as well as on providing a stepping stone for extracting syntactic paraphrases with equally reliable probabilities. In line</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006 Improved statistical machine translation using paraphrases. Proc. HLT/NAACL, pp. 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>Yuqiang Guan</author>
</authors>
<title>Information Theoretic Clustering of Sparse Co-Occurrence Data.</title>
<date>2003</date>
<booktitle>Proc. IEEE Int’l Conf. Data Mining,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="26361" citStr="Dhillon and Guan, 2003" startWordPosition="4624" endWordPosition="4627">ve cut-vertices does not reappear in the subsequent iterations of decomposing components generated by new residues, but the emergence of two giant components was observed: One consisting mostly of source type vertices and one of target type vertices. Without going into further details, the algorithm can extend to multiple giant components straightforwardly. For the merging process of the collection D we set 6 = 5000, to avoid the emergence of a giant component. The sizes of the resulting sub-phrasetables are shown in Figure 6. For the ITC algorithm we use the smoothing technique discussed in (Dhillon and Guan, 2003) with a = 106. For the monolingual graphs, we set a = 0.65 and discard graphs with more than 20 phrase vertices, as they contain mostly noise. Thus, the sizes of the graphs allow us to use analytical methods to compute the commute times: For a graph G, we form the transition matrix P, whose entries P(u, v) are given by eqn. (7), and the fundamen107 106 105 105 P0 100 100 102 104 106 102 101 100 100 102 104 106 rank size 104 103 8 106 105 104 103 102 101 100 100 102 104 106 rank Figure 6: Log-log plot of ranked sub-phrasetables according to their size (number of source and target phrases). the </context>
</contexts>
<marker>Dhillon, Guan, 2003</marker>
<rawString>Inderjit S. Dhillon and Yuqiang Guan. 2003. Information Theoretic Clustering of Sparse Co-Occurrence Data. Proc. IEEE Int’l Conf. Data Mining, pp. 517– 520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>Subramanyam Mallela</author>
<author>Dharmendra S Modha</author>
</authors>
<title>Information-Theoretic Coclustering.</title>
<date>2003</date>
<booktitle>Proc. ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining,</booktitle>
<pages>89--98</pages>
<contexts>
<context position="11798" citStr="Dhillon et al., 2003" startWordPosition="1956" endWordPosition="1959">onent in C. Figure 1: The decomposition of G0 with vertices si and tj: The cut-vertex of the ith iteration is denoted by ci, and r collects the residues after each iteration. The task is completed in Figure 2. Figure 2: Top: Residue graph with its components (no further decomposition is required). Bottom: Adding cut-vertices back to their components. 2.2 Clustering Connected Components The aim of this subsection is to generate separate clusters for the source and target phrases of each sub-phrase-table (component) C ∈ C. For this purpose the Information-Theoretic CoClustering (ITC) algorithm (Dhillon et al., 2003) is employed, which is a general principled clustering algorithm that generates hard clusters (i.e. evt1 s1 s1 s2 s3 s4 t1 t2 t3 t4 s3 t3 s4 t4 r={t2} G0 G11 G12 c0={s2} s3 s4 t4 t3 c1={t3} s3 t4 r &lt;--r∪{s4} G12 G21 s2 t2 s2 t2 s4 t3 R s4 t3 s3 t3 t4 t1 s1 s2 s2 s3 t4 t3 +c1 s3 t4 s1 t1 +c0 +c0 t3 s3 t4 4 ery element belongs to exactly one cluster) of two interdependent quantities and is known to perform well on high-dimensional and sparse data. In our case, the interdependent quantities are the source and target phrases and the sparse data is the phrase-table. ITC is a search algorithm simila</context>
</contexts>
<marker>Dhillon, Mallela, Modha, 2003</marker>
<rawString>Inderjit S. Dhillon, Subramanyam Mallela, and Dharmendra S. Modha. 2003. Information-Theoretic Coclustering. Proc. ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining, pp. 89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>Proc. COLING,</booktitle>
<pages>350--356</pages>
<contexts>
<context position="5045" citStr="Dolan et al., 2004" startWordPosition="762" endWordPosition="765">asure, the hitting time is known to perform well in harvesting paraphrases on a graph constructed from multiple phrase-tables (KB). Generally in NLP, power-law distributions are typically encountered in the collection of counts during the training stage. The distances of Section 3.1 are converted into artificial co-occurrence counts with a novel technique (Section 3.2). Although they need not be integers, the main challenge is the type of the underlying distributions; it should ideally emulate the resulting count distributions from the phrase extraction stage of a monolingual parallel corpus (Dolan et al., 2004). These counts give rise to the desired probability distributions by means of relative frequencies. 2 Sub-phrase-tables &amp; Clustering 2.1 Extracting Connected Components For the decomposition of the phrase-table into sub-phrase-tables it is convenient to view the phrase-table as an undirected, unweighted graph P with the vertex set being the source and target phrases and the edge set being the phrase-table entries. For the rest of this section, we do not distinguish between source and target phrases, i.e. both types are treated equally as vertices of P. When referring to the size of a graph, we</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>William Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. Proc. COLING, pp. 350-356.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Juri Ganitkevitch</author>
<author>Chris Callison-Burch</author>
<author>Courtney Napoles</author>
<author>Benjamin Van</author>
</authors>
<title>Durme 2011. Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation.</title>
<booktitle>Proc. EMNLP,</booktitle>
<pages>1168--1179</pages>
<marker>Ganitkevitch, Callison-Burch, Napoles, Van, </marker>
<rawString>Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme 2011. Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation. Proc. EMNLP, pp. 1168–1179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Grinstead</author>
<author>Laurie Snell</author>
</authors>
<title>Introduction to Probability.</title>
<date>2006</date>
<editor>Second ed.,</editor>
<publisher>American Mathematical Society.</publisher>
<contexts>
<context position="27845" citStr="Grinstead and Snell, 2006" startWordPosition="4906" endWordPosition="4909"> least one translation t such that n(s, t) &gt; 3, then a triplet (is, f(is), g(is)) is added to the graph as in Figure 8. The inflation vertex is establishes edges with all other phrase and inflation vertices in the graph and weights are computed as in Section 3.1. The pipeline remains the same up to eqn. (13), where all counts that include inflation vertices are ignored. size before merging after merging a b f (a) g(a) f (b) g(b) n(a ,b) =2.0 n(a , f (a)) =2.6 n(a,g(a)) =2.6 n(a , f (b)) =1.3 n(a,g(b)) =1.3 p(bja) =.20 p( f (a)ja) =.27 p(g(a)ja) =.27 p( f (b)ja) =.13 p(g(b)ja) =.13 tal matrix (Grinstead and Snell, 2006; Boley et al., 2011) Z = (I −P +17rT)−1, where I is the identity matrix, 1 denotes the vector of all ones and 7r is the vector of stationary probabilities (Aldous and Fill, 2001) which is such that 7rT P = 7rT and 7rT 1 = 1 and can be computed as in (Hunter, 2000). The commute time between any vertices u and v in G is then given by (Grinstead and Snell, 2006) r.(u, v) = (Z(v, v) − Z(u, v))/7r(v) + + (Z(u, u) − Z(v, u))/7r(u). (20) For the parameter of eqn. (4), an appropriate choice is c = |F(s) |+ 1; for reliable neighborhoods, this quantity is insignificant. POS tags and lemmata are generat</context>
</contexts>
<marker>Grinstead, Snell, 2006</marker>
<rawString>Charles Grinstead and Laurie Snell. 2006. Introduction to Probability. Second ed., American Mathematical Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey J Hunter</author>
</authors>
<title>A Survey of Generalized Inverses and their Use in Stochastic Modelling.</title>
<date>2000</date>
<journal>Res. Lett. Inf. Math. Sci.,</journal>
<volume>1</volume>
<pages>25--36</pages>
<contexts>
<context position="28110" citStr="Hunter, 2000" startWordPosition="4964" endWordPosition="4965">remains the same up to eqn. (13), where all counts that include inflation vertices are ignored. size before merging after merging a b f (a) g(a) f (b) g(b) n(a ,b) =2.0 n(a , f (a)) =2.6 n(a,g(a)) =2.6 n(a , f (b)) =1.3 n(a,g(b)) =1.3 p(bja) =.20 p( f (a)ja) =.27 p(g(a)ja) =.27 p( f (b)ja) =.13 p(g(b)ja) =.13 tal matrix (Grinstead and Snell, 2006; Boley et al., 2011) Z = (I −P +17rT)−1, where I is the identity matrix, 1 denotes the vector of all ones and 7r is the vector of stationary probabilities (Aldous and Fill, 2001) which is such that 7rT P = 7rT and 7rT 1 = 1 and can be computed as in (Hunter, 2000). The commute time between any vertices u and v in G is then given by (Grinstead and Snell, 2006) r.(u, v) = (Z(v, v) − Z(u, v))/7r(v) + + (Z(u, u) − Z(v, u))/7r(u). (20) For the parameter of eqn. (4), an appropriate choice is c = |F(s) |+ 1; for reliable neighborhoods, this quantity is insignificant. POS tags and lemmata are generated with TreeTagger1. Figure 7 depicts the most basic type of graph that can be extracted from a cluster; it includes two source phrase vertices a, b, of different syntactic information. Suppose that both a and b are highly reliable with strengths n(a; T) = n(b; T) </context>
</contexts>
<marker>Hunter, 2000</marker>
<rawString>Jeffrey J. Hunter. 2000. A Survey of Generalized Inverses and their Use in Stochastic Modelling. Res. Lett. Inf. Math. Sci., Vol. 1, pp. 25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2009</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="16946" citStr="Koehn, 2009" startWordPosition="2888" endWordPosition="2889">i had). Phrase, POS tag feature and stem feature vertices are drawn in circles, dotted rectangles and solid rectangles respectively. All edges are bidirectional. ture vertices. The purpose of the feature vertices, unlike KB, is primarily for smoothing and secondarily for identifying paraphrases with the same syntactic information and this will become clear in the description of the computation of weights. The set of all phrase vertices that are adjacent to s is written as F(s), and referred to as the neighborhood of s. Let n(s, t) denote the co-occurrence count of a phrase-table entry (s, t) (Koehn, 2009). We define the strength of s in the subgraph generated by cluster T as n(s; T) = � n(s, t), (2) tET which is simply a partial occurrence count for s. We proceed with computing weights for all edges of G: Phrase&amp;quot;phrase weights: Inspired by the notion of preferential attachment (Yule, 1925), which is known to produce power-law weight distributions for evolving weighted networks (Barrat et al., 2004), we set the weight of a directed edge from s to s&apos; to be proportional to the strengths of s&apos; in all subgraphs in which both s and s&apos; are members. Thus, in the random walk framework, s is more likely</context>
</contexts>
<marker>Koehn, 2009</marker>
<rawString>Philipp Koehn. 2009. Statistical Machine Translation. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Chris Brockett</author>
</authors>
<title>Hitting the Right Paraphrases in Good Time.</title>
<date>2010</date>
<booktitle>Proc. NAACL,</booktitle>
<pages>145--153</pages>
<contexts>
<context position="1691" citStr="Kok and Brockett (2010)" startWordPosition="240" endWordPosition="243">hrases from monolingual, comparable and bilingual corpora (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010); we focus on the latter and specifically on the phrase-table that is extracted from a bitext during the training stage of Statistical Machine Translation (SMT). Bannard and Callison-Burch (2005) introduced the pivoting approach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate. By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables are used. In SMT, extracted paraphrases with associated pivot-based (Callison-Burch et al., 2006; Onishi et al., 2010) and cluster-based (Kuhn et al., 2010) probabilities have been found to improve the quality of translation. Pivoting has also been employed in the extraction of syntactic paraphrases, which are a mixture of phrases and nonterminals (Zhao et al., 2008; Ganitkevitch et al., 2011). We develop a met</context>
<context position="30699" citStr="Kok and Brockett, 2010" startWordPosition="5413" endWordPosition="5416">) a p(s0|s) + p(f1(s0)|s) + p(f2(s0)|s), (21) where f1(s0) and f2(s0) denote the POS tag sequence and stem sequence of s0 respectively. All three summands of eqn. (21) are computed from eqn. (19). The baseline is given by pivoting (Bannard and Callison-Burch, 2005), P(s0|s) = � p(t|s)p(s0|t), (22) t where p(t|s) and p(s0|t) are the phrase-based relative frequencies of the translation model. We select 150 phrases (an equal number for unigrams, bigrams and trigrams), for which we expect to see paraphrases, and keep the top-10 paraphrases for each phrase, ranked by the above measures. We follow (Kok and Brockett, 2010; Metzler et al., 2011) in the evaluation of the extracted paraphrases: Each phrase-paraphrase pair is manually annotated with the following options: 0) Different meaning; 1) (i) Same meaning, but potential replacement of the phrase with the paraphrase in a sentence ruins the grammatical structure of the sentence. (ii) Tokens of the paraphrase are morphological inflections of the phrase’s tokens. 2) Same meaning. Although useful for SMT purposes, ‘super/substrings of’ are annotated with 0 to achieve an objective evaluation. Both methods are evaluated in terms of the Mean Expected Precision (ME</context>
</contexts>
<marker>Kok, Brockett, 2010</marker>
<rawString>Stanley Kok and Chris Brockett. 2010. Hitting the Right Paraphrases in Good Time. Proc. NAACL, pp.145–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Boxing Chen</author>
<author>George Foster</author>
<author>Evan Stratford</author>
</authors>
<title>Phrase Clustering for Smoothing TM Probabilities: or, how to Extract Paraphrases from Phrase Tables.</title>
<date>2010</date>
<booktitle>Proc. COLING,</booktitle>
<pages>608--616</pages>
<contexts>
<context position="2033" citStr="Kuhn et al., 2010" startWordPosition="292" endWordPosition="295">ach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate. By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables are used. In SMT, extracted paraphrases with associated pivot-based (Callison-Burch et al., 2006; Onishi et al., 2010) and cluster-based (Kuhn et al., 2010) probabilities have been found to improve the quality of translation. Pivoting has also been employed in the extraction of syntactic paraphrases, which are a mixture of phrases and nonterminals (Zhao et al., 2008; Ganitkevitch et al., 2011). We develop a method for extracting paraphrases from a bitext for both the source and target languages. Emphasis is placed on the quality of the phrase-paraphrase probabilities as well as on providing a stepping stone for extracting syntactic paraphrases with equally reliable probabilities. In line with previous work, our method depends on the connectivity </context>
</contexts>
<marker>Kuhn, Chen, Foster, Stratford, 2010</marker>
<rawString>Roland Kuhn, Boxing Chen, George Foster, and Evan Stratford. 2010. Phrase Clustering for Smoothing TM Probabilities: or, how to Extract Paraphrases from Phrase Tables. Proc. COLING, pp.608–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="1149" citStr="Madnani and Dorr, 2010" startWordPosition="157" endWordPosition="160">ers into graphs, add smoothing/syntacticinformation-carrier vertices, and compute the similarity between phrases with a random walk-based measure, the commute time. The resulting phrase-paraphrase probabilities are built upon the conversion of the commute times into artificial cooccurrence counts with a novel technique. The co-occurrence count distribution belongs to the power-law family. 1 Introduction Paraphrase extraction has emerged as an important problem in NLP. Currently, there exists an abundance of methods for extracting paraphrases from monolingual, comparable and bilingual corpora (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010); we focus on the latter and specifically on the phrase-table that is extracted from a bitext during the training stage of Statistical Machine Translation (SMT). Bannard and Callison-Burch (2005) introduced the pivoting approach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate. By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie Dorr. 2010. Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods. Computational Linguistics, 36(3):341–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>Eduard Hovy</author>
<author>Chunliang Zhang</author>
</authors>
<title>An Empirical Evaluation of DataDriven Paraphrase Generation Techniques.</title>
<date>2011</date>
<booktitle>Proc. ACL:Short Papers,</booktitle>
<pages>546--551</pages>
<contexts>
<context position="30722" citStr="Metzler et al., 2011" startWordPosition="5417" endWordPosition="5420">) + p(f2(s0)|s), (21) where f1(s0) and f2(s0) denote the POS tag sequence and stem sequence of s0 respectively. All three summands of eqn. (21) are computed from eqn. (19). The baseline is given by pivoting (Bannard and Callison-Burch, 2005), P(s0|s) = � p(t|s)p(s0|t), (22) t where p(t|s) and p(s0|t) are the phrase-based relative frequencies of the translation model. We select 150 phrases (an equal number for unigrams, bigrams and trigrams), for which we expect to see paraphrases, and keep the top-10 paraphrases for each phrase, ranked by the above measures. We follow (Kok and Brockett, 2010; Metzler et al., 2011) in the evaluation of the extracted paraphrases: Each phrase-paraphrase pair is manually annotated with the following options: 0) Different meaning; 1) (i) Same meaning, but potential replacement of the phrase with the paraphrase in a sentence ruins the grammatical structure of the sentence. (ii) Tokens of the paraphrase are morphological inflections of the phrase’s tokens. 2) Same meaning. Although useful for SMT purposes, ‘super/substrings of’ are annotated with 0 to achieve an objective evaluation. Both methods are evaluated in terms of the Mean Expected Precision (MEP) at k; the Expected P</context>
</contexts>
<marker>Metzler, Hovy, Zhang, 2011</marker>
<rawString>Donald Metzler, Eduard Hovy, and Chunliang Zhang. 2011. An Empirical Evaluation of DataDriven Paraphrase Generation Techniques. Proc. ACL:Short Papers, pp. 546–551.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Onishi</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Paraphrase Lattice for Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>Proc. ACL:Short Papers,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="1995" citStr="Onishi et al., 2010" startWordPosition="286" endWordPosition="289">rch (2005) introduced the pivoting approach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate. By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables are used. In SMT, extracted paraphrases with associated pivot-based (Callison-Burch et al., 2006; Onishi et al., 2010) and cluster-based (Kuhn et al., 2010) probabilities have been found to improve the quality of translation. Pivoting has also been employed in the extraction of syntactic paraphrases, which are a mixture of phrases and nonterminals (Zhao et al., 2008; Ganitkevitch et al., 2011). We develop a method for extracting paraphrases from a bitext for both the source and target languages. Emphasis is placed on the quality of the phrase-paraphrase probabilities as well as on providing a stepping stone for extracting syntactic paraphrases with equally reliable probabilities. In line with previous work, o</context>
</contexts>
<marker>Onishi, Utiyama, Sumita, 2010</marker>
<rawString>Takashi Onishi, Masao Utiyama, and Eiichiro Sumita. 2010. Paraphrase Lattice for Statistical Machine Translation. Proc. ACL:Short Papers, pp. 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Chris CallisonBurch</author>
</authors>
<title>Affinity Measures based on the Graph Laplacian.</title>
<date>2008</date>
<booktitle>Proc. Textgraphs Workshop on Graphbased Algorithms for NLP at COLING,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="4409" citStr="Rao et al., 2008" startWordPosition="662" endWordPosition="665">y of the source and target clusters gives rise to a natural graph representation for each cluster (Section 3.1). The vertices of the graphs consist of phrases and features with a dual smoothing/syntacticinformation-carrier role. The latter allow (a) redistribution of the mass for phrases with no appropriate paraphrases and (b) the extraction of syntactic paraphrases. The proximity among vertices of a graph is measured by means of a random walk distance measure, the commute time (Aldous and Fill, 2001). This measure is known to perform well in identifying similar words on the graph of WordNet (Rao et al., 2008) and a related measure, the hitting time is known to perform well in harvesting paraphrases on a graph constructed from multiple phrase-tables (KB). Generally in NLP, power-law distributions are typically encountered in the collection of counts during the training stage. The distances of Section 3.1 are converted into artificial co-occurrence counts with a novel technique (Section 3.2). Although they need not be integers, the main challenge is the type of the underlying distributions; it should ideally emulate the resulting count distributions from the phrase extraction stage of a monolingual </context>
</contexts>
<marker>Rao, Yarowsky, CallisonBurch, 2008</marker>
<rawString>Delip Rao, David Yarowsky, and Chris CallisonBurch. 2008. Affinity Measures based on the Graph Laplacian. Proc. Textgraphs Workshop on Graphbased Algorithms for NLP at COLING, pp. 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George U Yule</author>
</authors>
<title>A Mathematical Theory of Evolution, based on the Conclusions of Dr.</title>
<date>1925</date>
<journal>J. C. Willis, F.R.S. Philos. Trans. R. Soc. London, B</journal>
<volume>213</volume>
<pages>21--87</pages>
<contexts>
<context position="17236" citStr="Yule, 1925" startWordPosition="2940" endWordPosition="2941">ases with the same syntactic information and this will become clear in the description of the computation of weights. The set of all phrase vertices that are adjacent to s is written as F(s), and referred to as the neighborhood of s. Let n(s, t) denote the co-occurrence count of a phrase-table entry (s, t) (Koehn, 2009). We define the strength of s in the subgraph generated by cluster T as n(s; T) = � n(s, t), (2) tET which is simply a partial occurrence count for s. We proceed with computing weights for all edges of G: Phrase&amp;quot;phrase weights: Inspired by the notion of preferential attachment (Yule, 1925), which is known to produce power-law weight distributions for evolving weighted networks (Barrat et al., 2004), we set the weight of a directed edge from s to s&apos; to be proportional to the strengths of s&apos; in all subgraphs in which both s and s&apos; are members. Thus, in the random walk framework, s is more likely to visit a stronger (more reliable) neighbor. If Tss&apos; = {T |s and s&apos; coexist in subgraph generated by T}, then the weight w(s → s&apos;) of the directed edge from s to s&apos; is given by �w(s → s&apos;) = n(s&apos;; T), (3) T ETs�s&apos; if s&apos; ∈ F(s) and 0 otherwise. Phrase feature weights: As mentioned above, f</context>
</contexts>
<marker>Yule, 1925</marker>
<rawString>George U. Yule. 1925. A Mathematical Theory of Evolution, based on the Conclusions of Dr. J. C. Willis, F.R.S. Philos. Trans. R. Soc. London, B 213, pp. 21–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora.</title>
<date>2008</date>
<booktitle>Proc. ACL,</booktitle>
<pages>780--788</pages>
<contexts>
<context position="2245" citStr="Zhao et al., 2008" startWordPosition="328" endWordPosition="331">xtracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables are used. In SMT, extracted paraphrases with associated pivot-based (Callison-Burch et al., 2006; Onishi et al., 2010) and cluster-based (Kuhn et al., 2010) probabilities have been found to improve the quality of translation. Pivoting has also been employed in the extraction of syntactic paraphrases, which are a mixture of phrases and nonterminals (Zhao et al., 2008; Ganitkevitch et al., 2011). We develop a method for extracting paraphrases from a bitext for both the source and target languages. Emphasis is placed on the quality of the phrase-paraphrase probabilities as well as on providing a stepping stone for extracting syntactic paraphrases with equally reliable probabilities. In line with previous work, our method depends on the connectivity of the phrase-table, but the resulting construction treats each side separately, which can potentially be benefited from additional monolingual data. The initial problem in harvesting paraphrases from a phrase-ta</context>
</contexts>
<marker>Zhao, Wang, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2008. Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora. Proc. ACL, pp. 780–788.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>