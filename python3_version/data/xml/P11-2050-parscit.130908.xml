<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000044">
<title confidence="0.991044">
Coreference for Learning to Extract Relations:
Yes, Virginia, Coreference Matters
</title>
<author confidence="0.947633">
Ryan Gabbard Marjorie Freedman Ralph Weischedel
</author>
<email confidence="0.918881">
rgabbard@bbn.com mfreedma@bbn.com weischedel@bbn.com
</email>
<note confidence="0.917532">
Raytheon BBN Technologies, 10 Moulton St., Cambridge, MA 02138
The views expressed are those of the author and do not reflect the official policy or position of the De-
partment of Defense or the U.S. Government. This is in accordance with DoDI 5230.29, January 8, 2009.
</note>
<sectionHeader confidence="0.977903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999907058823529">
As an alternative to requiring substantial su-
pervised relation training data, many have ex-
plored bootstrapping relation extraction from
a few seed examples. Most techniques assume
that the examples are based on easily spotted
anchors, e.g., names or dates. Sentences in a
corpus which contain the anchors are then
used to induce alternative ways of expressing
the relation. We explore whether coreference
can improve the learning process. That is, if
the algorithm considered examples such as his
sister, would accuracy be improved? With co-
reference, we see on average a 2-fold increase
in F-Score. Despite using potentially errorful
machine coreference, we see significant in-
crease in recall on all relations. Precision in-
creases in four cases and decreases in six.
</bodyText>
<sectionHeader confidence="0.998128" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986875">
As an alternative to requiring substantial super-
vised relation training data (e.g. the ~300k words
of detailed, exhaustive annotation in Automatic
Content Extraction (ACE) evaluations1) many have
explored bootstrapping relation extraction from a
few (~20) seed instances of a relation. Key to such
approaches is a large body of unannotated text that
can be iteratively processed as follows:
</bodyText>
<listItem confidence="0.8136355">
1. Find sentences containing the seed instances.
2. Induce patterns of context from the sentences.
3. From those patterns, find more instances.
4. Go to 2 until some condition is reached.
Most techniques assume that relation instanc-
es, like hasBirthDate(Wolfgang Amadeus Mozart,
</listItem>
<equation confidence="0.361388">
1 http://www.nist.gov/speech/tests/ace/
</equation>
<bodyText confidence="0.99883575">
1756), are realized in the corpus as relation texts2
with easily spotted anchors like Wolfgang
Amadeus Mozart was born in 1756.
In this paper we explore whether using corefer-
ence can improve the learning process. That is, if
the algorithm considered texts like his birth in
1756 for the above relation, would performance of
the learned patterns be better?
</bodyText>
<sectionHeader confidence="0.998392" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.993272083333333">
There has been much work in relation extraction
both in traditional supervised settings and, more
recently, in bootstrapped, semi-supervised settings.
To set the stage for discussing related work, we
highlight some aspects of our system. Our work
initializes learning with about 20 seed relation in-
stances and uses about 9 million documents of un-
annotated text3 as a background bootstrapping
corpus. We use both normalized syntactic structure
and surface strings as features.
Much has been published on learning relation
extractors using lots of supervised training, as in
ACE, which evaluates system performance in de-
tecting a fixed set of concepts and relations in text.
Researchers have typically used this data to incor-
porate a great deal of structural syntactic infor-
mation in their models (e.g. Ramshaw, 2001), but
the obvious weakness of these approaches is the
resulting reliance on manually annotated examples,
which are expensive and time-consuming to create.
2 Throughout we will use relation instance to refer to a fact
(e.g. ORGHasEmployee(Apple, Steve Jobs)), while we will use
relation text to refer a particular sentence entailing a relation
instance (e.g. Steve Jobs is Apple’s CEO).
</bodyText>
<listItem confidence="0.286634">
3 Wikipedia and the LDC’s Gigaword newswire corpus.
</listItem>
<page confidence="0.467083">
288
</page>
<note confidence="0.876813">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 288–293,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999874">
Others have explored automatic pattern genera-
tion from seed examples. Agichtein &amp; Gravano
(2000) and Ravichandran &amp; Hovy (2002) reported
results for generating surface patterns for relation
identification; others have explored similar ap-
proaches (e.g. Pantel &amp; Pennacchiotti, 2006).
Mitchell et al. (2009) showed that for macro-
reading, precision and recall can be improved by
learning a large set of interconnected relations and
concepts simultaneously. In all cases, the ap-
proaches used surface (word) patterns without co-
reference. In contrast, we use the structural
features of predicate-argument structure and em-
ploy coreference. Section 3 describes our particular
approach to pattern and relation instance scoring
and selection.
Another research strand (Chen et al., 2006 &amp;
Zhou et al., 2008) explores semi-supervised rela-
tion learning using the ACE corpus and assuming
manual mention markup. They measure the accu-
racy of relation extraction alone, without including
the added challenge of resolving non-specific rela-
tion arguments to name references. They limit their
studies to the small ACE corpora where mention
markup is manually encoded.
Most approaches to automatic pattern genera-
tion have focused on precision, e.g., Ravichandran
and Hovy (2002) report results in the Text Retriev-
al Conference (TREC) Question Answering track,
where extracting one text of a relation instance can
be sufficient, rather than detecting all texts. Mitch-
ell et al. (2009), while demonstrating high preci-
sion, do not measure recall.
In contrast, our study has emphasized recall. A
primary focus on precision allows one to ignore
many relation texts that require coreference or
long-distance dependencies; one primary goal of
our work is to measure system performance in ex-
actly those areas. There are at least two reasons to
not lose sight of recall. For the majority of entities
there will be only a few mentions of that entity in
even a large corpus. Furthermore, for many infor-
mation-extraction problems the number documents
at runtime will be far less than web-scale.
</bodyText>
<sectionHeader confidence="0.993461" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.888623333333333">
Figure 1 depicts our approach for learning patterns
to detect relations. At each iteration, the steps are:
(1) Given the current relation instances, find possi-
ble texts that entail the relation by finding sentenc-
es in the corpus containing all arguments of an in-
stance.
</bodyText>
<listItem confidence="0.968051136363636">
(2) As in Freedman et al. (2010) and Boschee et
al. (2008), induce possible patterns using the con-
text in which the arguments appear. Patterns in-
clude both surface strings and normalized syntactic
structures.4 Each proposed pattern is applied to the
corpus to find a set of hypothesized texts. For each
pattern, a confidence score is assigned using esti-
mated precision5 and recall. The highest confi-
dence patterns are added to the pattern set.6
(3) The patterns are applied to the corpus to find
additional possible relation instances. For each
proposed instance, we estimate a score using a Na-
ive Bayes model with the patterns as the features.
When using coreference, this score is penalized if
an instance’s supporting evidence involves low-
confidence coreference links. The highest scoring
instances are added to the instance set.
(4) After the desired number of iterations (in these
experiments, 20) is complete, a human reviews the
resulting pattern set and removes those patterns
which are clearly incorrect (e.g. ‘X visited Y’ for
hasBirthPlace).7
</listItem>
<figureCaption confidence="0.997087">
Figure 1: Approach to learning relations
</figureCaption>
<bodyText confidence="0.972116266666667">
We ran this system in two versions: –Coref has
no access to coreference information, while +Coref
(the original system) does. The systems are other-
wise identical. Coreference information is provided
by BBN’s state-of-the-art information extraction
4 Surface text patterns with wild cards are not proposed until
the third iteration.
5 Estimated recall is the weighted fraction of known instances
found. Estimated precision is the weighted average of the
scores of matched instances; scores for unseen instances are 0.
6 As more patterns are accepted in a given iteration, we raise
the confidence threshold. Usually, ~10 patterns are accepted
per iteration.
7 This takes about ten minutes per relation, which is less than
the time to choose the initial seed instances.
</bodyText>
<figure confidence="0.95695535">
prune
and add
INVENTOR
retrieve from
corpus
pattern
database
iobj
granted
proposed
patterns
patent
obj
proposed
pairs
INVENTION
for
retrieve from corpus
proposed
instances
</figure>
<bodyText confidence="0.8695188">
Edison invented the light bulb
Bell built the first telephone
Edison was granted a U.S. patent
for the light bulb
Franklin invented the lightning rod
</bodyText>
<figure confidence="0.894443142857143">
induce
Thomas Edison ... light bulb
Alexander G. Bell ... telephone
Ben Franklin ... lightning rod
example pairs
instances
289
</figure>
<bodyText confidence="0.983640690140845">
system (Ramshaw, et al., 2011; NIST, 2007) in a Ethel Kennedy says that when the family gathered
mode which sacrifices some accuracy for speed for Thanksgiving she wanted the children to know
(most notably by reducing the parser’s search what a real turkey looked like. So she sent her son,
space). The IE system processes over 50MB/hour Robert F. Kennedy Jr., to a farm to buy two birds.
with an average EDR Value score when evaluated Figure 2: Passage entailing hasChild relation
on an 8-fold cross-validation of the ACE 2007.
+Coref can propose relation instances from text
in which the arguments are expressed as either
name or non-name mentions. When the text of an
argument of a proposed instance is a non-name, the
system uses coreference to resolve the non-name to
a name. -Coref can only propose instances based
on texts where both arguments are names.8
This has several implications: If a text that en-
tails a relation instance expresses one of the argu-
ments as a non-name mention (e.g. “Sue’s husband
is here.”), -Coref will be unable to learn an in-
stance from that text. Even when all arguments are
expressed as names, -Coref may need to use more
specific, complex patterns to learn the instance
(e.g. “Sue asked her son, Bob, to set the table”).
We expect the ability to run using a ‘denser,’ more
local space of patterns to be a significant advantage
of +Coref. Certain types of patterns (e.g. patterns
involving possessives) may also be less likely to be
learned by -Coref. Finally, +Coref has access to
much more training data at the outset because it
can find more matching seed instances,9 potentially
leading to better and more stable training.
Here we extend this idea to both precision and re-
call in a micro-reading context.
Precision is measured by running the system
over the background corpus and randomly sam-
pleing 100 texts that the system believes entail
each relation. From the mentions matching the ar-
gument slots of the patterns, we build a relation
instance. If these mentions are not names (only
possible for +Coref), they are resolved to names
using system coreference. For example, given the
passage in Figure 2 and the pattern ‘(Y, poss:X)’,
the system would match the mentions X=her and
Y=son, and build the relation instance
hasChild(Ethel Kennedy, Robert F. Kennedy Jr.).
During assessment, the annotator is asked
whether, in the context of the whole document, a
given sentence entails the relation instance. We
thus treat both incorrect relation extraction and
incorrect reference resolution as mistakes.
To measure recall, we select 20 test relation in-
stances and search the corpus for sentences con-
taining all arguments of a test instance (explicitly
or via coreference). We randomly sampled from
this set, choosing at most 10 sentences for each test
instance, to form a collection of at most 200 sen-
tences likely to be texts expressing the desired rela-
tion. These sentences were then manually
annotated in the same manner as the precision an-
notation. Sentences that did not correctly convey
the relation instance were removed, and the re-
maining set of sentences formed a recall set. We
consider a recall set instance to be found by a sys-
tem if the system finds a relation of the correct
type in the sentence. We intentionally chose to
sample 10 sentences from each test example, rather
than sampling from the set of all sentences found.
This prevents one or two very commonly ex-
pressed instances from dominating the recall set.
As a result, the recall test set is biased away from
“true” recall, because it places a higher weight on
the “long tail” of instances. However, this gives a
more accurate indication of the system’s ability to
</bodyText>
<table confidence="0.903343181818182">
find novel instances of a relation.
4 Evaluation Framework
Estimating recall for bootstrapped relation learning
is a challenge except for corpora small enough for
complete annotation to be feasible, e.g., the ACE
corpora. ACE typically had a test set of ~30,000
words and ~300k for training. Yet, with a small
corpus, rare relations will be inadequately repre-
sented.10 Macro-reading evaluations (e.g. Mitchell,
2009) have not estimated recall, but have measured
precision by sampling system output and determin-
ing whether the extracted fact is true in the world.
8 An instance like hasChild(his father, he) would be useful
neither during training nor (without coreference) at runtime.
9 An average of 12,583 matches versus 2,256 matches. If mul-
tiple mentions expressing an argument occur in one sentence,
each match is counted, inflating the difference.
10 Despite being selected to be rich in the 18 ACE relation
subtypes, the 10 most frequent subtypes account for over 90%
of the relations with the 4 most frequent accounting for 62%;
the 5 least frequent relation subtypes occur less than 50 times.
290
</table>
<sectionHeader confidence="0.991232" genericHeader="method">
5 Empirical Results
</sectionHeader>
<bodyText confidence="0.9993766">
Table 1 gives results for precision, recall, and F
for +Coref (+) and –Coref (-). In all cases remov-
ing coreference causes a drop in recall, ranging
from only 33%(hasBirthPlace) to over 90%
(GPEEmploys). The median drop is 68%.
</bodyText>
<table confidence="0.998616909090909">
P+ P- R+ R- R* F+ F-
attendSchool (1) 83 97 49 16 27 62 27
GPEEmploy(2) 91 96 29 3 3 44 5
GPELeader (3) 87 99 48 28 30 62 43
hasBirthPlace (4) 87 97 57 37 53 69 53
hasChild (5) 70 60 37 17 11 48 27
hasSibling (6) 73 69 67 17 17 70 28
hasSpouse (7) 61 96 72 22 31 68 36
ORGEmploys(8) 92 82 22 4 7 35 7
ORGLeader (9) 88 97 73 32 42 80 48
hasBirthDate (10) 90 85 45 13 32 60 23
</table>
<tableCaption confidence="0.999818">
Table 1: Precision, Recall, and F scores
</tableCaption>
<subsectionHeader confidence="0.960711">
5.1 Recall
</subsectionHeader>
<bodyText confidence="0.999924653846154">
There are two potential sources of –Coref’s
lower recall. For some relation instances, the text
will contain only non-named instances, and as a
result -Coref will be unable to find the instance.
-Coref is also at a disadvantage while learning,
since it has access to fewer texts during bootstrap-
ping. Figure 311 presents the fraction of instances
in the recall test set for which both argument
names appear in the sentence. Even with perfect
patterns, -Coref has no opportunity to find roughly
25% of the relation texts because at least one ar-
gument is not expressed as a name.
To further understand -Coref’s lower perfor-
mance, we created a third system, *Coref, which
used coreference at runtime but not during train-
ing.12 In a few cases, such as hasBirthPlace,
*Coref is able to almost match the recall of the
system that used coreference during learning
(+Coref), but on average the lack of coreference at
runtime accounts for only about 25% of the differ-
ence, with the rest accounted for by differences in
the pattern sets learned.
Figure 4 shows the distribution of argument
mention types for +Coref on the recall set. Com-
paring this to Figure 3, we see that +Coref uses
name-name pairs far less often than it could (less
</bodyText>
<figure confidence="0.561114">
11 Figures 3 &amp; 4 do not include hasBirthDate: There is only 1
potential named argument for this relation, the other is a date.
12 *Coref was added after reading paper reviews, so there was
not time to do annotation for a precision evaluation for it.
</figure>
<figureCaption confidence="0.995534">
Figure 3: Fraction of recall instances with name
mentions present in the sentence for both arguments.
</figureCaption>
<bodyText confidence="0.9990748">
than 50% of the time overall). Instead, even when
two names are present in a sentence that entails the
relation, +Coref chooses to find the relation in
name-descriptor and name-pronoun contexts which
are often more locally related in the sentences.
</bodyText>
<figureCaption confidence="0.9918345">
Figure 4: Distribution of argument mention types for
+Coref matches on the recall set
</figureCaption>
<bodyText confidence="0.999943714285714">
For the two cases with the largest drops in re-
call, ORGEmploys and GPEEmploys, +Coref and –
Coref have very different trajectories during train-
ing. For example, in the first iteration, –Coref
learns patterns involving director, president, and
head for ORGEmploys, while +Coref learns pat-
terns involving joined and hired. We speculate
that –Coref may become stuck because the most
frequent name-name constructions, e.g. ORG/GPE
title PERSON (e.g. Brazilian President Lula da
Silva), are typically used to introduce top officials.
For such cases, even without co-reference, system
specific effort and tuning could potentially have
improved –Coref’s ability to learn the relations.
</bodyText>
<subsectionHeader confidence="0.998725">
5.2 Precision
</subsectionHeader>
<bodyText confidence="0.9996058">
Results on precision are mixed. While for 4 of
the relations +Coref is higher, for the 6 others the
addition of coreference reduces precision. The av-
erage precisions for +Coref and –Coref are 82.2
and 87.8, and the F-score of +Coref exceeded that
</bodyText>
<figure confidence="0.982157703703704">
1 2 3 4 5 6 7 8 9
1.00
0.90
0.80
0.70
% Recall Instances
0.60
0.50
0.40
0.30
0.20
0.10
0.00
100%
40%
80%
60%
20%
0%
1 2 3 4 5 6 7 8 9
Other
Combi nati ons
Both Desc
Name &amp; Pron
Name &amp; Desc
Both Na me
291
</figure>
<bodyText confidence="0.99765275">
of –Coref for all relations. Thus while +Coref pays
a price in precision for its improved recall, in many
applications it may be a worthwhile tradeoff.
Though one might expect that errors in coref-
erence would reduce precision of +Coref, such er-
rors may be balanced by the need to use longer
patterns in –Coref. These patterns often include
error-prone wildcards which lead to a drop in pre-
cision. Patterns with multiple wildcards were also
more likely to be removed as unreliable in manual
pattern pruning, which may have harmed the recall
of –Coref, while improving its precision.
</bodyText>
<subsectionHeader confidence="0.998371">
5.3 Further Analysis
</subsectionHeader>
<bodyText confidence="0.999981">
Our analysis thus far has focused on micro-
reading which requires a system find all mentions
of an instance relation – i,e, in our evaluation Or-
gLeader(Apple, Steve Jobs) might occur in as
many as 20 different contexts. While –Coref per-
forms poorly at micro-reading, it could still be ef-
fective for macro-reading, i.e. finding at least one
instance of the relation OrgLeader(Apple, Steve
Jobs). As a rough measure of this, we also evaluat-
ed recall by counting the number of test instances
for which at least one answer was found by the two
systems. With this method, +Coref’s recall is still
higher for all but one relation type, although the
gap between the systems narrows somewhat.
</bodyText>
<table confidence="0.999585">
+Coref -Coref #Test
Instances
ORGEmploys 8 2 20
GPEEmploys 12 3 19
hasSibling 11 4 19
hasBirthDate 12 5 17
hasSpouse 15 9 20
ORGLeader 14 9 19
attendedSchool 17 12 20
hasBirthPlace 19 15 20
GPELeader 15 13 19
hasChild 6 6 19
</table>
<tableCaption confidence="0.9932155">
Table 2: Number of test seeds where at least one
instance is found in the evaluation.
</tableCaption>
<bodyText confidence="0.999297142857143">
In addition to our recall evaluation, we meas-
ured the number of sentences containing relation
instances found by each of the systems when ap-
plied to 5,000 documents (see Table 3). For al-
most all relations, +Coref matches many more
sentences, including finding more sentences for
those relations for which it has higher precision.
</bodyText>
<table confidence="0.997632166666667">
Prec Number of Sentences
Relation P+ P- +Cnt -Cnt *Cnt
attendedSchool 83 97 541 212 544
hasChild 91 96 661 68 106
hasSpouse 87 99 1262 157 282
hasSibling 87 97 313 72 272
GPEEmploys 70 60 1208 308 313
GPELeader 73 69 1018 629 644
ORGEmploys 61 96 1698 142 209
ORGLeader 92 82 1095 207 286
hasBirthDate 88 97 231 131 182
hasBirthPlace 90 85 836 388 558
</table>
<tableCaption confidence="0.987336">
Table 3: Number of sentences in which each system
found relation instances
</tableCaption>
<bodyText confidence="0.99992452173913">
Our experiments suggest that in contexts where
recall is important incorporating coreference into a
relation extraction system may provide significant
gains. Despite being noisy, coreference infor-
mation improved F-scores for all relations in our
test, more than doubling the F-score for 5 of the
10.
Why is the high error rate of coreference not
very harmful to +Coref? We speculate that there
are two reasons. First, during training, not all co-
reference is treated equally. If the only evidence
we have for a proposed instance depends on low
confidence coreference links, it is very unlikely to
be added to our instance set for use in future itera-
tions. Second, for both training and runtime, many
of the coreference links relevant for extracting the
relation set examined here are fairly reliable, such
as wh-words in relative clauses.
There is room for more investigation of the
question, however. It is also unclear if the same
result would hold for a very different set of rela-
tions, especially those which are more event-like
than relation-like.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999935777777778">
This work was supported, in part, by DARPA un-
der AFRL Contract FA8750-09-C-179. The views
expressed are those of the authors and do not re-
flect the official policy or position of the Depart-
ment of Defense or the U.S. Government. We
would like to thank our reviewers for their helpful
comments and Martha Friedman, Michael Heller,
Elizabeth Roman, and Lorna Sigourney for doing
our evaluation annotation.
</bodyText>
<sectionHeader confidence="0.993894" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<page confidence="0.790628">
292
</page>
<sectionHeader confidence="0.994628" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999731044117647">
E. Agichtein and L. Gravano. 2000. Snowball: extract-
ing relations from large plain-text collections. In
Proceedings of the ACM Conference on Digital Li-
braries, pp. 85-94.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open Information Extraction
from the Web. In Proceedings of the International
Joint Conference on Artificial Intelligence.
A. Baron and M. Freedman. 2008. Who is Who and
What is What: Experiments in Cross Document Co-
Reference. In Empirical Methods in Natural Lan-
guage Processing.
A. Blum and T. Mitchell. 1998. Combining Labeled and
Unlabeled Data with Co-Training. In Proceedings of
the 1998 Conference on Computational Learning
Theory.
E. Boschee, V. Punyakanok, R. Weischedel. 2008. An
Exploratory Study Towards ‘Machines that Learn to
Read’. Proceedings of AAAI BICA Fall Symposium.
J. Chen, D. Ji, C. Tan and Z. Niu. 2006. Relation extrac-
tion using label propagation based semi-supervised
learning. COLING-ACL 2006: 129-136.
T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and
R. Wang. 2009. Populating the Semantic Web by
Macro-Reading Internet Text. Invited paper, Pro-
ceedings of the 8th International Semantic Web Con-
ference (ISWC 2009).
National Institute of Standards and Technology. 2007.
NIST 2007 Automatic Content Extraction Evaluation
Official Results. http://www.itl.nist.gov/iad/mig/
tests/ace/2007/doc/ace07_eval_official_results
20070402.html
_
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging Generic Patterns for Automatically Harvesting
Semantic Relations. In Proceedings of Conference on
Computational Linguistics / Association for Compu-
tational Linguistics (COLING/ACL-06). pp. 113-120.
Sydney, Australia.
L. Ramshaw, E. Boschee, S. Bratus, S. Miller, R. Stone,
R. Weischedel, A. Zamanian. 2001. Experiments in
multi-modal automatic content extraction, In Pro-
ceedings of Human Language Technology Confer-
ence.
L. Ramshaw, E. Boschee, M. Freedman, J. MacBride,
R. Weischedel, A. Zamanian. 2011. SERIF Language
Processing – Efficient Trainable Language Under-
standing. In Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global
Autonomous Language Exploitation. Springer.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2002),
pages 41–47, Philadelphia, PA.
E. Riloff. 1996. Automatically generating extraction
patterns from untagged text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044-1049.
G. Zhou, J. Li, L. Qian, Q. Zhu. 2008. Semi-Supervised
Learning for Relation Extraction. Proceedings of the
Third International Joint Conference on Natural
Language Processing: Volume-I.
Z. Kozareva and E. Hovy. Not All Seeds Are Equal:
Measuring the Quality of Text Mining Seeds. 2010.
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics pp. 618-626.
</reference>
<page confidence="0.958222">
293
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.557589">
<title confidence="0.994412">Coreference for Learning to Extract Yes, Virginia, Coreference Matters</title>
<author confidence="0.999662">Ryan Gabbard Marjorie Freedman Ralph Weischedel</author>
<email confidence="0.995928">rgabbard@bbn.commfreedma@bbn.comweischedel@bbn.com</email>
<note confidence="0.830022">Raytheon BBN Technologies, 10 Moulton St., Cambridge, MA 02138 The views expressed are those of the author and do not reflect the official policy or position of the Department of Defense or the U.S. Government. This is in accordance with DoDI 5230.29, January 8, 2009.</note>
<abstract confidence="0.997771833333333">As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if algorithm considered examples such as would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACM Conference on Digital Libraries,</booktitle>
<pages>85--94</pages>
<contexts>
<context position="3882" citStr="Agichtein &amp; Gravano (2000)" startWordPosition="581" endWordPosition="584">e expensive and time-consuming to create. 2 Throughout we will use relation instance to refer to a fact (e.g. ORGHasEmployee(Apple, Steve Jobs)), while we will use relation text to refer a particular sentence entailing a relation instance (e.g. Steve Jobs is Apple’s CEO). 3 Wikipedia and the LDC’s Gigaword newswire corpus. 288 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 288–293, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Others have explored automatic pattern generation from seed examples. Agichtein &amp; Gravano (2000) and Ravichandran &amp; Hovy (2002) reported results for generating surface patterns for relation identification; others have explored similar approaches (e.g. Pantel &amp; Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macroreading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. In all cases, the approaches used surface (word) patterns without coreference. In contrast, we use the structural features of predicate-argument structure and employ coreference. Section 3 describes our particular approach to pattern and rela</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>E. Agichtein and L. Gravano. 2000. Snowball: extracting relations from large plain-text collections. In Proceedings of the ACM Conference on Digital Libraries, pp. 85-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open Information Extraction from the Web.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open Information Extraction from the Web. In Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Baron</author>
<author>M Freedman</author>
</authors>
<title>Who is Who and What is What: Experiments in Cross Document CoReference.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<marker>Baron, Freedman, 2008</marker>
<rawString>A. Baron and M. Freedman. 2008. Who is Who and What is What: Experiments in Cross Document CoReference. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 Conference on Computational Learning Theory.</booktitle>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the 1998 Conference on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Boschee</author>
<author>V Punyakanok</author>
<author>R Weischedel</author>
</authors>
<title>An Exploratory Study Towards ‘Machines that Learn to Read’.</title>
<date>2008</date>
<booktitle>Proceedings of AAAI BICA Fall Symposium.</booktitle>
<contexts>
<context position="6187" citStr="Boschee et al. (2008)" startWordPosition="944" endWordPosition="947"> There are at least two reasons to not lose sight of recall. For the majority of entities there will be only a few mentions of that entity in even a large corpus. Furthermore, for many information-extraction problems the number documents at runtime will be far less than web-scale. 3 Approach Figure 1 depicts our approach for learning patterns to detect relations. At each iteration, the steps are: (1) Given the current relation instances, find possible texts that entail the relation by finding sentences in the corpus containing all arguments of an instance. (2) As in Freedman et al. (2010) and Boschee et al. (2008), induce possible patterns using the context in which the arguments appear. Patterns include both surface strings and normalized syntactic structures.4 Each proposed pattern is applied to the corpus to find a set of hypothesized texts. For each pattern, a confidence score is assigned using estimated precision5 and recall. The highest confidence patterns are added to the pattern set.6 (3) The patterns are applied to the corpus to find additional possible relation instances. For each proposed instance, we estimate a score using a Naive Bayes model with the patterns as the features. When using co</context>
</contexts>
<marker>Boschee, Punyakanok, Weischedel, 2008</marker>
<rawString>E. Boschee, V. Punyakanok, R. Weischedel. 2008. An Exploratory Study Towards ‘Machines that Learn to Read’. Proceedings of AAAI BICA Fall Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>D Ji</author>
<author>C Tan</author>
<author>Z Niu</author>
</authors>
<title>Relation extraction using label propagation based semi-supervised learning. COLING-ACL</title>
<date>2006</date>
<pages>129--136</pages>
<contexts>
<context position="4561" citStr="Chen et al., 2006" startWordPosition="680" endWordPosition="683">g surface patterns for relation identification; others have explored similar approaches (e.g. Pantel &amp; Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macroreading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. In all cases, the approaches used surface (word) patterns without coreference. In contrast, we use the structural features of predicate-argument structure and employ coreference. Section 3 describes our particular approach to pattern and relation instance scoring and selection. Another research strand (Chen et al., 2006 &amp; Zhou et al., 2008) explores semi-supervised relation learning using the ACE corpus and assuming manual mention markup. They measure the accuracy of relation extraction alone, without including the added challenge of resolving non-specific relation arguments to name references. They limit their studies to the small ACE corpora where mention markup is manually encoded. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relat</context>
</contexts>
<marker>Chen, Ji, Tan, Niu, 2006</marker>
<rawString>J. Chen, D. Ji, C. Tan and Z. Niu. 2006. Relation extraction using label propagation based semi-supervised learning. COLING-ACL 2006: 129-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
<author>J Betteridge</author>
<author>A Carlson</author>
<author>E Hruschka</author>
<author>R Wang</author>
</authors>
<title>Populating the Semantic Web by Macro-Reading Internet Text. Invited paper,</title>
<date>2009</date>
<booktitle>Proceedings of the 8th International Semantic Web Conference (ISWC</booktitle>
<contexts>
<context position="4091" citStr="Mitchell et al. (2009)" startWordPosition="610" endWordPosition="613">iling a relation instance (e.g. Steve Jobs is Apple’s CEO). 3 Wikipedia and the LDC’s Gigaword newswire corpus. 288 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 288–293, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Others have explored automatic pattern generation from seed examples. Agichtein &amp; Gravano (2000) and Ravichandran &amp; Hovy (2002) reported results for generating surface patterns for relation identification; others have explored similar approaches (e.g. Pantel &amp; Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macroreading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. In all cases, the approaches used surface (word) patterns without coreference. In contrast, we use the structural features of predicate-argument structure and employ coreference. Section 3 describes our particular approach to pattern and relation instance scoring and selection. Another research strand (Chen et al., 2006 &amp; Zhou et al., 2008) explores semi-supervised relation learning using the ACE corpus and assuming manual mention markup. They mea</context>
</contexts>
<marker>Mitchell, Betteridge, Carlson, Hruschka, Wang, 2009</marker>
<rawString>T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and R. Wang. 2009. Populating the Semantic Web by Macro-Reading Internet Text. Invited paper, Proceedings of the 8th International Semantic Web Conference (ISWC 2009).</rawString>
</citation>
<citation valid="true">
<title>of Standards and Technology.</title>
<date>2007</date>
<publisher>NIST</publisher>
<institution>National Institute</institution>
<note>http://www.itl.nist.gov/iad/mig/ tests/ace/2007/doc/ace07_eval_official_results</note>
<marker>2007</marker>
<rawString>National Institute of Standards and Technology. 2007. NIST 2007 Automatic Content Extraction Evaluation Official Results. http://www.itl.nist.gov/iad/mig/ tests/ace/2007/doc/ace07_eval_official_results</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations.</title>
<date>2006</date>
<booktitle>In Proceedings of Conference on Computational Linguistics / Association for Computational Linguistics (COLING/ACL-06).</booktitle>
<pages>113--120</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="4067" citStr="Pantel &amp; Pennacchiotti, 2006" startWordPosition="606" endWordPosition="609">efer a particular sentence entailing a relation instance (e.g. Steve Jobs is Apple’s CEO). 3 Wikipedia and the LDC’s Gigaword newswire corpus. 288 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 288–293, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Others have explored automatic pattern generation from seed examples. Agichtein &amp; Gravano (2000) and Ravichandran &amp; Hovy (2002) reported results for generating surface patterns for relation identification; others have explored similar approaches (e.g. Pantel &amp; Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macroreading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. In all cases, the approaches used surface (word) patterns without coreference. In contrast, we use the structural features of predicate-argument structure and employ coreference. Section 3 describes our particular approach to pattern and relation instance scoring and selection. Another research strand (Chen et al., 2006 &amp; Zhou et al., 2008) explores semi-supervised relation learning using the ACE corpus and assuming manual </context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations. In Proceedings of Conference on Computational Linguistics / Association for Computational Linguistics (COLING/ACL-06). pp. 113-120. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>E Boschee</author>
<author>S Bratus</author>
<author>S Miller</author>
<author>R Stone</author>
<author>R Weischedel</author>
<author>A Zamanian</author>
</authors>
<title>Experiments in multi-modal automatic content extraction,</title>
<date>2001</date>
<booktitle>In Proceedings of Human Language Technology Conference.</booktitle>
<marker>Ramshaw, Boschee, Bratus, Miller, Stone, Weischedel, Zamanian, 2001</marker>
<rawString>L. Ramshaw, E. Boschee, S. Bratus, S. Miller, R. Stone, R. Weischedel, A. Zamanian. 2001. Experiments in multi-modal automatic content extraction, In Proceedings of Human Language Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>E Boschee</author>
<author>M Freedman</author>
<author>J MacBride</author>
<author>R Weischedel</author>
<author>A Zamanian</author>
</authors>
<title>SERIF Language Processing – Efficient Trainable Language Understanding.</title>
<date>2011</date>
<booktitle>In Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="8469" citStr="Ramshaw, et al., 2011" startWordPosition="1301" endWordPosition="1304">rns are accepted per iteration. 7 This takes about ten minutes per relation, which is less than the time to choose the initial seed instances. prune and add INVENTOR retrieve from corpus pattern database iobj granted proposed patterns patent obj proposed pairs INVENTION for retrieve from corpus proposed instances Edison invented the light bulb Bell built the first telephone Edison was granted a U.S. patent for the light bulb Franklin invented the lightning rod induce Thomas Edison ... light bulb Alexander G. Bell ... telephone Ben Franklin ... lightning rod example pairs instances 289 system (Ramshaw, et al., 2011; NIST, 2007) in a Ethel Kennedy says that when the family gathered mode which sacrifices some accuracy for speed for Thanksgiving she wanted the children to know (most notably by reducing the parser’s search what a real turkey looked like. So she sent her son, space). The IE system processes over 50MB/hour Robert F. Kennedy Jr., to a farm to buy two birds. with an average EDR Value score when evaluated Figure 2: Passage entailing hasChild relation on an 8-fold cross-validation of the ACE 2007. +Coref can propose relation instances from text in which the arguments are expressed as either name </context>
</contexts>
<marker>Ramshaw, Boschee, Freedman, MacBride, Weischedel, Zamanian, 2011</marker>
<rawString>L. Ramshaw, E. Boschee, M. Freedman, J. MacBride, R. Weischedel, A. Zamanian. 2011. SERIF Language Processing – Efficient Trainable Language Understanding. In Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>41--47</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5043" citStr="Ravichandran and Hovy (2002)" startWordPosition="753" endWordPosition="756">ce. Section 3 describes our particular approach to pattern and relation instance scoring and selection. Another research strand (Chen et al., 2006 &amp; Zhou et al., 2008) explores semi-supervised relation learning using the ACE corpus and assuming manual mention markup. They measure the accuracy of relation extraction alone, without including the added challenge of resolving non-specific relation arguments to name references. They limit their studies to the small ACE corpora where mention markup is manually encoded. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. Mitchell et al. (2009), while demonstrating high precision, do not measure recall. In contrast, our study has emphasized recall. A primary focus on precision allows one to ignore many relation texts that require coreference or long-distance dependencies; one primary goal of our work is to measure system performance in exactly those areas. There are at least two reasons to not lose sight of recall. For the majority</context>
<context position="3913" citStr="Ravichandran &amp; Hovy (2002)" startWordPosition="586" endWordPosition="589">to create. 2 Throughout we will use relation instance to refer to a fact (e.g. ORGHasEmployee(Apple, Steve Jobs)), while we will use relation text to refer a particular sentence entailing a relation instance (e.g. Steve Jobs is Apple’s CEO). 3 Wikipedia and the LDC’s Gigaword newswire corpus. 288 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 288–293, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Others have explored automatic pattern generation from seed examples. Agichtein &amp; Gravano (2000) and Ravichandran &amp; Hovy (2002) reported results for generating surface patterns for relation identification; others have explored similar approaches (e.g. Pantel &amp; Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macroreading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. In all cases, the approaches used surface (word) patterns without coreference. In contrast, we use the structural features of predicate-argument structure and employ coreference. Section 3 describes our particular approach to pattern and relation instance scoring and selec</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E. Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 41–47, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1044-1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Li</author>
<author>L Qian</author>
<author>Q Zhu</author>
</authors>
<title>Semi-Supervised Learning for Relation Extraction.</title>
<date>2008</date>
<booktitle>Proceedings of the Third International Joint Conference on Natural Language Processing:</booktitle>
<publisher>Volume-I.</publisher>
<contexts>
<context position="4582" citStr="Zhou et al., 2008" startWordPosition="685" endWordPosition="688">or relation identification; others have explored similar approaches (e.g. Pantel &amp; Pennacchiotti, 2006). Mitchell et al. (2009) showed that for macroreading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. In all cases, the approaches used surface (word) patterns without coreference. In contrast, we use the structural features of predicate-argument structure and employ coreference. Section 3 describes our particular approach to pattern and relation instance scoring and selection. Another research strand (Chen et al., 2006 &amp; Zhou et al., 2008) explores semi-supervised relation learning using the ACE corpus and assuming manual mention markup. They measure the accuracy of relation extraction alone, without including the added challenge of resolving non-specific relation arguments to name references. They limit their studies to the small ACE corpora where mention markup is manually encoded. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be s</context>
</contexts>
<marker>Zhou, Li, Qian, Zhu, 2008</marker>
<rawString>G. Zhou, J. Li, L. Qian, Q. Zhu. 2008. Semi-Supervised Learning for Relation Extraction. Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-I.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Hovy</author>
</authors>
<title>Not All Seeds Are Equal: Measuring the Quality of Text Mining Seeds.</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics</booktitle>
<pages>618--626</pages>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Z. Kozareva and E. Hovy. Not All Seeds Are Equal: Measuring the Quality of Text Mining Seeds. 2010. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics pp. 618-626.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>