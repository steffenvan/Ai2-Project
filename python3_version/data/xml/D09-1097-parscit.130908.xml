<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994455">
Hypernym Discovery Based on Distributional Similarity
and Hierarchical Structures
</title>
<author confidence="0.9548035">
Ichiro Yamada†, Kentaro Torisawa†, Jun’ichi Kazama†, Kow Kuroda†,
Masaki Murata†, Stijn De Saeger†, Francis Bond† and Asuka Sumida‡
</author>
<affiliation confidence="0.99354">
†National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.564963">
3-5 Hikaridai, Keihannna Science City 619-0289, JAPAN
</address>
<email confidence="0.983876">
{iyamada,torisawa,kazama,kuroda,murata,stijn,bond}@nict.go.jp
</email>
<affiliation confidence="0.967169">
‡Japan Advanced Institute of Science and Technology
</affiliation>
<address confidence="0.967172">
1-1 Asahidai, Nomi-shi, Ishikawa-ken 923-1211, JAPAN
</address>
<email confidence="0.999415">
a-sumida@jaist.ac.jp
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913322580645">
This paper presents a new method of devel-
oping a large-scale hyponymy relation data-
base by combining Wikipedia and other Web
documents. We attach new words to the hy-
ponymy database extracted from Wikipedia
by using distributional similarity calculated
from documents on the Web. For a given tar-
get word, our algorithm first finds k similar
words from the Wikipedia database. Then,
the hypernyms of these k similar words are
assigned scores by considering the distribu-
tional similarities and hierarchical distances
in the Wikipedia database. Finally, new hy-
ponymy relations are output according to the
scores. In this paper, we tested two distribu-
tional similarities. One is based on raw verb-
noun dependencies (which we call “RVD”),
and the other is based on a large-scale clus-
tering of verb-noun dependencies (called
“CVD”). Our method achieved an attachment
accuracy of 91.0% for the top 10,000 rela-
tions, and an attachment accuracy of 74.5%
for the top 100,000 relations when using
CVD. This was a far better outcome com-
pared to the other baseline approaches. Ex-
cluding the region that had very high scores,
CVD was found to be more effective than
RVD. We also confirmed that most relations
extracted by our method cannot be extracted
merely by applying the well-known lexico-
syntactic patterns to Web documents.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917534883721">
Large-scale taxonomies such as WordNet (Fell-
baum 1998) play an important role in informa-
tion extraction and question answering. However,
extremely high costs are borne to manually en-
large and maintain such taxonomies. Thus, appli-
cations using these taxonomies tend to face the
drawback of data sparseness. This paper presents
a new method for discovering a large set of hy-
ponymy relations. Here, a word1 X is regarded as
a hypernym of a word Y if Y is a kind of X or Y
is an instance of X. We are able to generate
large-scale hyponymy relations by attaching new
words to the hyponymy database extracted from
Wikipedia (referred to as “Wikipedia relation
database”) by using distributional similarity cal-
culated from Web documents. Relations ex-
tracted from Wikipedia are relatively clean. On
the other hand, reliable distributional similarity
can be calculated using a large number of docu-
ments on the Web. In this paper, we combine the
advantages of these two resources.
Using distributional similarity, our algorithm
first computes k similar words for a target word.
Then, each k similar word assigns a score to its
ancestors in the hierarchical structures of the
Wikipedia relation database. The hypernym that
has the highest score for the target word is se-
lected as the hypernym of the target word. Figure
1 is an overview of the proposed approach.
In the experiment, we extracted hypernyms for
approximately 670,000 target words that are not
included in the Wikipedia relation database but
are found on the Web. We tested two distribu-
tional similarities: one based on raw verb-noun
dependencies (RVD) and the other based on a
large-scale clustering of verb-noun dependencies
(CVD). The experimental results showed that the
proposed methods were more effective than the
other baseline approaches. In addition, we con-
firmed that most of the relations extracted by our
method could not be extracted using the lexico-
syntactic pattern-based method.
In the remainder of this paper, we first intro-
</bodyText>
<footnote confidence="0.974145">
1 In this paper, we use the term “word” for both “a
single-word word” and “a multi-word word.”
</footnote>
<page confidence="0.922839">
929
</page>
<note confidence="0.996698">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929–937,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.820787">
Wikipedia relation database
A hypernym is selected for
each word independently.
Wikipedia
Selected from hypernyms in the
Wikipedia relation database.
Hyponymy relations are
extracted using the layout
information of Wikipedia.
Wikipedia-based approach
(Ponzetto et al. 2007 and
Sumida et al. 2008)
No direct co-occurrences of
hypernym and hyponym in
corpora are needed.
</bodyText>
<figure confidence="0.680995333333333">
: word
Target word: Selected from the Web
k similar words
</figure>
<figureCaption confidence="0.6691955">
Figure 1: Overview of the proposed approach.
hypernym :
</figureCaption>
<bodyText confidence="0.9992255">
duce some related works in Section 2. Section 3
describes the Wikipedia relation database. Sec-
tion 4 describes the distributional similarity cal-
culated by the two methods. In Section 5, we
describe a method to discover an appropriate
hypernym for each target word. The experimen-
tal results are presented in Section 6 before con-
cluding the paper in Section 7.
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.9997249375">
Most previous researchers have relied on lex-
ico-syntactic patterns for hyponymy acquisition.
Lexico-syntactic patterns were first used by
Hearst (1992). The patterns used by her included
“NP0 such as NP,,” in which NP0 is a hypernym
of NP,. Using these patterns as seeds, Hearst dis-
covered new patterns by which to semi-
automatically extract hyponymy relations. Pantel
et al. (2004a) proposed a method to automatical-
ly discover the patterns using a minimal edit dis-
tance. Ando et al. (2003) applied predefined lex-
ico-syntactic patterns to Japanese news articles.
Snow et al. (2005) generalized these lexico-
syntactic pattern-based methods by using depen-
dency path features for machine learning. Then,
they extended the framework such that this me-
thod was capable of making use of heterogenous
evidence (Snow et al. 2006). These pattern-based
methods require the co-occurrences of a target
word and the hypernym in a document. It should
be noted that the requirement of such co-
occurrences actually poses a problem when we
extract a large set of hyponymy relations since
they are not frequently observed (Shinzato et al.
2004, Pantel et al. 2004b).
Clustering-based methods have been proposed
as another approach. Caraballo (1999), Pantel et
al. (2004b), and Shinzato et al. (2004) proposed a
method to find a common hypernym for word
classes, which are automatically constructed us-
ing some measures of word similarities or hierar-
chical structures in HTML documents. Etzioni et
</bodyText>
<subsectionHeader confidence="0.421477">
Pattern-based method Clustering-based method
</subsectionHeader>
<note confidence="0.7946395">
(Hearst 1992, Pantel et al. (Caraballo 1999, Pantel et al.
2004a, Ando et al. 2003, 2004b, Shinzato et al. 2004,
Snow et al. 2005, Snow et al. and Etzioni et al. 2005)
2006, and Etzioni et al. 2005)
</note>
<figureCaption confidence="0.9964735">
Figure 2: Drawbacks in existing approaches for hypo-
nymy acquisition.
</figureCaption>
<bodyText confidence="0.999182384615384">
al. (2005) used both a pattern-based approach
and a clustering-based approach. The required
amount of co-occurrences is significantly re-
duced due to class-based generalization
processes. Note that these clustering-based me-
thods obtain the same hypernym for all the words
in a particular class. This causes a problem for
selecting an appropriate hypernym for each word
in the case when the granularity or the construc-
tion of the classes is incorrect. Figure 2 shows
the drawbacks of the existing approaches.
Ponzetto et al. (2007) and Sumida et al. (2008)
proposed a method for acquiring hyponymy rela-
tions from Wikipedia. This Wikipedia-based ap-
proach can extract a large volume of hyponymy
relations with high accuracy. However, it is also
true that this approach does not account for many
words that usually appear in Web documents;
this could be because of the unbalanced topics in
Wikipedia or merely because of the incomplete
coverage of articles on Wikipedia. Our method
can target words that frequently appear on the
Web but are not included in the Wikipedia rela-
tion database, thus making the results of the Wi-
kipedia-based approach richer and more ba-
lanced. Our approach uses distributional similari-
</bodyText>
<figure confidence="0.9945696">
Corpus/documents
hypernym such as word
Co-occurrences
in a pattern are
needed
Documents
The same hypernym
is selected for all
words in a class.
hypernym word
word
word
word
word word
Word Class
</figure>
<page confidence="0.992986">
930
</page>
<bodyText confidence="0.999970888888889">
ty, which is computed based on the noun-verb
dependency profiles on the Web. The use of dis-
tributional similarity resembles the clustering-
based approach; however, our method can select
a hypernym for each word independently, and it
does not suffer from class granularity mismatch
or the low quality of classes. In addition, our ap-
proach exploits the hierarchical structures of the
Wikipedia hypernym relations.
</bodyText>
<sectionHeader confidence="0.992124" genericHeader="method">
3 Wikipedia Relation Database
</sectionHeader>
<bodyText confidence="0.977465488372093">
Our Wikipedia relation database is based on the
extraction method of Sumida et al. (2008). They
proposed a method of automatically acquiring
hyponymy relations by focusing on the hierar-
chical layout of articles on Wikipedia. By way of
an example, Figure 3 shows part of the source
code clipped from the article titled “Penguin.”
An article has hierarchical structures composed
of titles, sections, itemizations, etc. The entire
article is divided into sections titled “Anatomy,”
“Mating habits,” “Systematics and evolution,”
“Penguins in popular culture,” and so on. The
section “Systematics and evolution” has a sub-
section “Systematics,” which is further divided
into “Aptenodytes,” “Eudyptes,” and so on.
Some of these section-subsection relations can be
regarded as valid hyponymy relations. In this
article, relations such as the one between “Apte-
nodytes” and “Emperor Penguin” and that be-
tween “Book” and “Penguins of the World” are
valid hyponymy relations.
First, Sumida et al. (2008) extracted hypony-
my relation candidates from hierarchical struc-
tures on Wikipedia. Then, they selected proper
hyponymy relations using a support vector ma-
chine classifier. They used several kinds of fea-
tures for the hyponymy relation candidate, such
as a POS tag for each word, the appearance of
morphemes of each word, the distance between
two words in the hierarchical structures of Wiki-
pedia, and the last character of each word. As a
result of their experiments, approximately 2.4
million hyponymy relations in Japanese were
extracted, with a precision rate of 90.1%.
Compared to the traditional taxonomies, these
extracted hyponymy relations have the following
characteristics (Fellbaum 1998, Bond et al. 2008).
(a) The database includes a more extensive
vocabulary.
(b) The database includes a large number of
named entities.
Popular Japanese taxonomies GoiTaikei (Ike-
hara et al. 1997) and Bunrui-Goi-Hyo (1996)
</bodyText>
<figure confidence="0.91319315">
&apos;&apos;&apos;Penguins&amp;quot;&apos; are a group of
[[Aquatic animal|aquatic]],
[[flightless bird]]s.
== Anatomy ==
== Mating habits ==
==Systematics and evolution==
===Systematics===
* Aptenodytes
**[[Emperor Penguin]]
** [[King Penguin]]
* Eudyptes
== Penguins in popular culture ==
== Book ==
* Penguins
* Penguins of the World
== Notes ==
* Penguinone
* the [[Penguin missile]]
[[Category:Penguins]]
[[Category:Birds]]
</figure>
<figureCaption confidence="0.9942775">
Figure 3: A part of source code clipped from the
article “Penguin” in Wikipedia.
</figureCaption>
<bodyText confidence="0.995808513513514">
contain approximately 300,000 words and
96,000 words, respectively. In contrast, the ex-
tracted hyponymy relations contain approximate-
ly 1.2 million hyponyms and are undoubtedly
much larger than the existing taxonomies.
Another difference is that since Wikipedia covers
a large number of named entities, the extracted
hyponymy relations also contain a large number
of named entities.
Note that the extracted relations have a hierar-
chical structure because one hypernym of a cer-
tain word may also be the hyponym of another
hypernym. However, we observed that the depth
of the hierarchy, on an average, is extremely
shallow. To make the hierarchy appropriate for
our method, we extended these into a deeper hie-
rarchical structure. The extracted relations in-
clude many compound nouns as hypernyms, and
we decomposed a compound noun into a se-
quence of nouns using a morphological analyzer.
Since Japanese is a head-final language, the suf-
fix of a noun sequence becomes the hypernym of
the original compound noun if the suffix forms
another valid compound noun. We extracted suf-
fixes of compound nouns and manually checked
whether they were valid compound nouns; then,
we constructed a hierarchy of compound nouns.
The hierarchy can be extended such that it in-
cludes the hyponyms of the original hypernym
and the resulting hierarchy constitutes a hierar-
chical taxonomy. We use this hierarchical tax-
onomy as a target for expansion.2
2 Note that this modification was performed as part of
another project of ours aimed at constructing a large-scale
and clean hypernym knowledge base by human annotation.
We do not think this cost is directly relevant to the method
proposed here.
</bodyText>
<page confidence="0.996448">
931
</page>
<sectionHeader confidence="0.992418" genericHeader="method">
4 Distributional Similarity
</sectionHeader>
<bodyText confidence="0.999986714285714">
The distributional hypothesis states that words
that occur in similar contexts tend to be semanti-
cally similar (Harris 1985). In this section, we
first introduce distributional similarity based on
raw verb-noun dependencies (RVD). To avoid
the sparseness problem of the co-occurrence of
verb-noun dependencies, we also use distribu-
tional similarity based on a large-scale clustering
of verb-noun dependencies (CVD).
In the experiment mentioned in the following
section, we used the TSUBAKI corpus (Shinzato
et al. 2008) to calculate distributional similarity.
This corpus provides a collection of 100 million
Japanese Web pages containing 6 × 109 sentences.
</bodyText>
<subsectionHeader confidence="0.995316">
4.1 Distributional Similarity Based on RVD
</subsectionHeader>
<bodyText confidence="0.9975793">
When calculating the distributional similarity
based on RVD, we use the triple &lt;v, rel, n&gt;,
where v is a verb, n is a noun phrase, and rel
stands for the relation between v and n. In Japa-
nese, a relation rel is represented by postposi-
tions attached to n and the phrase composed of n
and rel modifies v. Each triple is divided into two
parts. The first is &lt;v, rel&gt; and the second is n.
Then, we consider the conditional probability of
occurrence of the pair &lt;v, rel&gt;: P(&lt;v, rel&gt;|n).
P(&lt;v, rel&gt;|n) can be regarded as the distribution
of the grammatical contexts of the noun phrase n.
The distributional similarity can be defined as
the distance between these distributions. There
are several kinds of functions for evaluating the
distance between two distributions (Lee 1999).
Our method uses the Jensen-Shannon divergence.
The Jensen-Shannon divergence between two
probability distributions, P(⋅  |n1) and P(⋅  |n2) ,
can be calculated as follows:
</bodyText>
<equation confidence="0.999523333333333">
⋅ + ⋅
P (  |) (  |)
1 2
n P n
2
+DKL (P(⋅  |n2
</equation>
<bodyText confidence="0.9991935">
where DKL indicates the Kullback-Leibler diver-
gence and is defined as follows:
</bodyText>
<equation confidence="0.9975834">
( (  |)  ||(  |)) (
P n P n
⋅ ⋅ =
2 ∑P ⋅
1
</equation>
<bodyText confidence="0.9998385">
Finally, the distributional similarity between
two words, n1 and n2, is defined as follows:
</bodyText>
<equation confidence="0.983005">
sim(n1,n2) = 1−DTS(P(⋅  |n1)  ||P(⋅  |n2)).
</equation>
<bodyText confidence="0.999939166666667">
This similarity assumes a value from 0 to 1. If
two words are similar, the value will be close to
1; if two words have entirely different meanings,
the value will be 0.
In the experiment, we used 1,000,000 noun
phrases and 100,000 pairs of verbs and postposi-
tions to calculate the probability P(&lt;v, rel&gt;|n)
from the dependency relations extracted from the
above-mentioned Web corpus (Shinzato et al.
2008). The probabilities are computed using the
following equation by modifying for the fre-
quency using the log function:
</bodyText>
<equation confidence="0.8850955">
&lt; v rel n
, , &gt;)) +
&lt; v rel n &gt;
f ( , ,
&lt; v rel D
, &gt;∈
</equation>
<bodyText confidence="0.9712035">
if f (&lt; v, rel, n &gt;) &gt; 0,
where f(&lt;v, rel, n&gt;) is the frequency of a triple
&lt;v, rel, n&gt; and D is the set defined as { &lt;v, rel &gt; |
f(&lt;v, rel, n&gt;) &gt; 0 }. In the case of f(&lt;v, rel, n&gt;) =
0, P(&lt;v, rel&gt;|n) is set to 0.
Instead of using the observed frequency di-
rectly as in the usual maximum likelihood esti-
mation, we modified it as above. Although this
might seems strange, this kind of modification is
common in information retrieval as a term
weighing method (Manning et al. 1999) and it is
also applied in some studies to yield better word
similarities (Terada et al. 2006, Kazama et al.
2009). We also adopted this idea in this study.
</bodyText>
<subsectionHeader confidence="0.985964">
4.2 Distributional Similarity Based on CVD
</subsectionHeader>
<bodyText confidence="0.9999515">
Rooth et al. (1999) and Torisawa (2001) showed
that EM-based clustering using verb-noun de-
pendencies can produce semantically clean noun
clusters. We exploit these EM-based clustering
results as the smoothed contexts for noun n. In
Torisawa’s model (2001), the probability of oc-
currence of the triple &lt;v, rel, n&gt; is defined as
follows:
</bodyText>
<equation confidence="0.729325">
P(&lt; v, rel, n &gt;)
def ∑a∈ AP(&lt; v, rel &gt; |a)P(n  |a)P(a)
</equation>
<bodyText confidence="0.999958">
where a denotes a hidden class of &lt;v,rel&gt; and n.
In this equation, the probabilities P(&lt;v,rel&gt;|a),
P(n|a), and P(a) cannot be calculated directly
because class a is not observed in a given corpus.
The EM-based clustering method estimates these
probabilities using a given corpus. In the E-step,
</bodyText>
<equation confidence="0.9969356">
DTS
(P(⋅(⋅|n2))
||
nJ
P
1
) ||
2
(P(⋅  |n1
(DKL
)
)  ||P )),
2
DKL
P( |n1 )
 |) log
n1
P n
(  |)
⋅ 2
P(&lt; v, rel &gt; |n)
(
log(
f
∑
log(
1
+
)
1
,
(  |) (  |)
⋅ + ⋅
n P n
1 2
</equation>
<page confidence="0.91706">
932
</page>
<bodyText confidence="0.998619222222222">
the probability P(a|&lt;v,rel&gt;) is calculated. In the
M-step, the probabilities P(&lt;v,rel&gt;|a), P(n|a),
and P(a) are updated to arrive at the maximum
likelihood using the results of the E-step. From
the results of estimation of this EM-based clus-
tering method, we can obtain the probabilities
P(&lt;v,rel&gt;|a), P(n|a), and P(a) for each &lt;v, rel&gt;, n,
and a. Then, P(a|n) is calculated by the following
equation:
</bodyText>
<equation confidence="0.861282">
P(a  |n) = P(n  |a)P(a)
EaeAP(n |a)P(a)
P(a|n) can be used to find the class of n. For
</equation>
<bodyText confidence="0.960911041666667">
example, the class that has the maximum P(a|n)
can be regarded as the class to which n belongs.
Noun phrases that occur with similar pairs
&lt;v,rel&gt; tend to be classified in the same class.
Kazama et al. (2008) proposed the paralleliza-
tion of this EM-based clustering with the aim of
enabling large-scale clustering and using the re-
sulting clusters in named entity recognition. Ka-
zama et al. (2009) reported the calculation of
distributional similarity using the clustering re-
sults. The distributional similarity was calculated
by the Jensen-Shannon divergence, which was
used in this paper. Similar to the case in Kazama
et al., we performed word clustering using
1,000,000 noun phrases and 2,000 classes. Note
that the frequencies of dependencies were mod-
ified with the log function, as in RVD, described
in the previous section.
5 Discovering an Appropriate Hyper-
nym for a Target word
In the Wikipedia relation database, there are
about 95,000 hypernyms and about 1.2 million
hyponyms. In both RVD and CVD, the words
used were selected according to the number (the
number of kinds, not the frequency) of &lt;v, rel &gt;s
that n has dependencies in the data. As a result, 1
million words were selected. The number of
common words that are also included in the Wi-
kipedia relation database are as follows:
Hypernyms 28,015 (common hypernyms)
Hyponyms 175,022 (common hyponyms)
These common hypernyms become candidates
for hypernyms for a target word. On the other
hand, the common hyponyms are used as clues
for identifying appropriate hypernyms.
In our task, the potential target words are
about 810,000 in number and are not included in
the Wikipedia relation database. These include
some strange words or word phrases that are ex-
tracted due to the failure of morphological analy-
sis. We exclude these words using simple rules.
Consequently, the number of target words for our
process is reduced to about 670,000.
In the following section, we outline the scor-
ing method that uses k similar words to discover
an appropriate hypernym for a target word. We
also explain several baseline approaches that use
distributional similarity.
</bodyText>
<subsectionHeader confidence="0.999347">
5.1 Scoring with k similar Words
</subsectionHeader>
<bodyText confidence="0.999641416666667">
In this approach, we first calculate the similari-
ties between the common hyponyms and a target
word and select the k most similar common hy-
ponyms. Here, we use a similarity threshold val-
ue Smin to avoid the effect of words having lower
similarities. If the similarity is less than the thre-
shold value, the word is excluded from the set of
k similar words. Next, each k similar word votes
a score to its ancestors in the hierarchical struc-
tures of the Wikipedia relation database. The
score used to vote for a hypernym nhyper is as fol-
lows:
</bodyText>
<equation confidence="0.716222333333333">
score(nhyper )
= E dr(nhyper,nhypo)−1 X sim
nhypoe Desc (nhyper )nksimilar (ntrg )
</equation>
<bodyText confidence="0.99967856">
where ntrg is the target word, Desc(nhyper) is the
descendant of the hypernym nhyper, ksimilar(ntrg)
is the k similar word of ntrg, r(nhyper,nhypo)−1
d is a
penalty that depends on the differences in the
depth of hierarchy, d is a parameter for the penal-
ty value and has a value between 0 and 1, and
r(ntrg, nhypo) is the difference in the depth of hie-
rarchy between ntrg and nhypo. sim(ntrg,nhypo) is a
distributional similarity between ntrg and nhypo.
As a result of scoring, each hypernym has a
score for the target word. The hypernym that has
the highest score for the target word is selected
as its hypernym. The hyponymy relations thus
produced are ranked according to the scores.
Figure 4 shows an example of the scoring
process. In this example, we use CitroenAX as the
target word whose hypernym will be identified.
First, the k similar words are extracted from the
common hyponyms in the Wikipedia relation:
Opel Astra, TVR Tuscan, Mitsubishi Minica, and
Renault Lutecia are extracted. Next, each k simi-
lar word votes a score to its ancestors. The words
Opel Astra, TVR Tuscan, and Renault Lutecia
vote to their parent car and the word Mitsubishi
</bodyText>
<equation confidence="0.49541275">
.
,
)
(ntrg , nhypo
</equation>
<page confidence="0.962665">
933
</page>
<figureCaption confidence="0.999852">
Figure 4: Overview of the scoring process.
</figureCaption>
<bodyText confidence="0.999549">
Minica votes to its parent mini-vehicle and its
grandparent car with a small penalty. Finally, the
hypernym car, which has the highest score, is
selected as the hypernym of the target word Ci-
troenAX.
</bodyText>
<subsectionHeader confidence="0.99167">
5.2 Baseline Approaches
</subsectionHeader>
<bodyText confidence="0.999491305555556">
Using distributional similarity, we can also de-
velop the following baseline approaches to dis-
cover hyponymy relations.
Selecting the hypernym of the most similar hy-
ponym (baseline approach 1)
We use the heuristics that similar words tend to
have the same hypernym. In this approach, we
first calculate the similarities between the com-
mon hyponyms and the target word. The com-
mon hyponym most similar to the target word is
extracted. Then, the parent of the extracted
common hyponym is regarded as the hypernym
of the target word. This approach outputs several
hypernyms when the most similar hyponym has
several hypernyms. This approach can be consi-
dered to be the same as the scoring method using
k similar words when k = 1. We use the distribu-
tional similarity between the target word and the
most similar hyponym in the Wikipedia relation
database as the score for the appropriateness of
the resulting hyponymy.
Selecting the most similar hypernym (baseline
approach 2)
The distributional similarity between the com-
mon hypernym and the target word is calculated.
Then, the hypernym that has the highest distribu-
tional similarity is regarded as the hypernym of
the target word. The similarity is used as the
score of the appropriateness of the produced hy-
ponymy.
Scoring based on the average similarity of the
hypernym’s children (baseline approach 3)
This approach uses the probabilistic distributions
of the hypernym’s children. We define the prob-
ability Pchild (⋅  |nhyper) characterized by the children
of the hypernym nhyper, as follows:
</bodyText>
<equation confidence="0.9928545">
∑ P(⋅  |nhypo)P(nhypo)
nhypo∈Ch(nhyper)
</equation>
<bodyText confidence="0.92629584">
where Ch(nhyper) is a set of all children of
Then, distributional similarities between a com-
mon hypernym
and the target word nhypo are
calculated. The hypernym that has the highest
distributional similarity is selected as the hyper-
nym of the word. This distributional similarity is
used as the score of the appropriateness of the
produced hyponymy.
If a hypernym has only a few children, the re-
liability of the probabilistic distribution of
hypern
nhyper.
nhyper
ym defined here will be low because the
Wikipedia relation database includes some incor-
rect relations. For this reason, we use the hyper-
nym only if the number of children it has is more
than a threshold value.
Number of similar words:
(RVD)
k = 100.
Similarity threshold: Smin = 0.05.
Penalty value for ancestors: d = 0.6.
We tested the parameter values k =
</bodyText>
<footnote confidence="0.304295">
200, 300, 400,
500, 600, 700, 800, 900, 1000},
0.05, 0.1, 0.15, 0.2,
0.25, 0.3, 0.35, 0.4} and
</footnote>
<page confidence="0.774937666666667">
3
{100,
Smin={0,
</page>
<bodyText confidence="0.750854">
d={0.5, 0.55, 0.6, 0.65, 0.7, 0.75,
0.8, 0.85, 0.9, 0.95, 1.0}.
</bodyText>
<figure confidence="0.985632702702703">
Mitsubishi
Minica
x d1
x d0
: common hypernym(nhyper)
: k similar word &amp;
common hyponym(nhypo)
mini
vehicle
hybrid
vehicle
car
Each k-similar word
votes the score to its
ancestors in the Wikipedia
relation database.
Opel
Astra
CitroenAX
Target word selected
from the Web text (ntrg).
TVR
Tuscan
k similar words
x d0
Renault
Lutecia
)
∈Ch(nhyper
,
=
∑
(  |)
⋅ n hyper
Pchild
P(nhypo )
nhypo
</figure>
<sectionHeader confidence="0.887731" genericHeader="conclusions">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999885">
We evaluated our proposed methods by using it
in experiments to discover hypernyms from the
Wikipedia relation database for the target words
extracted from about 670,000 noun phrases.
</bodyText>
<subsectionHeader confidence="0.972287">
6.1 Parameter Estimation by Preliminary
Experiments
</subsectionHeader>
<bodyText confidence="0.999989">
In the proposed methods, there are several para-
meters. We performed parameter optimization by
randomly selecting 694 words as development
data in our preliminary experiments. The hyper-
nyms of these words were determined manually.
We adjusted the parameters so that each method
achieved the best performance for this develop-
ment data.
The parameters in the scoring method with k
similar words were adjusted as follows3:
</bodyText>
<page confidence="0.997332">
934
</page>
<tableCaption confidence="0.993966666666667">
Table 1: Precision of each approach based on the score ranking. CVD represents the method that uses the dis-
tributional similarity based on large-scale of clustering of verb-noun dependencies. RVD represents the
one based on raw verb-noun dependencies.
</tableCaption>
<table confidence="0.993316545454546">
k-similar words k-similar words k-similar words Baseline Baseline Baseline
(CVD) (RVD) (CVD, d = 0) approach 1 approach 2 approach 3
(CVD) (CVD) (CVD)
1,000 0.940 1.000 0.850
10,000 0.910 0.875 0.875
100,000 0.745 0.710 0.730
670,000 0.520 0.500 0.470
(CVD)
Number of similar words: k = 200.
Similarity threshold: Smin = 0.3.
Penalty value for ancestors: d = 0.6.
</table>
<bodyText confidence="0.870854">
The parameter in baseline approach 3 was ad-
justed as follows:
Threshold for the number of children: 20.
</bodyText>
<subsectionHeader confidence="0.844656">
6.2 Evaluation of the Experimental Results
on the Basis of Score Ranking
</subsectionHeader>
<bodyText confidence="0.981193948717949">
Using the adjusted parameters, we conducted
experiments to extract the hypernym of each tar-
get word with the help of the scoring method
based on k similar words. In these experiments,
two kinds of distributional similarity mentioned
in Section 4 were exploited individually. The
words that were used in the development data
were excluded.
We also conducted a comparative experiment
in which the parameter value for the penalty of
the hierarchal difference, d, was set to 0 to clari-
fy the ability of using hierarchal structures in the
k similar words method. This means each k simi-
lar word votes only to their parent.
We then judged the quality of each acquired
hypernym. The evaluation data sets were sam-
pled from the top 1,000, 10,000, 100,000, and
670,000 results that were ranked according to the
score of each method. Then, against 200 samples
that were randomly sampled from each set, one
of the authors judged whether the hypernym ex-
tracted by each method for the target word was
correct or not. In this evaluation, if the sentence
“The target word is a kind of the hypernym” or
“The target word is an instance of the hypernym”
was consistent, the extracted hyponymy was
judged as correct. It should be noted that the out-
puts of the compared methods are combined and
shuffled to enable fair comparison. In addition,
baseline approach 1 extracted several hypernyms
for the target word. In this case, we judged the
hypernym as correct when the case where one of
0.730 0.290 0.630
0.555 0.300 0.445
0.500 0.280 0.435
0.345 0.115 0.170
the hypernyms was correct.
The precision of each result is shown in Table
1. The results of the k similar words method are
far better than those of the other baseline me-
thods. In particular, the k similar words method
with CVD outperformed the methods of the k
similar words where the parameter value d was
set to 0 and the method using RVD except for the
top 1,000 results. This means that the use of hie-
rarchal structures and the clustering process for
calculating distributional similarity are effective
for this task. We confirmed the significant differ-
ences of the proposed method (CVD) as com-
pared with all the baseline approaches at the 1%
significant level by the Fisher’s exact test (Hays
1988).
The precision of baseline approach 2 that se-
lected the most similar hypernym was the worst
among all the methods. There were words that
were similar to the target word among the hyper-
nyms extracted incorrectly. For example, the
word semento-kojo (cement factory) was ex-
tracted for the hypernym of the word kuriningu-
kojo (dry cleaning plant). It is difficult to judge
whether the word is a hypernym or just a similar
word by using only the similarity measure.
As for the results of baseline approach 1 using
the most similar hyponym and baseline approach
3 using the similarity of the set of hypernym’s
children, the noise on the Wikipedia relation da-
tabase decreased the precision. Moreover, over-
specified hypernyms were extracted incorrectly
by these methods. In contrast, the method of
scoring based on the use of k similar words was
robust against noise because it uses the voting
approach for the similarities. Further, this me-
thod can extract hypernyms that are not over-
specific because it uses all descendants for scor-
ing.
Table 2 shows some examples of relations ex-
tracted by the k similar words method using
CVD.
</bodyText>
<page confidence="0.99838">
935
</page>
<tableCaption confidence="0.980954">
Table2: Hypernym discovery results by the k-similar
words based approach (CVD). The underline indi-
cates the hypernyms which are extracted incorrectly.
</tableCaption>
<table confidence="0.796560666666667">
Score Target word Extracted hypernym
58.6 INDIVI burando
(fashion label)
54.3 kureome (Cleome) hana (flower)
34.4 UOKR gemu (game)
21.7 Okido (Okido) machi (town)
</table>
<bodyText confidence="0.999541555555556">
(book) appeared 20 times. As shown in Table 2,
hon (book) was also extracted for the target word
meru-seminah (mail seminar). It is really diffi-
cult even for a human to identify whether the
title is that of the book or the event. If we can
identify these difficult hypernyms in advance, we
can improve precision by excluding them from
the target hypernyms. This will be one of the top-
ics for future study.
</bodyText>
<figure confidence="0.9866446875">
20.5 Sumatofotsu kuruma
(Smart fortwo) (car) 7 Conclusion
15.6 Fukagawameshi ryori (dish)
(Fukagawa rice)
8.9 John Barry sakkyokuka
(composer)
8.5 JVM sofuto-wea
(software)
6.6 metangasu genso
(methane gas) (chemical element)
5.4 me-ru semina Hon (book)
(mail seminar)
3.9 gurometto shohin
(grommet) (merchandise)
3.1 supuringubakku gensho
(spring back) (phenomenon)
</figure>
<subsectionHeader confidence="0.946664">
6.3 Investigation of the Extracted Relation
Overlap with a Conventional Method
</subsectionHeader>
<bodyText confidence="0.9999716875">
We randomly sampled 300 hyponymy rela-
tions that were extracted correctly using the k
similar words method exploiting CVD and inves-
tigated whether or not these relations can be ex-
tracted by the conventional method based on the
lexico-syntactic pattern. The possible hyponymy
relations were extracted using the pattern-based
method (Ando et al. 2003) from the TSUBAKI
corpus (Shinzato et al. 2008). From a comparison
of these relations, we found only 57 common
hyponymy relations. That is, the remaining 243
hyponymy relations were not included in the
possible hyponymy relations. This result indi-
cates that our method can acquire the hyponymy
relations that cannot be extracted by the conven-
tional pattern-based method.
</bodyText>
<subsectionHeader confidence="0.942683">
6.4 Discussions
</subsectionHeader>
<bodyText confidence="0.99999375">
We investigated the reason for the errors gener-
ated by the method of scoring using k similar
words exploiting CVD. We conducted experi-
ments on hypernym extraction targeting 694
words in the development data mentioned in Sec-
tion 6.1. Among these, 286 relations were ex-
tracted incorrectly. In these relations, there were
some frequent hypernyms. For example, the
word sakuhin (work) appeared 28 times and hon
In this paper, we proposed a method for disco-
vering hyponymy relations between nouns by
fusing the Wikipedia relation database and words
from the Web. We demonstrated that the method
using k similar words has high accuracy. The
experimental results showed the effectiveness of
using hierarchal structures and the clustering
process for calculating distributional similarity
for this task. The experimental results showed
that our method could achieve 91.0% attachment
accuracy for the top 10,000 hyponymy relations
and 74.5% attachment accuracy for the top
100,000 relations when using the clustering-
based similarity. We confirmed that most rela-
tions extracted by the proposed method could not
be handled by the lexico-syntactic pattern-based
method. Future work will be to filter out difficult
hypernyms for hyponymy extraction process to
achieve higher precision.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859714285714">
M. Ando, S. Sekine and S. Ishizaki. 2003. Automatic
Extraction of Hyponyms from Newspaper Using
Lexicosyntactic Patterns. IPSJ SIG Notes, 2003-
NL-157, pp. 77–82 (in Japanese).
F. Bond, H. Isahara, K. Kanzaki and K. Uchimoto.
2008. Boot-strapping a WordNet Using Multiple
Existing WordNets. In the 6th International Confe-
rence on Language Resources and Evaluation
(LREC), Marrakech.
Bunruigoihyo. 1996. The National Language Re-
search Institute (in Japanese).
S. A. Caraballo. 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. In
Proceedings of the Conference of the Association
for Computational Linguistics (ACL).
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T.
Shaked, S. Soderland, D. Weld and A. Yates. 2005.
Unsupervised Named-Entity Extraction from the
Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91–134.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
</reference>
<page confidence="0.985619">
936
</page>
<reference confidence="0.999807802325581">
Database. Cambridge, MA: MIT Press.
Z. Harris. 1985. Distributional Structure. In Katz, J. J.
(ed.) The Philosophy of Linguistics, Oxford Uni-
versity Press, pp. 26–47.
W. L. Hays. 1988. Statistics: Analyzing Qualitative
Data, Rinehart and Winston, Inc., Ch. 18, pp. 769–
783.
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proceedings of
the 14th Conference on Computational Linguistics
(COLING), pp. 539–545.
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Na-
kaiwa, K. Ogura, Y. Ooyama and Y. Hayashi. 1997.
Goi-Taikei A Japanese Lexicon, Iwanami Shoten.
J. Kazama and K. Torisawa. 2008. Inducing Gazet-
teers for Named Entity Recognition by Large-scale
Clustering of Dependency Relations. In Proceed-
ings of ACL-08: HLT, pp. 407–415.
J. Kazama, Stijn De Saeger, K. Torisawa and M. Mu-
rata. 2009. Generating a Large-scale Analogy List
Using a Probabilistic Clustering Based on Noun-
Verb Dependency Profiles. In 15th Annual Meeting
of the Association for Natural Language
Processing, C1–3 (in Japanese).
L. Lee. 1999. Measures of Distributional Similarity.
In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, pp. 25–
32.
C. D. Manning and H. Schutze. 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press.
P. Pantel, D. Ravichandran and E. Hovy. 2004a. To-
wards Terascale Knowledge Acquisition. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics.
P. Pantel and D. Ravichandran. 2004b. Automatically
Labeling Semantic Classes. In Proceedings of the
Human Language Technology and North American
Chapter of the Association for Computational Lin-
guistics Conference.
S. P. Ponzetto, and M. Strube. 2007. Deriving a Large
Scale Taxonomy from Wikipedia. In Proceedings
of the 22nd National Conference on Artificial Intel-
ligence, pp. 1440–1445.
M. Rooth, S. Riezler, D. Presher, G. Carroll and F.
Beil. 1999. Inducing a Semantically Annotated
Lexicon via EM-based Clustering. In Proceedings
of the 37th annual meeting of the Association for
Computational Linguistics, pp. 104–111.
K. Shinzato and K. Torisawa. 2004. Acquiring Hypo-
nymy Relations from Web Documents. In Proceed-
ings of HLT-NAACL, pp. 73–80.
K. Shinzato, D. Kawahara, C. Hashimoto and S. Ku-
rohashi. 2008. A Large-Scale Web Data Collection
as A Natural Language Processing Infrastructure.
In the 6th International Conference on Language
Resources and Evaluation (LREC).
R. Snow, D. Jurafsky and A. Y. Ng. 2005. Learning
Syntactic Patterns for Automatic Hypernym Dis-
covery. NIPS 2005.
R. Snow, D. Jurafsky, A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogenous Evidence. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pp. 801–808.
A. Sumida, N. Yoshinaga and K. Torisawa. 2008.
Boosting Precision and Recall of Hyponymy Rela-
tion Acquisition from Hierarchical Layouts in Wi-
kipedia. In the 6th International Conference on
Language Resources and Evaluation (LREC).
A. Terada, M. Yoshida, H. Nakagawa. 2006. A Tool
for Constructing a Synonym Dictionary using con-
text Information. In proceedings of IPSJ SIG Tech-
nical Reports, vol.2006 No.124, pp. 87-94. (In Jap-
anese).
K. Torisawa. 2001. An Unsupervised Method for Ca-
nonicalization of Japanese Postpositions. In Pro-
ceedings of the 6th Natural Language Processing
Pacific Rim Symposium (NLPRS), pp. 211–218.
K. Torisawa, Stijn De Saeger, Y. Kakizawa, J. Kaza-
ma, M. Murata, D. Noguchi and A. Sumida. 2008.
TORISHIKI-KAI, An Autogenerated Web Search
Directory. In Proceedings of the second interna-
tional symposium on universal communication, pp.
179–186, 2008.
</reference>
<page confidence="0.997581">
937
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.676932">
<title confidence="0.9985495">Hypernym Discovery Based on Distributional and Hierarchical Structures</title>
<author confidence="0.971857">Kentaro Jun’ichi Kow</author>
<affiliation confidence="0.9268435">Stijn De Francis and Asuka Institute of Information and Communications</affiliation>
<address confidence="0.99579">3-5 Hikaridai, Keihannna Science City 619-0289, JAPAN</address>
<email confidence="0.958275">iyamada@nict.go.jp</email>
<email confidence="0.958275">torisawa@nict.go.jp</email>
<email confidence="0.958275">kazama@nict.go.jp</email>
<email confidence="0.958275">kuroda@nict.go.jp</email>
<email confidence="0.958275">murata@nict.go.jp</email>
<email confidence="0.958275">stijn@nict.go.jp</email>
<email confidence="0.958275">bond@nict.go.jp</email>
<affiliation confidence="0.992942">Advanced Institute of Science and</affiliation>
<address confidence="0.884303">1-1 Asahidai, Nomi-shi, Ishikawa-ken 923-1211,</address>
<email confidence="0.955397">a-sumida@jaist.ac.jp</email>
<abstract confidence="0.99890946875">This paper presents a new method of developing a large-scale hyponymy relation database by combining Wikipedia and other Web documents. We attach new words to the hyponymy database extracted from Wikipedia by using distributional similarity calculated from documents on the Web. For a given tarword, our algorithm first finds words from the Wikipedia database. Then, hypernyms of these words are assigned scores by considering the distributional similarities and hierarchical distances in the Wikipedia database. Finally, new hyponymy relations are output according to the scores. In this paper, we tested two distributional similarities. One is based on raw verbnoun dependencies (which we call “RVD”), and the other is based on a large-scale clustering of verb-noun dependencies (called “CVD”). Our method achieved an attachment accuracy of 91.0% for the top 10,000 relations, and an attachment accuracy of 74.5% for the top 100,000 relations when using CVD. This was a far better outcome compared to the other baseline approaches. Excluding the region that had very high scores, CVD was found to be more effective than RVD. We also confirmed that most relations extracted by our method cannot be extracted merely by applying the well-known lexicosyntactic patterns to Web documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Ando</author>
<author>S Sekine</author>
<author>S Ishizaki</author>
</authors>
<title>Automatic Extraction of Hyponyms from Newspaper Using Lexicosyntactic Patterns.</title>
<date>2003</date>
<booktitle>IPSJ SIG Notes, 2003-NL-157,</booktitle>
<pages>77--82</pages>
<note>(in Japanese).</note>
<contexts>
<context position="5451" citStr="Ando et al. (2003)" startWordPosition="851" endWordPosition="854"> each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Related Works Most previous researchers have relied on lexico-syntactic patterns for hyponymy acquisition. Lexico-syntactic patterns were first used by Hearst (1992). The patterns used by her included “NP0 such as NP,,” in which NP0 is a hypernym of NP,. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequentl</context>
<context position="30774" citStr="Ando et al. 2003" startWordPosition="5060" endWordPosition="5063">metangasu genso (methane gas) (chemical element) 5.4 me-ru semina Hon (book) (mail seminar) 3.9 gurometto shohin (grommet) (merchandise) 3.1 supuringubakku gensho (spring back) (phenomenon) 6.3 Investigation of the Extracted Relation Overlap with a Conventional Method We randomly sampled 300 hyponymy relations that were extracted correctly using the k similar words method exploiting CVD and investigated whether or not these relations can be extracted by the conventional method based on the lexico-syntactic pattern. The possible hyponymy relations were extracted using the pattern-based method (Ando et al. 2003) from the TSUBAKI corpus (Shinzato et al. 2008). From a comparison of these relations, we found only 57 common hyponymy relations. That is, the remaining 243 hyponymy relations were not included in the possible hyponymy relations. This result indicates that our method can acquire the hyponymy relations that cannot be extracted by the conventional pattern-based method. 6.4 Discussions We investigated the reason for the errors generated by the method of scoring using k similar words exploiting CVD. We conducted experiments on hypernym extraction targeting 694 words in the development data mentio</context>
</contexts>
<marker>Ando, Sekine, Ishizaki, 2003</marker>
<rawString>M. Ando, S. Sekine and S. Ishizaki. 2003. Automatic Extraction of Hyponyms from Newspaper Using Lexicosyntactic Patterns. IPSJ SIG Notes, 2003-NL-157, pp. 77–82 (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bond</author>
<author>H Isahara</author>
<author>K Kanzaki</author>
<author>K Uchimoto</author>
</authors>
<title>Boot-strapping a WordNet Using Multiple Existing WordNets.</title>
<date>2008</date>
<booktitle>In the 6th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Marrakech.</location>
<contexts>
<context position="10302" citStr="Bond et al. 2008" startWordPosition="1613" endWordPosition="1616">cted proper hyponymy relations using a support vector machine classifier. They used several kinds of features for the hyponymy relation candidate, such as a POS tag for each word, the appearance of morphemes of each word, the distance between two words in the hierarchical structures of Wikipedia, and the last character of each word. As a result of their experiments, approximately 2.4 million hyponymy relations in Japanese were extracted, with a precision rate of 90.1%. Compared to the traditional taxonomies, these extracted hyponymy relations have the following characteristics (Fellbaum 1998, Bond et al. 2008). (a) The database includes a more extensive vocabulary. (b) The database includes a large number of named entities. Popular Japanese taxonomies GoiTaikei (Ikehara et al. 1997) and Bunrui-Goi-Hyo (1996) &apos;&apos;&apos;Penguins&amp;quot;&apos; are a group of [[Aquatic animal|aquatic]], [[flightless bird]]s. == Anatomy == == Mating habits == ==Systematics and evolution== ===Systematics=== * Aptenodytes **[[Emperor Penguin]] ** [[King Penguin]] * Eudyptes == Penguins in popular culture == == Book == * Penguins * Penguins of the World == Notes == * Penguinone * the [[Penguin missile]] [[Category:Penguins]] [[Category:Birds</context>
</contexts>
<marker>Bond, Isahara, Kanzaki, Uchimoto, 2008</marker>
<rawString>F. Bond, H. Isahara, K. Kanzaki and K. Uchimoto. 2008. Boot-strapping a WordNet Using Multiple Existing WordNets. In the 6th International Conference on Language Resources and Evaluation (LREC), Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bunruigoihyo</author>
</authors>
<title>The National Language Research Institute</title>
<date>1996</date>
<note>(in Japanese).</note>
<marker>Bunruigoihyo, 1996</marker>
<rawString>Bunruigoihyo. 1996. The National Language Research Institute (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Caraballo</author>
</authors>
<title>Automatic Construction of a Hypernym-labeled Noun Hierarchy from Text.</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6188" citStr="Caraballo (1999)" startWordPosition="967" endWordPosition="968">ctic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities or hierarchical structures in HTML documents. Etzioni et Pattern-based method Clustering-based method (Hearst 1992, Pantel et al. (Caraballo 1999, Pantel et al. 2004a, Ando et al. 2003, 2004b, Shinzato et al. 2004, Snow et al. 2005, Snow et al. and Etzioni et al. 2005) 2006, and Etzioni et al. 2005) Figure 2: Drawbacks in existing approaches for hyponymy acquisition. al. (2005) used both a pattern-based ap</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>S. A. Caraballo. 1999. Automatic Construction of a Hypernym-labeled Noun Hierarchy from Text. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised Named-Entity Extraction from the Web: An Experimental Study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="6648" citStr="Etzioni et al. 2005" startWordPosition="1041" endWordPosition="1044"> they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities or hierarchical structures in HTML documents. Etzioni et Pattern-based method Clustering-based method (Hearst 1992, Pantel et al. (Caraballo 1999, Pantel et al. 2004a, Ando et al. 2003, 2004b, Shinzato et al. 2004, Snow et al. 2005, Snow et al. and Etzioni et al. 2005) 2006, and Etzioni et al. 2005) Figure 2: Drawbacks in existing approaches for hyponymy acquisition. al. (2005) used both a pattern-based approach and a clustering-based approach. The required amount of co-occurrences is significantly reduced due to class-based generalization processes. Note that these clustering-based methods obtain the same hypernym for all the words in a particular class. This causes a problem for selecting an appropriate hypernym for each word in the case when the granularity or the construction of the classes is incorrect. Figure 2 shows the drawbacks of the existing appr</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld and A. Yates. 2005. Unsupervised Named-Entity Extraction from the Web: An Experimental Study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="1913" citStr="Fellbaum 1998" startWordPosition="279" endWordPosition="281">le clustering of verb-noun dependencies (called “CVD”). Our method achieved an attachment accuracy of 91.0% for the top 10,000 relations, and an attachment accuracy of 74.5% for the top 100,000 relations when using CVD. This was a far better outcome compared to the other baseline approaches. Excluding the region that had very high scores, CVD was found to be more effective than RVD. We also confirmed that most relations extracted by our method cannot be extracted merely by applying the well-known lexicosyntactic patterns to Web documents. 1 Introduction Large-scale taxonomies such as WordNet (Fellbaum 1998) play an important role in information extraction and question answering. However, extremely high costs are borne to manually enlarge and maintain such taxonomies. Thus, applications using these taxonomies tend to face the drawback of data sparseness. This paper presents a new method for discovering a large set of hyponymy relations. Here, a word1 X is regarded as a hypernym of a word Y if Y is a kind of X or Y is an instance of X. We are able to generate large-scale hyponymy relations by attaching new words to the hyponymy database extracted from Wikipedia (referred to as “Wikipedia relation </context>
<context position="10283" citStr="Fellbaum 1998" startWordPosition="1611" endWordPosition="1612">Then, they selected proper hyponymy relations using a support vector machine classifier. They used several kinds of features for the hyponymy relation candidate, such as a POS tag for each word, the appearance of morphemes of each word, the distance between two words in the hierarchical structures of Wikipedia, and the last character of each word. As a result of their experiments, approximately 2.4 million hyponymy relations in Japanese were extracted, with a precision rate of 90.1%. Compared to the traditional taxonomies, these extracted hyponymy relations have the following characteristics (Fellbaum 1998, Bond et al. 2008). (a) The database includes a more extensive vocabulary. (b) The database includes a large number of named entities. Popular Japanese taxonomies GoiTaikei (Ikehara et al. 1997) and Bunrui-Goi-Hyo (1996) &apos;&apos;&apos;Penguins&amp;quot;&apos; are a group of [[Aquatic animal|aquatic]], [[flightless bird]]s. == Anatomy == == Mating habits == ==Systematics and evolution== ===Systematics=== * Aptenodytes **[[Emperor Penguin]] ** [[King Penguin]] * Eudyptes == Penguins in popular culture == == Book == * Penguins * Penguins of the World == Notes == * Penguinone * the [[Penguin missile]] [[Category:Penguins</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional Structure.</title>
<date>1985</date>
<booktitle>The Philosophy of Linguistics,</booktitle>
<pages>26--47</pages>
<editor>In Katz, J. J. (ed.)</editor>
<publisher>University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="12814" citStr="Harris 1985" startWordPosition="2007" endWordPosition="2008">rarchy can be extended such that it includes the hyponyms of the original hypernym and the resulting hierarchy constitutes a hierarchical taxonomy. We use this hierarchical taxonomy as a target for expansion.2 2 Note that this modification was performed as part of another project of ours aimed at constructing a large-scale and clean hypernym knowledge base by human annotation. We do not think this cost is directly relevant to the method proposed here. 931 4 Distributional Similarity The distributional hypothesis states that words that occur in similar contexts tend to be semantically similar (Harris 1985). In this section, we first introduce distributional similarity based on raw verb-noun dependencies (RVD). To avoid the sparseness problem of the co-occurrence of verb-noun dependencies, we also use distributional similarity based on a large-scale clustering of verb-noun dependencies (CVD). In the experiment mentioned in the following section, we used the TSUBAKI corpus (Shinzato et al. 2008) to calculate distributional similarity. This corpus provides a collection of 100 million Japanese Web pages containing 6 × 109 sentences. 4.1 Distributional Similarity Based on RVD When calculating the di</context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Z. Harris. 1985. Distributional Structure. In Katz, J. J. (ed.) The Philosophy of Linguistics, Oxford University Press, pp. 26–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Hays</author>
</authors>
<title>Statistics: Analyzing Qualitative Data, Rinehart and</title>
<date>1988</date>
<journal>Winston, Inc., Ch.</journal>
<volume>18</volume>
<pages>769--783</pages>
<contexts>
<context position="28133" citStr="Hays 1988" startWordPosition="4639" endWordPosition="4640">he k similar words method are far better than those of the other baseline methods. In particular, the k similar words method with CVD outperformed the methods of the k similar words where the parameter value d was set to 0 and the method using RVD except for the top 1,000 results. This means that the use of hierarchal structures and the clustering process for calculating distributional similarity are effective for this task. We confirmed the significant differences of the proposed method (CVD) as compared with all the baseline approaches at the 1% significant level by the Fisher’s exact test (Hays 1988). The precision of baseline approach 2 that selected the most similar hypernym was the worst among all the methods. There were words that were similar to the target word among the hypernyms extracted incorrectly. For example, the word semento-kojo (cement factory) was extracted for the hypernym of the word kuriningukojo (dry cleaning plant). It is difficult to judge whether the word is a hypernym or just a similar word by using only the similarity measure. As for the results of baseline approach 1 using the most similar hyponym and baseline approach 3 using the similarity of the set of hyperny</context>
</contexts>
<marker>Hays, 1988</marker>
<rawString>W. L. Hays. 1988. Statistics: Analyzing Qualitative Data, Rinehart and Winston, Inc., Ch. 18, pp. 769– 783.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th Conference on Computational Linguistics (COLING),</booktitle>
<pages>539--545</pages>
<contexts>
<context position="5113" citStr="Hearst (1992)" startWordPosition="794" endWordPosition="795">d from the Web k similar words Figure 1: Overview of the proposed approach. hypernym : duce some related works in Section 2. Section 3 describes the Wikipedia relation database. Section 4 describes the distributional similarity calculated by the two methods. In Section 5, we describe a method to discover an appropriate hypernym for each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Related Works Most previous researchers have relied on lexico-syntactic patterns for hyponymy acquisition. Lexico-syntactic patterns were first used by Hearst (1992). The patterns used by her included “NP0 such as NP,,” in which NP0 is a hypernym of NP,. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method wa</context>
<context position="6493" citStr="Hearst 1992" startWordPosition="1013" endWordPosition="1014">nt. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities or hierarchical structures in HTML documents. Etzioni et Pattern-based method Clustering-based method (Hearst 1992, Pantel et al. (Caraballo 1999, Pantel et al. 2004a, Ando et al. 2003, 2004b, Shinzato et al. 2004, Snow et al. 2005, Snow et al. and Etzioni et al. 2005) 2006, and Etzioni et al. 2005) Figure 2: Drawbacks in existing approaches for hyponymy acquisition. al. (2005) used both a pattern-based approach and a clustering-based approach. The required amount of co-occurrences is significantly reduced due to class-based generalization processes. Note that these clustering-based methods obtain the same hypernym for all the words in a particular class. This causes a problem for selecting an appropriate</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Proceedings of the 14th Conference on Computational Linguistics (COLING), pp. 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ikehara</author>
<author>M Miyazaki</author>
<author>S Shirai</author>
<author>A Yokoo</author>
<author>H Nakaiwa</author>
<author>K Ogura</author>
<author>Y Ooyama</author>
<author>Y Hayashi</author>
</authors>
<title>Goi-Taikei A Japanese Lexicon, Iwanami Shoten.</title>
<date>1997</date>
<contexts>
<context position="10478" citStr="Ikehara et al. 1997" startWordPosition="1639" endWordPosition="1643"> word, the appearance of morphemes of each word, the distance between two words in the hierarchical structures of Wikipedia, and the last character of each word. As a result of their experiments, approximately 2.4 million hyponymy relations in Japanese were extracted, with a precision rate of 90.1%. Compared to the traditional taxonomies, these extracted hyponymy relations have the following characteristics (Fellbaum 1998, Bond et al. 2008). (a) The database includes a more extensive vocabulary. (b) The database includes a large number of named entities. Popular Japanese taxonomies GoiTaikei (Ikehara et al. 1997) and Bunrui-Goi-Hyo (1996) &apos;&apos;&apos;Penguins&amp;quot;&apos; are a group of [[Aquatic animal|aquatic]], [[flightless bird]]s. == Anatomy == == Mating habits == ==Systematics and evolution== ===Systematics=== * Aptenodytes **[[Emperor Penguin]] ** [[King Penguin]] * Eudyptes == Penguins in popular culture == == Book == * Penguins * Penguins of the World == Notes == * Penguinone * the [[Penguin missile]] [[Category:Penguins]] [[Category:Birds]] Figure 3: A part of source code clipped from the article “Penguin” in Wikipedia. contain approximately 300,000 words and 96,000 words, respectively. In contrast, the extract</context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, Yokoo, Nakaiwa, Ogura, Ooyama, Hayashi, 1997</marker>
<rawString>S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Nakaiwa, K. Ogura, Y. Ooyama and Y. Hayashi. 1997. Goi-Taikei A Japanese Lexicon, Iwanami Shoten.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>K Torisawa</author>
</authors>
<title>Inducing Gazetteers for Named Entity Recognition by Large-scale Clustering of Dependency Relations.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>407--415</pages>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>J. Kazama and K. Torisawa. 2008. Inducing Gazetteers for Named Entity Recognition by Large-scale Clustering of Dependency Relations. In Proceedings of ACL-08: HLT, pp. 407–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>Stijn De Saeger</author>
<author>K Torisawa</author>
<author>M Murata</author>
</authors>
<title>Generating a Large-scale Analogy List Using a Probabilistic Clustering Based on NounVerb Dependency Profiles.</title>
<date>2009</date>
<booktitle>In 15th Annual Meeting of the Association for Natural Language Processing, C1–3 (in Japanese).</booktitle>
<marker>Kazama, De Saeger, Torisawa, Murata, 2009</marker>
<rawString>J. Kazama, Stijn De Saeger, K. Torisawa and M. Murata. 2009. Generating a Large-scale Analogy List Using a Probabilistic Clustering Based on NounVerb Dependency Profiles. In 15th Annual Meeting of the Association for Natural Language Processing, C1–3 (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Measures of Distributional Similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="14164" citStr="Lee 1999" startWordPosition="2225" endWordPosition="2226">etween v and n. In Japanese, a relation rel is represented by postpositions attached to n and the phrase composed of n and rel modifies v. Each triple is divided into two parts. The first is &lt;v, rel&gt; and the second is n. Then, we consider the conditional probability of occurrence of the pair &lt;v, rel&gt;: P(&lt;v, rel&gt;|n). P(&lt;v, rel&gt;|n) can be regarded as the distribution of the grammatical contexts of the noun phrase n. The distributional similarity can be defined as the distance between these distributions. There are several kinds of functions for evaluating the distance between two distributions (Lee 1999). Our method uses the Jensen-Shannon divergence. The Jensen-Shannon divergence between two probability distributions, P(⋅ |n1) and P(⋅ |n2) , can be calculated as follows: ⋅ + ⋅ P ( |) ( |) 1 2 n P n 2 +DKL (P(⋅ |n2 where DKL indicates the Kullback-Leibler divergence and is defined as follows: ( ( |) ||( |)) ( P n P n ⋅ ⋅ = 2 ∑P ⋅ 1 Finally, the distributional similarity between two words, n1 and n2, is defined as follows: sim(n1,n2) = 1−DTS(P(⋅ |n1) ||P(⋅ |n2)). This similarity assumes a value from 0 to 1. If two words are similar, the value will be close to 1; if two words have entirely diff</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>L. Lee. 1999. Measures of Distributional Similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pp. 25– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. D. Manning and H. Schutze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Towards Terascale Knowledge Acquisition.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="5342" citStr="Pantel et al. (2004" startWordPosition="832" endWordPosition="835">rity calculated by the two methods. In Section 5, we describe a method to discover an appropriate hypernym for each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Related Works Most previous researchers have relied on lexico-syntactic patterns for hyponymy acquisition. Lexico-syntactic patterns were first used by Hearst (1992). The patterns used by her included “NP0 such as NP,,” in which NP0 is a hypernym of NP,. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurr</context>
</contexts>
<marker>Pantel, Ravichandran, Hovy, 2004</marker>
<rawString>P. Pantel, D. Ravichandran and E. Hovy. 2004a. Towards Terascale Knowledge Acquisition. In Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
</authors>
<title>Automatically Labeling Semantic Classes.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology and North American Chapter of the Association for Computational Linguistics Conference.</booktitle>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>P. Pantel and D. Ravichandran. 2004b. Automatically Labeling Semantic Classes. In Proceedings of the Human Language Technology and North American Chapter of the Association for Computational Linguistics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Deriving a Large Scale Taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd National Conference on Artificial Intelligence,</booktitle>
<pages>1440--1445</pages>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>S. P. Ponzetto, and M. Strube. 2007. Deriving a Large Scale Taxonomy from Wikipedia. In Proceedings of the 22nd National Conference on Artificial Intelligence, pp. 1440–1445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rooth</author>
<author>S Riezler</author>
<author>D Presher</author>
<author>G Carroll</author>
<author>F Beil</author>
</authors>
<title>Inducing a Semantically Annotated Lexicon via EM-based Clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="15906" citStr="Rooth et al. (1999)" startWordPosition="2561" endWordPosition="2564">, rel, n&gt; and D is the set defined as { &lt;v, rel &gt; | f(&lt;v, rel, n&gt;) &gt; 0 }. In the case of f(&lt;v, rel, n&gt;) = 0, P(&lt;v, rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the usual maximum likelihood estimation, we modified it as above. Although this might seems strange, this kind of modification is common in information retrieval as a term weighing method (Manning et al. 1999) and it is also applied in some studies to yield better word similarities (Terada et al. 2006, Kazama et al. 2009). We also adopted this idea in this study. 4.2 Distributional Similarity Based on CVD Rooth et al. (1999) and Torisawa (2001) showed that EM-based clustering using verb-noun dependencies can produce semantically clean noun clusters. We exploit these EM-based clustering results as the smoothed contexts for noun n. In Torisawa’s model (2001), the probability of occurrence of the triple &lt;v, rel, n&gt; is defined as follows: P(&lt; v, rel, n &gt;) def ∑a∈ AP(&lt; v, rel &gt; |a)P(n |a)P(a) where a denotes a hidden class of &lt;v,rel&gt; and n. In this equation, the probabilities P(&lt;v,rel&gt;|a), P(n|a), and P(a) cannot be calculated directly because class a is not observed in a given corpus. The EM-based clustering method e</context>
</contexts>
<marker>Rooth, Riezler, Presher, Carroll, Beil, 1999</marker>
<rawString>M. Rooth, S. Riezler, D. Presher, G. Carroll and F. Beil. 1999. Inducing a Semantically Annotated Lexicon via EM-based Clustering. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics, pp. 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shinzato</author>
<author>K Torisawa</author>
</authors>
<title>Acquiring Hyponymy Relations from Web Documents.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>73--80</pages>
<marker>Shinzato, Torisawa, 2004</marker>
<rawString>K. Shinzato and K. Torisawa. 2004. Acquiring Hyponymy Relations from Web Documents. In Proceedings of HLT-NAACL, pp. 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shinzato</author>
<author>D Kawahara</author>
<author>C Hashimoto</author>
<author>S Kurohashi</author>
</authors>
<title>A Large-Scale Web Data Collection as A Natural Language Processing Infrastructure.</title>
<date>2008</date>
<booktitle>In the 6th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="13209" citStr="Shinzato et al. 2008" startWordPosition="2062" endWordPosition="2065">not think this cost is directly relevant to the method proposed here. 931 4 Distributional Similarity The distributional hypothesis states that words that occur in similar contexts tend to be semantically similar (Harris 1985). In this section, we first introduce distributional similarity based on raw verb-noun dependencies (RVD). To avoid the sparseness problem of the co-occurrence of verb-noun dependencies, we also use distributional similarity based on a large-scale clustering of verb-noun dependencies (CVD). In the experiment mentioned in the following section, we used the TSUBAKI corpus (Shinzato et al. 2008) to calculate distributional similarity. This corpus provides a collection of 100 million Japanese Web pages containing 6 × 109 sentences. 4.1 Distributional Similarity Based on RVD When calculating the distributional similarity based on RVD, we use the triple &lt;v, rel, n&gt;, where v is a verb, n is a noun phrase, and rel stands for the relation between v and n. In Japanese, a relation rel is represented by postpositions attached to n and the phrase composed of n and rel modifies v. Each triple is divided into two parts. The first is &lt;v, rel&gt; and the second is n. Then, we consider the conditional</context>
<context position="15037" citStr="Shinzato et al. 2008" startWordPosition="2382" endWordPosition="2385">bler divergence and is defined as follows: ( ( |) ||( |)) ( P n P n ⋅ ⋅ = 2 ∑P ⋅ 1 Finally, the distributional similarity between two words, n1 and n2, is defined as follows: sim(n1,n2) = 1−DTS(P(⋅ |n1) ||P(⋅ |n2)). This similarity assumes a value from 0 to 1. If two words are similar, the value will be close to 1; if two words have entirely different meanings, the value will be 0. In the experiment, we used 1,000,000 noun phrases and 100,000 pairs of verbs and postpositions to calculate the probability P(&lt;v, rel&gt;|n) from the dependency relations extracted from the above-mentioned Web corpus (Shinzato et al. 2008). The probabilities are computed using the following equation by modifying for the frequency using the log function: &lt; v rel n , , &gt;)) + &lt; v rel n &gt; f ( , , &lt; v rel D , &gt;∈ if f (&lt; v, rel, n &gt;) &gt; 0, where f(&lt;v, rel, n&gt;) is the frequency of a triple &lt;v, rel, n&gt; and D is the set defined as { &lt;v, rel &gt; | f(&lt;v, rel, n&gt;) &gt; 0 }. In the case of f(&lt;v, rel, n&gt;) = 0, P(&lt;v, rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the usual maximum likelihood estimation, we modified it as above. Although this might seems strange, this kind of modification is common in information retriev</context>
<context position="30821" citStr="Shinzato et al. 2008" startWordPosition="5068" endWordPosition="5071">ment) 5.4 me-ru semina Hon (book) (mail seminar) 3.9 gurometto shohin (grommet) (merchandise) 3.1 supuringubakku gensho (spring back) (phenomenon) 6.3 Investigation of the Extracted Relation Overlap with a Conventional Method We randomly sampled 300 hyponymy relations that were extracted correctly using the k similar words method exploiting CVD and investigated whether or not these relations can be extracted by the conventional method based on the lexico-syntactic pattern. The possible hyponymy relations were extracted using the pattern-based method (Ando et al. 2003) from the TSUBAKI corpus (Shinzato et al. 2008). From a comparison of these relations, we found only 57 common hyponymy relations. That is, the remaining 243 hyponymy relations were not included in the possible hyponymy relations. This result indicates that our method can acquire the hyponymy relations that cannot be extracted by the conventional pattern-based method. 6.4 Discussions We investigated the reason for the errors generated by the method of scoring using k similar words exploiting CVD. We conducted experiments on hypernym extraction targeting 694 words in the development data mentioned in Section 6.1. Among these, 286 relations </context>
</contexts>
<marker>Shinzato, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>K. Shinzato, D. Kawahara, C. Hashimoto and S. Kurohashi. 2008. A Large-Scale Web Data Collection as A Natural Language Processing Infrastructure. In the 6th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Learning Syntactic Patterns for Automatic Hypernym Discovery.</title>
<date>2005</date>
<tech>NIPS</tech>
<contexts>
<context position="5542" citStr="Snow et al. (2005)" startWordPosition="864" endWordPosition="867">e paper in Section 7. 2 Related Works Most previous researchers have relied on lexico-syntactic patterns for hyponymy acquisition. Lexico-syntactic patterns were first used by Hearst (1992). The patterns used by her included “NP0 such as NP,,” in which NP0 is a hypernym of NP,. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>R. Snow, D. Jurafsky and A. Y. Ng. 2005. Learning Syntactic Patterns for Automatic Hypernym Discovery. NIPS 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Semantic Taxonomy Induction from Heterogenous Evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="5780" citStr="Snow et al. 2006" startWordPosition="901" endWordPosition="904">P,,” in which NP0 is a hypernym of NP,. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities o</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, A. Y. Ng. 2006. Semantic Taxonomy Induction from Heterogenous Evidence. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pp. 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sumida</author>
<author>N Yoshinaga</author>
<author>K Torisawa</author>
</authors>
<title>Boosting Precision and Recall of Hyponymy Relation Acquisition from Hierarchical Layouts in Wikipedia.</title>
<date>2008</date>
<booktitle>In the 6th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="4400" citStr="Sumida et al. 2008" startWordPosition="678" endWordPosition="681">tic pattern-based method. In the remainder of this paper, we first intro1 In this paper, we use the term “word” for both “a single-word word” and “a multi-word word.” 929 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929–937, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Wikipedia relation database A hypernym is selected for each word independently. Wikipedia Selected from hypernyms in the Wikipedia relation database. Hyponymy relations are extracted using the layout information of Wikipedia. Wikipedia-based approach (Ponzetto et al. 2007 and Sumida et al. 2008) No direct co-occurrences of hypernym and hyponym in corpora are needed. : word Target word: Selected from the Web k similar words Figure 1: Overview of the proposed approach. hypernym : duce some related works in Section 2. Section 3 describes the Wikipedia relation database. Section 4 describes the distributional similarity calculated by the two methods. In Section 5, we describe a method to discover an appropriate hypernym for each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Related Works Most previous researchers have relied </context>
<context position="7303" citStr="Sumida et al. (2008)" startWordPosition="1145" endWordPosition="1148">ure 2: Drawbacks in existing approaches for hyponymy acquisition. al. (2005) used both a pattern-based approach and a clustering-based approach. The required amount of co-occurrences is significantly reduced due to class-based generalization processes. Note that these clustering-based methods obtain the same hypernym for all the words in a particular class. This causes a problem for selecting an appropriate hypernym for each word in the case when the granularity or the construction of the classes is incorrect. Figure 2 shows the drawbacks of the existing approaches. Ponzetto et al. (2007) and Sumida et al. (2008) proposed a method for acquiring hyponymy relations from Wikipedia. This Wikipedia-based approach can extract a large volume of hyponymy relations with high accuracy. However, it is also true that this approach does not account for many words that usually appear in Web documents; this could be because of the unbalanced topics in Wikipedia or merely because of the incomplete coverage of articles on Wikipedia. Our method can target words that frequently appear on the Web but are not included in the Wikipedia relation database, thus making the results of the Wikipedia-based approach richer and mo</context>
<context position="8685" citStr="Sumida et al. (2008)" startWordPosition="1367" endWordPosition="1370"> selected for all words in a class. hypernym word word word word word word Word Class 930 ty, which is computed based on the noun-verb dependency profiles on the Web. The use of distributional similarity resembles the clusteringbased approach; however, our method can select a hypernym for each word independently, and it does not suffer from class granularity mismatch or the low quality of classes. In addition, our approach exploits the hierarchical structures of the Wikipedia hypernym relations. 3 Wikipedia Relation Database Our Wikipedia relation database is based on the extraction method of Sumida et al. (2008). They proposed a method of automatically acquiring hyponymy relations by focusing on the hierarchical layout of articles on Wikipedia. By way of an example, Figure 3 shows part of the source code clipped from the article titled “Penguin.” An article has hierarchical structures composed of titles, sections, itemizations, etc. The entire article is divided into sections titled “Anatomy,” “Mating habits,” “Systematics and evolution,” “Penguins in popular culture,” and so on. The section “Systematics and evolution” has a subsection “Systematics,” which is further divided into “Aptenodytes,” “Eudy</context>
</contexts>
<marker>Sumida, Yoshinaga, Torisawa, 2008</marker>
<rawString>A. Sumida, N. Yoshinaga and K. Torisawa. 2008. Boosting Precision and Recall of Hyponymy Relation Acquisition from Hierarchical Layouts in Wikipedia. In the 6th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Terada</author>
<author>M Yoshida</author>
<author>H Nakagawa</author>
</authors>
<title>A Tool for Constructing a Synonym Dictionary using context Information.</title>
<date>2006</date>
<booktitle>In proceedings of IPSJ SIG Technical Reports, vol.2006 No.124,</booktitle>
<pages>87--94</pages>
<note>(In Japanese).</note>
<contexts>
<context position="15780" citStr="Terada et al. 2006" startWordPosition="2539" endWordPosition="2542">n , , &gt;)) + &lt; v rel n &gt; f ( , , &lt; v rel D , &gt;∈ if f (&lt; v, rel, n &gt;) &gt; 0, where f(&lt;v, rel, n&gt;) is the frequency of a triple &lt;v, rel, n&gt; and D is the set defined as { &lt;v, rel &gt; | f(&lt;v, rel, n&gt;) &gt; 0 }. In the case of f(&lt;v, rel, n&gt;) = 0, P(&lt;v, rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the usual maximum likelihood estimation, we modified it as above. Although this might seems strange, this kind of modification is common in information retrieval as a term weighing method (Manning et al. 1999) and it is also applied in some studies to yield better word similarities (Terada et al. 2006, Kazama et al. 2009). We also adopted this idea in this study. 4.2 Distributional Similarity Based on CVD Rooth et al. (1999) and Torisawa (2001) showed that EM-based clustering using verb-noun dependencies can produce semantically clean noun clusters. We exploit these EM-based clustering results as the smoothed contexts for noun n. In Torisawa’s model (2001), the probability of occurrence of the triple &lt;v, rel, n&gt; is defined as follows: P(&lt; v, rel, n &gt;) def ∑a∈ AP(&lt; v, rel &gt; |a)P(n |a)P(a) where a denotes a hidden class of &lt;v,rel&gt; and n. In this equation, the probabilities P(&lt;v,rel&gt;|a), P(n|</context>
</contexts>
<marker>Terada, Yoshida, Nakagawa, 2006</marker>
<rawString>A. Terada, M. Yoshida, H. Nakagawa. 2006. A Tool for Constructing a Synonym Dictionary using context Information. In proceedings of IPSJ SIG Technical Reports, vol.2006 No.124, pp. 87-94. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Torisawa</author>
</authors>
<title>An Unsupervised Method for Canonicalization of Japanese Postpositions.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium (NLPRS),</booktitle>
<pages>211--218</pages>
<contexts>
<context position="15926" citStr="Torisawa (2001)" startWordPosition="2566" endWordPosition="2567">et defined as { &lt;v, rel &gt; | f(&lt;v, rel, n&gt;) &gt; 0 }. In the case of f(&lt;v, rel, n&gt;) = 0, P(&lt;v, rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the usual maximum likelihood estimation, we modified it as above. Although this might seems strange, this kind of modification is common in information retrieval as a term weighing method (Manning et al. 1999) and it is also applied in some studies to yield better word similarities (Terada et al. 2006, Kazama et al. 2009). We also adopted this idea in this study. 4.2 Distributional Similarity Based on CVD Rooth et al. (1999) and Torisawa (2001) showed that EM-based clustering using verb-noun dependencies can produce semantically clean noun clusters. We exploit these EM-based clustering results as the smoothed contexts for noun n. In Torisawa’s model (2001), the probability of occurrence of the triple &lt;v, rel, n&gt; is defined as follows: P(&lt; v, rel, n &gt;) def ∑a∈ AP(&lt; v, rel &gt; |a)P(n |a)P(a) where a denotes a hidden class of &lt;v,rel&gt; and n. In this equation, the probabilities P(&lt;v,rel&gt;|a), P(n|a), and P(a) cannot be calculated directly because class a is not observed in a given corpus. The EM-based clustering method estimates these proba</context>
</contexts>
<marker>Torisawa, 2001</marker>
<rawString>K. Torisawa. 2001. An Unsupervised Method for Canonicalization of Japanese Postpositions. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium (NLPRS), pp. 211–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Torisawa</author>
<author>Stijn De Saeger</author>
<author>Y Kakizawa</author>
<author>J Kazama</author>
<author>M Murata</author>
<author>D Noguchi</author>
<author>A Sumida</author>
</authors>
<title>TORISHIKI-KAI, An Autogenerated Web Search Directory.</title>
<date>2008</date>
<booktitle>In Proceedings of the second international symposium on universal communication,</booktitle>
<pages>179--186</pages>
<marker>Torisawa, De Saeger, Kakizawa, Kazama, Murata, Noguchi, Sumida, 2008</marker>
<rawString>K. Torisawa, Stijn De Saeger, Y. Kakizawa, J. Kazama, M. Murata, D. Noguchi and A. Sumida. 2008. TORISHIKI-KAI, An Autogenerated Web Search Directory. In Proceedings of the second international symposium on universal communication, pp. 179–186, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>