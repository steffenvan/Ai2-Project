<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.6228" genericHeader="abstract">
CONVERSATIONAL IMPLICATURES IN INDIRECT REPLIES
</sectionHeader>
<author confidence="0.5812025">
Nancy Green
Sandra Carberry
</author>
<affiliation confidence="0.99599">
Department of Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.646461">
Newark, Delaware 19716, USA
</address>
<email confidence="0.997964">
email: green@cis.udel.edu, carberry@cis.udel.edu
</email>
<sectionHeader confidence="0.652356" genericHeader="keywords">
Abstract&apos;
</sectionHeader>
<bodyText confidence="0.999043736842105">
In this paper we present algorithms for the
interpretation and generation of a kind of particu-
larized conversational implicature occurring in cer-
tain indirect replies. Our algorithms make use of
discourse expectations, discourse plans, and dis-
course relations. The algorithms calculate implica-
tures of discourse units of one or more sentences.
Our approach has several advantages. First, by
taking discourse relations into account, it can cap-
ture a variety of implicatures not handled before.
Second, by treating implicatures of discourse units
which may consist of more than one sentence, it
avoids the limitations of a sentence-at-a-time ap-
proach. Third, by making use of properties of dis-
course which have been used in models of other dis-
course phenomena, our approach can be integrated
with those models. Also, our model permits the
same information to be used both in interpretation
and generation.
</bodyText>
<sectionHeader confidence="0.9991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.904305318181818">
In this paper we present algorithms for the
interpretation and generation of a certain kind of
conversational implicature occurring in the follow-
ing type of conversational exchange. One partici-
pant (Q) makes an illocutionary-level request&apos; to
be informed if p; the addressee (A), whose reply
may consist of more than one sentence, conversa-
tionally implicates one of these replies: p, that
there is support for p, or that there is support for
For example, in (1), assuming Q&apos;s utterance
has been interpreted as a request to be informed if
A went shopping, and given certain mutual beliefs
(e.g., that A&apos;s car breaking down would normally
be sufficient to prevent A from going shopping, and
1We wish to thank Kathy McCoy for her comments on
this paper.
2i.e., using Austin&apos;s (Austin, 1962) distinction between
locutionary and illocutionary force, Q&apos;s utterance is intended
to function as a request (although it need not have the gram-
matical form of a question)
that A&apos;s reply is coherent and cooperative), A&apos;s re-
ply is intended to convey, in part, a &apos;no&apos;.
</bodyText>
<listItem confidence="0.973132666666667">
(1) Q: Did you go shopping?
A: a. My car&apos;s not running.
b. The timing belt broke.
</listItem>
<bodyText confidence="0.996834909090909">
Such indirect replies satisfy the conditions
proposed by Grice and others (Grice, 1975;
Hirschberg, 1985; Sadock, 1978) for being classi-
fied as particularized conversational implicatures.
First, A&apos;s reply does not entail (in virtue of its
conventional meaning) that A did not go shopping.
Second, the putative implicature can be cancelled;
for example, it can be denied without the result
sounding inconsistent, as can be seen by consider-
ing the addition of (2) to the end of A&apos;s reply in
(1).
</bodyText>
<listItem confidence="0.950317">
(2) A: So I took the bus to the mall.
</listItem>
<bodyText confidence="0.999872866666667">
Third, it is reinforceable; A&apos;s reply in (1) could have
been preceded by an explicit &amp;quot;no&amp;quot; without destroy-
ing coherency or sounding redundant. Fourth, the
putative implicature is nondetachable; the same re-
ply would have been conveyed by an alternative re-
alization of (la) and (lb) (assuming that the al-
ternative did not convey a Manner-based implica-
ture). Fifth, Q and A must mutually believe that,
given the assumption that A&apos;s reply is cooperative,
and given certain shared background information,
Q can and will infer that by A&apos;s reply, A meant &apos;no&apos;.
This paper presents algorithms for calculating such
an inference from an indirect response and for gen-
erating an indirect response intended to carry such
an inference.
</bodyText>
<sectionHeader confidence="0.99786" genericHeader="method">
2 Solution
</sectionHeader>
<subsectionHeader confidence="0.967457">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.99905775">
Our algorithms are based upon three notions
from discourse research: discourse expectations,
discourse plans, and implicit relational propositions
in discourse.
</bodyText>
<page confidence="0.998824">
64
</page>
<bodyText confidence="0.999694973684211">
At certain points in a coherent conversa-
tion, the participants share certain expectations
(Reichman, 1984; Carberry, 1990) about what kind
of utterance is appropriate. In the type of exchange
we are studying, at the point after Q&apos;s contribu-
tion, the participants share the beliefs that Q has
requested to be informed if p and that the request
was appropriate; hence, they share the discourse
expectation that for A to be cooperative, he must
now say as much as he can truthfully say in regard
to the truth of p. (For convenience, we shall refer
to this expectation as Answer-YNQ(p).)
A discourse plan operator&apos; (Lambert &amp; Car-
berry, 1991) is a representation of a normal or con-
ventional way of accomplishing certain communica-
tive goals. Alternatively, a discourse plan operator
could be considered as a defeasible rule expressing
the typical (intended) effect(s) of a sequence of illo-
cutionary acts in a context in which certain appli-
cability conditions hold. These discourse plan op-
erators are mutually known by the conversational
participants, and can be used by a speaker to con-
struct a plan for achieving his communicative goals.
We provide a set of discourse plan operators which
can be used by A as part of a plan for fulfilling
Answer-YNQ(p).
Mann and Thompson (Mann &amp; Thompson,
1983; Mann St Thompson, 1987) have described
how the structure of a written text can be analyzed
in terms of certain implicit relational propositions
that may plausibly be attributed to the writer to
preserve the assumption of textual coherency.4 The
role of discourse relations in our approach is moti-
vated by the observation that direct replies may
occur as part of a discourse unit conveying a rela-
tional proposition. For example, in (3), (b) is pro-
vided as the (most salient) obstacle to the action
(going shopping) denied by (a);
</bodyText>
<listItem confidence="0.999456333333333">
(3) Q: Did you go shopping?
A:a. No,
b. my car&apos;s not running.
</listItem>
<bodyText confidence="0.6565745">
in (4), as an elaboration of the action (going shop-
ping) conveyed by (a);
</bodyText>
<listItem confidence="0.991737">
(4) Q: Did you go shopping?
A:a. Yes,
b. I bought some shoes.
and in (5), as a concession for failing to do the
action (washing the dishes) denied by (a).
(5) Q: Did you wash the dishes?
A:a. No,
b. (but) I scraped them.
</listItem>
<footnote confidence="0.985835833333333">
3in Pollack&apos;s terminology, a recipe-for-action (Pollack,
1988; Grosz Si Sidner, 1988)
4Although they did not study dialogue, they suggested
that it can be analyzed similarly. Also note that the rela-
tional predicates which we define are similar but not neces-
sarily identical to theirs.
</footnote>
<bodyText confidence="0.999973769230769">
Note that given appropriate context, the (b) replies
in (3) through (5) would be sufficient to conversa-
tionally implicate the corresponding direct replies.
This, we claim, is by virtue of the recognition of
the relational proposition that would be conveyed
by use of the direct reply and the (b) sentences.
Our strategy, then, is to generate/interpret
A&apos;s contribution using a set of discourse plan oper-
ators having the following properties: (1) if the ap-
plicability conditions hold, then executing the body
would generate a sequence of utterances intended to
implicitly convey a relational proposition R(p, q);
(2) the applicability conditions include the condi-
tion that R(p, q) is plausible in the discourse con-
text; (3) one of the goals is that Q believe that p,
where p is the content of the direct reply; and (4)
the step of the body which realizes the direct re-
ply can be omitted under certain conditions. Thus,
whenever the direct reply is omitted, it is neverthe-
less implicated as long as the intended relational
proposition can be recognized. Note that prop-
erty (2) requires a judgment that some relational
proposition is plausible. Such judgments will be de-
scribed using defeasible inference rules. The next
section describes our discourse relation inference
rules and discourse plan operators.
</bodyText>
<subsectionHeader confidence="0.969068666666667">
2.2 Discourse Plan Opera-
tors and Discourse Relation In-
ference Rules
</subsectionHeader>
<bodyText confidence="0.952491">
A typical reason for the failure of an agent&apos;s
attempt to achieve a domain goal is that the agent&apos;s
domain plan encountered an obstacle. Thus, we
give the rule in (6) for inferring a plausible discourse
relation of Obstacle.&apos;
If (i) coherentlyâ€”related(A,B), and
</bodyText>
<listItem confidence="0.822872666666667">
(ii) A is a proposition that an agent
failed to perform an action of
act type T, and
(iii) B is a proposition that
a) a normal applicability condition
of T did not hold, or
b) a normal precondition of T
failed, or
c) a normal step of T failed, or
d) the agent did not want to
achieve a normal goal of T,
then plausible(Obstacle(B,A)).
</listItem>
<bodyText confidence="0.9890464">
In (6) and in the rules to follow, &apos;coherently-
related(A,B)&apos; means that the propositions A and B
are assumed to be coherently related in the dis-
course. The terminology in clause (iii) is that of
the extended STRIPS planning formalism (Fikes
</bodyText>
<footnote confidence="0.930822333333333">
5For simplicity of exposition, (6) and the discourse rela-
tion inference rules to follow are stated in terms of the past;
we plan to extend their coverage of times.
</footnote>
<page confidence="0.999303">
65
</page>
<bodyText confidence="0.8080124">
&amp; Nilsson, 1971; Allen, 1979; Carberry, 1990; Lit-
man &amp; Allen, 1987).
Examples of A and B satisfying each of the
conditions in (6.iii) are given in (7a) - (7d), respec-
tively.
</bodyText>
<listItem confidence="0.8640174">
(7) [A] I didn&apos;t go shopping.
a. [B] The stores were closed.
b. [B] My car wasn&apos;t running.
c. [B] My car broke down on the way.
d. [B] I didn&apos;t want to buy anything.
</listItem>
<bodyText confidence="0.9993034">
The discourse plan operator given in (8) de-
scribes a standard way of performing a denial (ex-
emplified in (3)) that uses the discourse relation of
Obstacle given in (6). In (8), as in (6), A is a propo-
sition that an action of type T was not performed.
</bodyText>
<figure confidence="0.986321222222222">
(8) Deny (with Obstacle)
Applicability conditions:
1) S BMB plausible(Obstacle(B,A))
Body (unordered):
1) (optional) S inform H that A
2) Tell(S,H,B)
Goals:
1) H believe that A
2) H believe that Obstacle(B,A)
</figure>
<bodyText confidence="0.999079259259259">
In (8) (and in the discourse plan operators
to follow) the formalism described above is used;
&apos;S&apos; and &apos;H&apos; denote speaker and hearer, respectively;
`BMB&apos; is the one-sided mutual belief6 operator
(Clark &amp; Marshall, 1981); &apos;inform&apos; denotes an il-
locutionary act of informing; &apos;believe&apos; is Hintikka&apos;s
(Hintikka, 1962) belief operator; `Tell(S,H,B)&apos; is a
subgoal that can be achieved in a number of ways
(to be discussed shortly), including just by S in-
forming H that B; and steps of the body are not
ordered. (Note that to use these operators for gen-
eration of direct replies, we must provide a method
to determine a suitable ordering of the steps. Also,
although it is sufficient for interpretation to spec-
ify that step 1 is optional, for generation, more in-
formation is required to decide whether it can or
should be omitted; e.g., it should not be omitted if
S believes that H might believe that some relation
besides Obstacle is plausible in the context.7 These
are areas which we are currently investigating; for
related research, see section 3.)
Next, consider that a speaker may wish to
inform the hearer of an aspect of the plan by which
she accomplished a goal, if she believes that H may
not be aware of that aspect. Thus, we give the rule
in (9) for inferring a plausible discourse relation of
Elaboration.
</bodyText>
<footnote confidence="0.688625">
6&apos;S BMB p&apos; is to be read as &apos;S believes that it is mutually
believed between S and H that p&apos;.
7A related question, which has been studied by oth-
ers (Joshi, Webber &amp; Weischedel, 1984a; Joshi, Webber
Weischedel, 1984b), is in what situations is a speaker re-
quired to supply step 2 to avoid misleading the hearer?
</footnote>
<listItem confidence="0.863101931034483">
(9)
If (i) coherently-related(A,B), and
(ii) A is a proposition that an agent
performed some action of act type
T, and
(iii) B is a proposition that describes
information believed to be new to
H about
a) the satisfaction of a normal
applicability condition of T such
that its satisfaction is not
believed likely by H, or
b) the satisfaction of a normal
precondition of T such that its
satisfaction is not believed
likely by H, or
c) the success of a normal step of T,
or
d) the achievement of a normal goal
of T,
then plausible(Elaboration(B,A)).
Examples of A and B satisfying each of the
conditions in (9.iii) are given in (10a) - (10d), re-
spectively.
(10) [A] I went shopping today.
a. [B] I found a store that was open.
b. [B] I got my car fixed yesterday.
c. [B] I went to Macy&apos;s.
d. [B] I got running shoes.
</listItem>
<bodyText confidence="0.99712625">
The discourse plan operator given in (11) de-
scribes a standard way of performing an affirmation
(exemplified in (4)) that uses the discourse relation
of Elaboration.
</bodyText>
<figure confidence="0.884155666666667">
(11) Affirm (with Elaboration)
Applicability conditions:
1) S BMB plausible(Elaboration(B,A))
Body (unordered):
1) (optional) S inform H that A
2) Tell(S,H,B)
Goals:
1) H believe that A
2) H believe that Elaboration(B,A)
</figure>
<bodyText confidence="0.999215166666667">
Finally, note that a speaker may concede a
failure to achieve a certain goal while seeking credit
for the partial success of a plan to achieve that goal.
For example, the [B] utterances in (10) can be used
following (12) (or alone, in the right context) to
concede failure.
</bodyText>
<listItem confidence="0.503115">
(12) [A] I didn&apos;t go shopping today, but
</listItem>
<bodyText confidence="0.991564666666667">
Thus, the rule we give in (13) for inferring a
plausible discourse relation of Concession is similar
(but not identical) to (9).
</bodyText>
<page confidence="0.798067">
66
</page>
<bodyText confidence="0.900845923076923">
(13)
If (i) coherently-related(A,B), and
(ii) A is a proposition that an agent
failed to do an action of act
type T, and
(iii) B is a proposition that describes
a) the satisfaction of a normal
applicability condition of T, or
b) the satisfaction of a normal
precondition of T, or
c) the success of a normal step of T
Or
d) the achievement of a normal goal
of T, and
(iv) the achievement of the plan&apos;s
component in B may bring credit
to the agent,
then plausible(Concession(B,A)).
A discourse plan operator, Deny (with Con-
cession), can be given to describe another standard
way of performing a denial (exemplified in (5)).
This operator is similar to the one given in (8),
except with Concession in the place of Obstacle.
An interesting implication of the discourse
plan operators for Affirm (with Elaboration) and
Deny (with Concession) is that, in cases where the
speaker chooses not to perform the optional step
(i.e., chooses to omit the direct reply), it requires
that the intended discourse relation be inferred in
order to correctly interpret the indirect reply, since
either an affirmation or denial could be realized
with the same utterance. (Although (9) and (13)
contain some features that differentiate Elaboration
and Concession, other factors, such as intonation,
will be considered in future research.)
The next two discourse relations (described
in (14) and (16)) may be part of plan operators for
conveying a &apos;yes&apos; similar to Affirm (with Elabora-
tion).
</bodyText>
<listItem confidence="0.8054735">
(14)
If (i) coherently-related(A,B), and
(ii) A is a proposition that an agent
performed an action X, and
(iii) B is a proposition that normally
implies that the agent has a goal
G, and
(iv) X is a type of action occurring
as a normal part of a plan to
achieve G,
then plausible(
Motivate-Volitional-Action(B,A)).
(15) shows the use of Motivate-Volitional-Action
(MVA) in an indirect (affirmative) reply.
(15) Q: Did you close the window?
A: I was cold.
(16)
If (0 coherently-related(A,B), and
(ii) A is a proposition that an event E
occurred, and
(iii) B is a proposition that an event F
occurred, and
(iv) it is not believed that F followed
E, and
(v) F-type events normally cause
E-type events,
then plausible(Cause-Non-Volitional(B,A)).
(17) shows the use of Cause-Non-Volitional (CNV)
in an indirect (affirmative) reply.
(17) Q: Did you wake up very early?
</listItem>
<bodyText confidence="0.96633475">
A: The neighbor&apos;s dog was barking.
The discourse relation described in (18) may
be part of a plan operator similar to Deny (with
Obstacle) for conveying a &apos;no&apos;.
</bodyText>
<listItem confidence="0.929957416666667">
(18)
If (i) coherently-related(A,B), and
(ii) A is a proposition that an event E
did not occur, and
(iii) B is a proposition that an action F
was performed, and
(iv) F-type actions are normally
performed as a way of preventing
E-type events,
then plausible(Prevent(B,A)).
(19) shows the use of Prevent in an indirect denial.
(19) Q: Did you catch the flu?
</listItem>
<bodyText confidence="0.928451166666667">
A: I got a flu shot.
The discourse relation described in (20) can
be part of a plan operator similar to the others de-
scribed above except that one of the speaker&apos;s goals
is, rather than affirming or denying p, to provide
support for the belief that p.
(20)
If (i) coherently-related(A,B), and
(ii) B is a proposition that describes
a typical result of the situation
described in proposition A,
then plausible(Evidence(B,A)).
</bodyText>
<page confidence="0.998833">
67
</page>
<bodyText confidence="0.996225333333333">
Assuming an appropriate context, (21) is an
example of use of this relation to convey support,
i.e., to convey that it is likely that someone is home.
</bodyText>
<figure confidence="0.618212785714286">
(21) Q: Is anyone home?
A: The upstairs lights are on.
A similar rule could be defined for a relation used
to convey support against a belief.
2.3 Implicatures of Discourse Units
(23). Consider the similar dialogues in (22) and
(22) Q: Did you go shopping?
A:a. I had to take the bus.
b. (because) My car&apos;s not running.
c. (You see,) The timing belt broke.
(23) Q: Did you go shopping?
A:a. My car&apos;s not running.
b. The timing belt broke.
c. (So) I had to take the bus.
</figure>
<bodyText confidence="0.997060085714286">
First, note that although the order of the sentences
realizing A&apos;s reply varies in (22) and (23), A&apos;s over-
all discourse purpose in both is to convey a &apos;yes&apos;.
Second, note that it is necessary to have a rule so
that if A&apos;s reply consists solely of (22a) (=23c), an
implicated &apos;yes&apos; is derived; and if it consists solely
of (22b) (=23a), an implicated &apos;no&apos;.
In existing sentence-at-a-time models of cal-
culating implicatures (Gazdar, 1979; Hirschberg,
1985), processing (22a) would result in an impli-
cated &apos;yes&apos; being added to the context, which would
successfully block the addition of an implicated &apos;no&apos;
on processing (22b). However, processing (23a)
would result in a putatively implicated &apos;no&apos; be-
ing added to the context (incorrectly attributing
a fleeting intention of A to convey a `no&apos;); then, on
processing (23c) the conflicting but intended &apos;yes&apos;
would be blocked by context, giving an incorrect
result. Thus, a sentence-at-a-time model must pre-
dict when (23c) should override (23a). Also, in that
model, processing (23) requires &amp;quot;extra effort&amp;quot;, a
nonmonotonic revision of belief not needed to han-
dle (22); yet (23) seems more like (22) than a case
in which a speaker actually changes her mind.
In our model, since implicatures correspond
to goals of inferred or constructed hierarchical
plans, we avoid this problem. (22A) and (23A) both
correspond to step 2 of Affirm (with Elaboration),
Tell(S,H,B); several different discourse plan opera-
tors can be used to construct a plan for this Tell
action. For example, one operator for Tell(S,H,B)
is given below in (24); the operator represents that
in telling H that B, where B describes an agent&apos;s
volitional action, a speaker may provide motivation
for the agent&apos;s action.
</bodyText>
<figure confidence="0.992222545454545">
(24) Tell(S,H,p)
Applicability Conditions:
1) S BMB plausible(
Motivate-Volitional-Action(q,p))
Body (unordered):
1) Tell(S,H,q)
2) S inform H that p
Goals:
1) H believe that p
2) H believe that
Motivate-Volitional-Action(q,p)
</figure>
<bodyText confidence="0.99967425">
(We are currently investigating, in generation,
when to use an operator such as (24). For ex-
ample, a speaker might want to use (24) in case
he thinks that the hearer might doubt the truth
of B unless he knows of the motivation.) Thus,
(22a)/(23c) corresponds to step 2 of (24); (22b) -
(22c), as well as (23a) - (23b), correspond to step
1. Another operator for Tell(S,H,p) could represent
that in telling H that p, a speaker may provide the
cause of an event; i.e., the operator would be like
(24) but with Cause-Non-Volitional as the discourse
relation. This operator could be used to decom-
pose (22b) - (22c)/(23a) - (23b). The structure pro-
posed for (22A)/(23A) is illustrated in Figure 1.8
Linear precedence in the tree does not necessarily
represent narrative order; one way of ordering the
two nodes directly dominated by Tell(MVA) gives
(22A), another gives (23A). (Narrative order in the
generation of indirect replies is an area we are cur-
rently investigating also; for related research, see
section 3.)
Note that Deny (with Obstacle) can not be
used to generate/interpret (22A) or (23A) since
its body can not be expanded to account for
(22a)/(23c).8 Thus, the correct implicatures can
be derived without attributing spurious intentions
to A, and without requiring cancellation of spurious
implicatures.
</bodyText>
<footnote confidence="0.8177176">
8To use the terminology of (Moore Sz Paris, 1989; Moore
&amp; Paris, 1988), the labelled arcs represent satellites, and the
unlabelled arcs nucleii. However, note that in their model, a
nucleus can not be optional. This differs from our approach,
in that we have shown that direct replies are optional in
contexts such as those described by plan operators such as
Affirm (with Elaboration).
8Determining this requires that the end of the relevant
discourse unit be marked/recognized by cue phrases, into-
nation, or shift of focus; we plan to investigate this problem.
</footnote>
<page confidence="0.997872">
68
</page>
<figure confidence="0.98905825">
Affirm (with Elaboration) 1 (Elaboration)
I went shopping Tell (MVA)
(Motivate-Volitional-Action)
Tell (CNV)
I had to take the bus
1 (Cause-Non-Volitional)
My car&apos;s not running
The tim ng belt broke
</figure>
<figureCaption confidence="0.9998">
Figure 1. A Sample Discourse Structure
</figureCaption>
<subsectionHeader confidence="0.996568">
2.4 Algorithms
</subsectionHeader>
<bodyText confidence="0.999569333333333">
Generation and interpretation algorithms
are given in (25) and (26), respectively. They
presuppose that the plausible discourse relation is
available.10 The generation algorithm assumes as
given an illocutionary-level representation of A&apos;s
communicative goals.&amp;quot;
</bodyText>
<listItem confidence="0.930973875">
(25) Generation of indirect reply:
1. Select discourse plan operator: Select
from the Answer-YNQ(p) plan operators
all those for which
a) the applicability conditions hold,
and
b) the goals include S&apos;s goals.
2. If more than one operator was selected
</listItem>
<bodyText confidence="0.996911">
in step 1, then choose one. Also,
determine step ordering and whether it
is necessary to include optional steps.
(We are currently investigating how
these choices are determined.)
</bodyText>
<listItem confidence="0.718452">
3. Construct a plan from the chosen
operator and execute it.
</listItem>
<bodyText confidence="0.772835166666667">
10 We plan to implement an inference mechanism for the
discourse relation inference rules.
11 Note that A&apos;s goals depend, in part, on the illocutionary-
level representation of Q&apos;s request. We assume that an
analysis, such as provided in (Perrault &amp; Allen, 1980), is
available.
</bodyText>
<listItem confidence="0.730947882352941">
(26) Interpretation of indirect reply:
1. Infer discourse plan: Select from the
Answer-YNQ(p) plan operators all those
for which
a) the second step of the body matches
S&apos;s contribution, and
b) the applicability conditions hold,
and
c) it is mutually believed that the
goals are consistent with S&apos;s goals.
2. If more than one operator was selected
in step 1, then choose one. (We are
currently investigating what factors
are involved in this choice. Of course,
the utterance may be ambiguous.)
3. Ascribe to S the goal(s) of the chosen
plan operator.
</listItem>
<sectionHeader confidence="0.901406" genericHeader="method">
3 Comparison to Past Re-
search
</sectionHeader>
<bodyText confidence="0.998636333333333">
Most previous work in computational or for-
mal linguistics on particularized conversational im-
plicature (Green, 1990; Horacek, 1991; Joshi,
Webber &amp; Weischedel, 1984a; Joshi, Webber &amp;
Weischedel, 1984b; Reiter, 1990; Wainer &amp; Maida,
1991) has treated other kinds of implicature than
we consider here. Hirschberg (Hirschberg, 1985)
provided licensing rules making use of mutual be-
liefs about salient partial orderings of entities in
</bodyText>
<page confidence="0.998435">
69
</page>
<bodyText confidence="0.984455796296297">
the discourse context to calculate the scalar im-
plicatures of an utterance. Our model is similar
to Hirschberg&apos;s in that both rely on the represen-
tation of aspects of context to generate implica-
tures, and our discourse plan operators are roughly
analogous in function to her licensing rules. How-
ever, her model makes no use of discourse relations.
Therefore, it does not handle several kinds of indi-
rect replies which we treat. For example, although
A in (27) could be analyzed as scalar implicating
a &apos;no&apos; in some contexts, Hirschberg&apos;s model could
not account for the use of A in other contexts as an
elaboration (of how A managed to read chapter 1)
intended to convey a &apos;yes&apos;.12
(27) Q: Did you read the first chapter?
A: I took it to the beach with me.
Furthermore, Hirschberg provided no computa-
tional method for determining the salient partially
ordered set in a context. Also, in her model, impli-
catures are calculated one sentence at a time, which
has the potential problems described above.
Lascarides, Asher, and Oberlander
(Lascarides &amp; Asher, 1991; Lascarides &amp; OberIan-
der, 1992) described the interpretation and gen-
eration of temporal implicatures. Although that
type of implicature (being Manner-based) is some-
what different from what we are studying, we have
adopted their technique of providing defeasible in-
ference rules for inferring discourse relations.
In philosophy, Thomason (Thomason, 1990)
suggested that discourse expectations play a role in
some implicatures. McCafferty (McCafferty, 1987)
argued that interpreting certain implicated replies
requires domain plan reconstruction. However, he
did not provide a computational method for inter-
preting implicatures. Also, his proposed technique
can not handle many types of indirect replies. For
example, it can not account for the implicated nega-
tive replies in (1) and (5), since their interpretation
involves reconstructing domain plans that were not
executed successfully; it can not account for the im-
plicated affirmative reply in (17), in which no rea-
soning about domain plans is involved; and it can
not account for implicated replies conveying sup-
port for or against a belief, as in (21). Lastly, his
approach cannot handle implicatures conveyed by
discourse units containing more than one sentence.
Finally, note that our approach of including
rhetorical goals in discourse plans is modelled on
the work of Hovy (Hovy, 1988) and Moore and
Paris (Moore &amp; Paris, 1989; Moore &amp; Paris, 1988),
who used rhetorical plans to generate coherent text.
12 The two intended interpretations are marked by different
intonations.
</bodyText>
<sectionHeader confidence="0.999303" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99997988">
We have provided algorithms for the inter-
pretation/generation of a type of reply involving
a highly context-dependent conversational implica-
ture. Our algorithms make use of discourse ex-
pectations, discourse plans, and discourse relations.
The algorithms calculate implicatures of discourse
units of one or more sentences. Our approach has
several advantages. First, by taking discourse rela-
tions into account, it can capture a variety of im-
plicatures not handled before. Second, by treating
implicatures of discourse units which may consist of
more than one sentence, it avoids the limitations of
a sentence-at-a-time approach. Third, by making
use of properties of discourse which have been used
in models of other discourse phenomena, our ap-
proach can be integrated with those models. Also,
our model permits the same information to be used
both in interpretation and in generation.
Our current and anticipated research in-
cludes: refining and implementing our algorithms
(including developing an inference mechanism for
the discourse relation rules); extending our model
to other types of implicatures; and investigating the
integration of our model into general interpretation
and generation frameworks.
</bodyText>
<sectionHeader confidence="0.998069" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995526703703704">
Allen, James F. (1979). A Plan-Based Approach to
Speech Act Recognition. PhD thesis, University
of Toronto, Toronto, Ontario, Canada.
Austin, J. L. (1962). How To Do Things With
Words. Cambridge, Massachusetts: Harvard
University Press.
Carberry, Sandra (1990). Plan Recognition in Nat-
ural Language Dialogue. Cambridge, Mas-
sachusetts: MIT Press.
Clark, H. &amp; Marshall, C. (1981). Definite refer-
ence and mutual knowledge. In A. K. Joshi,
B. Webber, &amp; I. Sag (Eds.), Elements of dis-
course understanding. Cambridge: Cambridge
University Press.
Fikes, R. E. &amp; Nilsson, N. J. (1971). Strips: A new
approach to the application of theorem proving
to problem solving. Artificial Intelligence, 2,
189-208.
Gazdar, G. (1979). Pragmatics: Implicature, Pre-
supposition, and Logical Form. New York:
Academic Press.
Green, Nancy L. (1990). Normal state impli-
cature. In Proceedings of the 28th Annual
Meeting, Pittsburgh. Association for Compu-
tational Linguistics.
Grice, H. Paul (1975). Logic and conversation. In
Cole, P. &amp; Morgan, J. L. (Eds.), Syntax and
</reference>
<page confidence="0.967652">
70
</page>
<reference confidence="0.999573821052631">
Semantics III: Speech Acts, (pp. 41-58)., New
York. Academic Press.
Grosz, Barbara &amp; Sidner, Candace (1988). Plans
for discourse. In P. Cohen, J. Morgan, &amp;
M. Pollack (Eds.), Intentions in Communica-
tion. MIT Press.
Hintikka, J. (1962). Knowledge and Belief Ithaca:
Cornell University Press.
Hirschberg, Julia B. (1985). A Theory of Scalar
Inzplicature. PhD thesis, University of Penn-
sylvania.
Horacek, Helmut (1991). Exploiting conversa-
tional implicature for generating concise expla-
nations. In Proceedings. European Association
for Computational Linguistics.
Hovy, Eduard H. (1988). Planning coherent multi-
sentential text. In Proceedings of the 26th An-
nual Meeting, (pp. 163-169). Association for
Computational Linguistics.
Joshi, Aravind, Webber, Bonnie, &amp; Weischedel,
Ralph (1984a). Living up to expectations:
Computing expert responses. In Proceedings
of the Fourth National Conference on Artificial
Intelligence, (pp. 169-175)., Austin, Texas.
Joshi, Aravind, Webber, Bonnie, &amp; Weischedel,
Ralph (1984b). Preventing false inferences.
In Proceedings of Coling84, (pp. 134-138).,
Stanford University, California. Association for
Computational Linguistics.
Lambert, Lynn &amp; Carberry, Sandra (1991). A tri-
partite plan-based model of dialogue. In Pro-
ceedings of the 29th Annual Meeting, (pp. 47-
54). Association for Computational Linguis-
tics.
Lascarides, Alex &amp; Asher, Nicholas (1991). Dis-
course relations and defeasible knowledge. In
Proceedings of the 29th Annual Meeting, (pp.
55-62). Association for Computational Lin-
guistics.
Lascarides, Alex &amp; Oberlander, Jon (1992). Tem-
poral coherence and defeasible knowledge.
Theoretical Linguistics, 18.
Litman, Diane &amp; Allen, James (1987). A plan
recognition model for subdialogues in conver-
sation. Cognitive Science, 11, 163-200.
Mann, William C. &amp; Thompson, Sandra A. (1983).
Relational propositions in discourse. Technical
Report ISI/RR-83-115, ISI/USC.
Mann, William C. &amp; Thompson, Sandra A. (1987).
Rhetorical structure theory: Toward a func-
tional theory of text organization. Text, 8(3),
167-182.
McCafferty, Andrew S. (1987). Reasoning about Im-
plicature: a Plan-Based Approach. PhD thesis,
University of Pittsburgh.
Moore, Johanna D. &amp; Paris, Cecile (1989). Plan-
ning text for advisory dialogues. In Proceed-
ings of the 27th Annual Meeting, University of
British Columbia, Vancouver. Association of
Computational Linguistics.
Moore, Johanna D. &amp; Paris, Cecile L. (1988). Con-
structing coherent text using rhetorical rela-
tions. In Proc. 10th Annual Conference. Cog-
nitive Science Society.
Perrault, Raymond &amp; Allen, James (1980). A
plan-based analysis of indirect speech acts.
American Journal of Computational Linguis-
tics, 6(3-4), 167-182.
Pollack, Martha (1988). Plans as complex men-
tal attitudes. In P. Cohen, J. Morgan, &amp;
M. Pollack (Eds.), Intentions in Communica-
tion. MIT Press.
Reichman, Rachel (1984). Extended person-
machine interface. Artificial Intelligence, 22,
157-218.
Reiter, Ehud (1990). The computational complex-
ity of avoiding conversational implicatures. In
Proceedings of the 28th Annual Meeting, (pp.
97-104)., Pittsburgh. Association for Compu-
tational Linguistics.
Sadock, Jerrold M. (1978). On testing for conversa-
tional implicature. In Cole, P. &amp; Morgan, J. L.
(Eds.), Syntax and Semantics, (pp. 281-297).,
N.Y. Academic Press.
Thomason, Richmond H. (1990). Accommoda-
tion, meaning, and implicature: Interdisci-
plinary foundations for pragmatics. In P. Co-
hen, J. Morgan, &amp; M. Pollack (Eds.), In-
tentions in Communication. Cambridge, Mas-
sachusetts: MIT Press.
Wainer, Jacques &amp; Maida, Anthony (1991). Good
and bad news in formalizing generalized impli-
catures. In Proceedings of the Sixteenth An-
nual Meeting of the Berkeley Linguistics Soci-
ety, (pp. 66-71)., Berkeley, California.
</reference>
<page confidence="0.99914">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930367">
<title confidence="0.995454">CONVERSATIONAL IMPLICATURES IN INDIRECT REPLIES</title>
<author confidence="0.997648">Nancy Green Sandra Carberry</author>
<affiliation confidence="0.9999075">Department of Computer and Information Sciences University of Delaware</affiliation>
<address confidence="0.999936">Newark, Delaware 19716, USA</address>
<email confidence="0.999944">green@cis.udel.edu,carberry@cis.udel.edu</email>
<abstract confidence="0.9969039">In this paper we present algorithms for the interpretation and generation of a kind of particularized conversational implicature occurring in certain indirect replies. Our algorithms make use of discourse expectations, discourse plans, and discourse relations. The algorithms calculate implicatures of discourse units of one or more sentences. Our approach has several advantages. First, by taking discourse relations into account, it can capture a variety of implicatures not handled before. Second, by treating implicatures of discourse units which may consist of more than one sentence, it avoids the limitations of a sentence-at-a-time approach. Third, by making use of properties of discourse which have been used in models of other discourse phenomena, our approach can be integrated with those models. Also, our model permits the same information to be used both in interpretation and generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>A Plan-Based Approach to Speech Act Recognition.</title>
<date>1979</date>
<tech>PhD thesis,</tech>
<institution>University of Toronto,</institution>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="8597" citStr="Allen, 1979" startWordPosition="1422" endWordPosition="1423"> not hold, or b) a normal precondition of T failed, or c) a normal step of T failed, or d) the agent did not want to achieve a normal goal of T, then plausible(Obstacle(B,A)). In (6) and in the rules to follow, &apos;coherentlyrelated(A,B)&apos; means that the propositions A and B are assumed to be coherently related in the discourse. The terminology in clause (iii) is that of the extended STRIPS planning formalism (Fikes 5For simplicity of exposition, (6) and the discourse relation inference rules to follow are stated in terms of the past; we plan to extend their coverage of times. 65 &amp; Nilsson, 1971; Allen, 1979; Carberry, 1990; Litman &amp; Allen, 1987). Examples of A and B satisfying each of the conditions in (6.iii) are given in (7a) - (7d), respectively. (7) [A] I didn&apos;t go shopping. a. [B] The stores were closed. b. [B] My car wasn&apos;t running. c. [B] My car broke down on the way. d. [B] I didn&apos;t want to buy anything. The discourse plan operator given in (8) describes a standard way of performing a denial (exemplified in (3)) that uses the discourse relation of Obstacle given in (6). In (8), as in (6), A is a proposition that an action of type T was not performed. (8) Deny (with Obstacle) Applicabilit</context>
</contexts>
<marker>Allen, 1979</marker>
<rawString>Allen, James F. (1979). A Plan-Based Approach to Speech Act Recognition. PhD thesis, University of Toronto, Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Austin</author>
</authors>
<title>How To Do Things With Words.</title>
<date>1962</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="1940" citStr="Austin, 1962" startWordPosition="298" endWordPosition="299">change. One participant (Q) makes an illocutionary-level request&apos; to be informed if p; the addressee (A), whose reply may consist of more than one sentence, conversationally implicates one of these replies: p, that there is support for p, or that there is support for For example, in (1), assuming Q&apos;s utterance has been interpreted as a request to be informed if A went shopping, and given certain mutual beliefs (e.g., that A&apos;s car breaking down would normally be sufficient to prevent A from going shopping, and 1We wish to thank Kathy McCoy for her comments on this paper. 2i.e., using Austin&apos;s (Austin, 1962) distinction between locutionary and illocutionary force, Q&apos;s utterance is intended to function as a request (although it need not have the grammatical form of a question) that A&apos;s reply is coherent and cooperative), A&apos;s reply is intended to convey, in part, a &apos;no&apos;. (1) Q: Did you go shopping? A: a. My car&apos;s not running. b. The timing belt broke. Such indirect replies satisfy the conditions proposed by Grice and others (Grice, 1975; Hirschberg, 1985; Sadock, 1978) for being classified as particularized conversational implicatures. First, A&apos;s reply does not entail (in virtue of its conventional</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>Austin, J. L. (1962). How To Do Things With Words. Cambridge, Massachusetts: Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Carberry</author>
</authors>
<title>Plan Recognition in Natural Language Dialogue.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="3852" citStr="Carberry, 1990" startWordPosition="610" endWordPosition="611">on that A&apos;s reply is cooperative, and given certain shared background information, Q can and will infer that by A&apos;s reply, A meant &apos;no&apos;. This paper presents algorithms for calculating such an inference from an indirect response and for generating an indirect response intended to carry such an inference. 2 Solution 2.1 Overview Our algorithms are based upon three notions from discourse research: discourse expectations, discourse plans, and implicit relational propositions in discourse. 64 At certain points in a coherent conversation, the participants share certain expectations (Reichman, 1984; Carberry, 1990) about what kind of utterance is appropriate. In the type of exchange we are studying, at the point after Q&apos;s contribution, the participants share the beliefs that Q has requested to be informed if p and that the request was appropriate; hence, they share the discourse expectation that for A to be cooperative, he must now say as much as he can truthfully say in regard to the truth of p. (For convenience, we shall refer to this expectation as Answer-YNQ(p).) A discourse plan operator&apos; (Lambert &amp; Carberry, 1991) is a representation of a normal or conventional way of accomplishing certain communi</context>
<context position="8613" citStr="Carberry, 1990" startWordPosition="1424" endWordPosition="1425"> b) a normal precondition of T failed, or c) a normal step of T failed, or d) the agent did not want to achieve a normal goal of T, then plausible(Obstacle(B,A)). In (6) and in the rules to follow, &apos;coherentlyrelated(A,B)&apos; means that the propositions A and B are assumed to be coherently related in the discourse. The terminology in clause (iii) is that of the extended STRIPS planning formalism (Fikes 5For simplicity of exposition, (6) and the discourse relation inference rules to follow are stated in terms of the past; we plan to extend their coverage of times. 65 &amp; Nilsson, 1971; Allen, 1979; Carberry, 1990; Litman &amp; Allen, 1987). Examples of A and B satisfying each of the conditions in (6.iii) are given in (7a) - (7d), respectively. (7) [A] I didn&apos;t go shopping. a. [B] The stores were closed. b. [B] My car wasn&apos;t running. c. [B] My car broke down on the way. d. [B] I didn&apos;t want to buy anything. The discourse plan operator given in (8) describes a standard way of performing a denial (exemplified in (3)) that uses the discourse relation of Obstacle given in (6). In (8), as in (6), A is a proposition that an action of type T was not performed. (8) Deny (with Obstacle) Applicability conditions: 1)</context>
</contexts>
<marker>Carberry, 1990</marker>
<rawString>Carberry, Sandra (1990). Plan Recognition in Natural Language Dialogue. Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Clark</author>
<author>C Marshall</author>
</authors>
<title>Definite reference and mutual knowledge. In</title>
<date>1981</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9587" citStr="Clark &amp; Marshall, 1981" startWordPosition="1598" endWordPosition="1601">andard way of performing a denial (exemplified in (3)) that uses the discourse relation of Obstacle given in (6). In (8), as in (6), A is a proposition that an action of type T was not performed. (8) Deny (with Obstacle) Applicability conditions: 1) S BMB plausible(Obstacle(B,A)) Body (unordered): 1) (optional) S inform H that A 2) Tell(S,H,B) Goals: 1) H believe that A 2) H believe that Obstacle(B,A) In (8) (and in the discourse plan operators to follow) the formalism described above is used; &apos;S&apos; and &apos;H&apos; denote speaker and hearer, respectively; `BMB&apos; is the one-sided mutual belief6 operator (Clark &amp; Marshall, 1981); &apos;inform&apos; denotes an illocutionary act of informing; &apos;believe&apos; is Hintikka&apos;s (Hintikka, 1962) belief operator; `Tell(S,H,B)&apos; is a subgoal that can be achieved in a number of ways (to be discussed shortly), including just by S informing H that B; and steps of the body are not ordered. (Note that to use these operators for generation of direct replies, we must provide a method to determine a suitable ordering of the steps. Also, although it is sufficient for interpretation to specify that step 1 is optional, for generation, more information is required to decide whether it can or should be omit</context>
</contexts>
<marker>Clark, Marshall, 1981</marker>
<rawString>Clark, H. &amp; Marshall, C. (1981). Definite reference and mutual knowledge. In A. K. Joshi, B. Webber, &amp; I. Sag (Eds.), Elements of discourse understanding. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Fikes</author>
<author>N J Nilsson</author>
</authors>
<title>Strips: A new approach to the application of theorem proving to problem solving.</title>
<date>1971</date>
<journal>Artificial Intelligence,</journal>
<volume>2</volume>
<pages>189--208</pages>
<marker>Fikes, Nilsson, 1971</marker>
<rawString>Fikes, R. E. &amp; Nilsson, N. J. (1971). Strips: A new approach to the application of theorem proving to problem solving. Artificial Intelligence, 2, 189-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Pragmatics: Implicature, Presupposition, and Logical Form.</title>
<date>1979</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="16973" citStr="Gazdar, 1979" startWordPosition="2885" endWordPosition="2886">use) My car&apos;s not running. c. (You see,) The timing belt broke. (23) Q: Did you go shopping? A:a. My car&apos;s not running. b. The timing belt broke. c. (So) I had to take the bus. First, note that although the order of the sentences realizing A&apos;s reply varies in (22) and (23), A&apos;s overall discourse purpose in both is to convey a &apos;yes&apos;. Second, note that it is necessary to have a rule so that if A&apos;s reply consists solely of (22a) (=23c), an implicated &apos;yes&apos; is derived; and if it consists solely of (22b) (=23a), an implicated &apos;no&apos;. In existing sentence-at-a-time models of calculating implicatures (Gazdar, 1979; Hirschberg, 1985), processing (22a) would result in an implicated &apos;yes&apos; being added to the context, which would successfully block the addition of an implicated &apos;no&apos; on processing (22b). However, processing (23a) would result in a putatively implicated &apos;no&apos; being added to the context (incorrectly attributing a fleeting intention of A to convey a `no&apos;); then, on processing (23c) the conflicting but intended &apos;yes&apos; would be blocked by context, giving an incorrect result. Thus, a sentence-at-a-time model must predict when (23c) should override (23a). Also, in that model, processing (23) requires</context>
</contexts>
<marker>Gazdar, 1979</marker>
<rawString>Gazdar, G. (1979). Pragmatics: Implicature, Presupposition, and Logical Form. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy L Green</author>
</authors>
<title>Normal state implicature.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Pittsburgh.</location>
<contexts>
<context position="22350" citStr="Green, 1990" startWordPosition="3742" endWordPosition="3743">er-YNQ(p) plan operators all those for which a) the second step of the body matches S&apos;s contribution, and b) the applicability conditions hold, and c) it is mutually believed that the goals are consistent with S&apos;s goals. 2. If more than one operator was selected in step 1, then choose one. (We are currently investigating what factors are involved in this choice. Of course, the utterance may be ambiguous.) 3. Ascribe to S the goal(s) of the chosen plan operator. 3 Comparison to Past Research Most previous work in computational or formal linguistics on particularized conversational implicature (Green, 1990; Horacek, 1991; Joshi, Webber &amp; Weischedel, 1984a; Joshi, Webber &amp; Weischedel, 1984b; Reiter, 1990; Wainer &amp; Maida, 1991) has treated other kinds of implicature than we consider here. Hirschberg (Hirschberg, 1985) provided licensing rules making use of mutual beliefs about salient partial orderings of entities in 69 the discourse context to calculate the scalar implicatures of an utterance. Our model is similar to Hirschberg&apos;s in that both rely on the representation of aspects of context to generate implicatures, and our discourse plan operators are roughly analogous in function to her licens</context>
</contexts>
<marker>Green, 1990</marker>
<rawString>Green, Nancy L. (1990). Normal state implicature. In Proceedings of the 28th Annual Meeting, Pittsburgh. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<pages>41--58</pages>
<publisher>Academic Press.</publisher>
<location>New York.</location>
<contexts>
<context position="2375" citStr="Grice, 1975" startWordPosition="373" endWordPosition="374">aking down would normally be sufficient to prevent A from going shopping, and 1We wish to thank Kathy McCoy for her comments on this paper. 2i.e., using Austin&apos;s (Austin, 1962) distinction between locutionary and illocutionary force, Q&apos;s utterance is intended to function as a request (although it need not have the grammatical form of a question) that A&apos;s reply is coherent and cooperative), A&apos;s reply is intended to convey, in part, a &apos;no&apos;. (1) Q: Did you go shopping? A: a. My car&apos;s not running. b. The timing belt broke. Such indirect replies satisfy the conditions proposed by Grice and others (Grice, 1975; Hirschberg, 1985; Sadock, 1978) for being classified as particularized conversational implicatures. First, A&apos;s reply does not entail (in virtue of its conventional meaning) that A did not go shopping. Second, the putative implicature can be cancelled; for example, it can be denied without the result sounding inconsistent, as can be seen by considering the addition of (2) to the end of A&apos;s reply in (1). (2) A: So I took the bus to the mall. Third, it is reinforceable; A&apos;s reply in (1) could have been preceded by an explicit &amp;quot;no&amp;quot; without destroying coherency or sounding redundant. Fourth, the </context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Grice, H. Paul (1975). Logic and conversation. In Cole, P. &amp; Morgan, J. L. (Eds.), Syntax and Semantics III: Speech Acts, (pp. 41-58)., New York. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Sidner</author>
</authors>
<title>Plans for discourse. In</title>
<date>1988</date>
<publisher>MIT Press.</publisher>
<marker>Grosz, Sidner, 1988</marker>
<rawString>Grosz, Barbara &amp; Sidner, Candace (1988). Plans for discourse. In P. Cohen, J. Morgan, &amp; M. Pollack (Eds.), Intentions in Communication. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hintikka</author>
</authors>
<title>Knowledge and Belief Ithaca:</title>
<date>1962</date>
<publisher>Cornell University Press.</publisher>
<contexts>
<context position="9681" citStr="Hintikka, 1962" startWordPosition="1613" endWordPosition="1614">ven in (6). In (8), as in (6), A is a proposition that an action of type T was not performed. (8) Deny (with Obstacle) Applicability conditions: 1) S BMB plausible(Obstacle(B,A)) Body (unordered): 1) (optional) S inform H that A 2) Tell(S,H,B) Goals: 1) H believe that A 2) H believe that Obstacle(B,A) In (8) (and in the discourse plan operators to follow) the formalism described above is used; &apos;S&apos; and &apos;H&apos; denote speaker and hearer, respectively; `BMB&apos; is the one-sided mutual belief6 operator (Clark &amp; Marshall, 1981); &apos;inform&apos; denotes an illocutionary act of informing; &apos;believe&apos; is Hintikka&apos;s (Hintikka, 1962) belief operator; `Tell(S,H,B)&apos; is a subgoal that can be achieved in a number of ways (to be discussed shortly), including just by S informing H that B; and steps of the body are not ordered. (Note that to use these operators for generation of direct replies, we must provide a method to determine a suitable ordering of the steps. Also, although it is sufficient for interpretation to specify that step 1 is optional, for generation, more information is required to decide whether it can or should be omitted; e.g., it should not be omitted if S believes that H might believe that some relation besi</context>
</contexts>
<marker>Hintikka, 1962</marker>
<rawString>Hintikka, J. (1962). Knowledge and Belief Ithaca: Cornell University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia B Hirschberg</author>
</authors>
<title>A Theory of Scalar Inzplicature.</title>
<date>1985</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2393" citStr="Hirschberg, 1985" startWordPosition="375" endWordPosition="376">uld normally be sufficient to prevent A from going shopping, and 1We wish to thank Kathy McCoy for her comments on this paper. 2i.e., using Austin&apos;s (Austin, 1962) distinction between locutionary and illocutionary force, Q&apos;s utterance is intended to function as a request (although it need not have the grammatical form of a question) that A&apos;s reply is coherent and cooperative), A&apos;s reply is intended to convey, in part, a &apos;no&apos;. (1) Q: Did you go shopping? A: a. My car&apos;s not running. b. The timing belt broke. Such indirect replies satisfy the conditions proposed by Grice and others (Grice, 1975; Hirschberg, 1985; Sadock, 1978) for being classified as particularized conversational implicatures. First, A&apos;s reply does not entail (in virtue of its conventional meaning) that A did not go shopping. Second, the putative implicature can be cancelled; for example, it can be denied without the result sounding inconsistent, as can be seen by considering the addition of (2) to the end of A&apos;s reply in (1). (2) A: So I took the bus to the mall. Third, it is reinforceable; A&apos;s reply in (1) could have been preceded by an explicit &amp;quot;no&amp;quot; without destroying coherency or sounding redundant. Fourth, the putative implicatu</context>
<context position="16992" citStr="Hirschberg, 1985" startWordPosition="2887" endWordPosition="2888">not running. c. (You see,) The timing belt broke. (23) Q: Did you go shopping? A:a. My car&apos;s not running. b. The timing belt broke. c. (So) I had to take the bus. First, note that although the order of the sentences realizing A&apos;s reply varies in (22) and (23), A&apos;s overall discourse purpose in both is to convey a &apos;yes&apos;. Second, note that it is necessary to have a rule so that if A&apos;s reply consists solely of (22a) (=23c), an implicated &apos;yes&apos; is derived; and if it consists solely of (22b) (=23a), an implicated &apos;no&apos;. In existing sentence-at-a-time models of calculating implicatures (Gazdar, 1979; Hirschberg, 1985), processing (22a) would result in an implicated &apos;yes&apos; being added to the context, which would successfully block the addition of an implicated &apos;no&apos; on processing (22b). However, processing (23a) would result in a putatively implicated &apos;no&apos; being added to the context (incorrectly attributing a fleeting intention of A to convey a `no&apos;); then, on processing (23c) the conflicting but intended &apos;yes&apos; would be blocked by context, giving an incorrect result. Thus, a sentence-at-a-time model must predict when (23c) should override (23a). Also, in that model, processing (23) requires &amp;quot;extra effort&amp;quot;, a </context>
<context position="22564" citStr="Hirschberg, 1985" startWordPosition="3773" endWordPosition="3774">h S&apos;s goals. 2. If more than one operator was selected in step 1, then choose one. (We are currently investigating what factors are involved in this choice. Of course, the utterance may be ambiguous.) 3. Ascribe to S the goal(s) of the chosen plan operator. 3 Comparison to Past Research Most previous work in computational or formal linguistics on particularized conversational implicature (Green, 1990; Horacek, 1991; Joshi, Webber &amp; Weischedel, 1984a; Joshi, Webber &amp; Weischedel, 1984b; Reiter, 1990; Wainer &amp; Maida, 1991) has treated other kinds of implicature than we consider here. Hirschberg (Hirschberg, 1985) provided licensing rules making use of mutual beliefs about salient partial orderings of entities in 69 the discourse context to calculate the scalar implicatures of an utterance. Our model is similar to Hirschberg&apos;s in that both rely on the representation of aspects of context to generate implicatures, and our discourse plan operators are roughly analogous in function to her licensing rules. However, her model makes no use of discourse relations. Therefore, it does not handle several kinds of indirect replies which we treat. For example, although A in (27) could be analyzed as scalar implica</context>
</contexts>
<marker>Hirschberg, 1985</marker>
<rawString>Hirschberg, Julia B. (1985). A Theory of Scalar Inzplicature. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>Exploiting conversational implicature for generating concise explanations. In</title>
<date>1991</date>
<booktitle>Proceedings. European Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22365" citStr="Horacek, 1991" startWordPosition="3744" endWordPosition="3745">n operators all those for which a) the second step of the body matches S&apos;s contribution, and b) the applicability conditions hold, and c) it is mutually believed that the goals are consistent with S&apos;s goals. 2. If more than one operator was selected in step 1, then choose one. (We are currently investigating what factors are involved in this choice. Of course, the utterance may be ambiguous.) 3. Ascribe to S the goal(s) of the chosen plan operator. 3 Comparison to Past Research Most previous work in computational or formal linguistics on particularized conversational implicature (Green, 1990; Horacek, 1991; Joshi, Webber &amp; Weischedel, 1984a; Joshi, Webber &amp; Weischedel, 1984b; Reiter, 1990; Wainer &amp; Maida, 1991) has treated other kinds of implicature than we consider here. Hirschberg (Hirschberg, 1985) provided licensing rules making use of mutual beliefs about salient partial orderings of entities in 69 the discourse context to calculate the scalar implicatures of an utterance. Our model is similar to Hirschberg&apos;s in that both rely on the representation of aspects of context to generate implicatures, and our discourse plan operators are roughly analogous in function to her licensing rules. Howe</context>
</contexts>
<marker>Horacek, 1991</marker>
<rawString>Horacek, Helmut (1991). Exploiting conversational implicature for generating concise explanations. In Proceedings. European Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Planning coherent multisentential text.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting,</booktitle>
<pages>163--169</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="25073" citStr="Hovy, 1988" startWordPosition="4172" endWordPosition="4173">t for the implicated negative replies in (1) and (5), since their interpretation involves reconstructing domain plans that were not executed successfully; it can not account for the implicated affirmative reply in (17), in which no reasoning about domain plans is involved; and it can not account for implicated replies conveying support for or against a belief, as in (21). Lastly, his approach cannot handle implicatures conveyed by discourse units containing more than one sentence. Finally, note that our approach of including rhetorical goals in discourse plans is modelled on the work of Hovy (Hovy, 1988) and Moore and Paris (Moore &amp; Paris, 1989; Moore &amp; Paris, 1988), who used rhetorical plans to generate coherent text. 12 The two intended interpretations are marked by different intonations. 4 Conclusions We have provided algorithms for the interpretation/generation of a type of reply involving a highly context-dependent conversational implicature. Our algorithms make use of discourse expectations, discourse plans, and discourse relations. The algorithms calculate implicatures of discourse units of one or more sentences. Our approach has several advantages. First, by taking discourse relations</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Hovy, Eduard H. (1988). Planning coherent multisentential text. In Proceedings of the 26th Annual Meeting, (pp. 163-169). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
<author>Weischedel</author>
</authors>
<title>Ralph (1984a). Living up to expectations: Computing expert responses.</title>
<booktitle>In Proceedings of the Fourth National Conference on Artificial Intelligence,</booktitle>
<pages>169--175</pages>
<location>Austin, Texas.</location>
<marker>Joshi, Webber, Weischedel, </marker>
<rawString>Joshi, Aravind, Webber, Bonnie, &amp; Weischedel, Ralph (1984a). Living up to expectations: Computing expert responses. In Proceedings of the Fourth National Conference on Artificial Intelligence, (pp. 169-175)., Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
<author>Ralph Weischedel</author>
</authors>
<title>Preventing false inferences.</title>
<date>1984</date>
<booktitle>In Proceedings of Coling84,</booktitle>
<pages>134--138</pages>
<institution>Stanford University, California. Association for Computational Linguistics.</institution>
<marker>Joshi, Webber, Weischedel, 1984</marker>
<rawString>Joshi, Aravind, Webber, Bonnie, &amp; Weischedel, Ralph (1984b). Preventing false inferences. In Proceedings of Coling84, (pp. 134-138)., Stanford University, California. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Lambert</author>
<author>Carberry</author>
</authors>
<title>A tripartite plan-based model of dialogue.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting,</booktitle>
<pages>47--54</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="4367" citStr="Lambert &amp; Carberry, 1991" startWordPosition="698" endWordPosition="702">oints in a coherent conversation, the participants share certain expectations (Reichman, 1984; Carberry, 1990) about what kind of utterance is appropriate. In the type of exchange we are studying, at the point after Q&apos;s contribution, the participants share the beliefs that Q has requested to be informed if p and that the request was appropriate; hence, they share the discourse expectation that for A to be cooperative, he must now say as much as he can truthfully say in regard to the truth of p. (For convenience, we shall refer to this expectation as Answer-YNQ(p).) A discourse plan operator&apos; (Lambert &amp; Carberry, 1991) is a representation of a normal or conventional way of accomplishing certain communicative goals. Alternatively, a discourse plan operator could be considered as a defeasible rule expressing the typical (intended) effect(s) of a sequence of illocutionary acts in a context in which certain applicability conditions hold. These discourse plan operators are mutually known by the conversational participants, and can be used by a speaker to construct a plan for achieving his communicative goals. We provide a set of discourse plan operators which can be used by A as part of a plan for fulfilling Ans</context>
</contexts>
<marker>Lambert, Carberry, 1991</marker>
<rawString>Lambert, Lynn &amp; Carberry, Sandra (1991). A tripartite plan-based model of dialogue. In Proceedings of the 29th Annual Meeting, (pp. 47-54). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Nicholas Asher</author>
</authors>
<title>Discourse relations and defeasible knowledge.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting,</booktitle>
<pages>55--62</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="23728" citStr="Lascarides &amp; Asher, 1991" startWordPosition="3968" endWordPosition="3971">mple, although A in (27) could be analyzed as scalar implicating a &apos;no&apos; in some contexts, Hirschberg&apos;s model could not account for the use of A in other contexts as an elaboration (of how A managed to read chapter 1) intended to convey a &apos;yes&apos;.12 (27) Q: Did you read the first chapter? A: I took it to the beach with me. Furthermore, Hirschberg provided no computational method for determining the salient partially ordered set in a context. Also, in her model, implicatures are calculated one sentence at a time, which has the potential problems described above. Lascarides, Asher, and Oberlander (Lascarides &amp; Asher, 1991; Lascarides &amp; OberIander, 1992) described the interpretation and generation of temporal implicatures. Although that type of implicature (being Manner-based) is somewhat different from what we are studying, we have adopted their technique of providing defeasible inference rules for inferring discourse relations. In philosophy, Thomason (Thomason, 1990) suggested that discourse expectations play a role in some implicatures. McCafferty (McCafferty, 1987) argued that interpreting certain implicated replies requires domain plan reconstruction. However, he did not provide a computational method for</context>
</contexts>
<marker>Lascarides, Asher, 1991</marker>
<rawString>Lascarides, Alex &amp; Asher, Nicholas (1991). Discourse relations and defeasible knowledge. In Proceedings of the 29th Annual Meeting, (pp. 55-62). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Jon Oberlander</author>
</authors>
<title>Temporal coherence and defeasible knowledge.</title>
<date>1992</date>
<journal>Theoretical Linguistics,</journal>
<volume>18</volume>
<marker>Lascarides, Oberlander, 1992</marker>
<rawString>Lascarides, Alex &amp; Oberlander, Jon (1992). Temporal coherence and defeasible knowledge. Theoretical Linguistics, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>James Allen</author>
</authors>
<title>A plan recognition model for subdialogues in conversation.</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<volume>11</volume>
<pages>163--200</pages>
<contexts>
<context position="8636" citStr="Litman &amp; Allen, 1987" startWordPosition="1426" endWordPosition="1430">condition of T failed, or c) a normal step of T failed, or d) the agent did not want to achieve a normal goal of T, then plausible(Obstacle(B,A)). In (6) and in the rules to follow, &apos;coherentlyrelated(A,B)&apos; means that the propositions A and B are assumed to be coherently related in the discourse. The terminology in clause (iii) is that of the extended STRIPS planning formalism (Fikes 5For simplicity of exposition, (6) and the discourse relation inference rules to follow are stated in terms of the past; we plan to extend their coverage of times. 65 &amp; Nilsson, 1971; Allen, 1979; Carberry, 1990; Litman &amp; Allen, 1987). Examples of A and B satisfying each of the conditions in (6.iii) are given in (7a) - (7d), respectively. (7) [A] I didn&apos;t go shopping. a. [B] The stores were closed. b. [B] My car wasn&apos;t running. c. [B] My car broke down on the way. d. [B] I didn&apos;t want to buy anything. The discourse plan operator given in (8) describes a standard way of performing a denial (exemplified in (3)) that uses the discourse relation of Obstacle given in (6). In (8), as in (6), A is a proposition that an action of type T was not performed. (8) Deny (with Obstacle) Applicability conditions: 1) S BMB plausible(Obstac</context>
</contexts>
<marker>Litman, Allen, 1987</marker>
<rawString>Litman, Diane &amp; Allen, James (1987). A plan recognition model for subdialogues in conversation. Cognitive Science, 11, 163-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Relational propositions in discourse.</title>
<date>1983</date>
<tech>Technical Report ISI/RR-83-115, ISI/USC.</tech>
<contexts>
<context position="5019" citStr="Mann &amp; Thompson, 1983" startWordPosition="807" endWordPosition="810">l or conventional way of accomplishing certain communicative goals. Alternatively, a discourse plan operator could be considered as a defeasible rule expressing the typical (intended) effect(s) of a sequence of illocutionary acts in a context in which certain applicability conditions hold. These discourse plan operators are mutually known by the conversational participants, and can be used by a speaker to construct a plan for achieving his communicative goals. We provide a set of discourse plan operators which can be used by A as part of a plan for fulfilling Answer-YNQ(p). Mann and Thompson (Mann &amp; Thompson, 1983; Mann St Thompson, 1987) have described how the structure of a written text can be analyzed in terms of certain implicit relational propositions that may plausibly be attributed to the writer to preserve the assumption of textual coherency.4 The role of discourse relations in our approach is motivated by the observation that direct replies may occur as part of a discourse unit conveying a relational proposition. For example, in (3), (b) is provided as the (most salient) obstacle to the action (going shopping) denied by (a); (3) Q: Did you go shopping? A:a. No, b. my car&apos;s not running. in (4),</context>
</contexts>
<marker>Mann, Thompson, 1983</marker>
<rawString>Mann, William C. &amp; Thompson, Sandra A. (1983). Relational propositions in discourse. Technical Report ISI/RR-83-115, ISI/USC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1987</date>
<journal>Text,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>167--182</pages>
<marker>Mann, Thompson, 1987</marker>
<rawString>Mann, William C. &amp; Thompson, Sandra A. (1987). Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3), 167-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew S McCafferty</author>
</authors>
<title>Reasoning about Implicature: a Plan-Based Approach.</title>
<date>1987</date>
<tech>PhD thesis,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="24184" citStr="McCafferty, 1987" startWordPosition="4033" endWordPosition="4034">implicatures are calculated one sentence at a time, which has the potential problems described above. Lascarides, Asher, and Oberlander (Lascarides &amp; Asher, 1991; Lascarides &amp; OberIander, 1992) described the interpretation and generation of temporal implicatures. Although that type of implicature (being Manner-based) is somewhat different from what we are studying, we have adopted their technique of providing defeasible inference rules for inferring discourse relations. In philosophy, Thomason (Thomason, 1990) suggested that discourse expectations play a role in some implicatures. McCafferty (McCafferty, 1987) argued that interpreting certain implicated replies requires domain plan reconstruction. However, he did not provide a computational method for interpreting implicatures. Also, his proposed technique can not handle many types of indirect replies. For example, it can not account for the implicated negative replies in (1) and (5), since their interpretation involves reconstructing domain plans that were not executed successfully; it can not account for the implicated affirmative reply in (17), in which no reasoning about domain plans is involved; and it can not account for implicated replies co</context>
</contexts>
<marker>McCafferty, 1987</marker>
<rawString>McCafferty, Andrew S. (1987). Reasoning about Implicature: a Plan-Based Approach. PhD thesis, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
<author>Paris</author>
</authors>
<title>Planning text for advisory dialogues.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting,</booktitle>
<institution>University of British Columbia, Vancouver. Association of Computational Linguistics.</institution>
<contexts>
<context position="25114" citStr="Moore &amp; Paris, 1989" startWordPosition="4178" endWordPosition="4181">lies in (1) and (5), since their interpretation involves reconstructing domain plans that were not executed successfully; it can not account for the implicated affirmative reply in (17), in which no reasoning about domain plans is involved; and it can not account for implicated replies conveying support for or against a belief, as in (21). Lastly, his approach cannot handle implicatures conveyed by discourse units containing more than one sentence. Finally, note that our approach of including rhetorical goals in discourse plans is modelled on the work of Hovy (Hovy, 1988) and Moore and Paris (Moore &amp; Paris, 1989; Moore &amp; Paris, 1988), who used rhetorical plans to generate coherent text. 12 The two intended interpretations are marked by different intonations. 4 Conclusions We have provided algorithms for the interpretation/generation of a type of reply involving a highly context-dependent conversational implicature. Our algorithms make use of discourse expectations, discourse plans, and discourse relations. The algorithms calculate implicatures of discourse units of one or more sentences. Our approach has several advantages. First, by taking discourse relations into account, it can capture a variety o</context>
</contexts>
<marker>Moore, Paris, 1989</marker>
<rawString>Moore, Johanna D. &amp; Paris, Cecile (1989). Planning text for advisory dialogues. In Proceedings of the 27th Annual Meeting, University of British Columbia, Vancouver. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
<author>Cecile L Paris</author>
</authors>
<title>Constructing coherent text using rhetorical relations.</title>
<date>1988</date>
<booktitle>In Proc. 10th Annual Conference.</booktitle>
<publisher>Cognitive Science Society.</publisher>
<contexts>
<context position="19876" citStr="Moore &amp; Paris, 1988" startWordPosition="3351" endWordPosition="3354"> order; one way of ordering the two nodes directly dominated by Tell(MVA) gives (22A), another gives (23A). (Narrative order in the generation of indirect replies is an area we are currently investigating also; for related research, see section 3.) Note that Deny (with Obstacle) can not be used to generate/interpret (22A) or (23A) since its body can not be expanded to account for (22a)/(23c).8 Thus, the correct implicatures can be derived without attributing spurious intentions to A, and without requiring cancellation of spurious implicatures. 8To use the terminology of (Moore Sz Paris, 1989; Moore &amp; Paris, 1988), the labelled arcs represent satellites, and the unlabelled arcs nucleii. However, note that in their model, a nucleus can not be optional. This differs from our approach, in that we have shown that direct replies are optional in contexts such as those described by plan operators such as Affirm (with Elaboration). 8Determining this requires that the end of the relevant discourse unit be marked/recognized by cue phrases, intonation, or shift of focus; we plan to investigate this problem. 68 Affirm (with Elaboration) 1 (Elaboration) I went shopping Tell (MVA) (Motivate-Volitional-Action) Tell (</context>
<context position="25136" citStr="Moore &amp; Paris, 1988" startWordPosition="4182" endWordPosition="4185">since their interpretation involves reconstructing domain plans that were not executed successfully; it can not account for the implicated affirmative reply in (17), in which no reasoning about domain plans is involved; and it can not account for implicated replies conveying support for or against a belief, as in (21). Lastly, his approach cannot handle implicatures conveyed by discourse units containing more than one sentence. Finally, note that our approach of including rhetorical goals in discourse plans is modelled on the work of Hovy (Hovy, 1988) and Moore and Paris (Moore &amp; Paris, 1989; Moore &amp; Paris, 1988), who used rhetorical plans to generate coherent text. 12 The two intended interpretations are marked by different intonations. 4 Conclusions We have provided algorithms for the interpretation/generation of a type of reply involving a highly context-dependent conversational implicature. Our algorithms make use of discourse expectations, discourse plans, and discourse relations. The algorithms calculate implicatures of discourse units of one or more sentences. Our approach has several advantages. First, by taking discourse relations into account, it can capture a variety of implicatures not han</context>
</contexts>
<marker>Moore, Paris, 1988</marker>
<rawString>Moore, Johanna D. &amp; Paris, Cecile L. (1988). Constructing coherent text using rhetorical relations. In Proc. 10th Annual Conference. Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Perrault</author>
<author>James Allen</author>
</authors>
<title>A plan-based analysis of indirect speech acts.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>167--182</pages>
<contexts>
<context position="21639" citStr="Perrault &amp; Allen, 1980" startWordPosition="3624" endWordPosition="3627">which a) the applicability conditions hold, and b) the goals include S&apos;s goals. 2. If more than one operator was selected in step 1, then choose one. Also, determine step ordering and whether it is necessary to include optional steps. (We are currently investigating how these choices are determined.) 3. Construct a plan from the chosen operator and execute it. 10 We plan to implement an inference mechanism for the discourse relation inference rules. 11 Note that A&apos;s goals depend, in part, on the illocutionarylevel representation of Q&apos;s request. We assume that an analysis, such as provided in (Perrault &amp; Allen, 1980), is available. (26) Interpretation of indirect reply: 1. Infer discourse plan: Select from the Answer-YNQ(p) plan operators all those for which a) the second step of the body matches S&apos;s contribution, and b) the applicability conditions hold, and c) it is mutually believed that the goals are consistent with S&apos;s goals. 2. If more than one operator was selected in step 1, then choose one. (We are currently investigating what factors are involved in this choice. Of course, the utterance may be ambiguous.) 3. Ascribe to S the goal(s) of the chosen plan operator. 3 Comparison to Past Research Most</context>
</contexts>
<marker>Perrault, Allen, 1980</marker>
<rawString>Perrault, Raymond &amp; Allen, James (1980). A plan-based analysis of indirect speech acts. American Journal of Computational Linguistics, 6(3-4), 167-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Pollack</author>
</authors>
<title>Plans as complex mental attitudes. In</title>
<date>1988</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5968" citStr="Pollack, 1988" startWordPosition="977" endWordPosition="978">eplies may occur as part of a discourse unit conveying a relational proposition. For example, in (3), (b) is provided as the (most salient) obstacle to the action (going shopping) denied by (a); (3) Q: Did you go shopping? A:a. No, b. my car&apos;s not running. in (4), as an elaboration of the action (going shopping) conveyed by (a); (4) Q: Did you go shopping? A:a. Yes, b. I bought some shoes. and in (5), as a concession for failing to do the action (washing the dishes) denied by (a). (5) Q: Did you wash the dishes? A:a. No, b. (but) I scraped them. 3in Pollack&apos;s terminology, a recipe-for-action (Pollack, 1988; Grosz Si Sidner, 1988) 4Although they did not study dialogue, they suggested that it can be analyzed similarly. Also note that the relational predicates which we define are similar but not necessarily identical to theirs. Note that given appropriate context, the (b) replies in (3) through (5) would be sufficient to conversationally implicate the corresponding direct replies. This, we claim, is by virtue of the recognition of the relational proposition that would be conveyed by use of the direct reply and the (b) sentences. Our strategy, then, is to generate/interpret A&apos;s contribution using a</context>
</contexts>
<marker>Pollack, 1988</marker>
<rawString>Pollack, Martha (1988). Plans as complex mental attitudes. In P. Cohen, J. Morgan, &amp; M. Pollack (Eds.), Intentions in Communication. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Reichman</author>
</authors>
<title>Extended personmachine interface.</title>
<date>1984</date>
<journal>Artificial Intelligence,</journal>
<volume>22</volume>
<pages>157--218</pages>
<contexts>
<context position="3835" citStr="Reichman, 1984" startWordPosition="608" endWordPosition="609">ven the assumption that A&apos;s reply is cooperative, and given certain shared background information, Q can and will infer that by A&apos;s reply, A meant &apos;no&apos;. This paper presents algorithms for calculating such an inference from an indirect response and for generating an indirect response intended to carry such an inference. 2 Solution 2.1 Overview Our algorithms are based upon three notions from discourse research: discourse expectations, discourse plans, and implicit relational propositions in discourse. 64 At certain points in a coherent conversation, the participants share certain expectations (Reichman, 1984; Carberry, 1990) about what kind of utterance is appropriate. In the type of exchange we are studying, at the point after Q&apos;s contribution, the participants share the beliefs that Q has requested to be informed if p and that the request was appropriate; hence, they share the discourse expectation that for A to be cooperative, he must now say as much as he can truthfully say in regard to the truth of p. (For convenience, we shall refer to this expectation as Answer-YNQ(p).) A discourse plan operator&apos; (Lambert &amp; Carberry, 1991) is a representation of a normal or conventional way of accomplishin</context>
</contexts>
<marker>Reichman, 1984</marker>
<rawString>Reichman, Rachel (1984). Extended personmachine interface. Artificial Intelligence, 22, 157-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>The computational complexity of avoiding conversational implicatures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting,</booktitle>
<pages>97--104</pages>
<institution>Pittsburgh. Association for Computational Linguistics.</institution>
<contexts>
<context position="22449" citStr="Reiter, 1990" startWordPosition="3756" endWordPosition="3757">on, and b) the applicability conditions hold, and c) it is mutually believed that the goals are consistent with S&apos;s goals. 2. If more than one operator was selected in step 1, then choose one. (We are currently investigating what factors are involved in this choice. Of course, the utterance may be ambiguous.) 3. Ascribe to S the goal(s) of the chosen plan operator. 3 Comparison to Past Research Most previous work in computational or formal linguistics on particularized conversational implicature (Green, 1990; Horacek, 1991; Joshi, Webber &amp; Weischedel, 1984a; Joshi, Webber &amp; Weischedel, 1984b; Reiter, 1990; Wainer &amp; Maida, 1991) has treated other kinds of implicature than we consider here. Hirschberg (Hirschberg, 1985) provided licensing rules making use of mutual beliefs about salient partial orderings of entities in 69 the discourse context to calculate the scalar implicatures of an utterance. Our model is similar to Hirschberg&apos;s in that both rely on the representation of aspects of context to generate implicatures, and our discourse plan operators are roughly analogous in function to her licensing rules. However, her model makes no use of discourse relations. Therefore, it does not handle se</context>
</contexts>
<marker>Reiter, 1990</marker>
<rawString>Reiter, Ehud (1990). The computational complexity of avoiding conversational implicatures. In Proceedings of the 28th Annual Meeting, (pp. 97-104)., Pittsburgh. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold M Sadock</author>
</authors>
<title>On testing for conversational implicature. In</title>
<date>1978</date>
<pages>281--297</pages>
<publisher>Academic Press.</publisher>
<contexts>
<context position="2408" citStr="Sadock, 1978" startWordPosition="377" endWordPosition="378">fficient to prevent A from going shopping, and 1We wish to thank Kathy McCoy for her comments on this paper. 2i.e., using Austin&apos;s (Austin, 1962) distinction between locutionary and illocutionary force, Q&apos;s utterance is intended to function as a request (although it need not have the grammatical form of a question) that A&apos;s reply is coherent and cooperative), A&apos;s reply is intended to convey, in part, a &apos;no&apos;. (1) Q: Did you go shopping? A: a. My car&apos;s not running. b. The timing belt broke. Such indirect replies satisfy the conditions proposed by Grice and others (Grice, 1975; Hirschberg, 1985; Sadock, 1978) for being classified as particularized conversational implicatures. First, A&apos;s reply does not entail (in virtue of its conventional meaning) that A did not go shopping. Second, the putative implicature can be cancelled; for example, it can be denied without the result sounding inconsistent, as can be seen by considering the addition of (2) to the end of A&apos;s reply in (1). (2) A: So I took the bus to the mall. Third, it is reinforceable; A&apos;s reply in (1) could have been preceded by an explicit &amp;quot;no&amp;quot; without destroying coherency or sounding redundant. Fourth, the putative implicature is nondetach</context>
</contexts>
<marker>Sadock, 1978</marker>
<rawString>Sadock, Jerrold M. (1978). On testing for conversational implicature. In Cole, P. &amp; Morgan, J. L. (Eds.), Syntax and Semantics, (pp. 281-297)., N.Y. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richmond H Thomason</author>
</authors>
<title>Accommodation, meaning, and implicature: Interdisciplinary foundations for pragmatics. In</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="24082" citStr="Thomason, 1990" startWordPosition="4020" endWordPosition="4021">tational method for determining the salient partially ordered set in a context. Also, in her model, implicatures are calculated one sentence at a time, which has the potential problems described above. Lascarides, Asher, and Oberlander (Lascarides &amp; Asher, 1991; Lascarides &amp; OberIander, 1992) described the interpretation and generation of temporal implicatures. Although that type of implicature (being Manner-based) is somewhat different from what we are studying, we have adopted their technique of providing defeasible inference rules for inferring discourse relations. In philosophy, Thomason (Thomason, 1990) suggested that discourse expectations play a role in some implicatures. McCafferty (McCafferty, 1987) argued that interpreting certain implicated replies requires domain plan reconstruction. However, he did not provide a computational method for interpreting implicatures. Also, his proposed technique can not handle many types of indirect replies. For example, it can not account for the implicated negative replies in (1) and (5), since their interpretation involves reconstructing domain plans that were not executed successfully; it can not account for the implicated affirmative reply in (17), </context>
</contexts>
<marker>Thomason, 1990</marker>
<rawString>Thomason, Richmond H. (1990). Accommodation, meaning, and implicature: Interdisciplinary foundations for pragmatics. In P. Cohen, J. Morgan, &amp; M. Pollack (Eds.), Intentions in Communication. Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Wainer</author>
<author>Anthony Maida</author>
</authors>
<title>Good and bad news in formalizing generalized implicatures.</title>
<date>1991</date>
<booktitle>In Proceedings of the Sixteenth Annual Meeting of the Berkeley Linguistics Society,</booktitle>
<pages>66--71</pages>
<location>Berkeley, California.</location>
<contexts>
<context position="22472" citStr="Wainer &amp; Maida, 1991" startWordPosition="3758" endWordPosition="3761"> applicability conditions hold, and c) it is mutually believed that the goals are consistent with S&apos;s goals. 2. If more than one operator was selected in step 1, then choose one. (We are currently investigating what factors are involved in this choice. Of course, the utterance may be ambiguous.) 3. Ascribe to S the goal(s) of the chosen plan operator. 3 Comparison to Past Research Most previous work in computational or formal linguistics on particularized conversational implicature (Green, 1990; Horacek, 1991; Joshi, Webber &amp; Weischedel, 1984a; Joshi, Webber &amp; Weischedel, 1984b; Reiter, 1990; Wainer &amp; Maida, 1991) has treated other kinds of implicature than we consider here. Hirschberg (Hirschberg, 1985) provided licensing rules making use of mutual beliefs about salient partial orderings of entities in 69 the discourse context to calculate the scalar implicatures of an utterance. Our model is similar to Hirschberg&apos;s in that both rely on the representation of aspects of context to generate implicatures, and our discourse plan operators are roughly analogous in function to her licensing rules. However, her model makes no use of discourse relations. Therefore, it does not handle several kinds of indirect</context>
</contexts>
<marker>Wainer, Maida, 1991</marker>
<rawString>Wainer, Jacques &amp; Maida, Anthony (1991). Good and bad news in formalizing generalized implicatures. In Proceedings of the Sixteenth Annual Meeting of the Berkeley Linguistics Society, (pp. 66-71)., Berkeley, California.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>