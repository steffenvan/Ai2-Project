<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000102">
<title confidence="0.998413">
Using Error-Correcting Output Codes with Model-Refinement to
Boost Centroid Text Classifier
</title>
<author confidence="0.977129">
Songbo Tan
</author>
<affiliation confidence="0.957381">
Information Security Center, ICT, P.O. Box 2704, Beijing, 100080, China
</affiliation>
<email confidence="0.997642">
tansongbo@software.ict.ac.cn, tansongbo@gmail.com
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958090909091">
In this work, we investigate the use of
error-correcting output codes (ECOC) for
boosting centroid text classifier. The
implementation framework is to decompose
one multi-class problem into multiple
binary problems and then learn the
individual binary classification problems
by centroid classifier. However, this kind
of decomposition incurs considerable bias
for centroid classifier, which results in
noticeable degradation of performance for
centroid classifier. In order to address this
issue, we use Model-Refinement to adjust
this so-called bias. The basic idea is to take
advantage of misclassified examples in the
training data to iteratively refine and adjust
the centroids of text data. The experimental
results reveal that Model-Refinement can
dramatically decrease the bias introduced
by ECOC, and the combined classifier is
comparable to or even better than SVM
classifier in performance.
</bodyText>
<sectionHeader confidence="0.99836" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999915090909091">
In recent years, ECOC has been applied to
boost the naïve bayes, decision tree and SVM
classifier for text data (Berger 1999, Ghani 2000,
Ghani 2002, Rennie et al. 2001). Following this
research direction, in this work, we explore the
use of ECOC to enhance the performance of
centroid classifier (Han et al. 2000). To the best of
our knowledge, no previous work has been
conducted on exactly this problem. The
framework we adopted is to decompose one
multi-class problem into multiple binary problems
and then use centroid classifier to learn the
individual binary classification problems.
However, this kind of decomposition incurs
considerable bias (Liu et al. 2002) for centroid
classifier. In substance, centroid classifier (Han et
al. 2000) relies on a simple decision rule that a
given document should be assigned a particular
class if the similarity (or distance) of this
document to the centroid of the class is the largest
(or smallest). This decision rule is based on a
straightforward assumption that the documents in
one category should share some similarities with
each other. However, this hypothesis is often
violated by ECOC on the grounds that it ignores
the similarities of original classes when
disassembling one multi-class problem into
multiple binary problems.
In order to attack this problem, we use Model-
Refinement (Tan et al. 2005) to reduce this so-
called bias. The basic idea is to take advantage of
misclassified examples in the training data to
iteratively refine and adjust the centroids. This
technique is very flexible, which only needs one
classification method and there is no change to
the method in any way.
To examine the performance of proposed
method, we conduct an extensive experiment on
two commonly used datasets, i.e., Newsgroup and
Industry Sector. The results indicate that Model-
Refinement can dramatically decrease the bias
introduce by ECOC, and the resulted classifier is
comparable to or even better than SVM classifier
in performance.
</bodyText>
<sectionHeader confidence="0.951865" genericHeader="introduction">
2. Error-Correcting Output Coding
</sectionHeader>
<bodyText confidence="0.9957494">
Error-Correcting Output Coding (ECOC) is a
form of combination of multiple classifiers
(Ghani 2000). It works by converting a multi-
class supervised learning problem into a large
number (L) of two-class supervised learning
problems (Ghani 2000). Any learning algorithm
that can handle two-class learning problems, such
as Naïve Bayes (Sebastiani 2002), can then be
applied to learn each of these L problems. L can
then be thought of as the length of the codewords
</bodyText>
<page confidence="0.980992">
81
</page>
<bodyText confidence="0.834751">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 81–84,
Prague, June 2007. c�2007 Association for Computational Linguistics
with one bit in each codeword for each classifier.
The ECOC algorithm is outlined in Figure 1.
</bodyText>
<figure confidence="0.7872344">
TRAINING
1 Load training data and parameters, i.e., the length of code
L and training class K.
2 Create a L-bit code for the K classes using a kind of
coding algorithm.
3 For each bit, train the base classifier using the binary
class (0 and 1) over the total training data.
TESTING
1 Apply each of the L classifiers to the test example.
2 Assign the test example the class with the largest votes.
</figure>
<figureCaption confidence="0.999611">
Figure 1: Outline of ECOC
</figureCaption>
<sectionHeader confidence="0.995165" genericHeader="method">
3. Methodology
</sectionHeader>
<subsectionHeader confidence="0.997235">
3.1 The bias incurred by ECOC for
</subsectionHeader>
<bodyText confidence="0.990460255813953">
centroid classifier
Centroid classifier is a linear, simple and yet
efficient method for text categorization. The basic
idea of centroid classifier is to construct a
centroid Ci for each class ci using formula (1)
where d denotes one document vector and |z|
indicates the cardinality of set z. In substance,
centroid classifier makes a simple decision rule
(formula (2)) that a given document should be
assigned a particular class if the similarity (or
distance) of this document to the centroid of the
class is the largest (or smallest). This rule is based
on a straightforward assumption: the documents
in one category should share some similarities
with each other.
⎛ ⎜⎜ d• C ⎞ ⎟⎟ (2)
⎝ ⎠
c arg max c i i
d C
2
i 2
For example, the single-topic documents
involved with “sport” or “education” can meet
with the presumption; while the hybrid documents
involved with “sport” as well as “education”
break this supposition.
As such, ECOC based centroid classifier also
breaks this hypothesis. This is because ECOC
ignores the similarities of original classes when
producing binary problems. In this scenario, many
different classes are often merged into one
category. For example, the class “sport” and
“education” may be assembled into one class. As
a result, the assumption will inevitably be broken.
Let’s take a simple multi-class classification
task with 12 classes. After coding the original
classes, we obtain the dataset as Figure 2. Class 0
consists of 6 original categories, and class 1
contains another 6 categories. Then we calculate
the centroids of merged class 0 and merged class
1 using formula (1), and draw a Middle Line that
is the perpendicular bisector of the line between
the two centroids.
</bodyText>
<figureCaption confidence="0.692795">
Figure 2: Original Centroids of Merged Class 0 and
Class 1
</figureCaption>
<bodyText confidence="0.999702714285714">
According to the decision rule (formula (2)) of
centroid classifier, the examples of class 0 on the
right of the Middle Line will be misclassified into
class 1. This is the mechanism why ECOC can
bring bias for centroid classifier. In other words,
the ECOC method conflicts with the assumption
of centroid classifier to some degree.
</bodyText>
<subsectionHeader confidence="0.9841865">
3.2 Why Model-Refinement can reduce
this bias?
</subsectionHeader>
<bodyText confidence="0.999855714285714">
In order to decrease this kind of bias, we
employ the Model-Refinement to adjust the class
representative, i.e., the centroids. The basic idea
of Model-Refinement is to make use of training
errors to adjust class centroids so that the biases
can be reduced gradually, and then the training-
set error rate can also be reduced gradually.
</bodyText>
<table confidence="0.7741518">
1 Load training data and parameters;
2 Calculate centroid for each class;
3 For iter=1 to MaxIteration Do
3.1 For each document d in training set Do
3.1.1 Classify d labeled “A1” into class “A2”;
</table>
<tableCaption confidence="0.84960875">
3.1.2 If (A1!=A2) Do
Drag centroid of class A1 to d using formula (3);
Push centroid of class A2 against d using
formula (4);
</tableCaption>
<figureCaption confidence="0.998383">
Figure 3: Outline of Model-Refinement Strategy
</figureCaption>
<bodyText confidence="0.995466">
For example, if document d of class 1 is
misclassified into class 2, both centroids C1 and
C2 should be moved right by the following
formulas (3-4) respectively,
</bodyText>
<equation confidence="0.725048125">
C, =C1+η•d (3)
Cz = CZ — q • d (4)
C 1 (1)
c
i
Y
= d
i
</equation>
<figure confidence="0.988023875">
d
∈ ci
Class 0
Middle Line
Class 1
C0
C1
d
</figure>
<page confidence="0.994348">
82
</page>
<bodyText confidence="0.999930153846154">
where η (0&lt;η&lt;1) is the Learning Rate which
controls the step-size of updating operation.
The Model-Refinement for centroid classifier is
outlined in Figure 3 where MaxIteration denotes
the pre-defined steps for iteration. More details
can be found in (Tan et al. 2005). The time
requirement of Model-Refinement is O(MTKW)
where M denotes the iteration steps.
With this so-called move operation, C0 and C1
are both moving right gradually. At the end of this
kind of move operation (see Figure 4), no
example of class 0 locates at the right of Middle
Line so no example will be misclassified.
</bodyText>
<subsectionHeader confidence="0.9989555">
3.3 The combination of ECOC and Model-
Refinement for centroid classifier
</subsectionHeader>
<bodyText confidence="0.999978666666667">
In this subsection, we present the outline
(Figure 5) of combining ECOC and Model-
Refinement for centroid classifier. In substance,
the improved ECOC combines the strengths of
ECOC and Model-Refinement. ECOC research in
ensemble learning techniques has shown that it is
well suited for classification tasks with a large
number of categories. On the other hand, Model-
Refinement has proved to be an effective
approach to reduce the bias of base classifier, that
is to say, it can dramatically boost the
performance of the base classifier.
</bodyText>
<sectionHeader confidence="0.512728" genericHeader="method">
TRAINING
</sectionHeader>
<tableCaption confidence="0.7723515">
1 Load training data and parameters, i.e., the length of
code L and training class K.
2 Create a L-bit code for the K classes using a kind of
coding algorithm.
3 For each bit, train centroid classifier using the binary
class (0 and 1) over the total training data.
</tableCaption>
<sectionHeader confidence="0.73617225" genericHeader="method">
4 Use Model-Refinement approach to adjust centroids.
TESTING
1 Apply each of the L classifiers to the test example.
2 Assign the test example the class with the largest votes.
</sectionHeader>
<figureCaption confidence="0.949806">
Figure 5: Outline of combining ECOC and Model-
Refinement
</figureCaption>
<footnote confidence="0.9596135">
1 www-2.cs.cmu.edu/afs/cs/project/theo-11/www/wwkb.
2 www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/.
</footnote>
<figure confidence="0.991064">
Class 0
C*0
Middle Line
d
Class 1
C*1
</figure>
<figureCaption confidence="0.949053">
Figure 4: Refined Centroids of Merged Class 0 and
</figureCaption>
<figure confidence="0.881566333333333">
Class 1
4. Experiment Results
4.1 Datasets
</figure>
<bodyText confidence="0.9854475">
In our experiment, we use two corpora:
NewsGroup1, and Industry Sector2.
NewsGroup The NewsGroup dataset contains
approximately 20,000 articles evenly divided
among 20 Usenet newsgroups. We use a subset
consisting of total categories and 19,446
documents.
Industry Sector The set consists of company
homepages that are categorized in a hierarchy of
industry sectors, but we disregard the hierarchy.
There were 9,637 documents in the dataset, which
were divided into 105 classes. We use a subset
called as Sector-48 consisting of 48 categories
and in all 4,581 documents.
</bodyText>
<subsectionHeader confidence="0.571341">
4.2 Experimental Design
</subsectionHeader>
<bodyText confidence="0.996666">
To evaluate a text classification system, we use
MicroF1 and MacroF1 measures (Chai et al.
2002). We employ Information Gain as feature
selection method because it consistently performs
well in most cases (Yang et al. 1997). We employ
TFIDF (Sebastiani 2002) to compute feature
weight. For SVM classifier we employ
SVMTorch. (www.idiap.ch/~bengio/projects/SVMTorch.html).
</bodyText>
<subsectionHeader confidence="0.490997">
4.3 Comparison and Analysis
</subsectionHeader>
<bodyText confidence="0.999655133333333">
Table 1 and table 2 show the performance
comparison of different method on two datasets
when using 10,000 features. For ECOC, we use
63-bit BCH coding; for Model-Refinement, we
fix its MaxIteration as 8. For brevity, we use MR
to denote Model-Refinement.
From the two tables, we can observe that
ECOC indeed brings significant bias for centroid
classifier, which results in considerable decrease
in accuracy. Especially on sector-48, the bias
reduces the MicroF1 of centroid classifier from
0.7985 to 0.6422.
On the other hand, the combination of ECOC
and Model-Refinement makes a significant
performance improvement over centroid classifier.
</bodyText>
<page confidence="0.996052">
83
</page>
<bodyText confidence="0.995977428571429">
On Newsgroup, it beats centroid classifier by 4
percents; on Sector-48, it beats centroid classifier
by 11 percents. More encouraging, it yields better
performance than SVM classifier on Sector-48.
This improvement also indicates that Model-
Refinement can effectively reduce the bias
incurred by ECOC.
</bodyText>
<tableCaption confidence="0.999797">
Table 1: The MicroF1 of different methods
</tableCaption>
<table confidence="0.9731972">
Method ECOC
MR ECOC
Centroid + MR SVM
+Centroid +Centroid
Dataset +Centroid
Sector-48 0.7985 0.8671 0.6422 0.9122 0.8948
NewsGroup 0.8371 0.8697 0.8085 0.8788 0.8777
able 2: The MacroF1 of different methods
Method ECOC
MR ECOC
Centroid + MR SVM
+Centroid +Centroid
Dataset +Centroid
Sector-48 0.8097 0.8701 0.6559 0.9138 0.8970
NewsGroup 0.8331 0.8661 0.7936 0.8757 0.8759
</table>
<bodyText confidence="0.9543752">
Table 3 and 4 report the classification accuracy
of combining ECOC with Model-Refinement on
two datasets vs. the length BCH coding. For
Model-Refinement, we fix its MaxIteration as 8;
the number of features is fixed as 10,000.
</bodyText>
<tableCaption confidence="0.998889">
Table 3: the MicroF1 vs. the length of BCH coding
</tableCaption>
<table confidence="0.9996738">
Bit
15bit 31bit 63bit
Dataset
Sector-48 0.8461 0.8948 0.9105
NewsGroup 0.8463 0.8745 0.8788
</table>
<tableCaption confidence="0.999333">
Table 4: the MacroF1 vs. the length of BCH coding
</tableCaption>
<table confidence="0.9907932">
Bit
15bit 31bit 63bit
Dataset
Sector-48 0.8459 0.8961 0.9122
NewsGroup 0.8430 0.8714 0.8757
</table>
<bodyText confidence="0.999687285714286">
We can clearly observe that increasing the
length of the codes increases the classification
accuracy. However, the increase in accuracy is
not directly proportional to the increase in the
length of the code. As the codes get larger, the
accuracies start leveling off as we can observe
from the two tables.
</bodyText>
<sectionHeader confidence="0.994439" genericHeader="conclusions">
5. Conclusion Remarks
</sectionHeader>
<bodyText confidence="0.999766764705883">
In this work, we examine the use of ECOC for
improving centroid text classifier. The
implementation framework is to decompose
multi-class problems into multiple binary
problems and then learn the individual binary
classification problems by centroid classifier.
Meanwhile, Model-Refinement is employed to
reduce the bias incurred by ECOC.
In order to investigate the effectiveness and
robustness of proposed method, we conduct an
extensive experiment on two commonly used
corpora, i.e., Industry Sector and Newsgroup. The
experimental results indicate that the combination
of ECOC with Model-Refinement makes a
considerable performance improvement over
traditional centroid classifier, and even performs
comparably with SVM classifier.
</bodyText>
<sectionHeader confidence="0.999371" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999447777777778">
Berger, A. Error-correcting output coding for text
classification. In Proceedings of IJCAI, 1999.
Chai, K., Chieu, H. and Ng, H. Bayesian online
classifiers for text classification and filtering. SIGIR.
2002, 97-104
Ghani, R. Using error-correcting codes for text
classification. ICML. 2000
Ghani, R. Combining labeled and unlabeled data for
multiclass text categorization. ICML. 2002
Han, E. and Karypis, G. Centroid-Based Document
Classification Analysis &amp; Experimental Result.
PKDD. 2000.
Liu, Y., Yang, Y. and Carbonell, J. Boosting to
Correct Inductive Bias in Text Classification. CIKM.
2002, 348-355
Rennie, J. and Rifkin, R. Improving multiclass text
classification with the support vector machine. In
MIT. AI Memo AIM-2001-026, 2001.
Sebastiani, F. Machine learning in automated text
categorization. ACM Computing Surveys,
2002,34(1): 1-47.
Tan, S., Cheng, X., Ghanem, M., Wang, B. and Xu,
H. A novel refinement approach for text
categorization. CIKM. 2005, 469-476
Yang, Y. and Pedersen, J. A Comparative Study on
Feature Selection in Text Categorization. ICML.
1997, 412-420.
</reference>
<page confidence="0.999239">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621105">
<title confidence="0.9878135">Using Error-Correcting Output Codes with Model-Refinement to Boost Centroid Text Classifier</title>
<author confidence="0.971214">Songbo Tan</author>
<address confidence="0.700602">Information Security Center, ICT, P.O. Box 2704, Beijing, 100080, China</address>
<email confidence="0.999169">tansongbo@software.ict.ac.cn,tansongbo@gmail.com</email>
<abstract confidence="0.997010347826087">In this work, we investigate the use of error-correcting output codes (ECOC) for boosting centroid text classifier. The implementation framework is to decompose one multi-class problem into multiple binary problems and then learn the individual binary classification problems by centroid classifier. However, this kind of decomposition incurs considerable bias for centroid classifier, which results in noticeable degradation of performance for centroid classifier. In order to address this issue, we use Model-Refinement to adjust this so-called bias. The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids of text data. The experimental results reveal that Model-Refinement can dramatically decrease the bias introduced by ECOC, and the combined classifier is comparable to or even better than SVM classifier in performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
</authors>
<title>Error-correcting output coding for text classification.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<contexts>
<context position="1276" citStr="Berger 1999" startWordPosition="177" endWordPosition="178">rformance for centroid classifier. In order to address this issue, we use Model-Refinement to adjust this so-called bias. The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids of text data. The experimental results reveal that Model-Refinement can dramatically decrease the bias introduced by ECOC, and the combined classifier is comparable to or even better than SVM classifier in performance. 1. Introduction In recent years, ECOC has been applied to boost the naïve bayes, decision tree and SVM classifier for text data (Berger 1999, Ghani 2000, Ghani 2002, Rennie et al. 2001). Following this research direction, in this work, we explore the use of ECOC to enhance the performance of centroid classifier (Han et al. 2000). To the best of our knowledge, no previous work has been conducted on exactly this problem. The framework we adopted is to decompose one multi-class problem into multiple binary problems and then use centroid classifier to learn the individual binary classification problems. However, this kind of decomposition incurs considerable bias (Liu et al. 2002) for centroid classifier. In substance, centroid classi</context>
</contexts>
<marker>Berger, 1999</marker>
<rawString>Berger, A. Error-correcting output coding for text classification. In Proceedings of IJCAI, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Chai</author>
<author>H Chieu</author>
<author>H Ng</author>
</authors>
<title>Bayesian online classifiers for text classification and</title>
<date>2002</date>
<pages>97--104</pages>
<contexts>
<context position="10097" citStr="Chai et al. 2002" startWordPosition="1635" endWordPosition="1638">ewsGroup The NewsGroup dataset contains approximately 20,000 articles evenly divided among 20 Usenet newsgroups. We use a subset consisting of total categories and 19,446 documents. Industry Sector The set consists of company homepages that are categorized in a hierarchy of industry sectors, but we disregard the hierarchy. There were 9,637 documents in the dataset, which were divided into 105 classes. We use a subset called as Sector-48 consisting of 48 categories and in all 4,581 documents. 4.2 Experimental Design To evaluate a text classification system, we use MicroF1 and MacroF1 measures (Chai et al. 2002). We employ Information Gain as feature selection method because it consistently performs well in most cases (Yang et al. 1997). We employ TFIDF (Sebastiani 2002) to compute feature weight. For SVM classifier we employ SVMTorch. (www.idiap.ch/~bengio/projects/SVMTorch.html). 4.3 Comparison and Analysis Table 1 and table 2 show the performance comparison of different method on two datasets when using 10,000 features. For ECOC, we use 63-bit BCH coding; for Model-Refinement, we fix its MaxIteration as 8. For brevity, we use MR to denote Model-Refinement. From the two tables, we can observe that </context>
</contexts>
<marker>Chai, Chieu, Ng, 2002</marker>
<rawString>Chai, K., Chieu, H. and Ng, H. Bayesian online classifiers for text classification and filtering. SIGIR. 2002, 97-104</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ghani</author>
</authors>
<title>Using error-correcting codes for text classification. ICML.</title>
<date>2000</date>
<contexts>
<context position="1288" citStr="Ghani 2000" startWordPosition="179" endWordPosition="180"> centroid classifier. In order to address this issue, we use Model-Refinement to adjust this so-called bias. The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids of text data. The experimental results reveal that Model-Refinement can dramatically decrease the bias introduced by ECOC, and the combined classifier is comparable to or even better than SVM classifier in performance. 1. Introduction In recent years, ECOC has been applied to boost the naïve bayes, decision tree and SVM classifier for text data (Berger 1999, Ghani 2000, Ghani 2002, Rennie et al. 2001). Following this research direction, in this work, we explore the use of ECOC to enhance the performance of centroid classifier (Han et al. 2000). To the best of our knowledge, no previous work has been conducted on exactly this problem. The framework we adopted is to decompose one multi-class problem into multiple binary problems and then use centroid classifier to learn the individual binary classification problems. However, this kind of decomposition incurs considerable bias (Liu et al. 2002) for centroid classifier. In substance, centroid classifier (Han et</context>
<context position="3271" citStr="Ghani 2000" startWordPosition="491" endWordPosition="492">centroids. This technique is very flexible, which only needs one classification method and there is no change to the method in any way. To examine the performance of proposed method, we conduct an extensive experiment on two commonly used datasets, i.e., Newsgroup and Industry Sector. The results indicate that ModelRefinement can dramatically decrease the bias introduce by ECOC, and the resulted classifier is comparable to or even better than SVM classifier in performance. 2. Error-Correcting Output Coding Error-Correcting Output Coding (ECOC) is a form of combination of multiple classifiers (Ghani 2000). It works by converting a multiclass supervised learning problem into a large number (L) of two-class supervised learning problems (Ghani 2000). Any learning algorithm that can handle two-class learning problems, such as Naïve Bayes (Sebastiani 2002), can then be applied to learn each of these L problems. L can then be thought of as the length of the codewords 81 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 81–84, Prague, June 2007. c�2007 Association for Computational Linguistics with one bit in each codeword for each classifier. The ECOC algorithm is outlined in Figure 1. TRA</context>
</contexts>
<marker>Ghani, 2000</marker>
<rawString>Ghani, R. Using error-correcting codes for text classification. ICML. 2000</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ghani</author>
</authors>
<title>Combining labeled and unlabeled data for multiclass text categorization. ICML.</title>
<date>2002</date>
<contexts>
<context position="1300" citStr="Ghani 2002" startWordPosition="181" endWordPosition="182">assifier. In order to address this issue, we use Model-Refinement to adjust this so-called bias. The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids of text data. The experimental results reveal that Model-Refinement can dramatically decrease the bias introduced by ECOC, and the combined classifier is comparable to or even better than SVM classifier in performance. 1. Introduction In recent years, ECOC has been applied to boost the naïve bayes, decision tree and SVM classifier for text data (Berger 1999, Ghani 2000, Ghani 2002, Rennie et al. 2001). Following this research direction, in this work, we explore the use of ECOC to enhance the performance of centroid classifier (Han et al. 2000). To the best of our knowledge, no previous work has been conducted on exactly this problem. The framework we adopted is to decompose one multi-class problem into multiple binary problems and then use centroid classifier to learn the individual binary classification problems. However, this kind of decomposition incurs considerable bias (Liu et al. 2002) for centroid classifier. In substance, centroid classifier (Han et al. 2000) r</context>
</contexts>
<marker>Ghani, 2002</marker>
<rawString>Ghani, R. Combining labeled and unlabeled data for multiclass text categorization. ICML. 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Han</author>
<author>G Karypis</author>
</authors>
<title>Centroid-Based Document Classification Analysis &amp; Experimental Result.</title>
<date>2000</date>
<location>PKDD.</location>
<marker>Han, Karypis, 2000</marker>
<rawString>Han, E. and Karypis, G. Centroid-Based Document Classification Analysis &amp; Experimental Result. PKDD. 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>Y Yang</author>
<author>J Carbonell</author>
</authors>
<title>Boosting to Correct Inductive Bias</title>
<date>2002</date>
<booktitle>in Text Classification. CIKM.</booktitle>
<pages>348--355</pages>
<contexts>
<context position="1821" citStr="Liu et al. 2002" startWordPosition="261" endWordPosition="264">aïve bayes, decision tree and SVM classifier for text data (Berger 1999, Ghani 2000, Ghani 2002, Rennie et al. 2001). Following this research direction, in this work, we explore the use of ECOC to enhance the performance of centroid classifier (Han et al. 2000). To the best of our knowledge, no previous work has been conducted on exactly this problem. The framework we adopted is to decompose one multi-class problem into multiple binary problems and then use centroid classifier to learn the individual binary classification problems. However, this kind of decomposition incurs considerable bias (Liu et al. 2002) for centroid classifier. In substance, centroid classifier (Han et al. 2000) relies on a simple decision rule that a given document should be assigned a particular class if the similarity (or distance) of this document to the centroid of the class is the largest (or smallest). This decision rule is based on a straightforward assumption that the documents in one category should share some similarities with each other. However, this hypothesis is often violated by ECOC on the grounds that it ignores the similarities of original classes when disassembling one multi-class problem into multiple bi</context>
</contexts>
<marker>Liu, Yang, Carbonell, 2002</marker>
<rawString>Liu, Y., Yang, Y. and Carbonell, J. Boosting to Correct Inductive Bias in Text Classification. CIKM. 2002, 348-355</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rennie</author>
<author>R Rifkin</author>
</authors>
<title>Improving multiclass text classification with the support vector machine.</title>
<date>2001</date>
<booktitle>In MIT. AI Memo</booktitle>
<pages>2001--026</pages>
<marker>Rennie, Rifkin, 2001</marker>
<rawString>Rennie, J. and Rifkin, R. Improving multiclass text classification with the support vector machine. In MIT. AI Memo AIM-2001-026, 2001.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>1--47</pages>
<marker>Sebastiani, </marker>
<rawString>Sebastiani, F. Machine learning in automated text categorization. ACM Computing Surveys, 2002,34(1): 1-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
<author>X Cheng</author>
<author>M Ghanem</author>
<author>B Wang</author>
<author>H Xu</author>
</authors>
<title>A novel refinement approach for text categorization. CIKM.</title>
<date>2005</date>
<pages>469--476</pages>
<contexts>
<context position="2509" citStr="Tan et al. 2005" startWordPosition="371" endWordPosition="374">2000) relies on a simple decision rule that a given document should be assigned a particular class if the similarity (or distance) of this document to the centroid of the class is the largest (or smallest). This decision rule is based on a straightforward assumption that the documents in one category should share some similarities with each other. However, this hypothesis is often violated by ECOC on the grounds that it ignores the similarities of original classes when disassembling one multi-class problem into multiple binary problems. In order to attack this problem, we use ModelRefinement (Tan et al. 2005) to reduce this socalled bias. The basic idea is to take advantage of misclassified examples in the training data to iteratively refine and adjust the centroids. This technique is very flexible, which only needs one classification method and there is no change to the method in any way. To examine the performance of proposed method, we conduct an extensive experiment on two commonly used datasets, i.e., Newsgroup and Industry Sector. The results indicate that ModelRefinement can dramatically decrease the bias introduce by ECOC, and the resulted classifier is comparable to or even better than SV</context>
<context position="7725" citStr="Tan et al. 2005" startWordPosition="1257" endWordPosition="1260">class A2 against d using formula (4); Figure 3: Outline of Model-Refinement Strategy For example, if document d of class 1 is misclassified into class 2, both centroids C1 and C2 should be moved right by the following formulas (3-4) respectively, C, =C1+η•d (3) Cz = CZ — q • d (4) C 1 (1) c i Y = d i d ∈ ci Class 0 Middle Line Class 1 C0 C1 d 82 where η (0&lt;η&lt;1) is the Learning Rate which controls the step-size of updating operation. The Model-Refinement for centroid classifier is outlined in Figure 3 where MaxIteration denotes the pre-defined steps for iteration. More details can be found in (Tan et al. 2005). The time requirement of Model-Refinement is O(MTKW) where M denotes the iteration steps. With this so-called move operation, C0 and C1 are both moving right gradually. At the end of this kind of move operation (see Figure 4), no example of class 0 locates at the right of Middle Line so no example will be misclassified. 3.3 The combination of ECOC and ModelRefinement for centroid classifier In this subsection, we present the outline (Figure 5) of combining ECOC and ModelRefinement for centroid classifier. In substance, the improved ECOC combines the strengths of ECOC and Model-Refinement. ECO</context>
</contexts>
<marker>Tan, Cheng, Ghanem, Wang, Xu, 2005</marker>
<rawString>Tan, S., Cheng, X., Ghanem, M., Wang, B. and Xu, H. A novel refinement approach for text categorization. CIKM. 2005, 469-476</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J Pedersen</author>
</authors>
<title>A Comparative Study on Feature Selection</title>
<date>1997</date>
<booktitle>in Text Categorization. ICML.</booktitle>
<pages>412--420</pages>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yang, Y. and Pedersen, J. A Comparative Study on Feature Selection in Text Categorization. ICML. 1997, 412-420.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>