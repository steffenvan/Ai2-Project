<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99611">
Accurate Context-Free Parsing with Combinatory Categorial Grammar
</title>
<author confidence="0.996148">
Timothy A. D. Fowler and Gerald Penn
</author>
<affiliation confidence="0.999685">
Department of Computer Science, University of Toronto
</affiliation>
<address confidence="0.979921">
Toronto, ON, M5S 3G4, Canada
</address>
<email confidence="0.998036">
{tfowler, gpenn}@cs.toronto.edu
</email>
<sectionHeader confidence="0.997356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999296">
The definition of combinatory categorial
grammar (CCG) in the literature varies
quite a bit from author to author. How-
ever, the differences between the defini-
tions are important in terms of the lan-
guage classes of each CCG. We prove
that a wide range of CCGs are strongly
context-free, including the CCG of CCG-
bank and of the parser of Clark and Cur-
ran (2007). In light of these new results,
we train the PCFG parser of Petrov and
Klein (2007) on CCGbank and achieve
state of the art results in supertagging ac-
curacy, PARSEVAL measures and depen-
dency accuracy.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870142857143">
Combinatory categorial grammar (CCG) is a vari-
ant of categorial grammar which has attracted in-
terest for both theoretical and practical reasons.
On the theoretical side, we know that it is mildly
context-sensitive (Vijay-Shanker and Weir, 1994)
and that it can elegantly analyze a wide range of
linguistic phenomena (Steedman, 2000). On the
practical side, we have corpora with CCG deriva-
tions for each sentence (Hockenmaier and Steed-
man, 2007), a wide-coverage parser trained on that
corpus (Clark and Curran, 2007) and a system for
converting CCG derivations into semantic repre-
sentations (Bos et al., 2004).
However, despite being treated as a single uni-
fied grammar formalism, each of these authors use
variations of CCG which differ primarily on which
combinators are included in the grammar and the
restrictions that are put on them. These differences
are important because they affect whether the
mild context-sensitivity proof of Vijay-Shanker
and Weir (1994) applies. We will provide a gen-
eralized framework for CCG within which the full
variation of CCG seen in the literature can be de-
fined. Then, we prove that for a wide range of
CCGs there is a context-free grammar (CFG) that
has exactly the same derivations. Included in this
class of strongly context-free CCGs are a grammar
including all the derivations in CCGbank and the
grammar used in the Clark and Curran parser.
Due to this insight, we investigate the potential
of using tools from the probabilistic CFG com-
munity to improve CCG parsing results. The
Petrov parser (Petrov and Klein, 2007) uses la-
tent variables to refine the grammar extracted from
a corpus to improve accuracy, originally used
to improve parsing results on the Penn treebank
(PTB). We train the Petrov parser on CCGbank
and achieve the best results to date on sentences
from section 23 in terms of supertagging accuracy,
PARSEVAL measures and dependency accuracy.
These results should not be interpreted as proof
that grammars extracted from the Penn treebank
and from CCGbank are equivalent. Bos’s system
for building semantic representations from CCG
derivations is only possible due to the categorial
nature of CCG. Furthermore, the long distance de-
pendencies involved in extraction and coordina-
tion phenomena have a more natural representa-
tion in CCG.
</bodyText>
<sectionHeader confidence="0.894755" genericHeader="method">
2 The Language Classes of Combinatory
Categorial Grammars
</sectionHeader>
<bodyText confidence="0.882507727272727">
A categorial grammar is a grammatical system
consisting of a finite set of words, a set of cate-
gories, a finite set of sentential categories, a finite
lexicon mapping words to categories and a rule
system dictating how the categories can be com-
bined. The set of categories are constructed from a
finite set of atoms A (e.g. A = {5, NP, N, PP})
and a finite set of binary connectives B (e.g.
B = {/, \}) to build an infinite set of categories
C(A, B) (e.g. C(A, B) = {5, 5\NP, (5\NP)/
NP, ...}). For a category C, its size |C |is the
</bodyText>
<page confidence="0.972994">
335
</page>
<note confidence="0.944011">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 335–344,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.973314857142857">
number of atom occurrences it contains. When not
specified, connectives are left associative.
According to the literature, combinatory cate-
gorial grammar has been defined to have a vari-
ety of rule systems. These rule systems vary from
a small rule set, motivated theoretically (Vijay-
Shanker and Weir, 1994), to a larger rule set,
motivated linguistically, (Steedman, 2000) to a
very large rule set, motivated by practical cover-
age (Hockenmaier and Steedman, 2007; Clark and
Curran, 2007). We provide a definition general
enough to incorporate these four main variants of
CCG, as well as others.
A combinatory categorial grammar (CCG) is a
categorial grammar whose rule system consists of
rule schemata where the left side is a sequence of
categories and the right side is a single category
where the categories may include variables over
both categories and connectives. In addition, rule
schemata may specify a sequence of categories
and connectives using the ... convention1. When
... appears in a rule, it matches any sequence of
categories and connectives according to the con-
nectives adjacent to the .... For example, the rule
schema for forward composition is:
X/Y, Y/Z → X/Z
and the rule schema for generalized forward
crossed composition is:
</bodyText>
<equation confidence="0.449624">
X/Y,Y |1Z1|2 ... |nZn → X|1Z1|2 ... |nZn
</equation>
<bodyText confidence="0.999512875">
where X, Y and Zi for 1 ≤ i ≤ n are variables
over categories and |i for 1 ≤ i ≤ n are variables
over connectives. Figure 1 shows a CCG deriva-
tion from CCGbank.
A well-known categorial grammar which is not
a CCG is Lambek categorial grammar (Lambek,
1958) whose introduction rules cannot be charac-
terized as combinatory rules (Zielonka, 1981).
</bodyText>
<subsectionHeader confidence="0.536959">
2.1 Classes for defining CCG
</subsectionHeader>
<bodyText confidence="0.999816857142857">
We define a number of schema classes general
enough that the important variants of CCG can be
defined by selecting some subset of the classes. In
addition to the schema classes, we also define two
restriction classes which define ways in which the
rule schemata from the schema classes can be re-
stricted. We define the following schema classes:
</bodyText>
<footnote confidence="0.6009665">
1The ... convention (Vijay-Shanker and Weir, 1994) is
essentially identical to the $ convention of Steedman (2000).
</footnote>
<listItem confidence="0.99862875">
(1) Application
• X/Y,Y → X
• Y,X\Y → X
(2) Composition
• X/Y, Y/Z → X/Z
• Y \Z, X\Y → X\Z
(3) Crossed Composition
• X/Y,Y\Z → X\Z
• Y/Z, X\Y → X/Z
(4) Generalized Composition
• X/Y,Y/Z1/... /Zn → X/Z1/... /Zn
• Y \Z1\... \Zn,X\Y → X\Z1\... \Zn
(5) Generalized Crossed Composition
• X/Y, Y |1Z1|2 ... |nZn
→ X|1Z1|2 ... |nZn
• Y |1Z1|2 ...|nZn,X\Y
→ X|1Z1|2 ... |nZn
(6) Reducing Generalized Crossed Composition
Generalized Composition or Generalized
Crossed Composition where |X |≤ |Y |.
(7) Substitution
• (X/Y )|1Z,Y |1Z → X|1Z
• Y |1Z,(X\Y )|1Z → X|1Z
(8) D Combinator2
• X/(Y |1Z),Y |2W → X|2(W|1Z)
• Y |2W,X\(Y |1Z) → X|2(W|1Z)
(9) Type-Raising
• X → T/(T\X)
• X → T\(T/X)
(10) Finitely Restricted Type-Raising
• X → T/(T\X) where hX, Ti ∈ 5 for fi-
nite 5
• X → T\(T/X) where hX, Ti ∈ 5 for fi-
nite 5
(11) Finite Unrestricted Variable-Free Rules
• X� → Y where h X, Y i ∈ 5 for finite 5
</listItem>
<footnote confidence="0.742297">
2Hoyt and Baldridge (2008) argue for the inclusion of the
D Combinator in CCG.
</footnote>
<page confidence="0.996793">
336
</page>
<figureCaption confidence="0.809535">
Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .
Figure 1: A CCG derivation from section 00 of CCGbank.
</figureCaption>
<figure confidence="0.9472648">
5[dcl]
5[dcl]
5[dcl]\NP
NP
NP\NP
NP
NP [conj]
NP
NP
N
N/N N 5[dcl]\NP/NP N NP\NP/NP N/N N , NP[nb]/N N/N N/N N .
NP
N
NP
N
N
We define the following restriction classes:
such rules, limiting the new atoms to a finite num-
ber.
(A) Rule Restriction to a Finite Set
</figure>
<bodyText confidence="0.988308666666667">
The rule schemata in the schema classes of a
CCG are limited to a finite number of instan-
tiations.
</bodyText>
<listItem confidence="0.720874">
(B) Rule Restrictions to Certain Categories 3
</listItem>
<bodyText confidence="0.999961375">
The rule schemata in the schema classes of a
CCG are limited to a finite number of instan-
tiations although variables are allowed in the
instantiations.
Vijay-Shanker and Weir (1994) define CCG to
be schema class (4) with restriction class (B) .
Steedman (2000) defines CCG to be schema
classes (1-5), (6), (10) with restriction class (B).
</bodyText>
<subsectionHeader confidence="0.994267">
2.2 Strongly Context-Free CCGs
</subsectionHeader>
<bodyText confidence="0.895153066666667">
Proposition 1. The set of atoms in any derivation
of any CCG consisting of a subset of the schema
classes (1-8) and (10-11) isfinite.
Proof. A finite lexicon can introduce only a finite
number of atoms in lexical categories.
Any rule corresponding to a schema in the
schema classes (1-8) has only those atoms on the
right that occur somewhere on the left. Rules in
classes (10-11) can each introduce a finite number
of atoms, but there can be only a finite number of
3Baldridge (2002) introduced a variant of CCG where
modalities are added to the connectives / and \ along with
variants of the combinatory rules based on these modalities.
Our proofs about restriction class (B) are essentially identical
to proofs regarding the multi-modal variant.
</bodyText>
<construct confidence="0.470504625">
Definition 1. The subcategories for a category c
are c1 and c2 if c = c1 • c2 for • E B and c if c is
atomic. Its second subcategories are the subcate-
gories of its subcategories.
Proposition 2. Any CCG consisting of a subset
of the rule schemata (1-3), (6-8) and (10-11) has
derivations consisting of only a finite number of
categories.
</construct>
<bodyText confidence="0.999929181818182">
Proof. We first prove the proposition excluding
schema class (8). We will use structural induction
on the derivations to prove that there is a bound on
the size of the subcategories of any category in the
derivation. The base case is the assignment of a
lexical category to a word and the inductive step is
the use of a rule from schema classes (1-4), (6-7)
and (10-11).
Given that the lexicon is finite, there is a bound
k on the size of the subcategories of lexical cate-
gories. Furthermore, there is a bound l on the size
of the subcategories of categories on the right side
of any rule in (10) and (11). Let m = max(k, l).
For rules from schema class (1), the category
on the right is a subcategory of the first category
on the left, so the subcategories on the right are
bound by m. For rules from schema classes (2-3),
the category on the right has subcategories X and
Z each of which is bound in size by m since they
occur as subcategories of categories on the left.
For rules from schema class (6), since reduc-
ing generalized composition is a special case of re-
</bodyText>
<page confidence="0.991157">
337
</page>
<bodyText confidence="0.999128541666667">
ducing generalized crossing composition, we need
only consider the latter. The category on the right
has subcategories X|1Z1|2 ... |n−1|Zn−1 and Zn.
Zn is bound in size by m because it occurs as
a subcategory of the second category on the left.
Then, the size of Y |1Z1|2 ... |n−1|Zn−1 must be
bound by m and since |X |≤ |Y |, the size of
X|1Z1|2 ... |n−1|Zn−1 must also be bound by m.
For rules from schema class (7), the category on
the right has subcategories X and Z. The size of
Z is bound by m because it is a subcategory of a
category on the left. The size of X is bound by
m because it is a second subcategory of a category
on the left.
Finally, the use of rules in schema classes (10-
11) have categories on the right that are bounded
by l, which is, in turn, bounded by m. Then, by
proposition 1, there must only be a finite number
of categories in any derivation in a CCG consisting
of a subset of rule schemata (1-3), (6-7) and (10-
11).
The proof including schema class (8) is essen-
tially identical except that k must be defined in
terms of the size of the second subcategories.
</bodyText>
<construct confidence="0.6105505">
Definition 2. A grammar is strongly context-free
if there exists a CFG such that the derivations of
the two grammars are identical.
Proposition 3. Any CCG consisting of a subset
of the schema classes (1-3), (6-8) and (10-11) is
strongly context-free.
</construct>
<bodyText confidence="0.983913103448276">
Proof. Since the CCG generates derivations
whose categories are finite in number let C be that
set of categories. Let 5(C, X) be the subset of C
matching category X (which may have variables).
Then, for each rule schema C1, C2 → C3 in (1-3)
and (6-8), we construct a context-free rule C′3 →
C′1, C′2 for each C′ i in 5(C, Ci) for 1 ≤ i ≤ 3.
Similarly, for each rule schema C1 → C2 in (10),
we construct a context-free rule C′2 → C′1 which
results in a finite number of such rules. Finally, for
�
each rule schema X → Z in (11) we construct a
�
context-free rule Z → X. Then, for each entry in
the lexicon w → C, we construct a context-free
rule C → w.
The constructed CFG has precisely the same
rules as the CCG restricted to the categories in C
except that the left and right sides have been re-
versed. Thus, by proposition 2, the CFG has ex-
actly the same derivations as the CCG.
Proposition 4. Any CCG consisting of a subset of
the schema classes (1-3), (6-8) and (10-11) along
with restriction class (B) is strongly context-free.
Proof. If a CCG is allowed to restrict the use of
its rules to certain categories as in schema class
(B), then when we construct the context-free rules
by enumerating only those categories in the set C
allowed by the restriction.
</bodyText>
<figureCaption confidence="0.4195415">
Proposition 5. Any CCG that includes restriction
class (A) is strongly context-free.
</figureCaption>
<bodyText confidence="0.999531555555556">
Proof. We construct a context-free grammar with
exactly those rules in the finite set of instantiations
of the CCG rule schemata along with context-
free rules corresponding to the lexicon. This
CFG generates exactly the same derivations as the
CCG.
We have thus proved that of a wide range of the
rule schemata used to define CCGs are context-
free.
</bodyText>
<subsectionHeader confidence="0.9664555">
2.3 Combinatory Categorial Grammars in
Practice
</subsectionHeader>
<bodyText confidence="0.999825692307692">
CCGbank (Hockenmaier and Steedman, 2007)
is a corpus of CCG derivations that was semi-
automatically converted from the Wall Street Jour-
nal section of the Penn treebank. Figure 2 shows
a categorization of the rules used in CCGbank ac-
cording to the schema classes defined in the pre-
ceding section where a rule is placed into the least
general class to which it belongs. In addition to
having no generalized composition other than the
reducing variant, it should also be noted that in all
generalized composition rules, X = Y implying
that the reducing class of generalized composition
is a very natural schema class for CCGbank.
If we assume that type-raising is restricted to
those instances occurring in CCGbank4, then a
CCG consisting of schema classes (1-3), (6-7) and
(10-11) can generate all the derivations in CCG-
bank. By proposition 3, such a CCG is strongly
context-free. One could also observe that since
CCGbank is finite, its grammar is not only a
context-free grammar but can produce only a finite
number of derivations. However, our statement is
much stronger because this CCG can generate all
of the derivations in CCGbank given only the lex-
icon, the finite set of unrestricted rules and the fi-
nite number of type-raising rules.
</bodyText>
<footnote confidence="0.950356">
4Without such an assumption, parsing is intractable.
</footnote>
<page confidence="0.972237">
338
</page>
<table confidence="0.999436692307692">
Schema Class Rules Instances
Application 519 902176
Composition 102 7189
Crossed Composition 64 14114
Reducing Generalized 50 612
Crossed Composition
Generalized Composition 0 0
Generalized Crossed 0 0
Composition
Substitution 3 4
Type-Raising 27 3996
Unrestricted Rules 642 335011
Total 1407 1263102
</table>
<figureCaption confidence="0.998553">
Figure 2: The rules of CCGbank by schema class.
</figureCaption>
<bodyText confidence="0.99984775">
The Clark and Curran CCG Parser (Clark and
Curran, 2007) is a CCG parser which uses CCG-
bank as a training corpus. Despite the fact that
there is a strongly context-free CCG which gener-
ates all of the derivations in CCGbank, it is still
possible that the grammar learned by the Clark
and Curran parser is not a context-free grammar.
However, in addition to rule schemata (1-6) and
(10-11) they also include restriction class (A) by
restricting rules to only those found in the train-
ing data5. Thus, by proposition 5, the Clark and
Curran parser is a context-free parser.
</bodyText>
<sectionHeader confidence="0.998289" genericHeader="method">
3 A Latent Variable CCG Parser
</sectionHeader>
<bodyText confidence="0.999956764705882">
The context-freeness of a number of CCGs should
not be considered evidence that there is no ad-
vantage to CCG as a grammar formalism. Unlike
the context-free grammars extracted from the Penn
treebank, these allow for the categorial semantics
that accompanies any categorial parse and for a
more elegant analysis of linguistic structures such
as extraction and coordination. However, because
we now know that the CCG defined by CCGbank
is strongly context-free, we can use tools from the
CFG parsing community to improve CCG parsing.
To illustrate this point, we train the Petrov
parser (Petrov and Klein, 2007) on CCGbank.
The Petrov parser uses latent variables to refine
a coarse-grained grammar extracted from a train-
ing corpus to a grammar which makes much more
fine-grained syntactic distinctions. For example,
</bodyText>
<footnote confidence="0.9860826">
5The Clark and Curran parser has an option, which is dis-
abled by default, for not restricting the rules to those that ap-
pear in the training data. However, they find that this restric-
tion is “detrimental to neither parser accuracy or coverage”
(Clark and Curran, 2007).
</footnote>
<bodyText confidence="0.999700875">
in Petrov’s experiments on the Penn treebank, the
syntactic category NP was refined to the more
fine-grained NP1 and NP2 roughly correspond-
ing to NPs in subject and object positions. Rather
than requiring such distinctions to be made in the
corpus, the Petrov parser hypothesizes these splits
automatically.
The Petrov parser operates by performing a
fixed number of iterations of splitting, merging
and smoothing. The splitting process is done
by performing Expectation-Maximization to de-
termine a likely potential split for each syntactic
category. Then, during the merging process some
of the splits are undone to reduce grammar size
and avoid overfitting according to the likelihood
of the split against the training data.
The Petrov parser was chosen for our experi-
ments because it refines the grammar in a mathe-
matically principled way without altering the na-
ture of the derivations that are output. This is
important because the input to the semantic back-
end and the system that converts CCG derivations
to dependencies requires CCG derivations as they
appear in CCGbank.
</bodyText>
<subsectionHeader confidence="0.958638">
3.1 Experiments
</subsectionHeader>
<bodyText confidence="0.99998704">
Our experiments use CCGbank as the corpus and
we use sections 02-21 for training (39603 sen-
tences), 00 for development (1913 sentences) and
23 for testing (2407 sentences).
CCGbank, in addition to the basic atoms S, N,
NP and PP, also differentiates both the S and
NP atoms with features allowing more subtle dis-
tinctions. For example, declarative sentences are
S[dcl], wh-questions are S[wq] and sentence frag-
ments are S[frg] (Hockenmaier and Steedman,
2007). These features allow finer control of the use
of combinatory rules in the resulting grammars.
However, this fine-grained control is exactly what
the Petrov parser does automatically. Therefore,
we trained the Petrov parser twice, once on the
original version of CCGbank (denoted “Petrov”)
and once on a version of CCGbank without these
features (denoted “Petrov no feats”). Furthermore,
we will evaluate the parsers obtained after 0, 4, 5
and 6 training iterations (denoted I-0, I-4, I-5 and
I-6). When we evaluate on sets of sentences for
which not all parsers return an analysis, we report
the coverage (denoted “Cover”).
We use the evalb package for PARSEVAL
evaluation and a modified version of Clark and
</bodyText>
<page confidence="0.997746">
339
</page>
<table confidence="0.9989432">
Parser Accuracy % No feats %
C&amp;C Normal Form 92.92 93.38
C&amp;C Hybrid 93.06 93.52
Petrov I-5 93.18 93.73
Petrov no feats I-6 - 93.74
</table>
<figureCaption confidence="0.992458333333333">
Figure 3: Supertagging accuracy on the sentences
in section 00 that receive derivations from the four
parsers shown.
</figureCaption>
<table confidence="0.99023925">
Parser Accuracy % No feats %
C&amp;C Hybrid 92.98 93.43
Petrov I-5 93.10 93.59
Petrov no feats I-6 - 93.62
</table>
<figureCaption confidence="0.900907">
Figure 4: Supertagging accuracy on the sentences
in section 23 that receive derivations from the
three parsers shown.
</figureCaption>
<bodyText confidence="0.9836238">
Curran’s evaluate script for dependency eval-
uation. To determine statistical significance, we
obtain p-values from Bikel’s randomized parsing
evaluation comparator6, modified for use with tag-
ging accuracy, F-score and dependency accuracy.
</bodyText>
<subsectionHeader confidence="0.999503">
3.2 Supertag Evaluation
</subsectionHeader>
<bodyText confidence="0.99993545">
Before evaluating the parse trees as a whole, we
evaluate the categories assigned to words. In the
supertagging literature, POS tagging and supertag-
ging are distinguished – POS tags are the tradi-
tional Penn treebank tags (e.g. NN, VBZ and DT)
and supertags are CCG categories. However, be-
cause the Petrov parser trained on CCGbank has
no notion of Penn treebank POS tags, we can only
evaluate the accuracy of the supertags.
The results are shown in figures 3 and 4 where
the “Accuracy” column shows accuracy of the su-
pertags against the CCGbank categories and the
“No feats” column shows accuracy when features
are ignored. Despite the lack of POS tags in the
Petrov parser, we can see that it performs slightly
better than the Clark and Curran parser. The dif-
ference in accuracy is only statistically significant
between Clark and Curran’s Normal Form model
ignoring features and the Petrov parser trained on
CCGbank without features (p-value = 0.013).
</bodyText>
<subsectionHeader confidence="0.999647">
3.3 Constituent Evaluation
</subsectionHeader>
<bodyText confidence="0.997189333333333">
In this section we evaluate the parsers using the
traditional PARSEVAL measures which measure
recall, precision and F-score on constituents in
</bodyText>
<footnote confidence="0.965862">
6http://www.cis.upenn.edu/ dbikel/software.html
</footnote>
<bodyText confidence="0.99995765116279">
both labeled and unlabeled versions. In addition,
we report a variant of the labeled PARSEVAL
measures where we ignore the features on the cat-
egories. For reasons of brevity, we report the PAR-
SEVAL measures for all sentences in sections 00
and 23, rather than for sentences of length is less
than 40 or less than 100. The results are essentially
identical for those two sets of sentences.
Figure 5 gives the PARSEVAL measures on sec-
tion 00 for Clark and Curran’s two best models
and the Petrov parser trained on the original CCG-
bank and the version without features after various
numbers of training iterations. Figure 7 gives the
accuracies on section 23.
In the case of Clark and Curran’s hybrid model,
the poor accuracy relative to the Petrov parsers can
be attributed to the fact that this model chooses
derivations based on the associated dependencies
at the expense of constituent accuracy (see section
3.4). In the case of Clark and Curran’s normal
form model, the large difference between labeled
and unlabeled accuracy is primarily due to the mis-
labeling of a small number of features (specifi-
cally, NP[nb] and NP[num]). The labeled accu-
racies without features gives the results when fea-
tures are disregarded.
Due to the similarity of the accuracies and the
difference in the coverage between I-5 of the
Petrov parser on CCGbank and I-6 of the Petrov
parser on CCGbank without features, we reevalu-
ate their results on only those sentences for which
they both return derivations in figures 6 and 8.
These results show that the features in CCGbank
actually inhibit accuracy (to a statistically signifi-
cant degree in the case of unlabeled accuracy on
section 00) when used as training data for the
Petrov parser.
Figure 9 gives a comparison between the Petrov
parser trained on the Penn treebank and on CCG-
bank. These numbers should not be directly com-
pared, but the similarity of the unlabeled measures
indicates that the difference between the structure
of the Penn treebank and CCGbank is not large.7
</bodyText>
<subsectionHeader confidence="0.984401">
3.4 Dependency Evaluation
</subsectionHeader>
<bodyText confidence="0.999845666666667">
The constituent-based PARSEVAL measures are
simple to calculate from the output of the Petrov
parser but the relationship of the PARSEVAL
</bodyText>
<footnote confidence="0.995318">
7Because punctuation in CCG can have grammatical
function, we include it in our accuracy calculations result-
ing in lower scores for the Petrov parser trained on the Penn
treebank than those reported in Petrov and Klein (2007).
</footnote>
<page confidence="0.983951">
340
</page>
<table confidence="0.965367173913044">
Parser R Labeled % F Labeled no feats % Unlabeled % Cover
P R P F R P F
C&amp;C Normal Form 71.14 70.76 70.95 80.66 80.24 80.45 86.16 85.71 85.94 98.95
C&amp;C Hybrid 50.08 49.47 49.77 58.13 57.43 57.78 61.27 60.53 60.90 98.95
Petrov I-0 74.19 74.27 74.23 74.66 74.74 74.70 78.65 78.73 78.69 99.95
Petrov I-4 85.86 85.78 85.82 86.36 86.29 86.32 89.96 89.88 89.92 99.90
Petrov I-5 86.30 86.16 86.23 86.84 86.70 86.77 90.28 90.13 90.21 99.90
Petrov I-6 85.95 85.68 85.81 86.51 86.23 86.37 90.22 89.93 90.08 99.22
Petrov no feats I-0 - - - 72.16 72.59 72.37 76.52 76.97 76.74 99.95
Petrov no feats I-5 - - - 86.67 86.57 86.62 90.30 90.20 90.25 99.90
Petrov no feats I-6 - - - 87.45 87.37 87.41 90.99 90.91 90.95 99.84
Figure 5: Constituent accuracy on all sentences from section 00.
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F
Petrov I-5 86.56 86.46 86.51 87.10 87.01 87.05 90.43 90.33 90.38
Petrov no feats I-6 - - - 87.45 87.37 87.41 90.99 90.91 90.95
p-value - - - 0.089 0.090 0.088 0.006 0.008 0.007
Figure 6: Constituent accuracy on the sentences in section 00 that receive a derivation from both parsers.
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F Cover
C&amp;C Normal Form 71.15 70.79 70.97 80.73 80.32 80.53 86.31 85.88 86.10 99.58
Petrov I-5 86.94 86.80 86.87 87.47 87.32 87.39 90.75 90.59 90.67 99.83
Petrov no feats I-6 - - - 87.49 87.49 87.49 90.81 90.82 90.81 99.96
</table>
<figureCaption confidence="0.726803">
Figure 7: Constituent accuracy on all sentences from section 23.
</figureCaption>
<table confidence="0.943424727272727">
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F
Petrov I-5 86.94 86.80 86.87 87.47 87.32 87.39 90.75 90.59 90.67
Petrov no feats I-6 - - - 87.48 87.49 87.49 90.81 90.82 90.81
p-value - - - 0.463 0.215 0.327 0.364 0.122 0.222
Figure 8: Constituent accuracy on the sentences in section 23 that receive a derivation from both parsers.
Labeled % Unlabeled %
Parser R P F R P F Cover
Petrov on PTB I-6 89.65 89.97 89.81 90.80 91.13 90.96 100.00
Petrov on CCGbank I-5 86.94 86.80 86.87 90.75 90.59 90.67 99.83
Petrov on CCGbank no feats I-6 87.49 87.49 87.49 90.81 90.82 90.81 99.96
</table>
<figureCaption confidence="0.983772">
Figure 9: Constituent accuracy for the Petrov parser on the corpora on all sentences from Section 23.
</figureCaption>
<figure confidence="0.623818">
N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP[nb]/N N/N N/N N .
</figure>
<figureCaption confidence="0.684837">
Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .
Figure 10: The argument-functor relations for the CCG derivation in figure 1.
</figureCaption>
<page confidence="0.926382">
341
</page>
<figure confidence="0.722015">
N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP[nb]/N N/N N/N N .
Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .
</figure>
<figureCaption confidence="0.964065">
Figure 11: The set of dependencies obtained by reorienting the argument-functor edges in figure 10.
</figureCaption>
<table confidence="0.9782864">
Parser R Labeled % F Unlabeled % Cover
P R P F
C&amp;C Normal Form 84.39 85.28 84.83 90.93 91.89 91.41 98.95
C&amp;C Hybrid 84.53 86.20 85.36 90.84 92.63 91.73 98.95
Petrov I-0 79.87 78.81 79.34 87.68 86.53 87.10 96.45
Petrov I-4 84.76 85.27 85.02 91.69 92.25 91.97 96.81
Petrov I-5 85.30 85.87 85.58 92.00 92.61 92.31 96.65
Petrov I-6 84.86 85.46 85.16 91.79 92.44 92.11 96.65
Figure 12: Dependency accuracy on CCGbank dependencies on all sentences from section 00.
Labeled % Unlabeled %
Parser R P F R P F
C&amp;C Hybrid 84.71 86.35 85.52 90.96 92.72 91.83
Petrov I-5 85.50 86.08 85.79 92.12 92.75 92.44
p-value 0.005 0.189 0.187 &lt; 0.001 0.437 0.001
Figure 13: Dependency accuracy on the section 00 sentences that receive an analysis from both parsers.
Labeled % Unlabeled %
Parser R P F R P F
C&amp;C Hybrid 85.11 86.46 85.78 91.15 92.60 91.87
Petrov I-5 85.73 86.29 86.01 92.04 92.64 92.34
p-value 0.013 0.278 0.197 &lt; 0.001 0.404 0.005
Figure 14: Dependency accuracy on the section 23 sentences that receive an analysis from both parsers.
Parser Training Time Parsing Time Training RAM
in CPU minutes in CPU minutes in gigabytes
Clark and Curran Normal Form Model 1152 2 28
Clark and Curran Hybrid Model 2672 4 37
Petrov on PTB I-0 1 5 2
Petrov on PTB I-5 180 20 8
Petrov on PTB I-6 660 21 16
Petrov on CCGbank I-0 1 5 2
Petrov on CCGbank I-4 103 70 8
Petrov on CCGbank I-5 410 600 14
Petrov on CCGbank I-6 2760 2880 24
Petrov on CCGbank no feats I-0 1 5 2
Petrov on CCGbank no feats I-5 360 240 7
Petrov on CCGbank no feats I-6 1980 390 13
</table>
<figureCaption confidence="0.968176">
Figure 15: Time and space usage when training on sections 02-21 and parsing on section 00.
</figureCaption>
<page confidence="0.991903">
342
</page>
<bodyText confidence="0.999985754385965">
scores to the quality of a parse is not entirely clear.
For this reason, the word to word dependencies
of categorial grammar parsers are often evaluated.
This evaluation is aided by the fact that in addition
to the CCG derivation for each sentence, CCG-
bank also includes a set of dependencies. Fur-
thermore, extracting dependencies from a CCG
derivation is well-established (Clark et al., 2002).
A CCG derivation can be converted into de-
pendencies by, first, determining which arguments
go with which functors as specified by the CCG
derivation. This can be represented as in figure
10. Although this is not difficult, some care must
be taken with respect to punctuation and the con-
junction rules. Next, we reorient some of the
edges according to information in the lexical cat-
egories. A language for specifying these instruc-
tions using variables and indices is given in Clark
et al. (2002). This process is shown in figures 1,
10 and 11 with the directions of the dependencies
reversed from Clark et al. (2002).
We used the CCG derivation to dependency
converter generate included in the C&amp;C tools
package to convert the output of the Petrov parser
to dependencies. Other than a CCG derivation,
their system requires only the lexicon of edge re-
orientation instructions and methods for convert-
ing the unrestricted rules of CCGbank into the
argument-functor relations. Important for the pur-
pose of comparison, this system does not depend
on their parser.
An unlabeled dependency is correct if the or-
dered pair of words is correct. A labeled depen-
dency is correct if the ordered pair of words is cor-
rect, the head word has the correct category and
the position of the category that is the source of
that edge is correct. Figure 12 shows accuracies
from the Petrov parser trained on CCGbank along
with accuracies for the Clark and Curran parser.
We only show accuracies for the Petrov parser
trained on the original version of CCGbank be-
cause the dependency converter cannot currently
generate dependencies for featureless derivations.
The relatively poor coverage of the Petrov
parser is due to the failure of the dependency con-
verter to output dependencies from valid CCG
derivations. However, the coverage of the depen-
dency converter is actually lower when run on the
gold standard derivations indicating that this cov-
erage problem is not indicative of inaccuracies in
the Petrov parser. Due to the difference in cover-
age, we again evaluate the top two parsers on only
those sentences that they both generate dependen-
cies for and report those results in figures 13 and
14. The Petrov parser has better results by a sta-
tistically significant margin for both labeled and
unlabeled recall and unlabeled F-score.
</bodyText>
<subsectionHeader confidence="0.898351">
3.5 Time and Space Evaluation
</subsectionHeader>
<bodyText confidence="0.999988181818182">
As a final evaluation, we compare the resources
that are required to both train and parse with the
Petrov parser on the Penn Treebank, the Petrov
parser on the original version of CCGbank, the
Petrov parser on CCGbank without features and
the Clark and Curran parser using the two mod-
els. All training and parsing was done on a 64-bit
machine with 8 dual core 2.8 Ghz Opteron 8220
CPUs and 64GB of RAM. Our training times are
much larger than those reported in Clark and Cur-
ran (2007) because we report the cumulative time
spent on all CPUs rather than the maximum time
spent on a CPU. Figure 15 shows the results.
As can be seen, the Clark and Curran parser
has similar training times, although signifi-
cantly greater RAM requirements than the Petrov
parsers. In contrast, the Clark and Curran parser is
significantly faster than the Petrov parsers, which
we hypothesize to be attributed to the degree
to which Clark and Curran have optimized their
code, their use of C++ as opposed to Java and
their use of a supertagger to prune the lexicon.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999993">
We have provided a number of theoretical results
proving that CCGbank contains no non-context-
free structure and that the Clark and Curran parser
is actually a context-free parser. Based on these
results, we trained the Petrov parser on CCGbank
and achieved state of the art results in terms of
supertagging accuracy, PARSEVAL measures and
dependency accuracy.
This demonstrates the following. First, the abil-
ity to extract semantic representations from CCG
derivations is not dependent on the language class
of a CCG. Second, using a dedicated supertagger,
as opposed to simply using a general purpose tag-
ger, is not necessary to accurately parse with CCG.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998604666666667">
We would like to thank Stephen Clark, James Cur-
ran, Jackie C. K. Cheung and our three anonymous
reviewers for their insightful comments.
</bodyText>
<page confidence="0.99891">
343
</page>
<sectionHeader confidence="0.998336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999712552631579">
J. Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
J. Bos, S. Clark, M. Steedman, J. R Curran, and
J. Hockenmaier. 2004. Wide-coverage semantic
representations from a CCG parser. In Proceedings
of COLING, volume 4, page 1240–1246.
S. Clark and J. R. Curran. 2007. Wide-Coverage ef-
ficient statistical parsing with CCG and Log-Linear
models. Computational Linguistics, 33(4):493–552.
S. Clark, J. Hockenmaier, and M. Steedman. 2002.
Building deep dependency structures with a wide-
coverage CCG parser. In Proceedings of the 40th
Meeting of the ACL, page 327–334.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
a corpus of CCG derivations and dependency struc-
tures extracted from the penn treebank. Computa-
tional Linguistics, 33(3):355–396.
F. Hoyt and J. Baldridge. 2008. A logical basis for
the d combinator and normal form in CCG. In Pro-
ceedings of ACL-08: HLT, page 326–334, Colum-
bus, Ohio. Association for Computational Linguis-
tics.
J. Lambek. 1958. The mathematics of sen-
tence structure. American Mathematical Monthly,
65(3):154–170.
S. Petrov and D. Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings ofNAACL
HLT 2007, page 404–411.
M. Steedman. 2000. The syntactic process. MIT
Press.
K. Vijay-Shanker and D. Weir. 1994. The equivalence
of four extensions of context-free grammars. Math-
ematical Systems Theory, 27(6):511–546.
W. Zielonka. 1981. Axiomatizability of Ajdukiewicz-
Lambek calculus by means of cancellation schemes.
Zeitschrift fur Mathematische Logik und Grundla-
gen der Mathematik, 27:215–224.
</reference>
<page confidence="0.998983">
344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.859858">
<title confidence="0.999043">Accurate Context-Free Parsing with Combinatory Categorial Grammar</title>
<author confidence="0.998687">A D Fowler Penn</author>
<affiliation confidence="0.99998">Department of Computer Science, University of Toronto</affiliation>
<address confidence="0.999698">Toronto, ON, M5S 3G4, Canada</address>
<abstract confidence="0.9903738125">The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Baldridge</author>
</authors>
<title>Lexically Specified Derivational Control in Combinatory Categorial Grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="8368" citStr="Baldridge (2002)" startWordPosition="1428" endWordPosition="1429"> (B) . Steedman (2000) defines CCG to be schema classes (1-5), (6), (10) with restriction class (B). 2.2 Strongly Context-Free CCGs Proposition 1. The set of atoms in any derivation of any CCG consisting of a subset of the schema classes (1-8) and (10-11) isfinite. Proof. A finite lexicon can introduce only a finite number of atoms in lexical categories. Any rule corresponding to a schema in the schema classes (1-8) has only those atoms on the right that occur somewhere on the left. Rules in classes (10-11) can each introduce a finite number of atoms, but there can be only a finite number of 3Baldridge (2002) introduced a variant of CCG where modalities are added to the connectives / and \ along with variants of the combinatory rules based on these modalities. Our proofs about restriction class (B) are essentially identical to proofs regarding the multi-modal variant. Definition 1. The subcategories for a category c are c1 and c2 if c = c1 • c2 for • E B and c if c is atomic. Its second subcategories are the subcategories of its subcategories. Proposition 2. Any CCG consisting of a subset of the rule schemata (1-3), (6-8) and (10-11) has derivations consisting of only a finite number of categories</context>
</contexts>
<marker>Baldridge, 2002</marker>
<rawString>J. Baldridge. 2002. Lexically Specified Derivational Control in Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>S Clark</author>
<author>M Steedman</author>
<author>J R Curran</author>
<author>J Hockenmaier</author>
</authors>
<title>Wide-coverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<volume>4</volume>
<pages>1240--1246</pages>
<contexts>
<context position="1412" citStr="Bos et al., 2004" startWordPosition="223" endWordPosition="226">duction Combinatory categorial grammar (CCG) is a variant of categorial grammar which has attracted interest for both theoretical and practical reasons. On the theoretical side, we know that it is mildly context-sensitive (Vijay-Shanker and Weir, 1994) and that it can elegantly analyze a wide range of linguistic phenomena (Steedman, 2000). On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steedman, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al., 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. These differences are important because they affect whether the mild context-sensitivity proof of Vijay-Shanker and Weir (1994) applies. We will provide a generalized framework for CCG within which the full variation of CCG seen in the literature can be defined. Then, we prove that for a wide range of CCGs there is a context-free grammar (CFG) that has exactly the sa</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>J. Bos, S. Clark, M. Steedman, J. R Curran, and J. Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In Proceedings of COLING, volume 4, page 1240–1246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Wide-Coverage efficient statistical parsing with CCG and Log-Linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1319" citStr="Clark and Curran, 2007" startWordPosition="208" endWordPosition="211">ate of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy. 1 Introduction Combinatory categorial grammar (CCG) is a variant of categorial grammar which has attracted interest for both theoretical and practical reasons. On the theoretical side, we know that it is mildly context-sensitive (Vijay-Shanker and Weir, 1994) and that it can elegantly analyze a wide range of linguistic phenomena (Steedman, 2000). On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steedman, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al., 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. These differences are important because they affect whether the mild context-sensitivity proof of Vijay-Shanker and Weir (1994) applies. We will provide a generalized framework for CCG within which the full variation of CCG seen in the literature can be defined. Then, we prov</context>
<context position="4369" citStr="Clark and Curran, 2007" startWordPosition="709" endWordPosition="712">iation for Computational Linguistics, pages 335–344, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics number of atom occurrences it contains. When not specified, connectives are left associative. According to the literature, combinatory categorial grammar has been defined to have a variety of rule systems. These rule systems vary from a small rule set, motivated theoretically (VijayShanker and Weir, 1994), to a larger rule set, motivated linguistically, (Steedman, 2000) to a very large rule set, motivated by practical coverage (Hockenmaier and Steedman, 2007; Clark and Curran, 2007). We provide a definition general enough to incorporate these four main variants of CCG, as well as others. A combinatory categorial grammar (CCG) is a categorial grammar whose rule system consists of rule schemata where the left side is a sequence of categories and the right side is a single category where the categories may include variables over both categories and connectives. In addition, rule schemata may specify a sequence of categories and connectives using the ... convention1. When ... appears in a rule, it matches any sequence of categories and connectives according to the connective</context>
<context position="14830" citStr="Clark and Curran, 2007" startWordPosition="2583" endWordPosition="2586">his CCG can generate all of the derivations in CCGbank given only the lexicon, the finite set of unrestricted rules and the finite number of type-raising rules. 4Without such an assumption, parsing is intractable. 338 Schema Class Rules Instances Application 519 902176 Composition 102 7189 Crossed Composition 64 14114 Reducing Generalized 50 612 Crossed Composition Generalized Composition 0 0 Generalized Crossed 0 0 Composition Substitution 3 4 Type-Raising 27 3996 Unrestricted Rules 642 335011 Total 1407 1263102 Figure 2: The rules of CCGbank by schema class. The Clark and Curran CCG Parser (Clark and Curran, 2007) is a CCG parser which uses CCGbank as a training corpus. Despite the fact that there is a strongly context-free CCG which generates all of the derivations in CCGbank, it is still possible that the grammar learned by the Clark and Curran parser is not a context-free grammar. However, in addition to rule schemata (1-6) and (10-11) they also include restriction class (A) by restricting rules to only those found in the training data5. Thus, by proposition 5, the Clark and Curran parser is a context-free parser. 3 A Latent Variable CCG Parser The context-freeness of a number of CCGs should not be </context>
<context position="16458" citStr="Clark and Curran, 2007" startWordPosition="2855" endWordPosition="2858">we can use tools from the CFG parsing community to improve CCG parsing. To illustrate this point, we train the Petrov parser (Petrov and Klein, 2007) on CCGbank. The Petrov parser uses latent variables to refine a coarse-grained grammar extracted from a training corpus to a grammar which makes much more fine-grained syntactic distinctions. For example, 5The Clark and Curran parser has an option, which is disabled by default, for not restricting the rules to those that appear in the training data. However, they find that this restriction is “detrimental to neither parser accuracy or coverage” (Clark and Curran, 2007). in Petrov’s experiments on the Penn treebank, the syntactic category NP was refined to the more fine-grained NP1 and NP2 roughly corresponding to NPs in subject and object positions. Rather than requiring such distinctions to be made in the corpus, the Petrov parser hypothesizes these splits automatically. The Petrov parser operates by performing a fixed number of iterations of splitting, merging and smoothing. The splitting process is done by performing Expectation-Maximization to determine a likely potential split for each syntactic category. Then, during the merging process some of the sp</context>
<context position="30497" citStr="Clark and Curran (2007)" startWordPosition="5267" endWordPosition="5271">r results by a statistically significant margin for both labeled and unlabeled recall and unlabeled F-score. 3.5 Time and Space Evaluation As a final evaluation, we compare the resources that are required to both train and parse with the Petrov parser on the Penn Treebank, the Petrov parser on the original version of CCGbank, the Petrov parser on CCGbank without features and the Clark and Curran parser using the two models. All training and parsing was done on a 64-bit machine with 8 dual core 2.8 Ghz Opteron 8220 CPUs and 64GB of RAM. Our training times are much larger than those reported in Clark and Curran (2007) because we report the cumulative time spent on all CPUs rather than the maximum time spent on a CPU. Figure 15 shows the results. As can be seen, the Clark and Curran parser has similar training times, although significantly greater RAM requirements than the Petrov parsers. In contrast, the Clark and Curran parser is significantly faster than the Petrov parsers, which we hypothesize to be attributed to the degree to which Clark and Curran have optimized their code, their use of C++ as opposed to Java and their use of a supertagger to prune the lexicon. 4 Conclusion We have provided a number o</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>S. Clark and J. R. Curran. 2007. Wide-Coverage efficient statistical parsing with CCG and Log-Linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>Building deep dependency structures with a widecoverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>327--334</pages>
<contexts>
<context position="27668" citStr="Clark et al., 2002" startWordPosition="4787" endWordPosition="4790">0 2880 24 Petrov on CCGbank no feats I-0 1 5 2 Petrov on CCGbank no feats I-5 360 240 7 Petrov on CCGbank no feats I-6 1980 390 13 Figure 15: Time and space usage when training on sections 02-21 and parsing on section 00. 342 scores to the quality of a parse is not entirely clear. For this reason, the word to word dependencies of categorial grammar parsers are often evaluated. This evaluation is aided by the fact that in addition to the CCG derivation for each sentence, CCGbank also includes a set of dependencies. Furthermore, extracting dependencies from a CCG derivation is well-established (Clark et al., 2002). A CCG derivation can be converted into dependencies by, first, determining which arguments go with which functors as specified by the CCG derivation. This can be represented as in figure 10. Although this is not difficult, some care must be taken with respect to punctuation and the conjunction rules. Next, we reorient some of the edges according to information in the lexical categories. A language for specifying these instructions using variables and indices is given in Clark et al. (2002). This process is shown in figures 1, 10 and 11 with the directions of the dependencies reversed from Cl</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>S. Clark, J. Hockenmaier, and M. Steedman. 2002. Building deep dependency structures with a widecoverage CCG parser. In Proceedings of the 40th Meeting of the ACL, page 327–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the penn treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="1247" citStr="Hockenmaier and Steedman, 2007" startWordPosition="196" endWordPosition="200">s, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy. 1 Introduction Combinatory categorial grammar (CCG) is a variant of categorial grammar which has attracted interest for both theoretical and practical reasons. On the theoretical side, we know that it is mildly context-sensitive (Vijay-Shanker and Weir, 1994) and that it can elegantly analyze a wide range of linguistic phenomena (Steedman, 2000). On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steedman, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al., 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. These differences are important because they affect whether the mild context-sensitivity proof of Vijay-Shanker and Weir (1994) applies. We will provide a generalized framework for CCG within which the fu</context>
<context position="4344" citStr="Hockenmaier and Steedman, 2007" startWordPosition="705" endWordPosition="708">48th Annual Meeting of the Association for Computational Linguistics, pages 335–344, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics number of atom occurrences it contains. When not specified, connectives are left associative. According to the literature, combinatory categorial grammar has been defined to have a variety of rule systems. These rule systems vary from a small rule set, motivated theoretically (VijayShanker and Weir, 1994), to a larger rule set, motivated linguistically, (Steedman, 2000) to a very large rule set, motivated by practical coverage (Hockenmaier and Steedman, 2007; Clark and Curran, 2007). We provide a definition general enough to incorporate these four main variants of CCG, as well as others. A combinatory categorial grammar (CCG) is a categorial grammar whose rule system consists of rule schemata where the left side is a sequence of categories and the right side is a single category where the categories may include variables over both categories and connectives. In addition, rule schemata may specify a sequence of categories and connectives using the ... convention1. When ... appears in a rule, it matches any sequence of categories and connectives ac</context>
<context position="13167" citStr="Hockenmaier and Steedman, 2007" startWordPosition="2310" endWordPosition="2313">construct the context-free rules by enumerating only those categories in the set C allowed by the restriction. Proposition 5. Any CCG that includes restriction class (A) is strongly context-free. Proof. We construct a context-free grammar with exactly those rules in the finite set of instantiations of the CCG rule schemata along with contextfree rules corresponding to the lexicon. This CFG generates exactly the same derivations as the CCG. We have thus proved that of a wide range of the rule schemata used to define CCGs are contextfree. 2.3 Combinatory Categorial Grammars in Practice CCGbank (Hockenmaier and Steedman, 2007) is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank. Figure 2 shows a categorization of the rules used in CCGbank according to the schema classes defined in the preceding section where a rule is placed into the least general class to which it belongs. In addition to having no generalized composition other than the reducing variant, it should also be noted that in all generalized composition rules, X = Y implying that the reducing class of generalized composition is a very natural schema class for CCGbank. If we assume t</context>
<context position="18014" citStr="Hockenmaier and Steedman, 2007" startWordPosition="3102" endWordPosition="3105"> because the input to the semantic backend and the system that converts CCG derivations to dependencies requires CCG derivations as they appear in CCGbank. 3.1 Experiments Our experiments use CCGbank as the corpus and we use sections 02-21 for training (39603 sentences), 00 for development (1913 sentences) and 23 for testing (2407 sentences). CCGbank, in addition to the basic atoms S, N, NP and PP, also differentiates both the S and NP atoms with features allowing more subtle distinctions. For example, declarative sentences are S[dcl], wh-questions are S[wq] and sentence fragments are S[frg] (Hockenmaier and Steedman, 2007). These features allow finer control of the use of combinatory rules in the resulting grammars. However, this fine-grained control is exactly what the Petrov parser does automatically. Therefore, we trained the Petrov parser twice, once on the original version of CCGbank (denoted “Petrov”) and once on a version of CCGbank without these features (denoted “Petrov no feats”). Furthermore, we will evaluate the parsers obtained after 0, 4, 5 and 6 training iterations (denoted I-0, I-4, I-5 and I-6). When we evaluate on sets of sentences for which not all parsers return an analysis, we report the co</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>J. Hockenmaier and M. Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the penn treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Hoyt</author>
<author>J Baldridge</author>
</authors>
<title>A logical basis for the d combinator and normal form in CCG.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>326--334</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6924" citStr="Hoyt and Baldridge (2008)" startWordPosition="1166" endWordPosition="1169">Z1|2 ... |nZn → X|1Z1|2 ... |nZn • Y |1Z1|2 ...|nZn,X\Y → X|1Z1|2 ... |nZn (6) Reducing Generalized Crossed Composition Generalized Composition or Generalized Crossed Composition where |X |≤ |Y |. (7) Substitution • (X/Y )|1Z,Y |1Z → X|1Z • Y |1Z,(X\Y )|1Z → X|1Z (8) D Combinator2 • X/(Y |1Z),Y |2W → X|2(W|1Z) • Y |2W,X\(Y |1Z) → X|2(W|1Z) (9) Type-Raising • X → T/(T\X) • X → T\(T/X) (10) Finitely Restricted Type-Raising • X → T/(T\X) where hX, Ti ∈ 5 for finite 5 • X → T\(T/X) where hX, Ti ∈ 5 for finite 5 (11) Finite Unrestricted Variable-Free Rules • X� → Y where h X, Y i ∈ 5 for finite 5 2Hoyt and Baldridge (2008) argue for the inclusion of the D Combinator in CCG. 336 Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group . Figure 1: A CCG derivation from section 00 of CCGbank. 5[dcl] 5[dcl] 5[dcl]\NP NP NP\NP NP NP [conj] NP NP N N/N N 5[dcl]\NP/NP N NP\NP/NP N/N N , NP[nb]/N N/N N/N N . NP N NP N N We define the following restriction classes: such rules, limiting the new atoms to a finite number. (A) Rule Restriction to a Finite Set The rule schemata in the schema classes of a CCG are limited to a finite number of instantiations. (B) Rule Restrictions to Certain Categories 3 The rule s</context>
</contexts>
<marker>Hoyt, Baldridge, 2008</marker>
<rawString>F. Hoyt and J. Baldridge. 2008. A logical basis for the d combinator and normal form in CCG. In Proceedings of ACL-08: HLT, page 326–334, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>The mathematics of sentence structure.</title>
<date>1958</date>
<journal>American Mathematical Monthly,</journal>
<volume>65</volume>
<issue>3</issue>
<contexts>
<context position="5428" citStr="Lambek, 1958" startWordPosition="894" endWordPosition="895">s and connectives using the ... convention1. When ... appears in a rule, it matches any sequence of categories and connectives according to the connectives adjacent to the .... For example, the rule schema for forward composition is: X/Y, Y/Z → X/Z and the rule schema for generalized forward crossed composition is: X/Y,Y |1Z1|2 ... |nZn → X|1Z1|2 ... |nZn where X, Y and Zi for 1 ≤ i ≤ n are variables over categories and |i for 1 ≤ i ≤ n are variables over connectives. Figure 1 shows a CCG derivation from CCGbank. A well-known categorial grammar which is not a CCG is Lambek categorial grammar (Lambek, 1958) whose introduction rules cannot be characterized as combinatory rules (Zielonka, 1981). 2.1 Classes for defining CCG We define a number of schema classes general enough that the important variants of CCG can be defined by selecting some subset of the classes. In addition to the schema classes, we also define two restriction classes which define ways in which the rule schemata from the schema classes can be restricted. We define the following schema classes: 1The ... convention (Vijay-Shanker and Weir, 1994) is essentially identical to the $ convention of Steedman (2000). (1) Application • X/Y</context>
</contexts>
<marker>Lambek, 1958</marker>
<rawString>J. Lambek. 1958. The mathematics of sentence structure. American Mathematical Monthly, 65(3):154–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings ofNAACL HLT 2007,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="670" citStr="Petrov and Klein (2007)" startWordPosition="106" endWordPosition="109">Categorial Grammar Timothy A. D. Fowler and Gerald Penn Department of Computer Science, University of Toronto Toronto, ON, M5S 3G4, Canada {tfowler, gpenn}@cs.toronto.edu Abstract The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy. 1 Introduction Combinatory categorial grammar (CCG) is a variant of categorial grammar which has attracted interest for both theoretical and practical reasons. On the theoretical side, we know that it is mildly context-sensitive (Vijay-Shanker and Weir, 1994) and that it can elegantly analyze a wide range of linguistic phenomena (Steedman, 2000). On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steedman, 2007), a wide-coverage parse</context>
<context position="2365" citStr="Petrov and Klein, 2007" startWordPosition="381" endWordPosition="384">ijay-Shanker and Weir (1994) applies. We will provide a generalized framework for CCG within which the full variation of CCG seen in the literature can be defined. Then, we prove that for a wide range of CCGs there is a context-free grammar (CFG) that has exactly the same derivations. Included in this class of strongly context-free CCGs are a grammar including all the derivations in CCGbank and the grammar used in the Clark and Curran parser. Due to this insight, we investigate the potential of using tools from the probabilistic CFG community to improve CCG parsing results. The Petrov parser (Petrov and Klein, 2007) uses latent variables to refine the grammar extracted from a corpus to improve accuracy, originally used to improve parsing results on the Penn treebank (PTB). We train the Petrov parser on CCGbank and achieve the best results to date on sentences from section 23 in terms of supertagging accuracy, PARSEVAL measures and dependency accuracy. These results should not be interpreted as proof that grammars extracted from the Penn treebank and from CCGbank are equivalent. Bos’s system for building semantic representations from CCG derivations is only possible due to the categorial nature of CCG. Fu</context>
<context position="15984" citStr="Petrov and Klein, 2007" startWordPosition="2777" endWordPosition="2780">le CCG Parser The context-freeness of a number of CCGs should not be considered evidence that there is no advantage to CCG as a grammar formalism. Unlike the context-free grammars extracted from the Penn treebank, these allow for the categorial semantics that accompanies any categorial parse and for a more elegant analysis of linguistic structures such as extraction and coordination. However, because we now know that the CCG defined by CCGbank is strongly context-free, we can use tools from the CFG parsing community to improve CCG parsing. To illustrate this point, we train the Petrov parser (Petrov and Klein, 2007) on CCGbank. The Petrov parser uses latent variables to refine a coarse-grained grammar extracted from a training corpus to a grammar which makes much more fine-grained syntactic distinctions. For example, 5The Clark and Curran parser has an option, which is disabled by default, for not restricting the rules to those that appear in the training data. However, they find that this restriction is “detrimental to neither parser accuracy or coverage” (Clark and Curran, 2007). in Petrov’s experiments on the Penn treebank, the syntactic category NP was refined to the more fine-grained NP1 and NP2 rou</context>
<context position="23032" citStr="Petrov and Klein (2007)" startWordPosition="3920" endWordPosition="3923">ined on the Penn treebank and on CCGbank. These numbers should not be directly compared, but the similarity of the unlabeled measures indicates that the difference between the structure of the Penn treebank and CCGbank is not large.7 3.4 Dependency Evaluation The constituent-based PARSEVAL measures are simple to calculate from the output of the Petrov parser but the relationship of the PARSEVAL 7Because punctuation in CCG can have grammatical function, we include it in our accuracy calculations resulting in lower scores for the Petrov parser trained on the Penn treebank than those reported in Petrov and Klein (2007). 340 Parser R Labeled % F Labeled no feats % Unlabeled % Cover P R P F R P F C&amp;C Normal Form 71.14 70.76 70.95 80.66 80.24 80.45 86.16 85.71 85.94 98.95 C&amp;C Hybrid 50.08 49.47 49.77 58.13 57.43 57.78 61.27 60.53 60.90 98.95 Petrov I-0 74.19 74.27 74.23 74.66 74.74 74.70 78.65 78.73 78.69 99.95 Petrov I-4 85.86 85.78 85.82 86.36 86.29 86.32 89.96 89.88 89.92 99.90 Petrov I-5 86.30 86.16 86.23 86.84 86.70 86.77 90.28 90.13 90.21 99.90 Petrov I-6 85.95 85.68 85.81 86.51 86.23 86.37 90.22 89.93 90.08 99.22 Petrov no feats I-0 - - - 72.16 72.59 72.37 76.52 76.97 76.74 99.95 Petrov no feats I-5 - -</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings ofNAACL HLT 2007, page 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1135" citStr="Steedman, 2000" startWordPosition="180" endWordPosition="181">ng the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy. 1 Introduction Combinatory categorial grammar (CCG) is a variant of categorial grammar which has attracted interest for both theoretical and practical reasons. On the theoretical side, we know that it is mildly context-sensitive (Vijay-Shanker and Weir, 1994) and that it can elegantly analyze a wide range of linguistic phenomena (Steedman, 2000). On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steedman, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al., 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. These differences are important because they affect whether the mild context-sensitivity pro</context>
<context position="4254" citStr="Steedman, 2000" startWordPosition="692" endWordPosition="693">)/ NP, ...}). For a category C, its size |C |is the 335 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 335–344, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics number of atom occurrences it contains. When not specified, connectives are left associative. According to the literature, combinatory categorial grammar has been defined to have a variety of rule systems. These rule systems vary from a small rule set, motivated theoretically (VijayShanker and Weir, 1994), to a larger rule set, motivated linguistically, (Steedman, 2000) to a very large rule set, motivated by practical coverage (Hockenmaier and Steedman, 2007; Clark and Curran, 2007). We provide a definition general enough to incorporate these four main variants of CCG, as well as others. A combinatory categorial grammar (CCG) is a categorial grammar whose rule system consists of rule schemata where the left side is a sequence of categories and the right side is a single category where the categories may include variables over both categories and connectives. In addition, rule schemata may specify a sequence of categories and connectives using the ... convent</context>
<context position="6005" citStr="Steedman (2000)" startWordPosition="987" endWordPosition="988">mbek categorial grammar (Lambek, 1958) whose introduction rules cannot be characterized as combinatory rules (Zielonka, 1981). 2.1 Classes for defining CCG We define a number of schema classes general enough that the important variants of CCG can be defined by selecting some subset of the classes. In addition to the schema classes, we also define two restriction classes which define ways in which the rule schemata from the schema classes can be restricted. We define the following schema classes: 1The ... convention (Vijay-Shanker and Weir, 1994) is essentially identical to the $ convention of Steedman (2000). (1) Application • X/Y,Y → X • Y,X\Y → X (2) Composition • X/Y, Y/Z → X/Z • Y \Z, X\Y → X\Z (3) Crossed Composition • X/Y,Y\Z → X\Z • Y/Z, X\Y → X/Z (4) Generalized Composition • X/Y,Y/Z1/... /Zn → X/Z1/... /Zn • Y \Z1\... \Zn,X\Y → X\Z1\... \Zn (5) Generalized Crossed Composition • X/Y, Y |1Z1|2 ... |nZn → X|1Z1|2 ... |nZn • Y |1Z1|2 ...|nZn,X\Y → X|1Z1|2 ... |nZn (6) Reducing Generalized Crossed Composition Generalized Composition or Generalized Crossed Composition where |X |≤ |Y |. (7) Substitution • (X/Y )|1Z,Y |1Z → X|1Z • Y |1Z,(X\Y )|1Z → X|1Z (8) D Combinator2 • X/(Y |1Z),Y |2W → X|2(</context>
<context position="7774" citStr="Steedman (2000)" startWordPosition="1325" endWordPosition="1326">N N/N N 5[dcl]\NP/NP N NP\NP/NP N/N N , NP[nb]/N N/N N/N N . NP N NP N N We define the following restriction classes: such rules, limiting the new atoms to a finite number. (A) Rule Restriction to a Finite Set The rule schemata in the schema classes of a CCG are limited to a finite number of instantiations. (B) Rule Restrictions to Certain Categories 3 The rule schemata in the schema classes of a CCG are limited to a finite number of instantiations although variables are allowed in the instantiations. Vijay-Shanker and Weir (1994) define CCG to be schema class (4) with restriction class (B) . Steedman (2000) defines CCG to be schema classes (1-5), (6), (10) with restriction class (B). 2.2 Strongly Context-Free CCGs Proposition 1. The set of atoms in any derivation of any CCG consisting of a subset of the schema classes (1-8) and (10-11) isfinite. Proof. A finite lexicon can introduce only a finite number of atoms in lexical categories. Any rule corresponding to a schema in the schema classes (1-8) has only those atoms on the right that occur somewhere on the left. Rules in classes (10-11) can each introduce a finite number of atoms, but there can be only a finite number of 3Baldridge (2002) intro</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The syntactic process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D Weir</author>
</authors>
<title>The equivalence of four extensions of context-free grammars.</title>
<date>1994</date>
<booktitle>Mathematical Systems Theory,</booktitle>
<pages>27--6</pages>
<contexts>
<context position="1047" citStr="Vijay-Shanker and Weir, 1994" startWordPosition="164" endWordPosition="167">he language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy. 1 Introduction Combinatory categorial grammar (CCG) is a variant of categorial grammar which has attracted interest for both theoretical and practical reasons. On the theoretical side, we know that it is mildly context-sensitive (Vijay-Shanker and Weir, 1994) and that it can elegantly analyze a wide range of linguistic phenomena (Steedman, 2000). On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steedman, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al., 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. Thes</context>
<context position="5941" citStr="Vijay-Shanker and Weir, 1994" startWordPosition="975" endWordPosition="978">ivation from CCGbank. A well-known categorial grammar which is not a CCG is Lambek categorial grammar (Lambek, 1958) whose introduction rules cannot be characterized as combinatory rules (Zielonka, 1981). 2.1 Classes for defining CCG We define a number of schema classes general enough that the important variants of CCG can be defined by selecting some subset of the classes. In addition to the schema classes, we also define two restriction classes which define ways in which the rule schemata from the schema classes can be restricted. We define the following schema classes: 1The ... convention (Vijay-Shanker and Weir, 1994) is essentially identical to the $ convention of Steedman (2000). (1) Application • X/Y,Y → X • Y,X\Y → X (2) Composition • X/Y, Y/Z → X/Z • Y \Z, X\Y → X\Z (3) Crossed Composition • X/Y,Y\Z → X\Z • Y/Z, X\Y → X/Z (4) Generalized Composition • X/Y,Y/Z1/... /Zn → X/Z1/... /Zn • Y \Z1\... \Zn,X\Y → X\Z1\... \Zn (5) Generalized Crossed Composition • X/Y, Y |1Z1|2 ... |nZn → X|1Z1|2 ... |nZn • Y |1Z1|2 ...|nZn,X\Y → X|1Z1|2 ... |nZn (6) Reducing Generalized Crossed Composition Generalized Composition or Generalized Crossed Composition where |X |≤ |Y |. (7) Substitution • (X/Y )|1Z,Y |1Z → X|1Z • Y</context>
<context position="7695" citStr="Vijay-Shanker and Weir (1994)" startWordPosition="1309" endWordPosition="1312">G derivation from section 00 of CCGbank. 5[dcl] 5[dcl] 5[dcl]\NP NP NP\NP NP NP [conj] NP NP N N/N N 5[dcl]\NP/NP N NP\NP/NP N/N N , NP[nb]/N N/N N/N N . NP N NP N N We define the following restriction classes: such rules, limiting the new atoms to a finite number. (A) Rule Restriction to a Finite Set The rule schemata in the schema classes of a CCG are limited to a finite number of instantiations. (B) Rule Restrictions to Certain Categories 3 The rule schemata in the schema classes of a CCG are limited to a finite number of instantiations although variables are allowed in the instantiations. Vijay-Shanker and Weir (1994) define CCG to be schema class (4) with restriction class (B) . Steedman (2000) defines CCG to be schema classes (1-5), (6), (10) with restriction class (B). 2.2 Strongly Context-Free CCGs Proposition 1. The set of atoms in any derivation of any CCG consisting of a subset of the schema classes (1-8) and (10-11) isfinite. Proof. A finite lexicon can introduce only a finite number of atoms in lexical categories. Any rule corresponding to a schema in the schema classes (1-8) has only those atoms on the right that occur somewhere on the left. Rules in classes (10-11) can each introduce a finite nu</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1994</marker>
<rawString>K. Vijay-Shanker and D. Weir. 1994. The equivalence of four extensions of context-free grammars. Mathematical Systems Theory, 27(6):511–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zielonka</author>
</authors>
<title>Axiomatizability of AjdukiewiczLambek calculus by means of cancellation schemes.</title>
<date>1981</date>
<booktitle>Zeitschrift fur Mathematische Logik und Grundlagen der Mathematik,</booktitle>
<pages>27--215</pages>
<contexts>
<context position="5515" citStr="Zielonka, 1981" startWordPosition="906" endWordPosition="907">ny sequence of categories and connectives according to the connectives adjacent to the .... For example, the rule schema for forward composition is: X/Y, Y/Z → X/Z and the rule schema for generalized forward crossed composition is: X/Y,Y |1Z1|2 ... |nZn → X|1Z1|2 ... |nZn where X, Y and Zi for 1 ≤ i ≤ n are variables over categories and |i for 1 ≤ i ≤ n are variables over connectives. Figure 1 shows a CCG derivation from CCGbank. A well-known categorial grammar which is not a CCG is Lambek categorial grammar (Lambek, 1958) whose introduction rules cannot be characterized as combinatory rules (Zielonka, 1981). 2.1 Classes for defining CCG We define a number of schema classes general enough that the important variants of CCG can be defined by selecting some subset of the classes. In addition to the schema classes, we also define two restriction classes which define ways in which the rule schemata from the schema classes can be restricted. We define the following schema classes: 1The ... convention (Vijay-Shanker and Weir, 1994) is essentially identical to the $ convention of Steedman (2000). (1) Application • X/Y,Y → X • Y,X\Y → X (2) Composition • X/Y, Y/Z → X/Z • Y \Z, X\Y → X\Z (3) Crossed Compo</context>
</contexts>
<marker>Zielonka, 1981</marker>
<rawString>W. Zielonka. 1981. Axiomatizability of AjdukiewiczLambek calculus by means of cancellation schemes. Zeitschrift fur Mathematische Logik und Grundlagen der Mathematik, 27:215–224.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>