<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000035">
<title confidence="0.856513">
Efficient Inference of CRFs for Large-Scale Natural Language Data
</title>
<author confidence="0.944743">
Minwoo Jeong†* Chin-Yew Lin$ Gary Geunbae Lee†
</author>
<affiliation confidence="0.994878">
†Pohang University of Science &amp; Technology, Pohang, Korea
$Microsoft Research Asia, Beijing, China
</affiliation>
<email confidence="0.990374">
†{stardust,gblee}@postech.ac.kr $cyl@microsoft.com
</email>
<sectionHeader confidence="0.997271" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966076923077">
This paper presents an efficient inference algo-
rithm of conditional random fields (CRFs) for
large-scale data. Our key idea is to decompose
the output label state into an active set and an
inactive set in which most unsupported tran-
sitions become a constant. Our method uni-
fies two previous methods for efficient infer-
ence of CRFs, and also derives a simple but
robust special case that performs faster than
exact inference when the active sets are suffi-
ciently small. We demonstrate that our method
achieves dramatic speedup on six standard nat-
ural language processing problems.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993548888888889">
Conditional random fields (CRFs) are widely used in
natural language processing, but extending them to
large-scale problems remains a significant challenge.
For simple graphical structures (e.g. linear-chain), an
exact inference can be obtained efficiently if the num-
ber of output labels is not large. However, for large
number of output labels, the inference is often pro-
hibitively expensive.
To alleviate this problem, researchers have begun to
study the methods of increasing inference speeds of
CRFs. Pal et al. (2006) proposed a Sparse Forward-
Backward (SFB) algorithm, in which marginal distribu-
tion is compressed by approximating the true marginals
using Kullback-Leibler (KL) divergence. Cohn (2006)
proposed a Tied Potential (TP) algorithm which con-
strains the labeling considered in each feature function,
such that the functions can detect only a relatively small
set of labels. Both of these techniques efficiently com-
pute the marginals with a significantly reduced runtime,
resulting in faster training and decoding of CRFs.
This paper presents an efficient inference algorithm
of CRFs which unifies the SFB and TP approaches. We
first decompose output labels states into active and in-
active sets. Then, the active set is selected by feasible
heuristics and the parameters of the inactive set are held
a constant. The idea behind our method is that not all
of the states contribute to the marginals, that is, only a
*Parts of this work were conducted during the author’s
internship at Microsoft Research Asia.
small group of the labeling states has sufficient statis-
tics. We show that the SFB and the TP are special cases
of our method because they derive from our unified al-
gorithm with a different setting of parameters. We also
present a simple but robust variant algorithm in which
CRFs efficiently learn and predict large-scale natural
language data.
</bodyText>
<sectionHeader confidence="0.998267" genericHeader="method">
2 Linear-chain CRFs
</sectionHeader>
<bodyText confidence="0.999155333333333">
Many versions of CRFs have been developed for use
in natural language processing, computer vision, and
machine learning. For simplicity, we concentrate on
linear-chain CRFs (Lafferty et al., 2001; Sutton and
McCallum, 2006), but the generic idea described here
can be extended to CRFs of any structure.
Linear-chain CRFs are conditional probability dis-
tributions over label sequences which are conditioned
on input sequences (Lafferty et al., 2001). Formally,
x = {xt}Tt=1 and y = {yt}Tt=1 are sequences of in-
put and output variables. Respectively, where T is the
length of sequence, xt E X and yt E Y where X is the
finite set of the input observations and Y is that of the
output label state space. Then, a first-order linear-chain
CRF is defined as:
</bodyText>
<equation confidence="0.997127">
Ψt(yt, yt−1, x), (1)
x)
t=1
</equation>
<bodyText confidence="0.999939833333333">
where Ψt is the local potential that denotes the factor
at time t, and A is the parameter vector. Z(x) is a
partition function which ensures the probabilities of all
state sequences sum to one. We assume that the poten-
tials factorize according to a set of observation features
{01k} and transition features {02k}, as follows:
</bodyText>
<equation confidence="0.9996065">
1 2
Ψt(yt,yt−1,x) =Ψt(yt,x) &apos;Ψt(yt,yt−1), (2)
Ψt(yt, x) =eE k λ1k φ1k(yt,X), (3)
Ψt(yt,yt−1) =eEk λ�kφ�k(yt,yt−1), (4)
</equation>
<bodyText confidence="0.999958714285714">
where {a1k} and {A2k} are weight parameters which we
wish to learn from data.
Inference is significantly challenging both in learn-
ing and decoding CRFs. Time complexity is O(T|Y|2)
for exact inference (i.e., forward-backward and Viterbi
algorithm) of linear-chain CRFs (Lafferty et al., 2001).
The inference process is often prohibitively expensive
</bodyText>
<equation confidence="0.986665">
T
1
pa(y|x) =
Z(
�
</equation>
<page confidence="0.972393">
281
</page>
<note confidence="0.923419">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 281–284,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9974875">
when |Y |is large, as is common in large-scale tasks.
This problem can be alleviated by introducing approx-
imate inference methods based on reduction of the
search spaces to be explored.
</bodyText>
<sectionHeader confidence="0.960023" genericHeader="method">
3 Efficient Inference Algorithm
</sectionHeader>
<subsectionHeader confidence="0.99953">
3.1 Method
</subsectionHeader>
<bodyText confidence="0.9981025">
The key idea of our proposed efficient inference
method is that the output label state Y can be decom-
posed to an active set A and an inactive set Ac. Intu-
itively, many of the possible transitions (yt−1 → yt) do
not occur, or are unsupported, that is, only a small part
of the possible labeling set is informative. The infer-
ence algorithm need not precisely calculate marginals
or maximums (more generally, messages) for unsup-
ported transitions. Our efficient inference algorithm
approximates the unsupported transitions by assigning
them a constant value. When |A |&lt; |Y|, both train-
ing and decoding times are remarkably reduced by this
approach.
We first define the notation for our algorithm. Let
Ai be the active set and Aci be the inactive set of output
label i where Yi = Ai ∪ Aci. We define Ai as:
</bodyText>
<equation confidence="0.983824">
Ai = {j|δ(yt = i, yt−1 = j) &gt; 2} (5)
</equation>
<bodyText confidence="0.696633">
where δ is a criterion function of transitions (yt−1 →
yt) and 2 is a hyperparameter. For clarity, we define the
local factors as:
</bodyText>
<equation confidence="0.990761">
Ψ1t,i = Ψ1t(yt = i, X), (6)
Ψ2 j,i g Ψ2 t (yt−1 = j, yt = i). (7)
</equation>
<bodyText confidence="0.999959733333333">
Note that we can ignore the subscript t at Ψ2t (yt−1 =
j, yt = i) by defining an HMM-like model, that is,
transition matrix Ψ2 j,i is independent of t.
As exact inference, we use the forward-backward
procedure to calculate marginals (Sutton and McCal-
lum, 2006). We formally describe here an efficient
calculation of α and β recursions for the forward-
backward procedure. The forward value αt(i) is the
sum of the unnormalized scores for all partial paths that
start at t = 0 and converge at yt = i at time t. The
backward value βt(i) similarly defines the sum of un-
normalized scores for all partial paths that start at time
t + 1 with state yt+1 = j and continue until the end
of the sequences, t = T + 1. Then, we decompose the
equations of exact α and β recursions as follows:
</bodyText>
<equation confidence="0.9784926">
⎛ ⎞
⎝ X3 ´
αt(i) = Ψ1 Ψ2 j,i − ω αt−1(j) + ω ⎠ , (8)
t,i
j∈A;
</equation>
<bodyText confidence="0.676452333333333">
where
is a shared transition parameter value for set
that is,
=
if j
Note that
</bodyText>
<equation confidence="0.983838064516129">
ω
Aci,
Ψ2j,i
ω
∈Aci.
Pi αt(i) = 1
t(
)
t,i
max
mA
,i
t
1(j),ma
x
t
1(
)
γ
i
=Ψ
1/2
µ
Ψ2j
γ
−
Y
ωγ
−
j
¶3/4
</equation>
<bodyText confidence="0.991502538461539">
282 (Sutton and McCallum, 2006). Because all unsup-
ported transitions in
are calculated simultaneously,
the complexities of Eq. (8) and (9) are approximately
where
is the average number of
case complexity of our
equations is
Similarly, we decompose a
recursion for the
Viterbi algorithm as foll
where
is the sum of unnormalized scores for the
</bodyText>
<figure confidence="0.773996571428571">
best-scored partial path that starts at time t = 0 and
converges at yt = i at time t. Because
is constant,
can be pre-calculated at time t
1.
By analogy with Eq. (8) and (9), the complexity is ap-
proximately
</figure>
<subsectionHeader confidence="0.666463">
3.2 Setting
</subsectionHeader>
<bodyText confidence="0.892545714285714">
and
To implement our inference algorithm, we need a
method of choosing appropriate values for the setting
function
of the active set and for the constant value
of the inactive set. These two problems are closely
related. The size of the active set affects both the com-
plexity of inference algorithm and the quality of the
model. Therefore, our goal for selecting
and
is
to make a plausible assumption that does not sacrifice
much accuracy but speeds up when applying large state
tasks. We describe four variant special case algorithms.
</bodyText>
<equation confidence="0.9107505">
Method 1: We set
j) = Z(L) and
</equation>
<bodyText confidence="0.962861523809524">
= 0 where
L is a beam set, L =
... ,
and the sub-
partition function Z(L) is approximated by Z(L)
In this method, all sub-marginals in the inac-
tive set are totally excluded from calculation of the cur-
rent marginal.
and
in the inactive sets are set to 0
by default. Therefore, at each time step t the algorithm
prunes all states i in which
&lt;
It also generates
a subset L of output labels that will be exploited in next
time step t + 1.1 This method has been derived the-
oretically from the process of selecting a compressed
marginal distribution within a fixed KL divergence of
the true marginal (Pal et al., 2006). This method most
closely resembles SFB algorithm; hence we refer an al-
ternative of SFB.
</bodyText>
<equation confidence="0.442185512195122">
Method 2: We define
j)
and
= 1.
In practice, unsupported transition features are not pa-
rameterized2; this means that
= 0 and
= 1
if j
γt(i)
ω
maxj∈Yγt−1(j)
−
O(T|Aavg||Y|).
δ
ω
δ
ω
δ
ω
δ(i,
ω
{l1,l2,
lm}
≈αt−1(j).
α
β
αt(i)
2.
δ(i,
=|Ψ2j,i−1|
ω
λk
Ψ2j,i
∈Aci. Thus, this method estimates nearly-exact
Ψ1t,iβt(i),
(9)
Aci
O(T|Aavg||Y|)
|Aavg|
PT
</equation>
<bodyText confidence="0.233436">
states in the active set, i.e., 1 t=1 |Ai|. The worst
</bodyText>
<equation confidence="0.330722833333333">
T
α andβ
O(T|Y|2).
γ
ows:
(10)
</equation>
<bodyText confidence="0.961044">
practice, dynamically selecting L increases the num-
ber of computations, and this is the main disadvantage of
Method 1. However, in inactive sets
= 0 by de-
fault; hence, we need not calculate
Therefore, it
counterbalances the extra computations in Q recursion.
is a common practice in implementation of input
and output joint feature functions for large-scale problems.
This scheme uses only supported features that are used at
least once in the training examples. We call it the sparse
model. While a complete and dense feature
</bodyText>
<equation confidence="0.825472333333333">
1In
αt−1(j)
Qt−1(j).
2This
model may per-
X 3 ´ X
βt−1(j) = Ψ1 Ψ2 j,i − ω βt(i) + ω
i∈Aj t,i
i∈Y
</equation>
<bodyText confidence="0.999725555555556">
CRFs if the hyperparameter is 2 = 0; hence this cri-
terion does not change the parameter. Although this
method is simple, it is sufficiently efficient for training
and decoding CRFs in real data.
Method 3: We define 6(i, j) = Ep(02(i, j)) where
Ep(z) is an empirical count of event z in training data.
We also assign a real value for the inactive set, i.e.,
w = c E ]IR, c =� 0, 1. The value c is estimated in the
training phase; hence, c is a shared parameter for the
inactive set. This method is equivalent to TP (Cohn,
2006). By setting 2 larger, we can achieve faster infer-
ence, a tradeoff exists between efficiency and accuracy.
Method 4: We define the shared parameter as a func-
tion of output label y in the inactive set, i.e., c(y). As in
Method 3, c(y) is estimated during the training phase.
When the problem expects different aspects of unsup-
ported transitions, this method would be better than us-
ing only one parameter c for all labels in inactive set.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.99947871875">
We evaluated our method on six large-scale natu-
ral language data sets (Table 1): Penn Treebank3
for part-of-speech tagging (PTB), phrase chunk-
ing data4 (CoNLL00), named entity recognition
data5 (CoNLL03), grapheme-to-phoneme conversion
data6 (NetTalk), spoken language understanding data
(Communicator) (Jeong and Lee, 2006), and fine-
grained named entity recognition data (Encyclopedia)
(Lee et al., 2007). The active set is sufficiently small in
Communicator and Encyclopedia despite their large
numbers of output labels. In all data sets, we selected
the current word, f2 context words, bigrams, trigrams,
and prefix and suffix features as basic feature templates.
A template of part-of-speech tag features was added for
CoNLL00, CoNLL03, and Encyclopedia. In particu-
lar, all tasks except PTB and NetTalk require assigning
a label to a phrase rather than to a word; hence, we used
standard “BIO” encoding. We used un-normalized log-
likelihood, accuracy and training/decoding times as our
evaluation measures. We did not use cross validation
and development set for tuning the parameter because
our goal is to evaluate the efficiency of inference algo-
rithms. Moreover, using the previous state-of-the-art
features we expect the achievement of better accuracy.
All our models were trained until parameter estima-
tion converged with a Gaussian prior variance of 4.
During training, a pseudo-likelihood parameter estima-
tion (Sutton and McCallum, 2006) was used as an ini-
tial weight (estimated in 30 iterations). We used com-
plete and dense input/output joint features for dense
model (Dense), and only supported features that are
used at least once in the training examples for sparse
</bodyText>
<footnote confidence="0.942024333333333">
form better, the sparse model performs well in practice with-
out significant loss of accuracy (Sha and Pereira, 2003).
3Penn Treebank3: Catalog No. LDC99T42
4http://www.cnts.ua.ac.be/conll2000/chunking/
5http://www.cnts.ua.ac.be/conll2003/ner/
6http://archive.ics.uci.edu/ml/
</footnote>
<tableCaption confidence="0.9549">
Table 1: Data sets: number of sentences in the train-
</tableCaption>
<bodyText confidence="0.8063456">
ing (#Train) and the test data sets (#Test), and number
of output labels (#Label). |A��1
���  |denotes the average
number of active set when w = 1, i.e., the supported
transitions that are used at least once in the training set.
</bodyText>
<table confidence="0.99940775">
Set #Train #Test #Label |A���
��� |
PTB 38,219 5462 45 30.01
CoNLL00 8,936 2,012 22 6.59
CoNLL03 14,987 3,684 8 4.13
NetTalk 18,008 2,000 51 22.18
Communicator 13,111 1,193 120 3.67
Encyclopedia 25,348 6,336 279 3.27
</table>
<bodyText confidence="0.99951869047619">
model (Sparse). All of our model variants were based
on Sparse model. For the hyper parameter e, we empir-
ically selected 0.001 for Method 1 (this preserves 99%
of probability density), 0 for Method 2, and 4 for Meth-
ods 3 and 4. Note that 2 for Methods 2, 3, and 4 indi-
cates an empirical count of features in training set. All
experiments were implemented in C++ and executed in
Windows 2003 with XEON 2.33 GHz Quad-Core pro-
cessor and 8.0 Gbyte of main memory.
We first show that our method is efficient for learning
CRFs (Figure 1). In all learning curves, Dense gener-
ally has a higher training log-likelihood than Sparse.
For PTB and Encyclopedia, results for Dense are not
available because training in a single machine failed
due to out-of-memory errors. For both Dense and
Sparse, we executed the exact inference method. Our
proposed method (Method 1-4) performs faster than
Sparse. In most results, Method 1 was the fastest, be-
cause it was terminated after fewer iterations. How-
ever, Method 1 sometimes failed to converge, for ex-
ample, in Encyclopedia. Similarly, Method 3 and 4
could not find the optimal solution in the NetTalk data
set. Method 2 showed stable results.
Second, we evaluated the accuracy and decoding
time of our methods (Table 2). Most results obtained
using our method were as accurate as those of Dense
and Sparse. However, some results of Method 1, 3,
and 4 were significantly inferior to those of Dense and
Sparse for one of two reasons: 1) parameter estimation
failed (NetTalk and Encyclopedia), or 2) approximate
inference caused search errors (CoNLL00 and Com-
municator). The improvements of decoding time on
Communicator and Encyclopedia were remarkable.
Finally, we compared our method with two open-
source implementations of CRFs: MALLET7 and
CRF++8. MALLET can support the Sparse model, and
the CRF++ toolkit implements only the Dense model.
We compared them with Method 2 on the Commu-
nicator data set. In the accuracy measure, the re-
sults were 91.56 (MALLET), 91.87 (CRF++), and 91.92
(ours). Our method performs 5-50 times faster for
training (1,774 s for MALLET, 18,134 s for CRF++,
</bodyText>
<footnote confidence="0.9991545">
7Ver. 2.0 RC3, http://mallet.cs.umass.edu/
8Ver. 0.51, http://crfpp.sourceforge.net/
</footnote>
<page confidence="0.997856">
283
</page>
<figure confidence="0.999922733333333">
(a) PTB
0 10000 20000 30000 40000
Training time (sec)
(d) NetTalk
(b) CoNLL00
0 500 1500 2500
Training time (sec)
(e) Communicator
(c) CoNLL03
0 500 1000 1500
Training time (sec)
(f) Encyclopedia
Sparse
Method 1
Method 2
Method 3
Method 4
0
0 1000 3000 5000
Training time (sec)
og
Method 1
Method 2
Method 3
Method 4
Dense
Sparse
Method 1
Method 2
Method 3
Method 4
Dense
Method 1
Method 2
Method 3
Method 4
0 10
Training time (sec)
og−
og
0 1000 3000 5000
10
0
Dense
Sparse
Method 1
Method 2
Method 3
Method 4
3
0 100000 200000 300000
Training time (sec)
Method 1
Method 2
Method 3
Method 4
Dense
Sparse
Sparse
Sparse
</figure>
<figureCaption confidence="0.770471">
Figure 1: Result of training linear-chain CRFs: Un-normalized training log-likelihood and training times are
compared. Dashed lines denote the termination of training step.
</figureCaption>
<tableCaption confidence="0.854492">
Table 2: Decoding result; columns are percent accuracy (Acc), and decoding time in milliseconds (Time) measured
</tableCaption>
<bodyText confidence="0.9989535">
per testing example. ‘*’00indicates that the result is significantly different from the Sparse model. N/A indicates
failure due to out-of-memory error.
</bodyText>
<table confidence="0.959760875">
Method PTB CoNLL00 CoNLL03 NetTalk Communicator Encyclopedia
Acc Time Acc Time Acc Time Acc Time Acc Time Acc Time
Dense N/A N/A 96.1 0.89 95.8 0.26 88.4 0.49 91.6 0.94 N/A N/A
Sparse 96.6 1.12 95.9 0.62 95.9 0.21 88.4 0.44 91.9 0.83 93.6 34.75
Method 1 96.8 0.74 95.9 0.55 *94.0 0.24 *88.3 0.34 91.7 0.73 *69.2 15.77
Method 2 96.6 0.92 *95.7 0.52 95.9 0.21 *87.4 0.32 91.9 0.30 93.6 4.99
Method 3 96.5 0.84 *94.2 0.51 95.9 0.24 *78.2 0.29 *86.7 0.30 93.7 6.14
Method 4 96.6 0.85 *92.1 0.51 95.9 0.24 *77.9 0.30 91.9 0.29 93.3 4.88
</table>
<bodyText confidence="0.9997296">
and 368 s for ours) and 7∼12 times faster for decod-
ing (2.881 ms for MALLET, 5.028 ms for CRF++, and
0.418 ms for ours). This result demonstrates that learn-
ing and decoding CRFs for large-scale natural language
problems can be efficiently solved using our method.
</bodyText>
<sectionHeader confidence="0.999597" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999961">
We have demonstrated empirically that our efficient in-
ference method can function successfully, allowing for
a significant speedup of computation. Our method links
two previous algorithms, the SFB and the TP. We have
also showed that a simple and robust variant method
(Method 2) is effective in large-scale problems.9 The
empirical results show a significant improvement in
the training and decoding speeds especially when the
problem has a large state space of output labels. Fu-
ture work will consider applications to other large-scale
problems, and more-general graph topologies.
</bodyText>
<footnote confidence="0.9775925">
9Code used in this work is available at
http://argmax.sourceforge.net/.
</footnote>
<sectionHeader confidence="0.998193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998196">
T. Cohn. 2006. Efficient inference in large conditional ran-
dom fields. In Proc. ECML, pages 606–613.
M. Jeong and G. G. Lee. 2006. Exploiting non-local fea-
tures for spoken language understanding. In Proc. of COL-
ING/ACL, pages 412–419, Sydney, Australia, July.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML, pages 282–289.
C. Lee, Y. Hwang, and M. Jang. 2007. Fine-grained named
entity recognition and relation extraction for question an-
swering. In Proc. SIGIR Poster, pages 799–800.
C. Pal, C. Sutton, and A. McCallum. 2006. Sparse forward-
backward using minimum divergence beams for fast train-
ing of conditional random fields. In Proc. ICASSP.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL/HLT, pages 134–141.
C. Sutton and A. McCallum. 2006. An introduction to condi-
tional random fields for relational learning. In Lise Getoor
and Ben Taskar, editors, Introduction to Statistical Rela-
tional Learning. MIT Press, Cambridge, MA.
</reference>
<page confidence="0.998384">
284
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495378">
<title confidence="0.98986">Efficient Inference of CRFs for Large-Scale Natural Language Data</title>
<author confidence="0.544256">Geunbae</author>
<affiliation confidence="0.988183">University of Science &amp; Technology, Pohang, Korea</affiliation>
<address confidence="0.927466">Research Asia, Beijing, China</address>
<abstract confidence="0.998809642857143">This paper presents an efficient inference algorithm of conditional random fields (CRFs) for large-scale data. Our key idea is to decompose the output label state into an active set and an inactive set in which most unsupported transitions become a constant. Our method unifies two previous methods for efficient inference of CRFs, and also derives a simple but robust special case that performs faster than exact inference when the active sets are sufficiently small. We demonstrate that our method achieves dramatic speedup on six standard natural language processing problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Cohn</author>
</authors>
<title>Efficient inference in large conditional random fields.</title>
<date>2006</date>
<booktitle>In Proc. ECML,</booktitle>
<pages>606--613</pages>
<contexts>
<context position="1573" citStr="Cohn (2006)" startWordPosition="231" endWordPosition="232">them to large-scale problems remains a significant challenge. For simple graphical structures (e.g. linear-chain), an exact inference can be obtained efficiently if the number of output labels is not large. However, for large number of output labels, the inference is often prohibitively expensive. To alleviate this problem, researchers have begun to study the methods of increasing inference speeds of CRFs. Pal et al. (2006) proposed a Sparse ForwardBackward (SFB) algorithm, in which marginal distribution is compressed by approximating the true marginals using Kullback-Leibler (KL) divergence. Cohn (2006) proposed a Tied Potential (TP) algorithm which constrains the labeling considered in each feature function, such that the functions can detect only a relatively small set of labels. Both of these techniques efficiently compute the marginals with a significantly reduced runtime, resulting in faster training and decoding of CRFs. This paper presents an efficient inference algorithm of CRFs which unifies the SFB and TP approaches. We first decompose output labels states into active and inactive sets. Then, the active set is selected by feasible heuristics and the parameters of the inactive set a</context>
<context position="10181" citStr="Cohn, 2006" startWordPosition="1779" endWordPosition="1780">) Qt−1(j). 2This model may perX 3 ´ X βt−1(j) = Ψ1 Ψ2 j,i − ω βt(i) + ω i∈Aj t,i i∈Y CRFs if the hyperparameter is 2 = 0; hence this criterion does not change the parameter. Although this method is simple, it is sufficiently efficient for training and decoding CRFs in real data. Method 3: We define 6(i, j) = Ep(02(i, j)) where Ep(z) is an empirical count of event z in training data. We also assign a real value for the inactive set, i.e., w = c E ]IR, c =� 0, 1. The value c is estimated in the training phase; hence, c is a shared parameter for the inactive set. This method is equivalent to TP (Cohn, 2006). By setting 2 larger, we can achieve faster inference, a tradeoff exists between efficiency and accuracy. Method 4: We define the shared parameter as a function of output label y in the inactive set, i.e., c(y). As in Method 3, c(y) is estimated during the training phase. When the problem expects different aspects of unsupported transitions, this method would be better than using only one parameter c for all labels in inactive set. 4 Experiment We evaluated our method on six large-scale natural language data sets (Table 1): Penn Treebank3 for part-of-speech tagging (PTB), phrase chunking data</context>
</contexts>
<marker>Cohn, 2006</marker>
<rawString>T. Cohn. 2006. Efficient inference in large conditional random fields. In Proc. ECML, pages 606–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jeong</author>
<author>G G Lee</author>
</authors>
<title>Exploiting non-local features for spoken language understanding.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<pages>412--419</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="10955" citStr="Jeong and Lee, 2006" startWordPosition="1899" endWordPosition="1902">unction of output label y in the inactive set, i.e., c(y). As in Method 3, c(y) is estimated during the training phase. When the problem expects different aspects of unsupported transitions, this method would be better than using only one parameter c for all labels in inactive set. 4 Experiment We evaluated our method on six large-scale natural language data sets (Table 1): Penn Treebank3 for part-of-speech tagging (PTB), phrase chunking data4 (CoNLL00), named entity recognition data5 (CoNLL03), grapheme-to-phoneme conversion data6 (NetTalk), spoken language understanding data (Communicator) (Jeong and Lee, 2006), and finegrained named entity recognition data (Encyclopedia) (Lee et al., 2007). The active set is sufficiently small in Communicator and Encyclopedia despite their large numbers of output labels. In all data sets, we selected the current word, f2 context words, bigrams, trigrams, and prefix and suffix features as basic feature templates. A template of part-of-speech tag features was added for CoNLL00, CoNLL03, and Encyclopedia. In particular, all tasks except PTB and NetTalk require assigning a label to a phrase rather than to a word; hence, we used standard “BIO” encoding. We used un-norma</context>
</contexts>
<marker>Jeong, Lee, 2006</marker>
<rawString>M. Jeong and G. G. Lee. 2006. Exploiting non-local features for spoken language understanding. In Proc. of COLING/ACL, pages 412–419, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2950" citStr="Lafferty et al., 2001" startWordPosition="452" endWordPosition="455"> during the author’s internship at Microsoft Research Asia. small group of the labeling states has sufficient statistics. We show that the SFB and the TP are special cases of our method because they derive from our unified algorithm with a different setting of parameters. We also present a simple but robust variant algorithm in which CRFs efficiently learn and predict large-scale natural language data. 2 Linear-chain CRFs Many versions of CRFs have been developed for use in natural language processing, computer vision, and machine learning. For simplicity, we concentrate on linear-chain CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006), but the generic idea described here can be extended to CRFs of any structure. Linear-chain CRFs are conditional probability distributions over label sequences which are conditioned on input sequences (Lafferty et al., 2001). Formally, x = {xt}Tt=1 and y = {yt}Tt=1 are sequences of input and output variables. Respectively, where T is the length of sequence, xt E X and yt E Y where X is the finite set of the input observations and Y is that of the output label state space. Then, a first-order linear-chain CRF is defined as: Ψt(yt, yt−1, x), (1) x) t=1 where Ψt is th</context>
<context position="4273" citStr="Lafferty et al., 2001" startWordPosition="672" endWordPosition="675">on function which ensures the probabilities of all state sequences sum to one. We assume that the potentials factorize according to a set of observation features {01k} and transition features {02k}, as follows: 1 2 Ψt(yt,yt−1,x) =Ψt(yt,x) &apos;Ψt(yt,yt−1), (2) Ψt(yt, x) =eE k λ1k φ1k(yt,X), (3) Ψt(yt,yt−1) =eEk λ�kφ�k(yt,yt−1), (4) where {a1k} and {A2k} are weight parameters which we wish to learn from data. Inference is significantly challenging both in learning and decoding CRFs. Time complexity is O(T|Y|2) for exact inference (i.e., forward-backward and Viterbi algorithm) of linear-chain CRFs (Lafferty et al., 2001). The inference process is often prohibitively expensive T 1 pa(y|x) = Z( � 281 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 281–284, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP when |Y |is large, as is common in large-scale tasks. This problem can be alleviated by introducing approximate inference methods based on reduction of the search spaces to be explored. 3 Efficient Inference Algorithm 3.1 Method The key idea of our proposed efficient inference method is that the output label state Y can be decomposed to an active set A and an inactive set Ac. Intuitively</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lee</author>
<author>Y Hwang</author>
<author>M Jang</author>
</authors>
<title>Fine-grained named entity recognition and relation extraction for question answering.</title>
<date>2007</date>
<booktitle>In Proc. SIGIR Poster,</booktitle>
<pages>799--800</pages>
<contexts>
<context position="11036" citStr="Lee et al., 2007" startWordPosition="1911" endWordPosition="1914">stimated during the training phase. When the problem expects different aspects of unsupported transitions, this method would be better than using only one parameter c for all labels in inactive set. 4 Experiment We evaluated our method on six large-scale natural language data sets (Table 1): Penn Treebank3 for part-of-speech tagging (PTB), phrase chunking data4 (CoNLL00), named entity recognition data5 (CoNLL03), grapheme-to-phoneme conversion data6 (NetTalk), spoken language understanding data (Communicator) (Jeong and Lee, 2006), and finegrained named entity recognition data (Encyclopedia) (Lee et al., 2007). The active set is sufficiently small in Communicator and Encyclopedia despite their large numbers of output labels. In all data sets, we selected the current word, f2 context words, bigrams, trigrams, and prefix and suffix features as basic feature templates. A template of part-of-speech tag features was added for CoNLL00, CoNLL03, and Encyclopedia. In particular, all tasks except PTB and NetTalk require assigning a label to a phrase rather than to a word; hence, we used standard “BIO” encoding. We used un-normalized loglikelihood, accuracy and training/decoding times as our evaluation measu</context>
</contexts>
<marker>Lee, Hwang, Jang, 2007</marker>
<rawString>C. Lee, Y. Hwang, and M. Jang. 2007. Fine-grained named entity recognition and relation extraction for question answering. In Proc. SIGIR Poster, pages 799–800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pal</author>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>Sparse forwardbackward using minimum divergence beams for fast training of conditional random fields.</title>
<date>2006</date>
<booktitle>In Proc. ICASSP.</booktitle>
<contexts>
<context position="1389" citStr="Pal et al. (2006)" startWordPosition="204" endWordPosition="207">achieves dramatic speedup on six standard natural language processing problems. 1 Introduction Conditional random fields (CRFs) are widely used in natural language processing, but extending them to large-scale problems remains a significant challenge. For simple graphical structures (e.g. linear-chain), an exact inference can be obtained efficiently if the number of output labels is not large. However, for large number of output labels, the inference is often prohibitively expensive. To alleviate this problem, researchers have begun to study the methods of increasing inference speeds of CRFs. Pal et al. (2006) proposed a Sparse ForwardBackward (SFB) algorithm, in which marginal distribution is compressed by approximating the true marginals using Kullback-Leibler (KL) divergence. Cohn (2006) proposed a Tied Potential (TP) algorithm which constrains the labeling considered in each feature function, such that the functions can detect only a relatively small set of labels. Both of these techniques efficiently compute the marginals with a significantly reduced runtime, resulting in faster training and decoding of CRFs. This paper presents an efficient inference algorithm of CRFs which unifies the SFB an</context>
<context position="8513" citStr="Pal et al., 2006" startWordPosition="1468" endWordPosition="1471">L) and = 0 where L is a beam set, L = ... , and the subpartition function Z(L) is approximated by Z(L) In this method, all sub-marginals in the inactive set are totally excluded from calculation of the current marginal. and in the inactive sets are set to 0 by default. Therefore, at each time step t the algorithm prunes all states i in which &lt; It also generates a subset L of output labels that will be exploited in next time step t + 1.1 This method has been derived theoretically from the process of selecting a compressed marginal distribution within a fixed KL divergence of the true marginal (Pal et al., 2006). This method most closely resembles SFB algorithm; hence we refer an alternative of SFB. Method 2: We define j) and = 1. In practice, unsupported transition features are not parameterized2; this means that = 0 and = 1 if j γt(i) ω maxj∈Yγt−1(j) − O(T|Aavg||Y|). δ ω δ ω δ ω δ(i, ω {l1,l2, lm} ≈αt−1(j). α β αt(i) 2. δ(i, =|Ψ2j,i−1| ω λk Ψ2j,i ∈Aci. Thus, this method estimates nearly-exact Ψ1t,iβt(i), (9) Aci O(T|Aavg||Y|) |Aavg| PT states in the active set, i.e., 1 t=1 |Ai|. The worst T α andβ O(T|Y|2). γ ows: (10) practice, dynamically selecting L increases the number of computations, and this</context>
</contexts>
<marker>Pal, Sutton, McCallum, 2006</marker>
<rawString>C. Pal, C. Sutton, and A. McCallum. 2006. Sparse forwardbackward using minimum divergence beams for fast training of conditional random fields. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL/HLT,</booktitle>
<pages>134--141</pages>
<contexts>
<context position="12427" citStr="Sha and Pereira, 2003" startWordPosition="2129" endWordPosition="2132"> the previous state-of-the-art features we expect the achievement of better accuracy. All our models were trained until parameter estimation converged with a Gaussian prior variance of 4. During training, a pseudo-likelihood parameter estimation (Sutton and McCallum, 2006) was used as an initial weight (estimated in 30 iterations). We used complete and dense input/output joint features for dense model (Dense), and only supported features that are used at least once in the training examples for sparse form better, the sparse model performs well in practice without significant loss of accuracy (Sha and Pereira, 2003). 3Penn Treebank3: Catalog No. LDC99T42 4http://www.cnts.ua.ac.be/conll2000/chunking/ 5http://www.cnts.ua.ac.be/conll2003/ner/ 6http://archive.ics.uci.edu/ml/ Table 1: Data sets: number of sentences in the training (#Train) and the test data sets (#Test), and number of output labels (#Label). |A��1 ��� |denotes the average number of active set when w = 1, i.e., the supported transitions that are used at least once in the training set. Set #Train #Test #Label |A��� ��� | PTB 38,219 5462 45 30.01 CoNLL00 8,936 2,012 22 6.59 CoNLL03 14,987 3,684 8 4.13 NetTalk 18,008 2,000 51 22.18 Communicator 1</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of NAACL/HLT, pages 134–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>An introduction to conditional random fields for relational learning.</title>
<date>2006</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2978" citStr="Sutton and McCallum, 2006" startWordPosition="456" endWordPosition="459">ternship at Microsoft Research Asia. small group of the labeling states has sufficient statistics. We show that the SFB and the TP are special cases of our method because they derive from our unified algorithm with a different setting of parameters. We also present a simple but robust variant algorithm in which CRFs efficiently learn and predict large-scale natural language data. 2 Linear-chain CRFs Many versions of CRFs have been developed for use in natural language processing, computer vision, and machine learning. For simplicity, we concentrate on linear-chain CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006), but the generic idea described here can be extended to CRFs of any structure. Linear-chain CRFs are conditional probability distributions over label sequences which are conditioned on input sequences (Lafferty et al., 2001). Formally, x = {xt}Tt=1 and y = {yt}Tt=1 are sequences of input and output variables. Respectively, where T is the length of sequence, xt E X and yt E Y where X is the finite set of the input observations and Y is that of the output label state space. Then, a first-order linear-chain CRF is defined as: Ψt(yt, yt−1, x), (1) x) t=1 where Ψt is the local potential that denot</context>
<context position="6009" citStr="Sutton and McCallum, 2006" startWordPosition="983" endWordPosition="987">ion for our algorithm. Let Ai be the active set and Aci be the inactive set of output label i where Yi = Ai ∪ Aci. We define Ai as: Ai = {j|δ(yt = i, yt−1 = j) &gt; 2} (5) where δ is a criterion function of transitions (yt−1 → yt) and 2 is a hyperparameter. For clarity, we define the local factors as: Ψ1t,i = Ψ1t(yt = i, X), (6) Ψ2 j,i g Ψ2 t (yt−1 = j, yt = i). (7) Note that we can ignore the subscript t at Ψ2t (yt−1 = j, yt = i) by defining an HMM-like model, that is, transition matrix Ψ2 j,i is independent of t. As exact inference, we use the forward-backward procedure to calculate marginals (Sutton and McCallum, 2006). We formally describe here an efficient calculation of α and β recursions for the forwardbackward procedure. The forward value αt(i) is the sum of the unnormalized scores for all partial paths that start at t = 0 and converge at yt = i at time t. The backward value βt(i) similarly defines the sum of unnormalized scores for all partial paths that start at time t + 1 with state yt+1 = j and continue until the end of the sequences, t = T + 1. Then, we decompose the equations of exact α and β recursions as follows: ⎛ ⎞ ⎝ X3 ´ αt(i) = Ψ1 Ψ2 j,i − ω αt−1(j) + ω ⎠ , (8) t,i j∈A; where is a shared tr</context>
<context position="12078" citStr="Sutton and McCallum, 2006" startWordPosition="2070" endWordPosition="2073">ing a label to a phrase rather than to a word; hence, we used standard “BIO” encoding. We used un-normalized loglikelihood, accuracy and training/decoding times as our evaluation measures. We did not use cross validation and development set for tuning the parameter because our goal is to evaluate the efficiency of inference algorithms. Moreover, using the previous state-of-the-art features we expect the achievement of better accuracy. All our models were trained until parameter estimation converged with a Gaussian prior variance of 4. During training, a pseudo-likelihood parameter estimation (Sutton and McCallum, 2006) was used as an initial weight (estimated in 30 iterations). We used complete and dense input/output joint features for dense model (Dense), and only supported features that are used at least once in the training examples for sparse form better, the sparse model performs well in practice without significant loss of accuracy (Sha and Pereira, 2003). 3Penn Treebank3: Catalog No. LDC99T42 4http://www.cnts.ua.ac.be/conll2000/chunking/ 5http://www.cnts.ua.ac.be/conll2003/ner/ 6http://archive.ics.uci.edu/ml/ Table 1: Data sets: number of sentences in the training (#Train) and the test data sets (#Te</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>C. Sutton and A. McCallum. 2006. An introduction to conditional random fields for relational learning. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>