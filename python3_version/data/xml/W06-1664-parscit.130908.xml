<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995149">
Graph-based Word Clustering using a Web Search Engine
</title>
<author confidence="0.998107">
Yutaka Matsuo
</author>
<affiliation confidence="0.993725">
National Institute of Advanced
Industrial Science and Technology
</affiliation>
<address confidence="0.921626">
1-18-13 Sotokanda, Tokyo 101-0021
</address>
<email confidence="0.9927">
y.matsuo@aist.go.jp
</email>
<author confidence="0.950565">
Kˆoki Uchiyama
</author>
<affiliation confidence="0.903838">
Hottolink Inc.
</affiliation>
<address confidence="0.9391455">
2-11-17 Nishi-gotanda
Tokyo 141-0031
</address>
<email confidence="0.998081">
uchi@hottolink.co.jp
</email>
<sectionHeader confidence="0.99561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99952605882353">
Word clustering is important for automatic
thesaurus construction, text classification,
and word sense disambiguation. Recently,
several studies have reported using the
web as a corpus. This paper proposes
an unsupervised algorithm for word clus-
tering based on a word similarity mea-
sure by web counts. Each pair of words
is queried to a search engine, which pro-
duces a co-occurrence matrix. By calcu-
lating the similarity of words, a word co-
occurrence graph is obtained. A new kind
of graph clustering algorithm called New-
man clustering is applied for efficiently
identifying word clusters. Evaluations are
made on two sets of word groups derived
from a web directory and WordNet.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997609352941177">
The web is a good source of linguistic informa-
tion for several natural language techniques such
as question answering, language modeling, and
multilingual lexicon acquisition. Numerous stud-
ies have examined the use of the web as a corpus
(Kilgarriff, 2003).
Web-based models perform especially well
against the sparse data problem: Statistical tech-
niques perform poorly when the words are rarely
used. For example, F. Keller et al. (2002) use the
web to obtain frequencies for unseen bigrams in
a given corpus. They count for adjective-noun,
noun-noun, and verb-object bigrams by querying
a search engine, and demonstrate that web fre-
quencies (web counts) correlate with frequencies
from a carefully edited corpus such as the British
National Corpus (BNC). Aside from counting bi-
</bodyText>
<note confidence="0.53633625">
Takeshi Sakaki
University of Tokyo
7-3-1 Hongo
Tokyo 113-8656
</note>
<author confidence="0.875059">
Mitsuru Ishizuka
</author>
<affiliation confidence="0.977157">
University of Tokyo
</affiliation>
<address confidence="0.8535515">
7-3-1 Hongo
Tokyo 113-8656
</address>
<email confidence="0.984068">
ishizuka@i.u-tokyo.ac.jp
</email>
<bodyText confidence="0.999785552631579">
grams, various tasks are attainable using web-
based models: spelling correction, adjective order-
ing, compound noun bracketing, countability de-
tection, and so on (Lapata and Keller, 2004). For
some tasks, simple unsupervised models perform
better when n-gram frequencies are obtained from
the web rather than from a standard large corpus;
the web yields better counts than the BNC.
The web is an excellent source of information
on new words. Therefore, automatic thesaurus
construction (Curran, 2002) offers great potential
for various useful NLP applications. Several stud-
ies have addressed the extraction of hypernyms
and hyponyms from the web (Miura et al., 2004;
Cimiano et al., 2004). P. Turney (2001) presents a
method to recognize synonyms by obtaining word
counts and calculating pointwise mutual informa-
tion (PMI). For further development of automatic
thesaurus construction, word clustering is benefi-
cial, e.g. for obtaining synsets. It also contributes
to word sense disambiguation (Li and Abe, 1998)
and text classification (Dhillon et al., 2002) be-
cause the dimensionality is reduced efficiently.
This paper presents an unsupervised algorithm
for word clustering based on a word similarity
measure by web counts. Given a set of words, the
algorithm clusters the words into groups so that
the similar words are in the same cluster. Each pair
of words is queried to a search engine, which re-
sults in a co-occurrence matrix. By calculating the
similarity of words, a word co-occurrence graph
is created. Then, a new kind of graph clustering
algorithm, called Newman clustering, is applied.
Newman clustering emphasizes betweenness of an
edge and identifies densely connected subgraphs.
To the best of our knowledge, this is the first
attempt to obtain word groups using web counts.
Our contributions are summarized as follows:
</bodyText>
<page confidence="0.959125">
542
</page>
<bodyText confidence="0.881927">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 542–550,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<listItem confidence="0.849171444444444">
• A new algorithm for word clustering is de-
scribed. It has few parameters and thus is
easy to implement as a baseline method.
• We evaluate the algorithm on two sets of
word groups derived from a web directory
and WordNet. The chi-square measure and
Newman clustering are both used in our al-
gorithm, they are revealed to outperform PMI
and hierarchical clustering.
</listItem>
<bodyText confidence="0.999345666666667">
We target Japanese words in this paper. The re-
mainder of this paper is organized as follows: We
overview the related studies in the next section.
Our proposed algorithm is described in Section 3.
Sections 4 and 5 explain evaluations and advance
discussion. Finally, we conclude the paper.
</bodyText>
<sectionHeader confidence="0.999371" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999929083333334">
A number of studies have explained the use of
the web for NLP tasks e.g., creating multilingual
translation lexicons (Cheng et al., 2004), text clas-
sification (Huang et al., 2004), and word sense dis-
ambiguation (Turney, 2004). M. Baroni and M.
Ueyama summarize three approaches to use the
web as a corpus (Baroni and Ueyama, 2005): us-
ing web counts as frequency estimates, building
corpora through search engine queries, and crawl-
ing the web for linguistic purposes. Commercial
search engines are optimized for ordinary users.
Therefore, it is desirable to crawl the web and to
develop specific search engines for NLP applica-
tions (Cafarella and Etzioni, 2005). However, con-
sidering that great efforts are taken in commercial
search engines to maintain quality of crawling and
indexing, especially against spammers, it is still
important to pursue the possibility of using the
current search engines for NLP applications.
P. Turney (Turney, 2001) presents an unsu-
pervised learning algorithm for recognizing syn-
onyms by querying a web search engine. The
task of recognizing synonyms is, given a target
word and a set of alternative words, to choose the
word that is most similar in meaning to the tar-
get word. The algorithm uses pointwise mutual
information (PMI-IR) to measure the similarity of
pairs of words. It is evaluated using 80 synonym
test questions from the Test of English as a Foreign
Language (TOEFL) and 50 from the English as a
Second Language test (ESL). The algorithm ob-
tains a score of 74%, contrasted to that of 64% by
Latent Semantic Analysis (LSA). Terra and Clarke
(Terra and Clarke, 2003) provide a comparative in-
vestigation of co-occurrence frequency estimation
on the performance of synonym tests. They report
that PMI (with a certain window size) performs
best on average. Also, PMI-IR is useful for cal-
culating semantic orientation and rating reviews
(Turney, 2002).
As described, PMI is one of many measures to
calculate the strength of word similarity or word
association (Manning and Sch¨utze, 2002). An
important assumption is that similarity between
words is a consequence of word co-occurrence, or
that the proximity of words in text is indicative of
relationship between them, such as synonymy or
antonymy. A commonly used technique to obtain
word groups is distributional clustering (Baker and
McCallum, 1998). Distributional clustering of
words was first proposed by Pereira Tishby &amp; Lee
in (Pereira et al., 1993): They cluster nouns ac-
cording to their conditional verb distributions.
Graphic representations for word similarity
have also been advanced by several researchers.
Kageura et al. (2000) propose automatic thesaurus
generation based on a graphic representation. By
applying a minimum edge cut, the corresponding
English terms and Japanese terms are identified
as a cluster. Widdows and Dorow (2002) use a
graph model for unsupervised lexical acquisition.
A graph is produced by linking pairs of words
which participate in particular syntactic relation-
ships. An incremental cluster-building algorithm
achieves 82% accuracy at a lexical acquisition
task, evaluated against WordNet classes. Another
study builds a co-occurrence graph of terms and
decomposes it to identify relevant terms by dupli-
cating nodes and edges (Tanaka-Ishii and Iwasaki,
1996). It focuses on transitivity: if transitivity
does not hold between three nodes (e.g., if edge
a-b and b-c exist but edge a-c does not), the nodes
should be in separate clusters.
A network of words (or named entities) on the
web is investigated also in the context of the Se-
mantic Web (Cimiano et al., 2004; Bekkerman and
McCallum, 2005). Especially, a social network of
persons is mined from the web using a search en-
gine (Kautz et al., 1997; Mika, 2005; Matsuo et
al., 2006). In these studies, the Jaccard coefficient
is often used to measure the co-occurrence of enti-
ties. We compare Jaccard coefficients in our eval-
uations.
In the research field on complex networks,
</bodyText>
<page confidence="0.999277">
543
</page>
<tableCaption confidence="0.999809">
Table 1: Web counts for each word.
</tableCaption>
<figure confidence="0.336552">
printer print InterLaser ink TV Aquos Sharp
17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000
</figure>
<tableCaption confidence="0.985697">
Table 2: Co-occurrence matrix by web counts.
</tableCaption>
<bodyText confidence="0.903504588235294">
printer print InterLaser ink TV Aquos Sharp
printer — 4780000 179 4720000 4530000 201000 990000
print 4780000 — 183 4800000 8390000 86400 1390000
InterLaser 179 183 116 65 0 0
ink 4720000 4800000 116 — 10600000 144000 656000
TV 4530000 8390000 65 10600000 — 1660000 42300000
Aquos 201000 86400 0 144000 1660000 — 1790000
Sharp 990000 1390000 0 656000 42300000 1790000 —
structures of various networks are investigated in
detail. For example, Motter (2002) targeted a
conceptual network from a thesaurus and demon-
strated its small-world structure. Recently, nu-
merous works have identified communities (or
densely-connected subgraphs) from large net-
works (Newman, 2004; Girvan and Newman,
2002; Palla et al., 2005) as explained in the next
section.
</bodyText>
<sectionHeader confidence="0.922843" genericHeader="method">
3 Word Clustering using Web Counts
</sectionHeader>
<subsectionHeader confidence="0.999023">
3.1 Co-occurrence by a Search Engine
</subsectionHeader>
<bodyText confidence="0.99992675">
A typical word clustering task is described as fol-
lows: given a set of words (nouns), cluster words
into groups so that the similar words are in the
same cluster 1. Let us take an example. As-
sume a set of words is given: プリ,/タ (printer),
印刷 (print), 4,/ターレーザー (InterLaser), 4
,/ク (ink), TV (TV), Aquos (Aquos), and Sharp
(Sharp). Apparently, the first four words are re-
lated to a printer, and the last three words are re-
lated to a TV 2. In this case, we would like to have
two word groups: the first four and the last three.
We query a search engine3 to obtain word
counts. Table 1 shows web counts for each word.
Table 2 shows the web counts for pairs of words.
For example, we submit a query printer AND In-
terLaser to a search engine, and are directed to 179
documents. Thereby, nC2 queries are necessary to
obtain the matrix if we have n words. We call Ta-
ble 2 a co-occurrence matrix.
We can calculate the pointwise mutual informa-
</bodyText>
<footnote confidence="0.9976552">
1In this paper, we limit our scope to clustering nouns. We
discuss the extension in Section 4.
2InterLaser is a laser printer made by Epson Corp. Aquos
is a liquid crystal TV made by Sharp Corp.
3Google (www.google.co.jp) is used in our study.
</footnote>
<bodyText confidence="0.784954">
tion between word w1 and w2 as
</bodyText>
<equation confidence="0.9967645">
p(w1, w2)
PMI(w1, w2) = log2 p(w1)p(w2).
</equation>
<bodyText confidence="0.98654532">
Probability p(w1) is estimated by f,,,1/N, where
f,,,1 represents the web count of w1 and N repre-
sents the number of documents on the web. Prob-
ability of co-occurrence p(w1, w2) is estimated by
f,,,1,,,,2/N where f,,,1,,,,2 represents the web count
of w1 AND w2.
The PMI values are shown in Table 3. We set
N = 1010 according to the number of indexed
pages on Google. Some values are inconsistent
with our intuition: Aquos is inferred to have high
PMI to TV and Sharp, but also to printer. None
of the words has high PMI with TV. These are be-
cause the range of the word count is broad. Gen-
erally, mutual information tends to provide a large
value if either word is much rarer than the other.
Various statistical measures based on co-
occurrence analysis have been proposed for es-
timating term association: the DICE coefficient,
Jaccard coefficient, chi-square test, and the log-
likelihood ratio (Manning and Sch¨utze, 2002). In
our algorithm, we use the chi-square (χ2) value in-
stead of PMI. The chi-square value is calculated as
follows: We denote the number of pages contain-
ing both w1 and w2 as a. We also denote b, c, d as
follows4.
</bodyText>
<figure confidence="0.587420333333333">
w2 ¬w2
a b
c d
</figure>
<bodyText confidence="0.877306333333333">
Thereby, the expected frequency of (w1, w2) is
(a + c)(a + b)/N. Eventually, chi-square is calcu-
lated as follows (Manning and Sch¨utze, 2002).
</bodyText>
<footnote confidence="0.554663">
4Note that N = a + b + c + d.
</footnote>
<page confidence="0.595159666666667">
w1
¬w1
544
</page>
<tableCaption confidence="0.999068">
Table 3: A matrix of pointwise mutual information.
</tableCaption>
<table confidence="0.99957725">
printer print InterLaser ink TV Aquos Sharp
printer — 4.771 8.936 7.199 0.598 5.616 1.647
print 4.771 — 6.369 4.624 -1.111 1.799 -0.463
InterLaser 8.936 6.369 8.157 0.781 −∞* −∞*
ink 7.199 4.624 8.157 — 1.672 4.983 0.900
TV 0.598 -1.111 0.781 1.672 — 1.969 0.370
Aquos 5.616 1.799 −∞*. 4.983 1.969 — 5.319
Sharp 1.647 -0.463 −∞* 0.900 0.370 5.319 —
</table>
<tableCaption confidence="0.788792">
* represents that the PMI is not available because the co-occurrence web count is zero, in which case we set −∞.
Table 4: A matrix of chi-square values.
</tableCaption>
<table confidence="0.954823888888889">
printer print InterLaser ink TV Aquos Sharp
printer 6880482.6 399.2 5689710.7 0.0* 0.0* 0.0*
print 6880482.6 277.8 3321184.6 176855.5 0.0* 0.0*
InterLaser 399.2 277.8 44.8 0.0* 0.0 0.0
ink 5689710.7 3321184.6 44.8 — 1419485.5 0.0* 0.0*
TV 0.0* 176855.5 0.0* 1419485.5 26803.2 70790877.6
Aquos 0.0* 0.0* 0.0 0.0* 26803.2 — 729357.7
Sharp 0.0* 0.0* 0.0 0.0* 70790877.6 729357.7 —
* represents that the observed co-occurrence frequency is below the expected value, in which case we set 0.0.
</table>
<figureCaption confidence="0.998981">
Figure 1: Examples of Newman clustering.
</figureCaption>
<equation confidence="0.986160333333333">
X2(w1, w2)
= N × (a × d − b × c)2
(a + b) × (a + c) × (b + d) × (c + d)
</equation>
<bodyText confidence="0.87062725">
However, N is a huge number on the web and
sometimes it is difficult to know exactly. There-
fore we regard the co-occurrence matrix as a con-
tingency table:
</bodyText>
<equation confidence="0.89016475">
Eb0 = fw1,w , Ec0 = fw2,w;
w∈W;w6=w2 w∈W;w6=w1
Ed0 = Efw,w&apos; , N0 = fw,w&apos;,
w,w&apos;∈W;w and w&apos;6=w1 nor w2 w,w&apos;∈W
</equation>
<bodyText confidence="0.999844571428571">
where W represents a given set of words. Then
chi-square (within the word list W) is defined as
We should note that X2depends on a word
set W. It calculates the relative strength of co-
occurrences. Table 4 shows the X2values. Aquos
has high values only with TV and Sharp as ex-
pected.
</bodyText>
<subsectionHeader confidence="0.999853">
3.2 Clustering on Co-occurrence Graph
</subsectionHeader>
<bodyText confidence="0.999939153846154">
Recently, a series of effective graph clustering
methods has been advanced. Pioneering work that
specifically emphasizes edge betweenness was
done by Girvan and Newman (2002): we call the
method as GN algorithm. Betweenness of an edge
is the number of shortest paths between pairs of
nodes that run along it. Figure 1 (i) shows that
two “communities” (in Girvan’s term), i.e. {a,b,c}
and {d,e,f,g}, which are connected by edge c-d.
Edge c-d has high betweenness because numerous
shortest paths (e.g., from a to d, from b to e, ...)
traverse the edge. The graph is likely to be sepa-
rated into densely connected subgraphs if we cut
the high betweenness edge.
The GN algorithm is different from the mini-
mum edge cut. For (i), the results are identical: By
cutting edge c-d, which is a minimum edge cut, we
can obtain two clusters. However in case of (ii),
there are two candidates for the minimum edge
cut, whereas the highest betweenness edge is still
only edge c-d. Girvan et al. (2002) shows that this
clustering works well to various networks from
biological to social networks. Numerous studies
have been inspired by that work. One prominent
effort is a faster variant of GN algorithm (New-
man, 2004), which we call Newman clustering in
</bodyText>
<equation confidence="0.998227333333333">
N0 × (a × d0 − b0 × c0)2
X2W(w1, w2) =
(a + b0) × (a + c0) × (b0 + d0) × (c0 + d0).
</equation>
<page confidence="0.99564">
545
</page>
<figureCaption confidence="0.998265333333333">
Figure 3: A word graph for 88 Japanese words.
Figure 2: An illustration of graph-based word
clustering.
</figureCaption>
<bodyText confidence="0.988015181818182">
this paper.
In Newman clustering, instead of explicitly cal-
culating high-betweenness edges (which is com-
putationally demanding), an objective function is
defined as follows:
We assume that we have separate clusters, and that
eij is the fraction5 of edges in the network that
connect nodes in cluster i to those in cluster j.
The term eii denotes the fraction of edges within
the clusters. The term ∑j eij represents the ex-
pected fraction of edges within the cluster. If a par-
</bodyText>
<footnote confidence="0.897657">
5We can calculate eij using the number of edges between
cluster i and j divided by the number of all edges.
</footnote>
<bodyText confidence="0.999574172413793">
ticular division gives no more within-community
edges than would be expected by random chance,
then we would obtain Q = 0. In practice, values
greater than about 0.3 appear to indicate signifi-
cant group structure (Newman, 2004).
Newman clustering is agglomerative (although
we can intuitively understand that a graph with-
out high betweenness edges is ultimately ob-
tained). We repeatedly join clusters together in
pairs, choosing at each step the joint that provides
the greatest increase in Q. Currently, Newman
clustering is one of the most efficient methods for
graph-based clustering.
The illustration of our algorithm is shown in
Fig. 2. First, we obtain web counts among a given
set of words using a search engine. Then PMI or
the chi-square values are calculated. If the value is
above a certain threshold6, we invent an edge be-
tween the two nodes. Then, we apply graph clus-
tering and finally identify groups of words. This il-
lustration shows that the chi-square measure yields
the correct clusters.
The algorithm is described in Fig. 4. The pa-
rameters are few: a threshold dthre for a graph and,
optionally, the number of clusters nc. This enables
easy implementation of the algorithm. Figure 3
is a small network of 88 Japanese words obtained
through 3828 search queries. We can see that some
parts in the graph are densely connected.
</bodyText>
<sectionHeader confidence="0.998241" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.99794">
This section addresses evaluation. Two sets of
word groups are used for the evaluation: one is
derived from documents on a web directory; an-
other is from WordNet. We first evaluate the co-
</bodyText>
<footnote confidence="0.913435">
6In this example, 4.0 for PMI and 200 for x2.
</footnote>
<equation confidence="0.404171">
12
Q =∑ I eii — (∑eijJ) (1)
i j
</equation>
<page confidence="0.618474">
546
</page>
<figure confidence="0.973823">
� �
1. For each category, crawl 1000 documents ran-
domly°
</figure>
<listItem confidence="0.998610294117647">
2. Obtain frequencies Put a query for each pair of
words to a search engine, and obtain a co-
occurrence matrix. Then calculate the chi-square
matrix (alternatively a PMI matrix, or a Jaccard
matrix.)
3. Make a graph Set a node for each word, and an
edge to a pair of nodes whose x2 value is above a
threshold. The threshold is determined so that the
network density (the number of edges divided by
nC2) is dthre.
4. Apply Newman clustering Initially set each node
as a cluster. Then merge two clusters repeatedly
so that Q is maximized. Terminate if Q does
not increase anymore, or when a given number
of clusters nc is obtained. (Alternatively, apply
average-link hierarchical clustering.)
5. Output Output groups of words.
</listItem>
<figure confidence="0.419025">
� �
</figure>
<figureCaption confidence="0.99086">
Figure 4: Our algorithm for word clustering.
</figureCaption>
<bodyText confidence="0.9713065">
occurrence measures, then we evaluate the cluster-
ing methods.
</bodyText>
<subsectionHeader confidence="0.998695">
4.1 Word Groups from an Open Directory
</subsectionHeader>
<bodyText confidence="0.999926037037037">
We collected documents from the Japanese Open
Directory (dmoz.org/World/Japanese). The
dmoz japanese category contains about 130,000
documents and more than 10,000 classes. We
chose 9 categories out of the top 12 categories:
art, sports, computer, game, society, family, sci-
ence, and health. We crawled 1000 documents for
each category, i.e., 9000 documents in all.
For each category, a word group is obtained
through the procedure in Fig. 5. We consider
that the specific words to a category are relevant
to some extent, and that they can therefore be re-
garded as a word group. Examples are shown in
Table 5. In all, 90 word sets are obtained and
merged. We call the word set DMOZ-J data.
Our task is, given 90 words, to cluster the words
into the correct nine groups. Here we investigate
whether the correct nine words are selected for
each word using the co-occurrence measure. We
compare pointwise mutual information (PMI), the
Jaccard coefficient (Jaccard), and chi-square (χ2).
We chose these methods for comparison because
PMI performs best in (Terra and Clarke, 2003).
The Jaccard coefficient is often used in social net-
work mining from the web. Table 7 shows the pre-
cision of each method. Experiments are repeated
five times. We keep each method that outputs the
</bodyText>
<listItem confidence="0.98195625">
2. Apply the Japanese morphological analysis sys-
tem ChaSen (Matsumoto et al., 2000) to the doc-
uments. Calculate the score of each word w in
category c similarly to TF-IDF:
</listItem>
<equation confidence="0.832462">
score(w, c) = fc(w) × log(Nall/fall(w))
</equation>
<bodyText confidence="0.9564305">
where fc denotes the document frequency of
word w in category c, Nall denotes the number of
all documents, and fall(w) denotes the frequency
of word w in all documents.
3. For each category, the top 10 words are selected
as the word group.
°We first get all urls, sort them, and select a sample
randomly.
</bodyText>
<equation confidence="0.306472">
� �
</equation>
<figureCaption confidence="0.961459">
Figure 5: Procedure for obtaining word groups for
a category.
</figureCaption>
<tableCaption confidence="0.893508">
Table 7: Precision for DMOZ-J set.
</tableCaption>
<table confidence="0.9963794">
PMI Jaccard χ2
Mean 0.415 0.402 0.537
Min 0.396 0.376 0.493
Max 0.447 0.424 0.569
SD 0.020 0.020 0.032
</table>
<bodyText confidence="0.99012525">
highest nine words for each word, groups of ten
words. Therefore, recall is the same as the preci-
sion. From the table, the chi-square performs best.
PMI is slightly better than the Jaccard coefficient.
</bodyText>
<subsectionHeader confidence="0.999143">
4.2 Word Groups from WordNet
</subsectionHeader>
<bodyText confidence="0.999939428571428">
Next, we make a comparison using WordNet 7. By
extracting 10 words that have the same hypernym
(i.e. coordinates), we produce a word group. Ex-
amples are shown in Table 6. Nine word groups
are merged into one, as with DMOZ-J. The exper-
iments are repeated 10 times. Table 8 shows the
result. Again, the chi-square performs best among
the methods that were compared.
Detailed analyses of the results revealed that
word groups such as bacteria and diseases are clus-
tered correctly. However, word groups such as
computers (in which homepage, server and client
are included) are not well clustered: these words
tend to be polysemic, which causes difficulty.
</bodyText>
<subsectionHeader confidence="0.999824">
4.3 Evaluation of Clustering
</subsectionHeader>
<bodyText confidence="0.9990975">
We compare two clustering methods: Newman
clustering and average-link agglomerative cluster-
</bodyText>
<footnote confidence="0.47904775">
7We use a partly-translated version of WordNet.
� �
1. Input A set of words is given. The number of words
is denoted as n.
</footnote>
<page confidence="0.98662">
547
</page>
<tableCaption confidence="0.999168">
Table 5: Examples of word groups from DMOZ-J.
</tableCaption>
<table confidence="0.787405285714286">
category specific words to a category as a word group
ア 1 (art) WAN (gallery), fIPr&apos;111 (artwork), 010 (theater), 1-v3 (saxophone), �A&apos;fi (verse), 74T (live con-
cert), A�3 (guitar), W (performance), -&amp;quot;�1x_ (ballet), M% (personal exhibition)
AI (raising), tI- (poult), -AA3 (hamster), )A,p� ,Li � (travel diary), A ASW (national park),
?axe (brewing), 1X* (boat race), 1X4 (competition), 01 h JM (fishingpond)
NFA (health) 4,TA (illness), ,TA-&apos; (patient), M* (myositis), �# (surgery), A* (dialysis), A,5F&apos;rs4F (steroid), �
-A (test), A (medical ward), LIMPI (collagen disease), �AE (clinic)
</table>
<tableCaption confidence="0.977236">
Table 6: Examples of word groups from WordNet.
</tableCaption>
<table confidence="0.671165833333333">
hypernym hyponyms as a word group
�Z,,F (gem) ア,,�✓A1 (amethyst), ア3ア r9✓ (aquamarine), Y ✓F (diamond), _ &apos;7ILF (emer-
ald), A ✓A1 ✓ (moonstone), -Z9Fv1 (peridot), ILIf (ruby), 1-7 ア (sapphire),
1 /&apos;� X (topaz), 1 IL r9✓ (tourmaline)
*ra,9 (academic field) nA#* (natural science), * (mathematics), * (agronomics), * (architectonics), ti3w
* (geology), 6�* (psychology), &apos;R =* (computer science), %0#* (cognitive science), $�
�* (sociology), Ap* (linguistics)
0,1�t (drink) t-#L (milk), アIL=I IL (alcohol), MrJA0* (cooling beverage), I9� * (carbonated beverage),
1-4Y (soda), =I=Iア (cocoa), (fruitjuice), =I � (coffee), �3* (tea), �
-*7ILr7A- 3 (mineral water)
1x39_ ✓a✓
(recreation)
</table>
<tableCaption confidence="0.995806">
Table 8: Precision of WordNet set.
</tableCaption>
<table confidence="0.9999158">
PMI Jaccard x2
Mean 0.549 0.484 0.584
Min 0.473 0.415 0.498
Max 0.593 0.503 0.656
SD 0.037 0.027 0.048
</table>
<tableCaption confidence="0.9365615">
Table 9: Precision, recall and the F-measure for
each clustering.
</tableCaption>
<table confidence="0.953401428571429">
PMI Jaccard χ2
Average precision 0.633 0.603 0.486
-link recall 0.102 0.101 0.100
F-measure 0.179 0.173 0.164
Newman precision 0.751 0.739 0.546
recall 0.103 0.103 0.431
F-measure 0.182 0.181 0.480
</table>
<bodyText confidence="0.998447083333333">
ing, which is often used in word clustering.
A word co-occurrence graph is created using
PMI, Jaccard, and chi-square measures. The
threshold is determined so that the network den-
sity dthre is 0.3. Then, we apply clustering to ob-
tain nine clusters; nc = 9. Finally, we compare
the resultant clusters with the correct categories.
Clustering results for DMOZ-J sets are shown
in Table 9. Newman clustering produces higher
precision and recall. Especially, the combination
of chi-square and Newman is the best in our ex-
periments.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999333236842105">
In this paper, the scope of co-occurrence is
document-wide. One reason is that major com-
mercial search engines do not support a type of
query w1 NEAR w2. Another reason is in (Terra
and Clarke, 2003) document-wide co-occurrences
perform comparable to other Windows-based co-
occurrences.
Many types of co-occurrence exist other than
noun-noun. We limit our scope to noun-noun
co-occurrences in this paper. Other types of co-
occurrence such as verb-noun can be investigated
in future studies. Also, co-occurrence for the
second-order similarity can be sought. Because
web documents are sometimes difficult to analyze,
we keep our algorithm as simple as possible. An-
alyzing semantic relations and applying distribu-
tional clustering is another goal for future work.
A salient weak point of our algorithm is the
number of necessary queries allowed to a search
engine. For obtaining a graph of n words, O(n2)
queries are required, which discourages us from
undertaking large experiments. However some de-
vices are possible: if we analyze the texts of the
top retrieved pages by query w, we can guess what
words are likely to co-occur with w. This prepro-
cessing seems promising at least in social network
extraction: we can eliminate 85% of queries in
the 500 nodes case while retaining more than 90%
precision (Asada et al., 2005).
In our evaluation, the chi-square measure per-
formed well. One reason is that the PMI performs
worse when a word group contains rare or frequent
words, as is generally known for mutual informa-
tion measure (Manning and Sch¨utze, 2002). An-
other reason is that if we put one word and two
words to a search engine, the result might be in-
consistent. In an extreme case, the web count of
w1 is below the web count of w1ANDw2. This
</bodyText>
<page confidence="0.991213">
548
</page>
<bodyText confidence="0.999947">
phenomenon depends on how a search engine pro-
cesses AND operator, and results in unstable val-
ues for the PMI. On the other hand, our method
by the chi-square uses a co-occurrence matrix as a
contingency table. For that reason, it suffers less
from the problem. Other statistical measures such
as the likelihood ratio are also applicable.
</bodyText>
<sectionHeader confidence="0.999174" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999975461538462">
This paper describes a new approach for word
clustering using a search engine. The chi-square
measure is used to overcome the broad range of
word counts for a given set of words. We also ap-
ply recently-developed Newman clustering, which
yields promising results through our evaluations.
Our algorithm has few parameters. Therefore,
it can be used easily as a baseline, as suggested by
(Lapata and Keller, 2004). New words are gener-
ated day by day on the web. We believe that to
automatically identify new words and obtain word
groups potentially enhances many NLP applica-
tions.
</bodyText>
<sectionHeader confidence="0.997992" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999188026666667">
Yohei Asada, Yutaka Matsuo, and Mitsuru Ishizuka.
2005. Increasing scalability of researcher network
extraction from the web. Journal of Japanese Soci-
ety for Artificial Intelligence, 20(6).
D. Baker and A. McCallum. 1998. Distributional
clustering of words for text classification. In Proc.
SIGIR-98.
M. Baroni and M. Ueyama. 2005. Building general-
and special-purpose corpora by web crawling. In
Proc. NIJL International Workshop on Language
Corpora.
R. Bekkerman and A. McCallum. 2005. Disambiguat-
ing web appearances of people in a social network.
In Proc. WWW 2005.
M. Cafarella and O. Etzioni. 2005. A search engine for
natural language applications. In Proc. WWW2005.
P. Cheng, W. Lu, J. Teng, and L. Chien. 2004. Cre-
ating multilingual translation lexicons with regional
variations using web corpora. In Proc. ACL 2004,
pages 534–541.
P. Cimiano, S. Handschuh, and S. Staab. 2004. To-
wards the self-annotating web. In Proc. WWW2004,
pages 462–471.
J. Curran. 2002. Ensemble methods for automatic the-
saurus extraction. In Proc. EMNLP 2002.
I. Dhillon, S. Mallela, and R. Kumar. 2002. Enhanced
word clustering for hierarchical text classification.
In Proc. KDD-2002, pages 191–200.
Michelle Girvan and M. E. J. Newman. 2002. Com-
munity structure in social and biological networks.
Proceedings of National Academy of Sciences USA,
99:8271–8276.
C. Huang, S. Chuang, and L. Chien. 2004. Categoriz-
ing unknown text segments for information extrac-
tion using a search result mining approach. In Proc.
IJCNLP 2004, pages 576–586.
K. Kageura, K. Tsuji, and A. Aizawa. 2000. Auto-
matic thesaurus generation through multiple filter-
ing. In Proc. COLING 2000.
H. Kautz, B. Selman, and M. Shah. 1997. The hidden
Web. AI magazine, 18(2):27–35.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using
the web to overcome data sparseness. In EMNLP-
02, pages 230–237.
A. Kilgarriff. 2003. Introduction to the special issue
on the web as corpus. Computer Linguistics, 29(3).
M. Lapata and F. Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised
web-based models for a range of nlp tasks. In Proc.
HLT-NAACL 2004, pages 121–128.
H. Li and N. Abe. 1998. Word clustering and dis-
ambiguation based on co-occurrence data. In Proc.
COLING-ACL98.
C. D. Manning and H. Sch¨utze. 2002. Foundations
of statistical natural language processing. The MIT
Press, London.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hi-
rano, H. Matsuda, K. Takaoka, and M. Asahara.
2000. Morphological analysis system ChaSen ver-
sion 2.2.1 manual. Technical report, NIST.
Y. Matsuo, J. Mori, M. Hamasaki, H. Takeda,
T. Nishimura, K. Hasida, and M. Ishizuka. 2006.
POLYPHONET: An advanced social network ex-
traction system. In Proc. WWW 2006.
P. Mika. 2005. Flink: Semantic web technology for the
extraction and analysis of social networks. Journal
of Web Semantics, 3(2).
K. Miura, Y. Tsuruoka, and J. Tsujii. 2004. Auto-
matic acquisition of concept relations from web doc-
uments with sense clustering. In Proc. IJCNLP04.
A. Motter, A. de Moura, Y. Lai, and P. Dasgupta. 2002.
Topology of the conceptual network of language.
Physical Review E, 65.
M. Newman. 2004. Fast algorithm for detecting com-
munity structure in networks. Phys. Rev. E, 69.
</reference>
<page confidence="0.986049">
549
</page>
<reference confidence="0.999912153846154">
G. Palla, I. Derenyi, I. Farkas, and T. Vicsek. 2005.
Uncovering the overlapping community structure of
complex networks in nature and society. Nature,
435:814.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proc. ACL93, pages
183–190.
K. Tanaka-Ishii and H. Iwasaki. 1996. Clustering co-
occurrence graph using transitivity. In Proc. 16th In-
ternational Conference on Computational Linguis-
tics, pages 680–585.
E. Terra and C. Clarke. 2003. Frequency estimates
for statistical word similarity measures. In Proc.
HLT/NAACL 2003.
P. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proc. ECML-2001,
pages 491–502.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. ACL’02, pages 417–424.
P. Turney. 2004. Word sense disambiguation by web
mining for word co-occurrence probabilities. In
Proc. SENSEVAL-3.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. COLING
2002.
</reference>
<page confidence="0.99731">
550
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.091841">
<title confidence="0.98377">Graph-based Word Clustering using a Web Search Engine</title>
<author confidence="0.876149">Yutaka</author>
<affiliation confidence="0.997959">National Institute of Industrial Science and</affiliation>
<address confidence="0.935269">1-18-13 Sotokanda, Tokyo</address>
<email confidence="0.8064765">y.matsuo@aist.go.jpKˆoki</email>
<affiliation confidence="0.225244">Hottolink</affiliation>
<address confidence="0.4313345">2-11-17 Tokyo</address>
<email confidence="0.921745">uchi@hottolink.co.jp</email>
<abstract confidence="0.993274722222222">Word clustering is important for automatic thesaurus construction, text classification, and word sense disambiguation. Recently, several studies have reported using the web as a corpus. This paper proposes an unsupervised algorithm for word clustering based on a word similarity measure by web counts. Each pair of words is queried to a search engine, which produces a co-occurrence matrix. By calculating the similarity of words, a word cooccurrence graph is obtained. A new kind graph clustering algorithm called Newclustering applied for efficiently identifying word clusters. Evaluations are made on two sets of word groups derived from a web directory and WordNet.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yohei Asada</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Increasing scalability of researcher network extraction from the web.</title>
<date>2005</date>
<journal>Journal of Japanese Society for Artificial Intelligence,</journal>
<volume>20</volume>
<issue>6</issue>
<contexts>
<context position="25233" citStr="Asada et al., 2005" startWordPosition="4197" endWordPosition="4200">istributional clustering is another goal for future work. A salient weak point of our algorithm is the number of necessary queries allowed to a search engine. For obtaining a graph of n words, O(n2) queries are required, which discourages us from undertaking large experiments. However some devices are possible: if we analyze the texts of the top retrieved pages by query w, we can guess what words are likely to co-occur with w. This preprocessing seems promising at least in social network extraction: we can eliminate 85% of queries in the 500 nodes case while retaining more than 90% precision (Asada et al., 2005). In our evaluation, the chi-square measure performed well. One reason is that the PMI performs worse when a word group contains rare or frequent words, as is generally known for mutual information measure (Manning and Sch¨utze, 2002). Another reason is that if we put one word and two words to a search engine, the result might be inconsistent. In an extreme case, the web count of w1 is below the web count of w1ANDw2. This 548 phenomenon depends on how a search engine processes AND operator, and results in unstable values for the PMI. On the other hand, our method by the chi-square uses a co-oc</context>
</contexts>
<marker>Asada, Matsuo, Ishizuka, 2005</marker>
<rawString>Yohei Asada, Yutaka Matsuo, and Mitsuru Ishizuka. 2005. Increasing scalability of researcher network extraction from the web. Journal of Japanese Society for Artificial Intelligence, 20(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Baker</author>
<author>A McCallum</author>
</authors>
<title>Distributional clustering of words for text classification.</title>
<date>1998</date>
<booktitle>In Proc. SIGIR-98.</booktitle>
<contexts>
<context position="6946" citStr="Baker and McCallum, 1998" startWordPosition="1082" endWordPosition="1085">ts. They report that PMI (with a certain window size) performs best on average. Also, PMI-IR is useful for calculating semantic orientation and rating reviews (Turney, 2002). As described, PMI is one of many measures to calculate the strength of word similarity or word association (Manning and Sch¨utze, 2002). An important assumption is that similarity between words is a consequence of word co-occurrence, or that the proximity of words in text is indicative of relationship between them, such as synonymy or antonymy. A commonly used technique to obtain word groups is distributional clustering (Baker and McCallum, 1998). Distributional clustering of words was first proposed by Pereira Tishby &amp; Lee in (Pereira et al., 1993): They cluster nouns according to their conditional verb distributions. Graphic representations for word similarity have also been advanced by several researchers. Kageura et al. (2000) propose automatic thesaurus generation based on a graphic representation. By applying a minimum edge cut, the corresponding English terms and Japanese terms are identified as a cluster. Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition. A graph is produced by linking pairs of wo</context>
</contexts>
<marker>Baker, McCallum, 1998</marker>
<rawString>D. Baker and A. McCallum. 1998. Distributional clustering of words for text classification. In Proc. SIGIR-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>M Ueyama</author>
</authors>
<title>Building generaland special-purpose corpora by web crawling.</title>
<date>2005</date>
<booktitle>In Proc. NIJL International Workshop on Language Corpora.</booktitle>
<contexts>
<context position="4932" citStr="Baroni and Ueyama, 2005" startWordPosition="760" endWordPosition="763">apanese words in this paper. The remainder of this paper is organized as follows: We overview the related studies in the next section. Our proposed algorithm is described in Section 3. Sections 4 and 5 explain evaluations and advance discussion. Finally, we conclude the paper. 2 Related Works A number of studies have explained the use of the web for NLP tasks e.g., creating multilingual translation lexicons (Cheng et al., 2004), text classification (Huang et al., 2004), and word sense disambiguation (Turney, 2004). M. Baroni and M. Ueyama summarize three approaches to use the web as a corpus (Baroni and Ueyama, 2005): using web counts as frequency estimates, building corpora through search engine queries, and crawling the web for linguistic purposes. Commercial search engines are optimized for ordinary users. Therefore, it is desirable to crawl the web and to develop specific search engines for NLP applications (Cafarella and Etzioni, 2005). However, considering that great efforts are taken in commercial search engines to maintain quality of crawling and indexing, especially against spammers, it is still important to pursue the possibility of using the current search engines for NLP applications. P. Turne</context>
</contexts>
<marker>Baroni, Ueyama, 2005</marker>
<rawString>M. Baroni and M. Ueyama. 2005. Building generaland special-purpose corpora by web crawling. In Proc. NIJL International Workshop on Language Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
<author>A McCallum</author>
</authors>
<title>Disambiguating web appearances of people in a social network. In</title>
<date>2005</date>
<booktitle>Proc. WWW</booktitle>
<contexts>
<context position="8234" citStr="Bekkerman and McCallum, 2005" startWordPosition="1281" endWordPosition="1284">. An incremental cluster-building algorithm achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. Another study builds a co-occurrence graph of terms and decomposes it to identify relevant terms by duplicating nodes and edges (Tanaka-Ishii and Iwasaki, 1996). It focuses on transitivity: if transitivity does not hold between three nodes (e.g., if edge a-b and b-c exist but edge a-c does not), the nodes should be in separate clusters. A network of words (or named entities) on the web is investigated also in the context of the Semantic Web (Cimiano et al., 2004; Bekkerman and McCallum, 2005). Especially, a social network of persons is mined from the web using a search engine (Kautz et al., 1997; Mika, 2005; Matsuo et al., 2006). In these studies, the Jaccard coefficient is often used to measure the co-occurrence of entities. We compare Jaccard coefficients in our evaluations. In the research field on complex networks, 543 Table 1: Web counts for each word. printer print InterLaser ink TV Aquos Sharp 17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000 Table 2: Co-occurrence matrix by web counts. printer print InterLaser ink TV Aquos Sharp printer — 4780000 179 47</context>
</contexts>
<marker>Bekkerman, McCallum, 2005</marker>
<rawString>R. Bekkerman and A. McCallum. 2005. Disambiguating web appearances of people in a social network. In Proc. WWW 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cafarella</author>
<author>O Etzioni</author>
</authors>
<title>A search engine for natural language applications. In</title>
<date>2005</date>
<booktitle>Proc. WWW2005.</booktitle>
<contexts>
<context position="5262" citStr="Cafarella and Etzioni, 2005" startWordPosition="811" endWordPosition="814">ed the use of the web for NLP tasks e.g., creating multilingual translation lexicons (Cheng et al., 2004), text classification (Huang et al., 2004), and word sense disambiguation (Turney, 2004). M. Baroni and M. Ueyama summarize three approaches to use the web as a corpus (Baroni and Ueyama, 2005): using web counts as frequency estimates, building corpora through search engine queries, and crawling the web for linguistic purposes. Commercial search engines are optimized for ordinary users. Therefore, it is desirable to crawl the web and to develop specific search engines for NLP applications (Cafarella and Etzioni, 2005). However, considering that great efforts are taken in commercial search engines to maintain quality of crawling and indexing, especially against spammers, it is still important to pursue the possibility of using the current search engines for NLP applications. P. Turney (Turney, 2001) presents an unsupervised learning algorithm for recognizing synonyms by querying a web search engine. The task of recognizing synonyms is, given a target word and a set of alternative words, to choose the word that is most similar in meaning to the target word. The algorithm uses pointwise mutual information (PM</context>
</contexts>
<marker>Cafarella, Etzioni, 2005</marker>
<rawString>M. Cafarella and O. Etzioni. 2005. A search engine for natural language applications. In Proc. WWW2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cheng</author>
<author>W Lu</author>
<author>J Teng</author>
<author>L Chien</author>
</authors>
<title>Creating multilingual translation lexicons with regional variations using web corpora.</title>
<date>2004</date>
<booktitle>In Proc. ACL</booktitle>
<pages>534--541</pages>
<contexts>
<context position="4739" citStr="Cheng et al., 2004" startWordPosition="727" endWordPosition="730"> from a web directory and WordNet. The chi-square measure and Newman clustering are both used in our algorithm, they are revealed to outperform PMI and hierarchical clustering. We target Japanese words in this paper. The remainder of this paper is organized as follows: We overview the related studies in the next section. Our proposed algorithm is described in Section 3. Sections 4 and 5 explain evaluations and advance discussion. Finally, we conclude the paper. 2 Related Works A number of studies have explained the use of the web for NLP tasks e.g., creating multilingual translation lexicons (Cheng et al., 2004), text classification (Huang et al., 2004), and word sense disambiguation (Turney, 2004). M. Baroni and M. Ueyama summarize three approaches to use the web as a corpus (Baroni and Ueyama, 2005): using web counts as frequency estimates, building corpora through search engine queries, and crawling the web for linguistic purposes. Commercial search engines are optimized for ordinary users. Therefore, it is desirable to crawl the web and to develop specific search engines for NLP applications (Cafarella and Etzioni, 2005). However, considering that great efforts are taken in commercial search engi</context>
</contexts>
<marker>Cheng, Lu, Teng, Chien, 2004</marker>
<rawString>P. Cheng, W. Lu, J. Teng, and L. Chien. 2004. Creating multilingual translation lexicons with regional variations using web corpora. In Proc. ACL 2004, pages 534–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>S Handschuh</author>
<author>S Staab</author>
</authors>
<title>Towards the self-annotating web. In</title>
<date>2004</date>
<booktitle>Proc. WWW2004,</booktitle>
<pages>462--471</pages>
<contexts>
<context position="2595" citStr="Cimiano et al., 2004" startWordPosition="385" endWordPosition="388">ng correction, adjective ordering, compound noun bracketing, countability detection, and so on (Lapata and Keller, 2004). For some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a standard large corpus; the web yields better counts than the BNC. The web is an excellent source of information on new words. Therefore, automatic thesaurus construction (Curran, 2002) offers great potential for various useful NLP applications. Several studies have addressed the extraction of hypernyms and hyponyms from the web (Miura et al., 2004; Cimiano et al., 2004). P. Turney (2001) presents a method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (PMI). For further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets. It also contributes to word sense disambiguation (Li and Abe, 1998) and text classification (Dhillon et al., 2002) because the dimensionality is reduced efficiently. This paper presents an unsupervised algorithm for word clustering based on a word similarity measure by web counts. Given a set of words, the algorithm clusters the words into gr</context>
<context position="8203" citStr="Cimiano et al., 2004" startWordPosition="1277" endWordPosition="1280">yntactic relationships. An incremental cluster-building algorithm achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. Another study builds a co-occurrence graph of terms and decomposes it to identify relevant terms by duplicating nodes and edges (Tanaka-Ishii and Iwasaki, 1996). It focuses on transitivity: if transitivity does not hold between three nodes (e.g., if edge a-b and b-c exist but edge a-c does not), the nodes should be in separate clusters. A network of words (or named entities) on the web is investigated also in the context of the Semantic Web (Cimiano et al., 2004; Bekkerman and McCallum, 2005). Especially, a social network of persons is mined from the web using a search engine (Kautz et al., 1997; Mika, 2005; Matsuo et al., 2006). In these studies, the Jaccard coefficient is often used to measure the co-occurrence of entities. We compare Jaccard coefficients in our evaluations. In the research field on complex networks, 543 Table 1: Web counts for each word. printer print InterLaser ink TV Aquos Sharp 17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000 Table 2: Co-occurrence matrix by web counts. printer print InterLaser ink TV Aquos</context>
</contexts>
<marker>Cimiano, Handschuh, Staab, 2004</marker>
<rawString>P. Cimiano, S. Handschuh, and S. Staab. 2004. Towards the self-annotating web. In Proc. WWW2004, pages 462–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>Ensemble methods for automatic thesaurus extraction.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP</booktitle>
<contexts>
<context position="2407" citStr="Curran, 2002" startWordPosition="357" endWordPosition="358">-3-1 Hongo Tokyo 113-8656 Mitsuru Ishizuka University of Tokyo 7-3-1 Hongo Tokyo 113-8656 ishizuka@i.u-tokyo.ac.jp grams, various tasks are attainable using webbased models: spelling correction, adjective ordering, compound noun bracketing, countability detection, and so on (Lapata and Keller, 2004). For some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a standard large corpus; the web yields better counts than the BNC. The web is an excellent source of information on new words. Therefore, automatic thesaurus construction (Curran, 2002) offers great potential for various useful NLP applications. Several studies have addressed the extraction of hypernyms and hyponyms from the web (Miura et al., 2004; Cimiano et al., 2004). P. Turney (2001) presents a method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (PMI). For further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets. It also contributes to word sense disambiguation (Li and Abe, 1998) and text classification (Dhillon et al., 2002) because the dimensionality is reduced eff</context>
</contexts>
<marker>Curran, 2002</marker>
<rawString>J. Curran. 2002. Ensemble methods for automatic thesaurus extraction. In Proc. EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dhillon</author>
<author>S Mallela</author>
<author>R Kumar</author>
</authors>
<title>Enhanced word clustering for hierarchical text classification.</title>
<date>2002</date>
<booktitle>In Proc. KDD-2002,</booktitle>
<pages>191--200</pages>
<contexts>
<context position="2965" citStr="Dhillon et al., 2002" startWordPosition="439" endWordPosition="442">rds. Therefore, automatic thesaurus construction (Curran, 2002) offers great potential for various useful NLP applications. Several studies have addressed the extraction of hypernyms and hyponyms from the web (Miura et al., 2004; Cimiano et al., 2004). P. Turney (2001) presents a method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (PMI). For further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets. It also contributes to word sense disambiguation (Li and Abe, 1998) and text classification (Dhillon et al., 2002) because the dimensionality is reduced efficiently. This paper presents an unsupervised algorithm for word clustering based on a word similarity measure by web counts. Given a set of words, the algorithm clusters the words into groups so that the similar words are in the same cluster. Each pair of words is queried to a search engine, which results in a co-occurrence matrix. By calculating the similarity of words, a word co-occurrence graph is created. Then, a new kind of graph clustering algorithm, called Newman clustering, is applied. Newman clustering emphasizes betweenness of an edge and id</context>
</contexts>
<marker>Dhillon, Mallela, Kumar, 2002</marker>
<rawString>I. Dhillon, S. Mallela, and R. Kumar. 2002. Enhanced word clustering for hierarchical text classification. In Proc. KDD-2002, pages 191–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Girvan</author>
<author>M E J Newman</author>
</authors>
<title>Community structure in social and biological networks.</title>
<date>2002</date>
<booktitle>Proceedings of National Academy of Sciences USA,</booktitle>
<pages>99--8271</pages>
<contexts>
<context position="9457" citStr="Girvan and Newman, 2002" startWordPosition="1475" endWordPosition="1478">20000 4530000 201000 990000 print 4780000 — 183 4800000 8390000 86400 1390000 InterLaser 179 183 116 65 0 0 ink 4720000 4800000 116 — 10600000 144000 656000 TV 4530000 8390000 65 10600000 — 1660000 42300000 Aquos 201000 86400 0 144000 1660000 — 1790000 Sharp 990000 1390000 0 656000 42300000 1790000 — structures of various networks are investigated in detail. For example, Motter (2002) targeted a conceptual network from a thesaurus and demonstrated its small-world structure. Recently, numerous works have identified communities (or densely-connected subgraphs) from large networks (Newman, 2004; Girvan and Newman, 2002; Palla et al., 2005) as explained in the next section. 3 Word Clustering using Web Counts 3.1 Co-occurrence by a Search Engine A typical word clustering task is described as follows: given a set of words (nouns), cluster words into groups so that the similar words are in the same cluster 1. Let us take an example. Assume a set of words is given: プリ,/タ (printer), 印刷 (print), 4,/ターレーザー (InterLaser), 4 ,/ク (ink), TV (TV), Aquos (Aquos), and Sharp (Sharp). Apparently, the first four words are related to a printer, and the last three words are related to a TV 2. In this case, we would like to have</context>
<context position="14079" citStr="Girvan and Newman (2002)" startWordPosition="2320" endWordPosition="2323">a contingency table: Eb0 = fw1,w , Ec0 = fw2,w; w∈W;w6=w2 w∈W;w6=w1 Ed0 = Efw,w&apos; , N0 = fw,w&apos;, w,w&apos;∈W;w and w&apos;6=w1 nor w2 w,w&apos;∈W where W represents a given set of words. Then chi-square (within the word list W) is defined as We should note that X2depends on a word set W. It calculates the relative strength of cooccurrences. Table 4 shows the X2values. Aquos has high values only with TV and Sharp as expected. 3.2 Clustering on Co-occurrence Graph Recently, a series of effective graph clustering methods has been advanced. Pioneering work that specifically emphasizes edge betweenness was done by Girvan and Newman (2002): we call the method as GN algorithm. Betweenness of an edge is the number of shortest paths between pairs of nodes that run along it. Figure 1 (i) shows that two “communities” (in Girvan’s term), i.e. {a,b,c} and {d,e,f,g}, which are connected by edge c-d. Edge c-d has high betweenness because numerous shortest paths (e.g., from a to d, from b to e, ...) traverse the edge. The graph is likely to be separated into densely connected subgraphs if we cut the high betweenness edge. The GN algorithm is different from the minimum edge cut. For (i), the results are identical: By cutting edge c-d, whi</context>
</contexts>
<marker>Girvan, Newman, 2002</marker>
<rawString>Michelle Girvan and M. E. J. Newman. 2002. Community structure in social and biological networks. Proceedings of National Academy of Sciences USA, 99:8271–8276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Huang</author>
<author>S Chuang</author>
<author>L Chien</author>
</authors>
<title>Categorizing unknown text segments for information extraction using a search result mining approach.</title>
<date>2004</date>
<booktitle>In Proc. IJCNLP</booktitle>
<pages>576--586</pages>
<contexts>
<context position="4781" citStr="Huang et al., 2004" startWordPosition="734" endWordPosition="737">-square measure and Newman clustering are both used in our algorithm, they are revealed to outperform PMI and hierarchical clustering. We target Japanese words in this paper. The remainder of this paper is organized as follows: We overview the related studies in the next section. Our proposed algorithm is described in Section 3. Sections 4 and 5 explain evaluations and advance discussion. Finally, we conclude the paper. 2 Related Works A number of studies have explained the use of the web for NLP tasks e.g., creating multilingual translation lexicons (Cheng et al., 2004), text classification (Huang et al., 2004), and word sense disambiguation (Turney, 2004). M. Baroni and M. Ueyama summarize three approaches to use the web as a corpus (Baroni and Ueyama, 2005): using web counts as frequency estimates, building corpora through search engine queries, and crawling the web for linguistic purposes. Commercial search engines are optimized for ordinary users. Therefore, it is desirable to crawl the web and to develop specific search engines for NLP applications (Cafarella and Etzioni, 2005). However, considering that great efforts are taken in commercial search engines to maintain quality of crawling and in</context>
</contexts>
<marker>Huang, Chuang, Chien, 2004</marker>
<rawString>C. Huang, S. Chuang, and L. Chien. 2004. Categorizing unknown text segments for information extraction using a search result mining approach. In Proc. IJCNLP 2004, pages 576–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kageura</author>
<author>K Tsuji</author>
<author>A Aizawa</author>
</authors>
<title>Automatic thesaurus generation through multiple filtering.</title>
<date>2000</date>
<booktitle>In Proc. COLING</booktitle>
<contexts>
<context position="7236" citStr="Kageura et al. (2000)" startWordPosition="1125" endWordPosition="1128"> Sch¨utze, 2002). An important assumption is that similarity between words is a consequence of word co-occurrence, or that the proximity of words in text is indicative of relationship between them, such as synonymy or antonymy. A commonly used technique to obtain word groups is distributional clustering (Baker and McCallum, 1998). Distributional clustering of words was first proposed by Pereira Tishby &amp; Lee in (Pereira et al., 1993): They cluster nouns according to their conditional verb distributions. Graphic representations for word similarity have also been advanced by several researchers. Kageura et al. (2000) propose automatic thesaurus generation based on a graphic representation. By applying a minimum edge cut, the corresponding English terms and Japanese terms are identified as a cluster. Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition. A graph is produced by linking pairs of words which participate in particular syntactic relationships. An incremental cluster-building algorithm achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. Another study builds a co-occurrence graph of terms and decomposes it to identify relevant terms by</context>
</contexts>
<marker>Kageura, Tsuji, Aizawa, 2000</marker>
<rawString>K. Kageura, K. Tsuji, and A. Aizawa. 2000. Automatic thesaurus generation through multiple filtering. In Proc. COLING 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kautz</author>
<author>B Selman</author>
<author>M Shah</author>
</authors>
<date>1997</date>
<booktitle>The hidden Web. AI magazine,</booktitle>
<pages>18--2</pages>
<contexts>
<context position="8339" citStr="Kautz et al., 1997" startWordPosition="1301" endWordPosition="1304">ordNet classes. Another study builds a co-occurrence graph of terms and decomposes it to identify relevant terms by duplicating nodes and edges (Tanaka-Ishii and Iwasaki, 1996). It focuses on transitivity: if transitivity does not hold between three nodes (e.g., if edge a-b and b-c exist but edge a-c does not), the nodes should be in separate clusters. A network of words (or named entities) on the web is investigated also in the context of the Semantic Web (Cimiano et al., 2004; Bekkerman and McCallum, 2005). Especially, a social network of persons is mined from the web using a search engine (Kautz et al., 1997; Mika, 2005; Matsuo et al., 2006). In these studies, the Jaccard coefficient is often used to measure the co-occurrence of entities. We compare Jaccard coefficients in our evaluations. In the research field on complex networks, 543 Table 1: Web counts for each word. printer print InterLaser ink TV Aquos Sharp 17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000 Table 2: Co-occurrence matrix by web counts. printer print InterLaser ink TV Aquos Sharp printer — 4780000 179 4720000 4530000 201000 990000 print 4780000 — 183 4800000 8390000 86400 1390000 InterLaser 179 183 116 65 0</context>
</contexts>
<marker>Kautz, Selman, Shah, 1997</marker>
<rawString>H. Kautz, B. Selman, and M. Shah. 1997. The hidden Web. AI magazine, 18(2):27–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
<author>M Lapata</author>
<author>O Ourioupina</author>
</authors>
<title>Using the web to overcome data sparseness.</title>
<date>2002</date>
<booktitle>In EMNLP02,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="1417" citStr="Keller et al. (2002)" startWordPosition="208" endWordPosition="211">lled Newman clustering is applied for efficiently identifying word clusters. Evaluations are made on two sets of word groups derived from a web directory and WordNet. 1 Introduction The web is a good source of linguistic information for several natural language techniques such as question answering, language modeling, and multilingual lexicon acquisition. Numerous studies have examined the use of the web as a corpus (Kilgarriff, 2003). Web-based models perform especially well against the sparse data problem: Statistical techniques perform poorly when the words are rarely used. For example, F. Keller et al. (2002) use the web to obtain frequencies for unseen bigrams in a given corpus. They count for adjective-noun, noun-noun, and verb-object bigrams by querying a search engine, and demonstrate that web frequencies (web counts) correlate with frequencies from a carefully edited corpus such as the British National Corpus (BNC). Aside from counting biTakeshi Sakaki University of Tokyo 7-3-1 Hongo Tokyo 113-8656 Mitsuru Ishizuka University of Tokyo 7-3-1 Hongo Tokyo 113-8656 ishizuka@i.u-tokyo.ac.jp grams, various tasks are attainable using webbased models: spelling correction, adjective ordering, compound</context>
</contexts>
<marker>Keller, Lapata, Ourioupina, 2002</marker>
<rawString>F. Keller, M. Lapata, and O. Ourioupina. 2002. Using the web to overcome data sparseness. In EMNLP02, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Introduction to the special issue on the web as corpus.</title>
<date>2003</date>
<journal>Computer Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="1235" citStr="Kilgarriff, 2003" startWordPosition="182" endWordPosition="183"> a search engine, which produces a co-occurrence matrix. By calculating the similarity of words, a word cooccurrence graph is obtained. A new kind of graph clustering algorithm called Newman clustering is applied for efficiently identifying word clusters. Evaluations are made on two sets of word groups derived from a web directory and WordNet. 1 Introduction The web is a good source of linguistic information for several natural language techniques such as question answering, language modeling, and multilingual lexicon acquisition. Numerous studies have examined the use of the web as a corpus (Kilgarriff, 2003). Web-based models perform especially well against the sparse data problem: Statistical techniques perform poorly when the words are rarely used. For example, F. Keller et al. (2002) use the web to obtain frequencies for unseen bigrams in a given corpus. They count for adjective-noun, noun-noun, and verb-object bigrams by querying a search engine, and demonstrate that web frequencies (web counts) correlate with frequencies from a carefully edited corpus such as the British National Corpus (BNC). Aside from counting biTakeshi Sakaki University of Tokyo 7-3-1 Hongo Tokyo 113-8656 Mitsuru Ishizuk</context>
</contexts>
<marker>Kilgarriff, 2003</marker>
<rawString>A. Kilgarriff. 2003. Introduction to the special issue on the web as corpus. Computer Linguistics, 29(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of nlp tasks.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL</booktitle>
<pages>121--128</pages>
<contexts>
<context position="2094" citStr="Lapata and Keller, 2004" startWordPosition="307" endWordPosition="310"> in a given corpus. They count for adjective-noun, noun-noun, and verb-object bigrams by querying a search engine, and demonstrate that web frequencies (web counts) correlate with frequencies from a carefully edited corpus such as the British National Corpus (BNC). Aside from counting biTakeshi Sakaki University of Tokyo 7-3-1 Hongo Tokyo 113-8656 Mitsuru Ishizuka University of Tokyo 7-3-1 Hongo Tokyo 113-8656 ishizuka@i.u-tokyo.ac.jp grams, various tasks are attainable using webbased models: spelling correction, adjective ordering, compound noun bracketing, countability detection, and so on (Lapata and Keller, 2004). For some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a standard large corpus; the web yields better counts than the BNC. The web is an excellent source of information on new words. Therefore, automatic thesaurus construction (Curran, 2002) offers great potential for various useful NLP applications. Several studies have addressed the extraction of hypernyms and hyponyms from the web (Miura et al., 2004; Cimiano et al., 2004). P. Turney (2001) presents a method to recognize synonyms by obtaining word counts and calculating</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>M. Lapata and F. Keller. 2004. The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of nlp tasks. In Proc. HLT-NAACL 2004, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Word clustering and disambiguation based on co-occurrence data. In</title>
<date>1998</date>
<booktitle>Proc. COLING-ACL98.</booktitle>
<contexts>
<context position="2918" citStr="Li and Abe, 1998" startWordPosition="432" endWordPosition="435">n excellent source of information on new words. Therefore, automatic thesaurus construction (Curran, 2002) offers great potential for various useful NLP applications. Several studies have addressed the extraction of hypernyms and hyponyms from the web (Miura et al., 2004; Cimiano et al., 2004). P. Turney (2001) presents a method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (PMI). For further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets. It also contributes to word sense disambiguation (Li and Abe, 1998) and text classification (Dhillon et al., 2002) because the dimensionality is reduced efficiently. This paper presents an unsupervised algorithm for word clustering based on a word similarity measure by web counts. Given a set of words, the algorithm clusters the words into groups so that the similar words are in the same cluster. Each pair of words is queried to a search engine, which results in a co-occurrence matrix. By calculating the similarity of words, a word co-occurrence graph is created. Then, a new kind of graph clustering algorithm, called Newman clustering, is applied. Newman clus</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>H. Li and N. Abe. 1998. Word clustering and disambiguation based on co-occurrence data. In Proc. COLING-ACL98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>2002</date>
<publisher>The MIT Press,</publisher>
<location>London.</location>
<marker>Manning, Sch¨utze, 2002</marker>
<rawString>C. D. Manning and H. Sch¨utze. 2002. Foundations of statistical natural language processing. The MIT Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
<author>A Kitauchi</author>
<author>T Yamashita</author>
<author>Y Hirano</author>
<author>H Matsuda</author>
<author>K Takaoka</author>
<author>M Asahara</author>
</authors>
<title>Morphological analysis system ChaSen version 2.2.1 manual.</title>
<date>2000</date>
<tech>Technical report, NIST.</tech>
<contexts>
<context position="19839" citStr="Matsumoto et al., 2000" startWordPosition="3321" endWordPosition="3324">e words into the correct nine groups. Here we investigate whether the correct nine words are selected for each word using the co-occurrence measure. We compare pointwise mutual information (PMI), the Jaccard coefficient (Jaccard), and chi-square (χ2). We chose these methods for comparison because PMI performs best in (Terra and Clarke, 2003). The Jaccard coefficient is often used in social network mining from the web. Table 7 shows the precision of each method. Experiments are repeated five times. We keep each method that outputs the 2. Apply the Japanese morphological analysis system ChaSen (Matsumoto et al., 2000) to the documents. Calculate the score of each word w in category c similarly to TF-IDF: score(w, c) = fc(w) × log(Nall/fall(w)) where fc denotes the document frequency of word w in category c, Nall denotes the number of all documents, and fall(w) denotes the frequency of word w in all documents. 3. For each category, the top 10 words are selected as the word group. °We first get all urls, sort them, and select a sample randomly. � � Figure 5: Procedure for obtaining word groups for a category. Table 7: Precision for DMOZ-J set. PMI Jaccard χ2 Mean 0.415 0.402 0.537 Min 0.396 0.376 0.493 Max 0</context>
</contexts>
<marker>Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda, Takaoka, Asahara, 2000</marker>
<rawString>Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano, H. Matsuda, K. Takaoka, and M. Asahara. 2000. Morphological analysis system ChaSen version 2.2.1 manual. Technical report, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>J Mori</author>
<author>M Hamasaki</author>
<author>H Takeda</author>
<author>T Nishimura</author>
<author>K Hasida</author>
<author>M Ishizuka</author>
</authors>
<title>POLYPHONET: An advanced social network extraction system.</title>
<date>2006</date>
<booktitle>In Proc. WWW</booktitle>
<contexts>
<context position="8373" citStr="Matsuo et al., 2006" startWordPosition="1307" endWordPosition="1310">ilds a co-occurrence graph of terms and decomposes it to identify relevant terms by duplicating nodes and edges (Tanaka-Ishii and Iwasaki, 1996). It focuses on transitivity: if transitivity does not hold between three nodes (e.g., if edge a-b and b-c exist but edge a-c does not), the nodes should be in separate clusters. A network of words (or named entities) on the web is investigated also in the context of the Semantic Web (Cimiano et al., 2004; Bekkerman and McCallum, 2005). Especially, a social network of persons is mined from the web using a search engine (Kautz et al., 1997; Mika, 2005; Matsuo et al., 2006). In these studies, the Jaccard coefficient is often used to measure the co-occurrence of entities. We compare Jaccard coefficients in our evaluations. In the research field on complex networks, 543 Table 1: Web counts for each word. printer print InterLaser ink TV Aquos Sharp 17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000 Table 2: Co-occurrence matrix by web counts. printer print InterLaser ink TV Aquos Sharp printer — 4780000 179 4720000 4530000 201000 990000 print 4780000 — 183 4800000 8390000 86400 1390000 InterLaser 179 183 116 65 0 0 ink 4720000 4800000 116 — 10600</context>
</contexts>
<marker>Matsuo, Mori, Hamasaki, Takeda, Nishimura, Hasida, Ishizuka, 2006</marker>
<rawString>Y. Matsuo, J. Mori, M. Hamasaki, H. Takeda, T. Nishimura, K. Hasida, and M. Ishizuka. 2006. POLYPHONET: An advanced social network extraction system. In Proc. WWW 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mika</author>
</authors>
<title>Flink: Semantic web technology for the extraction and analysis of social networks.</title>
<date>2005</date>
<journal>Journal of Web Semantics,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="8351" citStr="Mika, 2005" startWordPosition="1305" endWordPosition="1306">her study builds a co-occurrence graph of terms and decomposes it to identify relevant terms by duplicating nodes and edges (Tanaka-Ishii and Iwasaki, 1996). It focuses on transitivity: if transitivity does not hold between three nodes (e.g., if edge a-b and b-c exist but edge a-c does not), the nodes should be in separate clusters. A network of words (or named entities) on the web is investigated also in the context of the Semantic Web (Cimiano et al., 2004; Bekkerman and McCallum, 2005). Especially, a social network of persons is mined from the web using a search engine (Kautz et al., 1997; Mika, 2005; Matsuo et al., 2006). In these studies, the Jaccard coefficient is often used to measure the co-occurrence of entities. We compare Jaccard coefficients in our evaluations. In the research field on complex networks, 543 Table 1: Web counts for each word. printer print InterLaser ink TV Aquos Sharp 17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000 Table 2: Co-occurrence matrix by web counts. printer print InterLaser ink TV Aquos Sharp printer — 4780000 179 4720000 4530000 201000 990000 print 4780000 — 183 4800000 8390000 86400 1390000 InterLaser 179 183 116 65 0 0 ink 47200</context>
</contexts>
<marker>Mika, 2005</marker>
<rawString>P. Mika. 2005. Flink: Semantic web technology for the extraction and analysis of social networks. Journal of Web Semantics, 3(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Miura</author>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
</authors>
<title>Automatic acquisition of concept relations from web documents with sense clustering.</title>
<date>2004</date>
<booktitle>In Proc. IJCNLP04.</booktitle>
<contexts>
<context position="2572" citStr="Miura et al., 2004" startWordPosition="381" endWordPosition="384">based models: spelling correction, adjective ordering, compound noun bracketing, countability detection, and so on (Lapata and Keller, 2004). For some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a standard large corpus; the web yields better counts than the BNC. The web is an excellent source of information on new words. Therefore, automatic thesaurus construction (Curran, 2002) offers great potential for various useful NLP applications. Several studies have addressed the extraction of hypernyms and hyponyms from the web (Miura et al., 2004; Cimiano et al., 2004). P. Turney (2001) presents a method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (PMI). For further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets. It also contributes to word sense disambiguation (Li and Abe, 1998) and text classification (Dhillon et al., 2002) because the dimensionality is reduced efficiently. This paper presents an unsupervised algorithm for word clustering based on a word similarity measure by web counts. Given a set of words, the algorithm clu</context>
</contexts>
<marker>Miura, Tsuruoka, Tsujii, 2004</marker>
<rawString>K. Miura, Y. Tsuruoka, and J. Tsujii. 2004. Automatic acquisition of concept relations from web documents with sense clustering. In Proc. IJCNLP04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Motter</author>
<author>A de Moura</author>
<author>Y Lai</author>
<author>P Dasgupta</author>
</authors>
<title>Topology of the conceptual network of language. Physical Review E,</title>
<date>2002</date>
<marker>Motter, de Moura, Lai, Dasgupta, 2002</marker>
<rawString>A. Motter, A. de Moura, Y. Lai, and P. Dasgupta. 2002. Topology of the conceptual network of language. Physical Review E, 65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Newman</author>
</authors>
<title>Fast algorithm for detecting community structure in networks.</title>
<date>2004</date>
<journal>Phys. Rev. E,</journal>
<volume>69</volume>
<contexts>
<context position="9432" citStr="Newman, 2004" startWordPosition="1473" endWordPosition="1474">4780000 179 4720000 4530000 201000 990000 print 4780000 — 183 4800000 8390000 86400 1390000 InterLaser 179 183 116 65 0 0 ink 4720000 4800000 116 — 10600000 144000 656000 TV 4530000 8390000 65 10600000 — 1660000 42300000 Aquos 201000 86400 0 144000 1660000 — 1790000 Sharp 990000 1390000 0 656000 42300000 1790000 — structures of various networks are investigated in detail. For example, Motter (2002) targeted a conceptual network from a thesaurus and demonstrated its small-world structure. Recently, numerous works have identified communities (or densely-connected subgraphs) from large networks (Newman, 2004; Girvan and Newman, 2002; Palla et al., 2005) as explained in the next section. 3 Word Clustering using Web Counts 3.1 Co-occurrence by a Search Engine A typical word clustering task is described as follows: given a set of words (nouns), cluster words into groups so that the similar words are in the same cluster 1. Let us take an example. Assume a set of words is given: プリ,/タ (printer), 印刷 (print), 4,/ターレーザー (InterLaser), 4 ,/ク (ink), TV (TV), Aquos (Aquos), and Sharp (Sharp). Apparently, the first four words are related to a printer, and the last three words are related to a TV 2. In this ca</context>
<context position="15106" citStr="Newman, 2004" startWordPosition="2502" endWordPosition="2504">nto densely connected subgraphs if we cut the high betweenness edge. The GN algorithm is different from the minimum edge cut. For (i), the results are identical: By cutting edge c-d, which is a minimum edge cut, we can obtain two clusters. However in case of (ii), there are two candidates for the minimum edge cut, whereas the highest betweenness edge is still only edge c-d. Girvan et al. (2002) shows that this clustering works well to various networks from biological to social networks. Numerous studies have been inspired by that work. One prominent effort is a faster variant of GN algorithm (Newman, 2004), which we call Newman clustering in N0 × (a × d0 − b0 × c0)2 X2W(w1, w2) = (a + b0) × (a + c0) × (b0 + d0) × (c0 + d0). 545 Figure 3: A word graph for 88 Japanese words. Figure 2: An illustration of graph-based word clustering. this paper. In Newman clustering, instead of explicitly calculating high-betweenness edges (which is computationally demanding), an objective function is defined as follows: We assume that we have separate clusters, and that eij is the fraction5 of edges in the network that connect nodes in cluster i to those in cluster j. The term eii denotes the fraction of edges wit</context>
</contexts>
<marker>Newman, 2004</marker>
<rawString>M. Newman. 2004. Fast algorithm for detecting community structure in networks. Phys. Rev. E, 69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Palla</author>
<author>I Derenyi</author>
<author>I Farkas</author>
<author>T Vicsek</author>
</authors>
<title>Uncovering the overlapping community structure of complex networks in nature and society.</title>
<date>2005</date>
<journal>Nature,</journal>
<pages>435--814</pages>
<contexts>
<context position="9478" citStr="Palla et al., 2005" startWordPosition="1479" endWordPosition="1482">00 print 4780000 — 183 4800000 8390000 86400 1390000 InterLaser 179 183 116 65 0 0 ink 4720000 4800000 116 — 10600000 144000 656000 TV 4530000 8390000 65 10600000 — 1660000 42300000 Aquos 201000 86400 0 144000 1660000 — 1790000 Sharp 990000 1390000 0 656000 42300000 1790000 — structures of various networks are investigated in detail. For example, Motter (2002) targeted a conceptual network from a thesaurus and demonstrated its small-world structure. Recently, numerous works have identified communities (or densely-connected subgraphs) from large networks (Newman, 2004; Girvan and Newman, 2002; Palla et al., 2005) as explained in the next section. 3 Word Clustering using Web Counts 3.1 Co-occurrence by a Search Engine A typical word clustering task is described as follows: given a set of words (nouns), cluster words into groups so that the similar words are in the same cluster 1. Let us take an example. Assume a set of words is given: プリ,/タ (printer), 印刷 (print), 4,/ターレーザー (InterLaser), 4 ,/ク (ink), TV (TV), Aquos (Aquos), and Sharp (Sharp). Apparently, the first four words are related to a printer, and the last three words are related to a TV 2. In this case, we would like to have two word groups: the</context>
</contexts>
<marker>Palla, Derenyi, Farkas, Vicsek, 2005</marker>
<rawString>G. Palla, I. Derenyi, I. Farkas, and T. Vicsek. 2005. Uncovering the overlapping community structure of complex networks in nature and society. Nature, 435:814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proc. ACL93,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="7051" citStr="Pereira et al., 1993" startWordPosition="1099" endWordPosition="1102">lculating semantic orientation and rating reviews (Turney, 2002). As described, PMI is one of many measures to calculate the strength of word similarity or word association (Manning and Sch¨utze, 2002). An important assumption is that similarity between words is a consequence of word co-occurrence, or that the proximity of words in text is indicative of relationship between them, such as synonymy or antonymy. A commonly used technique to obtain word groups is distributional clustering (Baker and McCallum, 1998). Distributional clustering of words was first proposed by Pereira Tishby &amp; Lee in (Pereira et al., 1993): They cluster nouns according to their conditional verb distributions. Graphic representations for word similarity have also been advanced by several researchers. Kageura et al. (2000) propose automatic thesaurus generation based on a graphic representation. By applying a minimum edge cut, the corresponding English terms and Japanese terms are identified as a cluster. Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition. A graph is produced by linking pairs of words which participate in particular syntactic relationships. An incremental cluster-building algorithm ac</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of English words. In Proc. ACL93, pages 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tanaka-Ishii</author>
<author>H Iwasaki</author>
</authors>
<title>Clustering cooccurrence graph using transitivity.</title>
<date>1996</date>
<booktitle>In Proc. 16th International Conference on Computational Linguistics,</booktitle>
<pages>680--585</pages>
<contexts>
<context position="7897" citStr="Tanaka-Ishii and Iwasaki, 1996" startWordPosition="1221" endWordPosition="1224">eneration based on a graphic representation. By applying a minimum edge cut, the corresponding English terms and Japanese terms are identified as a cluster. Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition. A graph is produced by linking pairs of words which participate in particular syntactic relationships. An incremental cluster-building algorithm achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. Another study builds a co-occurrence graph of terms and decomposes it to identify relevant terms by duplicating nodes and edges (Tanaka-Ishii and Iwasaki, 1996). It focuses on transitivity: if transitivity does not hold between three nodes (e.g., if edge a-b and b-c exist but edge a-c does not), the nodes should be in separate clusters. A network of words (or named entities) on the web is investigated also in the context of the Semantic Web (Cimiano et al., 2004; Bekkerman and McCallum, 2005). Especially, a social network of persons is mined from the web using a search engine (Kautz et al., 1997; Mika, 2005; Matsuo et al., 2006). In these studies, the Jaccard coefficient is often used to measure the co-occurrence of entities. We compare Jaccard coeff</context>
</contexts>
<marker>Tanaka-Ishii, Iwasaki, 1996</marker>
<rawString>K. Tanaka-Ishii and H. Iwasaki. 1996. Clustering cooccurrence graph using transitivity. In Proc. 16th International Conference on Computational Linguistics, pages 680–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Terra</author>
<author>C Clarke</author>
</authors>
<title>Frequency estimates for statistical word similarity measures.</title>
<date>2003</date>
<booktitle>In Proc. HLT/NAACL</booktitle>
<contexts>
<context position="6213" citStr="Terra and Clarke, 2003" startWordPosition="970" endWordPosition="973">hm for recognizing synonyms by querying a web search engine. The task of recognizing synonyms is, given a target word and a set of alternative words, to choose the word that is most similar in meaning to the target word. The algorithm uses pointwise mutual information (PMI-IR) to measure the similarity of pairs of words. It is evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 from the English as a Second Language test (ESL). The algorithm obtains a score of 74%, contrasted to that of 64% by Latent Semantic Analysis (LSA). Terra and Clarke (Terra and Clarke, 2003) provide a comparative investigation of co-occurrence frequency estimation on the performance of synonym tests. They report that PMI (with a certain window size) performs best on average. Also, PMI-IR is useful for calculating semantic orientation and rating reviews (Turney, 2002). As described, PMI is one of many measures to calculate the strength of word similarity or word association (Manning and Sch¨utze, 2002). An important assumption is that similarity between words is a consequence of word co-occurrence, or that the proximity of words in text is indicative of relationship between them, </context>
<context position="19559" citStr="Terra and Clarke, 2003" startWordPosition="3273" endWordPosition="3276">er that the specific words to a category are relevant to some extent, and that they can therefore be regarded as a word group. Examples are shown in Table 5. In all, 90 word sets are obtained and merged. We call the word set DMOZ-J data. Our task is, given 90 words, to cluster the words into the correct nine groups. Here we investigate whether the correct nine words are selected for each word using the co-occurrence measure. We compare pointwise mutual information (PMI), the Jaccard coefficient (Jaccard), and chi-square (χ2). We chose these methods for comparison because PMI performs best in (Terra and Clarke, 2003). The Jaccard coefficient is often used in social network mining from the web. Table 7 shows the precision of each method. Experiments are repeated five times. We keep each method that outputs the 2. Apply the Japanese morphological analysis system ChaSen (Matsumoto et al., 2000) to the documents. Calculate the score of each word w in category c similarly to TF-IDF: score(w, c) = fc(w) × log(Nall/fall(w)) where fc denotes the document frequency of word w in category c, Nall denotes the number of all documents, and fall(w) denotes the frequency of word w in all documents. 3. For each category, </context>
<context position="24111" citStr="Terra and Clarke, 2003" startWordPosition="4018" endWordPosition="4021">e measures. The threshold is determined so that the network density dthre is 0.3. Then, we apply clustering to obtain nine clusters; nc = 9. Finally, we compare the resultant clusters with the correct categories. Clustering results for DMOZ-J sets are shown in Table 9. Newman clustering produces higher precision and recall. Especially, the combination of chi-square and Newman is the best in our experiments. 5 Discussion In this paper, the scope of co-occurrence is document-wide. One reason is that major commercial search engines do not support a type of query w1 NEAR w2. Another reason is in (Terra and Clarke, 2003) document-wide co-occurrences perform comparable to other Windows-based cooccurrences. Many types of co-occurrence exist other than noun-noun. We limit our scope to noun-noun co-occurrences in this paper. Other types of cooccurrence such as verb-noun can be investigated in future studies. Also, co-occurrence for the second-order similarity can be sought. Because web documents are sometimes difficult to analyze, we keep our algorithm as simple as possible. Analyzing semantic relations and applying distributional clustering is another goal for future work. A salient weak point of our algorithm i</context>
</contexts>
<marker>Terra, Clarke, 2003</marker>
<rawString>E. Terra and C. Clarke. 2003. Frequency estimates for statistical word similarity measures. In Proc. HLT/NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Mining the web for synonyms: PMIIR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proc. ECML-2001,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="2613" citStr="Turney (2001)" startWordPosition="390" endWordPosition="391">rdering, compound noun bracketing, countability detection, and so on (Lapata and Keller, 2004). For some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a standard large corpus; the web yields better counts than the BNC. The web is an excellent source of information on new words. Therefore, automatic thesaurus construction (Curran, 2002) offers great potential for various useful NLP applications. Several studies have addressed the extraction of hypernyms and hyponyms from the web (Miura et al., 2004; Cimiano et al., 2004). P. Turney (2001) presents a method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (PMI). For further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets. It also contributes to word sense disambiguation (Li and Abe, 1998) and text classification (Dhillon et al., 2002) because the dimensionality is reduced efficiently. This paper presents an unsupervised algorithm for word clustering based on a word similarity measure by web counts. Given a set of words, the algorithm clusters the words into groups so that the s</context>
<context position="5548" citStr="Turney, 2001" startWordPosition="856" endWordPosition="857">sing web counts as frequency estimates, building corpora through search engine queries, and crawling the web for linguistic purposes. Commercial search engines are optimized for ordinary users. Therefore, it is desirable to crawl the web and to develop specific search engines for NLP applications (Cafarella and Etzioni, 2005). However, considering that great efforts are taken in commercial search engines to maintain quality of crawling and indexing, especially against spammers, it is still important to pursue the possibility of using the current search engines for NLP applications. P. Turney (Turney, 2001) presents an unsupervised learning algorithm for recognizing synonyms by querying a web search engine. The task of recognizing synonyms is, given a target word and a set of alternative words, to choose the word that is most similar in meaning to the target word. The algorithm uses pointwise mutual information (PMI-IR) to measure the similarity of pairs of words. It is evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 from the English as a Second Language test (ESL). The algorithm obtains a score of 74%, contrasted to that of 64% by Latent S</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. Turney. 2001. Mining the web for synonyms: PMIIR versus LSA on TOEFL. In Proc. ECML-2001, pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. ACL’02,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="6494" citStr="Turney, 2002" startWordPosition="1014" endWordPosition="1015">e similarity of pairs of words. It is evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 from the English as a Second Language test (ESL). The algorithm obtains a score of 74%, contrasted to that of 64% by Latent Semantic Analysis (LSA). Terra and Clarke (Terra and Clarke, 2003) provide a comparative investigation of co-occurrence frequency estimation on the performance of synonym tests. They report that PMI (with a certain window size) performs best on average. Also, PMI-IR is useful for calculating semantic orientation and rating reviews (Turney, 2002). As described, PMI is one of many measures to calculate the strength of word similarity or word association (Manning and Sch¨utze, 2002). An important assumption is that similarity between words is a consequence of word co-occurrence, or that the proximity of words in text is indicative of relationship between them, such as synonymy or antonymy. A commonly used technique to obtain word groups is distributional clustering (Baker and McCallum, 1998). Distributional clustering of words was first proposed by Pereira Tishby &amp; Lee in (Pereira et al., 1993): They cluster nouns according to their con</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proc. ACL’02, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Word sense disambiguation by web mining for word co-occurrence probabilities.</title>
<date>2004</date>
<booktitle>In Proc. SENSEVAL-3.</booktitle>
<contexts>
<context position="4827" citStr="Turney, 2004" startWordPosition="743" endWordPosition="744">in our algorithm, they are revealed to outperform PMI and hierarchical clustering. We target Japanese words in this paper. The remainder of this paper is organized as follows: We overview the related studies in the next section. Our proposed algorithm is described in Section 3. Sections 4 and 5 explain evaluations and advance discussion. Finally, we conclude the paper. 2 Related Works A number of studies have explained the use of the web for NLP tasks e.g., creating multilingual translation lexicons (Cheng et al., 2004), text classification (Huang et al., 2004), and word sense disambiguation (Turney, 2004). M. Baroni and M. Ueyama summarize three approaches to use the web as a corpus (Baroni and Ueyama, 2005): using web counts as frequency estimates, building corpora through search engine queries, and crawling the web for linguistic purposes. Commercial search engines are optimized for ordinary users. Therefore, it is desirable to crawl the web and to develop specific search engines for NLP applications (Cafarella and Etzioni, 2005). However, considering that great efforts are taken in commercial search engines to maintain quality of crawling and indexing, especially against spammers, it is sti</context>
</contexts>
<marker>Turney, 2004</marker>
<rawString>P. Turney. 2004. Word sense disambiguation by web mining for word co-occurrence probabilities. In Proc. SENSEVAL-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>B Dorow</author>
</authors>
<title>A graph model for unsupervised lexical acquisition.</title>
<date>2002</date>
<booktitle>In Proc. COLING</booktitle>
<contexts>
<context position="7447" citStr="Widdows and Dorow (2002)" startWordPosition="1156" endWordPosition="1159">nonymy or antonymy. A commonly used technique to obtain word groups is distributional clustering (Baker and McCallum, 1998). Distributional clustering of words was first proposed by Pereira Tishby &amp; Lee in (Pereira et al., 1993): They cluster nouns according to their conditional verb distributions. Graphic representations for word similarity have also been advanced by several researchers. Kageura et al. (2000) propose automatic thesaurus generation based on a graphic representation. By applying a minimum edge cut, the corresponding English terms and Japanese terms are identified as a cluster. Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition. A graph is produced by linking pairs of words which participate in particular syntactic relationships. An incremental cluster-building algorithm achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. Another study builds a co-occurrence graph of terms and decomposes it to identify relevant terms by duplicating nodes and edges (Tanaka-Ishii and Iwasaki, 1996). It focuses on transitivity: if transitivity does not hold between three nodes (e.g., if edge a-b and b-c exist but edge a-c does not), the nodes sho</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>D. Widdows and B. Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proc. COLING 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>