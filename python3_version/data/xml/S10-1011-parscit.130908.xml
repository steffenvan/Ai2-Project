<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.85875">
SemEval-2010 Task 14: Word Sense Induction &amp; Disambiguation
</title>
<author confidence="0.987212">
Suresh Manandhar
</author>
<affiliation confidence="0.9978205">
Department of Computer Science
University of York, UK
</affiliation>
<author confidence="0.975458">
Dmitriy Dligach
</author>
<affiliation confidence="0.9999185">
Department of Computer Science
University of Colorado, USA
</affiliation>
<sectionHeader confidence="0.989353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851692307692">
This paper presents the description and
evaluation framework of SemEval-2010
Word Sense Induction &amp; Disambiguation
task, as well as the evaluation results of 26
participating systems. In this task, partici-
pants were required to induce the senses of
100 target words using a training set, and
then disambiguate unseen instances of the
same words using the induced senses. Sys-
tems’ answers were evaluated in: (1) an
unsupervised manner by using two clus-
tering evaluation measures, and (2) a su-
pervised manner in a WSD task.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999750869565218">
Word senses are more beneficial than simple word
forms for a variety of tasks including Information
Retrieval, Machine Translation and others (Pantel
and Lin, 2002). However, word senses are usually
represented as a fixed-list of definitions of a manu-
ally constructed lexical database. Several deficien-
cies are caused by this representation, e.g. lexical
databases miss main domain-specific senses (Pan-
tel and Lin, 2002), they often contain general defi-
nitions and suffer from the lack of explicit seman-
tic or contextual links between concepts (Agirre
et al., 2001). More importantly, the definitions of
hand-crafted lexical databases often do not reflect
the exact meaning of a target word in a given con-
text (V´eronis, 2004).
Unsupervised Word Sense Induction (WSI)
aims to overcome these limitations of hand-
constructed lexicons by learning the senses of a
target word directly from text without relying on
any hand-crafted resources. The primary aim of
SemEval-2010 WSI task is to allow comparison
of unsupervised word sense induction and disam-
biguation systems.
</bodyText>
<sectionHeader confidence="0.366772" genericHeader="introduction">
Ioannis P. Klapaftis
</sectionHeader>
<affiliation confidence="0.997438">
Department of Computer Science
University of York, UK
</affiliation>
<sectionHeader confidence="0.900843" genericHeader="method">
Sameer S. Pradhan
BBN Technologies
Cambridge, USA
</sectionHeader>
<bodyText confidence="0.999982625">
The target word dataset consists of 100 words,
50 nouns and 50 verbs. For each target word, par-
ticipants were provided with a training set in or-
der to learn the senses of that word. In the next
step, participating systems were asked to disam-
biguate unseen instances of the same words using
their learned senses. The answers of the systems
were then sent to organisers for evaluation.
</bodyText>
<sectionHeader confidence="0.974948" genericHeader="method">
2 Task description
</sectionHeader>
<bodyText confidence="0.995995034482759">
Figure 1 provides an overview of the task. As
can be observed, the task consisted of three
separate phases. In the first phase, train-
ing phase, participating systems were provided
with a training dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to use this training dataset to induce the senses
of the target word. No other resources were al-
lowed with the exception of NLP components for
morphology and syntax. In the second phase,
testing phase, participating systems were pro-
vided with a testing dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to tag (disambiguate) each testing instance with
the senses induced during the training phase. In
the third and final phase, the tagged test instances
were received by the organisers in order to evalu-
ate the answers of the systems in a supervised and
an unsupervised framework. Table 1 shows the to-
tal number of target word instances in the training
and testing set, as well as the average number of
senses in the gold standard.
The main difference of the SemEval-2010 as
compared to the SemEval-2007 sense induction
task is that the training and testing data are treated
separately, i.e the testing data are only used for
sense tagging, while the training data are only used
</bodyText>
<page confidence="0.994763">
63
</page>
<note confidence="0.758906">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 63–68,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.992843">
Figure 1: Training, testing and evaluation phases of SemEval-2010 Task 14
</figureCaption>
<table confidence="0.99943725">
Training set Testing set Senses (#)
All 879807 8915 3.79
Nouns 716945 5285 4.46
Verbs 162862 3630 3.12
</table>
<tableCaption confidence="0.999963">
Table 1: Training &amp; testing set details
</tableCaption>
<bodyText confidence="0.999800142857143">
for sense induction. Treating the testing data as
new unseen instances ensures a realistic evalua-
tion that allows to evaluate the clustering models
of each participating system.
The evaluation framework of SemEval-2010
WSI task considered two types of evaluation.
In the first one, unsupervised evaluation, sys-
tems’ answers were evaluated according to: (1) V-
Measure (Rosenberg and Hirschberg, 2007), and
(2) paired F-Score (Artiles et al., 2009). Nei-
ther of these measures were used in the SemEval-
2007 WSI task. Manandhar &amp; Klapaftis (2009)
provide more details on the choice of this evalu-
ation setting and its differences with the previous
evaluation. The second type of evaluation, super-
vised evaluation, follows the supervised evalua-
tion of the SemEval-2007 WSI task (Agirre and
Soroa, 2007). In this evaluation, induced senses
are mapped to gold standard senses using a map-
ping corpus, and systems are then evaluated in a
standard WSD task.
</bodyText>
<subsectionHeader confidence="0.998664">
2.1 Training dataset
</subsectionHeader>
<bodyText confidence="0.99988575">
The target word dataset consisted of 100 words,
i.e. 50 nouns and 50 verbs. The training dataset
for each target noun or verb was created by follow-
ing a web-based semi-automatic method, similar
to the method for the construction of Topic Signa-
tures (Agirre et al., 2001). Specifically, for each
WordNet (Fellbaum, 1998) sense of a target word,
we created a query of the following form:
</bodyText>
<subsectionHeader confidence="0.612759">
&lt;Target Word&gt; AND &lt;Relative Set&gt;
</subsectionHeader>
<bodyText confidence="0.957537666666667">
The &lt;Target Word&gt; consisted of the target
word stem. The &lt;Relative Set&gt; consisted of a
disjunctive set of word lemmas that were related
</bodyText>
<table confidence="0.9976595">
Word Query
Sense
Sense 1 failure AND (loss OR nonconformity OR test
OR surrender OR ”force play” OR ...)
Sense 2 failure AND (ruination OR flop OR bust
OR stall OR ruin OR walloping OR ...)
</table>
<tableCaption confidence="0.906243">
Table 2: Training set creation: example queries for
target word failure
</tableCaption>
<bodyText confidence="0.999514944444445">
to the target word sense for which the query was
created. The relations considered were WordNet’s
hypernyms, hyponyms, synonyms, meronyms and
holonyms. Each query was manually checked by
one of the organisers to remove ambiguous words.
The following example shows the query created
for the first1 and second2 WordNet sense of the
target noun failure.
The created queries were issued to Yahoo!
search API3 and for each query a maximum of
1000 pages were downloaded. For each page we
extracted fragments of text that occurred in &lt;p&gt;
&lt;/p&gt; html tags and contained the target word
stem. In the final stage, each extracted fragment of
text was POS-tagged using the Genia tagger (Tsu-
ruoka and Tsujii, 2005) and was only retained, if
the POS of the target word in the extracted text
matched the POS of the target word in our dataset.
</bodyText>
<subsectionHeader confidence="0.999945">
2.2 Testing dataset
</subsectionHeader>
<bodyText confidence="0.999822">
The testing dataset consisted of instances of the
same target words from the training dataset. This
dataset is part of OntoNotes (Hovy et al., 2006).
We used the sense-tagged dataset in which sen-
tences containing target word instances are tagged
with OntoNotes (Hovy et al., 2006) senses. The
texts come from various news sources including
CNN, ABC and others.
</bodyText>
<footnote confidence="0.998949">
1An act that fails
2An event that does not accomplish its intended purpose
3http://developer.yahoo.com/search/ [Access:10/04/2010]
</footnote>
<page confidence="0.997403">
64
</page>
<table confidence="0.9871218">
G1 G2 G3
C1 10 10 15
C2 20 50 0
C3 1 10 60
C4 5 0 0
</table>
<tableCaption confidence="0.999238">
Table 3: Clusters &amp; GS senses matrix.
</tableCaption>
<sectionHeader confidence="0.987255" genericHeader="method">
3 Evaluation framework
</sectionHeader>
<bodyText confidence="0.999959166666667">
For the purposes of this section we provide an ex-
ample (Table 3) in which a target word has 181
instances and 3 GS senses. A system has gener-
ated a clustering solution with 4 clusters covering
all instances. Table 3 shows the number of com-
mon instances between clusters and GS senses.
</bodyText>
<subsectionHeader confidence="0.999726">
3.1 Unsupervised evaluation
</subsectionHeader>
<bodyText confidence="0.9999445">
This section presents the measures of unsuper-
vised evaluation, i.e V-Measure (Rosenberg and
Hirschberg, 2007) and (2) paired F-Score (Artiles
et al., 2009).
</bodyText>
<subsectionHeader confidence="0.510403">
3.1.1 V-Measure evaluation
</subsectionHeader>
<bodyText confidence="0.986764565217391">
Let w be a target word with N instances (data
points) in the testing dataset. Let K = {Cj|j =
1... n} be a set of automatically generated clus-
ters grouping these instances, and S = {Gi|i =
1... m} the set of gold standard classes contain-
ing the desirable groupings of w instances.
V-Measure (Rosenberg and Hirschberg, 2007)
assesses the quality of a clustering solution by ex-
plicitly measuring its homogeneity and its com-
pleteness. Homogeneity refers to the degree that
each cluster consists of data points primarily be-
longing to a single GS class, while completeness
refers to the degree that each GS class consists of
data points primarily assigned to a single cluster
(Rosenberg and Hirschberg, 2007). Let h be ho-
mogeneity and c completeness. V-Measure is the
harmonic mean of h and c, i.e. V M = 2·h·c
h+c .
Homogeneity. The homogeneity, h, of a clus-
tering solution is defined in Formula 1, where
H(S|K) is the conditional entropy of the class
distribution given the proposed clustering and
H(S) is the class entropy.
</bodyText>
<equation confidence="0.982790428571429">
�
1, if H(S) = 0
h = (1)
1 − H(S|K)
H(S) , otherwise
..|K|
jN aij log �jN aij (2)
</equation>
<bodyText confidence="0.991445090909091">
When H(S|K) is 0, the solution is perfectly
homogeneous, because each cluster only contains
data points that belong to a single class. How-
ever in an imperfect situation, H(S|K) depends
on the size of the dataset and the distribution of
class sizes. Hence, instead of taking the raw con-
ditional entropy, V-Measure normalises it by the
maximum reduction in entropy the clustering in-
formation could provide, i.e. H(S). When there
is only a single class (H(S) = 0), any clustering
would produce a perfectly homogeneous solution.
Completeness. Symmetrically to homogeneity,
the completeness, c, of a clustering solution is de-
fined in Formula 4, where H(K|S) is the condi-
tional entropy of the cluster distribution given the
class distribution and H(K) is the clustering en-
tropy. When H(K|S) is 0, the solution is perfectly
complete, because all data points of a class belong
to the same cluster.
For the clustering example in Table 3, homo-
geneity is equal to 0.404, completeness is equal to
0.37 and V-Measure is equal to 0.386.
</bodyText>
<equation confidence="0.989865222222222">
�
1, if H(K) = 0
c =(4)
1 − H(K|S)
H(K) , otherwise
�|S |�|S|
i=1 aij i=1 aij
N log (5)
N
</equation>
<subsubsectionHeader confidence="0.581704">
3.1.2 Paired F-Score evaluation
</subsubsectionHeader>
<bodyText confidence="0.941014888888889">
In this evaluation, the clustering problem is trans-
formed into a classification problem. For each
cluster Ci we generate (|Ci|� instance pairs, where
2
|Ci |is the total number of instances that belong to
cluster Ci. Similarly, for each GS class Gi we gen-
erate (|Gi|� instance pairs, where |Gi |is the total
2
number of instances that belong to GS class Gi.
Let F(K) be the set of instance pairs that ex-
ist in the automatically induced clusters and F(S)
be the set of instance pairs that exist in the gold
standard. Precision can be defined as the number
of common instance pairs between the two sets to
the total number of pairs in the clustering solu-
tion (Equation 7), while recall can be defined as
the number of common instance pairs between the
two sets to the total number of pairs in the gold
</bodyText>
<equation confidence="0.995085666666667">
H(S) = − � |S|
i=1
H(S|K) = − � |K |� |S |aij aij
j=1 i=1 N log (3)
|S|
&apos;k=1 akj
H(K) = − � |K|
j=1
H(K|S) = − � |S |� |K |aij aij
i=1 j=1 N log (6)
K
&apos;k=1 aik
</equation>
<page confidence="0.981302">
65
</page>
<bodyText confidence="0.993151">
standard (Equation 8). Finally, precision and re-
call are combined to produce the harmonic mean
</bodyText>
<equation confidence="0.949826444444444">
_ 2·P·R
(FS - P+R ).
|F(K) ∩ F(S)|
(7)
|F (K)|
R = |F(K) ∩ F(S) |(8)
|F(S)|
For example in Table 3, we can generate �35� in-
2
</equation>
<bodyText confidence="0.84792125">
stance pairs for C1 , �70� for C2, �71� for C3 and
�5� for C4, resulting in a total of 5505 instance
2 2
pairs. In the same vein, we can generate �36
</bodyText>
<equation confidence="0.73065275">
2 � in-
2
stance pairs for G1, �70� for G2 and �75 � for G3. In
2 2
</equation>
<bodyText confidence="0.99794075">
total, the GS classes contain 5820 instance pairs.
There are 3435 common instance pairs, hence pre-
cision is equal to 62.39%, recall is equal to 59.09%
and paired F-Score is equal to 60.69%.
</bodyText>
<subsectionHeader confidence="0.998093">
3.2 Supervised evaluation
</subsectionHeader>
<bodyText confidence="0.99369459375">
In this evaluation, the testing dataset is split into a
mapping and an evaluation corpus. The first one
is used to map the automatically induced clusters
to GS senses, while the second is used to evaluate
methods in a WSD setting. This evaluation fol-
lows the supervised evaluation of SemEval-2007
WSI task (Agirre and Soroa, 2007), with the dif-
ference that the reported results are an average
of 5 random splits. This repeated random sam-
pling was performed to avoid the problems of the
SemEval-2007 WSI challenge, in which different
splits were providing different system rankings.
Let us consider the example in Table 3 and as-
sume that this matrix has been created by using the
mapping corpus. Table 3 shows that C1 is more
likely to be associated with G3, C2 is more likely
to be associated with G2, C3 is more likely to be
associated with G3 and C4 is more likely to be as-
sociated with G1. This information can be utilised
to map the clusters to GS senses.
Particularly, the matrix shown in Table 3 is nor-
malised to produce a matrix M, in which each
entry depicts the estimated conditional probabil-
ity P(Gi|Cj). Given an instance I of tw from
the evaluation corpus, a row cluster vector IC is
created, in which each entry k corresponds to the
score assigned to Ck to be the winning cluster for
instance I. The product of IC and M provides a
row sense vector, IG, in which the highest scor-
ing entry a denotes that Ga is the winning sense.
For example, if we produce the row cluster vector
[C1 = 0.8,C2 = 0.1, C3 = 0.1, C4=0.0],and
</bodyText>
<table confidence="0.999374033333333">
System VM (%) VM (%) VM (%) #Cl
(All) (Nouns) (Verbs)
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
KSU KDD 15.7 18 12.4 17.5
Duluth-WSI 9 11.4 5.7 4.15
Duluth-WSI-SVD 9 11.4 5.7 4.15
Duluth-R-110 8.6 8.6 8.5 9.71
Duluth-WSI-Co 7.9 9.2 6 2.49
KCDC-PCGD 7.8 7.3 8.4 2.9
KCDC-PC 7.5 7.7 7.3 2.92
KCDC-PC-2 7.1 7.7 6.1 2.93
Duluth-Mix-Narrow-Gap 6.9 8 5.1 2.42
KCDC-GD-2 6.9 6.1 8 2.82
KCDC-GD 6.9 5.9 8.5 2.78
Duluth-Mix-Narrow-PK2 6.8 7.8 5.5 2.68
Duluth-MIX-PK2 5.6 5.8 5.2 2.66
Duluth-R-15 5.3 5.4 5.1 4.97
Duluth-WSI-Co-Gap 4.8 5.6 3.6 1.6
Random 4.4 4.2 4.6 4
Duluth-R-13 3.6 3.5 3.7 3
Duluth-WSI-Gap 3.1 4.2 1.5 1.4
Duluth-Mix-Gap 3 2.9 3 1.61
Duluth-Mix-Uni-PK2 2.4 0.8 4.7 2.04
Duluth-R-12 2.3 2.2 2.5 2
KCDC-PT 1.9 1 3.1 1.5
Duluth-Mix-Uni-Gap 1.4 0.2 3 1.39
KCDC-GDC 7 6.2 7.8 2.83
MFS 0 0 0 1
Duluth-WSI-SVD-Gap 0 0 0.1 1.02
</table>
<tableCaption confidence="0.999858">
Table 4: V-Measure unsupervised evaluation
</tableCaption>
<bodyText confidence="0.99949125">
multiply it with the normalised matrix of Table 3,
then we would get a row sense vector in which G3
would be the winning sense with a score equal to
0.43.
</bodyText>
<sectionHeader confidence="0.994869" genericHeader="evaluation">
4 Evaluation results
</sectionHeader>
<bodyText confidence="0.9999808">
In this section, we present the results of the 26
systems along with two baselines. The first base-
line, Most Frequent Sense (MFS), groups all test-
ing instances of a target word into one cluster. The
second baseline, Random, randomly assigns an in-
stance to one out of four clusters. The number
of clusters of Random was chosen to be roughly
equal to the average number of senses in the GS.
This baseline is executed five times and the results
are averaged.
</bodyText>
<subsectionHeader confidence="0.996479">
4.1 Unsupervised evaluation
</subsectionHeader>
<bodyText confidence="0.994904461538462">
Table 4 shows the V-Measure (VM) performance
of the 26 systems participating in the task. The last
column shows the number of induced clusters of
each system in the test set.The MFS baseline has a
V-Measure equal to 0, since by definition its com-
pleteness is 1 and homogeneity is 0. All systems
outperform this baseline, apart from one, whose
V-Measure is equal to 0. Regarding the Random
baseline, we observe that 17 perform better, which
indicates that they have learned useful information
better than chance.
Table 4 also shows that V-Measure tends to
favour systems producing a higher number of clus-
</bodyText>
<equation confidence="0.366392">
P=
</equation>
<page confidence="0.890876">
66
</page>
<table confidence="0.9997024">
System FS (%) FS (%) FS (%) #Cl
(All) (Nouns) (Verbs)
MFS 63.5 57.0 72.7 1
Duluth-WSI-SVD-Gap 63.3 57.0 72.4 1.02
KCDC-PT 61.8 56.4 69.7 1.5
KCDC-GD 59.2 51.6 70.0 2.78
Duluth-Mix-Gap 59.1 54.5 65.8 1.61
Duluth-Mix-Uni-Gap 58.7 57.0 61.2 1.39
KCDC-GD-2 58.2 50.4 69.3 2.82
KCDC-GDC 57.3 48.5 70.0 2.83
Duluth-Mix-Uni-PK2 56.6 57.1 55.9 2.04
KCDC-PC 55.5 50.4 62.9 2.92
KCDC-PC-2 54.7 49.7 61.7 2.93
Duluth-WSI-Gap 53.7 53.4 53.9 1.4
KCDC-PCGD 53.3 44.8 65.6 2.9
Duluth-WSI-Co-Gap 52.6 53.3 51.5 1.6
Duluth-MIX-PK2 50.4 51.7 48.3 2.66
UoY 49.8 38.2 66.6 11.54
Duluth-Mix-Narrow-Gap 49.7 47.4 51.3 2.42
Duluth-WSI-Co 49.5 50.2 48.2 2.49
Duluth-Mix-Narrow-PK2 47.8 37.1 48.2 2.68
Duluth-R-12 47.8 44.3 52.6 2
Duluth-WSI-SVD 41.1 37.1 46.7 4.15
Duluth-WSI 41.1 37.1 46.7 4.15
Duluth-R-13 38.4 36.2 41.5 3
KSU KDD 36.9 24.6 54.7 17.5
Random 31.9 30.4 34.1 4
Duluth-R-15 27.6 26.7 28.9 4.97
Hermit 26.7 24.4 30.1 10.78
Duluth-R-110 16.1 15.8 16.4 9.71
</table>
<tableCaption confidence="0.999592">
Table 5: Paired F-Score unsupervised evaluation
</tableCaption>
<bodyText confidence="0.999946857142857">
ters than the number of GS senses, although V-
Measure does not increase monotonically with the
number of clusters increasing. For that reason,
we introduced the second unsupervised evaluation
measure (paired F-Score) that penalises systems
when they produce: (1) a higher number of clus-
ters (low recall) or (2) a lower number of clusters
(low precision), than the GS number of senses.
Table 5 shows the performance of systems us-
ing the second unsupervised evaluation measure.
In this evaluation, we observe that most of the sys-
tems perform better than Random. Despite that,
none of the systems outperform the MFS baseline.
It seems that systems generating a smaller number
of clusters than the GS number of senses are bi-
ased towards the MFS, hence they are not able to
perform better. On the other hand, systems gen-
erating a higher number of clusters are penalised
by this measure. Systems generating a number of
clusters roughly the same as the GS tend to con-
flate the GS senses lot more than the MFS.
</bodyText>
<subsectionHeader confidence="0.997654">
4.2 Supervised evaluation results
</subsectionHeader>
<bodyText confidence="0.998778375">
Table 6 shows the results of this evaluation for a
80-20 test set split, i.e. 80% for mapping and 20%
for evaluation. The last columns shows the aver-
age number of GS senses identified by each sys-
tem in the five splits of the evaluation datasets.
Overall, 14 systems outperform the MFS, while 17
of them perform better than Random. The ranking
of systems in nouns and verbs is different. For in-
</bodyText>
<table confidence="0.999810866666667">
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.4 59.4 66.8 1.51
Duluth-WSI 60.5 54.7 68.9 1.66
Duluth-WSI-SVD 60.5 54.7 68.9 1.66
Duluth-WSI-Co-Gap 60.3 54.1 68.6 1.19
Duluth-WSI-Co 60.8 54.7 67.6 1.51
Duluth-WSI-Gap 59.8 54.4 67.8 1.11
KCDC-PC-2 59.8 54.1 68.0 1.21
KCDC-PC 59.7 54.6 67.3 1.39
KCDC-PCGD 59.5 53.3 68.6 1.47
KCDC-GDC 59.1 53.4 67.4 1.34
KCDC-GD 59.0 53.0 67.9 1.33
KCDC-PT 58.9 53.1 67.4 1.08
KCDC-GD-2 58.7 52.8 67.4 1.33
Duluth-WSI-SVD-Gap 58.7 53.2 66.7 1.01
MFS 58.7 53.2 66.6 1
Duluth-R-12 58.5 53.1 66.4 1.25
Hermit 58.3 53.6 65.3 2.06
Duluth-R-13 58.0 52.3 66.4 1.46
Random 57.3 51.5 65.7 1.53
Duluth-R-15 56.8 50.9 65.3 1.61
Duluth-Mix-Narrow-Gap 56.6 48.1 69.1 1.43
Duluth-Mix-Narrow-PK2 56.1 47.5 68.7 1.41
Duluth-R-110 54.8 48.3 64.2 1.94
KSU KDD 52.2 46.6 60.3 1.69
Duluth-MIX-PK2 51.6 41.1 67.0 1.23
Duluth-Mix-Gap 50.6 40.0 66.0 1.01
Duluth-Mix-Uni-PK2 19.3 1.8 44.8 0.62
Duluth-Mix-Uni-Gap 18.7 1.6 43.8 0.56
</table>
<tableCaption confidence="0.9896895">
Table 6: Supervised recall (SR) (test set split:80%
mapping, 20% evaluation)
</tableCaption>
<bodyText confidence="0.999510066666667">
stance, the highest ranked system in nouns is UoY,
while in verbs Duluth-Mix-Narrow-Gap. It seems
that depending on the part-of-speech of the target
word, different algorithms, features and parame-
ters’ tuning have different impact.
The supervised evaluation changes the distri-
bution of clusters by mapping each cluster to a
weighted vector of senses. Hence, it can poten-
tially favour systems generating a high number of
homogeneous clusters. For that reason, we applied
a second testing set split, where 60% of the testing
corpus was used for mapping and 40% for eval-
uation. Reducing the size of the mapping corpus
allows us to observe, whether the above statement
is correct, since systems with a high number of
clusters would suffer from unreliable mapping.
Table 7 shows the results of the second super-
vised evaluation. The ranking of participants did
not change significantly, i.e. we observe only dif-
ferent rankings among systems belonging to the
same participant. Despite that, Table 7 also shows
that the reduction of the mapping corpus has a dif-
ferent impact on systems generating a larger num-
ber of clusters than the GS number of senses.
For instance, UoY that generates 11.54 clusters
outperformed the MFS by 3.77% in the 80-20 split
and by 3.71% in the 60-40 split. The reduction of
the mapping corpus had a minimal impact on its
performance. In contrast, KSU KDD that gener-
ates 17.5 clusters was below the MFS by 6.49%
</bodyText>
<page confidence="0.99874">
67
</page>
<table confidence="0.999824933333333">
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.0 58.6 66.8 1.66
Duluth-WSI-Co 60.1 54.6 68.1 1.56
Duluth-WSI-Co-Gap 59.5 53.5 68.3 1.2
Duluth-WSI-SVD 59.5 53.5 68.3 1.73
Duluth-WSI 59.5 53.5 68.3 1.73
Duluth-WSI-Gap 59.3 53.2 68.2 1.11
KCDC-PCGD 59.1 52.6 68.6 1.54
KCDC-PC-2 58.9 53.4 67.0 1.25
KCDC-PC 58.9 53.6 66.6 1.44
KCDC-GDC 58.3 52.1 67.3 1.41
KCDC-GD 58.3 51.9 67.6 1.42
MFS 58.3 52.5 66.7 1
KCDC-PT 58.3 52.2 67.1 1.11
Duluth-WSI-SVD-Gap 58.2 52.5 66.7 1.01
KCDC-GD-2 57.9 51.7 67.0 1.44
Duluth-R-12 57.7 51.7 66.4 1.27
Duluth-R-13 57.6 51.1 67.0 1.48
Hermit 57.3 52.5 64.2 2.27
Duluth-R-15 56.5 50.0 66.1 1.76
Random 56.5 50.2 65.7 1.65
Duluth-Mix-Narrow-Gap 56.2 47.7 68.6 1.51
Duluth-Mix-Narrow-PK2 55.7 46.9 68.5 1.51
Duluth-R-110 53.6 46.7 63.6 2.18
Duluth-MIX-PK2 50.5 39.7 66.1 1.31
KSU KDD 50.4 44.3 59.4 1.92
Duluth-Mix-Gap 49.8 38.9 65.6 1.04
Duluth-Mix-Uni-PK2 19.1 1.8 44.4 0.63
Duluth-Mix-Uni-Gap 18.9 1.5 44.2 0.56
</table>
<tableCaption confidence="0.9882575">
Table 7: Supervised recall (SR) (test set split:60%
mapping, 40% evaluation)
</tableCaption>
<bodyText confidence="0.999949909090909">
in the 80-20 split and by 7.83% in the 60-40 split.
The reduction of the mapping corpus had a larger
impact in this case. This result indicates that the
performance in this evaluation also depends on the
distribution of instances within the clusters. Sys-
tems generating a skewed distribution, in which a
small number of homogeneous clusters tag the ma-
jority of instances and a larger number of clusters
tag only a few instances, are likely to have a bet-
ter performance than systems that produce a more
uniform distribution.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985230769231">
We presented the description, evaluation frame-
work and assessment of systems participating in
the SemEval-2010 sense induction task. The eval-
uation has shown that the current state-of-the-art
lacks unbiased measures that objectively evaluate
clustering.
The results of systems have shown that their
performance in the unsupervised and supervised
evaluation settings depends on cluster granularity
along with the distribution of instances within the
clusters. Our future work will focus on the assess-
ment of sense induction on a task-oriented basis as
well as on clustering evaluation.
</bodyText>
<sectionHeader confidence="0.999404" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.529391875">
We gratefully acknowledge the support of the EU
FP7 INDECT project, Grant No. 218086, the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team.
</bodyText>
<sectionHeader confidence="0.98175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99983711627907">
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating Word Sense Induction and Dis-
crimination Systems. In Proceedings of SemEval-
2007, pages 7–12, Prague, Czech Republic. ACL.
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching Wordnet Concepts With
Topic Signatures. ArXiv Computer Science e-prints.
Javier Artiles, Enrique Amig´o, and Julio Gonzalo.
2009. The role of named entities in web people
search. In Proceedings of EMNLP, pages 534–542.
ACL.
Christiane Fellbaum. 1998. Wordnet: An Electronic
Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL, Com-
panion Volume: Short Papers on XX, pages 57–60.
ACL.
Suresh Manandhar and Ioannis P. Klapaftis. 2009.
Semeval-2010 Task 14: Evaluation Setting for Word
Sense Induction &amp; Disambiguation Systems. In
DEW ’09: Proceedings of the Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions, pages 117–122, Boulder, Colorado,
USA. ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text. In KDD ’02: Proceedings
of the 8th ACM SIGKDD Conference, pages 613–
619, New York, NY, USA. ACM.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A Conditional Entropy-based External
Cluster Evaluation Measure. In Proceedings of the
2007 EMNLP-CoNLL Joint Conference, pages 410–
420, Prague, Czech Republic.
Yoshimasa Tsuruoka and Jun´ıchi Tsujii. 2005. Bidi-
rectional Inference With the Easiest-first Strategy
for Tagging Sequence Data. In Proceedings of
the HLT-EMNLP Joint Conference, pages 467–474,
Morristown, NJ, USA.
Jean V´eronis. 2004. Hyperlex: Lexical Cartography
for Information Retrieval. Computer Speech &amp; Lan-
guage, 18(3):223–252.
</reference>
<page confidence="0.999446">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.648784">
<title confidence="0.992041">SemEval-2010 Task 14: Word Sense Induction &amp; Disambiguation</title>
<author confidence="0.984004">Suresh Manandhar</author>
<affiliation confidence="0.9515644">Department of Computer Science University of York, UK Dmitriy Dligach Department of Computer Science University of Colorado, USA</affiliation>
<abstract confidence="0.986533571428572">This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction &amp; Disambiguation task, as well as the evaluation results of 26 participating systems. In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses. Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval2007,</booktitle>
<pages>7--12</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4905" citStr="Agirre and Soroa, 2007" startWordPosition="775" endWordPosition="778">ystem. The evaluation framework of SemEval-2010 WSI task considered two types of evaluation. In the first one, unsupervised evaluation, systems’ answers were evaluated according to: (1) VMeasure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al., 2009). Neither of these measures were used in the SemEval2007 WSI task. Manandhar &amp; Klapaftis (2009) provide more details on the choice of this evaluation setting and its differences with the previous evaluation. The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). In this evaluation, induced senses are mapped to gold standard senses using a mapping corpus, and systems are then evaluated in a standard WSD task. 2.1 Training dataset The target word dataset consisted of 100 words, i.e. 50 nouns and 50 verbs. The training dataset for each target noun or verb was created by following a web-based semi-automatic method, similar to the method for the construction of Topic Signatures (Agirre et al., 2001). Specifically, for each WordNet (Fellbaum, 1998) sense of a target word, we created a query of the following form: &lt;Target Word&gt; AND &lt;Relative Set&gt; The &lt;Targ</context>
<context position="12039" citStr="Agirre and Soroa, 2007" startWordPosition="2055" endWordPosition="2058">nerate �36 2 � in2 stance pairs for G1, �70� for G2 and �75 � for G3. In 2 2 total, the GS classes contain 5820 instance pairs. There are 3435 common instance pairs, hence precision is equal to 62.39%, recall is equal to 59.09% and paired F-Score is equal to 60.69%. 3.2 Supervised evaluation In this evaluation, the testing dataset is split into a mapping and an evaluation corpus. The first one is used to map the automatically induced clusters to GS senses, while the second is used to evaluate methods in a WSD setting. This evaluation follows the supervised evaluation of SemEval-2007 WSI task (Agirre and Soroa, 2007), with the difference that the reported results are an average of 5 random splits. This repeated random sampling was performed to avoid the problems of the SemEval-2007 WSI challenge, in which different splits were providing different system rankings. Let us consider the example in Table 3 and assume that this matrix has been created by using the mapping corpus. Table 3 shows that C1 is more likely to be associated with G3, C2 is more likely to be associated with G2, C3 is more likely to be associated with G3 and C4 is more likely to be associated with G1. This information can be utilised to m</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems. In Proceedings of SemEval2007, pages 7–12, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>David Martinez</author>
<author>Eduard Hovy</author>
</authors>
<title>Enriching Wordnet Concepts With Topic Signatures. ArXiv Computer Science e-prints.</title>
<date>2001</date>
<contexts>
<context position="1317" citStr="Agirre et al., 2001" startWordPosition="199" endWordPosition="202"> (2) a supervised manner in a WSD task. 1 Introduction Word senses are more beneficial than simple word forms for a variety of tasks including Information Retrieval, Machine Translation and others (Pantel and Lin, 2002). However, word senses are usually represented as a fixed-list of definitions of a manually constructed lexical database. Several deficiencies are caused by this representation, e.g. lexical databases miss main domain-specific senses (Pantel and Lin, 2002), they often contain general definitions and suffer from the lack of explicit semantic or contextual links between concepts (Agirre et al., 2001). More importantly, the definitions of hand-crafted lexical databases often do not reflect the exact meaning of a target word in a given context (V´eronis, 2004). Unsupervised Word Sense Induction (WSI) aims to overcome these limitations of handconstructed lexicons by learning the senses of a target word directly from text without relying on any hand-crafted resources. The primary aim of SemEval-2010 WSI task is to allow comparison of unsupervised word sense induction and disambiguation systems. Ioannis P. Klapaftis Department of Computer Science University of York, UK Sameer S. Pradhan BBN Te</context>
<context position="5347" citStr="Agirre et al., 2001" startWordPosition="851" endWordPosition="854">rences with the previous evaluation. The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). In this evaluation, induced senses are mapped to gold standard senses using a mapping corpus, and systems are then evaluated in a standard WSD task. 2.1 Training dataset The target word dataset consisted of 100 words, i.e. 50 nouns and 50 verbs. The training dataset for each target noun or verb was created by following a web-based semi-automatic method, similar to the method for the construction of Topic Signatures (Agirre et al., 2001). Specifically, for each WordNet (Fellbaum, 1998) sense of a target word, we created a query of the following form: &lt;Target Word&gt; AND &lt;Relative Set&gt; The &lt;Target Word&gt; consisted of the target word stem. The &lt;Relative Set&gt; consisted of a disjunctive set of word lemmas that were related Word Query Sense Sense 1 failure AND (loss OR nonconformity OR test OR surrender OR ”force play” OR ...) Sense 2 failure AND (ruination OR flop OR bust OR stall OR ruin OR walloping OR ...) Table 2: Training set creation: example queries for target word failure to the target word sense for which the query was crea</context>
</contexts>
<marker>Agirre, Ansa, Martinez, Hovy, 2001</marker>
<rawString>Eneko Agirre, Olatz Ansa, David Martinez, and Eduard Hovy. 2001. Enriching Wordnet Concepts With Topic Signatures. ArXiv Computer Science e-prints.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
</authors>
<title>The role of named entities in web people search.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>534--542</pages>
<publisher>ACL.</publisher>
<marker>Artiles, Amig´o, Gonzalo, 2009</marker>
<rawString>Javier Artiles, Enrique Amig´o, and Julio Gonzalo. 2009. The role of named entities in web people search. In Proceedings of EMNLP, pages 534–542. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>Wordnet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="5396" citStr="Fellbaum, 1998" startWordPosition="859" endWordPosition="860">f evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). In this evaluation, induced senses are mapped to gold standard senses using a mapping corpus, and systems are then evaluated in a standard WSD task. 2.1 Training dataset The target word dataset consisted of 100 words, i.e. 50 nouns and 50 verbs. The training dataset for each target noun or verb was created by following a web-based semi-automatic method, similar to the method for the construction of Topic Signatures (Agirre et al., 2001). Specifically, for each WordNet (Fellbaum, 1998) sense of a target word, we created a query of the following form: &lt;Target Word&gt; AND &lt;Relative Set&gt; The &lt;Target Word&gt; consisted of the target word stem. The &lt;Relative Set&gt; consisted of a disjunctive set of word lemmas that were related Word Query Sense Sense 1 failure AND (loss OR nonconformity OR test OR surrender OR ”force play” OR ...) Sense 2 failure AND (ruination OR flop OR bust OR stall OR ruin OR walloping OR ...) Table 2: Training set creation: example queries for target word failure to the target word sense for which the query was created. The relations considered were WordNet’s hype</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. Wordnet: An Electronic Lexical Database. MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL, Companion Volume: Short Papers on XX,</booktitle>
<pages>57--60</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6888" citStr="Hovy et al., 2006" startWordPosition="1114" endWordPosition="1117">issued to Yahoo! search API3 and for each query a maximum of 1000 pages were downloaded. For each page we extracted fragments of text that occurred in &lt;p&gt; &lt;/p&gt; html tags and contained the target word stem. In the final stage, each extracted fragment of text was POS-tagged using the Genia tagger (Tsuruoka and Tsujii, 2005) and was only retained, if the POS of the target word in the extracted text matched the POS of the target word in our dataset. 2.2 Testing dataset The testing dataset consisted of instances of the same target words from the training dataset. This dataset is part of OntoNotes (Hovy et al., 2006). We used the sense-tagged dataset in which sentences containing target word instances are tagged with OntoNotes (Hovy et al., 2006) senses. The texts come from various news sources including CNN, ABC and others. 1An act that fails 2An event that does not accomplish its intended purpose 3http://developer.yahoo.com/search/ [Access:10/04/2010] 64 G1 G2 G3 C1 10 10 15 C2 20 50 0 C3 1 10 60 C4 5 0 0 Table 3: Clusters &amp; GS senses matrix. 3 Evaluation framework For the purposes of this section we provide an example (Table 3) in which a target word has 181 instances and 3 GS senses. A system has gene</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In Proceedings of NAACL, Companion Volume: Short Papers on XX, pages 57–60. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis P Klapaftis</author>
</authors>
<title>Semeval-2010 Task 14: Evaluation Setting for Word Sense Induction &amp; Disambiguation Systems.</title>
<date>2009</date>
<booktitle>In DEW ’09: Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>117--122</pages>
<publisher>ACL.</publisher>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="4651" citStr="Manandhar &amp; Klapaftis (2009)" startWordPosition="735" endWordPosition="738">9 Nouns 716945 5285 4.46 Verbs 162862 3630 3.12 Table 1: Training &amp; testing set details for sense induction. Treating the testing data as new unseen instances ensures a realistic evaluation that allows to evaluate the clustering models of each participating system. The evaluation framework of SemEval-2010 WSI task considered two types of evaluation. In the first one, unsupervised evaluation, systems’ answers were evaluated according to: (1) VMeasure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al., 2009). Neither of these measures were used in the SemEval2007 WSI task. Manandhar &amp; Klapaftis (2009) provide more details on the choice of this evaluation setting and its differences with the previous evaluation. The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). In this evaluation, induced senses are mapped to gold standard senses using a mapping corpus, and systems are then evaluated in a standard WSD task. 2.1 Training dataset The target word dataset consisted of 100 words, i.e. 50 nouns and 50 verbs. The training dataset for each target noun or verb was created by following a web-based semi-automa</context>
</contexts>
<marker>Manandhar, Klapaftis, 2009</marker>
<rawString>Suresh Manandhar and Ioannis P. Klapaftis. 2009. Semeval-2010 Task 14: Evaluation Setting for Word Sense Induction &amp; Disambiguation Systems. In DEW ’09: Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 117–122, Boulder, Colorado, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>In KDD ’02: Proceedings of the 8th ACM SIGKDD Conference,</booktitle>
<pages>613--619</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="916" citStr="Pantel and Lin, 2002" startWordPosition="137" endWordPosition="140">d Sense Induction &amp; Disambiguation task, as well as the evaluation results of 26 participating systems. In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses. Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task. 1 Introduction Word senses are more beneficial than simple word forms for a variety of tasks including Information Retrieval, Machine Translation and others (Pantel and Lin, 2002). However, word senses are usually represented as a fixed-list of definitions of a manually constructed lexical database. Several deficiencies are caused by this representation, e.g. lexical databases miss main domain-specific senses (Pantel and Lin, 2002), they often contain general definitions and suffer from the lack of explicit semantic or contextual links between concepts (Agirre et al., 2001). More importantly, the definitions of hand-crafted lexical databases often do not reflect the exact meaning of a target word in a given context (V´eronis, 2004). Unsupervised Word Sense Induction (W</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering Word Senses from Text. In KDD ’02: Proceedings of the 8th ACM SIGKDD Conference, pages 613– 619, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A Conditional Entropy-based External Cluster Evaluation Measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 EMNLP-CoNLL Joint Conference,</booktitle>
<pages>410--420</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4509" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="710" endWordPosition="713">nal Linguistics Figure 1: Training, testing and evaluation phases of SemEval-2010 Task 14 Training set Testing set Senses (#) All 879807 8915 3.79 Nouns 716945 5285 4.46 Verbs 162862 3630 3.12 Table 1: Training &amp; testing set details for sense induction. Treating the testing data as new unseen instances ensures a realistic evaluation that allows to evaluate the clustering models of each participating system. The evaluation framework of SemEval-2010 WSI task considered two types of evaluation. In the first one, unsupervised evaluation, systems’ answers were evaluated according to: (1) VMeasure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al., 2009). Neither of these measures were used in the SemEval2007 WSI task. Manandhar &amp; Klapaftis (2009) provide more details on the choice of this evaluation setting and its differences with the previous evaluation. The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). In this evaluation, induced senses are mapped to gold standard senses using a mapping corpus, and systems are then evaluated in a standard WSD task. 2.1 Training dataset The target word dataset consiste</context>
<context position="7770" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="1264" endWordPosition="1267"> not accomplish its intended purpose 3http://developer.yahoo.com/search/ [Access:10/04/2010] 64 G1 G2 G3 C1 10 10 15 C2 20 50 0 C3 1 10 60 C4 5 0 0 Table 3: Clusters &amp; GS senses matrix. 3 Evaluation framework For the purposes of this section we provide an example (Table 3) in which a target word has 181 instances and 3 GS senses. A system has generated a clustering solution with 4 clusters covering all instances. Table 3 shows the number of common instances between clusters and GS senses. 3.1 Unsupervised evaluation This section presents the measures of unsupervised evaluation, i.e V-Measure (Rosenberg and Hirschberg, 2007) and (2) paired F-Score (Artiles et al., 2009). 3.1.1 V-Measure evaluation Let w be a target word with N instances (data points) in the testing dataset. Let K = {Cj|j = 1... n} be a set of automatically generated clusters grouping these instances, and S = {Gi|i = 1... m} the set of gold standard classes containing the desirable groupings of w instances. V-Measure (Rosenberg and Hirschberg, 2007) assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness. Homogeneity refers to the degree that each cluster consists of data points primarily belongin</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A Conditional Entropy-based External Cluster Evaluation Measure. In Proceedings of the 2007 EMNLP-CoNLL Joint Conference, pages 410– 420, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun´ıchi Tsujii</author>
</authors>
<title>Bidirectional Inference With the Easiest-first Strategy for Tagging Sequence Data.</title>
<date>2005</date>
<booktitle>In Proceedings of the HLT-EMNLP Joint Conference,</booktitle>
<pages>467--474</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6593" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="1060" endWordPosition="1064">onsidered were WordNet’s hypernyms, hyponyms, synonyms, meronyms and holonyms. Each query was manually checked by one of the organisers to remove ambiguous words. The following example shows the query created for the first1 and second2 WordNet sense of the target noun failure. The created queries were issued to Yahoo! search API3 and for each query a maximum of 1000 pages were downloaded. For each page we extracted fragments of text that occurred in &lt;p&gt; &lt;/p&gt; html tags and contained the target word stem. In the final stage, each extracted fragment of text was POS-tagged using the Genia tagger (Tsuruoka and Tsujii, 2005) and was only retained, if the POS of the target word in the extracted text matched the POS of the target word in our dataset. 2.2 Testing dataset The testing dataset consisted of instances of the same target words from the training dataset. This dataset is part of OntoNotes (Hovy et al., 2006). We used the sense-tagged dataset in which sentences containing target word instances are tagged with OntoNotes (Hovy et al., 2006) senses. The texts come from various news sources including CNN, ABC and others. 1An act that fails 2An event that does not accomplish its intended purpose 3http://developer</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun´ıchi Tsujii. 2005. Bidirectional Inference With the Easiest-first Strategy for Tagging Sequence Data. In Proceedings of the HLT-EMNLP Joint Conference, pages 467–474, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>Hyperlex: Lexical Cartography for Information Retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>Jean V´eronis. 2004. Hyperlex: Lexical Cartography for Information Retrieval. Computer Speech &amp; Language, 18(3):223–252.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>