<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000797">
<title confidence="0.999131">
Translating from Morphologically Complex Languages:
A Paraphrase-Based Approach
</title>
<author confidence="0.992762">
Preslav Nakov
</author>
<affiliation confidence="0.999901">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.974823">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.998496">
nakov@comp.nus.edu.sg
</email>
<author confidence="0.985961">
Hwee Tou Ng
</author>
<affiliation confidence="0.9998895">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.974784">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.998567">
nght@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999812842105263">
We propose a novel approach to translating
from a morphologically complex language.
Unlike previous research, which has targeted
word inflections and concatenations, we fo-
cus on the pairwise relationship between mor-
phologically related words, which we treat as
potential paraphrases and handle using para-
phrasing techniques at the word, phrase, and
sentence level. An important advantage of
this framework is that it can cope with deriva-
tional morphology, which has so far remained
largely beyond the capabilities of statistical
machine translation systems. Our experiments
translating from Malay, whose morphology is
mostly derivational, into English show signif-
icant improvements over rivaling approaches
based on five automatic evaluation measures
(for 320,000 sentence pairs; 9.5 million En-
glish word tokens).
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968425531915">
Traditionally, statistical machine translation (SMT)
models have assumed that the word should be the ba-
sic token-unit of translation, thus ignoring any word-
internal morphological structure. This assumption
can be traced back to the first word-based models of
IBM (Brown et al., 1993), which were initially pro-
posed for two languages with limited morphology:
French and English. While several significantly im-
proved models have been developed since then, in-
cluding phrase-based (Koehn et al., 2003), hierarchi-
cal (Chiang, 2005), treelet (Quirk et al., 2005), and
syntactic (Galley et al., 2004) models, they all pre-
served the assumption that words should be atomic.
Ignoring morphology was fine as long as the main
research interest remained focused on languages
with limited (e.g., English, French, Spanish) or min-
imal (e.g., Chinese) morphology. Since the attention
shifted to languages like Arabic, however, the im-
portance of morphology became obvious and sev-
eral approaches to handle it have been proposed.
Depending on the particular language of interest,
researchers have paid attention to word inflections
and clitics, e.g., for Arabic, Finnish, and Turkish,
or to noun compounds, e.g., for German. However,
derivational morphology has not been specifically
targeted so far.
In this paper, we propose a paraphrase-based ap-
proach to translating from a morphologically com-
plex language. Unlike previous research, we focus
on the pairwise relationship between morphologi-
cally related wordforms, which we treat as poten-
tial paraphrases, and which we handle using para-
phrasing techniques at various levels: word, phrase,
and sentence level. An important advantage of this
framework is that it can cope with various kinds
of morphological wordforms, including derivational
ones. We demonstrate its potential on Malay, whose
morphology is mostly derivational.
The remainder of the paper is organized as fol-
lows: Section 2 gives an overview of Malay mor-
phology, Section 3 introduces our paraphrase-based
approach to translating from morphologically com-
plex languages, Section 4 describes our dataset and
our experimental setup, Section 5 presents and anal-
yses the results, and Section 6 compares our work to
previous research. Finally, Section 7 concludes the
paper and suggests directions for future work.
</bodyText>
<page confidence="0.935198">
1298
</page>
<note confidence="0.98911">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.454655" genericHeader="method">
2 Malay Morphology and SMT
</sectionHeader>
<bodyText confidence="0.985056944444445">
Malay is an Astronesian language, spoken by about
180 million people. It is of~cial in Malaysia, In-
donesia, Singapore, and Brunei, and has two major
dialects, sometimes regarded as separate languages,
which are mutually intelligible, but occasionally dif-
fer in orthography/pronunciation and vocabulary:
Bahasa Malaysia (lit. ‘language of Malaysia’) and
Bahasa Indonesia (lit. ‘language of Indonesia’).
Malay is an agglutinative language with very rich
morphology. Unlike other agglutinative languages
such as Finnish, Hungarian, and Turkish, which
are rich in both inflectional and derivational forms,
Malay morphology is mostly derivational. Inflec-
tionally,1 Malay is very similar to Chinese: there is
no grammatical gender, number, or tense, verbs are
not marked for person, etc.
In Malay, new words can be formed by the fol-
lowing three morphological processes:
</bodyText>
<listItem confidence="0.302624">
•
</listItem>
<bodyText confidence="0.943987">
Affixation, i.e., attaching af~xes, which are not
words themselves, to a word. These can be pre-
~xes (e.g., ajar/‘teach’ —* pelajar/‘student’),
suf~xes (e.g., ajar —* ajaran/‘teachings’), cir-
cum~xes (e.g., ajar —* pengajaran/‘lesson’),
and in~xes (e.g., gigi/‘teeth’ —* gerigi/‘toothed
blade’). In~xes only apply to a small number
of words and are not productive.
</bodyText>
<listItem confidence="0.996175285714286">
• Compounding, i.e., forming a new word by
putting two or more existing words together.
For example, kereta/‘car’ + api/‘~re’ make
kereta api and keretapi in Bahasa Indonesia and
Bahasa Malaysia, respectively, both meaning
‘train’. As in English, Malay compounds are
written separately, but some stable ones like
kerjasama/‘collaboration’ (from kerja/‘work’
and sama/‘same’) are concatenated. Concate-
nation is also required when a circum~x is
applied to a compound, e.g., ambil alih/‘take
over’ (ambil/‘take’ + alih/‘move’) is con-
catenated to form pengambilalihan/‘takeover’
when targeted by the circum~x peng-...-an.
</listItem>
<footnote confidence="0.869017">
1Inflection is variation in the form of a word that is oblig-
atory in some given grammatical context. For example, plays,
playing, played are all inflected forms of the verb play. It does
not yield a new word and cannot change the part of speech.
</footnote>
<listItem confidence="0.967798428571428">
• Reduplication, i.e., word repetition. In
Malay, reduplication requires using a dash. It
can be full (e.g., pelajar-pelajar/‘students’),
partial (e.g., adik-beradik/‘siblings’, from
adik/‘younger brother/sister’), and rhythmic
(e.g., gunung-ganang/‘mountains’, from the
word gunung/‘mountain’).
</listItem>
<bodyText confidence="0.999492694444444">
Malay has very little inflectional morphology, It
also has some clitics2, which are not very frequent
and are typically spelled concatenated to the preced-
ing word. For example, the politeness marker lah
can be added to the command duduk/‘sit down’ to
yield duduklah/‘please, sit down’, and the pronoun
nya can attach to kereta to form keretanya/‘his car’.
Note that clitics are not af~xes, and clitic attachment
is not a word derivation or a word inflection process.
Taken together, af~xation, compounding, redu-
plication, and clitic attachment yield a rich vari-
ety of wordforms, which cause data sparseness is-
sues. Moreover, the predominantly derivational na-
ture of Malay morphology limits the applicabil-
ity of standard techniques such as (1) removing
some/all of the source-language inflections, (2) seg-
menting af~xes from the root, and (3) clustering
words with the same target translation. For example,
ifpelajar/‘student’ is an unknown word and lemma-
tization/stemming reduces it to ajar/‘teach’, would
this enable a good translation? Similarly, would seg-
menting3 pelajar as peN+ ajar, i.e., as ‘person do-
ing the action’ + ‘teach’, make it possible to gener-
ate ‘student’ (e.g., as opposed to ‘teacher’)? Finally,
if af~xes tend to change semantics so much, how
likely are we to ~nd morphologically related word-
forms that share the same translation? Still, there
are many good reasons to believe that morphologi-
cal processing should help SMT for Malay.
Consider affixation, which can yield words with
similar semantics that can use each other’s trans-
lation options, e.g., diajar/‘be taught (intransitive)’
and diajarkan/‘be taught (transitive)’. However, this
cannot be predicted from the af~x, e.g., compare
minum/‘drink (verb)’ – minuman/‘drink (noun)’ and
makan/‘eat’ – makanan/‘food’.
</bodyText>
<footnote confidence="0.860395">
2A clitic is a morpheme that has the syntactic characteristics
of a word, but is phonologically bound to another word. For
example, ’s is a clitic in The Queen of England’s crown.
3The pre~x peN suffers a nasal replacement of the
archiphoneme N to become pel in pelajar.
</footnote>
<page confidence="0.998005">
1299
</page>
<bodyText confidence="0.999600739130435">
Looking at compounding, it is often the case that
the semantics of a compound is a specialization of
the semantics of its head, and thus the target lan-
guage translations available for the head could be us-
able to translate the whole compound, e.g., compare
kerjasama/‘collaboration’ and kerja/‘work’. Alter-
natively, it might be useful to consider a segmented
version of the compound, e.g., kerja sama.
Reduplication, among other functions, expresses
plural, e.g., pelajar-pelajar/‘students’. Note, how-
ever, that it is not used when a quantity or a num-
ber word is present, e.g., dua pelajar/‘two students’
and banyak pelajar/‘many students’. Thus, if we do
not know how to translate pelajar-pelajar, it would
be reasonable to consider the translation options for
pelajar since it could potentially contain among its
translation options the plural ‘students’.
Finally, consider clitics. In some cases, a clitic
could express a fine-grained distinction such as po-
liteness, which might not be expressible in the target
language; thus, it might be feasible to simply remove
it. In other cases, e.g., when it is a pronoun, it might
be better to segment it out as a separate word.
</bodyText>
<sectionHeader confidence="0.982305" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999993545454546">
We propose a paraphrase-based approach to Malay
morphology, where we use paraphrases at three dif-
ferent levels: word, phrase, and sentence level.
First, we transform each development/testing
Malay sentence into a word lattice, where we add
simplified word-level paraphrasing alternatives for
each morphologically complex word. In the lattice,
each alternative w′ of an original word w is assigned
the weight of Pr(w′|w), which is estimated using
pivoting over the English side of the training bi-
text. Then, we generate sentence-level paraphrases
of the training Malay sentences, in which exactly
one morphologically complex word is substituted by
a simpler alternative. Finally, we extract additional
Malay phrases from these sentences, which we use
to augment the phrase table with additional transla-
tion options to match the alternative wordforms in
the lattice. We assign each such additional phrase
p′ a probability maxi, Pr(p′|p), where p is a Malay
phrase that is found in the original training Malay
text. The probability is calculated using phrase-level
pivoting over the English side of the training bi-text.
</bodyText>
<subsectionHeader confidence="0.999773">
3.1 Morphological Analysis
</subsectionHeader>
<bodyText confidence="0.995733">
Given a Malay word, we build a list of morpholog-
ically simpler words that could be derived from it;
we also generate alternative word segmentations:
</bodyText>
<listItem confidence="0.974290923076923">
(a) words obtainable by affix stripping
e.g., pelajaran → pelajar, ajaran, ajar
(b) words that are part of a compound word
e.g., kerjasama → kerja
(c) words appearing on either side of a dash
e.g., adik-beradik → adik, beradik
(d) words without clitics
e.g., keretanya → kereta
(e) clitic-segmented word sequences
e.g., keretanya → kereta nya
(f) dash-segmented wordforms
e.g., aceh-nias → aceh -nias
(g) combinations of the above.
</listItem>
<bodyText confidence="0.999137933333334">
The list is built by reversing the basic morpho-
logical processes in Malay: (a) addresses affixation,
(b) handles compounding, (c) takes care of redu-
plication, and (d) and (e) deal with clitics. Strictly
speaking, (f) does not necessarily model a morpho-
logical process: it proposes an alternative tokeniza-
tion, but this could make morphological sense too.
Note that (g) could cause potential problems when
interacting with (f), e.g., adik-beradik would be-
come adik - beradik and then by (a) it would turn
into adik - adik, which could cause the SMT sys-
tem to generate two separate translations for the two
instances of adik. To prevent this, we forbid the
application of (f) to reduplications. Taking into ac-
count that reduplications can be partial, we only al-
</bodyText>
<equation confidence="0.9741515">
low (f) if |LCS(l,r) |&lt; 0.5 where l and r are the
mm�l r|) ,
</equation>
<bodyText confidence="0.999954444444445">
strings to the aftand to the right of the dash, re-
spectively, LC5(x, y) is the longest common char-
acter subsequence, not necessarily consecutive, of
the strings x and y, and |x |is the length of the string
x. For example, LC5(adik,beradik)=adik, and thus,
the ratio is 1 (≥ 0.5) for adik-beradik. Similarly,
LC5(gunung,ganang)=gnng, and thus, the ratio is
4/6=0.67 (≥ 0.5) for gunung-ganang. However, for
aceh-nias, it is 1/4=0.25, and thus (f) is applicable.
</bodyText>
<page confidence="0.935886">
1300
</page>
<bodyText confidence="0.999995142857143">
As an illustration, here are the wordforms we
generate for adik-beradiknya/‘his siblings’: adik,
adik-beradiknya, adik-beradik nya, adik-beradik,
beradiknya, beradik nya, adik nya, and beradik.
And for berpelajaran/‘is educated’, we build the list:
berpelajaran, pelajaran, pelajar, ajaran, and ajar.
Note that the lists do include the original word.
To generate the above wordforms, we used two
morphological analyzers: a freely available Malay
lemmatizer (Baldwin and Awab, 2006), and an in-
house re-implementation of the Indonesian stemmer
described in (Adriani et al., 2007). Note that these
tools’ objective is to return a single lemma/stem,
e.g., they would return adik for adik-beradiknya, and
ajar for berpelajaran. However, it was straightfor-
ward to modify them to also output the above in-
termediary wordforms, which the tools were gener-
ating internally anyway when looking for the final
lemma/stem. Finally, since the two modified ana-
lyzers had different strengths and weaknesses, we
combined their outputs to increase recall.
</bodyText>
<subsectionHeader confidence="0.998317">
3.2 Word-Level Paraphrasing
</subsectionHeader>
<bodyText confidence="0.999927173913044">
We perform word-level paraphrasing of the Malay
sides of the development and the testing bi-texts.
First, for each Malay word, we generate the
above-described list of morphologically simpler
words and alternative word segmentations; we think
of the words in this list as word-level paraphrases.
Then, for each development/testing Malay sentence,
we generate a lattice encoding all possible para-
phrasing options for each individual word.
We further specify a weight for each arc. We as-
sign 1 to the original Malay word w, and Pr(w′|w)
to each paraphrase w′ of w, where Pr(w′|w) is the
probability that w′ is a good paraphrase of w. Note
that multi-word paraphrases, e.g., resulting from
clitic segmentation, are encoded using a sequence of
arcs; in such cases, we assign Pr(w′|w) to the first
arc, and 1 to each subsequent arc.
We calculate the probability Pr(w′|w) using the
training Malay-English bi-text, which we align at
the word level using IBM model 4 (Brown et al.,
1993), and we observe which English words w and
w′ are aligned to. More precisely, we use pivoting to
estimate the probability Pr(w′|w) as follows:
</bodyText>
<equation confidence="0.98406">
Pr(w′|w) = Ei Pr(w′|w, ei)Pr(ei|w)
</equation>
<bodyText confidence="0.99995325">
Then, following (Callison-Burch et al., 2006; Wu
and Wang, 2007), we make the simplifying assump-
tion that w′ is conditionally independent of w given
ei, thus obtaining the following expression:
</bodyText>
<equation confidence="0.997334">
Pr(w′|w) = Ei Pr(w′|ei)Pr(ei|w)
</equation>
<bodyText confidence="0.9998985">
We estimate the probability Pr(ei|w) directly
from the word-aligned training bi-text as follows:
</bodyText>
<equation confidence="0.992775">
Pr(ei|w) = ##(w,)j)
</equation>
<bodyText confidence="0.9976586">
where #(x, e) is the number of times the Malay
word x is aligned to the English word e.
Estimating Pr(w′|ei) cannot be done directly
since w′ might not be present on the Malay side of
the training bi-text, e.g., because it is a multi-token
sequence generated by clitic segmentation. Thus, we
think of w′ as a pseudoword that stands for the union
of all Malay words in the training bi-text that are re-
ducible to w′ by our morphological analysis proce-
dure. So, we estimate Pr(w′|ei) as follows:
</bodyText>
<equation confidence="0.983829">
Pr(w′|ei) = Pr({v : w′ E forms(v)}|ei)
</equation>
<bodyText confidence="0.9999188">
where forms(x) is the set of the word-level para-
phrases4 for the Malay word x.
Since the training bi-text occurrences of the words
that are reducible to w′ are distinct, we can rewrite
the above as follows:
</bodyText>
<equation confidence="0.997007">
Pr(w′|ei) = Ev:w′Eforms(v) Pr(v|ei)
</equation>
<bodyText confidence="0.649133">
Finally, the probability Pr(v|ei) can be estimated
using maximum likelihood:
</bodyText>
<equation confidence="0.948661">
Pr(v|ei) = #(v,ei)
E
u #(u,ei)
</equation>
<subsectionHeader confidence="0.996433">
3.3 Sentence-Level Paraphrasing
</subsectionHeader>
<bodyText confidence="0.999996333333333">
In order for the word-level paraphrases to work,
there should be phrases in the phrase table that could
potentially match them. For some of the words, e.g.,
the lemmata, there could already be such phrases,
but for other transformations, e.g., clitic segmenta-
tion, this is unlikely. Thus, we need to augment the
phrase table with additional translation options.
One approach would be to modify the phrase ta-
ble directly, e.g., by adding additional entries, where
one or more Malay words are replaced by their para-
phrases. This would be problematic since the phrase
translation probabilities associated with these new
</bodyText>
<footnote confidence="0.9157805">
4Note that our paraphrasing process is directed: the para-
phrases are morphologically simpler than the original word.
</footnote>
<page confidence="0.986872">
1301
</page>
<bodyText confidence="0.999963955555556">
entries would be hard to estimate. For example, the
clitics, and even many of the intermediate morpho-
logical forms, would not exist as individual words in
the training bi-text, which means that there would be
no word alignments or lexical probabilities available
for them.
Another option would be to generate separate
word alignments for the original training bi-text and
for a version of it where the source (Malay) side
has been paraphrased. Then, the two bi-texts and
their word alignments would be concatenated and
used to build a phrase table (Dyer, 2007; Dyer et
al., 2008; Dyer, 2009). This would solve the prob-
lems with the word alignments and the phrase pair
probabilities estimations in a principled manner, but
it would require choosing for each word only one of
the paraphrases available to it, while we would pre-
fer to have a way to allow all options. Moreover, the
paraphrased and the original versions of the corpus
would be given equal weights, which might not be
desirable. Finally, since the two versions of the bi-
text would be word-aligned separately, there would
be no interaction between them, which might lead
to missed opportunities for improved alignments in
both parts of the bi-text (Nakov and Ng, 2009).
We avoid the above issues by adopting a sentence-
level paraphrasing approach. Following the gen-
eral framework proposed in (Nakov, 2008), we first
create multiple paraphrased versions of the source-
side sentences of the training bi-text. Then, each
paraphrased source sentence is paired with its orig-
inal translation. This augmented bi-text is word-
aligned and a phrase table T′ is built from it, which
is merged with a phrase table T for the original bi-
text. The merged table contains all phrase entries
from T, and the entries for the phrase pairs from T′
that are not in T. Following Nakov and Ng (2009),
we add up to three additional indicator features (tak-
ing the values 0.5 and 1) to each entry in the merged
phrase table, showing whether the entry came from
(1) T only, (2) T′ only, or (3) both T and T′. We also
try using the first one or two features only. We set
all feature weights using minimum error rate train-
ing (Och, 2003), and we optimize their number (one,
two, or three) on the development dataset.5
</bodyText>
<footnote confidence="0.8657">
5In theory, we should re-normalize the probabilities; in prac-
tice, this is not strictly required by the log-linear SMT model.
</footnote>
<bodyText confidence="0.999862111111111">
Each of our paraphrased sentences differs from its
original sentence by a single word, which prevents
combinatorial explosions: on average, we generate
14 paraphrased versions per input sentence. It fur-
ther ensures that the paraphrased parts of the sen-
tences will not dominate the word alignments or the
phrase pairs, and that there would be sufficient inter-
action at word alignment time between the original
sentences and their paraphrased versions.
</bodyText>
<subsectionHeader confidence="0.902218">
3.4 Phrase-Level Paraphrasing
</subsectionHeader>
<bodyText confidence="0.999983">
While our sentence-level paraphrasing informs the
decoder about the origin of each phrase pair (orig-
inal or paraphrased bi-text), it provides no indica-
tion about how good the phrase pairs from the para-
phrased bi-text are likely to be.
Following Callison-Burch et al. (2006), we fur-
ther augment the phrase table with one additional
feature whose value is 1 for the phrase pairs com-
ing from the original bi-text, and maxp Pr(p′|p) for
the phrase pairs extracted from the paraphrased bi-
text. Here p is a Malay phrase from T, and p′ is a
Malay phrase from T′ that does not exist in T but is
obtainable from p by substituting one or more words
in p with their derivationally related forms generated
by morphological analysis. The probability Pr(p′|p)
is calculated using phrase-level pivoting through En-
glish in the original phrase table T as follows (unlike
word-level pivoting, here ei is an English phrase):
</bodyText>
<equation confidence="0.976415">
Pr(p′|p) _ Ei Pr(p′|ei)Pr(ei|p)
</equation>
<bodyText confidence="0.999964">
We estimate the probabilities Pr(ei|p) and
Pr(p′|ei) as we did for word-level pivoting, except
that this time we use the list of the phrase pairs ex-
tracted from the original training bi-text, while be-
fore we used IBM model 4 word alignments. When
calculating Pr(p′|ei), we think of p′ as the set of all
possible Malay phrases q in T that are reducible to
p′ by morphological analysis of the words they con-
tain. This can be rewritten as follows:
</bodyText>
<equation confidence="0.977071">
Pr(p′|ei) _ Eq:p′∈par(q) Pr(q|ei)
</equation>
<bodyText confidence="0.999461333333333">
where par(q) is the set of all possible phrase-level
paraphrases for the Malay phrase q.
The probability Pr(q|ei) is estimated using maxi-
mum likelihood from the list of phrase pairs. There
is no combinatorial explosion here, since the phrases
are short and contain very few paraphrasable words.
</bodyText>
<page confidence="0.947041">
1302
</page>
<table confidence="0.994530333333333">
1K 2K 5K 10K 20K 40K 80K 160K 320K
30K 60K 151K 301K 602K 1.2M 2.4M 4.7M 9.5M
23.81 27.43 31.53 33.69 36.68 38.49 40.53 41.80 43.02
22.67 26.20 29.68 31.53 33.91 35.64 37.17 38.58 39.68
-1.14 -1.23 -1.85 -2.16 -2.77 -2.85 -3.36 -3.22 -3.34
23.27 28.42 32.66 33.69 37.16 38.14 39.79 41.76 42.77
-0.54 +0.99 +1.13 +0.00 +0.48 -0.35 -0.74 -0.04 -0.25
24.71 28.65 32.42 34.95 37.32 38.40 39.82 41.97 43.36
+0.90 +1.22 +0.89 +1.26 +0.64 -0.09 -0.71 +0.17 +0.34
24.97 29.11 33.03 35.12 37.39 38.73 41.04 42.24 43.52
+1.16 +1.68 +1.50 +1.43 +0.71 +0.24 +0.51 +0.44 +0.50
25.14 29.17 33.00 35.09 37.39 38.76 40.75 42.23 43.58
+1.33 +1.74 +1.47 +1.40 +0.71 +0.27 +0.22 +0.43 +0.56
25.27 29.19 33.35 35.23 37.46 39.00 40.95 42.30 43.73
+1.46 +1.76 +1.82 +1.54 +0.78 +0.51 +0.42 +0.50 +0.71
</table>
<figure confidence="0.917488888888889">
Number of sentence pairs
Number of English words
baseline
lemmatize all
‘noisier’ channel model (Dyer, 2007)
lattice + sent-par (orig+lemma)
lattice + sent-par
lattice + sent-par + word-par
lattice + sent-par + word-par + phrase-par
</figure>
<tableCaption confidence="0.954903">
Table 1: Evaluation results. Shown are BLEU scores and improvements over the baseline (in %) for different numbers
of training sentences. Statistically significant improvements are in bold for p &lt; 0.01 and in italic for p &lt; 0.05.
</tableCaption>
<sectionHeader confidence="0.998351" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.894266">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99993">
We created our Malay-English training and develop-
ment datasets from data that we downloaded from
the Web and then sentence-aligned using various
heuristics. Thus, we ended up with 350,003 training
sentence pairs, including 10.4M English and 9.7M
Malay word tokens. We further downloaded 49.8M
word tokens of monolingual English text, which we
used for language modeling.
For testing, we used 1,420 sentences with 28.8K
Malay word tokens, which were translated by three
human translators, yielding translations of 32.8K,
32.4K, and 32.9K English word tokens, respectively.
For development, we used 2,000 sentence pairs of
63.4K English and 58.5K Malay word tokens.
</bodyText>
<subsectionHeader confidence="0.986614">
4.2 General Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99998604">
First, we tokenized and lowercased all datasets:
training, development, and testing. We then built
directed word-level alignments for the training bi-
text for English-*Malay and for Malay-*English
using IBM model 4 (Brown et al., 1993), which
we symmetrized using the intersect+grow heuristic
(Och and Ney, 2003). Next, we extracted phrase-
level translation pairs of maximum length seven,
which we scored and used to build a phrase table
where each phrase pair is associated with the fol-
lowing five standard feature functions: forward and
reverse phrase translation probabilities, forward and
reverse lexicalized phrase translation probabilities,
and phrase penalty.
We trained a log-linear model using the following
standard SMT feature functions: trigram language
model probability, word penalty, distance-based dis-
tortion cost, and the five feature functions from the
phrase table. We set all weights on the development
dataset by optimizing BLEU (Papineni et al., 2002)
using minimum error rate training (Och, 2003), and
we plugged them in a beam search decoder (Koehn
et al., 2007) to translate the Malay test sentences to
English. Finally, we detokenized the output, and we
evaluated it against the three reference translations.
</bodyText>
<subsectionHeader confidence="0.993646">
4.3 Systems
</subsectionHeader>
<bodyText confidence="0.9993135">
Using the above general experimental setup, we im-
plemented the following baseline systems:
</bodyText>
<listItem confidence="0.9681854">
• baseline. This is the default system, which uses
no morphological processing.
• lemmatize all. This is the second baseline that
uses lemmatized versions of the Malay side of
the training, development and testing datasets.
• ‘noisier’ channel model.6 This is the model of
Dyer (2007). It uses 0-1 weights in the lattice
and only allows lemmata as alternative word-
forms; it uses no sentence-level or phrase-level
paraphrases.
</listItem>
<footnote confidence="0.879044142857143">
6We also tried the word segmentation model of Dyer (2009)
as implemented in the cdec decoder (Dyer et al., 2010), which
learns word segmentation lattices from raw text in an unsu-
pervised manner. Unfortunately, it could not learn meaning-
ful word segmentations for Malay, and thus we do not compare
against it. We believe this may be due to its focus on word seg-
mentation, which is of limited use for Malay.
</footnote>
<page confidence="0.856263">
1303
</page>
<table confidence="0.99975">
sent. system 1-gram 2-gram 3-gram 4-gram
1k baseline 59.78 29.60 17.36 10.46
paraphrases 62.23 31.19 18.53 11.35
2k baseline 64.20 33.46 20.41 12.92
paraphrases 66.38 35.42 21.97 14.06
5k baseline 68.12 38.12 24.20 15.72
paraphrases 70.41 40.13 25.71 17.02
10k baseline 70.13 40.67 26.15 17.27
paraphrases 72.04 42.28 27.55 18.36
20k baseline 73.19 44.12 29.14 19.50
paraphrases 73.28 44.43 29.77 20.31
40k baseline 74.66 45.97 30.70 20.83
paraphrases 75.47 46.54 31.09 21.17
80k baseline 75.72 48.08 32.80 22.59
paraphrases 76.03 48.47 33.20 23.00
160k baseline 76.55 49.21 34.09 23.78
paraphrases 77.14 49.89 34.57 24.06
320k baseline 77.72 50.54 35.19 24.78
paraphrases 78.03 51.24 35.99 25.42
</table>
<tableCaption confidence="0.874806333333333">
Table 2: Detailed BLEU n-gram precision scores: in
%, for different numbers of training sentence pairs, for
baseline and lattice + sent-par + word-par + phrase-par.
</tableCaption>
<bodyText confidence="0.991767538461539">
Our full morphological paraphrasing system is
lattice + sent-par + word-par + phrase-par. We
also experimented with some of its components
turned off. lattice + sent-par + word-par excludes
the additional feature from phrase-level paraphras-
ing. lattice + sent-par has all the morphologically
simpler derived forms in the lattice during decod-
ing, but their weights are uniformly set to 0 rather
than obtained using pivoting from word alignments.
Finally, in order to compare closely to the ‘noisier’
channel model, we further limited the morpholog-
ical variants of lattice + sent-par in the lattice to
lemmata only in lattice + sent-par (orig+lemma).
</bodyText>
<sectionHeader confidence="0.999312" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.996082285714286">
The experimental results are shown in Table 1.
First, we can see that lemmatize all has a consis-
tently disastrous effect on BLEU, which shows that
Malay morphology does indeed contain information
that is important when translating to English.
Second, Dyer (2007)’s ‘noisier’ channel model
helps for small datasets only. It performs worse than
lattice + sent-par (orig+lemma), from which it dif-
fers in the phrase table only; this confirms the im-
portance of our sentence-level paraphrasing.
Moving down to lattice + sent-par, we can see
that using multiple morphological wordforms in-
stead of just lemmata has a consistently positive im-
pact on BLEU for datasets of all sizes.
</bodyText>
<table confidence="0.999195210526316">
Sent. System BLEU NIST TER METEOR TESLA
1k baseline 23.81 6.7013 64.50 49.26 1.6794
paraphrases 25.27 6.9974 63.03 52.32 1.7579
2k baseline 27.43 7.3790 61.03 54.29 1.8718
paraphrases 29.19 7.7306 59.37 57.32 2.0031
5k baseline 31.53 8.0992 57.12 59.09 2.1172
paraphrases 33.35 8.4127 55.41 61.67 2.2240
10k baseline 33.69 8.5314 55.24 62.26 2.2656
paraphrases 35.23 8.7564 53.60 63.97 2.3634
20k baseline 36.68 8.9604 52.56 64.67 2.3961
paraphrases 37.46 9.0941 52.16 66.42 2.4621
40k baseline 38.49 9.3016 51.20 66.68 2.5166
paraphrases 39.00 9.4184 50.68 67.60 2.5604
80k baseline 40.53 9.6047 49.88 68.77 2.6331
paraphrases 40.95 9.6289 49.09 69.10 2.6628
160k baseline 41.80 9.7479 48.97 69.59 2.6887
paraphrases 42.30 9.8062 48.29 69.62 2.7049
320k baseline 43.02 9.8974 47.44 70.23 2.7398
paraphrases 43.73 9.9945 47.07 70.87 2.7856
</table>
<tableCaption confidence="0.774412333333333">
Table 3: Results for different evaluation measures: for
baseline and lattice + sent-par + word-par + phrase-par
(in % for all measures except for NIST).
</tableCaption>
<bodyText confidence="0.993323862068966">
Adding weights obtained using word-level piv-
oting in lattice + sent-par + word-par helps a
bit more, and also using phrase-level paraphrasing
weights yields even bigger further improvements for
lattice + sent-par + word-par + phrase-par.
Overall, our morphological paraphrases yield sta-
tistically significant improvements (p &lt; 0.01) in
BLEU, according to Collins et al. (2005)’s sign test,
for bi-texts as large as 320,000 sentence pairs.
A closer look at BLEU. Table 2 shows detailed
n-gram BLEU precision scores for n=1,2,3,4. Our
system outperforms the baseline on all precision
scores and for all numbers of training sentences.
Other evaluation measures. Table 3 reports
the results for five evaluation measures: BLEU
and NIST 11b, TER 0.7.25 (Snover et al., 2006),
METEOR 1.0 (Lavie and Denkowski, 2009), and
TESLA (Liu et al., 2010). Our system consistently
outperforms the baseline for all measures.
Example translations. Table 4 shows two trans-
lation examples. In the first example, the redupli-
cation bekalan-bekalan (‘supplies’) is an unknown
word, and was left untranslated by the baseline sys-
tem. It was not a problem for our system though,
which first paraphrased it as bekalan and then trans-
lated it as supply. Even though this is still wrong (we
need the plural supplies), it is arguably preferable to
passing the word untranslated; it also allowed for a
better translation of the surrounding context.
</bodyText>
<page confidence="0.992633">
1304
</page>
<bodyText confidence="0.9878015">
src : Mercy Relief telah menghantar 17 khemah khas bernilai $5,000 setiap satu yang boleh menampung kelas seramai 30
pelajar, selain bekalan-bekalan lain seperti 500 khemah biasa, barang makanan dan ubat-ubatan untuk mangsa gempa Sichuan.
ref1: Mercy Relief has sent 17 special tents valued at $5,000 each, that can accommodate a class of 30 students, including
other aid supplies such as 500 normal tents, food and medicine for the victims of Sichuan quake.
base: mercy relief has sent 17 special tents worth $5,000 each class could accommodate a total of 30 students, besides other
bekalan-bekalan 500 tents as usual, foodstuff and medicines for sichuan quake relief.
para: mercy relief has sent 17 special tents worth $5,000 each class could accommodate a total of 30 students, besides other
supply such as 500 tents, food and medicines for sichuan quake relief.
src : Walaupun hidup susah, kami tetap berusaha untuk menjalani kehidupan seperti biasa.
ref1: Even though life is difficult, we are still trying to go through life as usual.
base: despite the hard life, we will always strive to undergo training as usual.
para: despite the hard life, we will always strive to live normal.
</bodyText>
<tableCaption confidence="0.954989">
Table 4: Example translations. For each example, we show a source sentence (src), one of the three reference
translations (ref1), and the outputs of baseline (base) and of lattice + sent-par + word-par + phrase-par (para).
</tableCaption>
<bodyText confidence="0.999929333333333">
In the second example, the baseline system trans-
lated menjalani kehidupan (lit. ‘go through life’)
as undergo training, because of a bad phrase pair,
which was extracted from wrong word alignments.
Note that the words menjalani (‘go through’) and
kehidupan (‘life/existence’) are derivational forms
ofjalan (‘go’) and hidup (‘life/living’), respectively.
Thus, in the paraphrasing system, they were in-
volved in sentence-level paraphrasing, where the
alignments were improved. While the wrong phrase
pair was still available, the system chose a better one
from the paraphrased training bi-text.
</bodyText>
<sectionHeader confidence="0.999881" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999961910714286">
Most research in SMT for a morphologically rich
source language has focused on inflected forms of
the same word. The assumption is that they would
have similar semantics and thus could have the same
translation. Researchers have used stemming (Yang
and Kirchhoff, 2006), lemmatization (Al-Onaizan et
al., 1999; Goldwater and McClosky, 2005; Dyer,
2007), or direct clustering (Talbot and Osborne,
2006) to identify such groups of words and use them
as equivalence classes or as possible alternatives in
translation. Frameworks for the simultaneous use of
different word-level representations have been pro-
posed as well (Koehn and Hoang, 2007).
A second important line of research has focused
on word segmentation, which is useful for languages
like German, which are rich in compound words that
are spelled concatenated (Koehn and Knight, 2003;
Yang and Kirchhoff, 2006), or like Arabic, Turk-
ish, Finnish, and, to a lesser extent, Spanish and
Italian, where clitics often attach to the preceding
word (Habash and Sadat, 2006). For languages with
more or less regular inflectional morphology like
Arabic or Turkish, another good idea is to segment
words into morpheme sequences, e.g., prefix(es)-
stem-suffix(es), which can be used instead of the
original words (Lee, 2004) or in addition to them.
This can be achieved using a lattice input to the
translation system (Dyer et al., 2008; Dyer, 2009).
Unfortunately, none of these general lines of re-
search suits Malay well, whose compounds are
rarely concatenated, clitics are not so frequent, and
morphology is mostly derivational, and thus likely
to generate words whose semantics substantially dif-
fers from the semantics of the original word. There-
fore, we cannot expect the existence of equivalence
classes: it is only occasionally that two derivation-
ally related wordforms would share the same tar-
get language translation. Thus, instead of look-
ing for equivalence classes, we have focused on the
pairwise relationship between derivationally related
wordforms, which we treat as potential paraphrases.
Our approach is an extension of the ‘noisier’
channel model of Dyer (2007). He starts by generat-
ing separate word alignments for the original train-
ing bi-text and for a version of it where the source
side has been lemmatized. Then, the two bi-texts
and their word alignments are concatenated and used
to build a phrase table. Finally, the source sides of
the development and the test datasets are converted
into confusion networks where additional arcs are
added for word lemmata. The arc weights are set to
1 for the original wordforms and to 0 for the lem-
mata. In contrast, we provide multiple paraphras-
ing alternatives for each morphologically complex
word, including derivational forms that occupy in-
termediary positions between the original wordform
</bodyText>
<page confidence="0.970291">
1305
</page>
<bodyText confidence="0.999907115384615">
and its lemma. Note that some of those paraphrasing
alternatives are multi-word, and thus we use a lattice
instead of a confusion network. Moreover, we give
different weights to the different alternatives rather
then assigning them all 0.
Second, our work is related to that of Dyer et
al. (2008), who use a lattice to add a single alter-
native clitic-segmented version of the original word
for Arabic. However, we provide multiple alterna-
tives. We also include derivational forms in addi-
tion to clitic-segmented ones, and we give different
weights to the different alternatives (instead of 0).
Third, our work is also related to that of Dyer
(2009), who uses a lattice to add multiple alterna-
tive segmented versions of the original word for Ger-
man, Hungarian, and Turkish. However, we focus
on derivational morphology rather than on clitics
and inflections, add derivational forms in addition
to clitic-segmented ones, and use cross-lingual word
pivoting to estimate paraphrase probabilities.
Finally, our work is related to that of Callison-
Burch et al. (2006), who use cross-lingual pivot-
ing to generate phrase-level paraphrases with corre-
sponding probabilities. However, our paraphrases
are derived through morphological analysis; thus,
we do not need corpora in additional languages.
</bodyText>
<sectionHeader confidence="0.987889" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999993538461538">
We have presented a novel approach to trans-
lating from a morphologically complex language,
which uses paraphrases and paraphrasing tech-
niques at three different levels of translation: word-
level, phrase-level, and sentence-level. Our experi-
ments translating from Malay, whose morphology is
mostly derivational, into English have shown signif-
icant improvements over rivaling approaches based
on several automatic evaluation measures.
In future work, we want to improve the proba-
bility estimations for our paraphrasing models. We
also want to experiment with other morphologically
complex languages and other SMT models.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99950175">
This work was supported by research grant
POD0713875. We would like to thank the anony-
mous reviewers for their detailed and constructive
comments, which have helped us improve the paper.
</bodyText>
<sectionHeader confidence="0.990061" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999662673076923">
Mirna Adriani, Jelita Asian, Bobby Nazief, S. M.M.
Tahaghoghi, and Hugh E. Williams. 2007. Stemming
Indonesian: A confix-stripping approach. ACM Trans-
actions on Asian Language Information Processing,
6:1–33.
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, JHU Summer Workshop.
Timothy Baldwin and Su’ad Awab. 2006. Open source
corpus analysis tools for Malay. In Proceedings of the
5th International Conference on Language Resources
and Evaluation, LREC ’06, pages 2212–2215.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263–311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT-NAACL ’06, pages 17–24.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, ACL ’05, pages 263–270.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
ACL ’05, pages 531–540.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of the 46th Annual Meeting ofthe Association
for Computational Linguistics, ACL ’08, pages 1012–
1020.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, ACL
’10, pages 7–12.
Christopher Dyer. 2007. The ’noisier channel’: trans-
lation from morphologically complex languages. In
Proceedings of the Second Workshop on Statistical
Machine Translation, WMT ’07, pages 207–211.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings
of Human Language Technologies: The 2009 Annual
</reference>
<page confidence="0.820282">
1306
</page>
<reference confidence="0.999541704761905">
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, NAACL ’09,
pages 406–414.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of the Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT-NAACL ’04,
pages 273–280.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proceedings of the Conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT-EMNLP ’05, pages 676–683.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, HLT-NAACL ’06, pages 49–52.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ’07, pages 868–876.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the 10th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, EACL ’03, pages
187–193.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology, NAACL ’03,
pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics Companion Volume on
Demo and Poster Sessions, ACL ’07, pages 177–180.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23:105–115.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT-NAACL ’04, pages 57–60.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation evaluation of sentences
with linear-programming-based analysis. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, WMT ’10, pages 354–
359.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’09, pages 1358–
1367.
Preslav Nakov. 2008. Improved statistical machine
translation using monolingual paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence, ECAI ’08, pages 338–342.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, ACL ’03, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ’02, pages 311–318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, ACL ’05, pages 271–279.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Association for Machine Trans-
lation in the Americas, AMTA ’06, pages 223–231.
David Talbot and Miles Osborne. 2006. Modelling lex-
ical redundancy for machine translation. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, COLING-
ACL ’06, pages 969–976.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165–181.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of the European
Chapter of the Association for Computational Linguis-
tics, EACL ’06, pages 41–48.
</reference>
<page confidence="0.993771">
1307
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.270278">
<title confidence="0.9981615">Translating from Morphologically Complex A Paraphrase-Based Approach</title>
<author confidence="0.829862">Preslav</author>
<affiliation confidence="0.911292333333333">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.937569">Singapore</address>
<email confidence="0.978686">nakov@comp.nus.edu.sg</email>
<author confidence="0.977942">Hwee Tou</author>
<affiliation confidence="0.911509333333333">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.939476">Singapore</address>
<email confidence="0.990764">nght@comp.nus.edu.sg</email>
<abstract confidence="0.98113905">We propose a novel approach to translating from a morphologically complex language. Unlike previous research, which has targeted word inflections and concatenations, we focus on the pairwise relationship between morphologically related words, which we treat as paraphrases handle using paraphrasing techniques at the word, phrase, and sentence level. An important advantage of this framework is that it can cope with derivational morphology, which has so far remained largely beyond the capabilities of statistical machine translation systems. Our experiments translating from Malay, whose morphology is mostly derivational, into English show significant improvements over rivaling approaches based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mirna Adriani</author>
<author>Jelita Asian</author>
<author>Bobby Nazief</author>
<author>S M M Tahaghoghi</author>
<author>Hugh E Williams</author>
</authors>
<title>Stemming Indonesian: A confix-stripping approach.</title>
<date>2007</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<pages>6--1</pages>
<contexts>
<context position="12941" citStr="Adriani et al., 2007" startWordPosition="1957" endWordPosition="1960"> 1/4=0.25, and thus (f) is applicable. 1300 As an illustration, here are the wordforms we generate for adik-beradiknya/‘his siblings’: adik, adik-beradiknya, adik-beradik nya, adik-beradik, beradiknya, beradik nya, adik nya, and beradik. And for berpelajaran/‘is educated’, we build the list: berpelajaran, pelajaran, pelajar, ajaran, and ajar. Note that the lists do include the original word. To generate the above wordforms, we used two morphological analyzers: a freely available Malay lemmatizer (Baldwin and Awab, 2006), and an inhouse re-implementation of the Indonesian stemmer described in (Adriani et al., 2007). Note that these tools’ objective is to return a single lemma/stem, e.g., they would return adik for adik-beradiknya, and ajar for berpelajaran. However, it was straightforward to modify them to also output the above intermediary wordforms, which the tools were generating internally anyway when looking for the final lemma/stem. Finally, since the two modified analyzers had different strengths and weaknesses, we combined their outputs to increase recall. 3.2 Word-Level Paraphrasing We perform word-level paraphrasing of the Malay sides of the development and the testing bi-texts. First, for eac</context>
</contexts>
<marker>Adriani, Asian, Nazief, Tahaghoghi, Williams, 2007</marker>
<rawString>Mirna Adriani, Jelita Asian, Bobby Nazief, S. M.M. Tahaghoghi, and Hugh E. Williams. 2007. Stemming Indonesian: A confix-stripping approach. ACM Transactions on Asian Language Information Processing, 6:1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<tech>Technical report, JHU Summer Workshop.</tech>
<contexts>
<context position="32238" citStr="Al-Onaizan et al., 1999" startWordPosition="5057" endWordPosition="5060">rms ofjalan (‘go’) and hidup (‘life/living’), respectively. Thus, in the paraphrasing system, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and,</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation. Technical report, JHU Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Su’ad Awab</author>
</authors>
<title>Open source corpus analysis tools for Malay.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation, LREC ’06,</booktitle>
<pages>2212--2215</pages>
<contexts>
<context position="12845" citStr="Baldwin and Awab, 2006" startWordPosition="1942" endWordPosition="1945">ng)=gnng, and thus, the ratio is 4/6=0.67 (≥ 0.5) for gunung-ganang. However, for aceh-nias, it is 1/4=0.25, and thus (f) is applicable. 1300 As an illustration, here are the wordforms we generate for adik-beradiknya/‘his siblings’: adik, adik-beradiknya, adik-beradik nya, adik-beradik, beradiknya, beradik nya, adik nya, and beradik. And for berpelajaran/‘is educated’, we build the list: berpelajaran, pelajaran, pelajar, ajaran, and ajar. Note that the lists do include the original word. To generate the above wordforms, we used two morphological analyzers: a freely available Malay lemmatizer (Baldwin and Awab, 2006), and an inhouse re-implementation of the Indonesian stemmer described in (Adriani et al., 2007). Note that these tools’ objective is to return a single lemma/stem, e.g., they would return adik for adik-beradiknya, and ajar for berpelajaran. However, it was straightforward to modify them to also output the above intermediary wordforms, which the tools were generating internally anyway when looking for the final lemma/stem. Finally, since the two modified analyzers had different strengths and weaknesses, we combined their outputs to increase recall. 3.2 Word-Level Paraphrasing We perform word-l</context>
</contexts>
<marker>Baldwin, Awab, 2006</marker>
<rawString>Timothy Baldwin and Su’ad Awab. 2006. Open source corpus analysis tools for Malay. In Proceedings of the 5th International Conference on Language Resources and Evaluation, LREC ’06, pages 2212–2215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1470" citStr="Brown et al., 1993" startWordPosition="200" endWordPosition="203">ond the capabilities of statistical machine translation systems. Our experiments translating from Malay, whose morphology is mostly derivational, into English show significant improvements over rivaling approaches based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. Ignoring morphology was fine as long as the main research interest remained focused on languages with limited (e.g., English, French, Spanish) or minimal (e.g., Chinese) morphology. Since the attention shifted to langu</context>
<context position="14406" citStr="Brown et al., 1993" startWordPosition="2191" endWordPosition="2194"> lattice encoding all possible paraphrasing options for each individual word. We further specify a weight for each arc. We assign 1 to the original Malay word w, and Pr(w′|w) to each paraphrase w′ of w, where Pr(w′|w) is the probability that w′ is a good paraphrase of w. Note that multi-word paraphrases, e.g., resulting from clitic segmentation, are encoded using a sequence of arcs; in such cases, we assign Pr(w′|w) to the first arc, and 1 to each subsequent arc. We calculate the probability Pr(w′|w) using the training Malay-English bi-text, which we align at the word level using IBM model 4 (Brown et al., 1993), and we observe which English words w and w′ are aligned to. More precisely, we use pivoting to estimate the probability Pr(w′|w) as follows: Pr(w′|w) = Ei Pr(w′|w, ei)Pr(ei|w) Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007), we make the simplifying assumption that w′ is conditionally independent of w given ei, thus obtaining the following expression: Pr(w′|w) = Ei Pr(w′|ei)Pr(ei|w) We estimate the probability Pr(ei|w) directly from the word-aligned training bi-text as follows: Pr(ei|w) = ##(w,)j) where #(x, e) is the number of times the Malay word x is aligned to the English</context>
<context position="23346" citStr="Brown et al., 1993" startWordPosition="3665" endWordPosition="3668">nolingual English text, which we used for language modeling. For testing, we used 1,420 sentences with 28.8K Malay word tokens, which were translated by three human translators, yielding translations of 32.8K, 32.4K, and 32.9K English word tokens, respectively. For development, we used 2,000 sentence pairs of 63.4K English and 58.5K Malay word tokens. 4.2 General Experimental Setup First, we tokenized and lowercased all datasets: training, development, and testing. We then built directed word-level alignments for the training bitext for English-*Malay and for Malay-*English using IBM model 4 (Brown et al., 1993), which we symmetrized using the intersect+grow heuristic (Och and Ney, 2003). Next, we extracted phraselevel translation pairs of maximum length seven, which we scored and used to build a phrase table where each phrase pair is associated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and t</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="14628" citStr="Callison-Burch et al., 2006" startWordPosition="2226" endWordPosition="2229">|w) is the probability that w′ is a good paraphrase of w. Note that multi-word paraphrases, e.g., resulting from clitic segmentation, are encoded using a sequence of arcs; in such cases, we assign Pr(w′|w) to the first arc, and 1 to each subsequent arc. We calculate the probability Pr(w′|w) using the training Malay-English bi-text, which we align at the word level using IBM model 4 (Brown et al., 1993), and we observe which English words w and w′ are aligned to. More precisely, we use pivoting to estimate the probability Pr(w′|w) as follows: Pr(w′|w) = Ei Pr(w′|w, ei)Pr(ei|w) Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007), we make the simplifying assumption that w′ is conditionally independent of w given ei, thus obtaining the following expression: Pr(w′|w) = Ei Pr(w′|ei)Pr(ei|w) We estimate the probability Pr(ei|w) directly from the word-aligned training bi-text as follows: Pr(ei|w) = ##(w,)j) where #(x, e) is the number of times the Malay word x is aligned to the English word e. Estimating Pr(w′|ei) cannot be done directly since w′ might not be present on the Malay side of the training bi-text, e.g., because it is a multi-token sequence generated by clitic segmentation. Thus, we think of </context>
<context position="19707" citStr="Callison-Burch et al. (2006)" startWordPosition="3068" endWordPosition="3071">: on average, we generate 14 paraphrased versions per input sentence. It further ensures that the paraphrased parts of the sentences will not dominate the word alignments or the phrase pairs, and that there would be sufficient interaction at word alignment time between the original sentences and their paraphrased versions. 3.4 Phrase-Level Paraphrasing While our sentence-level paraphrasing informs the decoder about the origin of each phrase pair (original or paraphrased bi-text), it provides no indication about how good the phrase pairs from the paraphrased bi-text are likely to be. Following Callison-Burch et al. (2006), we further augment the phrase table with one additional feature whose value is 1 for the phrase pairs coming from the original bi-text, and maxp Pr(p′|p) for the phrase pairs extracted from the paraphrased bitext. Here p is a Malay phrase from T, and p′ is a Malay phrase from T′ that does not exist in T but is obtainable from p by substituting one or more words in p with their derivationally related forms generated by morphological analysis. The probability Pr(p′|p) is calculated using phrase-level pivoting through English in the original phrase table T as follows (unlike word-level pivoting</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL ’06, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1713" citStr="Chiang, 2005" startWordPosition="238" endWordPosition="239">measures (for 320,000 sentence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. Ignoring morphology was fine as long as the main research interest remained focused on languages with limited (e.g., English, French, Spanish) or minimal (e.g., Chinese) morphology. Since the attention shifted to languages like Arabic, however, the importance of morphology became obvious and several approaches to handle it have been proposed. Depending on the particular language of interest, researchers have paid attention to word inflections and clitics, e</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics, ACL ’08,</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="17159" citStr="Dyer et al., 2008" startWordPosition="2639" endWordPosition="2642">y simpler than the original word. 1301 entries would be hard to estimate. For example, the clitics, and even many of the intermediate morphological forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them. Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignment</context>
<context position="33312" citStr="Dyer et al., 2008" startWordPosition="5227" endWordPosition="5230">rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)- stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general lines of research suits Malay well, whose compounds are rarely concatenated, clitics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose semantics substantially differs from the semantics of the original word. Therefore, we cannot expect the existence of equivalence classes: it is only occasionally that two derivationally related wordforms would share the same target language translation. Thus, instead of looking for equivalence classes, we have focused on the pairwise relationship between deriv</context>
<context position="35038" citStr="Dyer et al. (2008)" startWordPosition="5502" endWordPosition="5505">dditional arcs are added for word lemmata. The arc weights are set to 1 for the original wordforms and to 0 for the lemmata. In contrast, we provide multiple paraphrasing alternatives for each morphologically complex word, including derivational forms that occupy intermediary positions between the original wordform 1305 and its lemma. Note that some of those paraphrasing alternatives are multi-word, and thus we use a lattice instead of a confusion network. Moreover, we give different weights to the different alternatives rather then assigning them all 0. Second, our work is related to that of Dyer et al. (2008), who use a lattice to add a single alternative clitic-segmented version of the original word for Arabic. However, we provide multiple alternatives. We also include derivational forms in addition to clitic-segmented ones, and we give different weights to the different alternatives (instead of 0). Third, our work is also related to that of Dyer (2009), who uses a lattice to add multiple alternative segmented versions of the original word for German, Hungarian, and Turkish. However, we focus on derivational morphology rather than on clitics and inflections, add derivational forms in addition to </context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics, ACL ’08, pages 1012– 1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations, ACL ’10,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="24986" citStr="Dyer et al., 2010" startWordPosition="3918" endWordPosition="3921">bove general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’ channel model.6 This is the model of Dyer (2007). It uses 0-1 weights in the lattice and only allows lemmata as alternative wordforms; it uses no sentence-level or phrase-level paraphrases. 6We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al., 2010), which learns word segmentation lattices from raw text in an unsupervised manner. Unfortunately, it could not learn meaningful word segmentations for Malay, and thus we do not compare against it. We believe this may be due to its focus on word segmentation, which is of limited use for Malay. 1303 sent. system 1-gram 2-gram 3-gram 4-gram 1k baseline 59.78 29.60 17.36 10.46 paraphrases 62.23 31.19 18.53 11.35 2k baseline 64.20 33.46 20.41 12.92 paraphrases 66.38 35.42 21.97 14.06 5k baseline 68.12 38.12 24.20 15.72 paraphrases 70.41 40.13 25.71 17.02 10k baseline 70.13 40.67 26.15 17.27 paraphr</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, ACL ’10, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
</authors>
<title>The ’noisier channel’: translation from morphologically complex languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07,</booktitle>
<pages>207--211</pages>
<contexts>
<context position="17140" citStr="Dyer, 2007" startWordPosition="2637" endWordPosition="2638">rphologically simpler than the original word. 1301 entries would be hard to estimate. For example, the clitics, and even many of the intermediate morphological forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them. Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for</context>
<context position="22039" citStr="Dyer, 2007" startWordPosition="3467" endWordPosition="3468">9.79 41.76 42.77 -0.54 +0.99 +1.13 +0.00 +0.48 -0.35 -0.74 -0.04 -0.25 24.71 28.65 32.42 34.95 37.32 38.40 39.82 41.97 43.36 +0.90 +1.22 +0.89 +1.26 +0.64 -0.09 -0.71 +0.17 +0.34 24.97 29.11 33.03 35.12 37.39 38.73 41.04 42.24 43.52 +1.16 +1.68 +1.50 +1.43 +0.71 +0.24 +0.51 +0.44 +0.50 25.14 29.17 33.00 35.09 37.39 38.76 40.75 42.23 43.58 +1.33 +1.74 +1.47 +1.40 +0.71 +0.27 +0.22 +0.43 +0.56 25.27 29.19 33.35 35.23 37.46 39.00 40.95 42.30 43.73 +1.46 +1.76 +1.82 +1.54 +0.78 +0.51 +0.42 +0.50 +0.71 Number of sentence pairs Number of English words baseline lemmatize all ‘noisier’ channel model (Dyer, 2007) lattice + sent-par (orig+lemma) lattice + sent-par lattice + sent-par + word-par lattice + sent-par + word-par + phrase-par Table 1: Evaluation results. Shown are BLEU scores and improvements over the baseline (in %) for different numbers of training sentences. Statistically significant improvements are in bold for p &lt; 0.01 and in italic for p &lt; 0.05. 4 Experiments 4.1 Data We created our Malay-English training and development datasets from data that we downloaded from the Web and then sentence-aligned using various heuristics. Thus, we ended up with 350,003 training sentence pairs, including</context>
<context position="24732" citStr="Dyer (2007)" startWordPosition="3878" endWordPosition="3879">h, 2003), and we plugged them in a beam search decoder (Koehn et al., 2007) to translate the Malay test sentences to English. Finally, we detokenized the output, and we evaluated it against the three reference translations. 4.3 Systems Using the above general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’ channel model.6 This is the model of Dyer (2007). It uses 0-1 weights in the lattice and only allows lemmata as alternative wordforms; it uses no sentence-level or phrase-level paraphrases. 6We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al., 2010), which learns word segmentation lattices from raw text in an unsupervised manner. Unfortunately, it could not learn meaningful word segmentations for Malay, and thus we do not compare against it. We believe this may be due to its focus on word segmentation, which is of limited use for Malay. 1303 sent. system 1-gram 2-gram 3-gram 4-gram 1k bas</context>
<context position="27083" citStr="Dyer (2007)" startWordPosition="4251" endWordPosition="4252">ice during decoding, but their weights are uniformly set to 0 rather than obtained using pivoting from word alignments. Finally, in order to compare closely to the ‘noisier’ channel model, we further limited the morphological variants of lattice + sent-par in the lattice to lemmata only in lattice + sent-par (orig+lemma). 5 Results and Discussion The experimental results are shown in Table 1. First, we can see that lemmatize all has a consistently disastrous effect on BLEU, which shows that Malay morphology does indeed contain information that is important when translating to English. Second, Dyer (2007)’s ‘noisier’ channel model helps for small datasets only. It performs worse than lattice + sent-par (orig+lemma), from which it differs in the phrase table only; this confirms the importance of our sentence-level paraphrasing. Moving down to lattice + sent-par, we can see that using multiple morphological wordforms instead of just lemmata has a consistently positive impact on BLEU for datasets of all sizes. Sent. System BLEU NIST TER METEOR TESLA 1k baseline 23.81 6.7013 64.50 49.26 1.6794 paraphrases 25.27 6.9974 63.03 52.32 1.7579 2k baseline 27.43 7.3790 61.03 54.29 1.8718 paraphrases 29.19</context>
<context position="32281" citStr="Dyer, 2007" startWordPosition="5065" endWordPosition="5066">ely. Thus, in the paraphrasing system, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, w</context>
<context position="34056" citStr="Dyer (2007)" startWordPosition="5342" endWordPosition="5343">ics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose semantics substantially differs from the semantics of the original word. Therefore, we cannot expect the existence of equivalence classes: it is only occasionally that two derivationally related wordforms would share the same target language translation. Thus, instead of looking for equivalence classes, we have focused on the pairwise relationship between derivationally related wordforms, which we treat as potential paraphrases. Our approach is an extension of the ‘noisier’ channel model of Dyer (2007). He starts by generating separate word alignments for the original training bi-text and for a version of it where the source side has been lemmatized. Then, the two bi-texts and their word alignments are concatenated and used to build a phrase table. Finally, the source sides of the development and the test datasets are converted into confusion networks where additional arcs are added for word lemmata. The arc weights are set to 1 for the original wordforms and to 0 for the lemmata. In contrast, we provide multiple paraphrasing alternatives for each morphologically complex word, including der</context>
</contexts>
<marker>Dyer, 2007</marker>
<rawString>Christopher Dyer. 2007. The ’noisier channel’: translation from morphologically complex languages. In Proceedings of the Second Workshop on Statistical Machine Translation, WMT ’07, pages 207–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for MT.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>406--414</pages>
<contexts>
<context position="17172" citStr="Dyer, 2009" startWordPosition="2643" endWordPosition="2644">original word. 1301 entries would be hard to estimate. For example, the clitics, and even many of the intermediate morphological forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them. Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignments in both par</context>
<context position="24931" citStr="Dyer (2009)" startWordPosition="3910" endWordPosition="3911"> reference translations. 4.3 Systems Using the above general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’ channel model.6 This is the model of Dyer (2007). It uses 0-1 weights in the lattice and only allows lemmata as alternative wordforms; it uses no sentence-level or phrase-level paraphrases. 6We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al., 2010), which learns word segmentation lattices from raw text in an unsupervised manner. Unfortunately, it could not learn meaningful word segmentations for Malay, and thus we do not compare against it. We believe this may be due to its focus on word segmentation, which is of limited use for Malay. 1303 sent. system 1-gram 2-gram 3-gram 4-gram 1k baseline 59.78 29.60 17.36 10.46 paraphrases 62.23 31.19 18.53 11.35 2k baseline 64.20 33.46 20.41 12.92 paraphrases 66.38 35.42 21.97 14.06 5k baseline 68.12 38.12 24.20 15.72 paraphrases 70.41 40.13 2</context>
<context position="33325" citStr="Dyer, 2009" startWordPosition="5231" endWordPosition="5232">rds that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)- stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general lines of research suits Malay well, whose compounds are rarely concatenated, clitics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose semantics substantially differs from the semantics of the original word. Therefore, we cannot expect the existence of equivalence classes: it is only occasionally that two derivationally related wordforms would share the same target language translation. Thus, instead of looking for equivalence classes, we have focused on the pairwise relationship between derivationally rel</context>
<context position="35390" citStr="Dyer (2009)" startWordPosition="5562" endWordPosition="5563">e paraphrasing alternatives are multi-word, and thus we use a lattice instead of a confusion network. Moreover, we give different weights to the different alternatives rather then assigning them all 0. Second, our work is related to that of Dyer et al. (2008), who use a lattice to add a single alternative clitic-segmented version of the original word for Arabic. However, we provide multiple alternatives. We also include derivational forms in addition to clitic-segmented ones, and we give different weights to the different alternatives (instead of 0). Third, our work is also related to that of Dyer (2009), who uses a lattice to add multiple alternative segmented versions of the original word for German, Hungarian, and Turkish. However, we focus on derivational morphology rather than on clitics and inflections, add derivational forms in addition to clitic-segmented ones, and use cross-lingual word pivoting to estimate paraphrase probabilities. Finally, our work is related to that of CallisonBurch et al. (2006), who use cross-lingual pivoting to generate phrase-level paraphrases with corresponding probabilities. However, our paraphrases are derived through morphological analysis; thus, we do not</context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>Chris Dyer. 2009. Using a maximum entropy model to build segmentation lattices for MT. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 406–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL ’04,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="1780" citStr="Galley et al., 2004" startWordPosition="247" endWordPosition="250">rd tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. Ignoring morphology was fine as long as the main research interest remained focused on languages with limited (e.g., English, French, Spanish) or minimal (e.g., Chinese) morphology. Since the attention shifted to languages like Arabic, however, the importance of morphology became obvious and several approaches to handle it have been proposed. Depending on the particular language of interest, researchers have paid attention to word inflections and clitics, e.g., for Arabic, Finnish, and Turkish, or to noun compounds, e.g., </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL ’04, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT-EMNLP ’05,</booktitle>
<pages>676--683</pages>
<contexts>
<context position="32268" citStr="Goldwater and McClosky, 2005" startWordPosition="5061" endWordPosition="5064">dup (‘life/living’), respectively. Thus, in the paraphrasing system, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish a</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT-EMNLP ’05, pages 676–683.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Fatiha Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, HLT-NAACL ’06,</booktitle>
<pages>49--52</pages>
<contexts>
<context position="32953" citStr="Habash and Sadat, 2006" startWordPosition="5169" endWordPosition="5172"> to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)- stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general lines of research suits Malay well, whose compounds are rarely concatenated, clitics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose sema</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, HLT-NAACL ’06, pages 49–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07,</booktitle>
<pages>868--876</pages>
<contexts>
<context position="32570" citStr="Koehn and Hoang, 2007" startWordPosition="5107" endWordPosition="5110">T for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)- stem-suffix(es), which can be used instead of </context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07, pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’03,</booktitle>
<pages>187--193</pages>
<contexts>
<context position="32771" citStr="Koehn and Knight, 2003" startWordPosition="5139" endWordPosition="5142">ers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)- stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general lines o</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’03, pages 187–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL ’03,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1684" citStr="Koehn et al., 2003" startWordPosition="232" endWordPosition="235">based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. Ignoring morphology was fine as long as the main research interest remained focused on languages with limited (e.g., English, French, Spanish) or minimal (e.g., Chinese) morphology. Since the attention shifted to languages like Arabic, however, the importance of morphology became obvious and several approaches to handle it have been proposed. Depending on the particular language of interest, researchers have paid attention to wo</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL ’03, pages 48–54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume on Demo and Poster Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="24196" citStr="Koehn et al., 2007" startWordPosition="3793" endWordPosition="3796">ociated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on the development dataset by optimizing BLEU (Papineni et al., 2002) using minimum error rate training (Och, 2003), and we plugged them in a beam search decoder (Koehn et al., 2007) to translate the Malay test sentences to English. Finally, we detokenized the output, and we evaluated it against the three reference translations. 4.3 Systems Using the above general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’ channel model.6 This is the model of Dyer (2007). It uses 0-1 weights in the lattice and only allows lemmata as </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume on Demo and Poster Sessions, ACL ’07, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>The meteor metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--105</pages>
<contexts>
<context position="29295" citStr="Lavie and Denkowski, 2009" startWordPosition="4590" endWordPosition="4593">vements for lattice + sent-par + word-par + phrase-par. Overall, our morphological paraphrases yield statistically significant improvements (p &lt; 0.01) in BLEU, according to Collins et al. (2005)’s sign test, for bi-texts as large as 320,000 sentence pairs. A closer look at BLEU. Table 2 shows detailed n-gram BLEU precision scores for n=1,2,3,4. Our system outperforms the baseline on all precision scores and for all numbers of training sentences. Other evaluation measures. Table 3 reports the results for five evaluation measures: BLEU and NIST 11b, TER 0.7.25 (Snover et al., 2006), METEOR 1.0 (Lavie and Denkowski, 2009), and TESLA (Liu et al., 2010). Our system consistently outperforms the baseline for all measures. Example translations. Table 4 shows two translation examples. In the first example, the reduplication bekalan-bekalan (‘supplies’) is an unknown word, and was left untranslated by the baseline system. It was not a problem for our system though, which first paraphrased it as bekalan and then translated it as supply. Even though this is still wrong (we need the plural supplies), it is arguably preferable to passing the word untranslated; it also allowed for a better translation of the surrounding c</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J. Denkowski. 2009. The meteor metric for automatic evaluation of machine translation. Machine Translation, 23:105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Morphological analysis for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL ’04,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="33200" citStr="Lee, 2004" startWordPosition="5208" endWordPosition="5209"> line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)- stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general lines of research suits Malay well, whose compounds are rarely concatenated, clitics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose semantics substantially differs from the semantics of the original word. Therefore, we cannot expect the existence of equivalence classes: it is only occasionally that two derivationally related wordforms would share the same target language translati</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-Suk Lee. 2004. Morphological analysis for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL ’04, pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>TESLA: Translation evaluation of sentences with linear-programming-based analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>354--359</pages>
<contexts>
<context position="29325" citStr="Liu et al., 2010" startWordPosition="4596" endWordPosition="4599">ar + phrase-par. Overall, our morphological paraphrases yield statistically significant improvements (p &lt; 0.01) in BLEU, according to Collins et al. (2005)’s sign test, for bi-texts as large as 320,000 sentence pairs. A closer look at BLEU. Table 2 shows detailed n-gram BLEU precision scores for n=1,2,3,4. Our system outperforms the baseline on all precision scores and for all numbers of training sentences. Other evaluation measures. Table 3 reports the results for five evaluation measures: BLEU and NIST 11b, TER 0.7.25 (Snover et al., 2006), METEOR 1.0 (Lavie and Denkowski, 2009), and TESLA (Liu et al., 2010). Our system consistently outperforms the baseline for all measures. Example translations. Table 4 shows two translation examples. In the first example, the reduplication bekalan-bekalan (‘supplies’) is an unknown word, and was left untranslated by the baseline system. It was not a problem for our system though, which first paraphrased it as bekalan and then translated it as supply. Even though this is still wrong (we need the plural supplies), it is arguably preferable to passing the word untranslated; it also allowed for a better translation of the surrounding context. 1304 src : Mercy Relie</context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2010</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010. TESLA: Translation evaluation of sentences with linear-programming-based analysis. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 354– 359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Improved statistical machine translation for resource-poor languages using related resource-rich languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09,</booktitle>
<pages>1358--1367</pages>
<contexts>
<context position="17810" citStr="Nakov and Ng, 2009" startWordPosition="2749" endWordPosition="2752">the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignments in both parts of the bi-text (Nakov and Ng, 2009). We avoid the above issues by adopting a sentencelevel paraphrasing approach. Following the general framework proposed in (Nakov, 2008), we first create multiple paraphrased versions of the sourceside sentences of the training bi-text. Then, each paraphrased source sentence is paired with its original translation. This augmented bi-text is wordaligned and a phrase table T′ is built from it, which is merged with a phrase table T for the original bitext. The merged table contains all phrase entries from T, and the entries for the phrase pairs from T′ that are not in T. Following Nakov and Ng (2</context>
</contexts>
<marker>Nakov, Ng, 2009</marker>
<rawString>Preslav Nakov and Hwee Tou Ng. 2009. Improved statistical machine translation for resource-poor languages using related resource-rich languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09, pages 1358– 1367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Improved statistical machine translation using monolingual paraphrases.</title>
<date>2008</date>
<booktitle>In Proceedings of the 18th European Conference on Artificial Intelligence, ECAI ’08,</booktitle>
<pages>338--342</pages>
<contexts>
<context position="17946" citStr="Nakov, 2008" startWordPosition="2772" endWordPosition="2773">ach word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignments in both parts of the bi-text (Nakov and Ng, 2009). We avoid the above issues by adopting a sentencelevel paraphrasing approach. Following the general framework proposed in (Nakov, 2008), we first create multiple paraphrased versions of the sourceside sentences of the training bi-text. Then, each paraphrased source sentence is paired with its original translation. This augmented bi-text is wordaligned and a phrase table T′ is built from it, which is merged with a phrase table T for the original bitext. The merged table contains all phrase entries from T, and the entries for the phrase pairs from T′ that are not in T. Following Nakov and Ng (2009), we add up to three additional indicator features (taking the values 0.5 and 1) to each entry in the merged phrase table, showing w</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008. Improved statistical machine translation using monolingual paraphrases. In Proceedings of the 18th European Conference on Artificial Intelligence, ECAI ’08, pages 338–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="23423" citStr="Och and Ney, 2003" startWordPosition="3676" endWordPosition="3679">sed 1,420 sentences with 28.8K Malay word tokens, which were translated by three human translators, yielding translations of 32.8K, 32.4K, and 32.9K English word tokens, respectively. For development, we used 2,000 sentence pairs of 63.4K English and 58.5K Malay word tokens. 4.2 General Experimental Setup First, we tokenized and lowercased all datasets: training, development, and testing. We then built directed word-level alignments for the training bitext for English-*Malay and for Malay-*English using IBM model 4 (Brown et al., 1993), which we symmetrized using the intersect+grow heuristic (Och and Ney, 2003). Next, we extracted phraselevel translation pairs of maximum length seven, which we scored and used to build a phrase table where each phrase pair is associated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on the de</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL ’03,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="18746" citStr="Och, 2003" startWordPosition="2920" endWordPosition="2921">ented bi-text is wordaligned and a phrase table T′ is built from it, which is merged with a phrase table T for the original bitext. The merged table contains all phrase entries from T, and the entries for the phrase pairs from T′ that are not in T. Following Nakov and Ng (2009), we add up to three additional indicator features (taking the values 0.5 and 1) to each entry in the merged phrase table, showing whether the entry came from (1) T only, (2) T′ only, or (3) both T and T′. We also try using the first one or two features only. We set all feature weights using minimum error rate training (Och, 2003), and we optimize their number (one, two, or three) on the development dataset.5 5In theory, we should re-normalize the probabilities; in practice, this is not strictly required by the log-linear SMT model. Each of our paraphrased sentences differs from its original sentence by a single word, which prevents combinatorial explosions: on average, we generate 14 paraphrased versions per input sentence. It further ensures that the paraphrased parts of the sentences will not dominate the word alignments or the phrase pairs, and that there would be sufficient interaction at word alignment time betwe</context>
<context position="24129" citStr="Och, 2003" startWordPosition="3782" endWordPosition="3783">used to build a phrase table where each phrase pair is associated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on the development dataset by optimizing BLEU (Papineni et al., 2002) using minimum error rate training (Och, 2003), and we plugged them in a beam search decoder (Koehn et al., 2007) to translate the Malay test sentences to English. Finally, we detokenized the output, and we evaluated it against the three reference translations. 4.3 Systems Using the above general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’ channel model.6 This is the model of Dyer (20</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL ’03, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="24083" citStr="Papineni et al., 2002" startWordPosition="3773" endWordPosition="3776">lation pairs of maximum length seven, which we scored and used to build a phrase table where each phrase pair is associated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on the development dataset by optimizing BLEU (Papineni et al., 2002) using minimum error rate training (Och, 2003), and we plugged them in a beam search decoder (Koehn et al., 2007) to translate the Malay test sentences to English. Finally, we detokenized the output, and we evaluated it against the three reference translations. 4.3 Systems Using the above general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL ’02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="1743" citStr="Quirk et al., 2005" startWordPosition="241" endWordPosition="244">ntence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. Ignoring morphology was fine as long as the main research interest remained focused on languages with limited (e.g., English, French, Spanish) or minimal (e.g., Chinese) morphology. Since the attention shifted to languages like Arabic, however, the importance of morphology became obvious and several approaches to handle it have been proposed. Depending on the particular language of interest, researchers have paid attention to word inflections and clitics, e.g., for Arabic, Finnish, and </context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Machine Translation in the Americas, AMTA ’06,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="29255" citStr="Snover et al., 2006" startWordPosition="4584" endWordPosition="4587">s yields even bigger further improvements for lattice + sent-par + word-par + phrase-par. Overall, our morphological paraphrases yield statistically significant improvements (p &lt; 0.01) in BLEU, according to Collins et al. (2005)’s sign test, for bi-texts as large as 320,000 sentence pairs. A closer look at BLEU. Table 2 shows detailed n-gram BLEU precision scores for n=1,2,3,4. Our system outperforms the baseline on all precision scores and for all numbers of training sentences. Other evaluation measures. Table 3 reports the results for five evaluation measures: BLEU and NIST 11b, TER 0.7.25 (Snover et al., 2006), METEOR 1.0 (Lavie and Denkowski, 2009), and TESLA (Liu et al., 2010). Our system consistently outperforms the baseline for all measures. Example translations. Table 4 shows two translation examples. In the first example, the reduplication bekalan-bekalan (‘supplies’) is an unknown word, and was left untranslated by the baseline system. It was not a problem for our system though, which first paraphrased it as bekalan and then translated it as supply. Even though this is still wrong (we need the plural supplies), it is arguably preferable to passing the word untranslated; it also allowed for a</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the Association for Machine Translation in the Americas, AMTA ’06, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Modelling lexical redundancy for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, COLINGACL ’06,</booktitle>
<pages>969--976</pages>
<contexts>
<context position="32330" citStr="Talbot and Osborne, 2006" startWordPosition="5070" endWordPosition="5073">em, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (</context>
</contexts>
<marker>Talbot, Osborne, 2006</marker>
<rawString>David Talbot and Miles Osborne. 2006. Modelling lexical redundancy for machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, COLINGACL ’06, pages 969–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Pivot language approach for phrase-based statistical machine translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="14648" citStr="Wu and Wang, 2007" startWordPosition="2230" endWordPosition="2233">′ is a good paraphrase of w. Note that multi-word paraphrases, e.g., resulting from clitic segmentation, are encoded using a sequence of arcs; in such cases, we assign Pr(w′|w) to the first arc, and 1 to each subsequent arc. We calculate the probability Pr(w′|w) using the training Malay-English bi-text, which we align at the word level using IBM model 4 (Brown et al., 1993), and we observe which English words w and w′ are aligned to. More precisely, we use pivoting to estimate the probability Pr(w′|w) as follows: Pr(w′|w) = Ei Pr(w′|w, ei)Pr(ei|w) Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007), we make the simplifying assumption that w′ is conditionally independent of w given ei, thus obtaining the following expression: Pr(w′|w) = Ei Pr(w′|ei)Pr(ei|w) We estimate the probability Pr(ei|w) directly from the word-aligned training bi-text as follows: Pr(ei|w) = ##(w,)j) where #(x, e) is the number of times the Malay word x is aligned to the English word e. Estimating Pr(w′|ei) cannot be done directly since w′ might not be present on the Malay side of the training bi-text, e.g., because it is a multi-token sequence generated by clitic segmentation. Thus, we think of w′ as a pseudoword t</context>
</contexts>
<marker>Wu, Wang, 2007</marker>
<rawString>Hua Wu and Haifeng Wang. 2007. Pivot language approach for phrase-based statistical machine translation. Machine Translation, 21(3):165–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mei Yang</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Phrase-based backoff models for machine translation of highly inflected languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics, EACL ’06,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="32198" citStr="Yang and Kirchhoff, 2006" startWordPosition="5052" endWordPosition="5055">pan (‘life/existence’) are derivational forms ofjalan (‘go’) and hidup (‘life/living’), respectively. Thus, in the paraphrasing system, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006)</context>
</contexts>
<marker>Yang, Kirchhoff, 2006</marker>
<rawString>Mei Yang and Katrin Kirchhoff. 2006. Phrase-based backoff models for machine translation of highly inflected languages. In Proceedings of the European Chapter of the Association for Computational Linguistics, EACL ’06, pages 41–48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>