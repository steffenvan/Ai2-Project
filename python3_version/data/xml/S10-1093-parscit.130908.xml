<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.034637">
<title confidence="0.96226">
Kyoto: An Integrated System for Specific Domain WSD
</title>
<author confidence="0.971761">
Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle Monica Monachini
</author>
<affiliation confidence="0.958847">
University of the Basque Country
</affiliation>
<email confidence="0.927571">
a.soroa@ehu.es
</email>
<author confidence="0.996455">
Jessie Lo, Shu-Kai Hsieh
</author>
<affiliation confidence="0.996507">
National Taiwan Normal University
</affiliation>
<email confidence="0.993355">
shukai@ntnu.edu.tw
</email>
<sectionHeader confidence="0.993782" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987894736842">
This document describes the prelimi-
nary release of the integrated Kyoto sys-
tem for specific domain WSD. The sys-
tem uses concept miners (Tybots) to ex-
tract domain-related terms and produces
a domain-related thesaurus, followed by
knowledge-based WSD based on word-
net graphs (UKB). The resulting system
can be applied to any language with a
lexical knowledge base, and is based on
publicly available software and resources.
Our participation in Semeval task #17 fo-
cused on producing running systems for
all languages in the task, and we attained
good results in all except Chinese. Due
to the pressure of the time-constraints in
the competition, the system is still under
development, and we expect results to im-
prove in the near future.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999662411764706">
In this paper we describe the participation of the
integrated Kyoto system on the “SemEval-2010
task #17: All-words Word Sense Disambigua-
tion on a Specific Domain” task (Agirre et al.,
2010). The goal of our participation was to eval-
uate the preliminary release of the integrated sys-
tem for specific domain WSD developed for the
Kyoto project1. Besides, we wanted to test the
performance of our domain specific WSD system
(Agirre et al., 2009) on this test set, and to inte-
grate the thesaurus construction software (Tybots)
developed for the project. The system can be run
for any language and domain if provided with a
lexical knowledge base and some background doc-
uments on the domain.
We will first present the components of our sys-
tem, followed by the experimental design and the
</bodyText>
<footnote confidence="0.969765">
1http://www.kyoto-project.eu
</footnote>
<note confidence="0.65862">
Istituto di Linguistica Computazionale
</note>
<email confidence="0.840721">
monica.monachini@ilc.cnr.it
</email>
<author confidence="0.6391205">
Wauter Bosma, Piek Vossen
Vrije Universiteit
</author>
<email confidence="0.864">
{p.vossen,w.bosma}@let.vu.nl
</email>
<bodyText confidence="0.959978">
results. Finally, the conclusions are presented.
</bodyText>
<sectionHeader confidence="0.9856965" genericHeader="method">
2 The Kyoto System for Domain Specific
WSD
</sectionHeader>
<bodyText confidence="0.9993455">
We will present in turn UKB, the Tybots, and the
lexical knowledge-bases used.
</bodyText>
<subsectionHeader confidence="0.900056">
2.1 UKB
</subsectionHeader>
<bodyText confidence="0.999961303030303">
UKB is a knowledge-based unsupervised WSD
system which exploits the structure of an under-
lying Language Knowledge Base (LKB) and finds
the most relevant concepts given an input con-
text (Agirre and Soroa, 2009). UKB starts by tak-
ing the LKB as a graph of concepts G = (V, E)
with a set of vertices V derived from LKB con-
cepts and a set of edges E representing relations
among them. Giving an input context, UKB ap-
plies the so called Personalized PageRank (Haveli-
wala, 2002) over it to obtain the most representa-
tive senses for the context.
PageRank (Brin and Page, 1998) is a method
for scoring the vertices V of a graph according
to each node’s structural importance. The algo-
rithm can be viewed as random walk process that
postulate the existence of a particle that randomly
traverses the graph, but at any time may jump to
a new vertex with a given damping factor (also
called teleport probability). After PageRank cal-
culation, the final weight of node i represents the
proportion of time that a random particle spends
visiting node i after a sufficiently long time. In
standard PageRank, the teleport vector is chosen
uniformly, whereas for Personalized PageRank it
is chosen from a nonuniform distribution of nodes,
specified by a teleport vector.
UKB concentrates the initial probability mass
of the teleport vector in the words occurring in
the context of the target word, causing all random
jumps on the walk to return to these words and
thus assigning a higher rank to the senses linked to
these words. Moreover, the high rank of the words
</bodyText>
<page confidence="0.969154">
417
</page>
<bodyText confidence="0.950659470588235">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 417–420,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
spreads through the links in the graph and make
all the nodes in its vicinity also receive high ranks.
Given a target word, the system checks which is
the relative ranking of its senses, and the WSD
system would output the one ranking highest.
UKB is very flexible and can be use to perform
WSD on different settings, depending on the con-
text used for disambiguating a word instance. In
this paper we use it to perform general and do-
main specific WSD, as shown in section 3. PageR-
ank is calculated by applying an iterative algo-
rithm until convergence below a given threshold
is achieved. Following usual practice, we used a
damping value of 0.85 and set the threshold value
at 0.001. We did not optimize these parameters.
</bodyText>
<subsectionHeader confidence="0.98689">
2.2 Tybots
</subsectionHeader>
<bodyText confidence="0.999599285714286">
Tybots (Term Yielding Robots) are text mining
software that mine domain terms from corpus
(e.g. web pages), organizing them in a hierar-
chical structure, connecting them to wordnets and
ontologies to create a semantic model for the do-
main (Bosma and Vossen, 2010). The software is
freely available using Subversion 2. Tybots try to
establish a view on the terminology of the domain
which is as complete as possible, discovering rela-
tions between terms and ranking terms by domain
relevance.
Preceding term extraction, we perform tok-
enization, part-of-speech tagging and lemmatiza-
tion, which is stored in Kyoto Annotation For-
mat (KAF) (Bosma et al., 2009). Tybots work
through KAF documents, acquire domain relevant
terms based on the syntactic features, gather co-
occurrence statistics to decide which terms are sig-
nificant in the domain and produce a thesaurus
with sets of related words. Section 3.3 describes
the specific settings that we used.
</bodyText>
<subsectionHeader confidence="0.999751">
2.3 Lexical Knowledge bases
</subsectionHeader>
<bodyText confidence="0.999737333333333">
We used the following wordnets, as suggested by
the organizers:
WN30g: English WordNet 3.0 with gloss relations
(Fellbaum, 1998).
Dutch: The Dutch LKB is part of the Cor-
netto database version 1.3 (Vossen et al., 2008).
The Cornetto database can be obtained from
the Dutch/Flanders Taalunie3. Cornetto com-
prises taxonomic relations and equivalence rela-
</bodyText>
<footnote confidence="0.9993145">
2http://kyoto.let.vu.nl/svn/kyoto/trunk
3http://www.inl.nl/nl/lexica/780
</footnote>
<table confidence="0.9997869">
#entries #synsets #rels. #WN30g
Monolingual
Chinese 8,186 14,243 20,433 20,584
Dutch 83,812 70,024 224,493 83,669
Italian 46,724 49,513 65,567 52,524
WN30g 147,306 117,522 525,351 n/a
Bilingual
Chinese-eng 8,186 141,561 566,368
Dutch-eng 83,812 188,511 833,513
Italian-eng 46,724 167,094 643,442
</table>
<tableCaption confidence="0.9514745">
Table 1: Wordnets and their sizes (entries, synsets,
relations and links to WN30g).
</tableCaption>
<bodyText confidence="0.996649347826087">
tions from both WordNet 2.0 and 3.0. Cornetto
concepts are mapped to English WordNet 3.0.
Italian: Italwordnet (Roventini et al., 2003) was
created in the framework of the EuroWordNet,
employs the same set of semantic relations used
in EuroWordNet, and includes links to WordNet
3.0 synsets.
Chinese: The Chinese WordNet (Version 1.6) is
now partially open to the public4 (Tsai et al.,
2001). The Chinese WordNet is also mapped to
WordNet 3.0.
Table 1 shows the sizes of the graphs created
using each LKB as a source. The upper part shows
the number of lexical entries, synsets and relations
of each LKB. It also depicts the number of links to
English WordNet 3.0 synsets.
In addition, we also created bilingual graphs for
Dutch, Italian and Chinese, comprising the orig-
inal monolingual LKB, the links to WordNet 3.0
and WordNet 3.0 itself. We expected this richer
graphs to perform better performance. The sizes
of the bilingual graphs are shown in the lower side
of Table 1.
</bodyText>
<sectionHeader confidence="0.997359" genericHeader="method">
3 Experimental setting
</sectionHeader>
<bodyText confidence="0.99994475">
All test documents were lemmatized and PoS-
tagged using the linguistic processors available
within the Kyoto project. In this section we de-
scribe the submitted runs.
</bodyText>
<subsectionHeader confidence="0.996241">
3.1 UKB parameters
</subsectionHeader>
<bodyText confidence="0.999584166666667">
We use UKB with the default parameters. In par-
ticular, we don’t use dictionary weights, which in
the case of English come from annotated corpora.
This is done in order to make the system fully un-
supervised. It’s also worth mentioning that in the
default setting parts of speech were not used.
</bodyText>
<footnote confidence="0.978383">
4http://cwn.ling.sinica.edu.tw
</footnote>
<page confidence="0.931139">
418
</page>
<table confidence="0.995209769230769">
RANK RUN P R R-NOUN R-VERB
Chinese
- ]sense 0.562 0.562 0.589 0.518
1 Best 0.559 0.559 - -
- Random 0.321 0.321 0.326 0.312
4 kyoto-3 0.322 0.296 0.257 0.360
3 kyoto-2 0.342 0.285 0.251 0.342
5 kyoto-1 0.310 0.258 0.256 0.261
Dutch
1 kyoto-3 0.526 0.526 0.575 0.450
2 kyoto-2 0.519 0.519 0.561 0.454
- ]sense 0.480 0.480 0.600 0.291
3 kyoto-1 0.465 0.465 0.505 0.403
- Random 0.328 0.328 0.350 0.293
English
1 Best 0.570 0.555 - -
- ]sense 0.505 0.505 0.519 0.454
10 kyoto-2 0.481 0.481 0.487 0.462
22 kyoto-1 0.384 0.384 0.382 0.391
- Random 0.232 0.232 0.253 0.172
Italian
1 kyoto-3 0.529 0.529 0.530 0.528
2 kyoto-2 0.521 0.521 0.522 0.519
3 kyoto-1 0.496 0.496 0.507 0.468
- ]sense 0.462 0.462 0.472 0.437
- Random 0.294 0.294 0.308 0.257
</table>
<tableCaption confidence="0.921529">
Table 2: Overall results of our runs, including pre-
</tableCaption>
<bodyText confidence="0.95780575">
cision (P) and recall (R), overall and for each PoS.
We include the First Sense (1sense) and random
baselines, as well as the best run, as provided by
the organizers.
</bodyText>
<subsectionHeader confidence="0.991721">
3.2 Run1: UKB using context
</subsectionHeader>
<bodyText confidence="0.999962863636364">
The first run is an application of the UKB tool in
the standard setting, as described in (Agirre and
Soroa, 2009). Given the input text, we split it in
sentences, and we disambiguate each sentence at a
time. We extract the lemmas which have an entry
in the LKB and then apply Personalized PageR-
ank over all of them, obtaining a score for every
concept of the LKB. To disambiguate the words in
the sentence we just choose its associated concept
(sense) with maximum score.
In our experiments we build a context of at least
20 content words for each sentence to be disam-
biguated, taking the sentences immediately before
when necessary. UKB allows two main methods
of disambiguation, namely ppr and ppr w2w. We
used the latter method, as it has been shown to per-
form best.
In this setting we used the monolingual graphs
for each language (cf. section 2.3). Note that
in this run there is no domain adaptation, it thus
serves us as a baseline for assessing the benefits of
applying domain adaptation techniques.
</bodyText>
<subsectionHeader confidence="0.983325">
3.3 Run2: UKB using related words
</subsectionHeader>
<bodyText confidence="0.99998078125">
Instead of disambiguating words using their con-
text of occurrence, we follow the method de-
scribed in (Agirre et al., 2009). The idea is to first
obtain a list of related words for each of the tar-
get words, as collected from a domain corpus. On
a second step each target word is disambiguated
using the N most related words as context (see
below). For instance, in order to disambiguate
the word environment, we would not take into
account the context of occurrence (as in Section
3.2), but we would use the list of most related
words in the thesaurus (e.g. “biodiversity, agri-
culture, ecosystem, nature, life, climate, ...”). Us-
ing UKB over these contexts we obtain the most
predominant sense for each target word in the do-
main(McCarthy et al., 2007), which is used to la-
bel all occurrences of the target word in the test
dataset.
In order to build the thesaurus with the lists of
related words, we used Tybots (c.f. section 2.2),
one for each corpus of the evaluation dataset, i.e.
Chinese, Dutch, English, and Italian. We used the
background documents provided by the organiz-
ers, which we processed using the linguistic pro-
cessors of the project to obtain the documents in
KAF. We used the Tybots with the following set-
tings. We discarded co-occurring words with fre-
quencies below 105. Distributional similarity was
computed using (Lin, 1998). Finally, we used up
to 50 related words for each target word.
As in run1, we used the monolingual graphs for
the LKBs in each language.
</bodyText>
<subsectionHeader confidence="0.9823065">
3.4 Run3: UKB using related words and
bilingual graphs
</subsectionHeader>
<bodyText confidence="0.9999095">
The third run is exactly the same as run2, except
that we used bilingual graphs instead of monolin-
gual ones for all languages other than English (cf.
section 2.3). There is no run3 for English.
</bodyText>
<sectionHeader confidence="0.99987" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.959493285714286">
Table 2 shows the results of our system on the
different languages. We will analyze different as-
pects of the results in turn.
Domain adaptation: Using Personalized Pager-
ank over related words (run2 and run3) con-
sistently outperforms the standard setting (run1)
in all languages. This result is consistent with
</bodyText>
<footnote confidence="0.9967555">
5In the case of Dutch we did not use any threshold due to
the small size of the background corpus.
</footnote>
<page confidence="0.997681">
419
</page>
<bodyText confidence="0.999965066666667">
our previous work on English (Agirre et al.,
2009), and shows that domain adaptation works
for knowledge-based systems.
Monolingual vs. Bilingual graphs: As ex-
pected, we obtained better results using the bilin-
gual graphs (run3) than with monolingual graphs
(run2), showing that the English WordNet has a
richer set of relations, and that those relations can
be successfully ported to other languages. This
confirms that aligning different wordnets at the
synset level is highly beneficial.
Overall results: the results of our runs are highly
satisfactory. In two languages (Dutch and Ital-
ian) our best runs perform better than the first
sense baseline, which is typically hard to beat for
knowledge-based systems. In English, our system
performs close but below the first sense baseline,
and in Chinese our method performed below the
random baseline.
The poor results obtained for Chinese can be
due the LKB topology; an analysis over the graph
shows that it is formed by a large number of
small components, unrelated with each other. This
’flat’ structure heavily penalizes the graph based
method, which is many times unable to discrimi-
nate among the concepts of a word. We are cur-
rently inspecting the results, and we don’t discard
bugs, due to the preliminary status of our software.
In particular, we need to re-examine the output of
the Tybot for Chinese.
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99997816">
This paper describes the results of the prelimi-
nary release of he integrated Kyoto system for do-
main specific WSD. It comprises Tybots to con-
struct a domain-related thesaurus, and UKB for
knowledge-based WSD based on wordnet graphs.
We applied our system to all languages in the
dataset, obtaining good results. In fact, our sys-
tem can be applied to any language with a lexical
knowledge base, and is based on publicly available
software and resources. We used the wordnets and
background texts provided by the organizers of the
task.
Our results show that we were succesful in
adapting our system to the domain, as we man-
aged to beat the first sense baseline in two lan-
guages. Our results also show that adding the En-
glish WordNet to the other language wordnets via
the available links is beneficial.
Our participation focused on producing running
systems for all languages in the task, and we at-
tained good results in all except Chinese. Due to
the pressure and the time-constraints in the com-
petition, the system is still under development. We
are currently revising our system for bugs and fine-
tuning it.
</bodyText>
<sectionHeader confidence="0.995497" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9979288">
This work task is partially funded by the Eu-
ropean Commission (KYOTO ICT-2007-211423),
the Spanish Research Department (KNOW-2
TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
</bodyText>
<sectionHeader confidence="0.999048" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999925840909091">
E. Agirre and A. Soroa. 2009. Personalizing pagerank for
word sense disambiguation. In Proceedings of EACL09,
pages 33–41. Association for Computational Linguistics.
E. Agirre, O. L´opez de Lacalle, and A. Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.”.
E. Agirre, O. L´opez de Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense dis-
ambiguation on a specific domain. In Same volume.
W. E. Bosma and P. Vossen. 2010. Bootstrapping language
neutral term extraction. In Proceedings of LREC2010,
May.
W. E. Bosma, P. Vossen, A. Soroa, G. Rigau, M. Tesconi,
A. Marchetti, M. Monachini, and C. Aliprandi. 2009.
KAF: a generic semantic annotation format. In Proceed-
ings of the GL2009 Workshop on Semantic Annotation.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks and
ISDN Systems, 30(1-7).
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW
’02: Proceedings of the 11th international conference on
WWW, pages 517–526, New York, NY, USA. ACM.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of ACL98, Montreal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2007.
Unsupervised acquisition of predominant word senses.
Computational Linguistics, 33(4).
A. Roventini, A. Alonge, F. Bertagna, N. Calzolari, J. Can-
cila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza,
and A. Zampolli. 2003. Italwordnet: building a large
semantic database for the automatic treatment of Italian.
Linguistica Computazionale, Special Issue (XVIII-XIX),
pages 745–791.
B.S. Tsai, C.R. Huang, S.c. Tseng, J.Y. Lin, K.J. Chen, and
Y.S. Chuang. 2001. Definition and tests for lexical se-
mantic relations in Chinese. In Proceedings CLSW 2001.
P. Vossen, I. Maks, R. Segers, H. van der Vliet, and H. van
Zutphen. 2008. The cornetto database: the architecture
and alignment issues. In Proceedings GWC 2008, pages
485–506.
</reference>
<page confidence="0.998373">
420
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.583386">
<title confidence="0.999415">Kyoto: An Integrated System for Specific Domain WSD</title>
<author confidence="0.998873">Aitor Soroa</author>
<author confidence="0.998873">Eneko Agirre</author>
<author confidence="0.998873">Oier Lopez de_Lacalle Monica Monachini</author>
<affiliation confidence="0.990971">University of the Basque Country</affiliation>
<email confidence="0.691469">a.soroa@ehu.es</email>
<author confidence="0.998062">Jessie Lo</author>
<author confidence="0.998062">Shu-Kai Hsieh</author>
<affiliation confidence="0.999929">National Taiwan Normal University</affiliation>
<email confidence="0.94604">shukai@ntnu.edu.tw</email>
<abstract confidence="0.9946577">This document describes the preliminary release of the integrated Kyoto system for specific domain WSD. The system uses concept miners (Tybots) to extract domain-related terms and produces a domain-related thesaurus, followed by knowledge-based WSD based on wordnet graphs (UKB). The resulting system can be applied to any language with a lexical knowledge base, and is based on publicly available software and resources. Our participation in Semeval task #17 focused on producing running systems for all languages in the task, and we attained good results in all except Chinese. Due to the pressure of the time-constraints in the competition, the system is still under development, and we expect results to improve in the near future.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL09,</booktitle>
<pages>33--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2346" citStr="Agirre and Soroa, 2009" startWordPosition="362" endWordPosition="365">nents of our system, followed by the experimental design and the 1http://www.kyoto-project.eu Istituto di Linguistica Computazionale monica.monachini@ilc.cnr.it Wauter Bosma, Piek Vossen Vrije Universiteit {p.vossen,w.bosma}@let.vu.nl results. Finally, the conclusions are presented. 2 The Kyoto System for Domain Specific WSD We will present in turn UKB, the Tybots, and the lexical knowledge-bases used. 2.1 UKB UKB is a knowledge-based unsupervised WSD system which exploits the structure of an underlying Language Knowledge Base (LKB) and finds the most relevant concepts given an input context (Agirre and Soroa, 2009). UKB starts by taking the LKB as a graph of concepts G = (V, E) with a set of vertices V derived from LKB concepts and a set of edges E representing relations among them. Giving an input context, UKB applies the so called Personalized PageRank (Haveliwala, 2002) over it to obtain the most representative senses for the context. PageRank (Brin and Page, 1998) is a method for scoring the vertices V of a graph according to each node’s structural importance. The algorithm can be viewed as random walk process that postulate the existence of a particle that randomly traverses the graph, but at any t</context>
<context position="8992" citStr="Agirre and Soroa, 2009" startWordPosition="1466" endWordPosition="1469">1 0.481 0.487 0.462 22 kyoto-1 0.384 0.384 0.382 0.391 - Random 0.232 0.232 0.253 0.172 Italian 1 kyoto-3 0.529 0.529 0.530 0.528 2 kyoto-2 0.521 0.521 0.522 0.519 3 kyoto-1 0.496 0.496 0.507 0.468 - ]sense 0.462 0.462 0.472 0.437 - Random 0.294 0.294 0.308 0.257 Table 2: Overall results of our runs, including precision (P) and recall (R), overall and for each PoS. We include the First Sense (1sense) and random baselines, as well as the best run, as provided by the organizers. 3.2 Run1: UKB using context The first run is an application of the UKB tool in the standard setting, as described in (Agirre and Soroa, 2009). Given the input text, we split it in sentences, and we disambiguate each sentence at a time. We extract the lemmas which have an entry in the LKB and then apply Personalized PageRank over all of them, obtaining a score for every concept of the LKB. To disambiguate the words in the sentence we just choose its associated concept (sense) with maximum score. In our experiments we build a context of at least 20 content words for each sentence to be disambiguated, taking the sentences immediately before when necessary. UKB allows two main methods of disambiguation, namely ppr and ppr w2w. We used </context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>E. Agirre and A. Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of EACL09, pages 33–41. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O L´opez de Lacalle</author>
<author>A Soroa</author>
</authors>
<title>Knowledge-based wsd on specific domains: Performing better than generic supervised wsd.</title>
<date>2009</date>
<booktitle>In Proceedigns of IJCAI.</booktitle>
<pages>1501--1506</pages>
<marker>Agirre, de Lacalle, Soroa, 2009</marker>
<rawString>E. Agirre, O. L´opez de Lacalle, and A. Soroa. 2009. Knowledge-based wsd on specific domains: Performing better than generic supervised wsd. In Proceedigns of IJCAI. pp. 1501-1506.”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>O L´opez de Lacalle</author>
<author>C Fellbaum</author>
<author>S K Hsieh</author>
<author>M Tesconi</author>
<author>M Monachini</author>
<author>P Vossen</author>
<author>R Segers</author>
</authors>
<title>Semeval-2010 task 17: All-words word sense disambiguation on a specific domain. In Same volume.</title>
<date>2010</date>
<marker>Agirre, de Lacalle, Fellbaum, Hsieh, Tesconi, Monachini, Vossen, Segers, 2010</marker>
<rawString>E. Agirre, O. L´opez de Lacalle, C. Fellbaum, S.K. Hsieh, M. Tesconi, M. Monachini, P. Vossen, and R. Segers. 2010. Semeval-2010 task 17: All-words word sense disambiguation on a specific domain. In Same volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W E Bosma</author>
<author>P Vossen</author>
</authors>
<title>Bootstrapping language neutral term extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC2010,</booktitle>
<contexts>
<context position="4862" citStr="Bosma and Vossen, 2010" startWordPosition="795" endWordPosition="798">d instance. In this paper we use it to perform general and domain specific WSD, as shown in section 3. PageRank is calculated by applying an iterative algorithm until convergence below a given threshold is achieved. Following usual practice, we used a damping value of 0.85 and set the threshold value at 0.001. We did not optimize these parameters. 2.2 Tybots Tybots (Term Yielding Robots) are text mining software that mine domain terms from corpus (e.g. web pages), organizing them in a hierarchical structure, connecting them to wordnets and ontologies to create a semantic model for the domain (Bosma and Vossen, 2010). The software is freely available using Subversion 2. Tybots try to establish a view on the terminology of the domain which is as complete as possible, discovering relations between terms and ranking terms by domain relevance. Preceding term extraction, we perform tokenization, part-of-speech tagging and lemmatization, which is stored in Kyoto Annotation Format (KAF) (Bosma et al., 2009). Tybots work through KAF documents, acquire domain relevant terms based on the syntactic features, gather cooccurrence statistics to decide which terms are significant in the domain and produce a thesaurus wi</context>
</contexts>
<marker>Bosma, Vossen, 2010</marker>
<rawString>W. E. Bosma and P. Vossen. 2010. Bootstrapping language neutral term extraction. In Proceedings of LREC2010, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W E Bosma</author>
<author>P Vossen</author>
<author>A Soroa</author>
<author>G Rigau</author>
<author>M Tesconi</author>
<author>A Marchetti</author>
<author>M Monachini</author>
<author>C Aliprandi</author>
</authors>
<title>KAF: a generic semantic annotation format.</title>
<date>2009</date>
<booktitle>In Proceedings of the GL2009 Workshop on Semantic Annotation.</booktitle>
<contexts>
<context position="5253" citStr="Bosma et al., 2009" startWordPosition="857" endWordPosition="860">text mining software that mine domain terms from corpus (e.g. web pages), organizing them in a hierarchical structure, connecting them to wordnets and ontologies to create a semantic model for the domain (Bosma and Vossen, 2010). The software is freely available using Subversion 2. Tybots try to establish a view on the terminology of the domain which is as complete as possible, discovering relations between terms and ranking terms by domain relevance. Preceding term extraction, we perform tokenization, part-of-speech tagging and lemmatization, which is stored in Kyoto Annotation Format (KAF) (Bosma et al., 2009). Tybots work through KAF documents, acquire domain relevant terms based on the syntactic features, gather cooccurrence statistics to decide which terms are significant in the domain and produce a thesaurus with sets of related words. Section 3.3 describes the specific settings that we used. 2.3 Lexical Knowledge bases We used the following wordnets, as suggested by the organizers: WN30g: English WordNet 3.0 with gloss relations (Fellbaum, 1998). Dutch: The Dutch LKB is part of the Cornetto database version 1.3 (Vossen et al., 2008). The Cornetto database can be obtained from the Dutch/Flander</context>
</contexts>
<marker>Bosma, Vossen, Soroa, Rigau, Tesconi, Marchetti, Monachini, Aliprandi, 2009</marker>
<rawString>W. E. Bosma, P. Vossen, A. Soroa, G. Rigau, M. Tesconi, A. Marchetti, M. Monachini, and C. Aliprandi. 2009. KAF: a generic semantic annotation format. In Proceedings of the GL2009 Workshop on Semantic Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="2706" citStr="Brin and Page, 1998" startWordPosition="431" endWordPosition="434">ots, and the lexical knowledge-bases used. 2.1 UKB UKB is a knowledge-based unsupervised WSD system which exploits the structure of an underlying Language Knowledge Base (LKB) and finds the most relevant concepts given an input context (Agirre and Soroa, 2009). UKB starts by taking the LKB as a graph of concepts G = (V, E) with a set of vertices V derived from LKB concepts and a set of edges E representing relations among them. Giving an input context, UKB applies the so called Personalized PageRank (Haveliwala, 2002) over it to obtain the most representative senses for the context. PageRank (Brin and Page, 1998) is a method for scoring the vertices V of a graph according to each node’s structural importance. The algorithm can be viewed as random walk process that postulate the existence of a particle that randomly traverses the graph, but at any time may jump to a new vertex with a given damping factor (also called teleport probability). After PageRank calculation, the final weight of node i represents the proportion of time that a random particle spends visiting node i after a sufficiently long time. In standard PageRank, the teleport vector is chosen uniformly, whereas for Personalized PageRank it </context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30(1-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronical Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5702" citStr="Fellbaum, 1998" startWordPosition="928" endWordPosition="929">e. Preceding term extraction, we perform tokenization, part-of-speech tagging and lemmatization, which is stored in Kyoto Annotation Format (KAF) (Bosma et al., 2009). Tybots work through KAF documents, acquire domain relevant terms based on the syntactic features, gather cooccurrence statistics to decide which terms are significant in the domain and produce a thesaurus with sets of related words. Section 3.3 describes the specific settings that we used. 2.3 Lexical Knowledge bases We used the following wordnets, as suggested by the organizers: WN30g: English WordNet 3.0 with gloss relations (Fellbaum, 1998). Dutch: The Dutch LKB is part of the Cornetto database version 1.3 (Vossen et al., 2008). The Cornetto database can be obtained from the Dutch/Flanders Taalunie3. Cornetto comprises taxonomic relations and equivalence rela2http://kyoto.let.vu.nl/svn/kyoto/trunk 3http://www.inl.nl/nl/lexica/780 #entries #synsets #rels. #WN30g Monolingual Chinese 8,186 14,243 20,433 20,584 Dutch 83,812 70,024 224,493 83,669 Italian 46,724 49,513 65,567 52,524 WN30g 147,306 117,522 525,351 n/a Bilingual Chinese-eng 8,186 141,561 566,368 Dutch-eng 83,812 188,511 833,513 Italian-eng 46,724 167,094 643,442 Table 1:</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronical Lexical Database. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Haveliwala</author>
</authors>
<title>Topic-sensitive pagerank.</title>
<date>2002</date>
<booktitle>In WWW ’02: Proceedings of the 11th international conference on WWW,</booktitle>
<pages>517--526</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2609" citStr="Haveliwala, 2002" startWordPosition="415" endWordPosition="417">are presented. 2 The Kyoto System for Domain Specific WSD We will present in turn UKB, the Tybots, and the lexical knowledge-bases used. 2.1 UKB UKB is a knowledge-based unsupervised WSD system which exploits the structure of an underlying Language Knowledge Base (LKB) and finds the most relevant concepts given an input context (Agirre and Soroa, 2009). UKB starts by taking the LKB as a graph of concepts G = (V, E) with a set of vertices V derived from LKB concepts and a set of edges E representing relations among them. Giving an input context, UKB applies the so called Personalized PageRank (Haveliwala, 2002) over it to obtain the most representative senses for the context. PageRank (Brin and Page, 1998) is a method for scoring the vertices V of a graph according to each node’s structural importance. The algorithm can be viewed as random walk process that postulate the existence of a particle that randomly traverses the graph, but at any time may jump to a new vertex with a given damping factor (also called teleport probability). After PageRank calculation, the final weight of node i represents the proportion of time that a random particle spends visiting node i after a sufficiently long time. In </context>
</contexts>
<marker>Haveliwala, 2002</marker>
<rawString>T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW ’02: Proceedings of the 11th international conference on WWW, pages 517–526, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="11265" citStr="Lin, 1998" startWordPosition="1863" endWordPosition="1864">hy et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105. Distributional similarity was computed using (Lin, 1998). Finally, we used up to 50 related words for each target word. As in run1, we used the monolingual graphs for the LKBs in each language. 3.4 Run3: UKB using related words and bilingual graphs The third run is exactly the same as run2, except that we used bilingual graphs instead of monolingual ones for all languages other than English (cf. section 2.3). There is no run3 for English. 4 Results Table 2 shows the results of our system on the different languages. We will analyze different aspects of the results in turn. Domain adaptation: Using Personalized Pagerank over related words (run2 and r</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of ACL98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Unsupervised acquisition of predominant word senses.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="10671" citStr="McCarthy et al., 2007" startWordPosition="1760" endWordPosition="1764">idea is to first obtain a list of related words for each of the target words, as collected from a domain corpus. On a second step each target word is disambiguated using the N most related words as context (see below). For instance, in order to disambiguate the word environment, we would not take into account the context of occurrence (as in Section 3.2), but we would use the list of most related words in the thesaurus (e.g. “biodiversity, agriculture, ecosystem, nature, life, climate, ...”). Using UKB over these contexts we obtain the most predominant sense for each target word in the domain(McCarthy et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105. Distributional similarity was computed using (Lin, 1998). Fina</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2007</marker>
<rawString>D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2007. Unsupervised acquisition of predominant word senses. Computational Linguistics, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Roventini</author>
<author>A Alonge</author>
<author>F Bertagna</author>
<author>N Calzolari</author>
<author>J Cancila</author>
<author>C Girardi</author>
<author>B Magnini</author>
<author>R Marinelli</author>
<author>M Speranza</author>
<author>A Zampolli</author>
</authors>
<title>Italwordnet: building a large semantic database for the automatic treatment of Italian. Linguistica Computazionale, Special Issue (XVIII-XIX),</title>
<date>2003</date>
<pages>745--791</pages>
<contexts>
<context position="6513" citStr="Roventini et al., 2003" startWordPosition="1035" endWordPosition="1038">omic relations and equivalence rela2http://kyoto.let.vu.nl/svn/kyoto/trunk 3http://www.inl.nl/nl/lexica/780 #entries #synsets #rels. #WN30g Monolingual Chinese 8,186 14,243 20,433 20,584 Dutch 83,812 70,024 224,493 83,669 Italian 46,724 49,513 65,567 52,524 WN30g 147,306 117,522 525,351 n/a Bilingual Chinese-eng 8,186 141,561 566,368 Dutch-eng 83,812 188,511 833,513 Italian-eng 46,724 167,094 643,442 Table 1: Wordnets and their sizes (entries, synsets, relations and links to WN30g). tions from both WordNet 2.0 and 3.0. Cornetto concepts are mapped to English WordNet 3.0. Italian: Italwordnet (Roventini et al., 2003) was created in the framework of the EuroWordNet, employs the same set of semantic relations used in EuroWordNet, and includes links to WordNet 3.0 synsets. Chinese: The Chinese WordNet (Version 1.6) is now partially open to the public4 (Tsai et al., 2001). The Chinese WordNet is also mapped to WordNet 3.0. Table 1 shows the sizes of the graphs created using each LKB as a source. The upper part shows the number of lexical entries, synsets and relations of each LKB. It also depicts the number of links to English WordNet 3.0 synsets. In addition, we also created bilingual graphs for Dutch, Itali</context>
</contexts>
<marker>Roventini, Alonge, Bertagna, Calzolari, Cancila, Girardi, Magnini, Marinelli, Speranza, Zampolli, 2003</marker>
<rawString>A. Roventini, A. Alonge, F. Bertagna, N. Calzolari, J. Cancila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza, and A. Zampolli. 2003. Italwordnet: building a large semantic database for the automatic treatment of Italian. Linguistica Computazionale, Special Issue (XVIII-XIX), pages 745–791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B S Tsai</author>
<author>C R Huang</author>
<author>S c Tseng</author>
<author>J Y Lin</author>
<author>K J Chen</author>
<author>Y S Chuang</author>
</authors>
<title>Definition and tests for lexical semantic relations in Chinese.</title>
<date>2001</date>
<booktitle>In Proceedings CLSW</booktitle>
<contexts>
<context position="6769" citStr="Tsai et al., 2001" startWordPosition="1077" endWordPosition="1080">0g 147,306 117,522 525,351 n/a Bilingual Chinese-eng 8,186 141,561 566,368 Dutch-eng 83,812 188,511 833,513 Italian-eng 46,724 167,094 643,442 Table 1: Wordnets and their sizes (entries, synsets, relations and links to WN30g). tions from both WordNet 2.0 and 3.0. Cornetto concepts are mapped to English WordNet 3.0. Italian: Italwordnet (Roventini et al., 2003) was created in the framework of the EuroWordNet, employs the same set of semantic relations used in EuroWordNet, and includes links to WordNet 3.0 synsets. Chinese: The Chinese WordNet (Version 1.6) is now partially open to the public4 (Tsai et al., 2001). The Chinese WordNet is also mapped to WordNet 3.0. Table 1 shows the sizes of the graphs created using each LKB as a source. The upper part shows the number of lexical entries, synsets and relations of each LKB. It also depicts the number of links to English WordNet 3.0 synsets. In addition, we also created bilingual graphs for Dutch, Italian and Chinese, comprising the original monolingual LKB, the links to WordNet 3.0 and WordNet 3.0 itself. We expected this richer graphs to perform better performance. The sizes of the bilingual graphs are shown in the lower side of Table 1. 3 Experimental</context>
</contexts>
<marker>Tsai, Huang, Tseng, Lin, Chen, Chuang, 2001</marker>
<rawString>B.S. Tsai, C.R. Huang, S.c. Tseng, J.Y. Lin, K.J. Chen, and Y.S. Chuang. 2001. Definition and tests for lexical semantic relations in Chinese. In Proceedings CLSW 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>I Maks</author>
<author>R Segers</author>
<author>H van der Vliet</author>
<author>H van Zutphen</author>
</authors>
<title>The cornetto database: the architecture and alignment issues.</title>
<date>2008</date>
<booktitle>In Proceedings GWC</booktitle>
<pages>485--506</pages>
<marker>Vossen, Maks, Segers, van der Vliet, van Zutphen, 2008</marker>
<rawString>P. Vossen, I. Maks, R. Segers, H. van der Vliet, and H. van Zutphen. 2008. The cornetto database: the architecture and alignment issues. In Proceedings GWC 2008, pages 485–506.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>