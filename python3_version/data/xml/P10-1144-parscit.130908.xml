<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000249">
<title confidence="0.996429">
Coreference Resolution across Corpora:
Languages, Coding Schemes, and Preprocessing Information
</title>
<author confidence="0.98066">
Marta Recasens
</author>
<affiliation confidence="0.973689">
CLiC - University of Barcelona
</affiliation>
<address confidence="0.8862225">
Gran Via 585
Barcelona, Spain
</address>
<email confidence="0.999529">
mrecasens@ub.edu
</email>
<sectionHeader confidence="0.993912" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893529411765">
This paper explores the effect that dif-
ferent corpus configurations have on the
performance of a coreference resolution
system, as measured by MUC, B3, and
CEAF. By varying separately three param-
eters (language, annotation scheme, and
preprocessing information) and applying
the same coreference resolution system,
the strong bonds between system and cor-
pus are demonstrated. The experiments
reveal problems in coreference resolution
evaluation relating to task definition, cod-
ing schemes, and features. They also ex-
pose systematic biases in the coreference
evaluation metrics. We show that system
comparison is only possible when corpus
parameters are in exact agreement.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998761">
The task of coreference resolution, which aims to
automatically identify the expressions in a text that
refer to the same discourse entity, has been an in-
creasing research topic in NLP ever since MUC-6
made available the first coreferentially annotated
corpus in 1995. Most research has centered around
the rules by which mentions are allowed to corefer,
the features characterizing mention pairs, the algo-
rithms for building coreference chains, and coref-
erence evaluation methods. The surprisingly im-
portant role played by different aspects of the cor-
pus, however, is an issue to which little attention
has been paid. We demonstrate the extent to which
a system will be evaluated as performing differ-
ently depending on parameters such as the corpus
language, the way coreference relations are de-
fined in the corresponding coding scheme, and the
nature and source of preprocessing information.
This paper unpacks these issues by running the
same system—a prototype entity-based architec-
</bodyText>
<note confidence="0.8619025">
Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey CA, USA
</note>
<email confidence="0.993393">
hovy@isi.edu
</email>
<bodyText confidence="0.999952682926829">
ture called CISTELL—on different corpus config-
urations, varying three parameters. First, we show
how much language-specific issues affect perfor-
mance when trained and tested on English and
Spanish. Second, we demonstrate the extent to
which the specific annotation scheme (used on the
same corpus) makes evaluated performance vary.
Third, we compare the performance using gold-
standard preprocessing information with that us-
ing automatic preprocessing tools.
Throughout, we apply the three principal coref-
erence evaluation measures in use today: MUC,
B3, and CEAF. We highlight the systematic prefer-
ences of each measure to reward different config-
urations. This raises the difficult question of why
one should use one or another evaluation mea-
sure, and how one should interpret their differ-
ences in reporting changes of performance score
due to ‘secondary’ factors like preprocessing in-
formation.
To this end, we employ three corpora: ACE
(Doddington et al., 2004), OntoNotes (Pradhan
et al., 2007), and AnCora (Recasens and Mart´ı,
2009). In order to isolate the three parameters
as far as possible, we benefit from a 100k-word
portion (from the TDT collection) that is common
to both ACE and OntoNotes. We apply the same
coreference resolution system in all cases. The re-
sults show that a system’s score is not informative
by itself, as different corpora or corpus parameters
lead to different scores. Our goal is not to achieve
the best performance to date, but rather to ex-
pose various issues raised by the choices of corpus
preparation and evaluation measure and to shed
light on the definition, methods, evaluation, and
complexities of the coreference resolution task.
The paper is organized as follows. Section 2
sets our work in context and provides the motiva-
tions for undertaking this study. Section 3 presents
the architecture of CISTELL, the system used in
the experimental evaluation. In Sections 4, 5,
</bodyText>
<page confidence="0.735839">
1423
</page>
<note confidence="0.9449745">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1423–1432,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997962666666667">
and 6, we describe the experiments on three differ-
ent datasets and discuss the results. We conclude
in Section 7.
</bodyText>
<sectionHeader confidence="0.957392" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999973348837209">
The bulk of research on automatic coreference res-
olution to date has been done for English and used
two different types of corpus: MUC (Hirschman
and Chinchor, 1997) and ACE (Doddington et al.,
2004). A variety of learning-based systems have
been trained and tested on the former (Soon et al.,
2001; Uryupina, 2006), on the latter (Culotta et
al., 2007; Bengtson and Roth, 2008; Denis and
Baldridge, 2009), or on both (Finkel and Manning,
2008; Haghighi and Klein, 2009). Testing on both
is needed given that the two annotation schemes
differ in some aspects. For example, only ACE
includes singletons (mentions that do not corefer)
and ACE is restricted to seven semantic types.1
Also, despite a critical discussion in the MUC task
definition (van Deemter and Kibble, 2000), the
ACE scheme continues to treat nominal predicates
and appositive phrases as coreferential.
A third coreferentially annotated corpus—the
largest for English—is OntoNotes (Pradhan et al.,
2007; Hovy et al., 2006). Unlike ACE, it is not
application-oriented, so coreference relations be-
tween all types of NPs are annotated. The identity
relation is kept apart from the attributive relation,
and it also contains gold-standard morphological,
syntactic and semantic information.
Since the MUC and ACE corpora are annotated
with only coreference information,2 existing sys-
tems first preprocess the data using automatic tools
(POS taggers, parsers, etc.) to obtain the infor-
mation needed for coreference resolution. How-
ever, given that the output from automatic tools
is far from perfect, it is hard to determine the
level of performance of a coreference module act-
ing on gold-standard preprocessing information.
OntoNotes makes it possible to separate the coref-
erence resolution problem from other tasks.
Our study adds to the previously reported evi-
dence by Stoyanov et al. (2009) that differences in
corpora and in the task definitions need to be taken
into account when comparing coreference resolu-
tion systems. We provide new insights as the cur-
rent analysis differs in four ways. First, Stoyanov
</bodyText>
<footnote confidence="0.999749333333333">
1The ACE-2004/05 semantic types are person, organiza-
tion, geo-political entity, location, facility, vehicle, weapon.
2ACE also specifies entity types and relations.
</footnote>
<bodyText confidence="0.99945696875">
et al. (2009) report on differences between MUC
and ACE, while we contrast ACE and OntoNotes.
Given that ACE and OntoNotes include some of
the same texts but annotated according to their re-
spective guidelines, we can better isolate the effect
of differences as well as add the additional dimen-
sion of gold preprocessing. Second, we evaluate
not only with the MUC and B3 scoring metrics,
but also with CEAF. Third, all our experiments
use true mentions3 to avoid effects due to spuri-
ous system mentions. Finally, including different
baselines and variations of the resolution model al-
lows us to reveal biases of the metrics.
Coreference resolution systems have been
tested on languages other than English only within
the ACE program (Luo and Zitouni, 2005), prob-
ably due to the fact that coreferentially annotated
corpora for other languages are scarce. Thus there
has been no discussion of the extent to which sys-
tems are portable across languages. This paper
studies the case of English and Spanish.4
Several coreference systems have been devel-
oped in the past (Culotta et al., 2007; Finkel
and Manning, 2008; Poon and Domingos, 2008;
Haghighi and Klein, 2009; Ng, 2009). It is not our
aim to compete with them. Rather, we conduct
three experiments under a specific setup for com-
parison purposes. To this end, we use a different,
neutral, system, and a dataset that is small and dif-
ferent from official ACE test sets despite the fact
that it prevents our results from being compared
directly with other systems.
</bodyText>
<sectionHeader confidence="0.996114" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.997667">
3.1 System Description
</subsectionHeader>
<bodyText confidence="0.999298">
The system architecture used in our experiments,
CISTELL, is based on the incrementality of dis-
course. As a discourse evolves, it constructs a
model that is updated with the new information
gradually provided. A key element in this model
are the entities the discourse is about, as they form
the discourse backbone, especially those that are
mentioned multiple times. Most entities, however,
are only mentioned once. Consider the growth of
the entity Mount Popocat´epetl in (1).5
</bodyText>
<footnote confidence="0.997561714285714">
3The adjective true contrasts with system and refers to the
gold standard.
4Multilinguality is one of the focuses of SemEval-2010
Task 1 (Recasens et al., 2010).
5Following the ACE terminology, we use the term men-
tion for an instance of reference to an object, and entity for a
collection of mentions referring to the same object. Entities
</footnote>
<page confidence="0.921275">
1424
</page>
<bodyText confidence="0.980696783783784">
(1) We have an update tonight on [this, the volcano in
Mexico, they call El Popo],,,,3 ... As the sun rises
over [Mt. Popo],,,,7 tonight, the only hint of the fire
storm inside, whiffs of smoke, but just a few hours
earlier, [the volcano],,,,11 exploding spewing rock
and red-hot lava. [The fourth largest mountain in
North America, nearly 18,000 feet high].15, erupt-
ing this week with [its],,,,20 most violent outburst in
1,200 years.
Mentions can be pronouns (m20), they can be a
(shortened) string repetition using either the name
(m7) or the type (m11), or they can add new infor-
mation about the entity: m15 provides the super-
type and informs the reader about the height of the
volcano and its ranking position.
In CISTELL,6 discourse entities are conceived
as ‘baskets’: they are empty at the beginning of
the discourse, but keep growing as new attributes
(e.g., name, type, location) are predicated about
them. Baskets are filled with this information,
which can appear within a mention or elsewhere
in the sentence. The ever-growing amount of in-
formation in a basket allows richer comparisons to
new mentions encountered in the text.
CISTELL follows the learning-based corefer-
ence architecture in which the task is split into
classification and clustering (Soon et al., 2001;
Bengtson and Roth, 2008) but combines them si-
multaneously. Clustering is identified with basket-
growing, the core process, and a pairwise clas-
sifier is called every time CISTELL considers
whether a basket must be clustered into a (grow-
ing) basket, which might contain one or more
mentions. We use a memory-based learning clas-
sifier trained with TiMBL (Daelemans and Bosch,
2005). Basket-growing is done in four different
ways, explained next.
</bodyText>
<subsectionHeader confidence="0.999905">
3.2 Baselines and Models
</subsectionHeader>
<bodyText confidence="0.995561666666667">
In each experiment, we compute three baselines
(1, 2, 3), and run CISTELL under four different
models (4, 5, 6, 7).
</bodyText>
<listItem confidence="0.866183804878049">
1. ALL SINGLETONS. No coreference link is
ever created. We include this baseline given
the high number of singletons in the datasets,
since some evaluation measures are affected
by large numbers of singletons.
2. HEAD MATCH. All non-pronominal NPs that
have the same head are clustered into the
same entity.
containing one single mention are referred to as singletons.
6‘Cistell’ is the Catalan word for ‘basket.’
3. HEAD MATCH + PRON. Like HEAD MATCH,
plus allowing personal and possessive pro-
nouns to link to the closest noun with which
they agree in gender and number.
4. STRONG MATCH. Each mention (e.g., m11) is
paired with previous mentions starting from
the beginning of the document (m1–m11, m2–
m11, etc.).7 When a pair (e.g., m3–m11) is
classified as coreferent, additional pairwise
checks are performed with all the mentions
contained in the (growing) entity basket (e.g.,
m7–m11). Only if all the pairs are classified
as coreferent is the mention under consider-
ation attached to the existing growing entity.
Otherwise, the search continues.8
5. SUPERSTRONG MATCH. Similar to STRONG
MATCH but with a threshold. Coreference
pairwise classifications are only accepted
when TiMBL distance is smaller than 0.09.9
6. BEST MATCH. Similar to STRONG MATCH
but following Ng and Cardie (2002)’s best
link approach. Thus, the mention under anal-
ysis is linked to the most confident men-
tion among the previous ones, using TiMBL’s
confidence score.
7. WEAK MATCH. A simplified version of
STRONG MATCH: not all mentions in the
growing entity need to be classified as coref-
erent with the mention under analysis. A sin-
gle positive pairwise decision suffices for the
mention to be clustered into that entity.10
</listItem>
<subsectionHeader confidence="0.979413">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999813727272727">
We follow Soon et al. (2001), Ng and Cardie
(2002) and Luo et al. (2004) to generate most
of the 29 features we use for the pairwise
model. These include features that capture in-
formation from different linguistic levels: textual
strings (head match, substring match, distance,
frequency), morphology (mention type, coordi-
nation, possessive phrase, gender match, number
match), syntax (nominal predicate, apposition, rel-
ative clause, grammatical function), and semantic
match (named-entity type, is-a type, supertype).
</bodyText>
<footnote confidence="0.999038888888889">
7The opposite search direction was also tried but gave
worse results.
8Taking the first mention classified as coreferent follows
Soon et al. (2001)’s first-link approach.
9In TiMBL, being a memory-based learner, the closer the
distance to an instance, the more confident the decision. We
chose 0.09 because it appeared to offer the best results.
10STRONG and WEAK MATCH are similar to Luo et al.
(2004)’s entity-mention and mention-pair models.
</footnote>
<page confidence="0.991056">
1425
</page>
<bodyText confidence="0.999797333333333">
For Spanish, we use 34 features as a few varia-
tions are needed for language-specific issues such
as zero subjects (Recasens and Hovy, 2009).
</bodyText>
<subsectionHeader confidence="0.94656">
3.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999975333333333">
Since they sometimes provide quite different re-
sults, we evaluate using three coreference mea-
sures, as there is no agreement on a standard.
</bodyText>
<listItem confidence="0.962386428571428">
• MUC (Vilain et al., 1995). It computes the
number of links common between the true
and system partitions. Recall (R) and preci-
sion (P) result from dividing it by the mini-
mum number of links required to specify the
true and the system partitions, respectively.
• B3 (Bagga and Baldwin, 1998). R and P are
computed for each mention and averaged at
the end. For each mention, the number of
common mentions between the true and the
system entity is divided by the number of
mentions in the true entity or in the system
entity to obtain R and P, respectively.
• CEAF (Luo, 2005). It finds the best one-to-
</listItem>
<bodyText confidence="0.840842833333333">
one alignment between true and system en-
tities. Using true mentions and the 03 sim-
ilarity function, R and P are the same and
correspond to the number of common men-
tions between the aligned entities divided by
the total number of mentions.
</bodyText>
<sectionHeader confidence="0.920077" genericHeader="method">
4 Parameter 1: Language
</sectionHeader>
<bodyText confidence="0.982973275862069">
The first experiment compared the performance
of a coreference resolution system on a Germanic
and a Romance language—English and Spanish—
to explore to what extent language-specific issues
such as zero subjects11 or grammatical gender
might influence a system.
Although OntoNotes and AnCora are two dif-
ferent corpora, they are very similar in those as-
pects that matter most for the study’s purpose:
they both include a substantial amount of texts
belonging to the same genre (news) and manu-
ally annotated from the morphological to the se-
mantic levels (POS tags, syntactic constituents,
NEs, WordNet synsets, and coreference relations).
More importantly, very similar coreference anno-
tation guidelines make AnCora the ideal Spanish
counterpart to OntoNotes.
11Most Romance languages are pro-drop allowing zero
subject pronouns, which can be inferred from the verb.
Datasets Two datasets of similar size were se-
lected from AnCora and OntoNotes in order to
rule out corpus size as an explanation of any differ-
ence in performance. Corpus statistics about the
distribution of mentions and entities are shown in
Tables 1 and 2. Given that this paper is focused on
coreference between NPs, the number of mentions
only includes NPs. Both AnCora and OntoNotes
annotate only multi-mention entities (i.e., those
containing two or more coreferent mentions), so
singleton entities are assumed to correspond to
NPs with no coreference annotation.
Apart from a larger number of mentions in
Spanish (Table 1), the two datasets look very sim-
ilar in the distribution of singletons and multi-
mention entities: about 85% and 15%, respec-
tively. Multi-mention entities have an average
of 3.9 mentions per entity in AnCora and 3.5 in
OntoNotes. The distribution of mention types (Ta-
ble 2), however, differs in two important respects:
AnCora has a smaller number of personal pro-
nouns as Spanish typically uses zero subjects, and
it has a smaller number of bare NPs as the definite
article accompanies more NPs than in English.
Results and Discussion Table 3 presents CIS-
TELL’s results for each dataset. They make evi-
dent problems with the evaluation metrics, namely
the fact that the generated rankings are contradic-
tory (Denis and Baldridge, 2009). They are con-
sistent across the two corpora though: MUC re-
wards WEAK MATCH the most, B3 rewards HEAD
MATCH the most, and CEAF is divided between
SUPER STRONG MATCH and BEST MATCH.
These preferences seem to reveal weaknesses
of the scoring methods that make them biased to-
wards a type of output. The model preferred by
MUC is one that clusters many mentions together,
thus getting a large number of correct coreference
links (notice the high R for WEAK MATCH), but
</bodyText>
<table confidence="0.998659545454545">
AnCora OntoNotes
Pronouns 14.09 17.62
Personal pronouns 2.00 12.10
Zero subject pronouns 6.51 –
Possessive pronouns 3.57 2.96
Demonstrative pronouns 0.39 1.83
Definite NPs 37.69 20.67
Indefinite NPs 7.17 8.44
Demonstrative NPs 1.98 3.41
Bare NPs 33.02 42.92
Misc. 6.05 6.94
</table>
<tableCaption confidence="0.992727">
Table 2: Mention types (%) in Table 1 datasets.
</tableCaption>
<page confidence="0.735352">
1426
</page>
<table confidence="0.994403142857143">
#docs #words #mentions #entities (e) #singleton e #multi-mention e
Training 955 299,014 91,904 64,535 54,991 9,544
AnCora
Test 30 9,851 2,991 2,189 1,877 312
Training 850 301,311 74,692 55,819 48,199 7,620
OntoNotes
Test 33 9,763 2,463 1,790 1,476 314
</table>
<tableCaption confidence="0.99373">
Table 1: Corpus statistics for the large portion of OntoNotes and AnCora.
</tableCaption>
<table confidence="0.999238666666667">
P MUC F P B3 F CEAF
R R P / R / F
AnCora - Spanish
1. ALL SINGLETONS – – – 100 73.32 84.61 73.32
2. HEAD MATCH 55.03 37.72 44.76 91.12 79.88 85.13 75.96
3. HEAD MATCH + PRON 48.22 44.24 46.14 86.21 80.66 83.34 76.30
4. STRONG MATCH 45.64 51.88 48.56 80.13 82.28 81.19 75.79
5. SUPER STRONG MATCH 45.68 36.47 40.56 86.10 79.09 82.45 77.20
6. BEST MATCH 43.10 35.59 38.98 85.24 79.67 82.36 75.23
7. WEAK MATCH 45.73 65.16 53.75 68.50 87.71 76.93 69.21
OntoNotes - English
1. ALL SINGLETONS – – – 100 72.68 84.18 72.68
2. HEAD MATCH 55.14 39.08 45.74 90.65 80.87 85.48 76.05
3. HEAD MATCH + PRON 47.10 53.05 49.90 82.28 83.13 82.70 75.15
4. STRONG MATCH 47.94 55.42 51.41 81.13 84.30 82.68 78.03
5. SUPER STRONG MATCH 48.27 47.55 47.90 84.00 82.27 83.13 78.24
6. BEST MATCH 50.97 46.66 48.72 86.19 82.70 84.41 78.44
7. WEAK MATCH 47.46 66.72 55.47 70.36 88.05 78.22 71.21
</table>
<tableCaption confidence="0.999396">
Table 3: CISTELL results varying the corpus language.
</tableCaption>
<bodyText confidence="0.999716473684211">
also many spurious links that are not duly penal-
ized. The resulting output is not very desirable.12
In contrast, B3 is more P-oriented and scores con-
servative outputs like HEAD MATCH and BEST
MATCH first, even if R is low. CEAF achieves a
better compromise between P and R, as corrobo-
rated by the quality of the output.
The baselines and the system runs perform very
similarly in the two corpora, but slightly better
for English. It seems that language-specific issues
do not result in significant differences—at least
for English and Spanish—once the feature set has
been appropriately adapted, e.g., including fea-
tures about zero subjects or removing those about
possessive phrases. Comparing the feature ranks,
we find that the features that work best for each
language largely overlap and are language inde-
pendent, like head match, is-a match, and whether
the mentions are pronominal.
</bodyText>
<sectionHeader confidence="0.98804" genericHeader="method">
5 Parameter 2: Annotation Scheme
</sectionHeader>
<bodyText confidence="0.997432413793104">
In the second experiment, we used the 100k-word
portion (from the TDT collection) shared by the
OntoNotes and ACE corpora (330 OntoNotes doc-
12Due to space constraints, the actual output cannot be
shown here. We are happy to send it to interested requesters.
uments occurred as 22 ACE-2003 documents, 185
ACE-2004 documents, and 123 ACE-2005 docu-
ments). CISTELL was trained on the same texts
in both corpora and applied to the remainder. The
three measures were then applied to each result.
Datasets Since the two annotation schemes dif-
fer significantly, we made the results comparable
by mapping the ACE entities (the simpler scheme)
onto the information contained in OntoNotes.13
The mapping allowed us to focus exclusively on
the differences expressed on both corpora: the
types of mentions that were annotated, the defi-
nition of identity of reference, etc.
Table 4 presents the statistics for the OntoNotes
dataset merged with the ACE entities. The map-
ping was not straightforward due to several prob-
lems: there was no match for some mentions
due to syntactic or spelling reasons (e.g., El Popo
in OntoNotes vs. Ell Popo in ACE). ACE men-
tions for which there was no parse tree node in
the OntoNotes gold-standard tree were omitted, as
creating a new node could have damaged the tree.
Given that only seven entity types are annotated
in ACE, the number of OntoNotes mentions is al-
</bodyText>
<footnote confidence="0.717447">
13Both ACE entities and types were mapped onto the
OntoNotes dataset.
</footnote>
<page confidence="0.982322">
1427
</page>
<table confidence="0.981397571428571">
#docs #words #mentions #entities (e) #singleton e #multi-mention e
Training 297 87,068 22,127 15,983 13,587 2,396
OntoNotes
Test 33 9,763 2,463 1,790 1,476 314
Training 297 87,068 12,951 5,873 3,599 2,274
ACE
Test 33 9,763 1,464 746 459 287
</table>
<tableCaption confidence="0.99205">
Table 4: Corpus statistics for the aligned portion of ACE and OntoNotes on gold-standard data.
</tableCaption>
<table confidence="0.628463722222222">
P MUC F P B3 F CEAF
R R P / R / F
OntoNotes scheme
1. ALL SINGLETONS – – – 100 72.68 84.18 72.68
2. HEAD MATCH 55.14 39.08 45.74 90.65 80.87 85.48 76.05
3. HEAD MATCH + PRON 47.10 53.05 49.90 82.28 83.13 82.70 75.15
4. STRONG MATCH 46.81 53.34 49.86 80.47 83.54 81.97 76.78
5. SUPER STRONG MATCH 46.51 40.56 43.33 84.95 80.16 82.48 76.70
6. BEST MATCH 52.47 47.40 49.80 86.10 82.80 84.42 77.87
7. WEAK MATCH 47.91 64.64 55.03 71.73 87.46 78.82 71.74
ACE scheme
1. ALL SINGLETONS – – – 100 50.96 67.51 50.96
2. HEAD MATCH 82.35 39.00 52.93 95.27 64.05 76.60 66.46
3. HEAD MATCH + PRON 70.11 53.90 60.94 86.49 68.20 76.27 68.44
4. STRONG MATCH 64.21 64.21 64.21 76.92 73.54 75.19 70.01
5. SUPER STRONG MATCH 60.51 56.55 58.46 76.71 69.19 72.76 66.87
6. BEST MATCH 67.50 56.69 61.62 82.18 71.67 76.57 69.88
7. WEAK MATCH 63.52 80.50 71.01 59.76 86.36 70.64 64.21
</table>
<tableCaption confidence="0.986467">
Table 5: CISTELL results varying the annotation scheme on gold-standard data.
</tableCaption>
<bodyText confidence="0.99617275">
most twice as large as the number of ACE men-
tions. Unlike OntoNotes, ACE mentions include
premodifiers (e.g., state in state lines), national
adjectives (e.g., Iraqi) and relative pronouns (e.g.,
who, that). Also, given that ACE entities corre-
spond to types that are usually coreferred (e.g.,
people, organizations, etc.), singletons only rep-
resent 61% of all entities, while they are 85% in
OntoNotes. The average entity size is 4 in ACE
and 3.5 in OntoNotes.
A second major difference is the definition of
coreference relations, illustrated here:
</bodyText>
<listItem confidence="0.9811454">
(2) [This] was [an all-white, all-Christian community
that all the sudden was taken over ... by different
groups].
(3) [ [Mayor] John Hyman] has a simple answer.
(4) [Postville] now has 22 different nationalities ... For
</listItem>
<bodyText confidence="0.9854552">
those who prefer [the old Postville], Mayor John
Hyman has a simple answer.
In ACE, nominal predicates corefer with their
subject (2), and appositive phrases corefer with
the noun they are modifying (3). In contrast,
they do not fall under the identity relation in
OntoNotes, which follows the linguistic under-
standing of coreference according to which nom-
inal predicates and appositives express properties
of an entity rather than refer to a second (corefer-
ent) entity (van Deemter and Kibble, 2000). Fi-
nally, the two schemes frequently disagree on bor-
derline cases in which coreference turns out to be
especially complex (4). As a result, some features
will behave differently, e.g., the appositive feature
has the opposite effect in the two datasets.
Results and Discussion From the differences
pointed out above, the results shown in Table 5
might be surprising at first. Given that OntoNotes
is not restricted to any semantic type and is based
on a more sophisticated definition of coreference,
one would not expect a system to perform better
on it than on ACE. The explanation is given by the
ALL SINGLETONS baseline, which is 73–84% for
OntoNotes and only 51–68% for ACE. The fact
that OntoNotes contains a much larger number of
singletons—as Table 4 shows—results in an ini-
tial boost of performance (except with the MUC
score, which ignores singletons). In contrast, the
score improvement achieved by HEAD MATCH is
much more noticeable on ACE than on OntoNotes,
which indicates that many of its coreferent men-
tions share the same head.
The systematic biases of the measures that were
observed in Table 3 appear again in the case of
</bodyText>
<page confidence="0.984393">
1428
</page>
<bodyText confidence="0.999836769230769">
MUC and B3. CEAF is divided between BEST
MATCH and STRONG MATCH. The higher value
of the MUC score for ACE is another indication
of its tendency to reward correct links much more
than to penalize spurious ones (ACE has a larger
proportion of multi-mention entities).
The feature rankings obtained for each dataset
generally coincide as to which features are ranked
best (namely NE match, is-a match, and head
match), but differ in their particular ordering.
It is also possible to compare the OntoNotes re-
sults in Tables 3 and 5, the only difference being
that the first training set was three times larger.
Contrary to expectation, the model trained on a
larger dataset performs just slightly better. The
fact that more training data does not necessarily
lead to an increase in performance conforms to
the observation that there appear to be few general
rules (e.g., head match) that systematically gov-
ern coreference relationships; rather, coreference
appeals to individual unique phenomena appear-
ing in each context, and thus after a point adding
more training data does not add much new gener-
alizable information. Pragmatic information (dis-
course structure, world knowledge, etc.) is proba-
bly the key, if ever there is a way to encode it.
</bodyText>
<sectionHeader confidence="0.918239" genericHeader="method">
6 Parameter 3: Preprocessing
</sectionHeader>
<bodyText confidence="0.983061540540541">
The goal of the third experiment was to determine
how much the source and nature of preprocess-
ing information matters. Since it is often stated
that coreference resolution depends on many lev-
els of analysis, we again compared the two cor-
pora, which differ in the amount and correctness
of such information. However, in this experiment,
entity mapping was applied in the opposite direc-
tion: the OntoNotes entities were mapped onto the
automatically preprocessed ACE dataset. This ex-
poses the shortcomings of automated preprocess-
ing in ACE for identifying all the mentions identi-
fied and linked in OntoNotes.
Datasets The ACE data was morphologically
annotated with a tokenizer based on manual rules
adapted from the one used in CoNLL (Tjong
Kim Sang and De Meulder, 2003), with TnT 2.2,
a trigram POS tagger based on Markov models
(Brants, 2000), and with the built-in WordNet lem-
matizer (Fellbaum, 1998). Syntactic chunks were
obtained from YamCha 1.33, an SVM-based NP-
chunker (Kudoh and Matsumoto, 2000), and parse
trees from Malt Parser 0.4, an SVM-based parser
(Hall et al., 2007).
Although the number of words in Tables 4 and 6
should in principle be the same, the latter con-
tains fewer words as it lacks the null elements
(traces, ellipsed material, etc.) manually anno-
tated in OntoNotes. Missing parse tree nodes in
the automatically parsed data account for the con-
siderably lower number of OntoNotes mentions
(approx. 5,700 fewer mentions).14 However, the
proportions of singleton:multi-mention entities as
well as the average entity size do not vary.
Results and Discussion The ACE scores for the
automatically preprocessed models in Table 7 are
about 3% lower than those based on OntoNotes
gold-standard data in Table 5, providing evidence
for the advantage offered by gold-standard prepro-
cessing information. In contrast, the similar—if
not higher—scores of OntoNotes can be attributed
to the use of the annotated ACE entity types. The
fact that these are annotated not only for proper
nouns (as predicted by an automatic NER) but also
for pronouns and full NPs is a very helpful feature
for a coreference resolution system.
Again, the scoring metrics exhibit similar bi-
ases, but note that CEAF prefers HEAD MATCH
+ PRON in the case of ACE, which is indicative of
the noise brought by automatic preprocessing.
A further insight is offered from comparing the
feature rankings with gold-standard syntax to that
with automatic preprocessing. Since we are evalu-
ating now on the ACE data, the NE match feature
is also ranked first for OntoNotes. Head and is-a
match are still ranked among the best, yet syntac-
tic features are not. Instead, features like NP type
have moved further up. This reranking probably
indicates that if there is noise in the syntactic infor-
mation due to automatic tools, then morphological
and syntactic features switch their positions.
Given that the noise brought by automatic pre-
processing can be harmful, we tried leaving out the
grammatical function feature. Indeed, the results
increased about 2–3%, STRONG MATCH scoring
the highest. This points out that conclusions drawn
from automatically preprocessed data about the
kind of knowledge relevant for coreference reso-
lution might be mistaken. Using the most success-
ful basic features can lead to the best results when
only automatic preprocessing is available.
14In order to make the set of mentions as similar as possible
to the set in Section 5, OntoNotes singletons were mapped
from the ones detected in the gold-standard treebank.
</bodyText>
<page confidence="0.984317">
1429
</page>
<table confidence="0.987803285714286">
#docs #words #mentions #entities (e) #singleton e #multi-mention e
Training 297 80,843 16,945 12,127 10,253 1,874
OntoNotes
Test 33 9,073 1,931 1,403 1,156 247
Training 297 80,843 13,648 6,041 3,652 2,389
ACE
Test 33 9,073 1,537 775 475 300
</table>
<tableCaption confidence="0.956132">
Table 6: Corpus statistics for the aligned portion of ACE and OntoNotes on automatically parsed data.
</tableCaption>
<table confidence="0.997501333333333">
P MUC F P B3 F CEAF
R R P / R / F
OntoNotes scheme
1. ALL SINGLETONS – – – 100 72.66 84.16 72.66
2. HEAD MATCH 56.76 35.80 43.90 92.18 80.52 85.95 76.33
3. HEAD MATCH + PRON 47.44 54.36 50.66 82.08 83.61 82.84 74.83
4. STRONG MATCH 52.66 58.14 55.27 83.11 85.05 84.07 78.30
5. SUPER STRONG MATCH 51.67 46.78 49.11 85.74 82.07 83.86 77.67
6. BEST MATCH 54.38 51.70 53.01 86.00 83.60 84.78 78.15
7. WEAK MATCH 49.78 64.58 56.22 75.63 87.79 81.26 74.62
ACE scheme
1. ALL SINGLETONS – – – 100 50.42 67.04 50.42
2. HEAD MATCH 81.25 39.24 52.92 94.73 63.82 76.26 65.97
3. HEAD MATCH + PRON 69.76 53.28 60.42 86.39 67.73 75.93 68.05
4. STRONG MATCH 58.85 58.92 58.89 73.36 70.35 71.82 66.30
5. SUPER STRONG MATCH 56.19 50.66 53.28 75.54 66.47 70.72 63.96
6. BEST MATCH 63.38 49.74 55.74 80.97 68.11 73.99 65.97
7. WEAK MATCH 60.22 78.48 68.15 55.17 84.86 66.87 59.08
</table>
<tableCaption confidence="0.995527">
Table 7: CISTELL results varying the annotation scheme on automatically preprocessed data.
</tableCaption>
<sectionHeader confidence="0.995564" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999985024390244">
Regarding evaluation, the results clearly expose
the systematic tendencies of the evaluation mea-
sures. The way each measure is computed makes
it biased towards a specific model: MUC is gen-
erally too lenient with spurious links, B3 scores
too high in the presence of a large number of sin-
gletons, and CEAF does not agree with either of
them. It is a cause for concern that they provide
contradictory indications about the core of coref-
erence, namely the resolution models—for exam-
ple, the model ranked highest by B3 in Table 7 is
ranked lowest by MUC. We always assume eval-
uation measures provide a ‘true’ reflection of our
approximation to a gold standard in order to guide
research in system development and tuning.
Further support to our claims comes from the
results of SemEval-2010 Task 1 (Recasens et al.,
2010). The performance of the six participating
systems shows similar problems with the evalua-
tion metrics, and the singleton baseline was hard
to beat even by the highest-performing systems.
Since the measures imply different conclusions
about the nature of the corpora and the preprocess-
ing information applied, should we use them now
to constrain the ways our corpora are created in
the first place, and what preprocessing we include
or omit? Doing so would seem like circular rea-
soning: it invalidates the notion of the existence of
a true and independent gold standard. But if ap-
parently incidental aspects of the corpora can have
such effects—effects rated quite differently by the
various measures—then we have no fixed ground
to stand on.
The worrisome fact that there is currently no
clearly preferred and ‘correct’ evaluation measure
for coreference resolution means that we cannot
draw definite conclusions about coreference reso-
lution systems at this time, unless they are com-
pared on exactly the same corpus, preprocessed
under the same conditions, and all three measures
agree in their rankings.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.982352125">
We thank Dr. M. Ant`onia Martifor her generosity
in allowing the first author to visit ISI to work with
the second. Special thanks to Edgar Gonz`alez for
his kind help with conversion issues.
This work was partially supported by the Span-
ish Ministry of Education through an FPU schol-
arship (AP2006-00994) and the TEXT-MESS 2.0
Project (TIN2009-13391-C04-04).
</bodyText>
<page confidence="0.988086">
1430
</page>
<sectionHeader confidence="0.987008" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999010657142857">
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC 1998 Workshop on Linguistic Coreference,
pages 563–566, Granada, Spain.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of EMNLP 2008, pages 294–303, Hon-
olulu, Hawaii.
Thorsten Brants. 2000. TnT – A statistical part-of-
speech tagger. In Proceedings of ANLP 2000, Seat-
tle, WA.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order probabilistic models
for coreference resolution. In Proceedings of HLT-
NAACL 2007, pages 81–88, Rochester, New York.
Walter Daelemans and Antal Van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87–96.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) Program - Tasks, Data, and Evaluation.
In Proceedings of LREC 2004, pages 837–840.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Jenny Rose Finkel and Christopher D. Manning.
2008. Enforcing transitivity in coreference resolu-
tion. In Proceedings of ACL-HLT 2008, pages 45–
48, Columbus, Ohio.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of EMNLP 2009, pages
1152–1161, Singapore. Association for Computa-
tional Linguistics.
Johan Hall, Jens Nilsson, Joakim Nivre, G¨ulsen
Eryigit, Be´ata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
A study in multilingual parser optimization. In
Proceedings of the CoNLL shared task session of
EMNLP-CoNLL 2007, pages 933–939.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-
7 Coreference Task Definition – Version 3.0. In Pro-
ceedings of MUC-7.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
HLT-NAACL 2006, pages 57–60.
Taku Kudoh and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In Pro-
ceedings of CoNLL 2000 and LLL 2000, pages 142–
144, Lisbon, Portugal.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
Proceedings of HLT-EMNLP 2005, pages 660–667,
Vancouver.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of ACL 2004, pages
21–26, Barcelona.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of HLT-
EMNLP 2005, pages 25–32, Vancouver.
Vincent Ng and Claire Cardie. 2002. Improving
machine learning approaches to coreference resolu-
tion. In Proceedings of ACL 2002, pages 104–111,
Philadelphia.
Vincent Ng. 2009. Graph-cut-based anaphoricity de-
termination for coreference resolution. In Proceed-
ings of NAACL-HLT 2009, pages 575–583, Boulder,
Colorado.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic.
In Proceedings of EMNLP 2008, pages 650–659,
Honolulu, Hawaii.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified rela-
tional semantic representation. In Proceedings of
ICSC 2007, pages 517–526, Washington, DC.
Marta Recasens and Eduard Hovy. 2009. A
Deeper Look into Features for Coreference Res-
olution. In S. Lalitha Devi, A. Branco, and
R. Mitkov, editors, Anaphora Processing and Ap-
plications (DAARC 2009), volume 5847 of LNAI,
pages 29–42. Springer-Verlag.
Marta Recasens and M. Ant`onia Mart´ı. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
DOI 10.1007/s10579-009-9108-x.
Marta Recasens, Llu´ıs M`arquez, Emili Sapena,
M. Ant`onia Mart´ı, Mariona Taul´e, V´eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In Proceedings of the Fifth In-
ternational Workshop on Semantic Evaluations (Se-
mEval 2010), Uppsala, Sweden.
Wee M. Soon, Hwee T. Ng, and Daniel C. Y. Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521–544.
</reference>
<page confidence="0.825715">
1431
</page>
<reference confidence="0.999721863636364">
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP 2009,
pages 656–664, Singapore.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-independent Named Entity Recog-
nition. In Walter Daelemans and Miles Osborne, ed-
itors, Proceedings of CoNLL 2003, pages 142–147.
Edmonton, Canada.
Olga Uryupina. 2006. Coreference resolution with
and without linguistic knowledge. In Proceedings
of LREC 2006.
Kees van Deemter and Rodger Kibble. 2000. On core-
ferring: Coreference in MUC and related annotation
schemes. Computational Linguistics, 26(4):629–
637.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45–52, San Francisco.
</reference>
<page confidence="0.994562">
1432
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.668275">
<title confidence="0.9970895">Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</title>
<author confidence="0.983805">Marta Recasens</author>
<affiliation confidence="0.935536">CLiC - University of Barcelona</affiliation>
<address confidence="0.786476">Gran Via 585 Barcelona, Spain</address>
<email confidence="0.999803">mrecasens@ub.edu</email>
<abstract confidence="0.998969888888889">This paper explores the effect that different corpus configurations have on the performance of a coreference resolution as measured by MUC, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also expose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the LREC 1998 Workshop on Linguistic Coreference,</booktitle>
<pages>563--566</pages>
<location>Granada,</location>
<contexts>
<context position="13971" citStr="Bagga and Baldwin, 1998" startWordPosition="2209" endWordPosition="2212">ntion and mention-pair models. 1425 For Spanish, we use 34 features as a few variations are needed for language-specific issues such as zero subjects (Recasens and Hovy, 2009). 3.4 Evaluation Since they sometimes provide quite different results, we evaluate using three coreference measures, as there is no agreement on a standard. • MUC (Vilain et al., 1995). It computes the number of links common between the true and system partitions. Recall (R) and precision (P) result from dividing it by the minimum number of links required to specify the true and the system partitions, respectively. • B3 (Bagga and Baldwin, 1998). R and P are computed for each mention and averaged at the end. For each mention, the number of common mentions between the true and the system entity is divided by the number of mentions in the true entity or in the system entity to obtain R and P, respectively. • CEAF (Luo, 2005). It finds the best one-toone alignment between true and system entities. Using true mentions and the 03 similarity function, R and P are the same and correspond to the number of common mentions between the aligned entities divided by the total number of mentions. 4 Parameter 1: Language The first experiment compare</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the LREC 1998 Workshop on Linguistic Coreference, pages 563–566, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>294--303</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="4587" citStr="Bengtson and Roth, 2008" startWordPosition="703" endWordPosition="706">ional Linguistics, pages 1423–1432, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al., 2007; Hovy e</context>
<context position="10138" citStr="Bengtson and Roth, 2008" startWordPosition="1595" endWordPosition="1598">olcano and its ranking position. In CISTELL,6 discourse entities are conceived as ‘baskets’: they are empty at the beginning of the discourse, but keep growing as new attributes (e.g., name, type, location) are predicated about them. Baskets are filled with this information, which can appear within a mention or elsewhere in the sentence. The ever-growing amount of information in a basket allows richer comparisons to new mentions encountered in the text. CISTELL follows the learning-based coreference architecture in which the task is split into classification and clustering (Soon et al., 2001; Bengtson and Roth, 2008) but combines them simultaneously. Clustering is identified with basketgrowing, the core process, and a pairwise classifier is called every time CISTELL considers whether a basket must be clustered into a (growing) basket, which might contain one or more mentions. We use a memory-based learning classifier trained with TiMBL (Daelemans and Bosch, 2005). Basket-growing is done in four different ways, explained next. 3.2 Baselines and Models In each experiment, we compute three baselines (1, 2, 3), and run CISTELL under four different models (4, 5, 6, 7). 1. ALL SINGLETONS. No coreference link is</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of EMNLP 2008, pages 294–303, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – A statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP 2000,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="26992" citStr="Brants, 2000" startWordPosition="4394" endWordPosition="4395">wo corpora, which differ in the amount and correctness of such information. However, in this experiment, entity mapping was applied in the opposite direction: the OntoNotes entities were mapped onto the automatically preprocessed ACE dataset. This exposes the shortcomings of automated preprocessing in ACE for identifying all the mentions identified and linked in OntoNotes. Datasets The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). Syntactic chunks were obtained from YamCha 1.33, an SVM-based NPchunker (Kudoh and Matsumoto, 2000), and parse trees from Malt Parser 0.4, an SVM-based parser (Hall et al., 2007). Although the number of words in Tables 4 and 6 should in principle be the same, the latter contains fewer words as it lacks the null elements (traces, ellipsed material, etc.) manually annotated in OntoNotes. Missing parse tree nodes in the automatically parsed data account for the considerably lower number of OntoNotes mentions (approx. 5,700 fewer mention</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT – A statistical part-ofspeech tagger. In Proceedings of ANLP 2000, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Robert Hall</author>
<author>Andrew McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL 2007,</booktitle>
<pages>81--88</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="4562" citStr="Culotta et al., 2007" startWordPosition="699" endWordPosition="702">sociation for Computational Linguistics, pages 1423–1432, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pra</context>
<context position="7532" citStr="Culotta et al., 2007" startWordPosition="1166" endWordPosition="1169">ffects due to spurious system mentions. Finally, including different baselines and variations of the resolution model allows us to reveal biases of the metrics. Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. Thus there has been no discussion of the extent to which systems are portable across languages. This paper studies the case of English and Spanish.4 Several coreference systems have been developed in the past (Culotta et al., 2007; Finkel and Manning, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Ng, 2009). It is not our aim to compete with them. Rather, we conduct three experiments under a specific setup for comparison purposes. To this end, we use a different, neutral, system, and a dataset that is small and different from official ACE test sets despite the fact that it prevents our results from being compared directly with other systems. 3 Experimental Setup 3.1 System Description The system architecture used in our experiments, CISTELL, is based on the incrementality of discourse. As a discourse evolves,</context>
</contexts>
<marker>Culotta, Wick, Hall, McCallum, 2007</marker>
<rawString>Aron Culotta, Michael Wick, Robert Hall, and Andrew McCallum. 2007. First-order probabilistic models for coreference resolution. In Proceedings of HLTNAACL 2007, pages 81–88, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal Van den Bosch</author>
</authors>
<title>Memory-Based Language Processing.</title>
<date>2005</date>
<publisher>Cambridge University Press.</publisher>
<marker>Daelemans, Van den Bosch, 2005</marker>
<rawString>Walter Daelemans and Antal Van den Bosch. 2005. Memory-Based Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Global joint models for coreference resolution and named entity classification.</title>
<date>2009</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>42--87</pages>
<contexts>
<context position="4615" citStr="Denis and Baldridge, 2009" startWordPosition="707" endWordPosition="710">1423–1432, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al., 2007; Hovy et al., 2006). Unlike ACE, it</context>
<context position="16767" citStr="Denis and Baldridge, 2009" startWordPosition="2668" endWordPosition="2671">5% and 15%, respectively. Multi-mention entities have an average of 3.9 mentions per entity in AnCora and 3.5 in OntoNotes. The distribution of mention types (Table 2), however, differs in two important respects: AnCora has a smaller number of personal pronouns as Spanish typically uses zero subjects, and it has a smaller number of bare NPs as the definite article accompanies more NPs than in English. Results and Discussion Table 3 presents CISTELL’s results for each dataset. They make evident problems with the evaluation metrics, namely the fact that the generated rankings are contradictory (Denis and Baldridge, 2009). They are consistent across the two corpora though: MUC rewards WEAK MATCH the most, B3 rewards HEAD MATCH the most, and CEAF is divided between SUPER STRONG MATCH and BEST MATCH. These preferences seem to reveal weaknesses of the scoring methods that make them biased towards a type of output. The model preferred by MUC is one that clusters many mentions together, thus getting a large number of correct coreference links (notice the high R for WEAK MATCH), but AnCora OntoNotes Pronouns 14.09 17.62 Personal pronouns 2.00 12.10 Zero subject pronouns 6.51 – Possessive pronouns 3.57 2.96 Demonstra</context>
</contexts>
<marker>Denis, Baldridge, 2009</marker>
<rawString>Pascal Denis and Jason Baldridge. 2009. Global joint models for coreference resolution and named entity classification. Procesamiento del Lenguaje Natural, 42:87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The Automatic Content Extraction (ACE) Program - Tasks, Data, and Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>837--840</pages>
<contexts>
<context position="2937" citStr="Doddington et al., 2004" startWordPosition="435" endWordPosition="438">we compare the performance using goldstandard preprocessing information with that using automatic preprocessing tools. Throughout, we apply the three principal coreference evaluation measures in use today: MUC, B3, and CEAF. We highlight the systematic preferences of each measure to reward different configurations. This raises the difficult question of why one should use one or another evaluation measure, and how one should interpret their differences in reporting changes of performance score due to ‘secondary’ factors like preprocessing information. To this end, we employ three corpora: ACE (Doddington et al., 2004), OntoNotes (Pradhan et al., 2007), and AnCora (Recasens and Mart´ı, 2009). In order to isolate the three parameters as far as possible, we benefit from a 100k-word portion (from the TDT collection) that is common to both ACE and OntoNotes. We apply the same coreference resolution system in all cases. The results show that a system’s score is not informative by itself, as different corpora or corpus parameters lead to different scores. Our goal is not to achieve the best performance to date, but rather to expose various issues raised by the choices of corpus preparation and evaluation measure </context>
<context position="4409" citStr="Doddington et al., 2004" startWordPosition="673" endWordPosition="676">presents the architecture of CISTELL, the system used in the experimental evaluation. In Sections 4, 5, 1423 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1423–1432, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continue</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content Extraction (ACE) Program - Tasks, Data, and Evaluation. In Proceedings of LREC 2004, pages 837–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="27051" citStr="Fellbaum, 1998" startWordPosition="4403" endWordPosition="4404"> such information. However, in this experiment, entity mapping was applied in the opposite direction: the OntoNotes entities were mapped onto the automatically preprocessed ACE dataset. This exposes the shortcomings of automated preprocessing in ACE for identifying all the mentions identified and linked in OntoNotes. Datasets The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). Syntactic chunks were obtained from YamCha 1.33, an SVM-based NPchunker (Kudoh and Matsumoto, 2000), and parse trees from Malt Parser 0.4, an SVM-based parser (Hall et al., 2007). Although the number of words in Tables 4 and 6 should in principle be the same, the latter contains fewer words as it lacks the null elements (traces, ellipsed material, etc.) manually annotated in OntoNotes. Missing parse tree nodes in the automatically parsed data account for the considerably lower number of OntoNotes mentions (approx. 5,700 fewer mentions).14 However, the proportions of singleton:multi-mention e</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT 2008,</booktitle>
<pages>45--48</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="4653" citStr="Finkel and Manning, 2008" startWordPosition="714" endWordPosition="717">010. c�2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al., 2007; Hovy et al., 2006). Unlike ACE, it is not application-oriented, so coref</context>
<context position="7558" citStr="Finkel and Manning, 2008" startWordPosition="1170" endWordPosition="1173"> system mentions. Finally, including different baselines and variations of the resolution model allows us to reveal biases of the metrics. Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. Thus there has been no discussion of the extent to which systems are portable across languages. This paper studies the case of English and Spanish.4 Several coreference systems have been developed in the past (Culotta et al., 2007; Finkel and Manning, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Ng, 2009). It is not our aim to compete with them. Rather, we conduct three experiments under a specific setup for comparison purposes. To this end, we use a different, neutral, system, and a dataset that is small and different from official ACE test sets despite the fact that it prevents our results from being compared directly with other systems. 3 Experimental Setup 3.1 System Description The system architecture used in our experiments, CISTELL, is based on the incrementality of discourse. As a discourse evolves, it constructs a model tha</context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2008. Enforcing transitivity in coreference resolution. In Proceedings of ACL-HLT 2008, pages 45– 48, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP 2009,</booktitle>
<pages>1152--1161</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="4680" citStr="Haghighi and Klein, 2009" startWordPosition="718" endWordPosition="721">r Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al., 2007; Hovy et al., 2006). Unlike ACE, it is not application-oriented, so coreference relations between al</context>
<context position="7609" citStr="Haghighi and Klein, 2009" startWordPosition="1178" endWordPosition="1181">lines and variations of the resolution model allows us to reveal biases of the metrics. Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. Thus there has been no discussion of the extent to which systems are portable across languages. This paper studies the case of English and Spanish.4 Several coreference systems have been developed in the past (Culotta et al., 2007; Finkel and Manning, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Ng, 2009). It is not our aim to compete with them. Rather, we conduct three experiments under a specific setup for comparison purposes. To this end, we use a different, neutral, system, and a dataset that is small and different from official ACE test sets despite the fact that it prevents our results from being compared directly with other systems. 3 Experimental Setup 3.1 System Description The system architecture used in our experiments, CISTELL, is based on the incrementality of discourse. As a discourse evolves, it constructs a model that is updated with the new information gradually pro</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of EMNLP 2009, pages 1152–1161, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>G¨ulsen Eryigit</author>
<author>Be´ata Megyesi</author>
<author>Mattias Nilsson</author>
<author>Markus Saers</author>
</authors>
<title>Single malt or blended? A study in multilingual parser optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL</booktitle>
<pages>933--939</pages>
<contexts>
<context position="27231" citStr="Hall et al., 2007" startWordPosition="4430" endWordPosition="4433">dataset. This exposes the shortcomings of automated preprocessing in ACE for identifying all the mentions identified and linked in OntoNotes. Datasets The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). Syntactic chunks were obtained from YamCha 1.33, an SVM-based NPchunker (Kudoh and Matsumoto, 2000), and parse trees from Malt Parser 0.4, an SVM-based parser (Hall et al., 2007). Although the number of words in Tables 4 and 6 should in principle be the same, the latter contains fewer words as it lacks the null elements (traces, ellipsed material, etc.) manually annotated in OntoNotes. Missing parse tree nodes in the automatically parsed data account for the considerably lower number of OntoNotes mentions (approx. 5,700 fewer mentions).14 However, the proportions of singleton:multi-mention entities as well as the average entity size do not vary. Results and Discussion The ACE scores for the automatically preprocessed models in Table 7 are about 3% lower than those bas</context>
</contexts>
<marker>Hall, Nilsson, Nivre, Eryigit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>Johan Hall, Jens Nilsson, Joakim Nivre, G¨ulsen Eryigit, Be´ata Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? A study in multilingual parser optimization. In Proceedings of the CoNLL shared task session of EMNLP-CoNLL 2007, pages 933–939.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Nancy Chinchor</author>
</authors>
<date>1997</date>
<booktitle>MUC7 Coreference Task Definition – Version 3.0. In Proceedings of MUC-7.</booktitle>
<contexts>
<context position="4375" citStr="Hirschman and Chinchor, 1997" startWordPosition="667" endWordPosition="670"> for undertaking this study. Section 3 presents the architecture of CISTELL, the system used in the experimental evaluation. In Sections 4, 5, 1423 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1423–1432, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibb</context>
</contexts>
<marker>Hirschman, Chinchor, 1997</marker>
<rawString>Lynette Hirschman and Nancy Chinchor. 1997. MUC7 Coreference Task Definition – Version 3.0. In Proceedings of MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>57--60</pages>
<contexts>
<context position="5199" citStr="Hovy et al., 2006" startWordPosition="797" endWordPosition="800">, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al., 2007; Hovy et al., 2006). Unlike ACE, it is not application-oriented, so coreference relations between all types of NPs are annotated. The identity relation is kept apart from the attributive relation, and it also contains gold-standard morphological, syntactic and semantic information. Since the MUC and ACE corpora are annotated with only coreference information,2 existing systems first preprocess the data using automatic tools (POS taggers, parsers, etc.) to obtain the information needed for coreference resolution. However, given that the output from automatic tools is far from perfect, it is hard to determine the </context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proceedings of HLT-NAACL 2006, pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudoh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>142--144</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="27152" citStr="Kudoh and Matsumoto, 2000" startWordPosition="4416" endWordPosition="4419"> direction: the OntoNotes entities were mapped onto the automatically preprocessed ACE dataset. This exposes the shortcomings of automated preprocessing in ACE for identifying all the mentions identified and linked in OntoNotes. Datasets The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). Syntactic chunks were obtained from YamCha 1.33, an SVM-based NPchunker (Kudoh and Matsumoto, 2000), and parse trees from Malt Parser 0.4, an SVM-based parser (Hall et al., 2007). Although the number of words in Tables 4 and 6 should in principle be the same, the latter contains fewer words as it lacks the null elements (traces, ellipsed material, etc.) manually annotated in OntoNotes. Missing parse tree nodes in the automatically parsed data account for the considerably lower number of OntoNotes mentions (approx. 5,700 fewer mentions).14 However, the proportions of singleton:multi-mention entities as well as the average entity size do not vary. Results and Discussion The ACE scores for the</context>
</contexts>
<marker>Kudoh, Matsumoto, 2000</marker>
<rawString>Taku Kudoh and Yuji Matsumoto. 2000. Use of support vector learning for chunk identification. In Proceedings of CoNLL 2000 and LLL 2000, pages 142– 144, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Imed Zitouni</author>
</authors>
<title>Multi-lingual coreference resolution with syntactic features.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<pages>660--667</pages>
<location>Vancouver.</location>
<contexts>
<context position="7204" citStr="Luo and Zitouni, 2005" startWordPosition="1111" endWordPosition="1114">me of the same texts but annotated according to their respective guidelines, we can better isolate the effect of differences as well as add the additional dimension of gold preprocessing. Second, we evaluate not only with the MUC and B3 scoring metrics, but also with CEAF. Third, all our experiments use true mentions3 to avoid effects due to spurious system mentions. Finally, including different baselines and variations of the resolution model allows us to reveal biases of the metrics. Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. Thus there has been no discussion of the extent to which systems are portable across languages. This paper studies the case of English and Spanish.4 Several coreference systems have been developed in the past (Culotta et al., 2007; Finkel and Manning, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Ng, 2009). It is not our aim to compete with them. Rather, we conduct three experiments under a specific setup for comparison purposes. To this end, we use a different, neutral, system, and a d</context>
</contexts>
<marker>Luo, Zitouni, 2005</marker>
<rawString>Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual coreference resolution with syntactic features. In Proceedings of HLT-EMNLP 2005, pages 660–667, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mentionsynchronous coreference resolution algorithm based on the Bell tree.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004,</booktitle>
<pages>21--26</pages>
<location>Barcelona.</location>
<contexts>
<context position="12486" citStr="Luo et al. (2004)" startWordPosition="1978" endWordPosition="1981">pted when TiMBL distance is smaller than 0.09.9 6. BEST MATCH. Similar to STRONG MATCH but following Ng and Cardie (2002)’s best link approach. Thus, the mention under analysis is linked to the most confident mention among the previous ones, using TiMBL’s confidence score. 7. WEAK MATCH. A simplified version of STRONG MATCH: not all mentions in the growing entity need to be classified as coreferent with the mention under analysis. A single positive pairwise decision suffices for the mention to be clustered into that entity.10 3.3 Features We follow Soon et al. (2001), Ng and Cardie (2002) and Luo et al. (2004) to generate most of the 29 features we use for the pairwise model. These include features that capture information from different linguistic levels: textual strings (head match, substring match, distance, frequency), morphology (mention type, coordination, possessive phrase, gender match, number match), syntax (nominal predicate, apposition, relative clause, grammatical function), and semantic match (named-entity type, is-a type, supertype). 7The opposite search direction was also tried but gave worse results. 8Taking the first mention classified as coreferent follows Soon et al. (2001)’s fir</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mentionsynchronous coreference resolution algorithm based on the Bell tree. In Proceedings of ACL 2004, pages 21–26, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP 2005,</booktitle>
<pages>25--32</pages>
<location>Vancouver.</location>
<contexts>
<context position="14254" citStr="Luo, 2005" startWordPosition="2265" endWordPosition="2266">ere is no agreement on a standard. • MUC (Vilain et al., 1995). It computes the number of links common between the true and system partitions. Recall (R) and precision (P) result from dividing it by the minimum number of links required to specify the true and the system partitions, respectively. • B3 (Bagga and Baldwin, 1998). R and P are computed for each mention and averaged at the end. For each mention, the number of common mentions between the true and the system entity is divided by the number of mentions in the true entity or in the system entity to obtain R and P, respectively. • CEAF (Luo, 2005). It finds the best one-toone alignment between true and system entities. Using true mentions and the 03 similarity function, R and P are the same and correspond to the number of common mentions between the aligned entities divided by the total number of mentions. 4 Parameter 1: Language The first experiment compared the performance of a coreference resolution system on a Germanic and a Romance language—English and Spanish— to explore to what extent language-specific issues such as zero subjects11 or grammatical gender might influence a system. Although OntoNotes and AnCora are two different c</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of HLTEMNLP 2005, pages 25–32, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>104--111</pages>
<location>Philadelphia.</location>
<contexts>
<context position="11990" citStr="Ng and Cardie (2002)" startWordPosition="1892" endWordPosition="1895">g of the document (m1–m11, m2– m11, etc.).7 When a pair (e.g., m3–m11) is classified as coreferent, additional pairwise checks are performed with all the mentions contained in the (growing) entity basket (e.g., m7–m11). Only if all the pairs are classified as coreferent is the mention under consideration attached to the existing growing entity. Otherwise, the search continues.8 5. SUPERSTRONG MATCH. Similar to STRONG MATCH but with a threshold. Coreference pairwise classifications are only accepted when TiMBL distance is smaller than 0.09.9 6. BEST MATCH. Similar to STRONG MATCH but following Ng and Cardie (2002)’s best link approach. Thus, the mention under analysis is linked to the most confident mention among the previous ones, using TiMBL’s confidence score. 7. WEAK MATCH. A simplified version of STRONG MATCH: not all mentions in the growing entity need to be classified as coreferent with the mention under analysis. A single positive pairwise decision suffices for the mention to be clustered into that entity.10 3.3 Features We follow Soon et al. (2001), Ng and Cardie (2002) and Luo et al. (2004) to generate most of the 29 features we use for the pairwise model. These include features that capture </context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of ACL 2002, pages 104–111, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Graph-cut-based anaphoricity determination for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT 2009,</booktitle>
<pages>575--583</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="7620" citStr="Ng, 2009" startWordPosition="1182" endWordPosition="1183">e resolution model allows us to reveal biases of the metrics. Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. Thus there has been no discussion of the extent to which systems are portable across languages. This paper studies the case of English and Spanish.4 Several coreference systems have been developed in the past (Culotta et al., 2007; Finkel and Manning, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Ng, 2009). It is not our aim to compete with them. Rather, we conduct three experiments under a specific setup for comparison purposes. To this end, we use a different, neutral, system, and a dataset that is small and different from official ACE test sets despite the fact that it prevents our results from being compared directly with other systems. 3 Experimental Setup 3.1 System Description The system architecture used in our experiments, CISTELL, is based on the incrementality of discourse. As a discourse evolves, it constructs a model that is updated with the new information gradually provided. A ke</context>
</contexts>
<marker>Ng, 2009</marker>
<rawString>Vincent Ng. 2009. Graph-cut-based anaphoricity determination for coreference resolution. In Proceedings of NAACL-HLT 2009, pages 575–583, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP 2008,</booktitle>
<pages>650--659</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="7583" citStr="Poon and Domingos, 2008" startWordPosition="1174" endWordPosition="1177"> including different baselines and variations of the resolution model allows us to reveal biases of the metrics. Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. Thus there has been no discussion of the extent to which systems are portable across languages. This paper studies the case of English and Spanish.4 Several coreference systems have been developed in the past (Culotta et al., 2007; Finkel and Manning, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Ng, 2009). It is not our aim to compete with them. Rather, we conduct three experiments under a specific setup for comparison purposes. To this end, we use a different, neutral, system, and a dataset that is small and different from official ACE test sets despite the fact that it prevents our results from being compared directly with other systems. 3 Experimental Setup 3.1 System Description The system architecture used in our experiments, CISTELL, is based on the incrementality of discourse. As a discourse evolves, it constructs a model that is updated with the new</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov logic. In Proceedings of EMNLP 2008, pages 650–659, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Eduard Hovy</author>
<author>Mitch Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: A unified relational semantic representation.</title>
<date>2007</date>
<booktitle>In Proceedings of ICSC 2007,</booktitle>
<pages>517--526</pages>
<location>Washington, DC.</location>
<contexts>
<context position="2971" citStr="Pradhan et al., 2007" startWordPosition="440" endWordPosition="443">standard preprocessing information with that using automatic preprocessing tools. Throughout, we apply the three principal coreference evaluation measures in use today: MUC, B3, and CEAF. We highlight the systematic preferences of each measure to reward different configurations. This raises the difficult question of why one should use one or another evaluation measure, and how one should interpret their differences in reporting changes of performance score due to ‘secondary’ factors like preprocessing information. To this end, we employ three corpora: ACE (Doddington et al., 2004), OntoNotes (Pradhan et al., 2007), and AnCora (Recasens and Mart´ı, 2009). In order to isolate the three parameters as far as possible, we benefit from a 100k-word portion (from the TDT collection) that is common to both ACE and OntoNotes. We apply the same coreference resolution system in all cases. The results show that a system’s score is not informative by itself, as different corpora or corpus parameters lead to different scores. Our goal is not to achieve the best performance to date, but rather to expose various issues raised by the choices of corpus preparation and evaluation measure and to shed light on the definitio</context>
<context position="5179" citStr="Pradhan et al., 2007" startWordPosition="793" endWordPosition="796">007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al., 2007; Hovy et al., 2006). Unlike ACE, it is not application-oriented, so coreference relations between all types of NPs are annotated. The identity relation is kept apart from the attributive relation, and it also contains gold-standard morphological, syntactic and semantic information. Since the MUC and ACE corpora are annotated with only coreference information,2 existing systems first preprocess the data using automatic tools (POS taggers, parsers, etc.) to obtain the information needed for coreference resolution. However, given that the output from automatic tools is far from perfect, it is ha</context>
</contexts>
<marker>Pradhan, Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2007</marker>
<rawString>Sameer S. Pradhan, Eduard Hovy, Mitch Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2007. Ontonotes: A unified relational semantic representation. In Proceedings of ICSC 2007, pages 517–526, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>A Deeper Look into Features for Coreference Resolution.</title>
<date>2009</date>
<booktitle>Anaphora Processing and Applications (DAARC 2009), volume 5847 of LNAI,</booktitle>
<pages>29--42</pages>
<editor>In S. Lalitha Devi, A. Branco, and R. Mitkov, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="13522" citStr="Recasens and Hovy, 2009" startWordPosition="2132" endWordPosition="2135">ntity type, is-a type, supertype). 7The opposite search direction was also tried but gave worse results. 8Taking the first mention classified as coreferent follows Soon et al. (2001)’s first-link approach. 9In TiMBL, being a memory-based learner, the closer the distance to an instance, the more confident the decision. We chose 0.09 because it appeared to offer the best results. 10STRONG and WEAK MATCH are similar to Luo et al. (2004)’s entity-mention and mention-pair models. 1425 For Spanish, we use 34 features as a few variations are needed for language-specific issues such as zero subjects (Recasens and Hovy, 2009). 3.4 Evaluation Since they sometimes provide quite different results, we evaluate using three coreference measures, as there is no agreement on a standard. • MUC (Vilain et al., 1995). It computes the number of links common between the true and system partitions. Recall (R) and precision (P) result from dividing it by the minimum number of links required to specify the true and the system partitions, respectively. • B3 (Bagga and Baldwin, 1998). R and P are computed for each mention and averaged at the end. For each mention, the number of common mentions between the true and the system entity</context>
</contexts>
<marker>Recasens, Hovy, 2009</marker>
<rawString>Marta Recasens and Eduard Hovy. 2009. A Deeper Look into Features for Coreference Resolution. In S. Lalitha Devi, A. Branco, and R. Mitkov, editors, Anaphora Processing and Applications (DAARC 2009), volume 5847 of LNAI, pages 29–42. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>M Ant`onia Mart´ı</author>
</authors>
<title>AnCoraCO: Coreferentially annotated corpora for Spanish and Catalan. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>10--1007</pages>
<location>DOI</location>
<marker>Recasens, Mart´ı, 2009</marker>
<rawString>Marta Recasens and M. Ant`onia Mart´ı. 2009. AnCoraCO: Coreferentially annotated corpora for Spanish and Catalan. Language Resources and Evaluation, DOI 10.1007/s10579-009-9108-x.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Llu´ıs M`arquez</author>
<author>Emili Sapena</author>
<author>M Ant`onia Mart´ı</author>
<author>Mariona Taul´e</author>
<author>V´eronique Hoste</author>
<author>Massimo Poesio</author>
<author>Yannick Versley</author>
</authors>
<title>SemEval-2010 Task 1: Coreference resolution in multiple languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fifth International Workshop on Semantic Evaluations (SemEval</booktitle>
<location>Uppsala,</location>
<marker>Recasens, M`arquez, Sapena, Mart´ı, Taul´e, Hoste, Poesio, Versley, 2010</marker>
<rawString>Marta Recasens, Llu´ıs M`arquez, Emili Sapena, M. Ant`onia Mart´ı, Mariona Taul´e, V´eronique Hoste, Massimo Poesio, and Yannick Versley. 2010. SemEval-2010 Task 1: Coreference resolution in multiple languages. In Proceedings of the Fifth International Workshop on Semantic Evaluations (SemEval 2010), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee M Soon</author>
<author>Hwee T Ng</author>
<author>Daniel C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="4508" citStr="Soon et al., 2001" startWordPosition="690" endWordPosition="693">23 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1423–1432, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially anno</context>
<context position="10112" citStr="Soon et al., 2001" startWordPosition="1591" endWordPosition="1594">the height of the volcano and its ranking position. In CISTELL,6 discourse entities are conceived as ‘baskets’: they are empty at the beginning of the discourse, but keep growing as new attributes (e.g., name, type, location) are predicated about them. Baskets are filled with this information, which can appear within a mention or elsewhere in the sentence. The ever-growing amount of information in a basket allows richer comparisons to new mentions encountered in the text. CISTELL follows the learning-based coreference architecture in which the task is split into classification and clustering (Soon et al., 2001; Bengtson and Roth, 2008) but combines them simultaneously. Clustering is identified with basketgrowing, the core process, and a pairwise classifier is called every time CISTELL considers whether a basket must be clustered into a (growing) basket, which might contain one or more mentions. We use a memory-based learning classifier trained with TiMBL (Daelemans and Bosch, 2005). Basket-growing is done in four different ways, explained next. 3.2 Baselines and Models In each experiment, we compute three baselines (1, 2, 3), and run CISTELL under four different models (4, 5, 6, 7). 1. ALL SINGLETO</context>
<context position="12442" citStr="Soon et al. (2001)" startWordPosition="1969" endWordPosition="1972">erence pairwise classifications are only accepted when TiMBL distance is smaller than 0.09.9 6. BEST MATCH. Similar to STRONG MATCH but following Ng and Cardie (2002)’s best link approach. Thus, the mention under analysis is linked to the most confident mention among the previous ones, using TiMBL’s confidence score. 7. WEAK MATCH. A simplified version of STRONG MATCH: not all mentions in the growing entity need to be classified as coreferent with the mention under analysis. A single positive pairwise decision suffices for the mention to be clustered into that entity.10 3.3 Features We follow Soon et al. (2001), Ng and Cardie (2002) and Luo et al. (2004) to generate most of the 29 features we use for the pairwise model. These include features that capture information from different linguistic levels: textual strings (head match, substring match, distance, frequency), morphology (mention type, coordination, possessive phrase, gender match, number match), syntax (nominal predicate, apposition, relative clause, grammatical function), and semantic match (named-entity type, is-a type, supertype). 7The opposite search direction was also tried but gave worse results. 8Taking the first mention classified as</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee M. Soon, Hwee T. Ng, and Daniel C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP 2009,</booktitle>
<pages>656--664</pages>
<contexts>
<context position="6064" citStr="Stoyanov et al. (2009)" startWordPosition="929" endWordPosition="932">emantic information. Since the MUC and ACE corpora are annotated with only coreference information,2 existing systems first preprocess the data using automatic tools (POS taggers, parsers, etc.) to obtain the information needed for coreference resolution. However, given that the output from automatic tools is far from perfect, it is hard to determine the level of performance of a coreference module acting on gold-standard preprocessing information. OntoNotes makes it possible to separate the coreference resolution problem from other tasks. Our study adds to the previously reported evidence by Stoyanov et al. (2009) that differences in corpora and in the task definitions need to be taken into account when comparing coreference resolution systems. We provide new insights as the current analysis differs in four ways. First, Stoyanov 1The ACE-2004/05 semantic types are person, organization, geo-political entity, location, facility, vehicle, weapon. 2ACE also specifies entity types and relations. et al. (2009) report on differences between MUC and ACE, while we contrast ACE and OntoNotes. Given that ACE and OntoNotes include some of the same texts but annotated according to their respective guidelines, we ca</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art. In Proceedings of ACL-IJCNLP 2009, pages 656–664, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>142--147</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-independent Named Entity Recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL 2003, pages 142–147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>Coreference resolution with and without linguistic knowledge.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="4525" citStr="Uryupina, 2006" startWordPosition="694" endWordPosition="695">he 48th Annual Meeting of the Association for Computational Linguistics, pages 1423–1432, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the </context>
</contexts>
<marker>Uryupina, 2006</marker>
<rawString>Olga Uryupina. 2006. Coreference resolution with and without linguistic knowledge. In Proceedings of LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Rodger Kibble</author>
</authors>
<title>On coreferring: Coreference in MUC and related annotation schemes.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<pages>637</pages>
<marker>van Deemter, Kibble, 2000</marker>
<rawString>Kees van Deemter and Rodger Kibble. 2000. On coreferring: Coreference in MUC and related annotation schemes. Computational Linguistics, 26(4):629– 637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6,</booktitle>
<pages>45--52</pages>
<location>San Francisco.</location>
<contexts>
<context position="13706" citStr="Vilain et al., 1995" startWordPosition="2163" endWordPosition="2166">st-link approach. 9In TiMBL, being a memory-based learner, the closer the distance to an instance, the more confident the decision. We chose 0.09 because it appeared to offer the best results. 10STRONG and WEAK MATCH are similar to Luo et al. (2004)’s entity-mention and mention-pair models. 1425 For Spanish, we use 34 features as a few variations are needed for language-specific issues such as zero subjects (Recasens and Hovy, 2009). 3.4 Evaluation Since they sometimes provide quite different results, we evaluate using three coreference measures, as there is no agreement on a standard. • MUC (Vilain et al., 1995). It computes the number of links common between the true and system partitions. Recall (R) and precision (P) result from dividing it by the minimum number of links required to specify the true and the system partitions, respectively. • B3 (Bagga and Baldwin, 1998). R and P are computed for each mention and averaged at the end. For each mention, the number of common mentions between the true and the system entity is divided by the number of mentions in the true entity or in the system entity to obtain R and P, respectively. • CEAF (Luo, 2005). It finds the best one-toone alignment between true</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of MUC-6, pages 45–52, San Francisco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>