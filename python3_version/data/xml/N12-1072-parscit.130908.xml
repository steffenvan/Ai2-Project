<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009090">
<title confidence="0.99011">
Stance Classification using Dialogic Properties of Persuasion
</title>
<author confidence="0.978266">
Marilyn A. Walker, Pranav Anand, Robert Abbott and Ricky Grant
</author>
<affiliation confidence="0.987587">
Baskin School of Engineering &amp; Linguistics Department
University of California Santa Cruz
</affiliation>
<address confidence="0.86151">
Santa Cruz, Ca. 95064, USA
</address>
<email confidence="0.999558">
maw,panand,abbott,rgrant@soe.ucsc.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999352416666667">
Public debate functions as a forum for both
expressing and forming opinions, an impor-
tant aspect of public life. We present results
for automatically classifying posts in online
debate as to the position, or STANCE that the
speaker takes on an issue, such as Pro or Con.
We show that representing the dialogic struc-
ture of the debates in terms of agreement rela-
tions between speakers, greatly improves per-
formance for stance classification, over mod-
els that operate on post content and parent-
post context alone.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998643263157895">
Public debate functions as a forum for both express-
ing and forming opinions. Three factors affect opin-
ion formation, e.g. the perlocutionary uptake of de-
bate arguments (Cialdini, 2000; Petty and Cacioppo,
1988; Petty et al., 1981). First, there is the ARGU-
MENT itself, i.e. the propositions discussed along
with the logical relations between them. Second is
the SOURCE of the argument (Chaiken, 1980), e.g.
the speaker’s expertise, or agreement relations be-
tween speakers. The third factor consists of proper-
ties of the AUDIENCE such as prior beliefs, social
identity, personality, and cognitive style (Davies,
1998). Perlocutionary uptake in debates primar-
ily occurs in the audience, who may be undecided,
while debaters typically express a particular position
or STANCE on an issue, e.g. Pro or Con, as in the
online debate dialogues in Figs. 1, 2, and 3.
Previous computational work on debate covers
three different debate settings: (1) congressional de-
</bodyText>
<note confidence="0.3622675">
Post Stance Utterance
P1 PRO I feel badly for your ignorance because although
</note>
<figureCaption confidence="0.671747782608696">
there maybe a sliver of doubt that mankind may
have evolved from previous animals, there is no
doubt that the Earth and the cosmos have gone
through evolution and are continuing to do so
P2 CON As long as there are people who doubt evolu-
tion, both lay and acedamia, then evolution is in
doubt. And please don’t feel bad for me. I am
perfectly secure in my “ignorance”.
P3 PRO By that measure, as long as organic chemistry,
physics and gravity are in doubt by both lay and
acedamia, then organic chemistry, physics and
gravity are in doubt. Gravity is a theory. Why
aren’t you giving it the same treatment you do
to evolution? Or is it because you are ignorant?
Angelic Falling anyone?
P4 CON I’m obviously ignorant. Look how many times
i’ve been given the title. “Gravity is a theory.
Why aren’t you giving it the same treatment you
do to evolution?” Because it doesn’t carry the
same weight. ;P
Figure 1: All posts linked via rebuttal links. The topic
was “Evolution”, with sides ”Yes, I Believe” vs. “No, I
Dont Believe”.
</figureCaption>
<bodyText confidence="0.945698214285714">
bates (Thomas et al., 2006; Bansal et al., 2008;
Yessenalina et al., 2010; Balahur et al., 2009; Bur-
foot et al., 2011); (2) company-internal discussion
sites (Murakami and Raymond, 2010; Agrawal et
al., 2003); and (3) online social and political public
forums (Somasundaran and Wiebe, 2009; Somasun-
daran and Wiebe, 2010; Wang and Ros´e, 2010; Bi-
ran and Rambow, 2011). Debates in online public
forums (e.g. Fig. 1) differ from debates in congress
and on company discussion sites in two ways.
First, the language is different. Online debaters
are highly involved, often using emotional and col-
orful language to make their points. These debates
are also personal, giving a strong sense of the indi-
</bodyText>
<page confidence="0.902886333333333">
592
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592–596,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.995678466666667">
vidual making the argument, and whether s/he fa-
vors emotive or factual modes of expression, e.g.
Let me answer.... NO! (P2 in Fig. 3). Other com-
mon features are sarcasm, e.g. I’m obviously igno-
rant. Look how many times i’ve been given the ti-
tle (P4 in Fig. 1), questioning another’s evidence or
assumptions: Yes there is always room for human
error, but is one accident that hasn’t happened yet
enough cause to get rid of a capital punishment? (P2
in Fig. 3), and insults: Or is it because you are ig-
norant? (P3 in Fig. 1). These properties may func-
tion to engage the audience and persuade them to
form a particular opinion, but they make computa-
tional analysis of such debates challenging, with the
best performance to date averaging 64% over several
topics (Somasundaran and Wiebe, 2010).
Post Stance Utterance
P1 Superman Batman is no match for superman. Not
only does he have SUPERnatural powers as
opposed to batman’s wit and gadgetry, but
his powers have increased in number over
the years. For example, when Superman’s
prowess was first documented in the comics
he did not have x-ray vision. It wasn’t until
his story was told on radio that he could see
through stuff. So no matter what new weapon
batman could obtain, Superman would add an-
other SUPERnatural weapon to foil the Caped
crusader.
P2 Batman Superman GAVE Batman a krytonite ring so
that Batman could take him down should he
need to. Superman did this because he knows
Batman is the only guy that could do it.
P3 Superman But, not being privy to private conversations
with S-man, you wouldn’t know that, being the
humble chap that he is, S-man allowed batman
the victory because he likes the bat and wanted
him to mantain some credibility. Honest.
P4 Batman Hmmm, this is confusing. Since we all know
that Supes doesn’t lie and yet at the time of
him being beaten by Batman he was under the
control of Poison Ivy and therefore could NOT
have LET Batman win on purpose. I have to
say that I am beginning to doubt you really are
friends with Supes at all.
</bodyText>
<figureCaption confidence="0.972783">
Figure 2: All posts linked via rebuttal links. The topic
was “Superman vs. Batman”
</figureCaption>
<bodyText confidence="0.994013518518519">
Second, the affordances of different online debate
sites provide differential support for dialogic rela-
tions between forum participants. For example, the
research of Somasundaran and Wiebe (2010), does
not explicitly model dialogue or author relations.
However debates in our corpus vary greatly by topic
on two dialogic factors: (1) the percent of posts that
are rebuttals to prior posts, and (2) the number of
Post Stance Utterance
P1 CON 69 people have been released from death row
since 1973 these people could have been killed
if there cases and evidence did not come up rong
also these people can have lost 20 years or more
to a false coviction. it is only a matter of time till
some one is killed yes u could say there doing
a good job now but it has been shown so many
times with humans that they will make the hu-
man error and cost an innocent person there life.
P2 PRO Yes there is always room for human error, but
is one accident that hasn’t happened yet enough
cause to get rid of a capital punishment? Let me
answer...NO! If you ban the death penalty crime
will skyrocket. It is an effective deterannce for
crime. The states that have strict death penalty
laws have less crime than states that don’t (Texas
vs. Michigan) Texas’s crime rate is lower than
Michigan and Texas has a higher population!!!!
</bodyText>
<figureCaption confidence="0.990011">
Figure 3: Posts linked via rebuttal links. The topic was
“Capital Punishment”, and the argument was framed as
“Yes we should keep it” vs. “No we should not”.
</figureCaption>
<bodyText confidence="0.9990078125">
posts per author. The first 5 columns of Table 2
shows the variation in these dimensions by topic.
In this paper we show that information about di-
alogic relations between authors (SOURCE factors)
improves performance for STANCE classification,
when compared to models that only have access to
properties of the ARGUMENT. We model SOURCE
relations with a graph, and add this information to
classifiers operating on the text of a post. Sec. 2
describes the corpus and our approach. Our cor-
pus is publicly available, see (Walker et al., 2012).
We show in Sec. 3 that modeling source properties
improves performance when the debates are highly
dialogic. We leave a more detailed comparison to
previous work to Sec. 3 so that we can contrast pre-
vious work with our approach.
</bodyText>
<sectionHeader confidence="0.973087" genericHeader="method">
2 Experimental Method and Approach
</sectionHeader>
<bodyText confidence="0.999900583333333">
Our corpus consists of two-sided debates from Con-
vinceme.net for 14 topics that range from play-
ful debates such as Superman vs. Batman (Fig. 2
to more heated political topics such as the Death
Penalty (Fig. 3. In total the corpus consists of 2902
two-sided debates (36,307 posts), totaling 3,080,874
words; the topic labelled debates which we use in
our experiments contain 575,818 words. On Con-
vinceme, a person starts a debate by posting a topic
or a question and providing sides such as for vs.
against. Debate participants can then post argu-
ments for one side or the other, essentially self-
</bodyText>
<page confidence="0.994911">
593
</page>
<bodyText confidence="0.9838085">
labelling their post for stance. Convinceme pro-
vides three possible sources of dialogic structure,
SIDE, REBUTTAL LINKS and TEMPORAL CONTEXT.
Timestamps for posts are only available by day and
there are no agreement links. Here, we use the self-
labelled SIDE as the stance to be predicted.
</bodyText>
<table confidence="0.997974">
Set/Factor Description
Basic Number of Characters in post, Average Word
Length, Unigrams, Bigrams
Sentiment LIWC counts and frequencies, Opinion De-
pendencies, LIWC Dependencies, negation
Argument Cue Words, Repeated Punctuation, Context,
POS-Generalized Dependencies, Quotes
</table>
<tableCaption confidence="0.998247">
Table 1: Feature Sets
</tableCaption>
<bodyText confidence="0.974856953125">
We construct features from the posts, along with a
representation of the parent post as context, and use
those features in several base classifiers. As shown
in Table 1, we distinguish between basic features,
such as length of the post and the words and bi-
grams in the post, and features capturing sentiment
and subjectivity, including using the LIWC tool for
emotion labelling (Pennebaker et al., 2001) and de-
riving generalized dependency features using LIWC
categories, as well as some limited aspects of the
argument structure, such as cue words signalling
rhetorical relations between posts, POS generalized
dependencies, and a representation of the parent post
(context). Only rebuttal posts have a parent post, and
thus values for the context features.
Figure 4: Sample maxcut to ConvinceMe siding. Sym-
bols (circle, cross, square, triangles) indicate authors and
fill colors (white,black) indicate true side. Rebuttal links
are marked by black edges, same-author links by red;
weights are 50 and -10, respectively. Edges in the max-
cut are highlighted in yellow, and the nodes in each cut
set are bounded by the green dotted line.
We then construct a graph (V,E) representing the
dialogue structure, using the rebuttal links and au-
thor identifiers from the forums site. Each node V
of the graph is a post, and edges E indicate dialogic
relations of agreement and disagreement between
posts. We assume only that authors always agree
with themselves, and that rebuttal links indicate dis-
agreement. Agreement links based on the inference
that if A, B disagree with C they agree with each
other were not added to the graph.
Maxcut attempts to partition a graph into two
sides. Fig. 4 illustrates a sample result of applying
MaxCut. Edges connecting the partitions are said
to be cut, while those within partitions are not. The
goal is to maximize the sum of cut edge weights. By
making edge weights high we reward the algorithm
for cutting the edge, by making edge weights nega-
tive we penalize the algorithm for cutting the edge.
Rebuttal links were assigned a weight +100/(num-
ber of rebuttals). Same author links were assigned a
weight -60/(number of posts by author). If author A
rebutted author B at some point, then a weight of 50
was assigned to all edges connecting posts by author
A and posts by author B. If author B rebutted author
A as well, that 50 was increased to 100. We applied
the MaxCut partitioning algorithm to this graph, and
then we orient each of the components automati-
cally using a traditional supervised classifier. We
consider each component separately where compo-
nents are defined using the original (pre-MaxCut)
graph. For each pair of partition side p E {P0, P1}
and classifier label l E {L0, L1}, we compute a
score Sp,l by summing the margins of all nodes as-
signed to that partition and label. We then compute
and compare the score differences for each partition.
Dp = Sp,L1 − Sp,L0 If Dp0 &lt; Dp1, then nodes in
partition P0 should be assigned label L0 and nodes
in P1 should be assigned label L1. Likewise, if
Dp0 &gt; Dp1, then nodes in partition P0 should be as-
signed label L1 and nodes in P1 should be assigned
label L0. If Dp0 = Dp1, then we orient the compo-
nent with a coin flip.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999898">
Table 2 summarizes our results for the base classi-
fier (JRIP) compared to using MaxCut over the so-
cial network defined by author and rebuttal links.
We report results for experiments using all the fea-
</bodyText>
<page confidence="0.664828666666667">
50
-10
594
</page>
<table confidence="0.9997513">
Topic Posts Topic Characteristics MLE MaxCut Algorithm Acc JRIP Algorithm R
Rebs P/A A&gt; 1p Acc F1 P R F1 P
Abortion 607 64% 2.73 42% 53% 82% 0.82 0.78 0.88 55% 0.55 0.52 0.59
Cats v. Dogs 162 40% 1.60 24% 53% 80% 0.78 0.80 0.76 61% 0.55 0.59 0.51
Climate Change 207 65% 2.92 41% 50% 64% 0.66 0.63 0.69 61% 0.62 0.60 0.63
Comm. v. Capitalism 214 62% 2.97 46% 55% 70% 0.67 0.66 0.68 53% 0.49 0.48 0.49
Death Penalty 331 60% 2.40 45% 56% 35% 0.31 0.29 0.34 55% 0.46 0.48 0.44
Evolution 818 66% 3.74 53% 58% 82% 0.78 0.78 0.79 56% 0.49 0.48 0.50
Existence Of God 852 76% 4.16 51% 56% 75% 0.73 0.70 0.76 52% 0.49 0.47 0.51
Firefox v. IE 233 38% 1.27 15% 79% 76% 0.47 0.44 0.49 72% 0.33 0.34 0.33
Gay Marriage 560 56% 2.01 28% 65% 84% 0.77 0.74 0.81 60% 0.43 0.43 0.44
Gun Control 135 59% 2.08 45% 63% 37% 0.24 0.21 0.27 53% 0.24 0.30 0.20
Healthcare 112 79% 3.11 53% 55% 73% 0.71 0.69 0.72 60% 0.49 0.56 0.44
Immigration 78 58% 1.95 33% 54% 33% 0.21 0.23 0.19 53% 0.39 0.48 0.33
Iphone v. Blackberry 25 44% 1.14 14% 67% 88% 0.80 0.86 0.75 71% 0.46 0.60 0.38
Israel v. Palestine 64 33% 3.37 53% 58% 85% 0.82 0.79 0.85 49% 0.48 0.42 0.56
Mac v. PC 126 37% 1.85 24% 52% 19% 0.18 0.17 0.18 46% 0.46 0.45 0.48
Marijuana legalization 229 45% 1.52 25% 71% 73% 0.56 0.52 0.60 63% 0.34 0.35 0.34
Star Wars vs. LOTR 102 44% 1.38 26% 53% 63% 0.62 0.60 0.65 63% 0.62 0.60 0.65
Superman v. Batman 146 30% 1.39 20% 54% 50% 0.40 0.44 0.37 56% 0.47 0.52 0.43
</table>
<tableCaption confidence="0.9506055">
Table 2: Results. KEY: Number of posts on the topic (Posts). Percent of Posts linked by Rebuttal links (Rebs). Posts
per author (P/A). Authors with more than one post (A &gt; 1P). Majority Class Baseline (MLE).
</tableCaption>
<bodyText confidence="0.999955396551724">
tures with k2 feature selection; we use JRIP as the
base classifier because margins are used by the auto-
matic MaxCut graph orientation algorithm. Exper-
iments with different learners (NB, SVM) did not
yield significant differences from JRIP. The results
show that, in general, representing dialogic infor-
mation in terms of a network of relations between
posts yields very large improvements. In the few
topics where performance is worse (Death Penalty,
Gun Control, Mac vs. PC, Superman vs. Batman),
the MaxCut graph gets oriented to the stance sides
the wrong way, so that the cut actually groups the
posts correctly into sides, but then assigns them to
the wrong side. For Maxcut, as expected, there are
significant correlations between the % of Rebuttals
in a debate and Precision (R = .16 ) and Recall (R=
.22), as well as between Posts/Author and Precision
(R = .25) and Recall (R = .43). This clearly indi-
cates that the degree of dialogic behavior (the graph
topology) has a strong influence on results per topic.
These results would be even stronger if all MaxCut
graphs were oriented correctly.
(Somasundaran and Wiebe, 2010) present an un-
supervised approach using ICA to stance classifica-
tion, showing that identifying argumentation struc-
ture improves performance, with a best performance
averaging 64% accuracy over all topics, but as high
as 70% for some topics. Other research classifies
the speaker’s side in a corpus of congressional floor
debates (Thomas et al., 2006; Bansal et al., 2008;
Balahur et al., 2009; Burfoot et al., 2011). Thomas
et al (2006) achieved accuracies of 71.3% by using
speaker agreement information in the graph-based
MinCut/Maxflow algorithm, as compared to accura-
cies around 70% via an an SVM classifier operating
on content alone. The best performance to date on
this corpus achieves accuracies around 82% for dif-
ferent graph-based approaches as compared to 76%
accuracy for content only classification (Burfoot et
al., 2011). Other work applies MaxCut to the reply
structure of company discussion forums, showing
that rules for identifying agreement (Murakami and
Raymond, 2010), defined on the textual content of
the post yield performance improvements over using
reply structures alone (Malouf and Mullen, 2008;
Agrawal et al., 2003)
Our results are not strictly comparable since we
use a different corpus with different properties, but
to our knowledge this is the first application of Max-
Cut to stance classification that shows large perfor-
mance improvements from modeling dialogic rela-
tions. In future work, we plan to explore whether
deeper linguistic features can yield large improve-
ments in both the base classifier and in MaxCut re-
sults, and to explore better ways of automatically
orienting the MaxCut graph to stance side. We also
hope to develop much better context features and to
make even more use of dialogue structure.
</bodyText>
<page confidence="0.998062">
595
</page>
<sectionHeader confidence="0.990353" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999565449438202">
R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of the 12th international
conference on World Wide Web, pages 529–535. ACM.
A. Balahur, Z. Kozareva, and A. Montoyo. 2009. Deter-
mining the polarity and source of opinions expressed
in political debates. Computational Linguistics and
Intelligent Text Processing, pages 468–480.
M. Bansal, C. Cardie, and L. Lee. 2008. The power
of negative thinking: Exploiting label disagreement in
the min-cut classification framework. Proceedings of
COLING: Companion volume: Posters, pages 13–16.
O. Biran and O. Rambow. 2011. Identifying justifica-
tions in written dialogs. In 2011 Fifth IEEE Inter-
national Conference on Semantic Computing (ICSC),
pages 162–168. IEEE.
C. Burfoot, S. Bird, and T. Baldwin. 2011. Collective
classification of congressional floor-debate transcripts.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1506–1515. As-
sociation for Computational Linguistics.
S. Chaiken. 1980. Heuristic versus systematic informa-
tion processing and the use of source versus message
cues in persuasion. Journal of personality and social
psychology, 39(5):752.
Robert B. Cialdini. 2000. Influence: Science and Prac-
tice (4th Edition). Allyn &amp; Bacon.
M.F. Davies. 1998. Dogmatism and belief formation:
Output interference in the processing of supporting
and contradictory cognitions. Journal of personality
and social psychology, 75(2):456.
R. Malouf and T. Mullen. 2008. Taking sides: User clas-
sification for informal online political discourse. In-
ternet Research, 18(2):177–190.
A. Murakami and R. Raymond. 2010. Support or
Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869–875.
Association for Computational Linguistics.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.
LIWC: Linguistic Inquiry and Word Count.
Richard E. Petty and John T. Cacioppo. 1988. The ef-
fects of involvement on responses to argument quan-
tity and quality: Central and peripheral routes to per-
suasion. Journal of Personality and Social Psychol-
ogy, 46(1):69–81.
R.E. Petty, J.T. Cacioppo, and R. Goldman. 1981. Per-
sonal involvement as a determinant of argument-based
persuasion. Journal of Personality and Social Psy-
chology, 41(5):847.
S. Somasundaran and J. Wiebe. 2009. Recognizing
stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 226–234. Association for Computational Lin-
guistics.
S. Somasundaran and J. Wiebe. 2010. Recognizing
stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in
Text, pages 116–124. Association for Computational
Linguistics.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the
vote: Determining support or opposition from Con-
gressional floor-debate transcripts. In Proceedings of
the 2006 conference on empirical methods in natural
language processing, pages 327–335. Association for
Computational Linguistics.
M. Walker, P. Anand, J. Fox Tree, R. Abbott, and J. King.
2012. A corpus for research on deliberation and
debate. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
LREC 2012, Istanbul, Turkey, May 23-25, 2012.
Y.C. Wang and C.P. Ros´e. 2010. Making conversational
structure explicit: identification of initiation-response
pairs within online discussions. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 673–676. Association for
Computational Linguistics.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1046–1056. Association for Computational
Linguistics.
</reference>
<page confidence="0.998654">
596
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.700477">
<title confidence="0.999803">Stance Classification using Dialogic Properties of Persuasion</title>
<author confidence="0.985599">Marilyn A Walker</author>
<author confidence="0.985599">Pranav Anand</author>
<author confidence="0.985599">Robert Abbott</author>
<author confidence="0.985599">Ricky</author>
<affiliation confidence="0.883519">Baskin School of Engineering &amp; Linguistics University of California Santa</affiliation>
<address confidence="0.975359">Santa Cruz, Ca. 95064,</address>
<email confidence="0.999623">maw,panand,abbott,rgrant@soe.ucsc.edu</email>
<abstract confidence="0.994801230769231">Public debate functions as a forum for both expressing and forming opinions, an important aspect of public life. We present results for automatically classifying posts in online as to the position, or the takes on an issue, such as We show that representing the dialogic structure of the debates in terms of agreement relations between speakers, greatly improves performance for stance classification, over models that operate on post content and parentpost context alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>S Rajagopalan</author>
<author>R Srikant</author>
<author>Y Xu</author>
</authors>
<title>Mining newsgroups using networks arising from social behavior.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th international conference on World Wide Web,</booktitle>
<pages>529--535</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3082" citStr="Agrawal et al., 2003" startWordPosition="503" endWordPosition="506"> to evolution? Or is it because you are ignorant? Angelic Falling anyone? P4 CON I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language</context>
<context position="16735" citStr="Agrawal et al., 2003" startWordPosition="2865" endWordPosition="2868">MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. The best performance to date on this corpus achieves accuracies around 82% for different graph-based approaches as compared to 76% accuracy for content only classification (Burfoot et al., 2011). Other work applies MaxCut to the reply structure of company discussion forums, showing that rules for identifying agreement (Murakami and Raymond, 2010), defined on the textual content of the post yield performance improvements over using reply structures alone (Malouf and Mullen, 2008; Agrawal et al., 2003) Our results are not strictly comparable since we use a different corpus with different properties, but to our knowledge this is the first application of MaxCut to stance classification that shows large performance improvements from modeling dialogic relations. In future work, we plan to explore whether deeper linguistic features can yield large improvements in both the base classifier and in MaxCut results, and to explore better ways of automatically orienting the MaxCut graph to stance side. We also hope to develop much better context features and to make even more use of dialogue structure.</context>
</contexts>
<marker>Agrawal, Rajagopalan, Srikant, Xu, 2003</marker>
<rawString>R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. 2003. Mining newsgroups using networks arising from social behavior. In Proceedings of the 12th international conference on World Wide Web, pages 529–535. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>Z Kozareva</author>
<author>A Montoyo</author>
</authors>
<title>Determining the polarity and source of opinions expressed in political debates.</title>
<date>2009</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>468--480</pages>
<contexts>
<context position="2969" citStr="Balahur et al., 2009" startWordPosition="486" endWordPosition="489">istry, physics and gravity are in doubt. Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? P4 CON I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi5</context>
<context position="15982" citStr="Balahur et al., 2009" startWordPosition="2751" endWordPosition="2754">y indicates that the degree of dialogic behavior (the graph topology) has a strong influence on results per topic. These results would be even stronger if all MaxCut graphs were oriented correctly. (Somasundaran and Wiebe, 2010) present an unsupervised approach using ICA to stance classification, showing that identifying argumentation structure improves performance, with a best performance averaging 64% accuracy over all topics, but as high as 70% for some topics. Other research classifies the speaker’s side in a corpus of congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Burfoot et al., 2011). Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. The best performance to date on this corpus achieves accuracies around 82% for different graph-based approaches as compared to 76% accuracy for content only classification (Burfoot et al., 2011). Other work applies MaxCut to the reply structure of company discussion forums, showing that rules for identifying agreement (Murakami and Raymond, 2010), de</context>
</contexts>
<marker>Balahur, Kozareva, Montoyo, 2009</marker>
<rawString>A. Balahur, Z. Kozareva, and A. Montoyo. 2009. Determining the polarity and source of opinions expressed in political debates. Computational Linguistics and Intelligent Text Processing, pages 468–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bansal</author>
<author>C Cardie</author>
<author>L Lee</author>
</authors>
<title>The power of negative thinking: Exploiting label disagreement in the min-cut classification framework.</title>
<date>2008</date>
<booktitle>Proceedings of COLING: Companion volume: Posters,</booktitle>
<pages>13--16</pages>
<contexts>
<context position="2921" citStr="Bansal et al., 2008" startWordPosition="478" endWordPosition="481">ubt by both lay and acedamia, then organic chemistry, physics and gravity are in doubt. Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? P4 CON I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are a</context>
<context position="15960" citStr="Bansal et al., 2008" startWordPosition="2747" endWordPosition="2750">R = .43). This clearly indicates that the degree of dialogic behavior (the graph topology) has a strong influence on results per topic. These results would be even stronger if all MaxCut graphs were oriented correctly. (Somasundaran and Wiebe, 2010) present an unsupervised approach using ICA to stance classification, showing that identifying argumentation structure improves performance, with a best performance averaging 64% accuracy over all topics, but as high as 70% for some topics. Other research classifies the speaker’s side in a corpus of congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Burfoot et al., 2011). Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. The best performance to date on this corpus achieves accuracies around 82% for different graph-based approaches as compared to 76% accuracy for content only classification (Burfoot et al., 2011). Other work applies MaxCut to the reply structure of company discussion forums, showing that rules for identifying agreement (Murakami </context>
</contexts>
<marker>Bansal, Cardie, Lee, 2008</marker>
<rawString>M. Bansal, C. Cardie, and L. Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. Proceedings of COLING: Companion volume: Posters, pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Biran</author>
<author>O Rambow</author>
</authors>
<title>Identifying justifications in written dialogs.</title>
<date>2011</date>
<booktitle>In 2011 Fifth IEEE International Conference on Semantic Computing (ICSC),</booktitle>
<pages>162--168</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3240" citStr="Biran and Rambow, 2011" startWordPosition="528" endWordPosition="532">vity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592–596, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics vidual making the argument, and whether s/he</context>
</contexts>
<marker>Biran, Rambow, 2011</marker>
<rawString>O. Biran and O. Rambow. 2011. Identifying justifications in written dialogs. In 2011 Fifth IEEE International Conference on Semantic Computing (ICSC), pages 162–168. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burfoot</author>
<author>S Bird</author>
<author>T Baldwin</author>
</authors>
<title>Collective classification of congressional floor-debate transcripts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>1506--1515</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2992" citStr="Burfoot et al., 2011" startWordPosition="490" endWordPosition="494">vity are in doubt. Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? P4 CON I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of t</context>
<context position="16005" citStr="Burfoot et al., 2011" startWordPosition="2755" endWordPosition="2758">egree of dialogic behavior (the graph topology) has a strong influence on results per topic. These results would be even stronger if all MaxCut graphs were oriented correctly. (Somasundaran and Wiebe, 2010) present an unsupervised approach using ICA to stance classification, showing that identifying argumentation structure improves performance, with a best performance averaging 64% accuracy over all topics, but as high as 70% for some topics. Other research classifies the speaker’s side in a corpus of congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Burfoot et al., 2011). Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. The best performance to date on this corpus achieves accuracies around 82% for different graph-based approaches as compared to 76% accuracy for content only classification (Burfoot et al., 2011). Other work applies MaxCut to the reply structure of company discussion forums, showing that rules for identifying agreement (Murakami and Raymond, 2010), defined on the textual co</context>
</contexts>
<marker>Burfoot, Bird, Baldwin, 2011</marker>
<rawString>C. Burfoot, S. Bird, and T. Baldwin. 2011. Collective classification of congressional floor-debate transcripts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1506–1515. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chaiken</author>
</authors>
<title>Heuristic versus systematic information processing and the use of source versus message cues in persuasion.</title>
<date>1980</date>
<booktitle>Journal of personality and social psychology,</booktitle>
<pages>39--5</pages>
<contexts>
<context position="1216" citStr="Chaiken, 1980" startWordPosition="186" endWordPosition="187">ucture of the debates in terms of agreement relations between speakers, greatly improves performance for stance classification, over models that operate on post content and parentpost context alone. 1 Introduction Public debate functions as a forum for both expressing and forming opinions. Three factors affect opinion formation, e.g. the perlocutionary uptake of debate arguments (Cialdini, 2000; Petty and Cacioppo, 1988; Petty et al., 1981). First, there is the ARGUMENT itself, i.e. the propositions discussed along with the logical relations between them. Second is the SOURCE of the argument (Chaiken, 1980), e.g. the speaker’s expertise, or agreement relations between speakers. The third factor consists of properties of the AUDIENCE such as prior beliefs, social identity, personality, and cognitive style (Davies, 1998). Perlocutionary uptake in debates primarily occurs in the audience, who may be undecided, while debaters typically express a particular position or STANCE on an issue, e.g. Pro or Con, as in the online debate dialogues in Figs. 1, 2, and 3. Previous computational work on debate covers three different debate settings: (1) congressional dePost Stance Utterance P1 PRO I feel badly fo</context>
</contexts>
<marker>Chaiken, 1980</marker>
<rawString>S. Chaiken. 1980. Heuristic versus systematic information processing and the use of source versus message cues in persuasion. Journal of personality and social psychology, 39(5):752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert B Cialdini</author>
</authors>
<date>2000</date>
<booktitle>Influence: Science and Practice (4th Edition).</booktitle>
<publisher>Allyn &amp; Bacon.</publisher>
<contexts>
<context position="999" citStr="Cialdini, 2000" startWordPosition="151" endWordPosition="152"> of public life. We present results for automatically classifying posts in online debate as to the position, or STANCE that the speaker takes on an issue, such as Pro or Con. We show that representing the dialogic structure of the debates in terms of agreement relations between speakers, greatly improves performance for stance classification, over models that operate on post content and parentpost context alone. 1 Introduction Public debate functions as a forum for both expressing and forming opinions. Three factors affect opinion formation, e.g. the perlocutionary uptake of debate arguments (Cialdini, 2000; Petty and Cacioppo, 1988; Petty et al., 1981). First, there is the ARGUMENT itself, i.e. the propositions discussed along with the logical relations between them. Second is the SOURCE of the argument (Chaiken, 1980), e.g. the speaker’s expertise, or agreement relations between speakers. The third factor consists of properties of the AUDIENCE such as prior beliefs, social identity, personality, and cognitive style (Davies, 1998). Perlocutionary uptake in debates primarily occurs in the audience, who may be undecided, while debaters typically express a particular position or STANCE on an issue</context>
</contexts>
<marker>Cialdini, 2000</marker>
<rawString>Robert B. Cialdini. 2000. Influence: Science and Practice (4th Edition). Allyn &amp; Bacon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Davies</author>
</authors>
<title>Dogmatism and belief formation: Output interference in the processing of supporting and contradictory cognitions. Journal of personality and social psychology,</title>
<date>1998</date>
<pages>75--2</pages>
<contexts>
<context position="1432" citStr="Davies, 1998" startWordPosition="218" endWordPosition="219">lic debate functions as a forum for both expressing and forming opinions. Three factors affect opinion formation, e.g. the perlocutionary uptake of debate arguments (Cialdini, 2000; Petty and Cacioppo, 1988; Petty et al., 1981). First, there is the ARGUMENT itself, i.e. the propositions discussed along with the logical relations between them. Second is the SOURCE of the argument (Chaiken, 1980), e.g. the speaker’s expertise, or agreement relations between speakers. The third factor consists of properties of the AUDIENCE such as prior beliefs, social identity, personality, and cognitive style (Davies, 1998). Perlocutionary uptake in debates primarily occurs in the audience, who may be undecided, while debaters typically express a particular position or STANCE on an issue, e.g. Pro or Con, as in the online debate dialogues in Figs. 1, 2, and 3. Previous computational work on debate covers three different debate settings: (1) congressional dePost Stance Utterance P1 PRO I feel badly for your ignorance because although there maybe a sliver of doubt that mankind may have evolved from previous animals, there is no doubt that the Earth and the cosmos have gone through evolution and are continuing to d</context>
</contexts>
<marker>Davies, 1998</marker>
<rawString>M.F. Davies. 1998. Dogmatism and belief formation: Output interference in the processing of supporting and contradictory cognitions. Journal of personality and social psychology, 75(2):456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
<author>T Mullen</author>
</authors>
<title>Taking sides: User classification for informal online political discourse.</title>
<date>2008</date>
<journal>Internet Research,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="16712" citStr="Malouf and Mullen, 2008" startWordPosition="2861" endWordPosition="2864">ation in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. The best performance to date on this corpus achieves accuracies around 82% for different graph-based approaches as compared to 76% accuracy for content only classification (Burfoot et al., 2011). Other work applies MaxCut to the reply structure of company discussion forums, showing that rules for identifying agreement (Murakami and Raymond, 2010), defined on the textual content of the post yield performance improvements over using reply structures alone (Malouf and Mullen, 2008; Agrawal et al., 2003) Our results are not strictly comparable since we use a different corpus with different properties, but to our knowledge this is the first application of MaxCut to stance classification that shows large performance improvements from modeling dialogic relations. In future work, we plan to explore whether deeper linguistic features can yield large improvements in both the base classifier and in MaxCut results, and to explore better ways of automatically orienting the MaxCut graph to stance side. We also hope to develop much better context features and to make even more use</context>
</contexts>
<marker>Malouf, Mullen, 2008</marker>
<rawString>R. Malouf and T. Mullen. 2008. Taking sides: User classification for informal online political discourse. Internet Research, 18(2):177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Murakami</author>
<author>R Raymond</author>
</authors>
<title>Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>869--875</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3059" citStr="Murakami and Raymond, 2010" startWordPosition="499" endWordPosition="502">it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? P4 CON I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of the North American Chapter of the Association for Computational Ling</context>
<context position="16578" citStr="Murakami and Raymond, 2010" startWordPosition="2841" endWordPosition="2844">al., 2008; Balahur et al., 2009; Burfoot et al., 2011). Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. The best performance to date on this corpus achieves accuracies around 82% for different graph-based approaches as compared to 76% accuracy for content only classification (Burfoot et al., 2011). Other work applies MaxCut to the reply structure of company discussion forums, showing that rules for identifying agreement (Murakami and Raymond, 2010), defined on the textual content of the post yield performance improvements over using reply structures alone (Malouf and Mullen, 2008; Agrawal et al., 2003) Our results are not strictly comparable since we use a different corpus with different properties, but to our knowledge this is the first application of MaxCut to stance classification that shows large performance improvements from modeling dialogic relations. In future work, we plan to explore whether deeper linguistic features can yield large improvements in both the base classifier and in MaxCut results, and to explore better ways of a</context>
</contexts>
<marker>Murakami, Raymond, 2010</marker>
<rawString>A. Murakami and R. Raymond. 2010. Support or Oppose? Classifying Positions in Online Debates from Reply Activities and Opinion Expressions. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 869–875. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>L E Francis</author>
<author>R J Booth</author>
</authors>
<title>LIWC: Linguistic Inquiry and Word Count.</title>
<date>2001</date>
<contexts>
<context position="9765" citStr="Pennebaker et al., 2001" startWordPosition="1638" endWordPosition="1641">th, Unigrams, Bigrams Sentiment LIWC counts and frequencies, Opinion Dependencies, LIWC Dependencies, negation Argument Cue Words, Repeated Punctuation, Context, POS-Generalized Dependencies, Quotes Table 1: Feature Sets We construct features from the posts, along with a representation of the parent post as context, and use those features in several base classifiers. As shown in Table 1, we distinguish between basic features, such as length of the post and the words and bigrams in the post, and features capturing sentiment and subjectivity, including using the LIWC tool for emotion labelling (Pennebaker et al., 2001) and deriving generalized dependency features using LIWC categories, as well as some limited aspects of the argument structure, such as cue words signalling rhetorical relations between posts, POS generalized dependencies, and a representation of the parent post (context). Only rebuttal posts have a parent post, and thus values for the context features. Figure 4: Sample maxcut to ConvinceMe siding. Symbols (circle, cross, square, triangles) indicate authors and fill colors (white,black) indicate true side. Rebuttal links are marked by black edges, same-author links by red; weights are 50 and -</context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001. LIWC: Linguistic Inquiry and Word Count.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard E Petty</author>
<author>John T Cacioppo</author>
</authors>
<title>The effects of involvement on responses to argument quantity and quality: Central and peripheral routes to persuasion.</title>
<date>1988</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="1025" citStr="Petty and Cacioppo, 1988" startWordPosition="153" endWordPosition="156"> We present results for automatically classifying posts in online debate as to the position, or STANCE that the speaker takes on an issue, such as Pro or Con. We show that representing the dialogic structure of the debates in terms of agreement relations between speakers, greatly improves performance for stance classification, over models that operate on post content and parentpost context alone. 1 Introduction Public debate functions as a forum for both expressing and forming opinions. Three factors affect opinion formation, e.g. the perlocutionary uptake of debate arguments (Cialdini, 2000; Petty and Cacioppo, 1988; Petty et al., 1981). First, there is the ARGUMENT itself, i.e. the propositions discussed along with the logical relations between them. Second is the SOURCE of the argument (Chaiken, 1980), e.g. the speaker’s expertise, or agreement relations between speakers. The third factor consists of properties of the AUDIENCE such as prior beliefs, social identity, personality, and cognitive style (Davies, 1998). Perlocutionary uptake in debates primarily occurs in the audience, who may be undecided, while debaters typically express a particular position or STANCE on an issue, e.g. Pro or Con, as in t</context>
</contexts>
<marker>Petty, Cacioppo, 1988</marker>
<rawString>Richard E. Petty and John T. Cacioppo. 1988. The effects of involvement on responses to argument quantity and quality: Central and peripheral routes to persuasion. Journal of Personality and Social Psychology, 46(1):69–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Petty</author>
<author>J T Cacioppo</author>
<author>R Goldman</author>
</authors>
<title>Personal involvement as a determinant of argument-based persuasion.</title>
<date>1981</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>41</volume>
<issue>5</issue>
<contexts>
<context position="1046" citStr="Petty et al., 1981" startWordPosition="157" endWordPosition="160">tomatically classifying posts in online debate as to the position, or STANCE that the speaker takes on an issue, such as Pro or Con. We show that representing the dialogic structure of the debates in terms of agreement relations between speakers, greatly improves performance for stance classification, over models that operate on post content and parentpost context alone. 1 Introduction Public debate functions as a forum for both expressing and forming opinions. Three factors affect opinion formation, e.g. the perlocutionary uptake of debate arguments (Cialdini, 2000; Petty and Cacioppo, 1988; Petty et al., 1981). First, there is the ARGUMENT itself, i.e. the propositions discussed along with the logical relations between them. Second is the SOURCE of the argument (Chaiken, 1980), e.g. the speaker’s expertise, or agreement relations between speakers. The third factor consists of properties of the AUDIENCE such as prior beliefs, social identity, personality, and cognitive style (Davies, 1998). Perlocutionary uptake in debates primarily occurs in the audience, who may be undecided, while debaters typically express a particular position or STANCE on an issue, e.g. Pro or Con, as in the online debate dial</context>
</contexts>
<marker>Petty, Cacioppo, Goldman, 1981</marker>
<rawString>R.E. Petty, J.T. Cacioppo, and R. Goldman. 1981. Personal involvement as a determinant of argument-based persuasion. Journal of Personality and Social Psychology, 41(5):847.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>226--234</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3163" citStr="Somasundaran and Wiebe, 2009" startWordPosition="515" endWordPosition="518">P4 CON I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592–596, Montr´eal, Canada, June 3-8, 2012. c�2012 Associati</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>S. Somasundaran and J. Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 226–234. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>116--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3193" citStr="Somasundaran and Wiebe, 2010" startWordPosition="519" endWordPosition="523"> Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a strong sense of the indi592 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592–596, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguisti</context>
<context position="4585" citStr="Somasundaran and Wiebe, 2010" startWordPosition="751" endWordPosition="754"> sarcasm, e.g. I’m obviously ignorant. Look how many times i’ve been given the title (P4 in Fig. 1), questioning another’s evidence or assumptions: Yes there is always room for human error, but is one accident that hasn’t happened yet enough cause to get rid of a capital punishment? (P2 in Fig. 3), and insults: Or is it because you are ignorant? (P3 in Fig. 1). These properties may function to engage the audience and persuade them to form a particular opinion, but they make computational analysis of such debates challenging, with the best performance to date averaging 64% over several topics (Somasundaran and Wiebe, 2010). Post Stance Utterance P1 Superman Batman is no match for superman. Not only does he have SUPERnatural powers as opposed to batman’s wit and gadgetry, but his powers have increased in number over the years. For example, when Superman’s prowess was first documented in the comics he did not have x-ray vision. It wasn’t until his story was told on radio that he could see through stuff. So no matter what new weapon batman could obtain, Superman would add another SUPERnatural weapon to foil the Caped crusader. P2 Batman Superman GAVE Batman a krytonite ring so that Batman could take him down shoul</context>
<context position="6097" citStr="Somasundaran and Wiebe (2010)" startWordPosition="1015" endWordPosition="1018">ted him to mantain some credibility. Honest. P4 Batman Hmmm, this is confusing. Since we all know that Supes doesn’t lie and yet at the time of him being beaten by Batman he was under the control of Poison Ivy and therefore could NOT have LET Batman win on purpose. I have to say that I am beginning to doubt you really are friends with Supes at all. Figure 2: All posts linked via rebuttal links. The topic was “Superman vs. Batman” Second, the affordances of different online debate sites provide differential support for dialogic relations between forum participants. For example, the research of Somasundaran and Wiebe (2010), does not explicitly model dialogue or author relations. However debates in our corpus vary greatly by topic on two dialogic factors: (1) the percent of posts that are rebuttals to prior posts, and (2) the number of Post Stance Utterance P1 CON 69 people have been released from death row since 1973 these people could have been killed if there cases and evidence did not come up rong also these people can have lost 20 years or more to a false coviction. it is only a matter of time till some one is killed yes u could say there doing a good job now but it has been shown so many times with humans </context>
<context position="15590" citStr="Somasundaran and Wiebe, 2010" startWordPosition="2689" endWordPosition="2692">MaxCut graph gets oriented to the stance sides the wrong way, so that the cut actually groups the posts correctly into sides, but then assigns them to the wrong side. For Maxcut, as expected, there are significant correlations between the % of Rebuttals in a debate and Precision (R = .16 ) and Recall (R= .22), as well as between Posts/Author and Precision (R = .25) and Recall (R = .43). This clearly indicates that the degree of dialogic behavior (the graph topology) has a strong influence on results per topic. These results would be even stronger if all MaxCut graphs were oriented correctly. (Somasundaran and Wiebe, 2010) present an unsupervised approach using ICA to stance classification, showing that identifying argumentation structure improves performance, with a best performance averaging 64% accuracy over all topics, but as high as 70% for some topics. Other research classifies the speaker’s side in a corpus of congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Burfoot et al., 2011). Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM</context>
</contexts>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>S. Somasundaran and J. Wiebe. 2010. Recognizing stances in ideological on-line debates. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116–124. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thomas</author>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 conference on empirical methods in natural language processing,</booktitle>
<pages>327--335</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2900" citStr="Thomas et al., 2006" startWordPosition="474" endWordPosition="477">and gravity are in doubt by both lay and acedamia, then organic chemistry, physics and gravity are in doubt. Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? P4 CON I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points</context>
<context position="15939" citStr="Thomas et al., 2006" startWordPosition="2743" endWordPosition="2746">R = .25) and Recall (R = .43). This clearly indicates that the degree of dialogic behavior (the graph topology) has a strong influence on results per topic. These results would be even stronger if all MaxCut graphs were oriented correctly. (Somasundaran and Wiebe, 2010) present an unsupervised approach using ICA to stance classification, showing that identifying argumentation structure improves performance, with a best performance averaging 64% accuracy over all topics, but as high as 70% for some topics. Other research classifies the speaker’s side in a corpus of congressional floor debates (Thomas et al., 2006; Bansal et al., 2008; Balahur et al., 2009; Burfoot et al., 2011). Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. The best performance to date on this corpus achieves accuracies around 82% for different graph-based approaches as compared to 76% accuracy for content only classification (Burfoot et al., 2011). Other work applies MaxCut to the reply structure of company discussion forums, showing that rules for identifying</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 327–335. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>P Anand</author>
<author>J Fox Tree</author>
<author>R Abbott</author>
<author>J King</author>
</authors>
<title>A corpus for research on deliberation and debate.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012,</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="7912" citStr="Walker et al., 2012" startWordPosition="1339" endWordPosition="1342">, and the argument was framed as “Yes we should keep it” vs. “No we should not”. posts per author. The first 5 columns of Table 2 shows the variation in these dimensions by topic. In this paper we show that information about dialogic relations between authors (SOURCE factors) improves performance for STANCE classification, when compared to models that only have access to properties of the ARGUMENT. We model SOURCE relations with a graph, and add this information to classifiers operating on the text of a post. Sec. 2 describes the corpus and our approach. Our corpus is publicly available, see (Walker et al., 2012). We show in Sec. 3 that modeling source properties improves performance when the debates are highly dialogic. We leave a more detailed comparison to previous work to Sec. 3 so that we can contrast previous work with our approach. 2 Experimental Method and Approach Our corpus consists of two-sided debates from Convinceme.net for 14 topics that range from playful debates such as Superman vs. Batman (Fig. 2 to more heated political topics such as the Death Penalty (Fig. 3. In total the corpus consists of 2902 two-sided debates (36,307 posts), totaling 3,080,874 words; the topic labelled debates </context>
</contexts>
<marker>Walker, Anand, Tree, Abbott, King, 2012</marker>
<rawString>M. Walker, P. Anand, J. Fox Tree, R. Abbott, and J. King. 2012. A corpus for research on deliberation and debate. In Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wang</author>
<author>C P Ros´e</author>
</authors>
<title>Making conversational structure explicit: identification of initiation-response pairs within online discussions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>673--676</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Wang, Ros´e, 2010</marker>
<rawString>Y.C. Wang and C.P. Ros´e. 2010. Making conversational structure explicit: identification of initiation-response pairs within online discussions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 673–676. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yessenalina</author>
<author>Y Yue</author>
<author>C Cardie</author>
</authors>
<title>Multilevel structured models for document-level sentiment classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1046--1056</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2947" citStr="Yessenalina et al., 2010" startWordPosition="482" endWordPosition="485">cedamia, then organic chemistry, physics and gravity are in doubt. Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution? Or is it because you are ignorant? Angelic Falling anyone? P4 CON I’m obviously ignorant. Look how many times i’ve been given the title. “Gravity is a theory. Why aren’t you giving it the same treatment you do to evolution?” Because it doesn’t carry the same weight. ;P Figure 1: All posts linked via rebuttal links. The topic was “Evolution”, with sides ”Yes, I Believe” vs. “No, I Dont Believe”. bates (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009; Burfoot et al., 2011); (2) company-internal discussion sites (Murakami and Raymond, 2010; Agrawal et al., 2003); and (3) online social and political public forums (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Wang and Ros´e, 2010; Biran and Rambow, 2011). Debates in online public forums (e.g. Fig. 1) differ from debates in congress and on company discussion sites in two ways. First, the language is different. Online debaters are highly involved, often using emotional and colorful language to make their points. These debates are also personal, giving a str</context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multilevel structured models for document-level sentiment classification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1046–1056. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>