<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021338">
<title confidence="0.9978435">
Probabilistic Document Modeling for Syntax Removal in Text
Summarization
</title>
<author confidence="0.998939">
William M. Darling
</author>
<affiliation confidence="0.99823">
School of Computer Science
University of Guelph
</affiliation>
<address confidence="0.936484">
50 Stone Rd E, Guelph, ON
N1G 2W1 Canada
</address>
<email confidence="0.998408">
wdarling@uoguelph.ca
</email>
<author confidence="0.994142">
Fei Song
</author>
<affiliation confidence="0.998201">
School of Computer Science
University of Guelph
</affiliation>
<address confidence="0.936633">
50 Stone Rd E, Guelph, ON
N1G 2W1 Canada
</address>
<email confidence="0.998815">
fsong@uoguelph.ca
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889304347826">
Statistical approaches to automatic text sum-
marization based on term frequency continue
to perform on par with more complex sum-
marization methods. To compute useful fre-
quency statistics, however, the semantically
important words must be separated from the
low-content function words. The standard ap-
proach of using an a priori stopword list tends
to result in both undercoverage, where syn-
tactical words are seen as semantically rele-
vant, and overcoverage, where words related
to content are ignored. We present a genera-
tive probabilistic modeling approach to build-
ing content distributions for use with statisti-
cal multi-document summarization where the
syntax words are learned directly from the
data with a Hidden Markov Model and are
thereby deemphasized in the term frequency
statistics. This approach is compared to both a
stopword-list and POS-tagging approach and
our method demonstrates improved coverage
on the DUC 2006 and TAC 2010 datasets us-
ing the ROUGE metric.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990256675">
While the dominant problem in Information Re-
trieval in the first part of the century was finding
relevant information within a datastream that is ex-
ponentially growing, the problem has arguably tran-
sitioned from finding what we are looking for to sift-
ing through it. We can now be quite confident that
search engines like Google will return several pages
relevant to our queries, but rarely does one have time
to go through the enormous amount of data that is
642
supplied. Therefore, automatic text summarization,
which aims at providing a shorter representation of
the salient parts of a large amount of information,
has been steadily growing in both importance and
popularity over the last several years. The summa-
rization tracks at the Document Understanding Con-
ference (DUC), and its successor the Text Analysis
Conference (TAC)1, have helped fuel this interest by
hosting yearly competitions to promote the advance-
ment of automatic text summarization methods.
The tasks at the DUC and TAC involve taking
a set of documents as input and outputting a short
summary (either 100 or 250 words, depending on
the year) containing what the system deems to be the
most important information contained in the original
documents. While a system matching human perfor-
mance will likely require deep language understand-
ing, most existing systems use an extractive, rather
than abstractive, approach whereby the most salient
sentences are extracted from the original documents
and strung together to form an output summary.2
In this paper, we present a summarization model
based on (Griffiths et al., 2005) that integrates top-
ics and syntax. We show that a simple model that
separates syntax and content words and uses the
content distribution as a representative model of
the important words in a document set can achieve
high performance in multi-document summariza-
tion, competitive with state-of-the-art summariza-
tion systems.
</bodyText>
<footnote confidence="0.995232666666667">
1http://www.nist.gov/tac
2NLP techniques such as sentence compression are often
used, but this is far from abstractive summarization.
</footnote>
<note confidence="0.8773315">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 642–647,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.3892665" genericHeader="related work">
2 Related Work one of its syntax classes is replaced with an LDA-
2.1 SumBasic like topic model. When the model is in the semantic
</sectionHeader>
<bodyText confidence="0.974453978723404">
Nenkova et al. (2006) describe SumBasic, a simple, class state, it chooses a topic from the given docu-
yet high-performing summarization system based on ment’s topic distribution, samples a word from that
term frequency. While the methodology underly- topic’s word distribution, and generates it. Other-
ing SumBasic departs very little from the pioneer- wise, the model samples a word from the current
ing summarization work performed at IBM in the syntax class in the HMM and outputs that word.
1950’s (Luhn, 1958), methods based on simple word 3 Our Summarization Model
statistics continue to outperform more complicated Nenkova et al. (2006) show that using term fre-
approaches to automatic summarization.3 Nenkova quency is a powerful approach to modeling human
et al. (2006) empirically showed that a word that ap- summarization. Nevertheless, for SumBasic to per-
pears more frequently in the original text will be form well, stop-words must be removed from the
more likely to appear in a human generated sum- composition scoring function. Because these words
mary. add nothing to the content of a summary, if they
The SumBasic algorithm uses the empirical uni- were not removed for the scoring calculation, the
gram probability distribution of the non-stop-words sentence scores would no longer provide a good fit
in the input such that for each word w, p(w) _ , with sentences that a human summarizer would find
where n,,, is the number of occurrences of word w salient. However, by simply removing pre-selected
and N is the total number of words in the input. Sen- words from a list, we will inevitably miss words
tences are then scored based on a composition func- that in different contexts would be considered non-
tion CF(·) that composes the score for the sentence content words. In contrast, if too many words are
based on its contained words. The most commonly removed, the opposite problem appears and we may
used composition function adds the probabilities of remove important information that would be useful
the words in a sentence together, and then divides by in determining sentence scores. These problems are
the number of words in that sentence. However, to referred to as undercoverage and overcoverage, re-
reduce redundancy, once a sentence has been chosen spectively.
for summary inclusion, the probability distribution To alleviate this problem, we would like to put
is recalculated such that any word that appears in less probability mass for our document set proba-
the chosen sentence has its probability diminished. bility distribution on non-content words and more
Sentences are continually marked for inclusion un- on words with strong semantic meaning. One ap-
til the summary word-limit is reached. Despite its proach that could achieve this would be to build sep-
simplicity, SumBasic continues to be one of the top arate stopword lists for specific domains, and there
summarization performers in both manual and auto- are approaches to automatically build such lists (Lo
matic evaluations (Nenkova et al., 2006). et al., 2005). However, a list-based approach can-
2.2 Modeling Content and Syntax not take context into account and therefore, among
Griffiths et al. (2005) describe a composite gener- other things, will encounter problems with poly-
ative model that combines syntax and semantics. semy and synonymy. Another approach would be to
The semantic portion of the model is similar to La- use a part-of-speech (POS) tagger on each sentence
tent Dirichlet Allocation and models long-range the- and ignore all non-noun words because high-content
matic word dependencies with a set of topics, while words are almost exclusively nouns. One could also
short-range (sentence-wide) word dependencies are include verbs, adverbs, adjectives, or any combina-
modeled with syntax classes using a Hidden Markov tion thereof, and therefore solve some of the context-
Model. The model has an HMM at its base where based problems associated with using a stopword
list. Nevertheless, this approach introduces deeper
context-related problems of its own (a noun, for ex-
ample, is not always a content word). A separate ap-
3A system based on SumBasic was one of the top performers
at the Text Analysis Conference 2010 summarization track.
643
</bodyText>
<figureCaption confidence="0.982754666666667">
Figure 1: Graphical model depiction of our content and
syntax summarization method. There are D document
sets, M documents in each set, NM words in document
</figureCaption>
<bodyText confidence="0.995476161290322">
M, and C syntax classes.
proach would be to model the syntax and semantic
words used in a document collection in an HMM, as
in Griffiths et al. (2005), and use the semantic class
as the content-word distribution for summarization.
Our approach to summarization builds on Sum-
Basic, and combines it with a similar approach
to separating content and syntax distributions as
that described in (Griffiths et al., 2005). Like
(Haghighi and Vanderwende, 2009), (Daum´e and
Marcu, 2006), and (Barzilay and Lee, 2004), we
model words as being generated from latent distribu-
tions. However, instead of background, content, and
document-specific distributions, we model all words
in a document set as being there for one of only two
purposes: a semantic (content) purpose, or a syntac-
tic (functional) purpose. We model the syntax class
distributions using an HMM and model the content
words using a simple language model. The princi-
pal difference between our generative model and the
one described in (Griffiths et al., 2005) is that we
simplify the model by assuming that each document
is generated solely from one topic distribution that is
shared throughout each document set. This results in
a smoothed language model for each document set’s
content distribution where the counts from content
words (as determined through inference) are used to
determine their probability, and the syntax words are
essentially discarded.
Therefore, our model describes the process of
generating a document as traversing an HMM and
</bodyText>
<figureCaption confidence="0.993395">
Figure 2: Portion of Content and Syntax HMM. The
left and right states show the top words for those syntax
classes while the middle state shows the top words for the
given document set’s content distribution.
</figureCaption>
<bodyText confidence="0.9998354">
emitting either a content word from a single topic’s
(document set’s) content word distribution, or a syn-
tax word from one of C corpus-wide syntax classes
where C is a parameter input to the algorithm. More
specifically, a document is generated as follows:
</bodyText>
<listItem confidence="0.997231714285714">
1. Choose a topic z corresponding to the given
document set (z = {z1, ..., zkI where k is the
number of document sets to summarize.)
2. For each word wz in document d
(a) Draw cz from 7r(&apos;i−1)
(b) If cz = 1, then draw wz from &amp;), other-
wise draw wz from 0(&apos;i)
</listItem>
<bodyText confidence="0.999676428571429">
Each class cz and topic z correspond to multinomial
distributions over words, and transitions between
classes follow the transition distribution 7r(&apos;i−1).
When cz = 1, a content word is emitted from
the topic word distribution ((�) for the given doc-
ument set z. Otherwise, a syntax word is emitted
from the corpus-wide syntax word distribution 0(&apos;i).
The word distributions and transition vectors are all
drawn from Dirichlet priors. A graphical model de-
piction of this distribution is shown in Figure 1. A
portion of an example HMM (from the DUC 2006
dataset) is shown in Figure 2 with the most proba-
ble words in the content class in the middle and two
syntax classes on either side of it.
</bodyText>
<subsectionHeader confidence="0.841079">
3.1 Inference
</subsectionHeader>
<bodyText confidence="0.9998195">
Because the posterior probability of the content
(document set) word distributions and syntax class
word distributions cannot be solved analytically, as
with many topic modeling approaches, we appeal
</bodyText>
<figure confidence="0.99507478125">
71
Y
0
C W
C
6
NM
M
R
Z
D
�
in
at
of
on
with
by
...
...
el
nino
weather
pacific
ocean
normal
temperatures
said
told
asked
say
says
</figure>
<page confidence="0.99636">
644
</page>
<bodyText confidence="0.999977666666667">
to an approximation. Following Griffiths et al.
(2005), we use Markov Chain Monte Carlo (see,
e.g. (Gilks et al., 1999)), or more specifically, “col-
lapsed” Gibbs sampling where the multinomial pa-
rameters are integrated out.4 We ran our sampler for
between 500 and 5,000 iterations (though the dis-
tributions would typically converge by 1,000 itera-
tions), and chose between 5 and 10 (with negligible
changes in results) for the cardinality of the classes
set C. We leave optimizing the number of syntax
classes, or determining them directly from the data,
for future work.
</bodyText>
<subsectionHeader confidence="0.999023">
3.2 Summarization
</subsectionHeader>
<bodyText confidence="0.99949837037037">
Here we describe how we use the estimated topic
and syntax distributions to perform extractive multi-
document summarization. We follow the SumBasic
algorithm, but replace the empirical unigram distri-
bution of the document set with the learned topic
distributions for the given documents. This models
the effect of not only ignoring stop-words, but also
reduces the amount of probability mass in the distri-
bution placed on functional words that serve no se-
mantic purpose and that would likely be less useful
in a summary. Because this is a fully probabilistic
model, we do not entirely “ignore” stop-words; in-
stead, the model forces the probability mass of these
words to the syntax classes.
For a given document set to be summarized, each
sentence is assigned a score corresponding to the
average probability of the words contained within
it: Score(S) = |s |E.CS p(w). In SumBasic,
p(wi) = ;. In our model, SyntaxSum, p(wi) =
p(wi|(W), where (W is a multinomial distribution
over the corpus’ fixed vocabulary that puts high
probabilities on content words that are used often
in the given document set and low probabilities
on words that are more important in other syntax
classes. The middle node in Figure 2 is a true repre-
sentation of the top words in the (W distribution for
document set 43 in the DUC 2006 dataset.
</bodyText>
<sectionHeader confidence="0.998356" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.996686">
Here we describe our experiments and give quanti-
tative results using the ROUGE automatic text sum-
</bodyText>
<footnote confidence="0.9947975">
4See http://lingpipe.files.wordpress.com/
2010/07/lda1.pdf for more information.
</footnote>
<table confidence="0.999618555555556">
Method R-1 ROUGE R-SU4 R-1 ROUGE (-s)
R-2 R-2 R-SU4
SB- 37.0 5.5 11.0 23.3 3.8 6.2
SumBasic 38.1 6.7 11.9 29.4 5.3 8.1
N 36.8 7.0 12.2 25.5 4.8 7.3
N,V 36.9 6.5 12.0 24.4 4.4 6.9
N,J 37.4 6.8 12.3 26.5 5.0 7.7
N,V,J 37.4 6.8 12.2 25.5 4.9 7.4
SBH 38.9 7.3 12.6 30.7 5.9 8.7
</table>
<tableCaption confidence="0.96314125">
Table 1: ROUGE Results on the DUC 2006 dataset. Re-
sults statistically significantly higher than SumBasic (as
determined by a pairwise t-test with 99% confidence) are
displayed in bold.
</tableCaption>
<bodyText confidence="0.999673571428571">
marization metric for unigram (R-1), bigram (R-2),
and skip-4 bigram (R-SU4) recall both with and
without (-s) stopwords removed (Lin, 2004). We
tested our models on the popular DUC 2006 dataset
which aids in model comparison and also on the
more recent TAC 2010 dataset. The DUC 2006
dataset consists of 50 sets of 25 news articles each,
whereas the TAC 2010 dataset consists of 46 sets of
10 news articles each.5 For DUC 2006, summaries
are a maximum of 250 words; for TAC 2010, they
can be at most 100. Our approach is compared to
using an a priori stopword list, and using a POS-
tagger to build distributions of words coming from
only a subset of the parts-of-speech.
</bodyText>
<subsectionHeader confidence="0.993642">
4.1 SumBasic
</subsectionHeader>
<bodyText confidence="0.999657230769231">
To cogently demonstrate the effect of ignoring non-
semantic words in term frequency-based summa-
rization, we implemented two initial versions of
SumBasic. The first, SB-, does not ignore stop-
words while the second, SumBasic, ignores all stop-
words from a list included in the Python NLTK li-
brary.6 For SumBasic without stop-word removal
(SB-), we obtain 3.8 R-2 and 6.2 R-SU4 (with the -s
flag).7 With stop-words removed from the sentence
scoring calculation (SumBasic), our results increase
to 5.3 R-2 and 8.1 R-SU4, a significantly large in-
crease. For complete ROUGE results of all of our
tested models on DUC 2006, see Table 1.
</bodyText>
<footnote confidence="0.9961152">
5We limit our testing to the initial TAC 2010 data as opposed
to the update portion.
6Available at http://www.nltk.org.
7Note that we present our ROUGE scores scaled by 100 to
aid in readability.
</footnote>
<page confidence="0.995255">
645
</page>
<subsectionHeader confidence="0.995098">
4.2 POS Tagger
</subsectionHeader>
<bodyText confidence="0.999987263157895">
Because the content distributions learned from our
model seem to favor almost exclusively nouns (see
Figure 2), another approach to building a seman-
tically strong word distribution for determining
salient sentences in summarization might be to ig-
nore all words except nouns. This would avoid
most stopwords (many of which are modeled as their
own part-of-speech) and would serve as a simpler
approach to finding important content. Neverthe-
less, adjectives and verbs also often carry impor-
tant semantic information. Therefore, we ran a POS
tagger over the input sentences and tried selecting
sentences based on word distributions that included
only nouns; nouns and verbs; nouns and adjectives;
and nouns, verbs, and adjectives. In each case,
this approach performs either worse than or no bet-
ter than SumBasic using a priori stopword removal.
The nouns and adjectives distribution did the best,
whereas the nouns and verbs were the worst.
</bodyText>
<subsectionHeader confidence="0.999184">
4.3 Content and Syntax Model
</subsectionHeader>
<bodyText confidence="0.999994">
Finally, we test our model. Using the content dis-
tributions found by separating the “content” words
from the “syntax” words in our modified topics and
syntax model, we replaced the unigram probabil-
ity distribution p(w) of each document set with the
learned content distribution for that document set’s
topic, &amp;), where z is the topic for the given docu-
ment set. Following this method, which we call SBH
for “SumBasic with HMM”, our ROUGE scores in-
crease considerably and we obtain 5.9 R-2 and 8.7
R-SU4 without stop-word removal. This is the high-
est performing model we tested. Due to space con-
straints, we omit full TAC 2010 results but R-2 and
R-SU4 results without stopwords improved from
SumBasic’s 7.3 and 8.6 to 8.0 and 9.1, respectively,
both of which were statistically significant increases.
</bodyText>
<sectionHeader confidence="0.998745" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99996176">
This paper has described using a domain-
independent document modeling approach of
avoiding low-content syntax words in an NLP task
where high-content semantic words should be the
principal focus. Specifically, we have shown that
we can increase summarization performance by
modeling the document set probability distribution
using a hybrid LDA-HMM content and syntax
model. We model a document set’s creation by
separating content and syntax words through
observing short-range and long-range word depen-
dencies, and then use that information to build a
word distribution more representative of content
than either a simple stopword-removed unigram
probability distribution, or one made up of words
from a particular subset of the parts-of-speech.
This is a very flexible approach to finding content
words and works well for increasing performance of
simple statistics-based text summarization. It could
also, however, prove to be useful in any other NLP
task where stopwords should be removed. Some
future work includes applying this model to areas
such as topic tracking and text segmentation, and
coherently adjusting it to fit an n-gram modeling
approach.
</bodyText>
<sectionHeader confidence="0.998937" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995233285714286">
William Darling is supported by an NSERC Doc-
toral Postgraduate Scholarship. The authors would
like to acknowledge the financial support provided
from Ontario Centres of Excellence (OCE) through
the OCE/Precarn Alliance Program. We also thank
the anonymous reviewers for their helpful com-
ments.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999250684210526">
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113–120. Best paper award.
Hal Daum´e, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In ACL-44: Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
305–312, Morristown, NJ, USA. Association for Com-
putational Linguistics.
W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. 1999.
Markov Chain Monte Carlo In Practice. Chapman and
Hall/CRC.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In In Advances in Neural Information Pro-
cessing Systems 17, pages 537–544. MIT Press.
</reference>
<page confidence="0.987083">
646
</page>
<reference confidence="0.99797884">
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
NAACL ’09: Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362–370, Morristown, NJ,
USA. Association for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74–
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Rachel Tsz-Wai Lo, Ben He, and Iadh Ounis. 2005. Au-
tomatically building a stopword list for an information
retrieval system. JDIM, pages 3–8.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM J. Res. Dev., 2(2):159–165.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKe-
own. 2006. A compositional context sensitive multi-
document summarizer: exploring the factors that in-
fluence summarization. In SIGIR ’06: Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 573–580, New York, NY, USA. ACM.
</reference>
<page confidence="0.998161">
647
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.729877">
<title confidence="0.9986615">Probabilistic Document Modeling for Syntax Removal in Summarization</title>
<author confidence="0.999892">M William</author>
<affiliation confidence="0.999522">School of Computer University of</affiliation>
<address confidence="0.9493275">50 Stone Rd E, Guelph, N1G 2W1</address>
<email confidence="0.992648">wdarling@uoguelph.ca</email>
<author confidence="0.963573">Fei</author>
<affiliation confidence="0.999516">School of Computer University of</affiliation>
<address confidence="0.943131">50 Stone Rd E, Guelph, N1G 2W1</address>
<email confidence="0.998016">fsong@uoguelph.ca</email>
<abstract confidence="0.998340333333334">Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard apof using an priori list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Proceedings of the Main Conference,</booktitle>
<pages>113--120</pages>
<note>Best paper award.</note>
<contexts>
<context position="8614" citStr="Barzilay and Lee, 2004" startWordPosition="1359" endWordPosition="1362"> content and syntax summarization method. There are D document sets, M documents in each set, NM words in document M, and C syntax classes. proach would be to model the syntax and semantic words used in a document collection in an HMM, as in Griffiths et al. (2005), and use the semantic class as the content-word distribution for summarization. Our approach to summarization builds on SumBasic, and combines it with a similar approach to separating content and syntax distributions as that described in (Griffiths et al., 2005). Like (Haghighi and Vanderwende, 2009), (Daum´e and Marcu, 2006), and (Barzilay and Lee, 2004), we model words as being generated from latent distributions. However, instead of background, content, and document-specific distributions, we model all words in a document set as being there for one of only two purposes: a semantic (content) purpose, or a syntactic (functional) purpose. We model the syntax class distributions using an HMM and model the content words using a simple language model. The principal difference between our generative model and the one described in (Griffiths et al., 2005) is that we simplify the model by assuming that each document is generated solely from one topi</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL 2004: Proceedings of the Main Conference, pages 113–120. Best paper award.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian query-focused summarization. In</title>
<date>2006</date>
<booktitle>ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>305--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e, III and Daniel Marcu. 2006. Bayesian query-focused summarization. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 305–312, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W R Gilks</author>
<author>S Richardson</author>
<author>D J Spiegelhalter</author>
</authors>
<title>Markov Chain Monte Carlo In Practice. Chapman and Hall/CRC.</title>
<date>1999</date>
<contexts>
<context position="11497" citStr="Gilks et al., 1999" startWordPosition="1853" endWordPosition="1856">he DUC 2006 dataset) is shown in Figure 2 with the most probable words in the content class in the middle and two syntax classes on either side of it. 3.1 Inference Because the posterior probability of the content (document set) word distributions and syntax class word distributions cannot be solved analytically, as with many topic modeling approaches, we appeal 71 Y 0 C W C 6 NM M R Z D � in at of on with by ... ... el nino weather pacific ocean normal temperatures said told asked say says 644 to an approximation. Following Griffiths et al. (2005), we use Markov Chain Monte Carlo (see, e.g. (Gilks et al., 1999)), or more specifically, “collapsed” Gibbs sampling where the multinomial parameters are integrated out.4 We ran our sampler for between 500 and 5,000 iterations (though the distributions would typically converge by 1,000 iterations), and chose between 5 and 10 (with negligible changes in results) for the cardinality of the classes set C. We leave optimizing the number of syntax classes, or determining them directly from the data, for future work. 3.2 Summarization Here we describe how we use the estimated topic and syntax distributions to perform extractive multidocument summarization. We fol</context>
</contexts>
<marker>Gilks, Richardson, Spiegelhalter, 1999</marker>
<rawString>W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. 1999. Markov Chain Monte Carlo In Practice. Chapman and Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2915" citStr="Griffiths et al., 2005" startWordPosition="455" endWordPosition="458">s at the DUC and TAC involve taking a set of documents as input and outputting a short summary (either 100 or 250 words, depending on the year) containing what the system deems to be the most important information contained in the original documents. While a system matching human performance will likely require deep language understanding, most existing systems use an extractive, rather than abstractive, approach whereby the most salient sentences are extracted from the original documents and strung together to form an output summary.2 In this paper, we present a summarization model based on (Griffiths et al., 2005) that integrates topics and syntax. We show that a simple model that separates syntax and content words and uses the content distribution as a representative model of the important words in a document set can achieve high performance in multi-document summarization, competitive with state-of-the-art summarization systems. 1http://www.nist.gov/tac 2NLP techniques such as sentence compression are often used, but this is far from abstractive summarization. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 642–647, Portland, Oregon, June 19-</context>
<context position="6896" citStr="Griffiths et al. (2005)" startWordPosition="1084" endWordPosition="1087">ent words and more Sentences are continually marked for inclusion un- on words with strong semantic meaning. One aptil the summary word-limit is reached. Despite its proach that could achieve this would be to build sepsimplicity, SumBasic continues to be one of the top arate stopword lists for specific domains, and there summarization performers in both manual and auto- are approaches to automatically build such lists (Lo matic evaluations (Nenkova et al., 2006). et al., 2005). However, a list-based approach can2.2 Modeling Content and Syntax not take context into account and therefore, among Griffiths et al. (2005) describe a composite gener- other things, will encounter problems with polyative model that combines syntax and semantics. semy and synonymy. Another approach would be to The semantic portion of the model is similar to La- use a part-of-speech (POS) tagger on each sentence tent Dirichlet Allocation and models long-range the- and ignore all non-noun words because high-content matic word dependencies with a set of topics, while words are almost exclusively nouns. One could also short-range (sentence-wide) word dependencies are include verbs, adverbs, adjectives, or any combinamodeled with synta</context>
<context position="8256" citStr="Griffiths et al. (2005)" startWordPosition="1305" endWordPosition="1308">problems associated with using a stopword list. Nevertheless, this approach introduces deeper context-related problems of its own (a noun, for example, is not always a content word). A separate ap3A system based on SumBasic was one of the top performers at the Text Analysis Conference 2010 summarization track. 643 Figure 1: Graphical model depiction of our content and syntax summarization method. There are D document sets, M documents in each set, NM words in document M, and C syntax classes. proach would be to model the syntax and semantic words used in a document collection in an HMM, as in Griffiths et al. (2005), and use the semantic class as the content-word distribution for summarization. Our approach to summarization builds on SumBasic, and combines it with a similar approach to separating content and syntax distributions as that described in (Griffiths et al., 2005). Like (Haghighi and Vanderwende, 2009), (Daum´e and Marcu, 2006), and (Barzilay and Lee, 2004), we model words as being generated from latent distributions. However, instead of background, content, and document-specific distributions, we model all words in a document set as being there for one of only two purposes: a semantic (content</context>
<context position="11432" citStr="Griffiths et al. (2005)" startWordPosition="1841" endWordPosition="1844">istribution is shown in Figure 1. A portion of an example HMM (from the DUC 2006 dataset) is shown in Figure 2 with the most probable words in the content class in the middle and two syntax classes on either side of it. 3.1 Inference Because the posterior probability of the content (document set) word distributions and syntax class word distributions cannot be solved analytically, as with many topic modeling approaches, we appeal 71 Y 0 C W C 6 NM M R Z D � in at of on with by ... ... el nino weather pacific ocean normal temperatures said told asked say says 644 to an approximation. Following Griffiths et al. (2005), we use Markov Chain Monte Carlo (see, e.g. (Gilks et al., 1999)), or more specifically, “collapsed” Gibbs sampling where the multinomial parameters are integrated out.4 We ran our sampler for between 500 and 5,000 iterations (though the distributions would typically converge by 1,000 iterations), and chose between 5 and 10 (with negligible changes in results) for the cardinality of the classes set C. We leave optimizing the number of syntax classes, or determining them directly from the data, for future work. 3.2 Summarization Here we describe how we use the estimated topic and syntax distri</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In In Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8558" citStr="Haghighi and Vanderwende, 2009" startWordPosition="1350" endWordPosition="1353">marization track. 643 Figure 1: Graphical model depiction of our content and syntax summarization method. There are D document sets, M documents in each set, NM words in document M, and C syntax classes. proach would be to model the syntax and semantic words used in a document collection in an HMM, as in Griffiths et al. (2005), and use the semantic class as the content-word distribution for summarization. Our approach to summarization builds on SumBasic, and combines it with a similar approach to separating content and syntax distributions as that described in (Griffiths et al., 2005). Like (Haghighi and Vanderwende, 2009), (Daum´e and Marcu, 2006), and (Barzilay and Lee, 2004), we model words as being generated from latent distributions. However, instead of background, content, and document-specific distributions, we model all words in a document set as being there for one of only two purposes: a semantic (content) purpose, or a syntactic (functional) purpose. We model the syntax class distributions using an HMM and model the content words using a simple language model. The principal difference between our generative model and the one described in (Griffiths et al., 2005) is that we simplify the model by assum</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz MarieFrancine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74– 81,</booktitle>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="14088" citStr="Lin, 2004" startWordPosition="2287" endWordPosition="2288">formation. Method R-1 ROUGE R-SU4 R-1 ROUGE (-s) R-2 R-2 R-SU4 SB- 37.0 5.5 11.0 23.3 3.8 6.2 SumBasic 38.1 6.7 11.9 29.4 5.3 8.1 N 36.8 7.0 12.2 25.5 4.8 7.3 N,V 36.9 6.5 12.0 24.4 4.4 6.9 N,J 37.4 6.8 12.3 26.5 5.0 7.7 N,V,J 37.4 6.8 12.2 25.5 4.9 7.4 SBH 38.9 7.3 12.6 30.7 5.9 8.7 Table 1: ROUGE Results on the DUC 2006 dataset. Results statistically significantly higher than SumBasic (as determined by a pairwise t-test with 99% confidence) are displayed in bold. marization metric for unigram (R-1), bigram (R-2), and skip-4 bigram (R-SU4) recall both with and without (-s) stopwords removed (Lin, 2004). We tested our models on the popular DUC 2006 dataset which aids in model comparison and also on the more recent TAC 2010 dataset. The DUC 2006 dataset consists of 50 sets of 25 news articles each, whereas the TAC 2010 dataset consists of 46 sets of 10 news articles each.5 For DUC 2006, summaries are a maximum of 250 words; for TAC 2010, they can be at most 100. Our approach is compared to using an a priori stopword list, and using a POStagger to build distributions of words coming from only a subset of the parts-of-speech. 4.1 SumBasic To cogently demonstrate the effect of ignoring nonsemant</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz MarieFrancine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74– 81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Tsz-Wai Lo</author>
<author>Ben He</author>
<author>Iadh Ounis</author>
</authors>
<title>Automatically building a stopword list for an information retrieval system.</title>
<date>2005</date>
<pages>3--8</pages>
<publisher>JDIM,</publisher>
<marker>Lo, He, Ounis, 2005</marker>
<rawString>Rachel Tsz-Wai Lo, Ben He, and Iadh Ounis. 2005. Automatically building a stopword list for an information retrieval system. JDIM, pages 3–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM J. Res. Dev.,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="4216" citStr="Luhn, 1958" startWordPosition="654" endWordPosition="655">x classes is replaced with an LDA2.1 SumBasic like topic model. When the model is in the semantic Nenkova et al. (2006) describe SumBasic, a simple, class state, it chooses a topic from the given docuyet high-performing summarization system based on ment’s topic distribution, samples a word from that term frequency. While the methodology underly- topic’s word distribution, and generates it. Othering SumBasic departs very little from the pioneer- wise, the model samples a word from the current ing summarization work performed at IBM in the syntax class in the HMM and outputs that word. 1950’s (Luhn, 1958), methods based on simple word 3 Our Summarization Model statistics continue to outperform more complicated Nenkova et al. (2006) show that using term freapproaches to automatic summarization.3 Nenkova quency is a powerful approach to modeling human et al. (2006) empirically showed that a word that ap- summarization. Nevertheless, for SumBasic to perpears more frequently in the original text will be form well, stop-words must be removed from the more likely to appear in a human generated sum- composition scoring function. Because these words mary. add nothing to the content of a summary, if th</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBM J. Res. Dev., 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
<author>Kathleen McKeown</author>
</authors>
<title>A compositional context sensitive multidocument summarizer: exploring the factors that influence summarization.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>573--580</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3724" citStr="Nenkova et al. (2006)" startWordPosition="574" endWordPosition="577">n a document set can achieve high performance in multi-document summarization, competitive with state-of-the-art summarization systems. 1http://www.nist.gov/tac 2NLP techniques such as sentence compression are often used, but this is far from abstractive summarization. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 642–647, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work one of its syntax classes is replaced with an LDA2.1 SumBasic like topic model. When the model is in the semantic Nenkova et al. (2006) describe SumBasic, a simple, class state, it chooses a topic from the given docuyet high-performing summarization system based on ment’s topic distribution, samples a word from that term frequency. While the methodology underly- topic’s word distribution, and generates it. Othering SumBasic departs very little from the pioneer- wise, the model samples a word from the current ing summarization work performed at IBM in the syntax class in the HMM and outputs that word. 1950’s (Luhn, 1958), methods based on simple word 3 Our Summarization Model statistics continue to outperform more complicated </context>
<context position="6739" citStr="Nenkova et al., 2006" startWordPosition="1059" endWordPosition="1062">ny word that appears in less probability mass for our document set probathe chosen sentence has its probability diminished. bility distribution on non-content words and more Sentences are continually marked for inclusion un- on words with strong semantic meaning. One aptil the summary word-limit is reached. Despite its proach that could achieve this would be to build sepsimplicity, SumBasic continues to be one of the top arate stopword lists for specific domains, and there summarization performers in both manual and auto- are approaches to automatically build such lists (Lo matic evaluations (Nenkova et al., 2006). et al., 2005). However, a list-based approach can2.2 Modeling Content and Syntax not take context into account and therefore, among Griffiths et al. (2005) describe a composite gener- other things, will encounter problems with polyative model that combines syntax and semantics. semy and synonymy. Another approach would be to The semantic portion of the model is similar to La- use a part-of-speech (POS) tagger on each sentence tent Dirichlet Allocation and models long-range the- and ignore all non-noun words because high-content matic word dependencies with a set of topics, while words are al</context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compositional context sensitive multidocument summarizer: exploring the factors that influence summarization. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 573–580, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>