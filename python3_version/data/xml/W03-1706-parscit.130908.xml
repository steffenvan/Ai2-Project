<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001382">
<title confidence="0.983114">
The Effect of Rhythm on Structural Disambiguation in Chinese
</title>
<author confidence="0.99956">
Honglin Sun Dan Jurafsky
</author>
<affiliation confidence="0.997346">
Center for Spoken Language Research
University of Colorado at Boulder
</affiliation>
<email confidence="0.998081">
{honglin.sun, jurafsky}@colorado.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999651071428572">
The length of a constituent (number of
syllables in a word or number of words in a
phrase), or rhythm, plays an important role
in Chinese syntax. This paper systematically
surveys the distribution of rhythm in
constructions in Chinese from the statistical
data acquired from a shallow tree bank.
Based on our survey, we then used the
rhythm feature in a practical shallow parsing
task by using rhythm as a statistical feature
to augment a PCFG model. Our results show
that using the probabilistic rhythm feature
significantly improves the performance of
our shallow parser.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999090147540983">
Syntactic research indicates that prosodic features,
including stress, rhythm, intonation, and others,
have an impact on syntactic structure. For example,
normally in a coordination construction like “A
and B”, A and B are interchangeable, that is to say,
you can say “B and A” and the change of word
order does not change the meaning. However,
sometimes A and B are not interchangeable. Quirk
et al.(1985) gives the following examples:
man and woman * woman and man
ladies and gentleman *gentleman and ladies
Obviously, the examples above cannot be
explained by gender preference. A reasonable
explanation is that the length of the words (perhaps
in syllables) is playing a role; the first constituent
tends to be shorter than the second constituent.
This feature of the length in syllables of a
constituent plays an even more important role in
Chinese syntax than in English (Feng, 2000). For
example, in the verb-object construction in
Chinese, there is a preference for the object to be
equal to or longer than the verb. Thus while both
“种”(plant) and “种植”(plant) are verbs and have
the same meaning, “ 种 /plant 树 /tree” is
grammatical while “ 种 植 /plant 树 /tree” is
ungrammatical. However, both verbs allow bi-
syllabic nouns as objects (e.g., “果树”(fruit tree),
“棉花”(cotton) etc.). The noun phrases formed by
“noun + verb” give us another example in which
rhythm feature places constraints on syntax, as
indicated in the following examples
(ungrammatical with *):
棉花/cotton 种植/planting
*棉花/cotton 种/planting
*花/flower 种植/planting
*花/flower 种/planting
“棉花/cotton 种植/planting” is grammatical but
“棉花/cotton 种/planting” , “花/flower 种植
/planting” and “花/flower 种/planting” are all
ungrammatical, although “棉花/cotton” and “花
/flour” , “种植/planting” and “种/planting” have
the same POS and the same or similar meaning.
The only difference lies in that they have different
number of syllables or different length.
This paper systematically surveys the effect of
rhythm on Chinese syntax from the statistical data
from a shallow tree bank. Based on the observation
that rhythm places constraints on syntax in Chinese,
we try to deploy a feature based on rhythm to
improve disambiguation in a probabilistic parser
by mixing the rhythm feature into a statistical
parsing model.
The rest of the paper is organized as follows: we
present specific statistical analyses of rhythm
feature in Chinese syntax in Section 2. Section 3
introduces the content chunk parsing which is the
task in our experiment. Section 4 presents the
statistical model used in our experiment in which a
probabilistic rhythm feature is integrated. Section 5
gives the experimental results and finally Section 6
draws some conclusions.
</bodyText>
<subsectionHeader confidence="0.643747">
2 Analysis of Rhythmic Constraints
</subsectionHeader>
<bodyText confidence="0.9999644">
We divide our analysis of the use of rhythm in
Chinese phrases into two categories, based on two
types of phrases in Chinese: (1) simple phrases,
containing only words, i.e. all the child nodes are
POS tag in the derivation tree; and (2) complex
phrases in which at least one constituent is a phrase
itself, i.e. it has at least one child node with phrase
type symbol (like NP, VP) in its derivation tree.
Below we will give the statistical analysis of the
distribution of rhythm feature in different
constructions from both simple and complex
phrases. The corpus from which the statistical data
is drawn contains 200K words of newspaper text
from the People’s Daily. The texts are word-
segmented, POS tagged and labeled with content
chunks. The content chunk is a phrase containing
only content words, akin to a generalization of a
BaseNP. These content chunks are parsed into
binary shallow trees. More details about content
chunks can be found in Section 3.
</bodyText>
<subsectionHeader confidence="0.9995">
2.1 Rhythm feature in simple phrases
</subsectionHeader>
<bodyText confidence="0.9996011">
Simple phrases contain two lexical words (since, as
discussed above, our parse trees are binary). The
rhythm feature of each word is defined to be the
number of syllables in it. Thus the rhythm feature
for a word can take on one of the following three
values: (1) monosyllabic; (2) bi-syllabic; and (3)
multi-syllabic, meaning with three syllables or
more.
Since each binary phrase contains two words,
the set of rhythm features for a simple phrase is:
</bodyText>
<equation confidence="0.9336065">
F = { (0,0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0),
(2,1), (2,2) }
</equation>
<bodyText confidence="0.999916125">
where 0, 1, 2 represent monosyllabic, bi-syllabic
and multi-syllabic respectively.
In the following sections, we will present three
case studies on the distributions of rhythm feature
in different constructions: (1) verbs as modifier or
head in NP; (2) the contrast between NPs and VPs
formed by “ verb + noun” sequences; (3) “noun +
verb” sequences.
</bodyText>
<subsectionHeader confidence="0.988262">
2.1.1 Case 1: Verb as modifier/head in NP
</subsectionHeader>
<bodyText confidence="0.992900272727273">
In Chinese, verbs can function as modifier or head
in a noun phrase without any change of forms. For
example, in “果树/fruit tree 栽培/growing”, “栽
培 ” is the head while in “ 栽 培 /growing 技术
/technique”, “栽培” is a modifier. However, in
such constructions, there are strong constraints on
the length of both verbs and nouns. Table 1 gives
the distributions of the rhythm feature in the rule
“NP -&gt; N V”(‘N’ and ‘V’ represent noun and verb
respectively) in which the verb is the head and
“NP -&gt; V N” in which the verb is a modifier.
</bodyText>
<tableCaption confidence="0.978957">
Table 1 Distribution of rhythm feature in NP with verb as modifier or head
</tableCaption>
<table confidence="0.997925666666667">
[0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] Total
NP -&gt; V N 0 4 0 0 1275 4 0 88 0 1371
NP -&gt; N V 13 10 0 401 2328 91 0 44 2 2889
</table>
<tableCaption confidence="0.867479">
Table 2 Distribution of rhythm feature in NP and VP formed by “V N”
</tableCaption>
<table confidence="0.998446">
[0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] Total
VP -&gt; V N 826 640 49 80 1221 121 0 11 1 2777
NP -&gt; V N 13 10 0 401 2328 91 0 44 2 2889
</table>
<tableCaption confidence="0.946427">
Table 3 Distribution of rhythm feature in phrases formed by “N V” sequence
</tableCaption>
<table confidence="0.94856375">
[0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] Total
NP -&gt; N V 0 4 0 0 1275 4 0 88 0 1371
NC -&gt; N V 384 578 42 1131 3718 143 90 435 15 6536
S -&gt; N V 28 1 2 17 347 22 2 43 8 470
</table>
<tableCaption confidence="0.838149">
Table 1 indicates that in both rules, the rhythm
</tableCaption>
<bodyText confidence="0.997470125">
pattern [1,1], ie. “bi-syllabic + bi-syllabic”,
prevails. In the rule “NP -&gt; V N”, this pattern
accounts for 93% among the nine possible patterns
while in the rule “NP -&gt; N V”, this pattern
accounts for 81%. We can also find that in both
cases, [0,2] and [2,0] are prohibited, that is to say,
both verbs and nouns cannot be longer than two
syllables.
</bodyText>
<subsectionHeader confidence="0.8864075">
2.1.2 Case 2: Contrast between NP and VP
formed by “V N” sequence
</subsectionHeader>
<bodyText confidence="0.999465666666667">
The sequence “V N”(“verb + noun”) can constitute
an NP or a VP. The rhythm patterns in the two
types of phrases are significantly different,
however, as shown in Table 2. We see that in the
NP case, verbs are mainly bi-syllabic. The total
number of examples with bi-syllabic verbs in NP is
2820, accounting for 98% of all the cases. On the
other hand, mono-syllabic verbs are less likely to
appear in this position. The total number of
examples with mono-syllabic verbs in NP is 23,
accounting for only 0.8% of all the cases. That is to
say, the likelihood of bi-syllabic verbs appearing in
this syntactic position is 122 times the likelihood
of mono-syllabic verbs. On the other hand, there is
no big difference between bi-syllabic verbs and
mono-syllabic verbs in the VP formed by “V + N”.
The ratios of bi-syllabic and mono-syllabic verbs
in VP are 48 % and 55% respectively. The
statistical facts tell us that for a “verb + noun”
sequence, if the verb is not bi-syllabic then it is
very unlikely to be an NP. Figure 1 depicts more
clearly the difference between NP and VP formed
by “V N” sequence in the distribution of rhythm
feature.
</bodyText>
<figure confidence="0.99884">
2500 VP-&gt; v n
2000 NP-&gt; v n
1500
1000
500
0
0,0 0,2 1,1 2,0 2,2
</figure>
<figureCaption confidence="0.986162">
Figure 1 Distributions of rhythm feature in NP
and VP formed by “verb + noun”
</figureCaption>
<bodyText confidence="0.924172">
.
</bodyText>
<subsectionHeader confidence="0.86258">
2.1 3 Case 3: “N V” sequence
</subsectionHeader>
<bodyText confidence="0.94801">
An “N V”(“noun + verb”) sequence can be mainly
divided into three types by the dominating phrasal
category:
</bodyText>
<listItem confidence="0.99301225">
(1) NP(noun phrase), e.g. “果树/fruit tree Ria
/growth”;
(2) S(subject-verb construction), e.g. “ 彩 旗
/colored flag 飘扬/flutter“;
</listItem>
<bodyText confidence="0.9989849">
(3)NC(non-constituent), eg. “经济/economy 发展
/develop” in “中国/China 的/DE 经济/economy
发展/develop 得/DE 很/very 快/fast”. (‘China’s
economy develops very fast’)
Table 3 gives the distribution of rhythm feature in
the three types of cases.
We see in Table 3, in rule “NP -&gt; N V”, that the
verb cannot be mono-syllabic since the first row is
0 in all the patterns in which verb is mono-
syllabic([0,0], [1,0],[2,0]). The “bi-syllabic + bi-
syllabic” ([1,1]) pattern accounts for 93%
(1275/1371) of the total number. Let’s look at the
cases with mono-syllabic verbs in all the three
types. The total number of such examples is 1652
in the corpus (adding all the numbers in columns
[0,0], [1,0] and [2,0] on the three rows). Among
these 1652 cases, there is not one example in
which the “N V” is an NP. The sequence has a
probability of 3%(47/1652) to be an S and 97
%(1605/1652) of being an NC(non-constituent).
</bodyText>
<subsectionHeader confidence="0.999573">
2.2 Rhythm feature in complex phrases
</subsectionHeader>
<bodyText confidence="0.9940387">
Just as we saw with two word simple phrases, the
rhythm feature also has an effect on complex
phrases where at least one component is a phrase,
i.e. spanning over two words or more. For example,
for the following fragment of a sentence:
跨/stride 进/into 三峡/the Three Gorges
工程/project 大门/gate
‘enter into the gate of the Three Gorges Project’
according to PCFG, the parse as indicated in
Figure 2 (a) is incorrectly assigned the greatest
</bodyText>
<equation confidence="0.617017">
VP
跨 进 三峡 工程 大门
(a)
VP
跨 进 三峡 工程 大门
(b)
</equation>
<figureCaption confidence="0.894783">
Figure 2 (a) incorrect parse and
(b) correct parse
</figureCaption>
<bodyText confidence="0.897435433333333">
probability but the correct parse is that given in
Figure 2 (b). One major error in (a) is that it
applies the rule “NP-&gt; VP N” (i.e. “进 三峡
程 ” modifying “ 大 门 ”). This rule has 216
occurrences in the corpus, of which 168 times it
contains a VP of 2 words, 30 times a VP of 3
words and 18 times a VP of more than 3 words.
These statistics indicate that this rule prefers to
choose a short VP acting as the modifier of a noun,
as in “NP(VP( 种 /grow 粮 /grain) 大 户 /large
family)” and “NP(VP(学/learn 雷锋/Lei Feng) 标
兵/model)”. But in the example in Figure 2(a), the
VP contains 3 words, so it is less likely to be a
modifier in an NP.
When a phrase works as a constituent in a larger
phrase, its rhythm feature is defined as the number
of words in it. Thus a phrase may take on one of
the three values for the rhythm feature: (1) two
words; (3) three words; and (3) more than three
words. Similar to that in the simple phrases, we
may use 0, 1, 2 to represent the three values
respectively. Therefore, for every construction
containing two constituents, its rhythm feature can
be described by a 3×3 matrix uniformly. For
example, in the examples for rule “NP -&gt; VP N”
above, the feature value for “NP(VP(种/grow 粮
/grain) 大户/large family)” is [0, 1] in which 0
indicates the VP contains 2 words and 1 represents
that the noun is bi-syllabic. The rule helps to
interpret the meaning of the feature value, i.e. the
</bodyText>
<figure confidence="0.6761408">
NP
VP
NP
NP
VP NP
</figure>
<bodyText confidence="0.999179333333333">
value is for a word or a phrase. For example, for
rule “VP -&gt; V N”, feature value [0, 1] means that
the verb is mono-syllabic and the noun is bi-
syllabic, while for rule “NP-&gt; VP N”, feature [0,1]
means that the VP contains two words and the
noun is bi-syllabic.
</bodyText>
<sectionHeader confidence="0.992705" genericHeader="method">
3 Content Chunk Parsing
</sectionHeader>
<bodyText confidence="0.9998203">
We have chosen the task of content chunk parsing
to test the usefulness of our rhythm feature to
Chinese text. In this section we address two
questions: (1) What is a content chunk? (2) Why
are we interested in content chunk parsing?
A content chunk is a phrase formed by a sequence
of content words, including nouns, verbs,
adjectives and content adverbs. There are three
kinds of cases for the mapping between content
word sequences and content chunks:
</bodyText>
<listItem confidence="0.978400909090909">
(1) A content word sequence is a content chunk. A
special case of this is that a whole sentence is a
content chunk when all the words in it are content
words, eg. [[HUU景/Prospect 公司/company]NP [推
ffl/release [n级/advanced [电脑/computer [排版
/typesetting 系 统 /system]NP]NP]NP]VP
(‘Prospect Company released an advanced
computer typesetting system.’).
(2) A content word sequence is not a content
chunk. For example, in “中Q/China M/AUX 经济
/economy 发 展 /develop 得 /AUX TR /very 快
/fast”(‘China’s economy develops very fast.’), “经
济 /economy 发 展 /develop” is a content word
sequence, but it’s not a phrase in the sentence.
(3) A part of a content word sequence is a content
chunk. For example, in “ fA 营 /private 经 济
/economy 发展/develop M/AUX 势头/trend TR
/very 好 /good”(‘The developmental trend of
private economy is very good.’), “fA营/private 经
济 /economy 发 展 /develop” is a content word
sequence, but it’s not a phrase; only “fA营/private
经济/economy” in it is a phrase.
</listItem>
<bodyText confidence="0.999747065217391">
The purpose of content chunk parsing is to
recognize phrases in a sequence of content words.
Specifically speaking, the content chunking
contains two subtasks: (1) to recognize the
maximum phrase in a sequence of content words;
(2) to analyze the hierarchical structure within the
phrase down to words. Like baseNP
chunking(Church, 1988; Ramshaw &amp; Marcus
1995), content chunk parsing is also a kind of
shallow parsing. Content chunk parsing is deeper
than baseNP chunking in two aspects: (1) a content
chunk may contain verb phrases and other phrases
even a full sentence as long as the all the
components are content words; (2) it may contain
recursive NPs. Thus the content chunk can supply
more structural information than a baseNP.
The motives for content chunk parsing are two-
fold: (1) Like other shallow parsing tasks, it can
simplify the parsing task. This can be explained in
two aspects. First, it can avoid the ambiguities
brought up by functional words. In Chinese, the
most salient syntactic ambiguities are prepositional
phrases and the “DE” construction. For
prepositional phrases, the difficulty lies in how to
determine the right boundary, because almost any
constituent can be the object of a preposition. For
“DE” constructions, the problem is how to
determine its left boundary, since almost any
constituent can be followed by “DE” to form a
“DE” construction. Second, content chunk parsing
can simplify the structure of a sentence. When a
content chunk is acquired, it can be replaced by its
head word, thus reducing the length of the original
sentence. If we get a parse from the reduced
sentence with a full parser, then we can get a parse
for the original sentence by replacing the head-
word nodes with the content chunks from which
the head-words are extracted. (2) The content
chunk parsing may be useful for applications like
information extraction and question answering.
When using template matching, a content chunk
Under this PCFG+PF model, the goal of a parser
is to choose a parse that maximizes the following
score:
may be just the correct level of shallow structure
for matching with an element in a template.
</bodyText>
<equation confidence="0.9802405">
4 PCFG + PF Model
n(6)
</equation>
<bodyText confidence="0.999972166666667">
In the experiment we propose a statistical model
integrating probabilistic context-free grammar
(PCFG) model with a simple probabilistic features
(PF) model. In this section we first give the
definition for the statistical model and then we will
give the method for parameter estimation.
</bodyText>
<subsectionHeader confidence="0.985074">
4.1 Definition
</subsectionHeader>
<bodyText confidence="0.994219">
According to PCFG, each rule r used to expand a
node n in a parse is assigned a probability, i.e.:
</bodyText>
<equation confidence="0.900954">
P(r(n))=P(β |A) (1)
</equation>
<bodyText confidence="0.993127">
where A -&gt; β is a CFG rule. The probability of a
parse T is the product of each rule used to expand
each node n in T:
</bodyText>
<equation confidence="0.997107">
P(T  |S) = ∏ P(r(n)) (2)
∈T
</equation>
<bodyText confidence="0.9999562">
We expand PCFG by the way that when a left
hand side category A is expanded into a string β, a
feature set FS related to β is also generated. Thus, a
probability is assigned for expansion of each node
n when a rule r is applied:
</bodyText>
<equation confidence="0.999437">
P(r(n)) = P(FS, β  |A) (3)
</equation>
<bodyText confidence="0.9982375">
where A -&gt; β is a CFG rule and FS is a feature set
related to β. From Equation (3) we get:
</bodyText>
<equation confidence="0.9997395">
P r n = P FS β A P β A
( ( )) (  |, ) * (  |) (4)
</equation>
<bodyText confidence="0.99965525">
where P(FS |β, A) is probabilistic feature(PF)
model and P(β  |A) is PCFG model. PF model
describes the probability of each feature in feature
set FS taking on specific values when a CFG rule
A -&gt; β is given. To make the model more practical
in parameter estimation, we assume the features in
feature set FS are independent from each other,
thus:
</bodyText>
<equation confidence="0.974250428571429">
P(FS  |, )
β A = ∏ P(Fi  |β, A) (5)
Fi FS
∈
Score T S
(  |) arg max
= PT fl (FS i Ai)
</equation>
<bodyText confidence="0.999757823529412">
Our model is thus a simplification of more
sophisticated models which integrate PCFGs with
features, such as those in Magerman(1995),
Collins(1997) and Goodman(1997). Compared
with these models, our model is more practical
when only small training data is available, since
we assume the independence between features. For
example, in Goodman’s probabilistic feature
grammar (PFG), each symbol in a PCFG is
replaced by a set of features, so it can describe
specific constraints on the rule. In the PFG model
the generation of each feature is dependent on all
the previously generated features, thus likely
leading to severe sparse data problem in parameter
estimation. Our simplified model assumes
independence between the features, thus data
sparseness problem can be significantly alleviated.
</bodyText>
<subsectionHeader confidence="0.995209">
4.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.99995325">
Let F be a feature associated with a string β, where
the possible values for F are f1,f2,..,fn, E is the set
of observations of rule A → β in the training
corpus, and thus E can be divided into n disjoint
subsets: E1,E2,..,En, corresponding to f1,f2,..,fn
respectively. The probability of F taking on a value
of fi given A → β can be estimated as follows,
according to MLE:
</bodyText>
<equation confidence="0.9970105">
E
P(F = fi  |β , A , 7) = E ( )
</equation>
<bodyText confidence="0.999716">
This indicates that feature F adds constraints on
CFG rule A → β by dividing Ω, the state space of
A → β, into n disjoint subspaces ω1, ω2,.., ωn, and
each case of F taking a value of fi given A → β is
viewed as a random event.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.956132">
n
</bodyText>
<subsectionHeader confidence="0.989402">
5.1 Training and Test Data
</subsectionHeader>
<bodyText confidence="0.998942666666667">
A Chinese corpus of 200K words extracted from
the People’s Daily are segmented, POS-tagged and
hand-labeled with content chunks in which all the
trees are binary. The corpus is divided into two
parts: (1) 180K for training set and (2) 20K for test
set.
</bodyText>
<subsectionHeader confidence="0.999068">
5.2 Metrics and results
</subsectionHeader>
<bodyText confidence="0.9998329">
We take two kinds of criteria to measure the
system’s performance: labeled and unlabeled.
According to the labeled criterion, a recognized
phrase is correct only if a phrase with the same
starting position, ending position and the same
label is found in the gold standard. According to
the unlabeled criterion, a recognized phrase is
correct as long as a phrase with the same starting
position and ending position is found in the gold
standard.
</bodyText>
<tableCaption confidence="0.98911">
Table 4 Experimental Results
</tableCaption>
<table confidence="0.99936">
Labeled Unlabeled
P R F P R F
PCFG 49.91 64.96 56.45 53.33 80.73 65.66
PCFG+RF in simple phrases 53.25 68.46 59.90 57.46 81.21 67.30
PCFG +RF in all the phrases 56.47 72.08 63.33 60.07 83.57 69.90
</table>
<tableCaption confidence="0.986431">
Table 5 Effect of rhythm feature on structural disambiguation
</tableCaption>
<table confidence="0.996363714285714">
Word sequence Rule P(β|A) RF P(RF=[0,1] P(RF=(0,1),β)|A)
|A,β)
国 捐躯 NC4 N V 0.120273 [0,1] 0.08843 0.010636
country sacrifice
国 捐躯 S 4 N V 0.161679 [0,1] 0.00292 0.000344
国 捐躯 NP4 N V 0.063159 [0,1] 0.00213 0.000184
国 捐躯 V 4 N V 0.011573 [0,1] 0.0 0.0
</table>
<bodyText confidence="0.97539104">
Within each criterion, precision, recall and F-
measure are given as metrics for the system’s
performance. Precision represents how many
phrases are correct among the phrases recognized,
recall represents how many phrases in the gold
standard are correctly recognized, and F-measure
is defined as follows:
Pr ecision call
x Re x 2
Precision call
+ Re
Table 4 gives the experimental results in three
different conditions: the first row gives the result
of PCFG model; the second row gives the result of
PCFG model integrated with rhythm feature model
(RF) where only the features of simple phrases are
considered; the last row gives the result of PCFG
model plus RF where the rhythm features in all the
phrases are considered. The results indicate that
the rhythm features in both simple and complex
phrases contribute to the improvement of
performance over PCFG model. We see that the
rhythm feature improves the labeled F-measure
6.88 percent and the unlabeled F-measure 4.24
percent over the unaugmented PCFG model.
</bodyText>
<subsectionHeader confidence="0.9997">
5.3 Effect of rhythm feature on parsing
</subsectionHeader>
<bodyText confidence="0.980256135135135">
The experiment shows that the rhythm feature can
help the performance of a parser in Chinese.
Specifically, the effects of rhythm feature on
parsing are shown in two ways:
(1) Help for the disambiguation of phrasal type.
Table 5 shows the difference of the results
between PCFG model and PCFG + RF model for
the sequence “国/country 捐躯/sacrifice” in the
sentence “ 该 /the 校 /school 有 /have 900 学 子
F − measure=
/students 为/for 国/country 捐躯/sacrifice“ (`900
students from this school gave their lives for their
country’).
In the sentence above, “国/country” is the object
of preposition “ 为 /for”, “ 国 /country 捐 躯
/sacrifice” is not a constituent. But the
unaugmented PCFG model incorrectly parses it as
a S(subject-predicate construction). Contrarily,
according to PCFG+RF model, the type with
greatest probability is the (correct) NC(non-
constituent) parse.
(2) Help for pruning.
Let’s give an example to explain it. For the
sentence “ 解 决 /solve 居 民 /resident 吃 /eat 菜
/vegetable 问 题/problem 十 分 /very 困 难
/difficult”(‘It’s very difficult to solve the vegetable
problem for the residents.’), the number of edges
generated by the PCFG is 1236, but the number
decreases to 348 after the rhythm feature is applied,
thus pruning 73% of the edges. As indicated in
Table 1, in the rule “NP -&gt; N V”, P(RF = [1,0] ) =
0, so “[居民/N 吃/V]NP” is pruned after adding
RF. Similarly, in rule “NP -&gt; V N”, P(RF = [0, 1] )
= 0.003, so “[吃/V 菜/N]NP” is pruned since it has
very low probability. With these two edges pruned,
more potential edges containing them will not be
generated.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999973705882353">
In this paper, we systematically survey the
distribution of rhythm (number of syllables per
word or numbers of words per phrase for a
constituent) in different constructions in Chinese.
Our analysis suggests that rhythm places strong
constraints on Chinese syntax. Based on this
observation, we used the rhythm feature in a
practical shallow parsing task in which a PCFG
model is augmented with a probabilistic
representation of the rhythm feature. The
experimental results show that the probabilistic
rhythm feature aids in disambiguation in Chinese
and thus helps to improve the performance of a
Chinese parser. We can expect that the
performance of the parser may further improve
when more features are considered under the
probabilistic feature (PF) model.
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99832">
This research was partially supported by the NSF,
via a KDD extension to NSF IIS-9978025.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996946166666667">
Church,,K.,1988.A stochastic parts program and
noun phrase parser for unrestricted text. In
Proceedings of the Second Conference on
Applied Natural Language Processing, pp.136-
143.
Collins, M. 1997. Three generative lexicalized
models for statistical parsing, in Proceedings of
the 35th Annual Meeting of the ACL, pp. 16-23.
Feng, Shengli. 2000. The Rhythmic syntax of
Chinese(in Chinese), Shanghai Education Press.
Goodman, J. 1997. Probabilistic Feature
Grammars, In Proceedings of the International
Workshop on Parsing Technologies, September
1997
Magerman, D. 1995. Statistical decision-tree
models for parsing, in Proceedings of the 33rd
Annual Meeting of the Association for
Computational Linguistics, pp.276-283.
Quirk et al. 1985. A Comprehensive Grammar of
English Languge, Longman.
Ramshaw L., and Marcus M. 1995. Text chunking
using transformation-based learning. In Proc-
eedings of the Third Workshop on Very Large
Corpora.pp.86-95.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989805">
<title confidence="0.999911">The Effect of Rhythm on Structural Disambiguation in Chinese</title>
<author confidence="0.999343">Honglin Sun Dan Jurafsky</author>
<affiliation confidence="0.998026">Center for Spoken Language University of Colorado at Boulder</affiliation>
<email confidence="0.999631">honglin.sun@colorado.edu</email>
<email confidence="0.999631">jurafsky@colorado.edu</email>
<abstract confidence="0.999647466666667">The length of a constituent (number of syllables in a word or number of words in a or an important role in Chinese syntax. This paper systematically surveys the distribution of rhythm in constructions in Chinese from the statistical data acquired from a shallow tree bank. Based on our survey, we then used the rhythm feature in a practical shallow parsing task by using rhythm as a statistical feature to augment a PCFG model. Our results show that using the probabilistic rhythm feature significantly improves the performance of our shallow parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Church,,K.,1988.A stochastic parts program and noun phrase parser for unrestricted text.</title>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<marker></marker>
<rawString>Church,,K.,1988.A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pp.136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative lexicalized models for statistical parsing,</title>
<date>1997</date>
<booktitle>in Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>16--23</pages>
<marker>Collins, 1997</marker>
<rawString>Collins, M. 1997. Three generative lexicalized models for statistical parsing, in Proceedings of the 35th Annual Meeting of the ACL, pp. 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shengli Feng</author>
</authors>
<title>The Rhythmic syntax of Chinese(in Chinese),</title>
<date>2000</date>
<publisher>Education Press.</publisher>
<location>Shanghai</location>
<contexts>
<context position="1680" citStr="Feng, 2000" startWordPosition="265" endWordPosition="266">and the change of word order does not change the meaning. However, sometimes A and B are not interchangeable. Quirk et al.(1985) gives the following examples: man and woman * woman and man ladies and gentleman *gentleman and ladies Obviously, the examples above cannot be explained by gender preference. A reasonable explanation is that the length of the words (perhaps in syllables) is playing a role; the first constituent tends to be shorter than the second constituent. This feature of the length in syllables of a constituent plays an even more important role in Chinese syntax than in English (Feng, 2000). For example, in the verb-object construction in Chinese, there is a preference for the object to be equal to or longer than the verb. Thus while both “种”(plant) and “种植”(plant) are verbs and have the same meaning, “ 种 /plant 树 /tree” is grammatical while “ 种 植 /plant 树 /tree” is ungrammatical. However, both verbs allow bisyllabic nouns as objects (e.g., “果树”(fruit tree), “棉花”(cotton) etc.). The noun phrases formed by “noun + verb” give us another example in which rhythm feature places constraints on syntax, as indicated in the following examples (ungrammatical with *): 棉花/cotton 种植/planting </context>
</contexts>
<marker>Feng, 2000</marker>
<rawString>Feng, Shengli. 2000. The Rhythmic syntax of Chinese(in Chinese), Shanghai Education Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Probabilistic Feature Grammars,</title>
<date>1997</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<marker>Goodman, 1997</marker>
<rawString>Goodman, J. 1997. Probabilistic Feature Grammars, In Proceedings of the International Workshop on Parsing Technologies, September 1997</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing,</title>
<date>1995</date>
<booktitle>in Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<marker>Magerman, 1995</marker>
<rawString>Magerman, D. 1995. Statistical decision-tree models for parsing, in Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pp.276-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quirk</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of English Languge, Longman.</journal>
<marker>Quirk, 1985</marker>
<rawString>Quirk et al. 1985. A Comprehensive Grammar of English Languge, Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora.pp.86-95.</booktitle>
<contexts>
<context position="13620" citStr="Ramshaw &amp; Marcus 1995" startWordPosition="2398" endWordPosition="2401">ate 经 济 /economy 发展/develop M/AUX 势头/trend TR /very 好 /good”(‘The developmental trend of private economy is very good.’), “fA营/private 经 济 /economy 发 展 /develop” is a content word sequence, but it’s not a phrase; only “fA营/private 经济/economy” in it is a phrase. The purpose of content chunk parsing is to recognize phrases in a sequence of content words. Specifically speaking, the content chunking contains two subtasks: (1) to recognize the maximum phrase in a sequence of content words; (2) to analyze the hierarchical structure within the phrase down to words. Like baseNP chunking(Church, 1988; Ramshaw &amp; Marcus 1995), content chunk parsing is also a kind of shallow parsing. Content chunk parsing is deeper than baseNP chunking in two aspects: (1) a content chunk may contain verb phrases and other phrases even a full sentence as long as the all the components are content words; (2) it may contain recursive NPs. Thus the content chunk can supply more structural information than a baseNP. The motives for content chunk parsing are twofold: (1) Like other shallow parsing tasks, it can simplify the parsing task. This can be explained in two aspects. First, it can avoid the ambiguities brought up by functional wo</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Ramshaw L., and Marcus M. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora.pp.86-95.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>