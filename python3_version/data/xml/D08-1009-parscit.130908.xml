<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.805">
Scaling Textual Inference to the Web
</title>
<author confidence="0.973776">
Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld
</author>
<affiliation confidence="0.977229333333333">
Turing Center
University of Washington
Computer Science and Engineering
</affiliation>
<address confidence="0.984819">
Box 352350
Seattle, WA 98195, USA
</address>
<email confidence="0.99907">
stef,etzioni,weld@cs.washington.edu
</email>
<sectionHeader confidence="0.994614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994852619047619">
Most Web-based Q/A systems work by find-
ing pages that contain an explicit answer to
a question. These systems are helpless if the
answer has to be inferred from multiple sen-
tences, possibly on different pages. To solve
this problem, we introduce the HOLMES sys-
tem, which utilizes textual inference (TI) over
tuples extracted from text.
Whereas previous work on TI (e.g., the lit-
erature on textual entailment) has been ap-
plied to paragraph-sized texts, HOLMES uti-
lizes knowledge-based model construction to
scale TI to a corpus of 117 million Web pages.
Given only a few minutes, HOLMES doubles
recall for example queries in three disparate
domains (geography, business, and nutrition).
Importantly, HOLMES’s runtime is linear in
the size of its input corpus due to a surprising
property of many textual relations in the Web
corpus—they are “approximately” functional
in a well-defined sense.
</bodyText>
<sectionHeader confidence="0.989409" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999886923076923">
Numerous researchers have identified the Web as
a rich source of answers to factual questions, e.g.,
(Kwok et al., 2001; Brill et al., 2002), but often the
desired information is not stated explicitly even in a
textual corpus as massive as the Web. Consider the
question “What vegetables help prevent osteoporo-
sis?” Since there is likely no sentence on the Web
directly stating “Kale prevents osteoporosis”, a sys-
tem must infer that kale is an answer by combining
facts from multiple sentences, possibly from differ-
ent pages, which justify that conclusion: i.e., that
kale is a vegetable, kale contains calcium, and cal-
cium helps prevent osteoporosis.
</bodyText>
<page confidence="0.994235">
79
</page>
<figureCaption confidence="0.999924">
Figure 1: The architecture of HOLMES.
</figureCaption>
<bodyText confidence="0.999798533333333">
Textual Inference (TI) methods have advanced in
recent years. For example, textual entailment tech-
niques aim to determine whether one textual frag-
ment (the hypothesis) follows from another (the text)
(Dagan et al., 2005). While most TI researchers have
focused on high-quality inferences from a small
source text, we seek to utilize sizable chunks of the
Web corpus as our source text. In order to do this,
we must confront two major challenges. The first is
uncertainty: TI is an imperfect process, particularly
when applied to the Web corpus, hence probabilistic
methods help to assess the confidence in inferences.
The second challenge is scalability: how does infer-
ence time scale given increasingly large corpora as
input?
</bodyText>
<subsectionHeader confidence="0.989523">
1.1 HOLMES: A Scalable TI System
</subsectionHeader>
<bodyText confidence="0.999946727272727">
This paper describes HOLMES, an implemented sys-
tem, which addresses both challenges by carrying
out scalable, probabilistic inference over ground
assertions extracted from the Web. The input to
HOLMES is a conjunctive query, a set of inference
rules expressed as Horn clauses, and large sets of
ground assertions extracted from the Web, WordNet,
and other knowledge bases. As shown in Figure 1,
HOLMES chains backward from the query, using the
inference rules to construct a forest of proof trees
from the ground assertions. This forest is converted
</bodyText>
<note confidence="0.823284">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 79–88,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.99296975">
into a Markov network (a form of Knowledge-
Based Model Construction (KBMC) (Wellman et
al., 1992)) and evaluated using approximate prob-
abilistic inference. HOLMES operates in an anytime
fashion — if desired it can keep iterating: search-
ing for more proofs, and elaborating the Markov net-
work.
HOLMES makes some important simplifying as-
sumptions. Specifically, we use simple ground
tuples to represent extracted assertions (e.g.,
contains(kale, calcium)). Syntactic prob-
lems (e.g., anaphora, relative clauses) and seman-
tic challenges (e.g., quantification, counterfactuals,
temporal qualification) are delegated to the extrac-
tion system or simply ignored. This paper focuses
on scalability for this subset of the TI task.
</bodyText>
<subsectionHeader confidence="0.986069">
1.2 Summary of Experimental Results
</subsectionHeader>
<bodyText confidence="0.999983105263158">
We tested HOLMES on 183 million distinct ground
assertions extracted from the Web by the TEX-
TRUNNER system (Banko et al., 2007), coupled
with 159 thousand ground assertions from Word-
Net (Miller et al., 1990), and a compact set of hand-
coded inference rules. Given a total of 55 to 145
seconds, HOLMES was able to produce high-quality
inferences that doubled the number of answers to
example queries in three disparate domains: geog-
raphy, business, and nutrition.
We also evaluated how the speed of HOLMES
scaled with the size of its input corpus. In the
general case, logical inference over a Horn theory
(needed in order to produce the probabilistic net-
work) is polynomial in the number of ground asser-
tions, and hence in the size of the textual corpus.1
Unfortunately, this is prohibitive, since even low-
order polynomial growth is fatal on a 117 million-
page corpus, let alone the full Web.
</bodyText>
<subsectionHeader confidence="0.965917">
1.3 Why HOLMES Scales Linearly
</subsectionHeader>
<bodyText confidence="0.999968875">
Fortunately, the Web’s long tail works in our favor.
The relations we extract from text are approximately
pseudo-functional (APF), as we formalize in Sec-
tion 3, and this property leads to runtime that scales
linearly with the corpus. To see the underlying in-
tuition, consider the APF relation denoted by the
phrase “is married to;” most of the time it maps a
person’s name to a small number of spousal names
</bodyText>
<footnote confidence="0.8655185">
1In fact, it is P-complete — as hard as any polynomial-time
problem.
</footnote>
<bodyText confidence="0.942112">
so this relation is APF. Section 3 shows why this
APF property ensures linear scaling, and Section 4
demonstrates linear scaling in practice.
</bodyText>
<sectionHeader confidence="0.764706" genericHeader="method">
2 An Overview of HOLMES
</sectionHeader>
<bodyText confidence="0.9996286">
HOLMES is a system designed to answer complex
queries over large, noisy knowledge bases. As a mo-
tivating example, we consider the question “What
vegetables help prevent osteoporosis?” As of this
writing, Google has no pages explicitly stating ‘kale
helps prevent osteoporosis’, making it challenging
to return “kale” as an answer. However, there are
numerous web pages stating that “kale is high in cal-
cium” and others declaring that “calcium helps pre-
vent osteoporosis”. If we could combine those facts
we could easily infer that “kale” is an answer to the
question “What vegetables help prevent osteoporo-
sis?” HOLMES was designed to make such infer-
ences while accounting for uncertainty in the pro-
cess.
Given a query, expressed as a conjunctive
Datalog rule, HOLMES generates a probabilistic
model using knowledge-based model construction
(KBMC) (Wellman et al., 1992). Specifically,
HOLMES utilizes fast, logical inference to find the
subset of ground assertions and inference rules that
may influence the answers to the query — enabling
the construction of a small and focused Markov net-
work. Since this graphical model is much smaller
than one incorporating all ground assertions, prob-
abilistic inference will be much faster than if naive
compilation were used.
Figure 1 summarizes the operation of HOLMES.
As with many theorem provers or KBMC systems,
HOLMES takes three inputs:
</bodyText>
<listItem confidence="0.9500865">
1. A set of knowledge bases — databases of
ground relational assertions, each with an
estimate of its probability, which can be
generated by TextRunner (Banko et al.,
2007) or Kylin (Wu and Weld, 2007). In
our example, we would extract the as-
sertions IsHighIn(kale, calcium) and
Prevents(calcium, osteoporosis) from
those sentences.
2. A domain theory – A set of probabilis-
tic inference rules written as Markov logic
Horn clauses, which can be used to de-
</listItem>
<bodyText confidence="0.9326815">
rive new assertions. The weight associ-
ated with each clause specifies its reliability.
</bodyText>
<page confidence="0.973032">
80
</page>
<figure confidence="0.99748406">
kale matches the query
(Inferred : 0.91)
broccoli matches the query
(Inferred : 0.58)
Query Result Query Result
kale
IS-A
vegetable
(WordNet : 0.9)
kale
helps prevent
osteoporosis
(Inferred : 0.88)
broccoli
IS-A
vegetable
(WordNet : 0.9)
broccoli
helps prevent
osteoporosis
(Inferred : 0.49)
Inf. Rule:
Transitive-Through
high in
kale
is high in
magnesium
(TextRun:: 0.39)
magnesium
helps prevent
osteoporosis
(TextRunner : 0.39)
kale
is high in
calcium
(TextRunner : 0.39)
calcium
helps prevent
osteoporosis
(TextRunner : 0.68)
Inf. Rule:
Transitive-Through
high in
broccoli
is high in
calcium
(TextRunner : 0.39)
Inf. Rule:
Transitive-Through
high in
</figure>
<bodyText confidence="0.99913775">
works (Pearl, 1988) model the joint distribution of a
set of variables by creating an undirected graph with
one node for each random variable, and represent-
ing dependencies between variables with cliques in
the graph. Each clique has a corresponding poten-
tial function Ok, which returns a non-negative value
based on the state of variables in the clique. The
probability of a state, x, is given by
</bodyText>
<equation confidence="0.483513">
1 P(x) = Z H Ok(x{k1)
</equation>
<figureCaption confidence="0.950058">
Figure 2: Partial proof ‘tree’ (DAG) for the query “What
vegetables help prevent osteoporosis?” Rectangles de-
pict ground assertions from a knowledge base, rounded
boxes are inferred assertions, and shaded squared repre-
sent the application of inference rules. HOLMES converts
this DAG into a Markov network in order to estimate the
probability of each node.
</figureCaption>
<bodyText confidence="0.864353230769231">
In Section 2.3 we identify several domain-
independent rules, but a user may (optionally)
specify additional, domain-specific rules if de-
sired. In our example, we assume we are given
the domain-specific rule: Prevents(X,Z) :-
IsHighIn(X,Y) ∧ Prevents(Y,Z)
3. A conjunctive query is specified as a Datalog
rule. For example, the question “What vegeta-
bles help prevent osteoporosis?” could be writ-
ten as: query(X) :- IS-A(X,Vegetable)
∧ Prevents(X,osteoporosis)
and returns a set of answers to the query, each with
an associated probability.
</bodyText>
<subsectionHeader confidence="0.993208">
2.1 Basic Operation
</subsectionHeader>
<bodyText confidence="0.997677037735849">
To find these answers and their associated proba-
bilities, HOLMES first finds all ground assertions in
the knowledge bases that are potentially relevant to
the query. This is efficiently done using the infer-
ence rules to chain backwards from the query. Note
that the generated candidate answers, themselves,
are less important than the associated proof trees.
Furthermore, since HOLMES uses these ‘trees’ (ac-
tually, DAGs) to generate a probabilistic graphical
model, HOLMES seeks to find as many proof trees
as possible for each query result — each may influ-
ence the final belief in that result. Figure 2 shows a
partial proof tree for our example query.
To handle uncertainty, HOLMES now constructs a
ground Markov network from the proof trees and the
Markov-logic-encoded inference rules. Markov net-
where the partition function Z is a normalizing term,
and x{k1 denotes the state of all the variables in
clique k.
HOLMES converts the proof trees into a Markov
network in a manner pioneered by the Markov Logic
framework of Richardson and Domingos (2006). A
Boolean variable is created to represent the truth of
each assertion in the proof forest. Next, HOLMES
adds edges to the Markov network to create a clique
corresponding to each application of an inference
rule in the proof forest.
Following the Markov Logic framework, the po-
tential function of a clique has form O(x) = e&apos;&apos; if all
member nodes are true (w denotes the weight of the
inference rule), and O(x) = 1 otherwise. The proba-
bilities of leaf nodes are derived from the underlying
knowledge base,2 and inferred nodes are biased with
an exponential prior.
Finally, HOLMES computes the approximate
probability of each answer by running a variant
of loopy belief propagation (Pearl, 1988) over the
Markov network. In our experience this method
performs well on networks derived from our Horn
clause proof forest, but one could use Monte Carlo
techniques or even exact methods if desired.
Note that this architecture allows HOLMES to
combine information from multiple web pages to in-
fer assertions not explicitly seen in the textual cor-
pus. Because this inference is done using a Markov
network, it correctly handles uncertain extractions
and probabilistic dependencies. By using KBMC to
create a custom, focused network for each query, the
2In our experiments, ground assertions from WordNet get
a uniformly high probability of correctness (0.9), but those ex-
tracted from the Web are assigned probabilities derived from
redundancy statistics, following the intuition that frequently ex-
tracted facts are more likely to be true (Etzioni et al., 2005).
</bodyText>
<page confidence="0.992002">
81
</page>
<bodyText confidence="0.9678795">
amount of probabilistic inference is reduced to man-
ageable proportions.
</bodyText>
<subsectionHeader confidence="0.998107">
2.2 Anytime, Incremental Expansion
</subsectionHeader>
<bodyText confidence="0.99998062962963">
Because exact probabilistic inference is #P-
complete, HOLMES uses approximate methods, but
even these techniques have problems if the Markov
network gets too large. As a result, HOLMES creates
the network incrementally. After the first proof trees
are generated, HOLMES creates the model and per-
forms approximate probabilistic inference. If more
time is available then HOLMES searches for addi-
tional proof trees and updates the network (Fig-
ure 1). This incremental process allows HOLMES
to return initial results (with preliminary probability
estimates) as soon as they are discovered.
For efficiency, HOLMES exploits standard Data-
log optimizations (e.g., it only expands proofs of re-
cently added nodes and it uses an approximation to
magic sets (Ullman, 1989), rather than simple back-
wards chaining). For tractability, we also allow the
user to limit the number of transitive inference steps
for any inference rule.
HOLMES also includes a few enhancements for
dealing with information extracted from natural lan-
guage. For example, HOLMES’s inference rules sup-
port substring/regex matching of ground assertions,
to accommodate simple variations in text. HOLMES
also can be restricted to only operate over proper
nouns, which is useful for queries involving named
entities.
</bodyText>
<subsectionHeader confidence="0.999361">
2.3 Markov Logic Inference Rules
</subsectionHeader>
<bodyText confidence="0.998791">
HOLMES is given the following set of six domain-
independent rules, which are similar to the up-
ward monotone rules introduced by (MacCartney
and Manning, 2007).
</bodyText>
<listItem confidence="0.989611444444444">
1. Observed relations are likely to be true:
R(X,Y) :- ObservedInCorpus(X, R, Y)
2. Synonym substitution preserves meaning:
RTR(X’,Y) :- RTR(X,Y) ∧ Synonym(X, X’)
3. RTR(X,Y’) :- RTR(X,Y) ∧ Synonym(Y, Y’)
4. Generalizations preserve meaning:
RTR(X’,Y) :- RTR(X,Y) ∧ IS-A(X, X’)
5. RTR(X,Y’) :- RTR(X,Y) ∧ IS-A(Y, Y’)
6. Transitivity of Part Meronyms:
</listItem>
<equation confidence="0.7853">
RTR(X,Y’) :- RTR(X,Y) ∧ Part-Of(Y, Y’)
</equation>
<bodyText confidence="0.99978325">
where RTR matches ‘* in’ (e.g., ‘born in’).
For example, if Q(X):-BornIn(X,‘France’),
and we know from WordNet that Paris is in
France, then by inference rule 6, we know that
BornIn(X,‘Paris’) will yield valid results for
Q(X). Although all of these rules contain at most
two relations in the body, HOLMES allows an
arbitrary number of relations in the query and rule
bodies. However, we have found that even simple
rules can dramatically improve some queries.
We set the rule weights to capture the intuition
that deeper inferences decrease the likelihood (as
there are more chances to make mistakes), whereas
additional, independent proof trees increase the
likelihood (as there is more supporting evidence).
Specifically, in our experiments we set the prior on
inferred facts to -0.75, the weight on rule 1 to 1.5,
and the weights on all other rules to 0.6.
At present, we define these weights manually, but
we expect to learn the parameter values in the future.
</bodyText>
<sectionHeader confidence="0.960938" genericHeader="method">
3 Scaling Inference to the Web
</sectionHeader>
<bodyText confidence="0.999671642857143">
If TI is applied to a corpus containing hundreds of
millions or even billions of pages, its run time has to
be at most linear in the size of the corpus. This sec-
tion shows that under some reasonable assumptions
inference does scale linearly.
We start our analysis with two simplifications.
First, we assume that the number of distinct, ground
assertions in the KBs, |A|, grows at most linearly
with the size of the textual corpus. This is cer-
tainly true for assertions extracted by TextRunner
and Kylin, and follows from our exclusion of texts
with complex quantified sentences. Our analysis
now proceeds to consider scaling with respect to |A|
for a fixed query and set of inference rules.
Our second assumption is that the size of every
proof tree is bounded by some constant, m. This
is a strong assumption and one that depends on the
precise set of inference rules and pattern of ground
assertions. However, it holds in our experience, and
if necessary could be enforced by terminating the
search for proof trees at a certain depth, e.g., log(m).
HOLMES’s knowledge-based model construction
has two parts: construction of the proof forest and
conversion of the forest into a Markov network.
Since the Markov network is essentially isomorphic
to the proof forest, the conversion will be O(|A|) if
the forest is linear in size, which is ensured if the
time to construct the proof trees is O(|A|). We show
</bodyText>
<page confidence="0.993805">
82
</page>
<bodyText confidence="0.999938522727273">
this in the remainder of this section.
Recall that HOLMES requires inference rules to
be function-free Horn clauses. While this limits ex-
pressivity to some degree, it provides a huge speed
benefit — logical inference over Horn clauses can
be done in polynomial time, whereas general propo-
sitional inference (i.e., from grounded first-order
rules) is NP-complete.
Alas, even low-order polynomial blowup is un-
acceptable when the textual corpus reaches Web
scale; we seek linear growth. Intuitively, there are
two places where polynomial expansion could cause
trouble. First, the number of different types of proofs
(i.e., first order proofs) could grow too quickly, and
secondly, a given type of proof tree might apply
to too many ground assertions (“tuples” in database
lingo). We treat these issues in turn.
Under our assumptions, each proof tree can be
represented as an expression in relational algebra
with at most m equijoins (Ullman, 1989),3 each
stemming from the application of an inference rule.
Since the number of rules is fixed, as is m, there are
a constant number of possible first-order proof trees.
The bigger concern is that any one of these first-
order trees might result in a polynomial number of
ground trees; if so, the size of the ground forest
(and corresponding Markov network) could grow
too quickly. In fact, polynomial growth is a common
phenomena in database query evaluation. Luckily,
most relations in the Web corpus behave more fa-
vorably. We introduce a property of relations that
ensures m-way joins, and therefore all proof trees
up to size m, can be computed in O(|A|) time.
The intuition is that most relations derived from
large corpora have a ‘heavy-tailed’ distribution,
wherein a few objects appear many times in a rela-
tion, but most appear only once or twice, thus joins
involving rare objects lead to a small number of re-
sults, and so the main limitation on scalability is
common objects. We now prove that if these com-
mon objects account for a small enough fraction of
the relation, then joins will still scale linearly. We
focus on binary relations, but these results can eas-
ily be extended to relations of larger arity.
</bodyText>
<footnote confidence="0.9504258">
3Note that an inference rule of the form H(X) :-
R1(X,Y),R2(Y,Z) is equivalent to the algebraic expression
irX(R1 m R2). First a join is performed between R1 and R2
testing for equality between values of Y ; then a projection elim-
inates all columns besides X.
</footnote>
<bodyText confidence="0.9606464">
Definition 1 A relation, R = {(xi, yi)} ⊆ X ×
Y, is pseudo-functional (PF) in x with degree k, if
∀x ∈ X : |{y|(x, y) ∈ R} |≤ k. When the precise
variable and degree is irrelevant to discussion, we
simply say “R is PF.”
An m-way equijoin over relations that are PF in
the join variables will have at most km ∗ |R |results.
Since km is constant for a given join and |R |scales
linearly in the size of the textual corpus, proof tree
construction over PF relations also scales linearly.
However, due to their heavy-tailed distributions,
most relations extracted from the Web fit the pseudo-
functional definition in most, but not all values of
X. Fortunately, it turns out that in most cases these
“bad” values of X are rare and hence don’t influence
the join size significantly. We formalize this intu-
ition by defining a class of approximately pseudo-
functional (APF) relations and proving that joining
two APF relations produces at most a linear number
of results.
</bodyText>
<equation confidence="0.876648333333333">
Definition 2 A relation, R, is approximately
pseudo-functional (APF) in x with degree k, if X
can be partitioned into two sets XG and XB such
that for all x ∈ XG R is PF with degree k and
|{y|(x, y) ∈ R} |≤ k ∗ log(|R|)
xEXB
</equation>
<bodyText confidence="0.977288666666667">
Theorem 1. If relation R1 is APF in y with de-
gree k1 and R2 is APF in y with degree k2 then
the relation Q = R1 ./ R2 has size at most
O(max(|R1|, |R2|)).
Proof. Since R1 and R2 are APF, we know that
Y can be partitioned into four groups: YBB =
</bodyText>
<equation confidence="0.955481">
n YB2,
YB1 n YB2, YBG = YB1 n YG2, YGB = YG1
</equation>
<bodyText confidence="0.9691783">
YGG = YG1 n YG2.4 We can show that each group
leads to at most O(|A|) entries in Q. For y ∈ YBB
there are at most k1 ∗ k2 ∗ log(|R1|) ∗ log(|R2|) en-
tries in Q. The y ∈ YGB and y ∈ YBG lead to at
most k1 ∗ k2 ∗ log(|R2|) and k1 ∗ k2 ∗ log(|R1|)
entries, respectively. For y ∈ YGG there are at
most k1 ∗ k2 ∗ max(|R1|, |R2|). Summing the re-
sults from the four partitions, we see that |Q |is
O(max(|R1|, |R2|)), thus itis O(|A|).
This theorem and proof can easily be extended to
</bodyText>
<footnote confidence="0.996177">
4YBB are the “doubly bad” values of y that violate the PF
definition for both relations, YGG are the values that do not vio-
late the PF definition for either relation, and YBG and YGB are
the values that violate it in only R1 or R2, resp.
</footnote>
<page confidence="0.999393">
83
</page>
<bodyText confidence="0.9767185">
an m-way equijoin, as long as each relation is APF
in all arguments that are being joined.
</bodyText>
<construct confidence="0.860461833333333">
Theorem 2. If Q is the relation obtained by an equi-
join over m relations R1..m, each having size at most
O(|A|), and if all R1..m are APF in all arguments
that they are joined in with degree at most kma,, and
if � log(|Ri|) &lt; |A|, then |Q |is O(|A|).
1&lt;i&lt;m
</construct>
<bodyText confidence="0.998760428571429">
The inequality in Theorem 2 relates the sizes of
the relations (|R|), the join (m) and the number of
ground assertions (|A|). However, in many cases we
are interested in much smaller values of m than the
inequality enables. We can relax the APF definition
to allow a broader, but still scalable, class of m-way-
APF relations.
</bodyText>
<equation confidence="0.7926266">
Corollary 3. If Q is the relation obtained by an m-
way join, and if each participating relation is APF
y
in their joined variables with a bound of ki * |Ri|
instead of ki * log(|Ri|), then the join is O(|A|).
</equation>
<bodyText confidence="0.9999291">
The final step in our scaling argument concerns
probabilistic inference, which is #P-Complete if per-
formed exactly. This is addressed in two ways. First,
HOLMES uses approximate methods, e.g., loopy be-
lief propagation, which avoids the cost of exact in-
ference — at the cost of reduced precision. Sec-
ondly, at a practical level, HOLMES’s incremental
construction of the graphical model (Figure 1) al-
lows it to bound the size of the network by terminat-
ing the search for additional proofs.
</bodyText>
<sectionHeader confidence="0.997232" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999948615384615">
This section reports on measurements that confirm
that linear scaling with |A |occurs in practice, and
that HOLMES’s inference is not only scalable but
also improves precision/recall on sample queries in
a diverse set of domains. After describing the exper-
imental domains and queries, Section 4.2 reports on
the boost to the area under the precision/recall curve
for a set of example queries in three domains: ge-
ography, business, and nutrition. Section 4.3 then
shows that APF relations are very common in the
Web corpus, and finally Section 4.4 demonstrates
empirically that HOLMES’s inference time scales
linearly with the number of pages in the corpus.
</bodyText>
<subsectionHeader confidence="0.966615">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.982590685714286">
HOLMES utilized two knowledge bases in these ex-
periments: TEXTRUNNER and WordNet. TEX-
TRUNNER contains approximately 183 million dis-
tinct ground assertions extracted from over 117 mil-
lion web pages, and WordNet contains 159 thousand
manually created IS-A, Part-Of, and Synonym asser-
tions.
In all queries, HOLMES utilizes the domain-
independent inference rules described in Sec-
tion 2.3. HOLMES additionally makes use of two
domain-specific inference rules in the Nutrition
domain, to demonstrate the benefits of including
domain-specific information. Estimating the preci-
sion and relative recall of HOLMES requires exten-
sive and careful manual tagging of HOLMES output.
To make this feasible, we restricted ourselves to a
set of twenty queries in three domains, but made the
domains diverse to illustrate the broad scope of the
system.
We now describe each domain briefly.
Geography: the query issued is: “Who was born in
one of the following countries?” More formally,
Q(X) :- BornIn(X,{country}) where {country}
is bound to each of the following nine countries
in turn {France, Germany, China, Thailand, Kenya,
Morocco, Peru, Columbia, Guatemala}, yielding a
total of nine queries.
Because Web text often refers to a person’s
birth city rather than birth country, this query il-
lustrates how combining an ground assertion (e.g.,
BornIn(Alberto Fujimori, Lima)) with back-
ground knowledge (e.g., LocatedIn(Lima, Peru))
enables the system to draw new conclusions (e.g.,
BornIn(Alberto Fujimori, Peru)).
Business: we issued the following two queries.
</bodyText>
<listItem confidence="0.882542125">
1) Which companies are acquiring software com-
panies? Formally, Q(X) :- Acquired(X, Y)
∧ Develops(Y, ‘software’) This query tests
HOLMES’s ability to scalably join a large number of
assertions from multiple pages.
2) Which companies are headquartered in the
USA? Q(X) :- HeadquarteredIn(X, ‘USA’)
∧ IS-A(X, ‘company’)
</listItem>
<bodyText confidence="0.99977225">
Answering this query comprehensively requires
HOLMES to combine a join (over the relations Head-
quarteredIn and IS-A) with transitive inference on
PartOf (e.g., Seattle is PartOf Washington which is
PartOf the USA) and on IS-A (e.g., Microsoft IS-A
software company which IS-A company). The IS-
A assertions came from both TEXTRUNNER (using
patterns from (Hearst, 1992)) and WordNet.
</bodyText>
<page confidence="0.990804">
84
</page>
<figure confidence="0.9979525">
0 1000 2000 3000 4000 5000
Estimated Recall
</figure>
<figureCaption confidence="0.9819515">
Figure 3: PR Curve for BornIn(X, {country}). Inference
boosts the Area under the PR Curve (AuC) by 102 %.
</figureCaption>
<table confidence="0.9998756">
Domain Increase Total Inference
in AuC Time
Geography +102% 55 s
Business +2,643% 145 s
Nutrition +5,595% 64 s
</table>
<tableCaption confidence="0.9891204">
Table 1: Improvement in the AuC of HOLMES over the
BASELINE and total inference time taken by HOLMES.
Results are summed over all queries in the geography,
business, and nutrition domains. Inference time mea-
sured on unoptimized prototype.
</tableCaption>
<bodyText confidence="0.983680111111111">
Nutrition: the nine queries issued are instances
of “What foods prevent disease?” Where a food is
a member of one of the classes: fruit, vegetable, or
grain, and a disease is one of: anemia, scurvy, or
osteoporosis. More formally, Q(X, {disease}) :-
Prevents(X, {disease}) n IS-A(X, {food})
Our experiments in the nutrition domain utilized
two domain-specific inference rules in addition to
the ones presented in Section 2.3:
</bodyText>
<equation confidence="0.925493">
Prevents(X,Y):-HighIn(X,Z) n Prevents(Z,Y)
Prevents(X,Y):-Contains(X,Z) n Prevents(Z,Y)
</equation>
<subsectionHeader confidence="0.999549">
4.2 Effect of Inference on Recall
</subsectionHeader>
<bodyText confidence="0.9999902">
To measure the cost and benefit of HOLMES’s in-
ference we need to define a baseline for compar-
ison. Answering the conjunctive queries in the
business and nutrition domains requires computing
joins, which TEXTRUNNER does not do. Thus, we
defined a baseline system, BASELINE, which has
access to the underlying Knowledge Bases (KBs)
(TEXTRUNNER and WordNet), and the ability to
compute joins using information explicitly stated in
either KB, but does not have the ability to infer new
assertions.
We compared HOLMES with BASELINE in all
three domains. Figure 3 depicts the combined pre-
cision/relative recall curves for the nine Geography
queries. HOLMES yields substantially higher re-
call (the shaded region) at modestly lower preci-
sion, doubling the area under the precision/recall
curve (AuC). The other precision/recall curves also
showed a slight drop in precision for substantial
gains in recall. Table 1 summarizes the results, along
with the total runtime needed for inference. Because
relations in the business domain are much larger
than in the other domains (i.e., 100x ground asser-
tions), inference is slower in this domain.
We note that inference is particularly helpful with
rarely mentioned instances. However, inference can
lead to errors when the proof tree contains joins on
generic terms (e.g., “company”) or common extrac-
tion errors (e.g., “LLC” as a company name). This
is a key area for future work.
</bodyText>
<subsectionHeader confidence="0.999825">
4.3 Prevalence of APF Relations
</subsectionHeader>
<bodyText confidence="0.995234964285714">
To determine the prevalence of APF relations in Web
text, we examined a sample of 500 binary relations
selected randomly from TEXTRUNNER’s ground as-
sertions. The surface forms of the relations and ar-
guments may misrepresent the true properties of the
underlying concepts, so to better estimate the true
properties we merged synonymous values as given
by Resolver (Yates and Etzioni, 2007) or the most
frequent sense of the word in WordNet. For exam-
ple, we would consider BornIn(baby, hospital)
and BornAt(infant, infirmary) to represent the
same concept, and so would merge them into one
instance of the ‘Born In’ relation. The largest two re-
lations had over 1.25 million unique instances each,
and 52% of the relations had more than 10,000 in-
stances.
For each relation R, we first found all instances
of R extracted by TEXTRUNNER and merged all
synonymous instances as described above. Then,
for each argument of R we computed the smallest
value, Kmin, such that R is APF with degree Kmin.
Since many interesting assertions can be inferred by
simply joining two relations, we also considered the
special case of 2-way joins using Corollary 3. We
computed the smallest value, K2./, such that the re-
lation is two-way-APF with degree K2./.
Figure 4 shows the fraction of relations with
Kmin and K2./ of at most K as a function of varying
</bodyText>
<figure confidence="0.987527444444444">
Precision
0.8
0.6
0.4
0.2
0
1
Baseline
Holmes
Increase in AuC
85
APF
APF for two-way join
160
140
120
100
40
80
60
20
0
Geography
Business
Nutrition
R2 = 0.9931
R2 = 0.9808
R2 = 0.9881
100%
0%
80%
60%
40%
20%
0 1000 2000 3000 4000 5000 6000
Degree of Approximate Pseudo-Functionality
</figure>
<figureCaption confidence="0.9908902">
Figure 4: Prevalence of APF relations in Web text. The
x-axis depicts the degree of pseudo-functionality, e.g.,
K,..i,,, and K2,,, (see definition 2); the y-axis lists the
percent of relations that are APF with that degree. Re-
sults are averaged over both arguments.
</figureCaption>
<bodyText confidence="0.99976996875">
values of K. The results are averaged over both ar-
guments of each binary relation. For arbitrary joins
in this KB, 80% of the relations are APF with de-
gree less than 496; for 2-way joins (like the ones in
our inference rules and test queries), 80% of the rela-
tions are APF with degree less than 65. These results
indicate that the majority of relations TEXTRUNNER
extracted from text are APF, and so we can expect
HOLMES’s techniques will allow efficient inference
over most relations.
While Theorem 2 guarantees that joins over those
relations will be O(|R|), that notation hides a poten-
tially large constant factor of Kminm. Fortunately
the constant factor is significantly smaller in prac-
tice. To see why, we re-examine the proof: the large
factor comes from assuming that all of R’s first ar-
guments which meet the PF definition are associated
with exactly Kmin distinct second arguments. How-
ever, in our corpus 83% of first arguments are as-
sociated with only one second argument. Clearly,
our worst-case analysis substantially over-estimates
inference time for most queries. Moreover, in ad-
ditional experiments (omitted due to space limita-
tions), measured join sizes grew linearly in the size
of the corpus, but were on average two to three or-
ders of magnitude smaller than the bounds given in
the theory. This observation held across relations
with different sizes and values of Kmin.
While the results in Figure 4 may vary for other
sets of relations, we believe the general trends
hold. This is promising for Question Answering and
Textual Inference systems, since if true it implies
</bodyText>
<figure confidence="0.722771">
0% 20% 40% 60% 80% 100%
Fraction of Corpus
</figure>
<figureCaption confidence="0.99329425">
Figure 5: The effects of corpus size on total inference
time. We see approximately linear growth in all domains,
and display the best fit lines and coefficient of determina-
tion (R2) of each.
</figureCaption>
<bodyText confidence="0.999692333333333">
that combining information from multiple difference
source is feasible, and can allow such systems to in-
fer answers not explicitly seen in any source.
</bodyText>
<subsectionHeader confidence="0.999662">
4.4 Scalability of Inference Speed
</subsectionHeader>
<bodyText confidence="0.999914076923077">
Since the previous subsection showed that most re-
lations are APF in their arguments, our theory pre-
dicts HOLMES’s inference will scale linearly. We
tested this hypothesis empirically by running infer-
ence over the test queries in our three domains, while
varying the number of pages in the textual corpus.
Figure 5 shows how the inference time HOLMES
used to answer all queries in each domain scales
with KB size. For these queries, and several oth-
ers we tested (not shown here), inference time grows
linearly with the size of the KB. Based on these re-
sults we believe that HOLMES can provide scalable
inference over a wide variety of domains.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999958461538462">
Textual Entailment systems are given two textual
fragments, text T and hypothesis H, and attempt to
decide if the meaning of H can be inferred from
the meaning of T (Dagan et al., 2005). While
many approaches have addressed this problem, our
work is most closely related to that of (Raina et al.,
2005; MacCartney and Manning, 2007; Tatu and
Moldovan, 2006; Braz et al., 2005), which convert
the inputs into logical forms and then attempt to
‘prove’ H from T plus a set of axioms. For in-
stance, (Braz et al., 2005) represents T, H, and a
set of rewrite rules in a description logic framework,
and determines entailment by solving an integer lin-
</bodyText>
<page confidence="0.988312">
86
</page>
<bodyText confidence="0.999964910714286">
ear program derived from that representation.
These approaches and related ones (e.g.,
(Van Durme and Schubert, 2008)) use highly
expressive representations, enabling them to ex-
press negation, temporal information, and more.
HOLMES’s representation is much simpler—
Markov Logic Horn Clauses for inference rules
coupled with a massive database of ground asser-
tions. However, this simplification allows HOLMES
to tackle a “text” of enormously larger size: 117
million Web pages versus a single paragraph. A sec-
ond, if smaller, difference stems from the fact that
instead of determining whether a single hypothesis
sentence, H, follows from the text, HOLMES tries to
find all consequents that match a conjunctive query.
HOLMES is also related to open-domain question-
answering systems such as Mulder (Kwok et al.,
2001), AskMSR (Brill et al., 2002), and others
(Harabagiu et al., 2000; Brill et al., 2001). How-
ever, these Q/A systems attempt to find individual
documents or sentences containing the answer. They
often perform deep analysis on promising texts, and
back off to shallower, less reliable methods if those
fail. In contrast, HOLMES utilizes TI and attempts
to combine information from multiple different sen-
tences in a scalable way.
While its ability to combine information from
multiple sources is promising, HOLMES has several
limitations these Q/A systems do not have. Since
HOLMES relies on an information extraction sys-
tem to convert sentences into ground predicates,
any limitations of the IE system will be propagated
to HOLMES. Additionally, the logical representa-
tion HOLMES uses limits the reasoning and types
of questions it can answer. HOLMES is geared to-
wards answering questions which are naturally ex-
pressed as properties and relations of entities, and is
not well suited to answering more abstract or open
ended questions. Although we have demonstrated
that HOLMES is scalable, further work is needed to
make it to run at interactive speeds.
Finally, research in statistical relational learning
such as MLNs (Richardson and Domingos, 2006),
RMNs (Taskar et al., 2002), and others (Getoor
and Taskar, 2007) have studied techniques for com-
bining logical and probabilistic inference. Our in-
ference rules are more restrictive than those al-
lowed in MLNs, but this trade-off allows us to ef-
ficiently scale inference to large, open domain cor-
pora. By constructing only cliques for satisfied in-
ference rules, HOLMES explicitly models the intu-
ition behind LazySAT inference (Singla and Domin-
gos, 2006) as used in MLNs. I.e., most Horn clause
inference rules will be trivially satisfied since their
antecedents will be false, so we only need to worry
about ones where the antecedent is true.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.991142884615385">
This paper makes three main contributions:
1. We introduce and evaluate the HOLMES sys-
tem, which leverages KBMC methods in order
to scale a class of TI methods to the Web.
2. We define the notion of Approximately Pseudo-
Functional (APF) relations and prove that, for
a APF relations, HOLMES’s inference time in-
creases linearly with the size of the input cor-
pus. We show empirically that APF relations
appear to be prevalent in our Web corpus (Fig-
ure 4), and that HOLMES’s runtime does scale
linearly with the size of its input (Figure 5), tak-
ing only a few CPU minutes when run over 183
million distinct ground assertions.
3. We present experiments demonstrating that, for
a set of queries in the domains of geography,
business, and nutrition, HOLMES substantially
improves the quality of answers (measured by
AuC) relative to a “no inference” baseline.
In the future, we plan more extensive tests to char-
acterize when HOLMES’s inference is helpful. We
also hope to examine in what cases jointly perform-
ing extraction and inference (as opposed to perform-
ing them separately) is feasible at scale. Finally, we
plan to examine methods for HOLMES to learn both
rule weights and new inference rules.
</bodyText>
<sectionHeader confidence="0.996523" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996930777777778">
We thank the following for helpful comments on
previous drafts: Fei Wu, Michele Banko, Mausam,
Doug Downey, and Alan Ritter. This research was
supported in part by NSF grants IIS-0535284, IIS-
0312988, and IIS-0307906, ONR grants N00014-
08-1-0431 and N00014-06-1-0147, CALO grant 03-
000225, the WRF / TJ Cable Professorship as well
as gifts from Google. The work was performed at
the University of Washington’s Turing Center.
</bodyText>
<page confidence="0.998544">
87
</page>
<sectionHeader confidence="0.993913" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999733548780488">
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sam-
mons. 2005. An inference model for semantic en-
tailment in natural language. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 1678–1679.
E. Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng.
2001. Data-intensive question answering. In Procs.
of Text REtrieval Conference (TREC-10), pages 393–
400.
Eric Brill, Susan Dumais, and Michele Banko. 2002. An
analysis of the AskMSR question-answering system.
In EMNLP ’02: Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 257–264, Morristown, NJ, USA. Association for
Computational Linguistics.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1–8.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91–134.
L. Getoor and B. Taskar. 2007. Introduction to Statistical
Relational Learning. MIT Press.
S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Exper-
iments with open-domain textual question answering.
In Procs. of the COLING-2000.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539–545, Nantes, France.
C.C.T. Kwok, O. Etzioni, and D.S. Weld. 2001. Scal-
ing question answering to the Web. Proceedings of
the 10th international conference on World Wide Web,
pages 150–161.
B. MacCartney and C.D. Manning. 2007. Natural Logic
for Textual Inference. In Workshop on Textual Entail-
ment and Paraphrasing.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to wordnet: An on-line
lexical database. International Journal of Lexicogra-
phy, 3(4):235–312.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks ofplausible inference. Morgan
Kaufmann Publishers Inc. San Francisco, CA, USA.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI 2005.
AAAI Press.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107–136.
Parag Singla and Pedro Domingos. 2006. Memory-
efficient inference in relational domains. In AAAI.
B. Taskar, P. Abbeel, and D. Koller. 2002. Discrimi-
native probabilistic models for relational data. Eigh-
teenth Conference on Uncertainty in Artificial Intelli-
gence (UAI02).
Marta Tatu and Dan Moldovan. 2006. A logic-based
semantic approach to recognizing textual entailment.
In Proceedings of the COLING/ACL on Main confer-
ence poster sessions, pages 819–826, Morristown, NJ,
USA. Association for Computational Linguistics.
J. Ullman. 1989. Database and knowledge-base systems.
Computer Science Press.
B. Van Durme and L.K. Schubert. 2008. Open knowl-
edge extraction through compositional language pro-
cessing. In Symposium on Semantics in Systems for
Text Processing.
M. Wellman, J. Breese, and R. Goldman. 1992. From
knowledge bases to decision models. The Knowledge
Engineering Review, 7(1):35–53.
F. Wu and D. Weld. 2007. Autonomously semantifying
Wikipedia. In Proceedings of the ACM Sixteenth Con-
ference on Information and Knowledge Management
(CIKM-07), Lisbon, Porgugal.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
</reference>
<page confidence="0.999401">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649684">
<title confidence="0.999881">Scaling Textual Inference to the Web</title>
<author confidence="0.99243">Stefan Schoenmackers</author>
<author confidence="0.99243">Oren Etzioni</author>
<author confidence="0.99243">S Daniel</author>
<affiliation confidence="0.921242666666667">Turing University of Computer Science and</affiliation>
<address confidence="0.928662">Box Seattle, WA 98195,</address>
<email confidence="0.999795">stef,etzioni,weld@cs.washington.edu</email>
<abstract confidence="0.999815681818182">Most Web-based Q/A systems work by findpages that contain an to a question. These systems are helpless if the answer has to be inferred from multiple sentences, possibly on different pages. To solve problem, we introduce the syswhich utilizes inference over tuples extracted from text. previous work on TI the literature on textual entailment) has been apto paragraph-sized texts, utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. only a few minutes, recall for example queries in three disparate domains (geography, business, and nutrition). runtime is the size of its input corpus due to a surprising property of many textual relations in the Web corpus—they are “approximately” functional in a well-defined sense.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the Web. In Procs. of IJCAI.</title>
<date>2007</date>
<contexts>
<context position="4226" citStr="Banko et al., 2007" startWordPosition="650" endWordPosition="653">orating the Markov network. HOLMES makes some important simplifying assumptions. Specifically, we use simple ground tuples to represent extracted assertions (e.g., contains(kale, calcium)). Syntactic problems (e.g., anaphora, relative clauses) and semantic challenges (e.g., quantification, counterfactuals, temporal qualification) are delegated to the extraction system or simply ignored. This paper focuses on scalability for this subset of the TI task. 1.2 Summary of Experimental Results We tested HOLMES on 183 million distinct ground assertions extracted from the Web by the TEXTRUNNER system (Banko et al., 2007), coupled with 159 thousand ground assertions from WordNet (Miller et al., 1990), and a compact set of handcoded inference rules. Given a total of 55 to 145 seconds, HOLMES was able to produce high-quality inferences that doubled the number of answers to example queries in three disparate domains: geography, business, and nutrition. We also evaluated how the speed of HOLMES scaled with the size of its input corpus. In the general case, logical inference over a Horn theory (needed in order to produce the probabilistic network) is polynomial in the number of ground assertions, and hence in the s</context>
<context position="7221" citStr="Banko et al., 2007" startWordPosition="1144" endWordPosition="1147">subset of ground assertions and inference rules that may influence the answers to the query — enabling the construction of a small and focused Markov network. Since this graphical model is much smaller than one incorporating all ground assertions, probabilistic inference will be much faster than if naive compilation were used. Figure 1 summarizes the operation of HOLMES. As with many theorem provers or KBMC systems, HOLMES takes three inputs: 1. A set of knowledge bases — databases of ground relational assertions, each with an estimate of its probability, which can be generated by TextRunner (Banko et al., 2007) or Kylin (Wu and Weld, 2007). In our example, we would extract the assertions IsHighIn(kale, calcium) and Prevents(calcium, osteoporosis) from those sentences. 2. A domain theory – A set of probabilistic inference rules written as Markov logic Horn clauses, which can be used to derive new assertions. The weight associated with each clause specifies its reliability. 80 kale matches the query (Inferred : 0.91) broccoli matches the query (Inferred : 0.58) Query Result Query Result kale IS-A vegetable (WordNet : 0.9) kale helps prevent osteoporosis (Inferred : 0.88) broccoli IS-A vegetable (WordN</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the Web. In Procs. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Braz</author>
<author>R Girju</author>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>M Sammons</author>
</authors>
<title>An inference model for semantic entailment in natural language.</title>
<date>2005</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>1678--1679</pages>
<contexts>
<context position="32925" citStr="Braz et al., 2005" startWordPosition="5424" endWordPosition="5427"> these queries, and several others we tested (not shown here), inference time grows linearly with the size of the KB. Based on these results we believe that HOLMES can provide scalable inference over a wide variety of domains. 5 Related Work Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005). While many approaches have addressed this problem, our work is most closely related to that of (Raina et al., 2005; MacCartney and Manning, 2007; Tatu and Moldovan, 2006; Braz et al., 2005), which convert the inputs into logical forms and then attempt to ‘prove’ H from T plus a set of axioms. For instance, (Braz et al., 2005) represents T, H, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer lin86 ear program derived from that representation. These approaches and related ones (e.g., (Van Durme and Schubert, 2008)) use highly expressive representations, enabling them to express negation, temporal information, and more. HOLMES’s representation is much simpler— Markov Logic Horn Clauses for inference rules coupled with a ma</context>
</contexts>
<marker>Braz, Girju, Punyakanok, Roth, Sammons, 2005</marker>
<rawString>R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sammons. 2005. An inference model for semantic entailment in natural language. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 1678–1679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Lin</author>
<author>M Banko</author>
<author>S T Dumais</author>
<author>A Y Ng</author>
</authors>
<title>Data-intensive question answering.</title>
<date>2001</date>
<booktitle>In Procs. of Text REtrieval Conference (TREC-10),</booktitle>
<pages>393--400</pages>
<contexts>
<context position="34098" citStr="Brill et al., 2001" startWordPosition="5613" endWordPosition="5616"> Clauses for inference rules coupled with a massive database of ground assertions. However, this simplification allows HOLMES to tackle a “text” of enormously larger size: 117 million Web pages versus a single paragraph. A second, if smaller, difference stems from the fact that instead of determining whether a single hypothesis sentence, H, follows from the text, HOLMES tries to find all consequents that match a conjunctive query. HOLMES is also related to open-domain questionanswering systems such as Mulder (Kwok et al., 2001), AskMSR (Brill et al., 2002), and others (Harabagiu et al., 2000; Brill et al., 2001). However, these Q/A systems attempt to find individual documents or sentences containing the answer. They often perform deep analysis on promising texts, and back off to shallower, less reliable methods if those fail. In contrast, HOLMES utilizes TI and attempts to combine information from multiple different sentences in a scalable way. While its ability to combine information from multiple sources is promising, HOLMES has several limitations these Q/A systems do not have. Since HOLMES relies on an information extraction system to convert sentences into ground predicates, any limitations of t</context>
</contexts>
<marker>Brill, Lin, Banko, Dumais, Ng, 2001</marker>
<rawString>E. Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng. 2001. Data-intensive question answering. In Procs. of Text REtrieval Conference (TREC-10), pages 393– 400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Susan Dumais</author>
<author>Michele Banko</author>
</authors>
<title>An analysis of the AskMSR question-answering system.</title>
<date>2002</date>
<booktitle>In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<pages>257--264</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1305" citStr="Brill et al., 2002" startWordPosition="198" endWordPosition="201">d texts, HOLMES utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, HOLMES doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, HOLMES’s runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus—they are “approximately” functional in a well-defined sense. 1 Introduction and Motivation Numerous researchers have identified the Web as a rich source of answers to factual questions, e.g., (Kwok et al., 2001; Brill et al., 2002), but often the desired information is not stated explicitly even in a textual corpus as massive as the Web. Consider the question “What vegetables help prevent osteoporosis?” Since there is likely no sentence on the Web directly stating “Kale prevents osteoporosis”, a system must infer that kale is an answer by combining facts from multiple sentences, possibly from different pages, which justify that conclusion: i.e., that kale is a vegetable, kale contains calcium, and calcium helps prevent osteoporosis. 79 Figure 1: The architecture of HOLMES. Textual Inference (TI) methods have advanced in</context>
<context position="34041" citStr="Brill et al., 2002" startWordPosition="5603" endWordPosition="5606">OLMES’s representation is much simpler— Markov Logic Horn Clauses for inference rules coupled with a massive database of ground assertions. However, this simplification allows HOLMES to tackle a “text” of enormously larger size: 117 million Web pages versus a single paragraph. A second, if smaller, difference stems from the fact that instead of determining whether a single hypothesis sentence, H, follows from the text, HOLMES tries to find all consequents that match a conjunctive query. HOLMES is also related to open-domain questionanswering systems such as Mulder (Kwok et al., 2001), AskMSR (Brill et al., 2002), and others (Harabagiu et al., 2000; Brill et al., 2001). However, these Q/A systems attempt to find individual documents or sentences containing the answer. They often perform deep analysis on promising texts, and back off to shallower, less reliable methods if those fail. In contrast, HOLMES utilizes TI and attempts to combine information from multiple different sentences in a scalable way. While its ability to combine information from multiple sources is promising, HOLMES has several limitations these Q/A systems do not have. Since HOLMES relies on an information extraction system to conve</context>
</contexts>
<marker>Brill, Dumais, Banko, 2002</marker>
<rawString>Eric Brill, Susan Dumais, and Michele Banko. 2002. An analysis of the AskMSR question-answering system. In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing, pages 257–264, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2078" citStr="Dagan et al., 2005" startWordPosition="321" endWordPosition="324">revent osteoporosis?” Since there is likely no sentence on the Web directly stating “Kale prevents osteoporosis”, a system must infer that kale is an answer by combining facts from multiple sentences, possibly from different pages, which justify that conclusion: i.e., that kale is a vegetable, kale contains calcium, and calcium helps prevent osteoporosis. 79 Figure 1: The architecture of HOLMES. Textual Inference (TI) methods have advanced in recent years. For example, textual entailment techniques aim to determine whether one textual fragment (the hypothesis) follows from another (the text) (Dagan et al., 2005). While most TI researchers have focused on high-quality inferences from a small source text, we seek to utilize sizable chunks of the Web corpus as our source text. In order to do this, we must confront two major challenges. The first is uncertainty: TI is an imperfect process, particularly when applied to the Web corpus, hence probabilistic methods help to assess the confidence in inferences. The second challenge is scalability: how does inference time scale given increasingly large corpora as input? 1.1 HOLMES: A Scalable TI System This paper describes HOLMES, an implemented system, which a</context>
<context position="32734" citStr="Dagan et al., 2005" startWordPosition="5392" endWordPosition="5395">s in our three domains, while varying the number of pages in the textual corpus. Figure 5 shows how the inference time HOLMES used to answer all queries in each domain scales with KB size. For these queries, and several others we tested (not shown here), inference time grows linearly with the size of the KB. Based on these results we believe that HOLMES can provide scalable inference over a wide variety of domains. 5 Related Work Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005). While many approaches have addressed this problem, our work is most closely related to that of (Raina et al., 2005; MacCartney and Manning, 2007; Tatu and Moldovan, 2006; Braz et al., 2005), which convert the inputs into logical forms and then attempt to ‘prove’ H from T plus a set of axioms. For instance, (Braz et al., 2005) represents T, H, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer lin86 ear program derived from that representation. These approaches and related ones (e.g., (Van Durme and Schubert, 2008)) use highly expressi</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL Recognising Textual Entailment Challenge. Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>S Kok</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="12190" citStr="Etzioni et al., 2005" startWordPosition="1937" endWordPosition="1940">ES to combine information from multiple web pages to infer assertions not explicitly seen in the textual corpus. Because this inference is done using a Markov network, it correctly handles uncertain extractions and probabilistic dependencies. By using KBMC to create a custom, focused network for each query, the 2In our experiments, ground assertions from WordNet get a uniformly high probability of correctness (0.9), but those extracted from the Web are assigned probabilities derived from redundancy statistics, following the intuition that frequently extracted facts are more likely to be true (Etzioni et al., 2005). 81 amount of probabilistic inference is reduced to manageable proportions. 2.2 Anytime, Incremental Expansion Because exact probabilistic inference is #Pcomplete, HOLMES uses approximate methods, but even these techniques have problems if the Markov network gets too large. As a result, HOLMES creates the network incrementally. After the first proof trees are generated, HOLMES creates the model and performs approximate probabilistic inference. If more time is available then HOLMES searches for additional proof trees and updates the network (Figure 1). This incremental process allows HOLMES to</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Getoor</author>
<author>B Taskar</author>
</authors>
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="35326" citStr="Getoor and Taskar, 2007" startWordPosition="5805" endWordPosition="5808">E system will be propagated to HOLMES. Additionally, the logical representation HOLMES uses limits the reasoning and types of questions it can answer. HOLMES is geared towards answering questions which are naturally expressed as properties and relations of entities, and is not well suited to answering more abstract or open ended questions. Although we have demonstrated that HOLMES is scalable, further work is needed to make it to run at interactive speeds. Finally, research in statistical relational learning such as MLNs (Richardson and Domingos, 2006), RMNs (Taskar et al., 2002), and others (Getoor and Taskar, 2007) have studied techniques for combining logical and probabilistic inference. Our inference rules are more restrictive than those allowed in MLNs, but this trade-off allows us to efficiently scale inference to large, open domain corpora. By constructing only cliques for satisfied inference rules, HOLMES explicitly models the intuition behind LazySAT inference (Singla and Domingos, 2006) as used in MLNs. I.e., most Horn clause inference rules will be trivially satisfied since their antecedents will be false, so we only need to worry about ones where the antecedent is true. 6 Conclusions This pape</context>
</contexts>
<marker>Getoor, Taskar, 2007</marker>
<rawString>L. Getoor and B. Taskar. 2007. Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>S Maiorano</author>
</authors>
<title>Experiments with open-domain textual question answering.</title>
<date>2000</date>
<booktitle>In Procs. of the COLING-2000.</booktitle>
<contexts>
<context position="34077" citStr="Harabagiu et al., 2000" startWordPosition="5609" endWordPosition="5612">mpler— Markov Logic Horn Clauses for inference rules coupled with a massive database of ground assertions. However, this simplification allows HOLMES to tackle a “text” of enormously larger size: 117 million Web pages versus a single paragraph. A second, if smaller, difference stems from the fact that instead of determining whether a single hypothesis sentence, H, follows from the text, HOLMES tries to find all consequents that match a conjunctive query. HOLMES is also related to open-domain questionanswering systems such as Mulder (Kwok et al., 2001), AskMSR (Brill et al., 2002), and others (Harabagiu et al., 2000; Brill et al., 2001). However, these Q/A systems attempt to find individual documents or sentences containing the answer. They often perform deep analysis on promising texts, and back off to shallower, less reliable methods if those fail. In contrast, HOLMES utilizes TI and attempts to combine information from multiple different sentences in a scalable way. While its ability to combine information from multiple sources is promising, HOLMES has several limitations these Q/A systems do not have. Since HOLMES relies on an information extraction system to convert sentences into ground predicates,</context>
</contexts>
<marker>Harabagiu, Pasca, Maiorano, 2000</marker>
<rawString>S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Experiments with open-domain textual question answering. In Procs. of the COLING-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Procs. of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="25478" citStr="Hearst, 1992" startWordPosition="4187" endWordPosition="4188"> Acquired(X, Y) ∧ Develops(Y, ‘software’) This query tests HOLMES’s ability to scalably join a large number of assertions from multiple pages. 2) Which companies are headquartered in the USA? Q(X) :- HeadquarteredIn(X, ‘USA’) ∧ IS-A(X, ‘company’) Answering this query comprehensively requires HOLMES to combine a join (over the relations HeadquarteredIn and IS-A) with transitive inference on PartOf (e.g., Seattle is PartOf Washington which is PartOf the USA) and on IS-A (e.g., Microsoft IS-A software company which IS-A company). The ISA assertions came from both TEXTRUNNER (using patterns from (Hearst, 1992)) and WordNet. 84 0 1000 2000 3000 4000 5000 Estimated Recall Figure 3: PR Curve for BornIn(X, {country}). Inference boosts the Area under the PR Curve (AuC) by 102 %. Domain Increase Total Inference in AuC Time Geography +102% 55 s Business +2,643% 145 s Nutrition +5,595% 64 s Table 1: Improvement in the AuC of HOLMES over the BASELINE and total inference time taken by HOLMES. Results are summed over all queries in the geography, business, and nutrition domains. Inference time measured on unoptimized prototype. Nutrition: the nine queries issued are instances of “What foods prevent disease?” </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Procs. of the 14th International Conference on Computational Linguistics, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C T Kwok</author>
<author>O Etzioni</author>
<author>D S Weld</author>
</authors>
<title>Scaling question answering to the Web.</title>
<date>2001</date>
<booktitle>Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>150--161</pages>
<contexts>
<context position="1284" citStr="Kwok et al., 2001" startWordPosition="194" endWordPosition="197">d to paragraph-sized texts, HOLMES utilizes knowledge-based model construction to scale TI to a corpus of 117 million Web pages. Given only a few minutes, HOLMES doubles recall for example queries in three disparate domains (geography, business, and nutrition). Importantly, HOLMES’s runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the Web corpus—they are “approximately” functional in a well-defined sense. 1 Introduction and Motivation Numerous researchers have identified the Web as a rich source of answers to factual questions, e.g., (Kwok et al., 2001; Brill et al., 2002), but often the desired information is not stated explicitly even in a textual corpus as massive as the Web. Consider the question “What vegetables help prevent osteoporosis?” Since there is likely no sentence on the Web directly stating “Kale prevents osteoporosis”, a system must infer that kale is an answer by combining facts from multiple sentences, possibly from different pages, which justify that conclusion: i.e., that kale is a vegetable, kale contains calcium, and calcium helps prevent osteoporosis. 79 Figure 1: The architecture of HOLMES. Textual Inference (TI) met</context>
<context position="34012" citStr="Kwok et al., 2001" startWordPosition="5598" endWordPosition="5601">ral information, and more. HOLMES’s representation is much simpler— Markov Logic Horn Clauses for inference rules coupled with a massive database of ground assertions. However, this simplification allows HOLMES to tackle a “text” of enormously larger size: 117 million Web pages versus a single paragraph. A second, if smaller, difference stems from the fact that instead of determining whether a single hypothesis sentence, H, follows from the text, HOLMES tries to find all consequents that match a conjunctive query. HOLMES is also related to open-domain questionanswering systems such as Mulder (Kwok et al., 2001), AskMSR (Brill et al., 2002), and others (Harabagiu et al., 2000; Brill et al., 2001). However, these Q/A systems attempt to find individual documents or sentences containing the answer. They often perform deep analysis on promising texts, and back off to shallower, less reliable methods if those fail. In contrast, HOLMES utilizes TI and attempts to combine information from multiple different sentences in a scalable way. While its ability to combine information from multiple sources is promising, HOLMES has several limitations these Q/A systems do not have. Since HOLMES relies on an informati</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>C.C.T. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling question answering to the Web. Proceedings of the 10th international conference on World Wide Web, pages 150–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Natural Logic for Textual Inference.</title>
<date>2007</date>
<booktitle>In Workshop on Textual Entailment and Paraphrasing.</booktitle>
<contexts>
<context position="13764" citStr="MacCartney and Manning, 2007" startWordPosition="2176" endWordPosition="2179">the user to limit the number of transitive inference steps for any inference rule. HOLMES also includes a few enhancements for dealing with information extracted from natural language. For example, HOLMES’s inference rules support substring/regex matching of ground assertions, to accommodate simple variations in text. HOLMES also can be restricted to only operate over proper nouns, which is useful for queries involving named entities. 2.3 Markov Logic Inference Rules HOLMES is given the following set of six domainindependent rules, which are similar to the upward monotone rules introduced by (MacCartney and Manning, 2007). 1. Observed relations are likely to be true: R(X,Y) :- ObservedInCorpus(X, R, Y) 2. Synonym substitution preserves meaning: RTR(X’,Y) :- RTR(X,Y) ∧ Synonym(X, X’) 3. RTR(X,Y’) :- RTR(X,Y) ∧ Synonym(Y, Y’) 4. Generalizations preserve meaning: RTR(X’,Y) :- RTR(X,Y) ∧ IS-A(X, X’) 5. RTR(X,Y’) :- RTR(X,Y) ∧ IS-A(Y, Y’) 6. Transitivity of Part Meronyms: RTR(X,Y’) :- RTR(X,Y) ∧ Part-Of(Y, Y’) where RTR matches ‘* in’ (e.g., ‘born in’). For example, if Q(X):-BornIn(X,‘France’), and we know from WordNet that Paris is in France, then by inference rule 6, we know that BornIn(X,‘Paris’) will yield vali</context>
<context position="32880" citStr="MacCartney and Manning, 2007" startWordPosition="5416" endWordPosition="5419">wer all queries in each domain scales with KB size. For these queries, and several others we tested (not shown here), inference time grows linearly with the size of the KB. Based on these results we believe that HOLMES can provide scalable inference over a wide variety of domains. 5 Related Work Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005). While many approaches have addressed this problem, our work is most closely related to that of (Raina et al., 2005; MacCartney and Manning, 2007; Tatu and Moldovan, 2006; Braz et al., 2005), which convert the inputs into logical forms and then attempt to ‘prove’ H from T plus a set of axioms. For instance, (Braz et al., 2005) represents T, H, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer lin86 ear program derived from that representation. These approaches and related ones (e.g., (Van Durme and Schubert, 2008)) use highly expressive representations, enabling them to express negation, temporal information, and more. HOLMES’s representation is much simpler— Markov Logic Horn </context>
</contexts>
<marker>MacCartney, Manning, 2007</marker>
<rawString>B. MacCartney and C.D. Manning. 2007. Natural Logic for Textual Inference. In Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="4306" citStr="Miller et al., 1990" startWordPosition="663" endWordPosition="666">. Specifically, we use simple ground tuples to represent extracted assertions (e.g., contains(kale, calcium)). Syntactic problems (e.g., anaphora, relative clauses) and semantic challenges (e.g., quantification, counterfactuals, temporal qualification) are delegated to the extraction system or simply ignored. This paper focuses on scalability for this subset of the TI task. 1.2 Summary of Experimental Results We tested HOLMES on 183 million distinct ground assertions extracted from the Web by the TEXTRUNNER system (Banko et al., 2007), coupled with 159 thousand ground assertions from WordNet (Miller et al., 1990), and a compact set of handcoded inference rules. Given a total of 55 to 145 seconds, HOLMES was able to produce high-quality inferences that doubled the number of answers to example queries in three disparate domains: geography, business, and nutrition. We also evaluated how the speed of HOLMES scaled with the size of its input corpus. In the general case, logical inference over a Horn theory (needed in order to produce the probabilistic network) is polynomial in the number of ground assertions, and hence in the size of the textual corpus.1 Unfortunately, this is prohibitive, since even lowor</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Introduction to wordnet: An on-line lexical database. International Journal of Lexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic reasoning in intelligent systems: networks ofplausible inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="8265" citStr="Pearl, 1988" startWordPosition="1306" endWordPosition="1307">e query (Inferred : 0.58) Query Result Query Result kale IS-A vegetable (WordNet : 0.9) kale helps prevent osteoporosis (Inferred : 0.88) broccoli IS-A vegetable (WordNet : 0.9) broccoli helps prevent osteoporosis (Inferred : 0.49) Inf. Rule: Transitive-Through high in kale is high in magnesium (TextRun:: 0.39) magnesium helps prevent osteoporosis (TextRunner : 0.39) kale is high in calcium (TextRunner : 0.39) calcium helps prevent osteoporosis (TextRunner : 0.68) Inf. Rule: Transitive-Through high in broccoli is high in calcium (TextRunner : 0.39) Inf. Rule: Transitive-Through high in works (Pearl, 1988) model the joint distribution of a set of variables by creating an undirected graph with one node for each random variable, and representing dependencies between variables with cliques in the graph. Each clique has a corresponding potential function Ok, which returns a non-negative value based on the state of variables in the clique. The probability of a state, x, is given by 1 P(x) = Z H Ok(x{k1) Figure 2: Partial proof ‘tree’ (DAG) for the query “What vegetables help prevent osteoporosis?” Rectangles depict ground assertions from a knowledge base, rounded boxes are inferred assertions, and s</context>
<context position="11330" citStr="Pearl, 1988" startWordPosition="1804" endWordPosition="1805">of forest. Next, HOLMES adds edges to the Markov network to create a clique corresponding to each application of an inference rule in the proof forest. Following the Markov Logic framework, the potential function of a clique has form O(x) = e&apos;&apos; if all member nodes are true (w denotes the weight of the inference rule), and O(x) = 1 otherwise. The probabilities of leaf nodes are derived from the underlying knowledge base,2 and inferred nodes are biased with an exponential prior. Finally, HOLMES computes the approximate probability of each answer by running a variant of loopy belief propagation (Pearl, 1988) over the Markov network. In our experience this method performs well on networks derived from our Horn clause proof forest, but one could use Monte Carlo techniques or even exact methods if desired. Note that this architecture allows HOLMES to combine information from multiple web pages to infer assertions not explicitly seen in the textual corpus. Because this inference is done using a Markov network, it correctly handles uncertain extractions and probabilistic dependencies. By using KBMC to create a custom, focused network for each query, the 2In our experiments, ground assertions from Word</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic reasoning in intelligent systems: networks ofplausible inference. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust textual inference via learning and abductive reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI</booktitle>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="32850" citStr="Raina et al., 2005" startWordPosition="5412" endWordPosition="5415">e HOLMES used to answer all queries in each domain scales with KB size. For these queries, and several others we tested (not shown here), inference time grows linearly with the size of the KB. Based on these results we believe that HOLMES can provide scalable inference over a wide variety of domains. 5 Related Work Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005). While many approaches have addressed this problem, our work is most closely related to that of (Raina et al., 2005; MacCartney and Manning, 2007; Tatu and Moldovan, 2006; Braz et al., 2005), which convert the inputs into logical forms and then attempt to ‘prove’ H from T plus a set of axioms. For instance, (Braz et al., 2005) represents T, H, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer lin86 ear program derived from that representation. These approaches and related ones (e.g., (Van Durme and Schubert, 2008)) use highly expressive representations, enabling them to express negation, temporal information, and more. HOLMES’s representation is mu</context>
</contexts>
<marker>Raina, Ng, Manning, 2005</marker>
<rawString>Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005. Robust textual inference via learning and abductive reasoning. In Proceedings of AAAI 2005. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov Logic Networks.</title>
<date>2006</date>
<booktitle>Machine Learning, 62(1-2):107–136. Parag Singla and</booktitle>
<contexts>
<context position="10635" citStr="Richardson and Domingos (2006)" startWordPosition="1685" endWordPosition="1688">generate a probabilistic graphical model, HOLMES seeks to find as many proof trees as possible for each query result — each may influence the final belief in that result. Figure 2 shows a partial proof tree for our example query. To handle uncertainty, HOLMES now constructs a ground Markov network from the proof trees and the Markov-logic-encoded inference rules. Markov netwhere the partition function Z is a normalizing term, and x{k1 denotes the state of all the variables in clique k. HOLMES converts the proof trees into a Markov network in a manner pioneered by the Markov Logic framework of Richardson and Domingos (2006). A Boolean variable is created to represent the truth of each assertion in the proof forest. Next, HOLMES adds edges to the Markov network to create a clique corresponding to each application of an inference rule in the proof forest. Following the Markov Logic framework, the potential function of a clique has form O(x) = e&apos;&apos; if all member nodes are true (w denotes the weight of the inference rule), and O(x) = 1 otherwise. The probabilities of leaf nodes are derived from the underlying knowledge base,2 and inferred nodes are biased with an exponential prior. Finally, HOLMES computes the approx</context>
<context position="35260" citStr="Richardson and Domingos, 2006" startWordPosition="5794" endWordPosition="5797">em to convert sentences into ground predicates, any limitations of the IE system will be propagated to HOLMES. Additionally, the logical representation HOLMES uses limits the reasoning and types of questions it can answer. HOLMES is geared towards answering questions which are naturally expressed as properties and relations of entities, and is not well suited to answering more abstract or open ended questions. Although we have demonstrated that HOLMES is scalable, further work is needed to make it to run at interactive speeds. Finally, research in statistical relational learning such as MLNs (Richardson and Domingos, 2006), RMNs (Taskar et al., 2002), and others (Getoor and Taskar, 2007) have studied techniques for combining logical and probabilistic inference. Our inference rules are more restrictive than those allowed in MLNs, but this trade-off allows us to efficiently scale inference to large, open domain corpora. By constructing only cliques for satisfied inference rules, HOLMES explicitly models the intuition behind LazySAT inference (Singla and Domingos, 2006) as used in MLNs. I.e., most Horn clause inference rules will be trivially satisfied since their antecedents will be false, so we only need to worr</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov Logic Networks. Machine Learning, 62(1-2):107–136. Parag Singla and Pedro Domingos. 2006. Memoryefficient inference in relational domains. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>P Abbeel</author>
<author>D Koller</author>
</authors>
<title>Discriminative probabilistic models for relational data.</title>
<date>2002</date>
<booktitle>Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI02).</booktitle>
<contexts>
<context position="35288" citStr="Taskar et al., 2002" startWordPosition="5799" endWordPosition="5802">edicates, any limitations of the IE system will be propagated to HOLMES. Additionally, the logical representation HOLMES uses limits the reasoning and types of questions it can answer. HOLMES is geared towards answering questions which are naturally expressed as properties and relations of entities, and is not well suited to answering more abstract or open ended questions. Although we have demonstrated that HOLMES is scalable, further work is needed to make it to run at interactive speeds. Finally, research in statistical relational learning such as MLNs (Richardson and Domingos, 2006), RMNs (Taskar et al., 2002), and others (Getoor and Taskar, 2007) have studied techniques for combining logical and probabilistic inference. Our inference rules are more restrictive than those allowed in MLNs, but this trade-off allows us to efficiently scale inference to large, open domain corpora. By constructing only cliques for satisfied inference rules, HOLMES explicitly models the intuition behind LazySAT inference (Singla and Domingos, 2006) as used in MLNs. I.e., most Horn clause inference rules will be trivially satisfied since their antecedents will be false, so we only need to worry about ones where the antec</context>
</contexts>
<marker>Taskar, Abbeel, Koller, 2002</marker>
<rawString>B. Taskar, P. Abbeel, and D. Koller. 2002. Discriminative probabilistic models for relational data. Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Dan Moldovan</author>
</authors>
<title>A logic-based semantic approach to recognizing textual entailment.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>819--826</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="32905" citStr="Tatu and Moldovan, 2006" startWordPosition="5420" endWordPosition="5423"> scales with KB size. For these queries, and several others we tested (not shown here), inference time grows linearly with the size of the KB. Based on these results we believe that HOLMES can provide scalable inference over a wide variety of domains. 5 Related Work Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005). While many approaches have addressed this problem, our work is most closely related to that of (Raina et al., 2005; MacCartney and Manning, 2007; Tatu and Moldovan, 2006; Braz et al., 2005), which convert the inputs into logical forms and then attempt to ‘prove’ H from T plus a set of axioms. For instance, (Braz et al., 2005) represents T, H, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer lin86 ear program derived from that representation. These approaches and related ones (e.g., (Van Durme and Schubert, 2008)) use highly expressive representations, enabling them to express negation, temporal information, and more. HOLMES’s representation is much simpler— Markov Logic Horn Clauses for inference rul</context>
</contexts>
<marker>Tatu, Moldovan, 2006</marker>
<rawString>Marta Tatu and Dan Moldovan. 2006. A logic-based semantic approach to recognizing textual entailment. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 819–826, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ullman</author>
</authors>
<title>Database and knowledge-base systems.</title>
<date>1989</date>
<publisher>Computer Science Press.</publisher>
<contexts>
<context position="13061" citStr="Ullman, 1989" startWordPosition="2070" endWordPosition="2071">k gets too large. As a result, HOLMES creates the network incrementally. After the first proof trees are generated, HOLMES creates the model and performs approximate probabilistic inference. If more time is available then HOLMES searches for additional proof trees and updates the network (Figure 1). This incremental process allows HOLMES to return initial results (with preliminary probability estimates) as soon as they are discovered. For efficiency, HOLMES exploits standard Datalog optimizations (e.g., it only expands proofs of recently added nodes and it uses an approximation to magic sets (Ullman, 1989), rather than simple backwards chaining). For tractability, we also allow the user to limit the number of transitive inference steps for any inference rule. HOLMES also includes a few enhancements for dealing with information extracted from natural language. For example, HOLMES’s inference rules support substring/regex matching of ground assertions, to accommodate simple variations in text. HOLMES also can be restricted to only operate over proper nouns, which is useful for queries involving named entities. 2.3 Markov Logic Inference Rules HOLMES is given the following set of six domainindepen</context>
<context position="17507" citStr="Ullman, 1989" startWordPosition="2794" endWordPosition="2795">st-order rules) is NP-complete. Alas, even low-order polynomial blowup is unacceptable when the textual corpus reaches Web scale; we seek linear growth. Intuitively, there are two places where polynomial expansion could cause trouble. First, the number of different types of proofs (i.e., first order proofs) could grow too quickly, and secondly, a given type of proof tree might apply to too many ground assertions (“tuples” in database lingo). We treat these issues in turn. Under our assumptions, each proof tree can be represented as an expression in relational algebra with at most m equijoins (Ullman, 1989),3 each stemming from the application of an inference rule. Since the number of rules is fixed, as is m, there are a constant number of possible first-order proof trees. The bigger concern is that any one of these firstorder trees might result in a polynomial number of ground trees; if so, the size of the ground forest (and corresponding Markov network) could grow too quickly. In fact, polynomial growth is a common phenomena in database query evaluation. Luckily, most relations in the Web corpus behave more favorably. We introduce a property of relations that ensures m-way joins, and therefore</context>
</contexts>
<marker>Ullman, 1989</marker>
<rawString>J. Ullman. 1989. Database and knowledge-base systems. Computer Science Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Van Durme</author>
<author>L K Schubert</author>
</authors>
<title>Open knowledge extraction through compositional language processing. In</title>
<date>2008</date>
<booktitle>Symposium on Semantics in Systems for Text Processing.</booktitle>
<marker>Van Durme, Schubert, 2008</marker>
<rawString>B. Van Durme and L.K. Schubert. 2008. Open knowledge extraction through compositional language processing. In Symposium on Semantics in Systems for Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wellman</author>
<author>J Breese</author>
<author>R Goldman</author>
</authors>
<title>From knowledge bases to decision models.</title>
<date>1992</date>
<journal>The Knowledge Engineering Review,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="3439" citStr="Wellman et al., 1992" startWordPosition="534" endWordPosition="537">S is a conjunctive query, a set of inference rules expressed as Horn clauses, and large sets of ground assertions extracted from the Web, WordNet, and other knowledge bases. As shown in Figure 1, HOLMES chains backward from the query, using the inference rules to construct a forest of proof trees from the ground assertions. This forest is converted Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 79–88, Honolulu, October 2008. c�2008 Association for Computational Linguistics into a Markov network (a form of KnowledgeBased Model Construction (KBMC) (Wellman et al., 1992)) and evaluated using approximate probabilistic inference. HOLMES operates in an anytime fashion — if desired it can keep iterating: searching for more proofs, and elaborating the Markov network. HOLMES makes some important simplifying assumptions. Specifically, we use simple ground tuples to represent extracted assertions (e.g., contains(kale, calcium)). Syntactic problems (e.g., anaphora, relative clauses) and semantic challenges (e.g., quantification, counterfactuals, temporal qualification) are delegated to the extraction system or simply ignored. This paper focuses on scalability for this</context>
<context position="6534" citStr="Wellman et al., 1992" startWordPosition="1034" endWordPosition="1037">helps prevent osteoporosis’, making it challenging to return “kale” as an answer. However, there are numerous web pages stating that “kale is high in calcium” and others declaring that “calcium helps prevent osteoporosis”. If we could combine those facts we could easily infer that “kale” is an answer to the question “What vegetables help prevent osteoporosis?” HOLMES was designed to make such inferences while accounting for uncertainty in the process. Given a query, expressed as a conjunctive Datalog rule, HOLMES generates a probabilistic model using knowledge-based model construction (KBMC) (Wellman et al., 1992). Specifically, HOLMES utilizes fast, logical inference to find the subset of ground assertions and inference rules that may influence the answers to the query — enabling the construction of a small and focused Markov network. Since this graphical model is much smaller than one incorporating all ground assertions, probabilistic inference will be much faster than if naive compilation were used. Figure 1 summarizes the operation of HOLMES. As with many theorem provers or KBMC systems, HOLMES takes three inputs: 1. A set of knowledge bases — databases of ground relational assertions, each with an</context>
</contexts>
<marker>Wellman, Breese, Goldman, 1992</marker>
<rawString>M. Wellman, J. Breese, and R. Goldman. 1992. From knowledge bases to decision models. The Knowledge Engineering Review, 7(1):35–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wu</author>
<author>D Weld</author>
</authors>
<title>Autonomously semantifying Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACM Sixteenth Conference on Information and Knowledge Management (CIKM-07),</booktitle>
<location>Lisbon, Porgugal.</location>
<contexts>
<context position="7250" citStr="Wu and Weld, 2007" startWordPosition="1150" endWordPosition="1153">d inference rules that may influence the answers to the query — enabling the construction of a small and focused Markov network. Since this graphical model is much smaller than one incorporating all ground assertions, probabilistic inference will be much faster than if naive compilation were used. Figure 1 summarizes the operation of HOLMES. As with many theorem provers or KBMC systems, HOLMES takes three inputs: 1. A set of knowledge bases — databases of ground relational assertions, each with an estimate of its probability, which can be generated by TextRunner (Banko et al., 2007) or Kylin (Wu and Weld, 2007). In our example, we would extract the assertions IsHighIn(kale, calcium) and Prevents(calcium, osteoporosis) from those sentences. 2. A domain theory – A set of probabilistic inference rules written as Markov logic Horn clauses, which can be used to derive new assertions. The weight associated with each clause specifies its reliability. 80 kale matches the query (Inferred : 0.91) broccoli matches the query (Inferred : 0.58) Query Result Query Result kale IS-A vegetable (WordNet : 0.9) kale helps prevent osteoporosis (Inferred : 0.88) broccoli IS-A vegetable (WordNet : 0.9) broccoli helps prev</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>F. Wu and D. Weld. 2007. Autonomously semantifying Wikipedia. In Proceedings of the ACM Sixteenth Conference on Information and Knowledge Management (CIKM-07), Lisbon, Porgugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yates</author>
<author>O Etzioni</author>
</authors>
<title>Unsupervised resolution of objects and relations on the Web. In Procs. of HLT.</title>
<date>2007</date>
<contexts>
<context position="28382" citStr="Yates and Etzioni, 2007" startWordPosition="4643" endWordPosition="4646">owever, inference can lead to errors when the proof tree contains joins on generic terms (e.g., “company”) or common extraction errors (e.g., “LLC” as a company name). This is a key area for future work. 4.3 Prevalence of APF Relations To determine the prevalence of APF relations in Web text, we examined a sample of 500 binary relations selected randomly from TEXTRUNNER’s ground assertions. The surface forms of the relations and arguments may misrepresent the true properties of the underlying concepts, so to better estimate the true properties we merged synonymous values as given by Resolver (Yates and Etzioni, 2007) or the most frequent sense of the word in WordNet. For example, we would consider BornIn(baby, hospital) and BornAt(infant, infirmary) to represent the same concept, and so would merge them into one instance of the ‘Born In’ relation. The largest two relations had over 1.25 million unique instances each, and 52% of the relations had more than 10,000 instances. For each relation R, we first found all instances of R extracted by TEXTRUNNER and merged all synonymous instances as described above. Then, for each argument of R we computed the smallest value, Kmin, such that R is APF with degree Kmi</context>
</contexts>
<marker>Yates, Etzioni, 2007</marker>
<rawString>A. Yates and O. Etzioni. 2007. Unsupervised resolution of objects and relations on the Web. In Procs. of HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>