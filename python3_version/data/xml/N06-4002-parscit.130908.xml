<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023599">
<title confidence="0.97578">
From Pipedreams to Products, and Promise!
</title>
<author confidence="0.983273">
Janet M. Baker Patri J. Pugliese
</author>
<affiliation confidence="0.954069">
Saras Institute / Dibner Institute Saras Institute / Dibner Institute
</affiliation>
<address confidence="0.952126666666667">
MIT – Bldg E56-100 MIT – Bldg E56-100
38 Memorial Drive 38 Memorial Drive
Cambridge, MA 02139 USA Cambridge, MA 02139 USA
</address>
<email confidence="0.997826">
HistSpch@mit.edu HistSpch@mit.edu
</email>
<sectionHeader confidence="0.995609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831363636364">
This demonstration provides a historical
perspective of a number of research and
commercial systems in Spoken Language
Technology over the past 20+ years. A
series of chronologically ordered video
clips from many sources will be pre-
sented to illustrate the many steps and the
tremendous progress that has been
achieved over the years. The clips them-
selves are drawn from diverse academic
and commercial research labs, product
presentations, and user applications. All
show systems being demonstrated or in
actual use. Over 20 different laboratory
systems, products, and companies are rep-
resented in this collection of video mate-
rials. Each of the clips has previously
been shown publicly. The present selec-
tion primarily focuses on speech and
natural language systems for speech rec-
ognition and synthesis. Additional contri-
butions to this collection are welcome.
</bodyText>
<sectionHeader confidence="0.885443" genericHeader="keywords">
1. Project Description
</sectionHeader>
<bodyText confidence="0.999663086956522">
Preparations of these materials are being done in
conjunction with the History of Speech and Lan-
guage Technology Project being conducted by
Saras Institute, in affiliation with the Dibner Insti-
tute for the History of Science and Technology, at
MIT (Cambridge, MA). The overall mission for
this project is to collect, preserve, and make readily
available information about significant research
discoveries, technical achievements, and business
developments in speech and language technology.
For further information on this project, please go to
www.SarasInstitute.org.
Work on this project is on-going. Additional
contributions of relevant materials are welcome in
the area of Spoken Language Technology, includ-
ing speech and natural language systems and ap-
plications incorporating speech recognition, speech
synthesis, interactive dialogue, information re-
trieval, machine translation, multimodal interfaces,
etc. Please contact us at HistSpch@mit.edu if you
have materials you would like to contribute or with
any inquiries, updates, corrections, and sugges-
tions.
</bodyText>
<sectionHeader confidence="0.998909" genericHeader="introduction">
2. Introduction
</sectionHeader>
<bodyText confidence="0.999545818181818">
This demonstration is intended to increase aware-
ness and understanding of the Spoken Language
Technology field, and an appreciation of the evolu-
tionary and revolutionary steps which have turned
pipedreams of the past into present products, and
future promise. The progression of many stages
from research and commercial laboratories into
working systems are well illustrated by this collec-
tion of video clips. Each video clip (typically 1-5
minutes duration) represents technology at the cut-
ting edge. There are 3 categories of video clips:
</bodyText>
<listItem confidence="0.845363">
1) Laboratory and Prototype Systems
</listItem>
<bodyText confidence="0.855384666666667">
In some cases, pioneers and major contributors of
the field are personally demonstrating their sys-
tems.
</bodyText>
<sectionHeader confidence="0.542773" genericHeader="method">
2) Commercial Product Demonstrations
</sectionHeader>
<bodyText confidence="0.980193333333333">
These run the gamut of televised inter-
views/demonstrations (including many live demos)
to instructional and commercial videos.
</bodyText>
<page confidence="0.925713">
257
</page>
<subsectionHeader confidence="0.75979">
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 257–260,
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.9667825">
Speak and Spell, model 2
(Owned by J. M. Baker)
</bodyText>
<sectionHeader confidence="0.622452" genericHeader="method">
3) User Applications
</sectionHeader>
<bodyText confidence="0.992438333333333">
Here users are demonstrating how they use their
systems on a routine basis in a variety of diverse
applications.
</bodyText>
<sectionHeader confidence="0.895662" genericHeader="method">
3. Hardware
</sectionHeader>
<bodyText confidence="0.999991695652174">
With the advent of ever more powerful inexpen-
sive silicon and the integration of computers with
audio interfaces, the computing platforms on
which spoken language technology resides have
undergone dramatic changes, progressively reach-
ing many more users, ever more conveniently.
Over the past 30+ years, we have witnessed the
transition from monolithic room-filling computers
to personal, even hand-held devices supporting
state-of-the-art speech technology. The type and
scope of applications have concomitantly multi-
plied with the ready access of affordable useful
technology. Like other initially expensive central-
ized hardware and even biological systems (!), as
the cost curves come down, evolution progresses,
and more processing can be conducted relatively or
wholly autonomously, the processing itself can be
far more distributed. So while there continue to be
operations or services (e.g. call centers) which are
still best done in a centralized fashion, the distribu-
tion and proliferation of stand-alone systems (e.g.
PCs, and cell phones) become progressively more
feasible and popular.
</bodyText>
<sectionHeader confidence="0.905334" genericHeader="method">
4. Speech Synthesis
</sectionHeader>
<bodyText confidence="0.999949173913043">
The primary focus of the present set of demonstra-
tions is on speech recognition and synthesis. Start-
ing with Homer Dudley&apos;s Voder (a manually-
controlled speech synthesizer) at the 1939 World&apos;s
Fair, audio examples of historical speech synthesis
approaches and techniques clearly demonstrate
extensive progress (see Resources). Video clips
illustrate users interacting with different types of
synthesis systems and applications.
Mechanical speaking machines in the 1700&apos;s
eventually gave way to electrical devices in the
early 1920&apos;s, which in turn gave way to computer
generation of speech by the 1960&apos;s. The invention
of the speech spectrogram in the 1940&apos;s spurred in-
depth speech research, and significantly facilitated
speech signal and waveform analyses, which blos-
somed in the 1960&apos;s.
Speech generation has taken two basic forms.
With analysis-
resynthesis the speech
waveform is first
parameterized and then
regenerated or played
back, as in the Voder.
With the advent of
powerful inexpensive
microprocessors and
DSP chips in the
1970&apos;s, a multitude of
diverse sound-
producing consumer
products hit the market.
The popular Speak &apos;n
Spell learning toy
introduced in 1978, was the most notable early en-
try.
Articulatory synthesis, first demonstrated in
1958, models the components and the characteris-
tics of the physical production system - the articu-
lators, their movements and their trajectories, as
well as the vocal tract, its resonances, excitations,
etc. A clear understanding of such a system is
highly desirable, and may eventually be achieved.
Unfortunately, the underlying complexity of the
speech production system still confounds under-
standing and application utility. Consequently,
while studies in articulatory synthesis are still on-
going, they were largely superceded by formant
synthesizers.
Formant synthesis, also referred to as synthesis-
by-rule, characterizes speech in terms of a source-
filter model. In this model, one or more sound
sources, representing the vibrating vocal cords and
noise dynamically produced at articulator constric-
tions, excite one or more filters, representing the
vocal tract and side-tube (e.g. nasal branch, etc.)
resonances. A catalog of sounds (corresponding to
(sub) phones, diphones, or other units) can be con-
structed and then reassembled (and smoothed) in
accordance with a dictionary of word or phrase
pronunciations. With careful selection of materials,
and the careful tuning and adjustments of parame-
ters, synthetic speech can be made to sound very
natural. Although computationally efficient, auto-
matically achieving high quality output is neither
easy nor consistently achievable.
Concatenative synthesis refers to the process of
sequentially combining prerecorded exemplars of
speech or other waveforms to produce the desired
</bodyText>
<page confidence="0.993581">
258
</page>
<bodyText confidence="0.999503384615385">
output. A large database (including many phonetic
elements, allophones, words, etc.) of well-
concatenated sound elements can easily produce
synthetic speech indistinguishable from natural
speech. This process is very memory-intensive, but
typically produces the highest quality speech syn-
thesis available. It is widely deployed in applica-
tions requiring natural-sounding speech output
from a given voice.
Although three major approaches are outlined
here, a number of hybrid synthesizers, HMM syn-
thesizers, and other synthesis methodologies are
utilized to address different requirements.
</bodyText>
<sectionHeader confidence="0.982959" genericHeader="method">
5. Speech Recognition
</sectionHeader>
<bodyText confidence="0.999980325581396">
In the past century, speech recognition has pro-
gressed from recognizing small vocabularies to
transcribing general purpose dictation in real time,
recognizing commands in noisy environments, and
reliably extracting words and information from
telephone conversations and television broadcasts.
In 1922, the toy dog &amp;quot;Rex&amp;quot;, would spring from his
doghouse when he
was called by name!
Early digit rec-
ognizers were
demonstrated in the
1950&apos;s and 1960&apos;s,
when the predomi-
nant approach was to
recognize whole
word templates. This
approach continued
up to the beginning
of the 1970&apos;s when it
started being
gradually replaced
by Hidden Markov
Model (HMM) sys-
tems using stochastic
models to more
accurately characterize the naturally highly vari-
able speech signal.
Spurred on by government funders for &amp;quot;speech
understanding&amp;quot; in Europe, Japan, and the USA,
many university and commercial laboratories
commenced to advance the technology. Proceeding
first through limited vocabulary systems and
highly constrained grammars, systems gradually
expanded the number of words they could simulta-
neously distinguish, despite greater variability in
speakers, languages, and progressively more chal-
lenging acoustic/channel and natural language en-
vironments. Starting in the 1970&apos;s, hefty special-
purpose commercial hardware systems were de-
ployed for limited vocabulary industrial applica-
tions (e.g. for hands-free command/control, data
entry, quality control, etc.), speaker verification,
simple telephone data input and query systems, etc.
Inexpensive PC sound board enabling discrete rec-
ognition started appearing in the market by the late
1970&apos;s to mid 1980&apos;s.
In the latter 1980&apos;s, vocabulary expanded to sev-
eral thousand words, and then in 1990, suddenly
exploded to full general-purpose dictation capabili-
ties, though still limited to discrete &amp;quot;one word at a
time&amp;quot; input. Meanwhile in the early 1990&apos;s, the
first speech audio-mining and audio information
retrieval capabilities were successfully proven to
work on prerecorded conversational continuous
speech, on telephone and broadcast data. Real-time
continuous speech dictation &amp;quot;software only&amp;quot; prod-
ucts became available and sold to millions of cus-
tomers by the end of the 1990&apos;s. The availability of
large corpora of recorded speech and text dramati-
cally improved modeling capabilities and system
performance for both dictation and transcription.
Meantime telephone query systems allowed for
users to engage in dialogue to get stock quotes,
weather updates, and even make train and plane
reservations. By the year 2000, commercial tele-
phone directory assistance systems started appear-
ing as well. Today call centers routinely employ
speech technology to elicit and supply customer
information through interactive spoken dialogue, in
large part replacing expensive human operators.
Though directed dialogues predominate, some
mixed initiative dialogues (&amp;quot;How can I help you?&amp;quot;)
are becoming available. Spoken language transla-
tion programs, coupling speech recognition to ma-
chine translation software, started appearing first in
the laboratory in the late 1990&apos;s, and then pro-
gressed to the marketplace on PCs and handheld
devices, by about 2000. Major government-funded
research initiatives are presently focusing on
speech-to-speech systems with different language
inputs and outputs.
With embedded systems, speech input and out-
put are now available on a growing number of
consumer products including automotive naviga-
tion systems, PDAs and toys; hands-free voice-
</bodyText>
<note confidence="0.729469333333333">
Radio Rex in his house
(Photo by Hy Murveit, Rex
owned by Michael Cohen)
</note>
<page confidence="0.997132">
259
</page>
<bodyText confidence="0.991476666666667">
dialing is shipping on millions of cell phones. De-
spite funding cuts and other setbacks about year
2000, the steady stream of ever improving speech
technology is gradually becoming an integral part
of systems and services, large and small. The is-
sues we face in improving speech technology con-
tinue to be very challenging. The retrospective
afforded by this demonstration reflects on the great
progress that we have made!
basic principals and history of speech recognition and
synthesis. From their Home page, click on &amp;quot;Training&amp;quot; to
get to the tutorial menu. http://www.eurovamp.com/
IEEE History Center has a website on Automatic
Speech Synthesis &amp; Recognition which includes inter-
views with seminal contributors (under &amp;quot;Archives&amp;quot;) to
speech technology an other relevant materials.
http://www.ieee.org/organizations/history_center/sloan/
ASSR/assr_index.html
</bodyText>
<sectionHeader confidence="0.874941" genericHeader="method">
6. Resources Saras Institute has a website for the History of Speech
</sectionHeader>
<reference confidence="0.945865666666667">
and Language Technology with information on histori-
cal artifacts, institutions, major contributors, resources,
etc. http://www.SarasInstitute.org
The Smithsonian Speech Synthesis History Project
http://www.aaai.org/AITopics/html/speech.html#readon
Janet M. Baker, &amp;quot;Milestones in Speech Technology –
Past and Future!&amp;quot;, Speech Technology Magazine, Sep-
tember/October 2005. http://www.mindspring.com/~ssshp/ssshp_cd/ss_home.
htm
</reference>
<bodyText confidence="0.9927485">
The American Association for Artificial Intelligence
(AAAI), founded in 1979, includes on their website an
variety of materials on the history of speech technology.
(SSSHP) provides a collection of tape recordings, tech-
nical records and artifacts of speech synthesis technol-
ogy from 1922 to the mid-1980s.
The web site &amp;quot;comp.speech Frequently Asked Ques-
tions&amp;quot; provides a myriad of links to sites dealing with
the various aspects of speech technology.
http://www.speech.cs.cmu.edu/comp.speech/
</bodyText>
<reference confidence="0.7132379">
Eurovamp (Voice Adapted Multipurpose Peripherals)
maintains a web site which includes tutorials on the
Special Workshop in Maui (SWIM): Lectures by Mas-
ters in Speech Processing, January 11-14, 2004, in-
cluded a series of papers by senior researchers on
various aspects of the history of speech technology. A
&amp;quot;Program Guide&amp;quot; with summaries of these &amp;quot;Peer Lec-
tures&amp;quot; is available at:
http://dspincars.sdsu.edu/swim/WorkshopGuide.pdf
&amp;quot;The Typewriter one hundred years hence&amp;quot;: cartoon from The Illustrated Phonographic World (June, 1984).
</reference>
<page confidence="0.98976">
260
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.313264">
<title confidence="0.806909">From Pipedreams to Products, and Promise!</title>
<author confidence="0.999857">Janet M Baker Patri J Pugliese</author>
<affiliation confidence="0.99886">Saras Institute / Dibner Institute Saras Institute / Dibner Institute</affiliation>
<address confidence="0.822365333333333">MIT – Bldg E56-100 MIT – Bldg E56-100 38 Memorial Drive 38 Memorial Drive Cambridge, MA 02139 USA Cambridge, MA 02139 USA</address>
<email confidence="0.924136">HistSpch@mit.eduHistSpch@mit.edu</email>
<abstract confidence="0.996109808510638">This demonstration provides a historical perspective of a number of research and commercial systems in Spoken Language Technology over the past 20+ years. A series of chronologically ordered video clips from many sources will be presented to illustrate the many steps and the tremendous progress that has been achieved over the years. The clips themselves are drawn from diverse academic and commercial research labs, product presentations, and user applications. All show systems being demonstrated or in actual use. Over 20 different laboratory systems, products, and companies are represented in this collection of video materials. Each of the clips has previously been shown publicly. The present selection primarily focuses on speech and natural language systems for speech recognition and synthesis. Additional contributions to this collection are welcome. 1. Project Description Preparations of these materials are being done in conjunction with the History of Speech and Language Technology Project being conducted by Saras Institute, in affiliation with the Dibner Institute for the History of Science and Technology, at MIT (Cambridge, MA). The overall mission for this project is to collect, preserve, and make readily available information about significant research discoveries, technical achievements, and business developments in speech and language technology. For further information on this project, please go to www.SarasInstitute.org. Work on this project is on-going. Additional contributions of relevant materials are welcome in the area of Spoken Language Technology, including speech and natural language systems and applications incorporating speech recognition, speech synthesis, interactive dialogue, information retrieval, machine translation, multimodal interfaces, etc. Please contact us at HistSpch@mit.edu if you have materials you would like to contribute or with any inquiries, updates, corrections, and suggestions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Language</author>
</authors>
<title>Technology with information on historical artifacts, institutions, major contributors, resources,</title>
<note>etc. http://www.SarasInstitute.org</note>
<marker>Language, </marker>
<rawString>and Language Technology with information on historical artifacts, institutions, major contributors, resources, etc. http://www.SarasInstitute.org</rawString>
</citation>
<citation valid="false">
<title>The Smithsonian Speech Synthesis History Project http://www.aaai.org/AITopics/html/speech.html#readon</title>
<marker></marker>
<rawString>The Smithsonian Speech Synthesis History Project http://www.aaai.org/AITopics/html/speech.html#readon</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet M Baker</author>
</authors>
<title>Milestones in Speech Technology – Past and Future!&amp;quot;, Speech Technology Magazine, September/October</title>
<date>2005</date>
<note>http://www.mindspring.com/~ssshp/ssshp_cd/ss_home. htm</note>
<marker>Baker, 2005</marker>
<rawString>Janet M. Baker, &amp;quot;Milestones in Speech Technology – Past and Future!&amp;quot;, Speech Technology Magazine, September/October 2005. http://www.mindspring.com/~ssshp/ssshp_cd/ss_home. htm</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eurovamp</author>
</authors>
<title>Adapted Multipurpose Peripherals) maintains a web site which includes tutorials on the Special Workshop</title>
<date>2004</date>
<booktitle>in Maui (SWIM): Lectures by Masters in Speech Processing,</booktitle>
<marker>Eurovamp, 2004</marker>
<rawString>Eurovamp (Voice Adapted Multipurpose Peripherals) maintains a web site which includes tutorials on the Special Workshop in Maui (SWIM): Lectures by Masters in Speech Processing, January 11-14, 2004, included a series of papers by senior researchers on various aspects of the history of speech technology. A &amp;quot;Program Guide&amp;quot; with summaries of these &amp;quot;Peer Lectures&amp;quot; is available at: http://dspincars.sdsu.edu/swim/WorkshopGuide.pdf</rawString>
</citation>
<citation valid="true">
<title>The Typewriter one hundred years hence&amp;quot;: cartoon from The Illustrated Phonographic World</title>
<date>1984</date>
<marker>1984</marker>
<rawString>&amp;quot;The Typewriter one hundred years hence&amp;quot;: cartoon from The Illustrated Phonographic World (June, 1984).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>