<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.899876">
American hunk&apos; of Computational Linguistics Microfiche 8
</note>
<sectionHeader confidence="0.945992" genericHeader="method">
STRING TRANSFORMATIONS
IN THE
REQUEST SYSTEM
</sectionHeader>
<author confidence="0.447224">
Warren J. Plath
</author>
<sectionHeader confidence="0.704879" genericHeader="method">
IBM Thomas J. Watson Research Center
Yorktown Heights
</sectionHeader>
<bodyText confidence="0.896107">
copyright 1974 by the Assocjation for Computational Linguistics
</bodyText>
<page confidence="0.48893">
2
</page>
<sectionHeader confidence="0.810428" genericHeader="method">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.997038653846154">
The REQUEST System is an experimental natural language query
system based on large transformational grammar of English. In
the original implementation of the system the process of computing
the underlying sLructures of input queries involved a sequence of
three steps: (1) preprocessing (including dictionary lookup),
(2) surface phrase structure parsing, and (3) transformational
parsing. This scheme has since been modified to permit transfor-
mational operations not only on the full trees available after com-
pletion of surface parsing, but also on the strings of lexical
trees which are the output of the preprocessing phase. Transfor-
mational rules of this latter type which are invoded prior to sur-
face parsing, are known as string transformations.
Since they must be defined in the absence of such structural
markers as the location of clause boundaries, string transforma-
tions are necessarily relatively local in scope. Despite this in-
herent limitation, they have so far proved to be an extremely use-
ful and surprisingly versatile addition to the REQUEST System.
Applications to date have included homograph resolution, analysis
of classifier constructions, idiom handling, and the suppression of
large numbers of unwanted surface parses. While by no means a
panacea for transformational parsing, the use of string transfor-
mations in REQUEST has permitted relatively rapid and painless ex-
tension of the English subset in a number of important areas with-
out corresponding adverse impact on the size of the lexicon, the
complexity of the surface grammar, and the number of surface parses
produced.
</bodyText>
<sectionHeader confidence="0.679568" genericHeader="method">
5
TABLE OF CONTENTS
</sectionHeader>
<keyword confidence="0.497572">
Page
</keyword>
<sectionHeader confidence="0.977593" genericHeader="method">
1. Introduction 4
2. REQUEST System Organization 6
3. Motivation ror the Introduction of String Transformations 11
</sectionHeader>
<subsectionHeader confidence="0.876571">
3.1 Some Relevant Design Principles 12
3.2 Early Experience with the Parser 16
</subsectionHeader>
<sectionHeader confidence="0.8352543" genericHeader="method">
3. 3 Problems of Qrowth of Coverage 20
4 The Use of String Transformations in the RFQUEST System 22
4.1 Classifier ConstructlUllb 23
4.2 Stranded Prepositions 31
4. 3 Horhograpb Resolution 34
4. 4 Idiom Processing 38
4. 5 Experiments in Limited Conjunction Processing 43
5. Summary and Conclusions 53
Appendix: Listing of String Transformations 56
References
</sectionHeader>
<page confidence="0.955333">
81
</page>
<bodyText confidence="0.477882">
String Transformations in the REQUEST System
</bodyText>
<sectionHeader confidence="0.828459" genericHeader="conclusions">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999568210526316">
The REQUEST (Restricted English Question-answering) System [1, L]
is an experimental natural language que.ry system which is being,developed
at the IBM Thomas J. Watson. Research Center. The system includes a
large transformational grammar, a transformational parser, and a Knuth-
style semantic interpreter. The grammar and its associated lexicon are
broadly oriented towards question-answering on periodic numerical data,
they also include material specific to natural English interaction with col-
lectioms of business statistics, as exemplified by the Fortune 500
The long-range objective of the work on REQUEST is to determine the
extent to which maChineâ€”undeestandable subsets of English can be developed
to provide non-programmers with a convenient and powerful tool for access-
ing information in formatted data bases without having to learn a formal
query language. In the interest of facilitating effective &amp;quot;understanding&amp;quot; on
the part of the system, the semantic scope of the English subset we are
currently dealing with is largely restricted to the world of business statis-
tics. Within that narrow domain of discourse, however, we are attempting
to cover a relatively broad fange of syntactic and lexical alternatives, in
the hope of permitting future users to employ their normal patterns of
written expression without major adjustment. The current REQUEST
</bodyText>
<page confidence="0.824831">
4
</page>
<bodyText confidence="0.9988548">
grammar covers a variety of basic English constructions in some depth,
including wh- and yes-no questions, relative clauses and clausal negation
It is now being extended into such areas as comparison, conjunction and
quantification which, while complex, appear to be of central importance
in providing a semantically powerful subset of English.
</bodyText>
<note confidence="0.539777">
S
</note>
<sectionHeader confidence="0.128144" genericHeader="acknowledgments">
2. REQUES1 System Organization
</sectionHeader>
<bodyText confidence="0.9983855">
The REQUEST System consists of a set of prografns written in LISP
1.5 together with an associated set of data files containing the lexicon,
grammar, semantic interpretation rules and data base. The system runs
interactively on a System/370 Model 158 under VM/370 in 768k bytes of
virtual core. As can be observed from Figure 1, the system containg two
major components, one transformational, the other interpretive.
The transformational component, which serves to analyze input wuiu
strings and contpute their underlying structures, consists of two main
parts: a preprocessor and a parser. The interpretive component also
,
has two major subcomponents: (i) a semantic interpreter, which trans-
lates each underlying structure into a logical form, i.e., a formal ex-
pression specifying the configuration of executable functions required to
access the data base and compute the answer to the corresponding question
</bodyText>
<sectionHeader confidence="0.688501" genericHeader="references">
4= I.
</sectionHeader>
<bodyText confidence="0.951391846153846">
and (ii) a retrieval component which contains the various data-accessing
testing, and output formatting functions needed ta evaluate the logical form
and complete the question-answering process.
Looking at the transformational component in somewhat greater de-
tail, the role of the preprocessor is to partition the input string into words
*
Implementation of the semantic interpreter, which operates according to
a scheme originally proposed by D. E. Knuth [.3], is clue to S. R. Petrick
[1, 4, 5], who has also devised the specific semantic interpretation rules
employed in REQUEST.
**
F. J. Dame rau is responsible for the design and implementation of the
current retrieval component.
</bodyText>
<figure confidence="0.995313424657534">
6
trsER )
7
Input Word
String
.00
00&apos;
r
1
1
I
1
1
1
1
1 TRANSFOR
MATIONAL
I COMPONENT
I
I
1
1
1
1
I
I
I
I
1
I
I
I
I
I
INTERPRETIVE
I COMPONENT
I
I
I
I
I
I
L
T
.00
PREPROCESSOR
SEMANTIC
IN.TERPRETER
RETRIEVAL
PARSER
_ I Out nit
I
I
J
1
Preprocessed
String
\
Executable \
Code \
(Logical FOrm)
Un.Elerlying
Structure (s)
ST RING
TRANSFOR-
TIONS
H
=NM
INVERSE
TRANSPOR,-
MAXIONAL
GRAMMAR
MOMS AMMO .4414414 mamma ANN/44RO AMMO 444/41/ 4=44 MIMI. /NNW.
</figure>
<figureCaption confidence="0.999098">
Figure 1 Overall System Organization
</figureCaption>
<tableCaption confidence="0.694076818181818">
and punctuation marks and then look up each segment in the lexicon, yield
ing a preprocessed string of lexical. trees which serves as input to the
parser. Multi-word strings that function as lexical units are identified by
a &amp;quot;longest match&amp;quot; lookup in a special phrase lexicon; while the lexical
trees corresponding to arabic numerals (which may variously represent
cardinals, ordinals, or year names) are supplied algorithmically rather
than by matching against the lexicon. In cases where there are gaps in
the preprocessed string, due to the presence in the input of misspellings,
unknown words, ambiguous pronoun references, and the like, the pre pro
cessor prompts the user to supply the required information.
*
</tableCaption>
<subsectionHeader confidence="0.899014">
Operation of the transformational parser proceeds in three stages:
</subsectionHeader>
<bodyText confidence="0.918653428571429">
(1) The preprocessed string is successively analyzed with
respect to thc structural description of each rule in a
linearly ordered list of string transformations. Each
successful match against a string transformation leads
to modification of one or more of the trees in the pre-
processed string through application of the operations
specified in the structural change of the rale in ques-
tion -- operations which are drawn from precisely the
*
The original design and implementation of the parser are due to Petrick
[6]. The version currently being used in REQUEST is the result of signifi-
cant revisions and extensions by M. Pivovonsky, who (with the aid of
E. 0. Lippmann) has also been chiefly responsible for implementing the
preprocessor.
</bodyText>
<page confidence="0.807375">
8
</page>
<bodyText confidence="0.993005666666667">
same inventory of elementary transformations that the
system makes available for the processing of full trees
by conventional cyclic and postcyclic transformations,
namely: deletion, replacement of a tree by a list of
trees. Chomsky adjunction, feature insertion, and fea-
ture deletion. (A more detailed account of the nature
of string transformations and the motivation far their
use in a transformational parser will be presented in
the remaining sections of the paper. )
(2) Upon completion of the string transformation phase,
the resulting transformed preprocessed string--
still in the form of a list of trees -- is passed to a
context-free parser in order to compute the surface
structure (s) of the sentence. (Although one major
effect of the employment of string transformations has
been a substantial reduction in the number of unwanted
surface parses, cases still occur with some frequency
where more than one surface parse is produced. )
</bodyText>
<listItem confidence="0.57735825">
(3) Finally, the transformational parser processes each
surface structure in turn, attempting to map it step
by step into a corresponding underlying structure
9
</listItem>
<bodyText confidence="0.99812825">
according to the rules of a transformational grammar.
In this process transformational inverses are applied
in an order precIsely, opposite to that in which their
&amp;quot;forward&amp;quot; counterparts would oc., invoked ifi sentence
generation: inverses of the postcyc lie transformations
are applied first, starting with the &amp;quot;latest&amp;quot; and ending
with the &apos;earliest&amp;quot;; then the inverses of the cyclic
transformations are applied (also in last-to-first order)
working down the tree from the main clause to those
that are most deeply embedded.
To help ensure validity of its final output, the parser checks each
intermediate output produced by successful application of an inverse bians-
formation to determine whether or not its constituent structure conforms
fully with the set of branching patterns that can be generated by the cur-
rent grammar in underlying or intermediate structures. At the end of
each inverse cycle, a similar check is performed to determine whether
all structure above the next (lower) level of embedded S s is consistent
with the inventory of allowable underlying structure patterns alone. Fail-
ure of either test results in immediate abandonment of the current analysis
path. (Aa described in [21, other, more stringent te.sts involving the
</bodyText>
<page confidence="0.8235">
10
</page>
<bodyText confidence="0.980739333333333">
application of corresponding forward transformations can optionally be
invoked in ordc r to provide a more definitive validation of inverse trans-
formational derivations.
3. Motivation for the Introduction of String Transformations
Within the series of major processing steps described in the preceding
section, the application of string transformations occurs at a point midway
between preprocessing (including lexical lookup) and surface phrase struc-
ture parsing. Taken in sequence, these three steps have the cumulative
effect of shifting the locus of analysis operations from the domain of word
strings to that of full sentence trees, where conventional transformations
(and their inverses) can meaningfully be invoked. Unlike the balance of
the transformational parsing process, these three preliminary steps do not
seem to bear a, direct correspondence to familiar generative operations.
Nevertheless, their combined effect is to produce the tree or trees which
exist at that stage of the &amp;quot;forward&amp;quot; generation where the last postcyclic
transformation has applied. Accordingly, it seems reasonable to view
them initially as constituting a kind of &amp;quot;bootstrap&amp;quot; whose function is to set
the stage for &amp;quot;true&amp;quot; transformational parsing.
Prior to the introduction of string transformations in the REQUEST
System, the entire burden of the &amp;quot;bootstrap&amp;quot;. role just outlined necessarily
fell on the preprocessor and the surface parser. Moreover, as will be
</bodyText>
<page confidence="0.916213">
11
</page>
<bodyText confidence="0.9994185">
explained below, certain basic principles concerning the nature of the
system&apos;s transformational component -- relating to the range of inputs
to be accepted and the criteria for satisfactory outputs -- had the effect of
ensuring that the burden would be a large one. The full dimensions of the
situation began to emerge once extensive testing of the first sizeable trans
formational grammar was underway. There followed a series of correc-
tive actions, the last and most far-reaching of which was the introduction
of string transformations.
</bodyText>
<subsectionHeader confidence="0.989719">
3.1 Some Relevant Design Principles
</subsectionHeader>
<bodyText confidence="0.999955916666667">
In the early design phases of what subsequently became the REQUEST
System&apos;s transformational grammar, it was decided to adopt a level of
underlying structure considerably more abstract than the deep structhre
of Chomsky&apos;s Aspects [7] a level which, somewhat in the spirit of gen-
erative semantics [B, 9], would go a long way towards direct representa-
tion of the meanings of sentences. Eschewing irrelevant details, the essen-
tials of the representation adopted (which bears certain strong resem-
blan.ces to the predicate calculus) are as follows: Each underlying struc-
ture tree represents a proposition (category Si) consisting of an underlying
predicate (V) and its associated arguments (NP&apos;s) in,that order. Argument
slots are filled either by embedded propositions (complement Sl&apos;&apos;s) or by
nominal expressions (NNW&apos;s). A nominal expression direcily dominates
</bodyText>
<page confidence="0.668018">
12
</page>
<bodyText confidence="0.99910047826087">
either a NOUN, or a NOM and an Si (the relative clause construction).
Each NOUN dominates an INDEX node which is specified as a constant
(+ CONST) in the case of proper nouns and as a variable (- CONST) other
wise. The INDEXes and the terminal nodes they dominate play an impor
tant role in the grammar, including the representation of coreference .
One major impact which this view of underlying structure had on what
the &amp;quot;bootstrap&amp;quot; had to accomplish involved the connection of prepositional
phrases to the balance of the surface structure tree. In underlying struc-
ture, the noun phrase corresponding to each surface prepositional phrase
would appear as a specific argument in a specific proposition, following.
the application of the generative transformations whose inverses the parser
would employ, the resulting prepositional phrase would in most cases still
be explicitly linked to the clause or clausal remnant derived from that un-
derlying proposition_ Thus, in order to make possible a correct inverse
transformational derivation, the surface parser would have to make all
stich linkages explicit, This requirement represented a signifizant depar-
ture from earlier practice in a number of phrase structure parsing systems,
notably those employing predictive analysis [10, 111, where the problem
of conneeting prepositional phrases to the correct level of structure was
simply ducked by making an arbitrary linkage to the nearest available
*
Much of the early work on the grammar, in particular the svatem of
variables and constants, reflects suggestions by Paul Postal.
</bodyText>
<page confidence="0.588858">
13
14
</page>
<bodyText confidence="0.979228347826087">
candidate, thereby avoiding what would inevitably have been a large in-
crease in the number of unwanted analyses. (A similar approach has re-
cently been followed in the ATN parser of Woods Lunar Sciences Naturat
Language Information System [12], but there the semantic interpreter is
Made to pick up the slack. )
A second design principle which had a major impact on the mech,anisTris
for computing surface structures from input strings was the already-men-
tioned goal of providing broad coverage of syntactic alternatives to promote
ease of use. (As should be fairly obvious, expansion of grammatical
coverage -- even in a restricted domain of discourse -- in general entails
not only an increase in the size and complexity of lexicon and surface gram-
mar but als9 an increase in the potential for lexical and syntactic ambi-
guity. )
Two classes of syntactic alternatives whose coverage at the surface
syntax level led to specific problems,ultimately resolved by the use of
string transformations were stranded prepositions and classifier construc-
tions. In both cases the problems stemmed from the introduCtion of new
possibilitieÂ§ for incorrectly connecting a preposition or prepositional
phrase to the balance of the surface structure. Strand-ed prepositions
occur with some frequency in wh-questions and relative clauses in English
often yielding results whose naturalness compares favorably with that, of
the corresponding non-strande,c1 versions, as in (1) (3) below. Because
3.f these circumstances, we felt obliged to provide for such constructions
</bodyText>
<listItem confidence="0.920897571428571">
(1) What companies did XYZ sell oil to?
To what companies did XYZ. sell oil?
(2.) a.. What was the city which ABC&apos;s headquarters was located in
in 1969?
b. What was the_city in which ABC&apos;s headquarters .was located in
1 969?
(S) What company was Mr. Jones the president oii in 1971
</listItem>
<bodyText confidence="0.9353312">
b. ? Of what company was Mr: Jones the presi-dent in 19722
even in early versions of our grammar. The case for including classifier
constructions, in which proper nouns are optionally accompanied by a com-
mon noun designating their semantic class (cf. the (a) versi&apos;ons of (4)- (7)),
did not seem quite as cpmpelling as that for stranded prepositions, since
</bodyText>
<figure confidence="0.487858230769231">
the City of Sheboygan of Massachuseti
Sheboygan
(State
the
Commonwealth
Massachusetts Co
(the) Tentacle
[mpany
Corporation /
Tentacle
the yea lâ€¢ (of) 1965
1965
15
</figure>
<bodyText confidence="0.984390071428572">
the versions with classifiers have a formai, slightly pedantic quality that
is absent from their classifier-less counterparts. Nevertheless, there
appeared to be no reasonable grounds (such as obscurity, doubtful gram-
maticality, and the like) for excluding them from the subset.
A third factor affecting the performance of the &amp;quot;bootstrap&amp;quot; was the
conscious decision to try to get along initially with a s&apos;urfac.e parser which
would be maximally simple with respect to both its computational mechan-
ism and its surface phrase structure grammar. In particular, this meant
employment of a context-free parser without eith.er the complications or
the benefits of sensitivity to syntactic and semantic features [11, 13]. The
hope was that any additional surface parse-s which resulted from this ap-
proach would be effectively filtered out during transformational parsing
by the various well-formedness checks on invetse derivations discussed at
the end of Section 2.
</bodyText>
<subsectionHeader confidence="0.995319">
3.2 Early Experience with the Parser
</subsectionHeader>
<bodyText confidence="0.9994026">
Starting in late 1971, tests began on an inverse transformational
grammar whose generative counterpart had been developed with the aid of
Joyce Friedman&apos;s transformational grammar tester [14] . In the interest
of debugging the system with as few unnecessary complications as possible,
the initial examples were &amp;quot;spoon fed&amp;quot; to the parser using a minimal lexicon
</bodyText>
<page confidence="0.4390305">
16
17
</page>
<bodyText confidence="0.998201756097561">
and surface grammar. While revealing no critical problems with the boot-
strap, these first trial runs indicated that incorrect surface structures
were indeed produced along with the correct ones and tended to give rise
to analysis paths which continued for some time before being aborted by
well-formedness tests. Sentences with ambiguous verb forms were a case
in point. Thus, in the question &amp;quot;What companies are computers made by?&amp;quot;
the surface parser produced two almost identical structures -- the first
with &amp;quot;made&amp;quot; taken asâ€¢ a finite verb in the past tense, the second with it
taken (correctly) as a past participle. The first analysis initiated a lengthy
inverse derivation that was terminated as ill-formed only after the entire
postc,ycle and the first inverse cycle had been traversed, meaning that
nearly as much time was spent in pursuing this incorrect path as was re-
quired to follow the correct one. In this and a number of similar cases,
however, it was observed that ill-formedness of the surface structure
could have readily been detected at or near the outset of the transformaâ€”
tional parsing process by performing tests employing the pattern-matchin
power of transformational rules. This observation led to the introduction
of so-called blocking rules in the transformational grammar, rules which
proved to be quite effective in detecting and filtering out ill-formed struc-
tures such as the incompatible auxiliary/finite verb combination in the
example just considered.
In the spring of 19721 the surface grammar was greatly expanded in
an attempt to cover the full range of structures that could be produced by
the set of transformational rules then in use. At that point, the combined
effect of the various design decisions affecting surface structures and sur-
face parsing became immediately and painfully evident in the form of a
combinatorial explosion: The brief and apparently innocuous question (8)
(8) &amp;quot;Is the headquarters of XYZ in Buffalo?&amp;quot;
produced no less than 1.9 surface parses, a figure which soared to 147
when a third prepositional phrase was introduced by replacing &amp;quot;Buffalo&amp;quot;
with the classifier construction &amp;quot;the city of Buffalo&amp;quot; Although a blocking
rule for detecting spurious stranded prepositions rather quicikly killed off
16 of the 19 analyses in the former case, thereby reducing the analysis
problem to tractable size, the system was unable to cope with the latter
situation at all, due to problems of storage overflow.
Thoughts of what would inevitably happen if we added yet another prep
ositional phrase (as in &amp;quot;Was the headquarters of XYZ in the city of Buffalo
in 1971? &amp;quot;) made it clear that killing off unwanted surface parses after the
fact by means of blocking rules was not enough, measures would have to
be adopted which would prevent formation of most such analyses in the
first place. Two corrective steps were taken almost immediatetV:
</bodyText>
<listItem confidence="0.5906325">
(a) coverage of classifier constructions was temporarily dropped, and
(b) it was decided to explore what could be done towards elimination of
</listItem>
<page confidence="0.466463">
18
</page>
<bodyText confidence="0.979352074074074">
4-9.
spurious surface parses through selective refinement of category distinc-
tions in the surface grammar.
In the latter area, it was discovered (not surprisingly) that differences
in the surface structure distribution of prepositional phrases, genitive
noun phrases, and other types of noun phrases could be effectively ex-
ploited to suppress incorrect parses, as could distributional contrasts be-
tween proper nouns ahd common nouns, finite verbs and participles, etc.
(In the case of (8) above, 13 of the original 19 parses were ruled out on the
ground that proper nouns cannot take modifiers, while 3 more analyses
(plus 4 of the 13 already eliminated) were excluded on the basis of distribu
tional distinctions between prepositional phrases and other noun phrases. )
Implementation of the refinements in the surface grammar required
mxrnerovis part-of-speech code changes in the lexicon and a substantial in-
crease in the number of rules in the surface grammar. Beyond this, the
central problem was that the transformational grammar defines a specific
cbass of surface structure-s -- e&apos;rnploying only elements from a fixed set of
intermediate symbols -- as the parses which must be found. In order to
meet this requirement, the by now considerably expancleci set of inter-
mediate symbols employed in the surface grammar had to be mapped onto
the smaller set compatible with the transformations. Thus, for example,
PP (prepositional phrase) and NPG (genitive noun phrase) nodes in each sur-
face structure would be replaced by NP nodes before transformational
parsing began -- fortunately an extremely simple and rapid operation. (In
the most recent version of REQUEST, the surface grammar employs a
total of 32 temporary nocl names for this purpose; they are subsequently
mapped onto a s4 of only 9 nodes for purposes of transformational parsing.
</bodyText>
<subsectionHeader confidence="0.637843">
3.3 Problems of Growth of Coverage
</subsectionHeader>
<bodyText confidence="0.999779625">
The various measures just described had the effect of stabilizing the
incidence of artificial surface structure ambiguities at a tolerably low
level for a period of about a year, during which the transformational
grammar roughly doubled in size from about 35 rules to over 70 as cover-
age was extended to include such structures as numerical quantifiers, time
compounds, and various expressions involving rank ahd ordinality. The
principal costs of ambiguity suppression were feat not in the analysis pro-.
grams, which required only negligible modification for that purpose, but
rather in the surface grammar, which grew much larger and more complex
to the point where it became rather difficult to work with. Since a number
of additional extensions of grammatical coverage were under active con-
sideration -- among them the restoration of classifier constructiohs to the
subset -- it seemed desirable to seek out some neiw approach to ambiguity
suppression which&apos; would not further overburden the surface grammar.
The alternatives originally considered were uniformly unattractive.
In he case of the classifier constructions, one could have achieved the
</bodyText>
<figure confidence="0.3572205">
20
21
</figure>
<bodyText confidence="0.888838487804878">
immediate objective by simply loading up the phrase lexicon with an entry
for each legitimate pairing of a classifier with a proper noun, thereby
achieving a minor gain in grammatical coverage at the price, of more than
doubling the size of the lexicon. Another approach would have involved
creating phrasal entries only for the classifiers themselves -- e. g.,
&amp;quot;the city of&amp;quot;, the state of&amp;quot;, etc. -- leaving it to special ad hoc routine .8 at
the end of the preprocessor first to check the preprocessed string for the
presence of immediately following proper nouns of the corresponding seman-
tic class and then to effect the appropriate amalgamations or deletions.
The second alterftative was quickly rejected as even more distasteful
than the first, since despite its relatively small initial :ost, it would, if
used abt all extensively, have meant abandonment of an otherwise orderly
and perspicuous analysis procedure. This train of thought, however,
eventually led to the idea of modifying the preprocessed string not by ad
hoc subroutines requiring accretions to the program, but by means of
locally defined transformational rules employing the same computational
apparatus and notational conventions as the existing forward and inverse
transformations. Within a week of its conception, the idea of a string
transformation facility became a reality through some minor modifications
to the flow of control of the parser.
;1&apos;Much of the ease of this transition stemmed from the generality of the
original properanalysis mechanism, which was designed to accept a
list of trees, rather than a single tree, as it input.
4. The Use of String Transformations in the REQUEST System
Because they apply to strings of unconnected* lexical trees, rather
than to full surface trees with their representation of the structure of
phrases and clauses, string transformations tend to be relatively local in
scope, typically being restricted to constructions with contiguous elements.
Despite dlis inherent limitation, such rules rapidly found a wide variety of
uses within the REQUEST System: Classifier constructions were readily
identified ..nd transformed into classifierless counterparts by a handful of
string transformations. Rules were also written for suppressing incorrect
stranded prepositions, resolving homography, and translating certain
idioms into a form more manageable for the surface parser. Finally, ex
periments were undertaken to explore the possibility of employing string
transformations to deal with a limited but potentially useful range of con-
junction constructions.
A common thread running through several of these apparently diverse
applications of string transformations is the application of what would
otherwise have been treated as the inverse of a late postcyclic transform.a-
tion at a point preceding surface structure parsing in order to achieve a
</bodyText>
<tableCaption confidence="0.859552166666667">
* At least initially. Some string transformations currently in use produce
what are in effect partial surface structures as output. In fact, it is
quite possible that an appropriately chosen cyclically ordered set of
string transformations could supi5lant the surface grammar entirely, how-
ever, such a development appears unattractive at this time due to ef-
ficiency considerations.
</tableCaption>
<page confidence="0.713369">
22
</page>
<bodyText confidence="0.975466333333333">
simplification of the surface grammar, a reduction in the number of
spurious surface parses,, or both. (The benefits of such a reordering stem
in large part from the fact that derived constituent&apos; structure patterns pro-
vided for at the string transformation level need not be dealt with in the
surface grammar, thereby reducing its size, its scope, and its potential
for producing incorrect surfq.ce parses.) In the case of classifier construc
tions (Section 4.1) and of certain idioms involving notions of rank (Sec-
tion 4. 4), existing postcyclic transformations were actually replaced by
string transformations; while in the case of stranded preposition preven-
tion (Section 4. 2), a string transformation was made to assume much of
the load of an existing postcyclic blocking rule, resulting in a highly bene-
ficial elimination of unwanted surface parses in both instances. In other
situations, such as those involving homograph resolution (Section 4.3) and
the treatment of the first group of idiom-processing rules discussed in
Section 4. 4, a correspondence of string transformations to locally-defined
postcyclic tansformations, while potentially possible, did not actually
exist, since no attempt had been made to cover the constructiOns in ques-
tion prior to the introduction of string transformations.
4. 1 Classifier Constructions
The string transformations relating to classifier constructions are
exemplified by-the rule &amp;quot;City, State, Year Classifier&amp;quot;, whose statement
</bodyText>
<page confidence="0.664921">
23
</page>
<bodyText confidence="0.999416363636364">
is displayed in Figure Z. using a hybrid tree/list notation in order to en-
hance legibility. Like&apos; all transformations in the REQUEST System, this
rule consists of a list with five main sections: header, structural pattern,
condition, strtictural change, and feature change. The header, which
serves to identify the rule and a number of basic attributes governing its
appLincation, is in the form of a list comprising the name, type (FORW,
INVDIR, INVINDIR, STRING, or BLOCK), optionality (OB or OP), and
mode (ALL, ANY, ONE, NA, or REANALYZE) of the transformation.
Thus the rule CSYCLSFR is labeled as a string transformation whose
execution is obligatory for all matches that may occur in the list of trees
being processed.
The structural pattern (possibly qualified by further constraints ex-
pressed in the condition section) defines the domain of applicability of
the transformation in the. form of a list of pattern elements, each specify-
ing a tree or class of trees. For a match to occur, it must be possible to
partition the input tree (or list of trees) into a list of non-overlapping,
adjacent trees each of which matches the corresponding pattern element.
Thus, the structural pattern in Figure 2 indicates that the rule CS&apos;YCLSFR
requires that the preprocessed string be partitionable into the following
six-segment sequence: (1) an arbitrary initial segment (possibly nulq
designated (X . 1), (2) an occurrence of the definite article THE, (3) a
common NOUN (already represented-in our surface structure as dominating
</bodyText>
<page confidence="0.625452">
24
</page>
<table confidence="0.9579942">
HeAder: (CSYCL5FR STRING OB ALL)
Structu,ral Pattern:
((X . 1 ) (THE . 2) NOUN (OF . 5) ((INDEX . 6) (X . 7))
(ORX(+ CITY
+ STATE + YEAR)))
</table>
<figure confidence="0.871696666666667">
(INDEX. 4)
{
(CITY . 3)
(STATE . 3)1
(YEAR . 3)
Condition:
(EQUAL ORX (QUOTE {+ (NODENAMEOF 3))) )
Structural Change
(1 2 3 4 5 6 7)
(1. 0 0 0 0 6 7)
Feature Change:
NIL
</figure>
<figureCaption confidence="0.7492625">
Figure 2: The String Transformation
&amp;quot;City, State, Year Classifier&amp;quot;
</figureCaption>
<page confidence="0.79692">
25
</page>
<bodyText confidence="0.973973434782609">
an underlying predicate (V) and an INDEX) which happens to be one of the
three classifiers CITY, STATE, ot YEAR, I4) an occurrende of the prep-
osition OF; (5) an INDEX bearing one of the feature pairs (+ CITY),
(+ STATE), or (+ YEAR) (the absence of a preceding V node here is suffi
cient to guarantee that any matching item will necessarily be an INDEX
(+ CONT)&apos; -- 1, e., a proper noun); and (6) an arbitrary (possibly null)
final segment, designated by (X . 7). The condition adds the further stipu-
lation that the value of the variable ORX* be compatible with node 3 in the
pattern -- i.e., the proper noun must belong to the semantic class desig-
nated by the classifier.
The structural change pf a transformational rule may be stated in one
of two wayt.
(I) If the change is relatively simple as here) it may conveniently be
stated in the form of two lists of numerals referring to the correspond-
ingly labelled elements of the structural pattern. The first list identifies
the elements under consicieratior the second list (which must contain the
same number of elements as the first) specifies what (if anything) happens
to each of them â€” replacement, deletion, sister adjunction tt another
element, etc. In the case of CSYCLStR, the change specified is the
* In addition to providing variables ALPIDA, BETA, and GAMMA, which
range over the set of feature values {+, -,), a the notational system of the
REQUEST transformational component includes the variables ORX, ORY
and ORZ, which range over sets of (feature value, feature name) pairs.
</bodyText>
<page confidence="0.5887825">
26
27
</page>
<bodyText confidence="0.996655340909091">
deletion of the trees whose top nodes are labelled 2, 3 4, and 5 (including,
by convention, any higher nodes which dominate only deleted nodes). Thus
the effect of the rule is to eliminate all classifiers of the designated type
from the preprocessed string:
(2) Alternatively the etruatural change may be expressed as a list of
elementary operations, drawn from the set REPLACE, DELETE,
LCHOMADJ, RCHOMADJI, and their arguments. This notation is typi-
cally employed when fixed trees are inserted (although the first option may
still be taken in such cases) and is obligatory whenever a choice is made
among alternative structural changes by evaluating one or more condi-
tional expressions. Had this second option been taken in the case of the
present rule, its structural change would have read: ((DEâ€˜LETE 2)
(DELETE 3) (DELETE 4) (DELETE 5)).
The feature change section of each transformation is always expressed
as a list of elementary operations which are members of the set (INSERT,
DELETE), together with their associated arguments. Where no feature
change is associated with a rule, as is the case for CSYCLSFR, this
final section- of the rule statement is specified as NIL, the eNnpty rist.
(The structural change and condition sections of transformations can
similarly be defined as NIL, denofing that the tree structure remains un-
changed and that there are.no extra conditions on aoolicability, respectively. )
28.
Two other classifier-deleting string transformations which are very
similar to &amp;quot;City, State, Year Classifier&amp;quot; are the rules &amp;quot;Year Clt.ssifier&amp;quot;
(YRCLASFR) and &amp;quot;Company Classifier&amp;quot; (COCLASFR) The former de-
letes the lexical trees corresponding to the underlined material in
eXamples like &amp;quot;...the year 1968... &amp;quot;, while the latter does the same
thing in examples such as &amp;quot; ... (the) American Can Company... . Although
the underlying predicate COMPANY is the only one specified in the strut-
tural pattern of COCLASFR, the rule actually applies to instances where
a form of either of the words &amp;quot;company&amp;quot; or&apos;&amp;quot;corpokation&amp;quot; has been used
in the input string, owing to the fact that the lexicon assigns the same under-
lying predicate to both in recognition of their synonymity
&amp;quot;City State Block&amp;quot; (CSBLOCK) and &amp;quot;City State&amp;quot; (CITYSTAT) are two
rules, related to the preceding ones, which illustrate additional aspects
of the system. Both of these rules follow CSYCLSFR in the list of string
transformations. As indicated by its header information., (Figure 3(a)),
CSBLOCK is a blocking rule (BLOCK), which entails that it is obligatory
(OB) and will result in termination of the current analysis path if the
structural pattern matches the preprocessed string at least once. The
structural pattern is identical to that for CSYCLSFR save for the omis-
sion of the alternatives relaticig to the predicate YEAR and the feature
(+ YEAR) Due to the parallelism of the structural patterns and the rela-
tive ordering of the two rules, it is necessarily the case lhat CSB-LOCK
</bodyText>
<equation confidence="0.904218">
Header: (CSBLOCK BLOCK OB ONE) 29
Stkuctural Pattern:
((X . 1 (THE . 2) NOUN (OF . 5) ((INDEX . 6) (X . 7))
(ORX (4- CITY
7 + STATE)))
V (INDEX. 4)
I
1 (CITY . 3) 1
(STt .3)
Condition: NIL
Structural Change: NIL
Feature Change: NIL
Header: (CITYSTAT STRING OB ALL)
Structural Pattern:
((X. 1) ((INDEX. 6)(+ CONST (COMMA. 3)
1 + CITY))
(W 2) NIL
Condition: NIL
Structural Change:
(1 2 3 4 5)
(1 (2 4) 0 0 5)
Feature Change:I
((INSERT 6 ((+ CITYSTATE))) )
(INDEX(+ CONST (X
1 + STATE))
(W . 4)
</equation>
<figureCaption confidence="0.998703">
Figure 3: The Rules CSBLOCK and CITYSTAT
</figureCaption>
<page confidence="0.325381">
30
</page>
<bodyText confidence="0.896211655172414">
will apply if and only if the classifier and the following proper noun do
not correspond (any corresponding classifiers having already been de-
leted by CSYCLSFR). Thus CSBLOCE has the effect of aborting analyses
where a proper name known to the system as designating a state has been
classified as denoting a city, or vice-versa
The rule CITYSTAT does not refer to classifiers as such, but it
does deal with a proper noun construction even more important for our
particular subset: the precise identification of a specific city by append-
ing the appropriate state name to the city name. This construction is essen-
tial in distinguishing among such cities as Portland, Maine and Portland,
Oregon, not to mention the eighteen varieties of Springfield in the con-
::-:*
tinental United States The structural pattern of the rule (Figure 3(b))
specifies a domain consisting of a city name ((INDEX . 6) (+ CONST + CITY))
followed by an optional comma, followed by a state name (INDEX (+ CONST
***
+ STATE)), where the actual city name is a single tree (W . 2) and the
*
Such a situation would always arise in processing such inputs as
&amp;quot;the ritY of New York&amp;quot;, effectively resolving the ambiguity of the
State
proper noun, if the user were not previously asked by the system to re-
solve it, as is our current practice.
**
Cf. reference 15.
***
The structural variable W is employed in structural patterns in place
of the more usual X whenever one wishes to specify the occurrence of
precisely one unknown tree.
</bodyText>
<page confidence="0.850639">
31
</page>
<bodyText confidence="0.929356441860465">
state name a single tree (W . 4). As indicated by the structural change,
each match results in the replacement of the tree labelled 2 by a list of
tre-es consisting of itself and the tree labelled 4, thereby pairing the state
name with the city name by what amounts to right sister adjunction. The
optional, comma (COMMA 3) an..d the state name (W . 4) -- plus, by the
convention cited earlier, the structure dominating 1., -- are deleted.
Finally, the feature (+ CITYSTATE) is added to the feature&apos;list of the
node (INDEX . 6), where its presence wilL eventually be noted by the
semantic interpreter as requiring a match on both elements of a (cityname,
Statename) pair in the data base. As far a.s the transformational compon-
ent is concerned, the net effect of the rulq iss.to make &amp;quot;city, state&amp;quot; con-
structions pass through both the surface parser and the inverse transforma-
tions as though they were simple city names.
4. 2 Stranded Prepositions
&amp;quot;Stranded Preposition Prevention&amp;quot; (Figure 4) is a string transforma-
tton designed to prevent surface structure parses in which non-stranded
prepositions are erroneously anatlyzed as stranded ones. Since most
prepfesitions, whether stranded or not, are obligatorily present in sur-
face structures, this rule necessarily â€žreflects an approach very differ-
ent from the &amp;quot;recognize and delete&amp;quot; strategy employed in the string trans-
formations inVolving classifiers. What is done here is to assign new word
32
class codes to those prepositions determined to be non-strandable, and to-
write the surface structure rules for the new codes in such a way that they
are only allowed to combine with a following noun phrase.
Expressed in or dinary English, the statement of the rule reads about
as follows: &amp;quot;Replace the word class code of each preposition by the corres-
ponding code for non-strandable prepositions except where the preposition
immediately precedes an auxiliary, a punctuation mark, a verb form, or
another prepilSition, assign any locative feature associated with the original
word class code to the new word class code&amp;quot;. As stated -- and as cur-
rently implemented -- the rule may well be at once both too weak and too
strong, at least man absolute sense. It is probably too weak in that it
will fail to label as non-strandable any preposition which immediately pre-
cedes a noun phrase beginning with.. an adjective (VADJ), as, for example;
in the sequence &amp;quot;to large compamies&amp;quot;. This sort of deficiency is of little
consequence, however, since the rule will serve its purpose well if it
fails to catch an occasional non-strandable preposition, leaving things as
ambiguous as before in those cases.
Excessive strength, in the sense of marking some stranded preposi-
tion as non-strandable, is potentially a much more serious flaw, since
it precludes obtaining a correct analysis in such instances. Examples
such as (9), where SPRPPREV would fail in just this way by applying
</bodyText>
<note confidence="0.534301">
Header: (SPRPPREV STRING OB ALL)
</note>
<figure confidence="0.986415423076923">
11
Structural Pattern:
1
(PREP. 2)
I
(W 5)
?.
(PREPOF 3)
(W I 5)
(X 4))
Condition: (NOT (ANALYSIS 4 NIL (QUOTE( ((BAUX)) ((X))) )))
((COMMA))
(1(DAUX))
((PREP))
((PUNCT))
On
((VADJ))
((VING))
Structural Change
((COND (2
(CONDUANALYSIS 1 NIL (QUOTE ( ((PREP((+ LOC2))) ))) )
(REPLACE ( (NSPrzp((+ LOC2))) ) 2))
5
(T (REPLACE NSP EP)2))) )
5
(3 (REPLACE (NSPRIEPOF) 3))) )
</figure>
<figureCaption confidence="0.789522">
Feature Chahge: NIL
Figure 4: The String Transformation &amp;quot;Stranded Preposition Preventioi
</figureCaption>
<bodyText confidence="0.877724222222222">
incorrectly, are not particularly difficult to think up.. However, the
(9) Was the company XYZ bought baIlbearings from a subsidiary or
Universal Nut &amp; Bolt7
great majority of such examples â€” including (9) -- seem to be irrelevant
to the present REQUEST data blase. Thus, while it is clear that our
initial rule for stranded preposition prevention does not provide anything
approaching a gen.eral solution to the problem, it does appear to be work-
ing satisfactorily for the moment in eliminating artificial surface ambig-
uities within a narrow domain of discourse.
</bodyText>
<subsectionHeader confidence="0.967778">
4.3 Homograph Resolution
</subsectionHeader>
<bodyText confidence="0.872037">
One of the simplest and yet most useful of the 33 string transforma-
tions in the current version of REQUEST is the rule &amp;quot;Ordinal Formation&amp;quot;
(ORDFORM). Its function is to match on each string consisting of an
arabic numeral immediately followed by any member of the set of English
ordinal-forming suffixes {d, nd, rd, st, th} and mark the sequence as an
o-rdinal numeral. The operation of ORDFORM (Figure 5) is entirely
straightforward. By this point in the analysis process, all arabic..numer-
als have already been assigned lexical trees dominated by the node
(VADJ (+ CARD)) -- the combination denoting a cardinal numeral â€” during
the input scanning phase of the preprocessor; while the ordinal-forming
suffixes have been assigned -trees dominated by the category ORD during&apos;
</bodyText>
<figure confidence="0.8962911">
34
Header: (ORDFORM STRING OB ALL)
Structural Pattern:
((X . 1) ((VADJ_. 2) (+ CARD)) (ORD 3) (X . 4))
Condition: NIL
Structural Change:
((DELETE 3))
Feature Change:
((DELETE 2 (CARD)) (INSERT 2 ((+ ORD))) )
35
</figure>
<figureCaption confidence="0.999914">
Figure 5: The String Transformation &amp;quot;Ordinal Formation&amp;quot;
</figureCaption>
<bodyText confidence="0.988278794117647">
36
the lexical lookup phase. ORDFORM simply finds each instance in the pre-
processed string where a (VADJ (+ CARD)) inithediately precedes an ORD,
deletes the ORD tree, and changes the feature on the VADJ from (+ CARD)
to (+ ORD), thereby identifying that item as an ordinal numeral rather
than a cardinal.
The approach just described has the advantage of putting an unlimited
set of ordinals at the disposal of the user at negligible cost, involving a
few very minor additions to the lexicon and none at all to either the surface
grammar or the preprocessor. The alternate of using a postcyclic trans-
formation instead of a str.ing transformation to achieve the same coverage
was avoided because it would have imposed the additional requirement that
the surface grammar be significantly enlarged through the inclusion of at
least three new category symbols (for cardinals, ordinals, and ordinal
suffixes) along with a set of context-free rules describing their distribu-
tion. Although identification of ordinal numerals of this type could also
have been effected by building,-the appropriate tests directly into the prepro-
cessor, the latter alternative would have been much less attractive than
the string transformation approach for at least two reasons: First, it is
inherently ;7nessier to bury such operations in a special program subroutine
than to deal with them as just another transformational rule. Second, and
more important, is the fact that the latter apii,roach makes the system less
general and flexible, since material_ specific to English is directly re-
flected in the structure of the program itself, rather than being confined
to the grammar, where it is readily accessible to the linguist who may
wish: to modify it -or replace- it by material de&apos;scribing some other natural
language.
Another string transformation currently employed to resolve word
class homography on the basis of local context is the rule &amp;quot;Cardinal
Noun&apos; (CARDNOUN), which will be discussed only briefly here, The
;ule distinguishes instances where a cardinal numeral functions as a
proper noun (10) from those in which it serves as a numerica.1 quantifier
pf a following nominal expression (11). It does so by checking the im-
mediate- right-hand cOntext of each (VADJ (+ CARD)) for the presence of
</bodyText>
<listItem confidence="0.909778">
(10) Is the number of companies in Chicago greater than 16?
(11) Wliat companies employed at least ZOO, 000 people in 19739
</listItem>
<bodyText confidence="0.999711666666667">
items (such 4s articles, auxiliaries? punctuation, and verbs) which are
incompatible with the latter possibility, replacing the VADJ structure by
a corresponding proper noun structure whenever a match occurs.
(CARDNOUN follows ORDFORM in the list of string transformations in
order to take advantage of the latter&apos;s replacement of certain cardinals
by corresponding ordinals. )
</bodyText>
<figure confidence="0.26931725">
37
38
4. 4 Idiom Processing
By their very definition, idiomatic expressions are items which pre-
</figure>
<bodyText confidence="0.997268">
sent problems in grammatical analysis, semantic interpretation, or both.
Although it would be very tei-npting to exclude all constructions of this
sort from the English subset of REQUEST, the currency and naturalness
of many idioms is so great that such a prohibition would entail abandon-
ment of our goal of permitting future users to employ their normal pat-
terns of expression.
For idioms such as &amp;quot;make money&amp;quot;, (in the. sense of &amp;quot;be profitable&amp;quot;),
where the components are adjacent and the number of paradigmatic var-
iants are few, one possible approach is to deal with the problem by putting
appropriate entries in the phrase lexicon. For example, the entry for
&amp;quot;makes money&amp;quot; in our present lexicon treats that combination as an in-
transitive verb in the present tense and singular number which dominates
the same underlying predicate and has the same selectional features as
the adjective &amp;quot;profitable&amp;quot;. Even in such a relatively straightforward
case, however, it is not difficult to think of minor extensions, such as
the inclusion of negatives (&amp;quot;make no money&amp;quot;), which will at least require
another set of phrasal entries. Moreover, the phrase lexicon approach
breaks down completely as soon as one. deals with an idiomatic construc-
tion :hat includes aft open class as one of its components, producing a
situation parallel to that encountered earlier for classifier constructions.
The attempt to provide broad coverage of constructions involving
notions of rank and ordinality led to the consideration of a number of
common idiomatic Patterns including arbitrary cardinal or ordinal numer-
als. These patterns, three of which are illustrated in (12) , were even-
tually dealt with successfully by the development of string transforma-
tions designed not only to cope with then- syntactic peculiarities but to
</bodyText>
<figure confidence="0.8367228">
(12) (a) What company was
ranked nurither 18 in 1972 sales?
_atÂ°
(b) What were the â€”25 highest -ranking companies with
respectito earnings in 1969
</figure>
<listItem confidence="0.674773">
(c) List the top 20 companies in 1973 growth rate!
</listItem>
<bodyText confidence="0.993617555555556">
set the stage for cortect semantic processing as well.
The nature of these idiom-processing transformations is perhaps
best illustrated by considering the rule &amp;quot;Top n&amp;quot; (TOPN), whose state-
i-nent appears in Figure 6. The structural pattern of TOPN specifies
a sequence of elements consisting of an initial arbitrary string of trees
(X . 1) followed in order by an occurrence of the definite article &amp;quot;the&amp;quot;
(THE . 2)) the word &amp;quot;top&amp;quot; (TOP 3), a cardinal numeral ((VAIXT . 4)
(+ CARD)), a no.minal expression (NOM . 5), either of the prepositions
&amp;quot;in&amp;quot; (IN . 6) or &amp;quot;with respect to&amp;quot;. (WITH_RESPECT_TO . 6), and a
</bodyText>
<figure confidence="0.954712111111111">
39
Header: (TOPN STRING OB ALL)
40
Structural Pattern:
((X . 1) (THE . 2) (TOP. 3Y ((VADJ . 4) (NOM â€¢ 5)
(+ CARD))
(W. 8)
(IN 6)
(WITH RESPECT&apos;
. 6)
Condition: NIL
Structural Change.
((REPLACE (5 (VING(+ ADJ (VADJ(+ ADJ PRP (VADJ(+ ADJ ) 5)
+ ING)) + ORD)) 4- ORD))
RANK
(DELETE 3) (DELETE 4))
Feature Change: NIL
(NQUOTE 1) THROUGH 8
</figure>
<figureCaption confidence="0.999878">
Figure 6: The Rule vTop n&amp;quot;
</figureCaption>
<page confidence="0.505339">
41
</page>
<bodyText confidence="0.963156147058824">
final arbitrary string of trees (X . 7). The structurial change includes a
ropiacement and two deletions.
The syntax of a replacement operation is of the form (REPLACE
&lt;list of trees&gt; &lt;tree&gt; ), its execution results in the replacement of tie
item corresponding to tree by the items corresponding to list of trees.
The replacement operation in TOPN is therefore to be understood as
follows: The nominal exptession tree in the input which matches the
pattern element (NOM . 5) is replaced by a list of elements consisting
of itself, followed by lexical trees corresponding to (i) the -ing form of
the verb &amp;quot;rank&apos;&apos;, (ii) the ordinal nunieral &amp;quot;first&amp;quot; (where the (NQUOTE 1)
notation capses the &amp;quot;1&amp;quot; to be interpreted as a literal, rather than as .a
reference to the pattern element (X . 1)), (iii) the preposition &amp;quot;through&amp;quot;,
and (iv) the ordinal numeral corresponding to the cardinal which matched
((VADJ . 4) (+ CARD)) in the structural pattern. The two deletion opera-
tions remove the lexical trees for the cardinal numeral and the adjec-
tive &apos;top&amp;quot; from the preprocessed string.
In the case of (12c), the overall effect of this structural change is
to replace the string of lexical trees corresponding to &amp;quot;the top 20 com-
panies&amp;quot; by theâ€¢string of trees corresponding to &amp;quot;the companies ranking
Clst through 20th&amp;quot;. A subsequent string transformation called &amp;quot;Rank
Interval&amp;quot; (RNKINTVL), operating in a fashion similar to that of &amp;quot;City
State&amp;quot; (cf. Section 4.1), then transforms the trees corresponding to
&amp;quot;1 8t through ZOth&amp;quot; into a single ordinal numeral .tree (bearing the feature
(4- INTERVAL)) which dominates the numerals i&apos;l&amp;quot; and &amp;quot;20&amp;quot; As a
result of these operations both surface and.transformational parsing of
such examples has become completely routine; while their semantic
interpretation has required only the addition of a simple mechanism --
triggered by the feature (+ INTERVAL) --&apos;for,generating a depse set of
integers from its endpoints.
Another group of string transformations involving rank are derived
from what were originally late ppstcyclic transforMations. The three.
rules in question -- &amp;quot;First Superlative&amp;quot; (FIRSTSUP) &amp;quot;Nth Superlative&amp;quot;
(N:THSUPER), and &amp;quot;Nth Plae-e&amp;quot; (NTHPLACE) -- collectively serve to
restore the various deletions illustrated in (13).
</bodyText>
<figure confidence="0.911765703703704">
42
(13) a.
b.
1
I ranked OB&gt;
(the) first highest
was
I ranked
â€”
- (the) highest -
was
first
ranked 01-&apos;&gt;
(in) (the) I second highest ,
wag
nth
first
ranked
(in) (the)&apos;Second
was
&amp;quot;nth
43
I ranked i OP
n (the) 1;ith highest place - &gt;
was
... .., ... Iranked 1(the nth hignest -- -
was
</figure>
<bodyText confidence="0.999607928571429">
The prime motivation for shifting these rules from the postcycle to
a point preceding surface parsing was that the structure and distribution
of the various phrase remnants resulting from the deletions are at best
difficult to describe within the framework of a context-free phrase struc
ture grammar. A variety of ad hpc 10.paratus, including special word
class codes for the verb &amp;quot;rank&amp;quot; and for superlative ac ectives, as Nxrell
as special phrase names for such sequences as &amp;quot;the + superlatiire&amp;quot;
and &amp;quot;ordinal numeral + superlative&amp;quot;, Would have to be introduced in order
to provide broad coverage without an accompanying combinatorial ex-
plosion. By restoring the deletions before surface parsing, however,
such distasteful and complicated meabures are entirely avoided, since
lexical categories are left unchanged and the surface parser has to do no
more than parse an ordinary prepositional phrase in the position following
the verb.
</bodyText>
<subsectionHeader confidence="0.471883">
4.5 Experiments in Limited Conjunction Processing
</subsectionHeader>
<bodyText confidence="0.796751333333333">
As was mentioned in the introduction to this paper, one of the princi-
C.
pal directions iii which we are currently seeking to extend the English
</bodyText>
<page confidence="0.41575">
44
</page>
<bodyText confidence="0.963548731707317">
subset accepted by the REQUEST System is in the caveragt of (coordin-
ate) conjunction constructions. The fact that the underlying variety and
complexity of these constructions teCtilds to be Masked by superficial simi-
larities makes a selective, piecemeal approach to their coverage a gen-
erally-dubious move in a system strch as ROUST, whose eventual
users can hardly be expected to make distinctions that may not be im-
mediately obvious even to a trained linguist. Despite strong reservatio,ns
on thi,s point, it was decidea to employ the string transformation mechan-
ism to deal with an extremely limited range of conjunction constructions
on an experimental basis.
The range of constructions chosen was confined to conjoined proper
nouns exclusively, subject to the further constraint that all terms of a
given conjunction be members of the same semantic class - i.e., for the
current data base, either company names, city names, state names
or year names. While undeniably highly limited in scope, this particu-
lar incremental increase in grammatical coverage (if successful) had
three distinct merits: (1) it app red to be compatible with the adjacency
constraints of string transformations, .owing to the tendency of proper
nouns to take no modifiers, (2) it seemed potentially explainable to a
naive user in simple terms, and (3) it could provide a natural language
interface to an existing, but as yet largely unused, capability of the out-
put formatting routines to generate and display tables of values containing
3uch information as the earnings of each of a set of companies over a
)erio.d of years.
The approach employed in the string transformations for processing
:onjoined proper nouns is exemplified by the rule &amp;quot;City, State, Year,
:ompany Conjunction&amp;quot; (CSYCOCNJ) , wilose statement is displayed in
Figure 7. The second and third elements of the structural pattern form
1 subpattern that is preceded by an asterisk and surrounded by a pair of
parentheses. This notation identifies the occurrence of a so-called
&amp;quot;Kleene star expression&amp;quot;, which is interpreted by the transformational
parser 4 s a pattern element that is to be matched by zero or more con-
secutive occurrences of tree sequences matching components. The
particular Kleene star expression used here will mlatch a string of any
length which% consists entirely of an, alternating sequence of proper nouns
and commas, provided that all the proper nouns are members of the same
**
semantic class The pattern elements fallowing the Kleene star ex-
pression specify that it must be followed by: (P), another instance of a
proper noun of the appropriate class this will be the initial instance if
the null value of the Kleene star expression is the only one that matches);
</bodyText>
<reference confidence="0.938090666666667">
4.
.1.
The effe-ct of the condition, which precludes any match where the left-
hand structural variabld (X . 1) ends in a sequence of trees satisfying
the pattern of the Kle&apos;en6 itar expression, is to force a (unique) match
of maximum length.
:4*
Repeated.accurrences of ORX in a structural pattern, whether implicit
or explicit,, are required to match the same feature, pair.
</reference>
<page confidence="0.576735">
45
</page>
<figure confidence="0.97948252173913">
Header: (CSYCOGNJ STRING OB ALL)
Structural Pattern:
((X. 1) (* (INDEX (ORX (+ CITY + STATE (COMMA 3))
+ YEAR +C0)))
(W . 2).
(INDEX (ORX (+ CITY + STATE (COMMA. 3)
1 + YEAR + CO))) NIL
(W . 2).
I
1 .(AND. 4)1 ((NOUN 8) (+ Sc)) (X . 7))
(ORR . 5) I
QINI)EX 9) (ORX))
(W 6)
Condition&apos;
(NOT (ANALYSIS 1 T (QUOTE (((X))((INDEX (ORX))) ((COMMA))) ))
Structural Change:
7)
Feature Changeâ€¢
((GOND (4 ((INSERT 9 ((+ ANDSET))) (INSERT 8 ((-SG))) ))
(5 (INSERT 9 ((+ ORSET))) )))
46
(1 2 3 4 5 6 7)
(1 0 0 0 0 (2 6)
</figure>
<figureCaption confidence="0.999976">
Figure 7: The Rule &amp;quot;City, State, Year, Company Conjunction&amp;quot;
</figureCaption>
<bodyText confidence="0.98498">
47
(ii) an optional comma; (iii) au instance of either of the coordinating
conjunctions &amp;quot;and&amp;quot; or &amp;quot;or&amp;quot; I represented internally as ORR, since
OR is already used to signal the presence of a disjunctive pattern element
to the rule-processing. routine); (iv) the final instance of a semantically
compatible proper noun, and (v) the usual end variable.
The structural change specifies (1) that the terminal elements of
all but the rightmost conjunct (which are collectively associated with
the pattern element (W . 2) during the pattern matching phase) are to be
sister adjoined to the terminal element of that rightmost conjunct and
(2) that the original occurrences of all trees but those corresponding to
the end variables and the final conjunct are to be deleted. Conditional
on the presence of the conjunction &amp;quot;and&amp;quot; (AND . 4), the feature change
adds the feature (+ ANDSET) to the feature list of the surviving INDEX
and the feature (- SG) to that of the NOUN node immediately above.
(The latter operation automatically results in replacement of the original
(+ SG)). If the conjunction is an &amp;quot;or&amp;quot; (ORR . 5) instead, the feature
change merely adds the feature (+ ORSET) to the feature list of the
INDEX, leaving the number of the NOUN unchanged.
The overall effect of the rule reflects the by now familiar strategy
of mapping a structure which would otherwise pose severe problems in
surface parsing into a significantly simpler one which will be processed
without difficulty by both the surface parser and the transformational
</bodyText>
<page confidence="0.545281">
48
</page>
<bodyText confidence="0.9991636">
parser. As in the case of CITYSTATE and RNKINTVL, a special .fa.-
tur.e is attached to the node in the output structure that directly dominates
two or more terminal symbols as a result of the 5 tructural change of the
rule. In each case, the purpose of the feature is to communicate to the
semantic interpreter how the elements of the set of terminal symbols are
to be treated -- as a (city, state) pair, as the endpoints of a dense set of
integers, or as the elements of a conjoined set of proper nouns.
The experimental approach to proper ncxun conjunction just described
appeared initially to be a rather effective one. Examples such as (14)
went through the transformational component as shloothly as ones like (15),
</bodyText>
<listItem confidence="0.513577333333333">
(14) How much did GM, Ford, and Chrysler earn in the years from
1967 through 1972?
whereupon the interpretive component produced what appeared to be an
appropriate answer -- in the case of (14), an earnings table with 18 entries
(15) How much did Ford earn in 1969?
listed by company and by year. It was not long, however, before considera-
tion of examples such as (16) and (17) revealed that the initial appearance
of an. adequate solution had been highly misleading.
(16) Was GM or Ford unprofitable in 19702
</listItem>
<page confidence="0.771019">
49
</page>
<bodyText confidence="0.982031930232558">
(17) What were the earnings of the Big Three auto companies tor tne
1 966-1 968 period?
For thet former example, at least two readings seem possible: one
as a seleckional question, paraphrased in (18a) (which would preclude a
(18)a. Which auto company was unprofitable in 1970 -- GM or Ford?
b. Was either G,M or Ford unprofitable in 19707
yes Â®r no answer), the other as a yes-no question (18b), where the con-
ditions for giving a positive answer depend upon the interpretation of the
&amp;quot;or&amp;quot; as inclusive or exclusive. In the case of (17), there seems to be
a series of possible readings, roughly paraphrased by (19a-d), reflecting
ambiguity as to whether what has been requested is earnings information
(19) a. What were the earnings of each of the Big Three auto companies
for each of the years 1)966-196,8?
b. What were the combined earnings of the Big Three auto com-
panies for each of the years 1966-1968?
c. What did the earnings of each of the Big Three auto companies
totai for the 1966-1968 period?
d What did the combined earnings of the Big Three auto companies
total for the 1966-1968 period?
(a) presented individually by company and by year, (b) summed over
companies hut not over years, (c) summed over years but not over com-
panies, or (d) summed over both companies and years.
50
Ambiguities of the types exemplified by (16) and (17) were found to
be quite widespread in the sort of material we are dealing with, occurring
in a number of examples such as (14) where their presence was not
initially perceived. Moreover, it was soon realized that such ambigui-
ties were totally different in character from the types we had previously
been most concerned with, since they involved instances of genuine multi-
ple meaning in the language, rather than ambiguities artificially intro-.
duced by the inadequacies of a grammatical description or a parsing
mechanism. It was also clear that the underlying structures assigned to
these ambiguous examples were seriously deficient, in that they did not
indicate the presence of an ambiguous situation, much less what the am-
biguous alternatives were.
Further investigation indicated that the ambiguities encountered were
not restricted to conjoined proper nouns, but could also occur in the case
of plural noun phrases. ror. example, (20) is ambiguous between a read-
ing requesting earnings listed individually by company and a reading
(20) What were the 1972 earnings of the companies in Chicago2
requesting a combined earnings figure -- exactly the same readings which
would exist if the phrase &amp;quot;the companies in Chica.go&amp;quot; were replaced by
the conjoined names of all companies satisfying that description. Thus,
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.024979">
<title confidence="0.96078575">hunk&apos; of Computational Linguistics 8 STRING TRANSFORMATIONS IN THE REQUEST SYSTEM</title>
<author confidence="0.999999">Warren J Plath</author>
<affiliation confidence="0.908827">IBM Thomas J. Watson Research Center Yorktown Heights</affiliation>
<address confidence="0.578314">1974 by the Computational Linguistics</address>
<email confidence="0.277787">2</email>
<abstract confidence="0.977213071428572">The REQUEST System is an experimental natural language query based large transformational grammar of English. In the original implementation of the system the process of computing underlying sLructures of input a sequence of three steps: (1) preprocessing (including dictionary lookup), (2) surface phrase structure parsing, and (3) transformational parsing. This scheme has since been modified to permit transforoperations not only on the available after comparsing, but also on the strings of lexical trees which are the output of the preprocessing phase. Transforrules of this latter type which are invoded priorto surare known as transformations. Since they must be defined in the absence of such structural markers as the location of clause boundaries, string transformations are necessarily relatively local in scope. Despite this inherent limitation, they have so far proved to be an extremely useful and surprisingly versatile addition to the REQUEST System. Applications to date have included homograph resolution, analysis of classifier constructions, idiom handling, and the suppression of large numbers of unwanted surface parses. While by no means a panacea for transformational parsing, the use of string transformations in REQUEST has permitted relatively rapid and painless extension of the English subset in a number of important areas without corresponding adverse impact on the size of the lexicon, the complexity of the surface grammar, and the number of surface parses produced. 5</abstract>
<affiliation confidence="0.390643">TABLE OF CONTENTS</affiliation>
<page confidence="0.633354"></page>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<volume>4</volume>
<pages>1</pages>
<marker></marker>
<rawString>4. .1.</rawString>
</citation>
<citation valid="false">
<title>The effe-ct of the condition, which precludes any match where the lefthand structural variabld (X . 1) ends in a sequence of trees satisfying the pattern of the Kle&apos;en6 itar expression, is to force a (unique) match of maximum length. Repeated.accurrences of ORX in a structural pattern, whether implicit or explicit,, are required to match the same feature,</title>
<pages>pair.</pages>
<marker></marker>
<rawString>The effe-ct of the condition, which precludes any match where the lefthand structural variabld (X . 1) ends in a sequence of trees satisfying the pattern of the Kle&apos;en6 itar expression, is to force a (unique) match of maximum length. Repeated.accurrences of ORX in a structural pattern, whether implicit or explicit,, are required to match the same feature, pair.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>