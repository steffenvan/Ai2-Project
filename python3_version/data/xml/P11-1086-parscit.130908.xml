<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000313">
<title confidence="0.991436">
Rule Markov Models for Fast Tree-to-String Translation
</title>
<author confidence="0.982099">
Ashish Vaswani
</author>
<affiliation confidence="0.991886">
Information Sciences Institute
University of Southern California
</affiliation>
<email confidence="0.991232">
avaswani@isi.edu
</email>
<author confidence="0.992013">
Haitao Mi
</author>
<affiliation confidence="0.980404">
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<email confidence="0.971327">
htmi@ict.ac.cn
</email>
<author confidence="0.999268">
Liang Huang and David Chiang
</author>
<affiliation confidence="0.996304">
Information Sciences Institute
University of Southern California
</affiliation>
<email confidence="0.999384">
{lhuang,chiang}@isi.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999572333333333">
Most statistical machine translation systems
rely on composed rules (rules that can be
formed out of smaller rules in the grammar).
Though this practice improves translation by
weakening independence assumptions in the
translation model, it nevertheless results in
huge, redundant grammars, making both train-
ing and decoding inefficient. Here, we take the
opposite approach, where we only use min-
imal rules (those that cannot be formed out
of other rules), and instead rely on a rule
Markov model of the derivation history to
capture dependencies between minimal rules.
Large-scale experiments on a state-of-the-art
tree-to-string translation system show that our
approach leads to a slimmer model, a faster
decoder, yet the same translation quality (mea-
sured using B ) as composed rules.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998554375">
Statistical machine translation systems typically
model the translation process as a sequence of trans-
lation steps, each of which uses a translation rule,
for example, a phrase pair in phrase-based transla-
tion or a tree-to-string rule in tree-to-string transla-
tion. These rules are usually applied independently
of each other, which violates the conventional wis-
dom that translation should be done in context.
To alleviate this problem, most state-of-the-art sys-
tems rely on composed rules, which are larger rules
that can be formed out of smaller rules (includ-
ing larger phrase pairs that can be formerd out of
smaller phrase pairs), as opposed to minimal rules,
which are rules that cannot be formed out of other
rules. Although this approach does improve trans-
lation quality dramatically by weakening the inde-
pendence assumptions in the translation model, they
suffer from two main problems. First, composition
can cause a combinatorial explosion in the number
of rules. To avoid this, ad-hoc limits are placed dur-
ing composition, like upper bounds on the number
of nodes in the composed rule, or the height of the
rule. Under such limits, the grammar size is man-
ageable, but still much larger than the minimal-rule
grammar. Second, due to large grammars, the de-
coder has to consider many more hypothesis transla-
tions, which slows it down. Nevertheless, the advan-
tages outweigh the disadvantages, and to our knowl-
edge, all top-performing systems, both phrase-based
and syntax-based, use composed rules. For exam-
ple, Galley et al. (2004) initially built a syntax-based
system using only minimal rules, and subsequently
reported (Galley et al., 2006) that composing rules
improves B by 3.6 points, while increasing gram-
mar size 60-fold and decoding time 15-fold.
The alternative we propose is to replace composed
rules with a rule Markov model that generates rules
conditioned on their context. In this work, we re-
strict a rule’s context to the vertical chain of ances-
tors of the rule. This ancestral context would play
the same role as the context formerly provided by
rule composition. The dependency treelet model de-
veloped by Quirk and Menezes (2006) takes such
an approach within the framework of dependency
translation. However, their study leaves unanswered
whether a rule Markov model can take the place
of composed rules. In this work, we investigate the
use of rule Markov models in the context of tree-
</bodyText>
<page confidence="0.993">
856
</page>
<note confidence="0.886222">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856–864,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.995987583333333">
to-string translation (Liu et al., 2006; Huang et al., The derivation tree is generated as follows. With
2006). We make three new contributions. probability P(r1  |c), we generate the rule at the root
First, we carry out a detailed comparison of rule node, r1. We then generate rule r2 with probability
Markov models with composed rules. Our experi- P(r2  |r1), and so on, always taking the leftmost open
ments show that, using trigram rule Markov mod- substitution site on the English derived tree, and gen-
els, we achieve an improvement of 2.2 B over erating a rule ri conditioned on its chain of ancestors
a baseline of minimal rules. When we compare with probability P(ri  |ancn1(ri)). We carry on until
against vertically composed rules, we find that our no more children can be generated. Thus the proba-
rule Markov model has the same accuracy, but our bility of a derivation tree T is
model is much smaller and decoding with our model P(T) = H P(r  |ancn1(r)) (1)
is 30% faster. When we compare against full com- r∈T
posed rules, we find that our rule Markov model still For the minimal rule derivation tree in Figure 2, the
often reaches the same level of accuracy, again with probability is:
savings in space and time. P(T) = P(r1  |c) · P(r2  |r1) · P(r3  |r1)
Second, we investigate methods for pruning rule · P(r4  |r1, r3) · P(r6  |r1, r3, r4)
Markov models, finding that even very simple prun- · P(r7  |r1, r3, r4) · P(r5  |r1, r3) (2)
ing criteria actually improve the accuracy of the Training We run the algorithm of Galley et al.
model, while of course decreasing its size. (2004) on word-aligned parallel text to obtain a sin-
Third, we present a very fast decoder for tree-to- gle derivation of minimal rules for each sentence
string grammars with rule Markov models. Huang pair. (Unaligned words are handled by attaching
and Mi (2010) have recently introduced an efficient them to the highest node possible in the parse tree.)
incremental decoding algorithm for tree-to-string The rule Markov model
translation, which operates top-down and maintains can then be trained on the path set of these deriva-
a derivation history of translation rules encountered. tion trees.
This history is exactly the vertical chain of ancestors Smoothing We use interpolation with absolute
corresponding to the contexts in our rule Markov discounting (Ney et al., 1994):
model, which makes it an ideal decoder for our Pabs(r  |ancn1(r)) = max c(r  |ancn 1(r)) − Dn, 0
model. Er′ c(r′  |ancn1(r′))
We start by describing our rule Markov model { 1
(Section 2) and then how to decode using the rule + (1 − An)Pabs(r  |ancn−1
Markov model (Section 3). 1 (r)), (3)
2 Rule Markov models where c(r  |ancn 1(r)) is the number of times we have
Our model which conditions the generation of a rule seen rule r after the vertical context ancn1(r), Dn is
on the vertical chain of its ancestors, which allows it the discount for a context of length n, and (1 − An) is
to capture interactions between rules. set to the value that makes the smoothed probability
Consider the example Chinese-English tree-to- distribution sum to one.
string grammar in Figure 1 and the example deriva- We experiment with bigram and trigram rule
tion in Figure 2. Each row is a derivation step; the Markov models. For each, we try different values of
tree on the left is the derivation tree (in which each D1 and D2, the discount for bigrams and trigrams,
node is a rule and its children are the rules that sub- respectively. Ney et al. (1994) suggest using the fol-
stitute into it) and the tree pair on the right is the lowing value for the discount Dn:
source and target derived tree. For any derivation n1
node r, let anc1(r) be the parent of r (or c if it has no Dn = (4)
parent), anc2(r) be the grandparent of node r (or c if n1 + n2
it has no grandparent), and so on. Let ancn1(r) be the
chain of ancestors anc1(r) · · · ancn(r).
857
rule id translation rule
</bodyText>
<equation confidence="0.990430555555556">
r1 IP(x1:NP x2:VP) x1 x2
r2 NP(B`ushi) Bush
r3 VP(x1:PP x2:VP) x2 x1
r4 PP(x1:P x2:NP) x1 x2
r5 VP(VV(jˇuxing) AS(le) NPB(huit´an)) held talks
r6 P(yˇu) with
r� P(yˇu) � and
6
r7 NP(Sh¯al´ong) Sharon
</equation>
<figureCaption confidence="0.999665">
Figure 1: Example tree-to-string grammar.
</figureCaption>
<figure confidence="0.8689814">
derivation tree derived tree pair
ǫ IP@ǫ : IP@ǫ
r3
r2
r1
r2 r3
Bush
:
VP@2.2 PP@2.1
: Bush held talks
P@2.1.1 NP@2.1.2
Sharon
: Bush held talks with
IP@ǫ
NP@1 VP@2 :
NP@1 VP@2
r1
IP@ǫ
NP@1
B`ushi
</figure>
<figureCaption confidence="0.9764655">
Figure 2: Example tree-to-string derivation. Each row shows a rewriting step; at each step, the leftmost nonterminal
symbol is rewritten using one of the rules in Figure 1.
</figureCaption>
<page confidence="0.935157">
858
</page>
<figure confidence="0.93764035">
VP@2
PP@2.1 VP@2.2
r6
r7
r1
r2 r3
r4
r5
r4
r5
IP@ǫ
jˇuxing
le
huit´an
le
IP@ǫ
NP@1
B`ushi
VP@2
jˇuxing
VP@2.2
PP@2.1
NP
VV
AS
P@2.1.1 NP@2.1.2
huit´an
NP@1
B`ushi
VP@2
VP@2.2
PP@2.1
NP@2.1.2
Sh¯al´ong
AS
VV
NP
P@2.1.1
yˇu
r1
</figure>
<bodyText confidence="0.9995472">
Here, n1 and n2 are the total number of n-grams with
exactly one and two counts, respectively. For our
corpus, D1 = 0.871 and D2 = 0.902. Additionally,
we experiment with 0.4 and 0.5 for Dn.
Pruning In addition to full n-gram Markov mod-
els, we experiment with three approaches to build
smaller models to investigate if pruning helps. Our
results will show that smaller models indeed give a
higher B score than the full bigram and trigram
models. The approaches we use are:
</bodyText>
<listItem confidence="0.87743075">
• RM-A: We keep only those contexts in which
more than P unique rules were observed. By
optimizing on the development set, we set P =
12.
• RM-B: We keep only those contexts that were
observed more than P times. Note that this is a
superset of RM-A. Again, by optimizing on the
development set, we set P = 12.
• RM-C: We try a more principled approach
for learning variable-length Markov models in-
spired by that of Bejerano and Yona (1999),
who learn a Prediction Suffix Tree (PST). They
grow the PST in an iterative manner by start-
ing from the root node (no context), and then
add contexts to the tree. A context is added if
the KL divergence between its predictive distri-
bution and that of its parent is above a certain
threshold and the probability of observing the
context is above another threshold.
3 Tree-to-string decoding with rule
</listItem>
<subsectionHeader confidence="0.650643">
Markov models
</subsectionHeader>
<bodyText confidence="0.999970538461538">
In this paper, we use our rule Markov model frame-
work in the context of tree-to-string translation.
Tree-to-string translation systems (Liu et al., 2006;
Huang et al., 2006) have gained popularity in recent
years due to their speed and simplicity. The input to
the translation system is a source parse tree and the
output is the target string. Huang and Mi (2010) have
recently introduced an efficient incremental decod-
ing algorithm for tree-to-string translation. The de-
coder operates top-down and maintains a derivation
history of translation rules encountered. The history
is exactly the vertical chain of ancestors correspond-
ing to the contexts in our rule Markov model. This
</bodyText>
<figure confidence="0.591496">
IP@ǫ
</figure>
<figureCaption confidence="0.999153">
Figure 3: Example input parse tree with tree addresses.
</figureCaption>
<bodyText confidence="0.999832060606061">
makes incremental decoding a natural fit with our
generative story. In this section, we describe how
to integrate our rule Markov model into this in-
cremental decoding algorithm. Note that it is also
possible to integrate our rule Markov model with
other decoding algorithms, for example, the more
common non-incremental top-down/bottom-up ap-
proach (Huang et al., 2006), but it would involve
a non-trivial change to the decoding algorithms to
keep track of the vertical derivation history, which
would result in significant overhead.
Algorithm Given the input parse tree in Figure 3,
Figure 4 illustrates the search process of the incre-
mental decoder with the grammar of Figure 1. We
write X@η for a tree node with label X at tree address
η (Shieber et al., 1995). The root node has address ǫ,
and the ith child of node η has address η.i. At each
step, the decoder maintains a stack of active rules,
which are rules that have not been completed yet,
and the rightmost (n − 1) English words translated
thus far (the hypothesis), where n is the order of the
word language model (in Figure 4, n = 2). The stack
together with the translated English words comprise
a state of the decoder. The last column in the fig-
ure shows the rule Markov model probabilities with
the conditioning context. In this example, we use a
trigram rule Markov model.
After initialization, the process starts at step 1,
where we predict rule r1 (the shaded rule) with prob-
ability P(r1  |ǫ) and push its English side onto the
stack, with variables replaced by the correspond-
ing tree nodes: x1 becomes NP@1 and x2 becomes
VP@2. This gives us the following stack:
</bodyText>
<figure confidence="0.966124090909091">
s = [. NP@1 VP@2]
The dot (.) indicates the next symbol to process in
NP@1
VP@2
P@2.1.1
NP@2.1.2
yˇu Sh¯al´ong
B`ushi
VP@2.2
PP@2.1
VV@2.2.1
jˇuxing
AS@2.2.2
le
NP@2.2.3
huit´an
859
stack hyp. MR prob.
0 [&lt;s&gt; . IP@E &lt;/s&gt;] &lt;s&gt;
1 [&lt;s&gt; . IP@E &lt;/s&gt;] [. NP@1 VP@2] &lt;s&gt; P(r1  |E)
2 [&lt;s&gt; . IP@E &lt;/s&gt;] [. NP@1 VP@2] [. Bush] &lt;s&gt; P(r2  |r1)
3 [&lt;s&gt; . IP@E &lt;/s&gt;] [. NP@1 VP@2] [Bush. ] ... Bush
4 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] ... Bush
5 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [. VP@2.2 PP@2.1] ... Bush P(r3  |r1)
6 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [. VP@2.2 PP@2.1] [. held talks] ... Bush P(r5  |r1,r3)
7 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [. VP@2.2 PP@2.1] [ held . talks] ... held
8 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [. VP@2.2 PP@2.1] [ held talks . ] ... talks
9 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] ... talks
10 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [. P@2.1.1 NP@2.1.2] ... talks P(r4  |r1,r3)
11 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [. P@2.1.1 NP@2.1.2] [. with] ... with P(r6  |r3, r4)
12 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [. P@2.1.1 NP@2.1.2] [with . ] ... with
13 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [P@2.1.1 . NP@2.1.2] ... with
14 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [P@2.1.1 . NP@2.1.2] [.Sharon] ... with P(r7  |r3, r4)
11′ [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [. P@2.1.1 NP@2.1.2] [. and]
... and P(r′6  |r3, r4)
12′ [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [. P@2.1.1 NP@2.1.2] [and . ] ... and
13′ [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [P@2.1.1 . NP@2.1.2] ... and
14′ [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [P@2.1.1 . NP@2.1.2] [. Sharon] ... and P(r7  |r3, r4)
15 [&lt;s&gt; . IP@E &lt;/s&gt;][NP@1 . VP@2] [VP@2.2 . PP@2.1] [P@2.1.1 . NP@2.1.2] [Sharon . ] ... Sharon
16 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 . PP@2.1] [P@2.1.1 NP@2.1.2 . ] ... Sharon
17 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 . VP@2] [VP@2.2 PP@2.1 . ] ... Sharon
18 [&lt;s&gt; . IP@E &lt;/s&gt;] [NP@1 VP@2 . ] ... Sharon
19 [&lt;s&gt; IP@E . &lt;/s&gt;] ... Sharon
20 [&lt;s&gt; IP@E &lt;/s&gt; . ] ... &lt;/s&gt;
</figure>
<figureCaption confidence="0.9897525">
Figure 4: Simulation of incremental decoding with rule Markov model. The solid arrows indicate one path and the
dashed arrows indicate an alternate path.
</figureCaption>
<page confidence="0.79064">
860
</page>
<figure confidence="0.73396">
yˇu
</figure>
<figureCaption confidence="0.9983225">
Figure 5: Vertical context r3 r4 which allows the model
to correctly translate yˇu as with.
</figureCaption>
<bodyText confidence="0.985549121951219">
the English word order. We expand node NP@1 first
with English word order. We then predict lexical rule
r2 with probability P(r2  |r1) and push rule r2 onto
the stack:
[• NP@1 VP@2 ] [. Bush]
In step 3, we perform a scan operation, in which
we append the English word just after the dot to the
current hypothesis and move the dot after the word.
Since the dot is at the end of the top rule in the stack,
we perform a complete operation in step 4 where we
pop the finished rule at the top of the stack. In the
scan and complete steps, we don’t need to compute
rule probabilities.
An interesting branch occurs after step 10 with
two competing lexical rules, r6 and r′6. The Chinese
word yˇu can be translated as either a preposition with
(leading to step 11) or a conjunction and (leading
to step 11′). The word n-gram model does not have
enough information to make the correct choice, with.
As a result, good translations might be pruned be-
cause of the beam. However, our rule Markov model
has the correct preference because of the condition-
ing ancestral sequence (r3, r4), shown in Figure 5.
Since VP@2.2 has a preference for yˇu translating to
with, our corpus statistics will give a higher proba-
bility to P(r6  |r3, r4) than P(r′6  |r3, r4). This helps
the decoder to score the correct translation higher.
Complexity analysis With the incremental decod-
ing algorithm, adding rule Markov models does not
change the time complexity, which is O(nc|V|g−1),
where n is the sentence length, c is the maximum
number of incoming hyperedges for each node in the
translation forest, V is the target-language vocabu-
lary, and g is the order of the n-gram language model
(Huang and Mi, 2010). However, if one were to use
rule Markov models with a conventional CKY-style
bottom-up decoder (Liu et al., 2006), the complexity
would increase to O(nCm−1|V|4(g−1)), where C is the
maximum number of outgoing hyperedges for each
node in the translation forest, and m is the order of
the rule Markov model.
</bodyText>
<sectionHeader confidence="0.986453" genericHeader="introduction">
4 Experiments and results
</sectionHeader>
<subsectionHeader confidence="0.964537">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999713206896552">
The training corpus consists of 1.5M sentence pairs
with 38M/32M words of Chinese/English, respec-
tively. Our development set is the newswire portion
of the 2006 NIST MT Evaluation test set (616 sen-
tences), and our test set is the newswire portion of
the 2008 NIST MT Evaluation test set (691 sen-
tences).
We word-aligned the training data using GIZA++
followed by link deletion (Fossum et al., 2008),
and then parsed the Chinese sentences using the
Berkeley parser (Petrov and Klein, 2007). To extract
tree-to-string translation rules, we applied the algo-
rithm of Galley et al. (2004). We trained our rule
Markov model on derivations of minimal rules as
described above. Our trigram word language model
was trained on the target side of the training cor-
pus using the SRILM toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing. The base feature
set for all systems is similar to the set used in Mi et
al. (2008). The features are combined into a standard
log-linear model, which we trained using minimum
error-rate training (Och, 2003) to maximize the B
score on the development set.
At decoding time, we again parse the input
sentences using the Berkeley parser, and convert
them into translation forests using rule pattern-
matching (Mi et al., 2008). We evaluate translation
quality using case-insensitive IBM B -4, calcu-
lated by the script mteval-v13a.pl.
</bodyText>
<sectionHeader confidence="0.715666" genericHeader="background">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999830125">
Table 1 presents the main results of our paper. We
used grammars of minimal rules and composed rules
of maximum height 3 as our baselines. For decod-
ing, we used a beam size of 50. Using the best
bigram rule Markov models and the minimal rule
grammar gives us an improvement of 1.5 B over
the minimal rule baseline. Using the best trigram
rule Markov model brings our gain up to 2.3 B .
</bodyText>
<equation confidence="0.983856">
VP@2
VP@2.2 PP@2.1
P@2.1.1
NP@2.1.2
</equation>
<page confidence="0.993701">
861
</page>
<table confidence="0.999047111111111">
grammar rule Markov max parameters (×106) B time
model rule height full dev+test test (sec/sent)
minimal None 3 4.9 0.3 24.2 1.2
RM-B bigram 3 4.9+4.7 0.3+0.5 25.7 1.8
RM-A trigram 3 4.9+7.6 0.3+0.6 26.5 2.0
vertical composed None 7 176.8 1.3 26.5 2.9
composed None 3 17.5 1.6 26.4 2.2
None 7 448.7 3.3 27.5 6.8
RM-A trigram 7 448.7+7.6 3.3+1.0 28.0 9.2
</table>
<tableCaption confidence="0.999719">
Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same
</tableCaption>
<bodyText confidence="0.993714314285714">
level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for
both the full model and the model filtered for the concatenation of the development and test sets (dev+test).
These gains are statistically significant with p &lt;
0.01, using bootstrap resampling with 1000 samples
(Koehn, 2004). We find that by just using bigram
context, we are able to get at least 1 B point
higher than the minimal rule grammar. It is interest-
ing to see that using just bigram rule interactions can
give us a reasonable boost. We get our highest gains
from using trigram context where our best perform-
ing rule Markov model gives us 2.3 B points over
minimal rules. This suggests that using longer con-
texts helps the decoder to find better translations.
We also compared rule Markov models against
composed rules. Since our models are currently lim-
ited to conditioning on vertical context, the closest
comparison is against vertically composed rules. We
find that our approach performs equally well using
much less time and space.
Comparing against full composed rules, we find
that our system matches the score of the base-
line composed rule grammar of maximum height 3,
while using many fewer parameters. (It should be
noted that a parameter in the rule Markov model is
just a floating-point number, whereas a parameter in
the composed-rule system is an entire rule; there-
fore the difference in memory usage would be even
greater.) Decoding with our model is 0.2 seconds
faster per sentence than with composed rules.
These experiments clearly show that rule Markov
models with minimal rules increase translation qual-
ity significantly and with lower memory require-
ments than composed rules. One might wonder if
the best performance can be obtained by combin-
ing composed rules with a rule Markov model. This
</bodyText>
<table confidence="0.998838833333333">
rule Markov D1 B time
model dev (sec/sent)
RM-A 0.871 29.2 1.8
RM-B 0.4 29.9 1.8
RM-C 0.871 29.8 1.8
RM-Full 0.4 29.7 1.9
</table>
<tableCaption confidence="0.8788095">
Table 2: For rule bigrams, RM-B with D1 = 0.4 gives the
best results on the development set.
</tableCaption>
<table confidence="0.999891166666667">
rule Markov D1 D2 B time
model dev (sec/sent)
RM-A 0.5 0.5 30.3 2.0
RM-B 0.5 0.5 29.9 2.0
RM-C 0.5 0.5 30.1 2.0
RM-Full 0.4 0.5 30.1 2.2
</table>
<tableCaption confidence="0.971929">
Table 3: For rule bigrams, RM-A with D1, D2 = 0.5 gives
the best results on the development set.
</tableCaption>
<bodyText confidence="0.9999649">
is straightforward to implement: the rule Markov
model is still defined over derivations of minimal
rules, but in the decoder’s prediction step, the rule
Markov model’s value on a composed rule is cal-
culated by decomposing it into minimal rules and
computing the product of their probabilities. We find
that using our best trigram rule Markov model with
composed rules gives us a 0.5 B gain on top of
the composed rule grammar, statistically significant
with p &lt; 0.05, achieving our highest score of 28.0.1
</bodyText>
<subsectionHeader confidence="0.997019">
4.3 Analysis
</subsectionHeader>
<bodyText confidence="0.856757">
Tables 2 and 3 show how the various types of rule
Markov models compare, for bigrams and trigrams,
</bodyText>
<footnote confidence="0.992986">
1For this experiment, a beam size of 100 was used.
</footnote>
<page confidence="0.985083">
862
</page>
<table confidence="0.999797">
parameters (×106) B dev/test time (sec/sent)
dev/test without RMM with RMM without/with RMM
2.6 31.0/27.0 31.1/27.4 4.5/7.0
2.9 31.5/27.7 31.4/27.3 5.6/8.1
3.3 31.4/27.5 31.4/28.0 6.8/9.2
</table>
<tableCaption confidence="0.9812">
Table 6: Adding rule Markov models to composed-rule grammars improves their translation performance.
</tableCaption>
<table confidence="0.9997824">
D2 0.4 D1 0.871
0.5
0.4 30.0 30.0
0.5 29.3 30.3
0.902 30.0
</table>
<tableCaption confidence="0.992553">
Table 4: RM-A is robust to different settings of Dn on the
development set.
</tableCaption>
<table confidence="0.999387">
parameters (×106) B test time
dev+test dev (sec/sent)
1.2 30.2 26.1 2.8
1.3 30.1 26.5 2.9
1.3 30.1 26.2 3.2
</table>
<tableCaption confidence="0.993727">
Table 5: Comparison of vertically composed rules using
various settings (maximum rule height 7).
</tableCaption>
<bodyText confidence="0.999727294117647">
respectively. It is interesting that the full bigram and
trigram rule Markov models do not give our high-
est B scores; pruning the models not only saves
space but improves their performance. We think that
this is probably due to overfitting.
Table 4 shows that the RM-A trigram model does
fairly well under all the settings of Dn we tried. Ta-
ble 5 shows the performance of vertically composed
rules at various settings. Here we have chosen the
setting that gives the best performance on the test
set for inclusion in Table 1.
Table 6 shows the performance offully composed
rules and fully composed rules with a rule Markov
Model at various settings.2 In the second line (2.9
million rules), the drop in B score resulting from
adding the rule Markov model is not statistically sig-
nificant.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.974816857142857">
Besides the Quirk and Menezes (2006) work dis-
cussed in Section 1, there are two other previous
2For these experiments, a beam size of 100 was used.
efforts both using a rule bigram model in machine
translation, that is, the probability of the current rule
only depends on the immediate previous rule in the
vertical context, whereas our rule Markov model
can condition on longer and sparser derivation his-
tories. Among them, Ding and Palmer (2005) also
use a dependency treelet model similar to Quirk and
Menezes (2006), and Liu and Gildea (2008) use a
tree-to-string model more like ours. Neither com-
pared to the scenario with composed rules.
Outside of machine translation, the idea of weak-
ening independence assumptions by modeling the
derivation history is also found in parsing (Johnson,
1998), where rule probabilities are conditioned on
parent and grand-parent nonterminals. However, be-
sides the difference between parsing and translation,
there are still two major differences. First, our work
conditions rule probabilities on parent and grandpar-
ent rules, not just nonterminals. Second, we com-
pare against a composed-rule system, which is anal-
ogous to the Data Oriented Parsing (DOP) approach
in parsing (Bod, 2003). To our knowledge, there has
been no direct comparison between a history-based
PCFG approach and DOP approach in the parsing
literature.
</bodyText>
<sectionHeader confidence="0.999248" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.995099583333333">
In this paper, we have investigated whether we can
eliminate composed rules without any loss in trans-
lation quality. We have developed a rule Markov
model that captures vertical bigrams and trigrams of
minimal rules, and tested it in the framework of tree-
to-string translation. We draw three main conclu-
sions from our experiments. First, our rule Markov
models dramatically improve a grammar of minimal
rules, giving an improvement of 2.3 B . Second,
when we compare against vertically composed rules
we are able to get about the same B score, but
our model is much smaller and decoding with our
</bodyText>
<page confidence="0.995834">
863
</page>
<bodyText confidence="0.999972888888889">
model is faster. Finally, when we compare against
full composed rules, we find that we can reach the
same level of performance under some conditions,
but in order to do so consistently, we believe we
need to extend our model to condition on horizon-
tal context in addition to vertical context. We hope
that by modeling context in both axes, we will be
able to completely replace composed-rule grammars
with smaller minimal-rule grammars.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999965090909091">
We would like to thank Fernando Pereira, Yoav
Goldberg, Michael Pust, Steve DeNeefe, Daniel
Marcu and Kevin Knight for their comments.
Mi’s contribution was made while he was vis-
iting USC/ISI. This work was supported in part
by DARPA under contracts HR0011-06-C-0022
(subcontract to BBN Technologies), HR0011-09-1-
0028, and DOI-NBC N10AP20031, by a Google
Faculty Research Award to Huang, and by the Na-
tional Natural Science Foundation of China under
contracts 60736014 and 90920004.
</bodyText>
<sectionHeader confidence="0.999099" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999570296875">
Gill Bejerano and Golan Yona. 1999. Modeling pro-
tein families using probabilistic suffix trees. In Proc.
RECOMB, pages 15–24. ACM Press.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of EACL, pages 19–26.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings ofACL, pages 541–
548.
Victoria Fossum, Kevin Knight, and Steve Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings ofHLT-NAACL, pages 273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961–968.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273–283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, pages
66–73.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24:613–
632.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388–395.
Ding Liu and Daniel Gildea. 2008. Improved tree-to-
string transducer for machine translation. In Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 62–69.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609–
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings ofACL: HLT, pages
192–199.
H. Ney, U. Essen, and R. Kneser. 1994. On structur-
ing probabilistic dependencies in stochastic language
modelling. Computer Speech and Language, 8:1–38.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings ofACL,
pages 160–167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, pages 404–411.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings ofNAACL
HLT, pages 9–16.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal ofLogic Programming, 24:3–36.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings ofICSLP, vol-
ume 30, pages 901–904.
</reference>
<page confidence="0.998623">
864
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.133693">
<title confidence="0.999188">Rule Markov Models for Fast Tree-to-String Translation</title>
<author confidence="0.738489">Ashish</author>
<affiliation confidence="0.9772425">Information Sciences University of Southern</affiliation>
<email confidence="0.998908">avaswani@isi.edu</email>
<author confidence="0.486002">Haitao</author>
<affiliation confidence="0.896461">Institute of Computing</affiliation>
<address confidence="0.532957">Chinese Academy of</address>
<web confidence="0.639344">htmi@ict.ac.cn</web>
<author confidence="0.98304">Huang Chiang</author>
<affiliation confidence="0.999457">Information Sciences Institute University of Southern California</affiliation>
<email confidence="0.99981">lhuang@isi.edu</email>
<email confidence="0.99981">chiang@isi.edu</email>
<abstract confidence="0.998996631578947">Most statistical machine translation systems on rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both trainand decoding Here, we take the approach, where we only use minrules that cannot be formed out other rules), and instead rely on a model the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gill Bejerano</author>
<author>Golan Yona</author>
</authors>
<title>Modeling protein families using probabilistic suffix trees.</title>
<date>1999</date>
<booktitle>In Proc. RECOMB,</booktitle>
<pages>15--24</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="9436" citStr="Bejerano and Yona (1999)" startWordPosition="1594" endWordPosition="1597">aller models to investigate if pruning helps. Our results will show that smaller models indeed give a higher B score than the full bigram and trigram models. The approaches we use are: • RM-A: We keep only those contexts in which more than P unique rules were observed. By optimizing on the development set, we set P = 12. • RM-B: We keep only those contexts that were observed more than P times. Note that this is a superset of RM-A. Again, by optimizing on the development set, we set P = 12. • RM-C: We try a more principled approach for learning variable-length Markov models inspired by that of Bejerano and Yona (1999), who learn a Prediction Suffix Tree (PST). They grow the PST in an iterative manner by starting from the root node (no context), and then add contexts to the tree. A context is added if the KL divergence between its predictive distribution and that of its parent is above a certain threshold and the probability of observing the context is above another threshold. 3 Tree-to-string decoding with rule Markov models In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gaine</context>
</contexts>
<marker>Bejerano, Yona, 1999</marker>
<rawString>Gill Bejerano and Golan Yona. 1999. Modeling protein families using probabilistic suffix trees. In Proc. RECOMB, pages 15–24. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An efficient implementation of a new DOP model.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>pages</pages>
<contexts>
<context position="24417" citStr="Bod, 2003" startWordPosition="4216" endWordPosition="4217">scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). To our knowledge, there has been no direct comparison between a history-based PCFG approach and DOP approach in the parsing literature. 6 Conclusion In this paper, we have investigated whether we can eliminate composed rules without any loss in translation quality. We have developed a rule Markov model that captures vertical bigrams and trigrams of minimal rules, and tested it in the framework of treeto-string translation. We draw three main conclusions from our experiments. First, our rule Markov models dramatically improve a grammar of minimal rules, giving an improvement of 2.3 B . Second</context>
</contexts>
<marker>Bod, 2003</marker>
<rawString>Rens Bod. 2003. An efficient implementation of a new DOP model. In Proceedings of EACL, pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probablisitic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="23640" citStr="Ding and Palmer (2005)" startWordPosition="4095" endWordPosition="4098">at various settings.2 In the second line (2.9 million rules), the drop in B score resulting from adding the rule Markov model is not statistically significant. 5 Related Work Besides the Quirk and Menezes (2006) work discussed in Section 1, there are two other previous 2For these experiments, a beam size of 100 was used. efforts both using a rule bigram model in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and g</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probablisitic synchronous dependency insertion grammars. In Proceedings ofACL, pages 541– 548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steve Abney</author>
</authors>
<title>Using syntax to improve word alignment precision for syntax-based machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="16931" citStr="Fossum et al., 2008" startWordPosition="2948" endWordPosition="2951">omplexity would increase to O(nCm−1|V|4(g−1)), where C is the maximum number of outgoing hyperedges for each node in the translation forest, and m is the order of the rule Markov model. 4 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained usin</context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steve Abney. 2008. Using syntax to improve word alignment precision for syntax-based machine translation. In Proceedings of the Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="2709" citStr="Galley et al. (2004)" startWordPosition="408" endWordPosition="411">ause a combinatorial explosion in the number of rules. To avoid this, ad-hoc limits are placed during composition, like upper bounds on the number of nodes in the composed rule, or the height of the rule. Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar. Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down. Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules. For example, Galley et al. (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves B by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk an</context>
<context position="17116" citStr="Galley et al. (2004)" startWordPosition="2977" endWordPosition="2980"> 4 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the B score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert the</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings ofHLT-NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="2821" citStr="Galley et al., 2006" startWordPosition="424" endWordPosition="427">n, like upper bounds on the number of nodes in the composed rule, or the height of the rule. Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar. Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down. Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules. For example, Galley et al. (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves B by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk and Menezes (2006) takes such an approach within the framework of dependency translation. However, their study lea</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Haitao Mi</author>
</authors>
<title>Efficient incremental decoding for tree-to-string translation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>273--283</pages>
<contexts>
<context position="10215" citStr="Huang and Mi (2010)" startWordPosition="1728" endWordPosition="1731">ree. A context is added if the KL divergence between its predictive distribution and that of its parent is above a certain threshold and the probability of observing the context is above another threshold. 3 Tree-to-string decoding with rule Markov models In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gained popularity in recent years due to their speed and simplicity. The input to the translation system is a source parse tree and the output is the target string. Huang and Mi (2010) have recently introduced an efficient incremental decoding algorithm for tree-to-string translation. The decoder operates top-down and maintains a derivation history of translation rules encountered. The history is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model. This IP@ǫ Figure 3: Example input parse tree with tree addresses. makes incremental decoding a natural fit with our generative story. In this section, we describe how to integrate our rule Markov model into this incremental decoding algorithm. Note that it is also possible to integrate o</context>
<context position="16189" citStr="Huang and Mi, 2010" startWordPosition="2824" endWordPosition="2827">ence (r3, r4), shown in Figure 5. Since VP@2.2 has a preference for yˇu translating to with, our corpus statistics will give a higher probability to P(r6 |r3, r4) than P(r′6 |r3, r4). This helps the decoder to score the correct translation higher. Complexity analysis With the incremental decoding algorithm, adding rule Markov models does not change the time complexity, which is O(nc|V|g−1), where n is the sentence length, c is the maximum number of incoming hyperedges for each node in the translation forest, V is the target-language vocabulary, and g is the order of the n-gram language model (Huang and Mi, 2010). However, if one were to use rule Markov models with a conventional CKY-style bottom-up decoder (Liu et al., 2006), the complexity would increase to O(nCm−1|V|4(g−1)), where C is the maximum number of outgoing hyperedges for each node in the translation forest, and m is the order of the rule Markov model. 4 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 20</context>
</contexts>
<marker>Huang, Mi, 2010</marker>
<rawString>Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings of EMNLP, pages 273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="10025" citStr="Huang et al., 2006" startWordPosition="1694" endWordPosition="1697">at of Bejerano and Yona (1999), who learn a Prediction Suffix Tree (PST). They grow the PST in an iterative manner by starting from the root node (no context), and then add contexts to the tree. A context is added if the KL divergence between its predictive distribution and that of its parent is above a certain threshold and the probability of observing the context is above another threshold. 3 Tree-to-string decoding with rule Markov models In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gained popularity in recent years due to their speed and simplicity. The input to the translation system is a source parse tree and the output is the target string. Huang and Mi (2010) have recently introduced an efficient incremental decoding algorithm for tree-to-string translation. The decoder operates top-down and maintains a derivation history of translation rules encountered. The history is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model. This IP@ǫ Figure 3: Example input parse tree with tree addresses. makes incremental decoding a nat</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of AMTA, pages 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>632</pages>
<contexts>
<context position="23991" citStr="Johnson, 1998" startWordPosition="4153" endWordPosition="4154">el in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). To our knowledge, there has been no direct comparison between a history-based PCFG approach and DOP approach in the parsing literature. 6 Conclusion In this paper, we have </context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24:613– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="19140" citStr="Koehn, 2004" startWordPosition="3324" endWordPosition="3325">0.6 26.5 2.0 vertical composed None 7 176.8 1.3 26.5 2.9 composed None 3 17.5 1.6 26.4 2.2 None 7 448.7 3.3 27.5 6.8 RM-A trigram 7 448.7+7.6 3.3+1.0 28.0 9.2 Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for both the full model and the model filtered for the concatenation of the development and test sets (dev+test). These gains are statistically significant with p &lt; 0.01, using bootstrap resampling with 1000 samples (Koehn, 2004). We find that by just using bigram context, we are able to get at least 1 B point higher than the minimal rule grammar. It is interesting to see that using just bigram rule interactions can give us a reasonable boost. We get our highest gains from using trigram context where our best performing rule Markov model gives us 2.3 B points over minimal rules. This suggests that using longer contexts helps the decoder to find better translations. We also compared rule Markov models against composed rules. Since our models are currently limited to conditioning on vertical context, the closest compari</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Improved tree-tostring transducer for machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>62--69</pages>
<contexts>
<context position="23739" citStr="Liu and Gildea (2008)" startWordPosition="4112" endWordPosition="4115">ing the rule Markov model is not statistically significant. 5 Related Work Besides the Quirk and Menezes (2006) work discussed in Section 1, there are two other previous 2For these experiments, a beam size of 100 was used. efforts both using a rule bigram model in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which i</context>
</contexts>
<marker>Liu, Gildea, 2008</marker>
<rawString>Ding Liu and Daniel Gildea. 2008. Improved tree-tostring transducer for machine translation. In Proceedings of the Workshop on Statistical Machine Translation, pages 62–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="3815" citStr="Liu et al., 2006" startWordPosition="583" endWordPosition="586"> role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk and Menezes (2006) takes such an approach within the framework of dependency translation. However, their study leaves unanswered whether a rule Markov model can take the place of composed rules. In this work, we investigate the use of rule Markov models in the context of tree856 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856–864, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics to-string translation (Liu et al., 2006; Huang et al., The derivation tree is generated as follows. With 2006). We make three new contributions. probability P(r1 |c), we generate the rule at the root First, we carry out a detailed comparison of rule node, r1. We then generate rule r2 with probability Markov models with composed rules. Our experi- P(r2 |r1), and so on, always taking the leftmost open ments show that, using trigram rule Markov mod- substitution site on the English derived tree, and genels, we achieve an improvement of 2.2 B over erating a rule ri conditioned on its chain of ancestors a baseline of minimal rules. When</context>
<context position="10004" citStr="Liu et al., 2006" startWordPosition="1690" endWordPosition="1693">els inspired by that of Bejerano and Yona (1999), who learn a Prediction Suffix Tree (PST). They grow the PST in an iterative manner by starting from the root node (no context), and then add contexts to the tree. A context is added if the KL divergence between its predictive distribution and that of its parent is above a certain threshold and the probability of observing the context is above another threshold. 3 Tree-to-string decoding with rule Markov models In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gained popularity in recent years due to their speed and simplicity. The input to the translation system is a source parse tree and the output is the target string. Huang and Mi (2010) have recently introduced an efficient incremental decoding algorithm for tree-to-string translation. The decoder operates top-down and maintains a derivation history of translation rules encountered. The history is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model. This IP@ǫ Figure 3: Example input parse tree with tree addresses. makes incre</context>
<context position="16304" citStr="Liu et al., 2006" startWordPosition="2843" endWordPosition="2846">ll give a higher probability to P(r6 |r3, r4) than P(r′6 |r3, r4). This helps the decoder to score the correct translation higher. Complexity analysis With the incremental decoding algorithm, adding rule Markov models does not change the time complexity, which is O(nc|V|g−1), where n is the sentence length, c is the maximum number of incoming hyperedges for each node in the translation forest, V is the target-language vocabulary, and g is the order of the n-gram language model (Huang and Mi, 2010). However, if one were to use rule Markov models with a conventional CKY-style bottom-up decoder (Liu et al., 2006), the complexity would increase to O(nCm−1|V|4(g−1)), where C is the maximum number of outgoing hyperedges for each node in the translation forest, and m is the order of the rule Markov model. 4 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link del</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of COLING-ACL, pages 609– 616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL: HLT,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="17448" citStr="Mi et al. (2008)" startWordPosition="3035" endWordPosition="3038">es). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the B score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule patternmatching (Mi et al., 2008). We evaluate translation quality using case-insensitive IBM B -4, calculated by the script mteval-v13a.pl. 4.2 Results Table 1 presents the main results of our paper. We used grammars of minimal rules and composed rules of maximum height 3 as our baselines. Fo</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings ofACL: HLT, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>U Essen</author>
<author>R Kneser</author>
</authors>
<title>On structuring probabilistic dependencies in stochastic language modelling. Computer Speech and Language,</title>
<date>1994</date>
<pages>8--1</pages>
<contexts>
<context position="6129" citStr="Ney et al., 1994" startWordPosition="980" endWordPosition="983">g grammars with rule Markov models. Huang pair. (Unaligned words are handled by attaching and Mi (2010) have recently introduced an efficient them to the highest node possible in the parse tree.) incremental decoding algorithm for tree-to-string The rule Markov model translation, which operates top-down and maintains can then be trained on the path set of these derivaa derivation history of translation rules encountered. tion trees. This history is exactly the vertical chain of ancestors Smoothing We use interpolation with absolute corresponding to the contexts in our rule Markov discounting (Ney et al., 1994): model, which makes it an ideal decoder for our Pabs(r |ancn1(r)) = max c(r |ancn 1(r)) − Dn, 0 model. Er′ c(r′ |ancn1(r′)) We start by describing our rule Markov model { 1 (Section 2) and then how to decode using the rule + (1 − An)Pabs(r |ancn−1 Markov model (Section 3). 1 (r)), (3) 2 Rule Markov models where c(r |ancn 1(r)) is the number of times we have Our model which conditions the generation of a rule seen rule r after the vertical context ancn1(r), Dn is on the vertical chain of its ancestors, which allows it the discount for a context of length n, and (1 − An) is to capture interacti</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>H. Ney, U. Essen, and R. Kneser. 1994. On structuring probabilistic dependencies in stochastic language modelling. Computer Speech and Language, 8:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="17572" citStr="Och, 2003" startWordPosition="3055" endWordPosition="3056">entences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the B score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule patternmatching (Mi et al., 2008). We evaluate translation quality using case-insensitive IBM B -4, calculated by the script mteval-v13a.pl. 4.2 Results Table 1 presents the main results of our paper. We used grammars of minimal rules and composed rules of maximum height 3 as our baselines. For decoding, we used a beam size of 50. Using the best bigram rule Markov models and the minimal rule grammar gives us an imp</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="17021" citStr="Petrov and Klein, 2007" startWordPosition="2962" endWordPosition="2965">g hyperedges for each node in the translation forest, and m is the order of the rule Markov model. 4 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the B score on the development set. </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Do we need phrases? Challenging the conventional wisdom in statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofNAACL HLT,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="3325" citStr="Quirk and Menezes (2006)" startWordPosition="509" endWordPosition="512">. (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves B by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk and Menezes (2006) takes such an approach within the framework of dependency translation. However, their study leaves unanswered whether a rule Markov model can take the place of composed rules. In this work, we investigate the use of rule Markov models in the context of tree856 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856–864, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics to-string translation (Liu et al., 2006; Huang et al., The derivation tree is generated as follows. With 2006). We make three new contributions. prob</context>
<context position="23229" citStr="Quirk and Menezes (2006)" startWordPosition="4025" endWordPosition="4028">bably due to overfitting. Table 4 shows that the RM-A trigram model does fairly well under all the settings of Dn we tried. Table 5 shows the performance of vertically composed rules at various settings. Here we have chosen the setting that gives the best performance on the test set for inclusion in Table 1. Table 6 shows the performance offully composed rules and fully composed rules with a rule Markov Model at various settings.2 In the second line (2.9 million rules), the drop in B score resulting from adding the rule Markov model is not statistically significant. 5 Related Work Besides the Quirk and Menezes (2006) work discussed in Section 1, there are two other previous 2For these experiments, a beam size of 100 was used. efforts both using a rule bigram model in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Do we need phrases? Challenging the conventional wisdom in statistical machine translation. In Proceedings ofNAACL HLT, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<booktitle>Journal ofLogic Programming,</booktitle>
<pages>24--3</pages>
<contexts>
<context position="11355" citStr="Shieber et al., 1995" startWordPosition="1910" endWordPosition="1913">this incremental decoding algorithm. Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al., 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. Algorithm Given the input parse tree in Figure 3, Figure 4 illustrates the search process of the incremental decoder with the grammar of Figure 1. We write X@η for a tree node with label X at tree address η (Shieber et al., 1995). The root node has address ǫ, and the ith child of node η has address η.i. At each step, the decoder maintains a stack of active rules, which are rules that have not been completed yet, and the rightmost (n − 1) English words translated thus far (the hypothesis), where n is the order of the word language model (in Figure 4, n = 2). The stack together with the translated English words comprise a state of the decoder. The last column in the figure shows the rule Markov model probabilities with the conditioning context. In this example, we use a trigram rule Markov model. After initialization, t</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal ofLogic Programming, 24:3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP,</booktitle>
<volume>30</volume>
<pages>901--904</pages>
<contexts>
<context position="17328" citStr="Stolcke, 2002" startWordPosition="3015" endWordPosition="3016">est set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the B score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule patternmatching (Mi et al., 2008). We evaluate translation quality using case-insensitive IBM B -4, calculated by the script mteval-v13a.pl. 4.2 Results Table 1 presents the </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings ofICSLP, volume 30, pages 901–904.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>