<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005327">
<title confidence="0.968956">
Context-dependent SMT Model using Bilingual Verb-Noun Collocation
</title>
<author confidence="0.927607">
Young-Sook Hwang
</author>
<affiliation confidence="0.685091">
ATR SLT Research Labs
</affiliation>
<address confidence="0.856612">
2-2-2 Hikaridai Seika-cho
Soraku-gun Kyoto, 619-0288, JAPAN
</address>
<email confidence="0.996779">
youngsook.hwang@atr.jp
</email>
<author confidence="0.511884">
Yutaka Sasaki
</author>
<affiliation confidence="0.380299">
ATR SLT Research Labs
</affiliation>
<address confidence="0.747036">
2-2-2 Hikaridai Seika-cho
Soraku-gun Kyoto, 619-0288, JAPAN
</address>
<email confidence="0.997321">
yutaka.sasaki@atr.jp
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958423076923">
In this paper, we propose a new context-
dependent SMT model that is tightly cou-
pled with a language model. It is de-
signed to decrease the translation ambi-
guities and efficiently search for an opti-
mal hypothesis by reducing the hypothe-
sis search space. It works through recipro-
cal incorporation between source and tar-
get context: a source word is determined
by the context of previous and correspond-
ing target words and the next target word
is predicted by the pair consisting of the
previous target word and its correspond-
ing source word. In order to alleviate
the data sparseness in chunk-based trans-
lation, we take a stepwise back-off trans-
lation strategy. Moreover, in order to ob-
tain more semantically plausible transla-
tion results, we use bilingual verb-noun
collocations; these are automatically ex-
tracted by using chunk alignment and a
monolingual dependency parser. As a case
study, we experimented on the language
pair of Japanese and Korean. As a result,
we could not only reduce the search space
but also improve the performance.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932111111111">
For decades, many research efforts have contributed
to the advance of statistical machine translation.
Recently, various works have improved the quality
of statistical machine translation systems by using
phrase translation (Koehn et al., 2003; Marcu et al.,
2002; Och et al., 1999; Och and Ney, 2000; Zens
et al., 2004). Most of the phrase-based translation
models have adopted the noisy-channel based IBM
style models (Brown et al., 1993):
</bodyText>
<equation confidence="0.998411">
ei = argmaxe1Pr(fJ Iei)Pr(ef) (1)
</equation>
<bodyText confidence="0.999969269230769">
In these model, we have two types of knowledge:
translation model, Pr(f1 Ie1) and language model,
Pr(eI). The translation model links the source lan-
guage sentence to the target language sentence. The
language model describes the well-formedness of
the target language sentence and might play a role
in restricting hypothesis expansion during decoding.
To recover the word order difference between two
languages, it also allows modeling the reordering by
introducing a relative distortion probability distribu-
tion. However, in spite of using such a language
model and a distortion model, the translation outputs
may not be fluent or in fact may produce nonsense.
To make things worse, the huge hypothesis search
space is much too large for an exhaustive search. If
arbitrary reorderings are allowed, the search prob-
lem is NP-complete (Knight, 1999). According
to a previous analysis (Koehn et al., 2004) of how
many hypotheses are generated during an exhaustive
search using the IBM models, the upper bound for
the number of states is estimated by N - 2j Ve�2J,
where J is the number of source words and IV�I is
the size of the target vocabulary. Even though the
number of possible translations of the last two words
is much smaller than V ~2, we still need to make
further improvement. The main concern is the ex-
</bodyText>
<page confidence="0.979524">
549
</page>
<note confidence="0.9916015">
Proceedings of the 43rd Annual Meeting of the ACL, pages 549–556,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999818040816326">
ponential explosion from the possible configurations
of source words covered by a hypothesis. In order
to reduce the number of possible configurations of
source words, decoding algorithms based on X as
well as the beam search algorithm have been pro-
posed (Koehn et al., 2004; Och et al., 2001). (Koehn
et al., 2004; Och et al., 2001) used heuristics for
pruning implausible hypotheses.
Our approach to this problem examines the pos-
sibility of utilizing context information in a given
language pair. Under a given target context, the cor-
responding source word of a given target word is al-
most deterministic. Conversely, if a translation pair
is given, then the related target or source context is
predictable. This implies that if we considered bilin-
gual context information in a given language pair
during decoding, we can reduce the computational
complexity of the hypothesis search; specifically, we
could reduce the possible configurations of source
words as well as the number of possible target trans-
lations.
In this study, we present a statistical machine
translation model as an alternative to the classical
IBM-style model. This model is tightly coupled
with target language model and utilizes bilingual
context information. It is designed to not only re-
duce the hypothesis search space by decreasing the
translation ambiguities but also improve translation
performance. It works through reciprocal incorpo-
ration between source and target context: source
words are determined by the context of previous
and corresponding target words, and the next target
words are predicted by the current translation pair.
Accordingly, we do not need to consider any dis-
tortion model or language model as is the case with
IBM-style models.
Under this framework, we propose a chunk-based
translation model for more grammatical, fluent and
accurate output. In order to alleviate the data sparse-
ness problem in chunk-based translation, we use a
stepwise back-off method in the order of a chunk,
sub-parts of the chunk, and word level. Moreover,
we utilize verb-noun collocations in dealing with
long-distance dependency which are automatically
extracted by using chunk alignment and a monolin-
gual dependency parser.
As a case study, we developed a Japanese-to-
Korean translation model and performed some ex-
periments on the BTEC corpus.
</bodyText>
<sectionHeader confidence="0.838692" genericHeader="method">
2 Overview of Translation Model
</sectionHeader>
<bodyText confidence="0.999949375">
The goal of machine translation is to transfer the
meaning of a source language sentence, f1 =
f, ... fj, into a target language sentence, e1 =
el ... er . In most types of statistical machine trans-
lation, conditional probability Pr(e1If1 ) is used to
describe the correspondence between two sentences.
This model is used directly for translation by solving
the following maximization problem:
</bodyText>
<equation confidence="0.999578">
e1 = argmaxe1Pr(eiIfi ) (2)
= argmaxei PP(( 1.ff)�) (3)
= argmaxelPr(e1, f1) (4)
</equation>
<bodyText confidence="0.999978375">
Since a source language sentence is given and the
Pr(f1 ) probability is applied to all possible corre-
sponding target sentences, we can ignore the denom-
inator in equation (3). As a result, the joint proba-
bility model can be used to describe the correspon-
dence between two sentences. We apply Markov
chain rules to the joint probability model and obtain
the following decomposed model:
</bodyText>
<equation confidence="0.994125666666667">
r
Pr(e1,f1) —_ Pr(fazIez, ez-1)Pr(eiIez-1faz-1)
Z=1
</equation>
<bodyText confidence="0.9392502">
(5)
where a2 is the index of the source word that is
aligned to the word e2 under the assumption of the
fixed one-to-one alignment. In this model, we have
two probabilities:
</bodyText>
<listItem confidence="0.6603375">
• source word prediction probability under a
given target language context, Pr(fazIe2-1, ei)
• target word prediction probability under the
preceding translation pair, Pr(e�Ie2-1, faz-1)
</listItem>
<bodyText confidence="0.9991505">
The probability of target word prediction is used for
selecting the target word that follows the previous
target words. In order to make this more determin-
istic, we use bilingual context, i.e. the translation
pair of the preceding target word. For a given target
word, the corresponding source word is predicted by
source word prediction probability based on the cur-
rent and preceding target words.
</bodyText>
<page confidence="0.972001">
550
</page>
<bodyText confidence="0.999956086956522">
Since a target and a source word are predicted
through reciprocal incorporation between source
and target context from the beginning of a target
sentence, the word order in the target sentence is
automatically determined and the number of pos-
sible configurations of source words is decreased.
Thus, we do not need to perform any computation
for word re-ordering. Moreover, since correspon-
dences are provided based on bilingual contextual
evidence, translation ambiguities can be decreased.
As a result, the proposed model is expected to re-
duce computational complexity during the decoding
as well as improve performance.
Furthermore, since a word-based translation ap-
proach is often incapable of handling complicated
expressions such as an idiomatic expressions or
complicated verb phrases, it often outputs nonsense
translations. To avoid nonsense translations and to
increase explanatory power, we incorporate struc-
tural aspects of the language into the chunk-based
translation model. In our model, one source chunk
is translated by exactly one target chunk, i.e., one-
to-one chunk alignment. Thus we obtain:
</bodyText>
<equation confidence="0.982814">
�eK ~ argmax,K, Pr(e�j , �fK ) (6)
</equation>
<bodyText confidence="0.99989075">
the head part. The head part is related to the mean-
ing of a given segment, while the tail part indicates
a grammatical role of the head in a given sentence.
By putting this linguistic knowledge to practical
use, we build a head-tail based translation model
as a back-off version of the chunk-based translation
model. We place several constraints on this head-tail
based translation model as follows:
</bodyText>
<listItem confidence="0.942894">
• The head of a given source chunk corresponds
to the head of a target chunk. The tail of the
source chunk corresponds to the tail of a target
chunk. If a chunk does not have a tail part, we
assign NUL to the tail of the chunk.
• The head of a given chunk follows the tail of the
preceding chunk and the tail follows the head of
the given chunk.
</listItem>
<bodyText confidence="0.99998575">
The constraints are designed to maintain the struc-
tural consistency of a chunk. Under these con-
straints, the head-tail based translation can be for-
mulated as the following equation:
</bodyText>
<equation confidence="0.996044">
�fail�ei, �ei-1)Pr(�eil�ei—i, fai_�) ~ (8)
Pr( f ail�eh , �ei-1)Pr(�eh l�ei-1�fai_,)
Pr(
K
Pr(�eK� , fl) — Pr( failei, ei—i)Pr(eilei—i, �fai_,)
i=1
(7)
</equation>
<bodyText confidence="0.999801">
where K is the number of chunks in a source and a
target sentence.
</bodyText>
<sectionHeader confidence="0.9681805" genericHeader="method">
3 Chunk-based J/K Translation Model
with Back-Off
</sectionHeader>
<bodyText confidence="0.999928666666666">
With the translation framework described above, we
built a chunk-based J/K translation model as a case
study. Since a chunk-based translation model causes
severe data sparseness, it is often impossible to ob-
tain any translation of a given source chunk. In order
to alleviate this problem, we apply back-off trans-
lation models while giving the consideration to lin-
guistic characteristics.
Japanese and Korean is a very close language pair.
Both are agglutinative and inflected languages in the
word formation of a bunsetsu and an eojeol. A bun-
setsu/eojeol consists of two sub parts: the head part
composed of content words and the tail part com-
posed of functional words agglutinated at the end of
where eh denotes the head of the Z,th chunk and Z
means the tail of the chunk.
In the worst case, even the head-tail based model
may fail to obtain translations. In this case, we
back it off into a word-based translation model. In
the word-based translation model, the constraints
on the head-tail based translation model are not ap-
plied. The concept of the chunk-based J/K transla-
tion framework with back-off scheme can be sum-
marized as follows:
</bodyText>
<listItem confidence="0.999283625">
1. Input a dependency-parsed sentence at the
chunk level,
2. Apply the chunk-based translation model to the
given sentence,
3. If one of chunks does not have any correspond-
ing translation:
• divide the failed chunk into a head and a
tail part,
</listItem>
<equation confidence="0.941389666666667">
fa e
i �Z, �e� � )P r(�e� �l�e� � �f� ��)
Pr(
</equation>
<page confidence="0.982259">
551
</page>
<figureCaption confidence="0.742524">
Figure 1: An example of (a) chunk alignment for chunk-based, head-tail based translation and (b) bilingual
verb-noun collocation by using the chunk alignment and a monolingual dependency parser
</figureCaption>
<listItem confidence="0.9704554">
• back-off the translation into the head-tail
based translation model,
• if the head or tail does not have any corre-
sponding translation, apply a word-based
translation model to the chunk.
</listItem>
<bodyText confidence="0.92353">
Here, the back-off model is applied only to the part
that failed to get translation candidates.
</bodyText>
<subsectionHeader confidence="0.999649">
3.1 Learning Chunk-based Translation
</subsectionHeader>
<bodyText confidence="0.9980115">
We learn chunk alignments from a corpus that has
been word-aligned by a training toolkit for word-
based translation models: the Giza++ (Och and
Ney, 2000) toolkit for the IBM models (Brown
et al., 1993). For aligning chunk pairs, we con-
sider word(bunsetsu/eojeol) sequences to be chunks
if they are in an immediate dependency relationship
in a dependency tree. To identify chunks, we use
a word-aligned corpus, in which source language
sentences are annotated with dependency parse trees
by a dependency parser (Kudo et al., 2002) and tar-
get language sentences are annotated with POS tags
by a part-of-speech tagger (Rim, 2003). If a se-
quence of target words is aligned with the words in
a single source chunk, the target word sequence is
regarded as one chunk corresponding to the given
source chunk. By applying this method to the cor-
pus, we obtain a word- and chunk-aligned corpus
(see Figure 1).
From the aligned corpus, we directly estimate
the phrase translation probabilities, Pr(�fle),
and the model parameters, Pr( �fail�ei, �ei-1),
Pr(eilei-1, �fai_J. These estimation are made
based on relative frequencies.
</bodyText>
<subsectionHeader confidence="0.987676">
3.2 Decoding
</subsectionHeader>
<bodyText confidence="0.999955433333333">
For efficient decoding, we implement a multi-stack
decoder and a beam search with A* algorithm. At
each search level, the beam search moves through at
most n-best translation candidates, and a multi-stack
is used for partial translations according to the trans-
lation cardinality. The output sentence is generated
from left to right in the form of partial translations.
Initially, we get n translation candidates for each
source chunk with the beam size n. Every possible
translation is sorted according to its translation prob-
ability. We start the decoding with the initialized
beams and initial stack So, the top of which has the
information of the initial hypothesis, (eo = $, f0 =
$). The decoding algorithm is described in Table 1.
In the decoding algorithm, estimating the back-
ward score is so complicated that the computational
complexity becomes too high because of the context
consideration. Thus, in order to simplify this prob-
lem, we assume the context-independence of only
the backward score estimation. The backward score
is estimated by the translation probability and lan-
guage model score of the uncovered segments. For
each uncovered segment, we select the best transla-
tion with the highest score by multiplying the trans-
lation probability of the segment by its language
model score. The translation probability and lan-
guage model score are computed without giving
consideration to context.
After estimating the forward and backward score
of each partial translation on stack Si, we try to
</bodyText>
<page confidence="0.97916">
552
</page>
<listItem confidence="0.999175294117647">
1. Push the initial hypothesis (eo = $, �fo = $) on the initial
stack So
2. for i=1 to K
• Pop the previous state information of (���—�� fa —1)
from stack Si—I
• Get next target ez and corresponding source fad
• for all pairs of (ez, i- i)
– Check the head-tail consistency
– Mark the source segment as a covered one
– Estimate forward and backward score
– Push the state of pair (ez, �f ai) onto stack Si
• Sort all translations on stack Si by the scores
• Prune the hypotheses
3. while (stack SK is not empty)
• Pop the state of the pair (eK, fait)
• Compose translation output, (e1 ... �eK)
4. Output the best N translations
</listItem>
<tableCaption confidence="0.993777">
Table 1: A* multi-stack decoding algorithm
</tableCaption>
<bodyText confidence="0.999829444444445">
prune the hypotheses. In pruning, we first sort the
partial translations on stack Si according to their
scores. If the gradient of scores steeply decreases
over the given threshold at the eh translation, we
prune the translations of lower scores than the lvh
one. Moreover, if the number of filtered translations
is larger than N, we only take the top N transla-
tions. As a final translation, we output the single
best translation.
</bodyText>
<sectionHeader confidence="0.992068" genericHeader="method">
4 Resolving Long-distance Dependency
</sectionHeader>
<bodyText confidence="0.999978888888889">
Since most of the current translation models take
only the local context into account, they cannot
account for long-distance dependency. This often
causes syntactically or semantically incorrect trans-
lation to be output. In this section, we describe
how this problem can be solved. For handling the
long-distance dependency problem, we utilize bilin-
gual verb-noun collocations that are automatically
acquired from the chunk-aligned bilingual corpora.
</bodyText>
<subsectionHeader confidence="0.9912215">
4.1 Automatic Extraction of Bilingual
Verb-Noun Collocation(BiVN)
</subsectionHeader>
<bodyText confidence="0.9999244">
To automatically extract the bilingual verb-noun
collocations, we utilize a monolingual dependency
parser and the chunk alignment result. The basic
concept is the same as that used in (Hwang et al.,
2004): bilingual dependency parses are obtained by
sharing the dependency relations of a monolingual
dependency parser among the aligned chunks. Then
bilingual verb sub-categorization patterns are ac-
quired by navigating the bilingual dependency trees.
A verb sub-categorization is the collocation of a verb
and all of its argument/adjunct nouns, i.e. verb-noun
collocation(see Figure 1).
To acquire more reliable and general knowledge,
we apply the following filtering method with statis-
tical x2 test and unification operation:
</bodyText>
<listItem confidence="0.9272297">
• step 1. Filter out the reliable translation corre-
spondences from all of the alignment pairs by
x2 test at a probability level of al
• step 2. Filter out reliable bilingual verb-noun
collocations BiVN by a unification and x2 test
at a probability level of a2: Here, we assume
that two bilingual pairs, (v f : ve) and (nf : ne)
are unifiable into a frame (v f : ve7 nf : n,) iff
both of them are reliable pairs filtered in step 1.
and they share the same verb pair (v f : ve).
</listItem>
<subsectionHeader confidence="0.99925">
4.2 Application of BiVN
</subsectionHeader>
<bodyText confidence="0.999911454545455">
The acquired BiVN is used to evaluate the bilingual
correspondence of a verb-noun pair dependent on
each other and to select the correct translation. It
can be applied to any verb-noun pair regardless of
the distance between them in a sentence. Moreover,
since the verb-noun relation in BiVN is bilingual
knowledge, the sense of each corresponding verb
and noun can be almost completely disambiguated
by each other.
In our translation system, we apply this BiVN
during decoding as follows:
</bodyText>
<listItem confidence="0.999750666666667">
1. Pivot verbs and their dependents in a given
dependency-parsed source sentence
2. When extending a hypothesis, if one of the piv-
oted verb and noun pairs is covered and its cor-
responding translation pair is in BiVN, we give
positive weight , &gt; 1 to the hypothesis.
</listItem>
<equation confidence="0.783627333333333">
V)(BiVNz) ��
1 if BiVNi E BiVN
0 otherwise
</equation>
<page confidence="0.992104">
553
</page>
<bodyText confidence="0.9999526">
where BiV Ni = (vf : ve, nf : ne) and O(BiV Ni)
is a function that indicates whether the bilingual
translation pair is in BiVN. By adding the weight
of the O(BiV Ni) function, we refine our model as
follows:
</bodyText>
<equation confidence="0.997568">
e�1 — argmax 11 i�1 Prl�fazl�ei, �ei_1) (10
Pr(eilei_1 ~fag_1��V N(faj)�b(BiV Nz)
</equation>
<bodyText confidence="0.999855">
where V N(faz) is a function indicating whether the
pair of a verb and its argument (vf, nf) is covered
with vf = faz or nf = faz and BiV Ni = (vf :
ve, nf : ne) is a bilingual translation pair in the hy-
pothesis.
</bodyText>
<sectionHeader confidence="0.999128" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.971524">
5.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9999513">
The corpus for the experiment was extracted from
the Basic Travel Expression Corpus (BTEC), a col-
lection of conversational travel phrases for Japanese
and Korean (see Table 2). The entire corpus was
split into two parts: 162,320 sentences in parallel for
training and 10,150 sentences for test. The Japanese
sentences were automatically dependency-parsed by
CaboCha (Kudo et al., 2002) and the Korean sen-
tences were automatically POS tagged by KUTag-
ger (Rim, 2003)
</bodyText>
<subsectionHeader confidence="0.999569">
5.2 Translation Systems
</subsectionHeader>
<bodyText confidence="0.998267294117647">
Four translation systems were implemented for
evaluation: 1) Word based IBM-style SMT Sys-
tem(WBIBM), 2) Chunk based IBM-style SMT Sys-
tem(CBIBM), 3) Word based LM tightly Coupled
SMT System(WBLMC), and 4) Chunk based LM
tightly Coupled SMT System(CBLMC). To exam-
ine the effect of BiVN, BiVN was optionally used
for each system.
The word-based IBM-style (WBIBM) system1
consisted of a word translation model and a bi-
gram language model. The bi-gram language
model was generated by using CMU LM toolkit
(Clarkson et al., 1997). Instead of using a fer-
tility model, we allowed a multi-word target of
a given source word if it aligned with more than
one word. We didn’t use any distortion model for
word re-ordering. And we used a log-linear model
</bodyText>
<footnote confidence="0.716331">
1In this experiment, a word denotes a morpheme
</footnote>
<bodyText confidence="0.993207458333333">
Pr(elf) = exp(Ei Aih(e, f)) for weighting the
language model and the translation model. For de-
coding, we used a multi-stack decoder based on the
A* algorithm, which is almost the same as that de-
scribed in Section 3. The difference is the use of
the language model for controlling the generation of
)target translations.
The chunk-based IBM-style (CBIBM) system
consisted of a chunk translation model and a bi-
gram language model. To alleviate the data sparse-
ness problem of the chunk translation model, we ap-
plied the back-off method at the head-tail or mor-
pheme level. The remaining conditions are the same
as those for WBIBM.
The word-based LM tightly coupled (WBLMC)
system was implemented for comparison with the
chunk-based systems. Except for setting the transla-
tion unit as a morpheme, the other conditions are the
same as those for the proposed chunk-based transla-
tion system.
The chunk-based LM tightly coupled (CBLMC)
system is the proposed translation system. A bi-
gram language model was used for estimating the
backward score.
</bodyText>
<subsectionHeader confidence="0.992611">
5.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.99519105">
Translation evaluations were carried out on 510 sen-
tences selected randomly from the test set. The met-
rics for the evaluations are as follows:
PER(Position independent WER), which pe-
nalizes without considering positional dis-
fluencies(Niesen et al., 2000).
mWER(multi-reference Word Error Rate), which is
based on the minimum edit distance between
the target sentence and the sentences in the ref-
erence set (Niesen et al., 2000).
BLEU, which is the ratio of the n-gram for
the translation results found in the reference
translations with a penalty for too short sen-
tences (Papineni et al., 2001).
NIST which is a weighted n-gram precision in
combination with a penalty for too short sen-
tences.
For this evaluation, we made 10 multiple references
available. We computed all of the above criteria with
respect to these multiple references.
</bodyText>
<page confidence="0.994811">
554
</page>
<table confidence="0.999792833333333">
Training Test
Japanese Korean Japanese Korean
# of sentences 162,320 10,150
# of total morphemes 1,153,954 1,179,753 74,366 76,540
# of bunsetsu/eojeol 448,438 587,503 28,882 38,386
vocabulary size 15,682 15,726 5,144 4,594
</table>
<tableCaption confidence="0.972811">
Table 2: Statistics of Basic Travel Expression Corpus
</tableCaption>
<table confidence="0.9999156">
PER mWER BLEU NIST
WBIBM 0.3415 / 0.3318 0.3668 / 0.3591 0.5747 / 0.5837 6.9075 / 7.1110
WBLMC 0.2667 / 0.2666 0.2998 / 0.2994 0.5681 / 0.5690 9.0149 / 9.0360
CBIBM 0.2677 / 0.2383 0.2992 / 0.2700 0.6347 / 0.6741 8.0900 / 8.6981
CBLMC 0.1954 / 0.1896 0.2176 / 0.2129 0.7060 / 0.7166 9.9167 / 10.027
</table>
<tableCaption confidence="0.997189">
Table 3: Evaluation Results of Translation Systems: without BiVN/with BiVN
</tableCaption>
<table confidence="0.9981525">
WBIBM WBLMC CBIBM CBLMC
0.8110 / 0.8330 2.5585 / 2.5547 0.3345 / 0.3399 0.9039 / 0.9052
</table>
<tableCaption confidence="0.999809">
Table 4: Translation Speed of Each Translation Systems(sec./sentence): without BiVN/with BiVN
</tableCaption>
<subsectionHeader confidence="0.991204">
5.4 Analysis and Discussion
</subsectionHeader>
<bodyText confidence="0.999977673469388">
Table 3 shows the performance evaluation of each
system. CBLMC outperformed CBIBM in overall
evaluation criteria. WBLMC showed much better
performance than WBIBM in most of the evalua-
tion criteria except for BLEU score. The interesting
point is that the performance of WBLMC is close to
that of CBIBM in PER and mWER. The BLEU score
of WBLMC is lower than that of CBIBM, but the
NIST score of WBLMC is much better than that of
CBIBM.
The reason the proposed model provided better
performance than the IBM-style models is because
the use of contextual information in CBLMC and
WBLMC enabled the system to reduce the transla-
tion ambiguities, which not only reduced the compu-
tational complexity during decoding, but also made
the translation accurate and deterministic. In addi-
tion, chunk-based translation systems outperformed
word-based systems. This is also strong evidence of
the advantage of contextual information.
To evaluate the effectiveness of bilingual verb-
noun collocations, we used the BiVN filtered with
al = .05, a2 = .l, where coverage is 64.86%
on the test set and average ambiguity is 2.99. We
suffered a slight loss in the speed by using the
BiVN(see Table 4), but we could improve perfor-
mance in all of the translation systems(see Table
3). In particular, the performance improvement in
CBIBM with BiVN was remarkable. This is a pos-
itive sign that the BiVN is useful for handling the
problem of long-distance dependency. From this re-
sult, we believe that if we increased the coverage of
BiVN and its accuracy, we could improve the per-
formance much more.
Table 4 shows the translation speed of each sys-
tem. For the evaluation of processing time, we used
the same machine, with a Xeon 2.8 GHz CPU and
4GB memory, and checked the time of the best per-
formance of each system. The chunk-based trans-
lation systems are much faster than the word-based
systems. It may be because the translation ambi-
guities of the chunk-based models are lower than
those of the word-based models. However, the pro-
cessing speed of the IBM-style models is faster than
the proposed model. This tendency can be analyzed
from two viewpoints: decoding algorithm and DB
system for parameter retrieval. Theoretically, the
computational complexity of the proposed model is
lower than that of the IBM models. The use of a
</bodyText>
<page confidence="0.994291">
555
</page>
<bodyText confidence="0.999895571428572">
sorting and pruning algorithm for partial translations
provides shorter search times in all system. Since
the number of parameters for the proposed model is
much more than for the IBM-style models, it took a
longer time to retrieve parameters. To decrease the
processing time, we need to construct a more effi-
cient DB system.
</bodyText>
<sectionHeader confidence="0.999341" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977">
In this paper, we proposed a new chunk-based statis-
tical machine translation model that is tightly cou-
pled with a language model. In order to alleviate
the data sparseness in chunk-based translation, we
applied the back-off translation method at the head-
tail and morpheme levels. Moreover, in order to
get more semantically plausible translation results
by considering long-distance dependency, we uti-
lized verb-noun collocations which were automat-
ically extracted by using chunk alignment and a
monolingual dependency parser. As a case study,
we experimented on the language pair of Japanese
and Korean. Experimental results showed that the
proposed translation model is very effective in im-
proving performance. The use of bilingual verb-
noun collocations is also useful for improving the
performance.
However, we still have some problems of the data
sparseness and the low coverage of bilingual verb-
noun collocation. In the near future, we will try to
solve the data sparseness problem and to increase the
coverage and accuracy of verb-noun collocations.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999640409836066">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation,
Computational Linguistics, 19(2):263-311.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical Lan-
guage Modeling Using the CMU-Cambridge Toolkit,
Proc. of ESCA Eurospeech.
Young-Sook Hwang, Kyonghee Paik, and Yutaka Sasaki.
2004. Bilingual Knowledge Extraction Using Chunk
Alignment, Proc. of the 18th Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC-18), pp. 127-137, Tokyo.
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models, Computational Lin-
guistics, Squibs Discussion, 25(4).
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003 Statistical Phrase-Based Translation, Proc.
of the Human Language Technology Confer-
ence(HLT/NAACL)
Philipp Koehn. 2004 Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models, Proc. of AMTA’04
Taku Kudo, Yuji Matsumoto. 2002. Japanese Depen-
dency Analyisis using Cascaded Chunking, Proc. of
CoNLL-2002
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability modelfor statistical machine transla-
tion , Proc. of EMNLP.
Sonja Niesen, Franz Josef Och, Gregor Leusch, Hermann
Ney. 2000. An Evaluation Tool for Machine Transla-
tion: Fast Evaluation for MT Research, Proc. of the
2nd International Conference on Language Resources
and Evaluation, pp. 39-45, Athens, Greece.
Franz Josef Och, Christoph Tillmann, Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation, Proc. of EMNLP/WVLC.
Franz Josef Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models , Proc. of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, pp. 440-447, Hongkong, China.
Franz Josef Och, Nicola Ueffing, Hermann Ney. 2001.
An Efficient A* Search Algorithm for Statistical Ma-
chine Translation , Data-Driven Machine Translation
Workshop, pp. 55-62, Toulouse, France.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalu-
ation of machine translation , IBM Research Report,
RC22176.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world,
Proc. ofLREC 2002, pp. 147-152, Spain.
Richard Zens and Hermann Ney. 2004. Improve-
ments in Phrase-Based Statistical Machine Transla-
tion, Proc. of the Human Language Technology Con-
ference (HLT-NAACL) , Boston, MA, pp. 257-264.
Hae-Chang Rim. 2003. Korean Morphological Analyzer
and Part-of-Speech Tagger, Technical Report, NLP
Lab. Dept. of Computer Science and Engineering, Ko-
rea University
</reference>
<page confidence="0.998499">
556
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684002">
<title confidence="0.999673">Context-dependent SMT Model using Bilingual Verb-Noun Collocation</title>
<author confidence="0.999351">Young-Sook Hwang</author>
<affiliation confidence="0.997107">ATR SLT Research Labs</affiliation>
<address confidence="0.920991">2-2-2 Hikaridai Seika-cho Soraku-gun Kyoto, 619-0288, JAPAN</address>
<email confidence="0.952709">youngsook.hwang@atr.jp</email>
<author confidence="0.999307">Yutaka Sasaki</author>
<affiliation confidence="0.99887">ATR SLT Research Labs</affiliation>
<address confidence="0.9231335">2-2-2 Hikaridai Seika-cho Soraku-gun Kyoto, 619-0288, JAPAN</address>
<email confidence="0.972615">yutaka.sasaki@atr.jp</email>
<abstract confidence="0.999422555555556">In this paper, we propose a new contextdependent SMT model that is tightly coupled with a language model. It is designed to decrease the translation ambiguities and efficiently search for an optimal hypothesis by reducing the hypothesis search space. It works through reciprocal incorporation between source and target context: a source word is determined by the context of previous and corresponding target words and the next target word is predicted by the pair consisting of the previous target word and its corresponding source word. In order to alleviate the data sparseness in chunk-based translation, we take a stepwise back-off translation strategy. Moreover, in order to obtain more semantically plausible translation results, we use bilingual verb-noun collocations; these are automatically extracted by using chunk alignment and a monolingual dependency parser. As a case study, we experimented on the language pair of Japanese and Korean. As a result, we could not only reduce the search space but also improve the performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation,</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1809" citStr="Brown et al., 1993" startWordPosition="276" endWordPosition="279">a case study, we experimented on the language pair of Japanese and Korean. As a result, we could not only reduce the search space but also improve the performance. 1 Introduction For decades, many research efforts have contributed to the advance of statistical machine translation. Recently, various works have improved the quality of statistical machine translation systems by using phrase translation (Koehn et al., 2003; Marcu et al., 2002; Och et al., 1999; Och and Ney, 2000; Zens et al., 2004). Most of the phrase-based translation models have adopted the noisy-channel based IBM style models (Brown et al., 1993): ei = argmaxe1Pr(fJ Iei)Pr(ef) (1) In these model, we have two types of knowledge: translation model, Pr(f1 Ie1) and language model, Pr(eI). The translation model links the source language sentence to the target language sentence. The language model describes the well-formedness of the target language sentence and might play a role in restricting hypothesis expansion during decoding. To recover the word order difference between two languages, it also allows modeling the reordering by introducing a relative distortion probability distribution. However, in spite of using such a language model a</context>
<context position="11827" citStr="Brown et al., 1993" startWordPosition="1912" endWordPosition="1915">ilingual verb-noun collocation by using the chunk alignment and a monolingual dependency parser • back-off the translation into the head-tail based translation model, • if the head or tail does not have any corresponding translation, apply a word-based translation model to the chunk. Here, the back-off model is applied only to the part that failed to get translation candidates. 3.1 Learning Chunk-based Translation We learn chunk alignments from a corpus that has been word-aligned by a training toolkit for wordbased translation models: the Giza++ (Och and Ney, 2000) toolkit for the IBM models (Brown et al., 1993). For aligning chunk pairs, we consider word(bunsetsu/eojeol) sequences to be chunks if they are in an immediate dependency relationship in a dependency tree. To identify chunks, we use a word-aligned corpus, in which source language sentences are annotated with dependency parse trees by a dependency parser (Kudo et al., 2002) and target language sentences are annotated with POS tags by a part-of-speech tagger (Rim, 2003). If a sequence of target words is aligned with the words in a single source chunk, the target word sequence is regarded as one chunk corresponding to the given source chunk. </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation, Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical Language Modeling Using the CMU-Cambridge Toolkit,</title>
<date>1997</date>
<booktitle>Proc. of ESCA Eurospeech.</booktitle>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>P.R. Clarkson and R. Rosenfeld. 1997. Statistical Language Modeling Using the CMU-Cambridge Toolkit, Proc. of ESCA Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Sook Hwang</author>
<author>Kyonghee Paik</author>
<author>Yutaka Sasaki</author>
</authors>
<title>Bilingual Knowledge Extraction Using Chunk Alignment,</title>
<date>2004</date>
<booktitle>Proc. of the 18th Pacific Asia Conference on Language, Information and Computation (PACLIC-18),</booktitle>
<pages>127--137</pages>
<location>Tokyo.</location>
<contexts>
<context position="16126" citStr="Hwang et al., 2004" startWordPosition="2614" endWordPosition="2617">ount for long-distance dependency. This often causes syntactically or semantically incorrect translation to be output. In this section, we describe how this problem can be solved. For handling the long-distance dependency problem, we utilize bilingual verb-noun collocations that are automatically acquired from the chunk-aligned bilingual corpora. 4.1 Automatic Extraction of Bilingual Verb-Noun Collocation(BiVN) To automatically extract the bilingual verb-noun collocations, we utilize a monolingual dependency parser and the chunk alignment result. The basic concept is the same as that used in (Hwang et al., 2004): bilingual dependency parses are obtained by sharing the dependency relations of a monolingual dependency parser among the aligned chunks. Then bilingual verb sub-categorization patterns are acquired by navigating the bilingual dependency trees. A verb sub-categorization is the collocation of a verb and all of its argument/adjunct nouns, i.e. verb-noun collocation(see Figure 1). To acquire more reliable and general knowledge, we apply the following filtering method with statistical x2 test and unification operation: • step 1. Filter out the reliable translation correspondences from all of the</context>
</contexts>
<marker>Hwang, Paik, Sasaki, 2004</marker>
<rawString>Young-Sook Hwang, Kyonghee Paik, and Yutaka Sasaki. 2004. Bilingual Knowledge Extraction Using Chunk Alignment, Proc. of the 18th Pacific Asia Conference on Language, Information and Computation (PACLIC-18), pp. 127-137, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding Complexity in WordReplacement Translation Models,</title>
<date>1999</date>
<journal>Computational Linguistics, Squibs Discussion,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="2692" citStr="Knight, 1999" startWordPosition="415" endWordPosition="416">formedness of the target language sentence and might play a role in restricting hypothesis expansion during decoding. To recover the word order difference between two languages, it also allows modeling the reordering by introducing a relative distortion probability distribution. However, in spite of using such a language model and a distortion model, the translation outputs may not be fluent or in fact may produce nonsense. To make things worse, the huge hypothesis search space is much too large for an exhaustive search. If arbitrary reorderings are allowed, the search problem is NP-complete (Knight, 1999). According to a previous analysis (Koehn et al., 2004) of how many hypotheses are generated during an exhaustive search using the IBM models, the upper bound for the number of states is estimated by N - 2j Ve�2J, where J is the number of source words and IV�I is the size of the target vocabulary. Even though the number of possible translations of the last two words is much smaller than V ~2, we still need to make further improvement. The main concern is the ex549 Proceedings of the 43rd Annual Meeting of the ACL, pages 549–556, Ann Arbor, June 2005. c�2005 Association for Computational Lingui</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding Complexity in WordReplacement Translation Models, Computational Linguistics, Squibs Discussion, 25(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<date>2003</date>
<booktitle>Statistical Phrase-Based Translation, Proc. of the Human Language Technology Conference(HLT/NAACL)</booktitle>
<contexts>
<context position="1612" citStr="Koehn et al., 2003" startWordPosition="242" endWordPosition="245">obtain more semantically plausible translation results, we use bilingual verb-noun collocations; these are automatically extracted by using chunk alignment and a monolingual dependency parser. As a case study, we experimented on the language pair of Japanese and Korean. As a result, we could not only reduce the search space but also improve the performance. 1 Introduction For decades, many research efforts have contributed to the advance of statistical machine translation. Recently, various works have improved the quality of statistical machine translation systems by using phrase translation (Koehn et al., 2003; Marcu et al., 2002; Och et al., 1999; Och and Ney, 2000; Zens et al., 2004). Most of the phrase-based translation models have adopted the noisy-channel based IBM style models (Brown et al., 1993): ei = argmaxe1Pr(fJ Iei)Pr(ef) (1) In these model, we have two types of knowledge: translation model, Pr(f1 Ie1) and language model, Pr(eI). The translation model links the source language sentence to the target language sentence. The language model describes the well-formedness of the target language sentence and might play a role in restricting hypothesis expansion during decoding. To recover the </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003 Statistical Phrase-Based Translation, Proc. of the Human Language Technology Conference(HLT/NAACL)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models,</title>
<date>2004</date>
<booktitle>Proc. of AMTA’04</booktitle>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004 Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models, Proc. of AMTA’04</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese Dependency Analyisis using Cascaded Chunking,</title>
<date>2002</date>
<booktitle>Proc. of CoNLL-2002</booktitle>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo, Yuji Matsumoto. 2002. Japanese Dependency Analyisis using Cascaded Chunking, Proc. of CoNLL-2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability modelfor statistical machine translation ,</title>
<date>2002</date>
<booktitle>Proc. of EMNLP.</booktitle>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability modelfor statistical machine translation , Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niesen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An Evaluation Tool for Machine Translation: Fast Evaluation for MT</title>
<date>2000</date>
<booktitle>Research, Proc. of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<pages>39--45</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="21066" citStr="Niesen et al., 2000" startWordPosition="3447" endWordPosition="3451">nted for comparison with the chunk-based systems. Except for setting the translation unit as a morpheme, the other conditions are the same as those for the proposed chunk-based translation system. The chunk-based LM tightly coupled (CBLMC) system is the proposed translation system. A bigram language model was used for estimating the backward score. 5.3 Evaluation Translation evaluations were carried out on 510 sentences selected randomly from the test set. The metrics for the evaluations are as follows: PER(Position independent WER), which penalizes without considering positional disfluencies(Niesen et al., 2000). mWER(multi-reference Word Error Rate), which is based on the minimum edit distance between the target sentence and the sentences in the reference set (Niesen et al., 2000). BLEU, which is the ratio of the n-gram for the translation results found in the reference translations with a penalty for too short sentences (Papineni et al., 2001). NIST which is a weighted n-gram precision in combination with a penalty for too short sentences. For this evaluation, we made 10 multiple references available. We computed all of the above criteria with respect to these multiple references. 554 Training Test</context>
</contexts>
<marker>Niesen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Niesen, Franz Josef Och, Gregor Leusch, Hermann Ney. 2000. An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research, Proc. of the 2nd International Conference on Language Resources and Evaluation, pp. 39-45, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation,</title>
<date>1999</date>
<booktitle>Proc. of EMNLP/WVLC.</booktitle>
<contexts>
<context position="1650" citStr="Och et al., 1999" startWordPosition="250" endWordPosition="253">lation results, we use bilingual verb-noun collocations; these are automatically extracted by using chunk alignment and a monolingual dependency parser. As a case study, we experimented on the language pair of Japanese and Korean. As a result, we could not only reduce the search space but also improve the performance. 1 Introduction For decades, many research efforts have contributed to the advance of statistical machine translation. Recently, various works have improved the quality of statistical machine translation systems by using phrase translation (Koehn et al., 2003; Marcu et al., 2002; Och et al., 1999; Och and Ney, 2000; Zens et al., 2004). Most of the phrase-based translation models have adopted the noisy-channel based IBM style models (Brown et al., 1993): ei = argmaxe1Pr(fJ Iei)Pr(ef) (1) In these model, we have two types of knowledge: translation model, Pr(f1 Ie1) and language model, Pr(eI). The translation model links the source language sentence to the target language sentence. The language model describes the well-formedness of the target language sentence and might play a role in restricting hypothesis expansion during decoding. To recover the word order difference between two lang</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, Hermann Ney. 1999. Improved alignment models for statistical machine translation, Proc. of EMNLP/WVLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models</title>
<date>2000</date>
<booktitle>Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hongkong,</location>
<contexts>
<context position="1669" citStr="Och and Ney, 2000" startWordPosition="254" endWordPosition="257"> use bilingual verb-noun collocations; these are automatically extracted by using chunk alignment and a monolingual dependency parser. As a case study, we experimented on the language pair of Japanese and Korean. As a result, we could not only reduce the search space but also improve the performance. 1 Introduction For decades, many research efforts have contributed to the advance of statistical machine translation. Recently, various works have improved the quality of statistical machine translation systems by using phrase translation (Koehn et al., 2003; Marcu et al., 2002; Och et al., 1999; Och and Ney, 2000; Zens et al., 2004). Most of the phrase-based translation models have adopted the noisy-channel based IBM style models (Brown et al., 1993): ei = argmaxe1Pr(fJ Iei)Pr(ef) (1) In these model, we have two types of knowledge: translation model, Pr(f1 Ie1) and language model, Pr(eI). The translation model links the source language sentence to the target language sentence. The language model describes the well-formedness of the target language sentence and might play a role in restricting hypothesis expansion during decoding. To recover the word order difference between two languages, it also allo</context>
<context position="11779" citStr="Och and Ney, 2000" startWordPosition="1903" endWordPosition="1906">nk-based, head-tail based translation and (b) bilingual verb-noun collocation by using the chunk alignment and a monolingual dependency parser • back-off the translation into the head-tail based translation model, • if the head or tail does not have any corresponding translation, apply a word-based translation model to the chunk. Here, the back-off model is applied only to the part that failed to get translation candidates. 3.1 Learning Chunk-based Translation We learn chunk alignments from a corpus that has been word-aligned by a training toolkit for wordbased translation models: the Giza++ (Och and Ney, 2000) toolkit for the IBM models (Brown et al., 1993). For aligning chunk pairs, we consider word(bunsetsu/eojeol) sequences to be chunks if they are in an immediate dependency relationship in a dependency tree. To identify chunks, we use a word-aligned corpus, in which source language sentences are annotated with dependency parse trees by a dependency parser (Kudo et al., 2002) and target language sentences are annotated with POS tags by a part-of-speech tagger (Rim, 2003). If a sequence of target words is aligned with the words in a single source chunk, the target word sequence is regarded as one</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models , Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 440-447, Hongkong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>An Efficient A* Search Algorithm for Statistical Machine Translation ,</title>
<date>2001</date>
<booktitle>Data-Driven Machine Translation Workshop,</booktitle>
<pages>55--62</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="3591" citStr="Och et al., 2001" startWordPosition="569" endWordPosition="572">rget vocabulary. Even though the number of possible translations of the last two words is much smaller than V ~2, we still need to make further improvement. The main concern is the ex549 Proceedings of the 43rd Annual Meeting of the ACL, pages 549–556, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics ponential explosion from the possible configurations of source words covered by a hypothesis. In order to reduce the number of possible configurations of source words, decoding algorithms based on X as well as the beam search algorithm have been proposed (Koehn et al., 2004; Och et al., 2001). (Koehn et al., 2004; Och et al., 2001) used heuristics for pruning implausible hypotheses. Our approach to this problem examines the possibility of utilizing context information in a given language pair. Under a given target context, the corresponding source word of a given target word is almost deterministic. Conversely, if a translation pair is given, then the related target or source context is predictable. This implies that if we considered bilingual context information in a given language pair during decoding, we can reduce the computational complexity of the hypothesis search; specific</context>
</contexts>
<marker>Och, Ueffing, Ney, 2001</marker>
<rawString>Franz Josef Och, Nicola Ueffing, Hermann Ney. 2001. An Efficient A* Search Algorithm for Statistical Machine Translation , Data-Driven Machine Translation Workshop, pp. 55-62, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation ,</title>
<date>2001</date>
<journal>IBM Research</journal>
<tech>Report, RC22176.</tech>
<contexts>
<context position="21406" citStr="Papineni et al., 2001" startWordPosition="3505" endWordPosition="3508">kward score. 5.3 Evaluation Translation evaluations were carried out on 510 sentences selected randomly from the test set. The metrics for the evaluations are as follows: PER(Position independent WER), which penalizes without considering positional disfluencies(Niesen et al., 2000). mWER(multi-reference Word Error Rate), which is based on the minimum edit distance between the target sentence and the sentences in the reference set (Niesen et al., 2000). BLEU, which is the ratio of the n-gram for the translation results found in the reference translations with a penalty for too short sentences (Papineni et al., 2001). NIST which is a weighted n-gram precision in combination with a penalty for too short sentences. For this evaluation, we made 10 multiple references available. We computed all of the above criteria with respect to these multiple references. 554 Training Test Japanese Korean Japanese Korean # of sentences 162,320 10,150 # of total morphemes 1,153,954 1,179,753 74,366 76,540 # of bunsetsu/eojeol 448,438 587,503 28,882 38,386 vocabulary size 15,682 15,726 5,144 4,594 Table 2: Statistics of Basic Travel Expression Corpus PER mWER BLEU NIST WBIBM 0.3415 / 0.3318 0.3668 / 0.3591 0.5747 / 0.5837 6.</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation , IBM Research Report, RC22176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
<author>Eiichiro Sumita</author>
<author>Fumiaki Sugaya</author>
<author>Hirofumi Yamamoto</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world,</title>
<date>2002</date>
<booktitle>Proc. ofLREC 2002,</booktitle>
<pages>147--152</pages>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world, Proc. ofLREC 2002, pp. 147-152, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<date>2004</date>
<booktitle>Improvements in Phrase-Based Statistical Machine Translation, Proc. of the Human Language Technology Conference (HLT-NAACL) ,</booktitle>
<pages>257--264</pages>
<location>Boston, MA,</location>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in Phrase-Based Statistical Machine Translation, Proc. of the Human Language Technology Conference (HLT-NAACL) , Boston, MA, pp. 257-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hae-Chang Rim</author>
</authors>
<title>Korean Morphological Analyzer and Part-of-Speech Tagger,</title>
<date>2003</date>
<tech>Technical Report, NLP</tech>
<institution>Lab. Dept. of Computer Science and Engineering, Korea University</institution>
<contexts>
<context position="12252" citStr="Rim, 2003" startWordPosition="1981" endWordPosition="1982">lignments from a corpus that has been word-aligned by a training toolkit for wordbased translation models: the Giza++ (Och and Ney, 2000) toolkit for the IBM models (Brown et al., 1993). For aligning chunk pairs, we consider word(bunsetsu/eojeol) sequences to be chunks if they are in an immediate dependency relationship in a dependency tree. To identify chunks, we use a word-aligned corpus, in which source language sentences are annotated with dependency parse trees by a dependency parser (Kudo et al., 2002) and target language sentences are annotated with POS tags by a part-of-speech tagger (Rim, 2003). If a sequence of target words is aligned with the words in a single source chunk, the target word sequence is regarded as one chunk corresponding to the given source chunk. By applying this method to the corpus, we obtain a word- and chunk-aligned corpus (see Figure 1). From the aligned corpus, we directly estimate the phrase translation probabilities, Pr(�fle), and the model parameters, Pr( �fail�ei, �ei-1), Pr(eilei-1, �fai_J. These estimation are made based on relative frequencies. 3.2 Decoding For efficient decoding, we implement a multi-stack decoder and a beam search with A* algorithm.</context>
<context position="18945" citStr="Rim, 2003" startWordPosition="3104" endWordPosition="3105"> is covered with vf = faz or nf = faz and BiV Ni = (vf : ve, nf : ne) is a bilingual translation pair in the hypothesis. 5 Experiments 5.1 Corpus The corpus for the experiment was extracted from the Basic Travel Expression Corpus (BTEC), a collection of conversational travel phrases for Japanese and Korean (see Table 2). The entire corpus was split into two parts: 162,320 sentences in parallel for training and 10,150 sentences for test. The Japanese sentences were automatically dependency-parsed by CaboCha (Kudo et al., 2002) and the Korean sentences were automatically POS tagged by KUTagger (Rim, 2003) 5.2 Translation Systems Four translation systems were implemented for evaluation: 1) Word based IBM-style SMT System(WBIBM), 2) Chunk based IBM-style SMT System(CBIBM), 3) Word based LM tightly Coupled SMT System(WBLMC), and 4) Chunk based LM tightly Coupled SMT System(CBLMC). To examine the effect of BiVN, BiVN was optionally used for each system. The word-based IBM-style (WBIBM) system1 consisted of a word translation model and a bigram language model. The bi-gram language model was generated by using CMU LM toolkit (Clarkson et al., 1997). Instead of using a fertility model, we allowed a m</context>
</contexts>
<marker>Rim, 2003</marker>
<rawString>Hae-Chang Rim. 2003. Korean Morphological Analyzer and Part-of-Speech Tagger, Technical Report, NLP Lab. Dept. of Computer Science and Engineering, Korea University</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>