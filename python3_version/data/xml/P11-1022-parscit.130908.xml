<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.845064">
Goodness: A Method for Measuring Machine Translation Confidence
</title>
<author confidence="0.982394">
Nguyen Bach*
</author>
<affiliation confidence="0.886297666666667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.980678">
nbach@cs.cmu.edu
</email>
<note confidence="0.652973">
Fei Huang and Yaser Al-Onaizan
IBM T.J. Watson Research Center
</note>
<address confidence="0.7389705">
1101 Kitchawan Rd
Yorktown Heights, NY 10567, USA
</address>
<email confidence="0.98858">
{huangfe, onaizan}@us.ibm.com
</email>
<sectionHeader confidence="0.994872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961238095238">
State-of-the-art statistical machine translation
(MT) systems have made significant progress
towards producing user-acceptable translation
output. However, there is still no efficient
way for MT systems to inform users which
words are likely translated correctly and how
confident it is about the whole sentence. We
propose a novel framework to predict word-
level and sentence-level MT errors with a large
number of novel features. Experimental re-
sults show that the MT error prediction accu-
racy is increased from 69.1 to 72.2 in F-score.
The Pearson correlation between the proposed
confidence measure and the human-targeted
translation edit rate (HTER) is 0.6. Improve-
ments between 0.4 and 0.9 TER reduction are
obtained with the n-best list reranking task us-
ing the proposed confidence measure. Also,
we present a visualization prototype of MT er-
rors at the word and sentence levels with the
objective to improve post-editor productivity.
</bodyText>
<sectionHeader confidence="0.998117" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998931875">
State-of-the-art Machine Translation (MT) systems are
making progress to generate more usable translation
outputs. In particular, statistical machine translation
systems (Koehn et al., 2007; Bach et al., 2007; Shen
et al., 2008) have advanced to a state that the transla-
tion quality for certain language pairs (e.g. Spanish-
English, French-English, Iraqi-English) in certain do-
mains (e.g. broadcasting news, force-protection, travel)
is acceptable to users.
However, a remaining open question is how to pre-
dict confidence scores for machine translated words
and sentences. An MT system typically returns the
best translation candidate from its search space, but
still has no reliable way to inform users which word
is likely to be correctly translated and how confident it
is about the whole sentence. Such information is vital
</bodyText>
<note confidence="0.624839333333333">
* Work done during an internship at IBM T.J. Watson
211
Research Center
</note>
<bodyText confidence="0.999932720930233">
to realize the utility of machine translation in many ar-
eas. For example, a post-editor would like to quickly
identify which sentences might be incorrectly trans-
lated and in need of correction. Other areas, such as
cross-lingual question-answering, information extrac-
tion and retrieval, can also benefit from the confidence
scores of MT output. Finally, even MT systems can
leverage such information to do n-best list reranking,
discriminative phrase table and rule filtering, and con-
straint decoding (Hildebrand and Vogel, 2008).
Numerous attempts have been made to tackle the
confidence estimation problem. The work of Blatz et
al. (2004) is perhaps the best known study of sentence
and word level features and their impact on transla-
tion error prediction. Along this line of research, im-
provements can be obtained by incorporating more fea-
tures as shown in (Quirk, 2004; Sanchis et al., 2007;
Raybaud et al., 2009; Specia et al., 2009). Sori-
cut and Echihabi (2010) developed regression models
which are used to predict the expected BLEU score
of a given translation hypothesis. Improvement also
can be obtained by using target part-of-speech and null
dependency link in a MaxEnt classifier (Xiong et al.,
2010). Ueffing and Ney (2007) introduced word pos-
terior probabilities (WPP) features and applied them in
the n-best list reranking. From the usability point of
view, back-translation is a tool to help users to assess
the accuracy level of MT output (Bach et al., 2007).
Literally, it translates backward the MT output into the
source language to see whether the output of backward
translation matches the original source sentence.
However, previous studies had a few shortcomings.
First, source-side features were not extensively inves-
tigated. Blatz et al.(2004) only investigated source n-
gram frequency statistics and source language model
features, while other work mainly focused on target
side features. Second, previous work attempted to in-
corporate more features but faced scalability issues,
i.e., to train many features we need many training ex-
amples and to train discriminatively we need to search
through all possible translations of each training exam-
ple. Another issue of previous work was that they are
all trained with BLEU/TER score computing against
</bodyText>
<note confidence="0.9848385">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 211–219,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999117357142857">
the translation references which is different from pre-
dicting the human-targeted translation edit rate (HTER)
which is crucial in post-editing applications (Snover et
al., 2006; Papineni et al., 2002). Finally, the back-
translation approach faces a serious issue when forward
and backward translation models are symmetric. In this
case, back-translation will not be very informative to
indicate forward translation quality.
In this paper, we predict error types of each word
in the MT output with a confidence score, extend it to
the sentence level, then apply it to n-best list reranking
task to improve MT quality, and finally design a vi-
sualization prototype. We try to answer the following
questions:
</bodyText>
<listItem confidence="0.99603325">
• Can we use a rich feature set such as source-
side information, alignment context, and depen-
dency structures to improve error prediction per-
formance?
• Can we predict more translation error types i.e
substitution, insertion, deletion and shift?
• How good do our prediction methods correlate
with human correction?
• Do confidence measures help the MT system to
select a better translation?
• How confidence score can be presented to im-
prove end-user perception?
</listItem>
<bodyText confidence="0.997386166666667">
In Section 2, we describe the models and training
method for the classifier. We describe novel features
including source-side, alignment context, and depen-
dency structures in Section 3. Experimental results and
analysis are reported in Section 4. Section 5 and 6
present applications of confidence scores.
</bodyText>
<sectionHeader confidence="0.986331" genericHeader="introduction">
2 Confidence Measure Model
</sectionHeader>
<subsectionHeader confidence="0.994713">
2.1 Problem setting
</subsectionHeader>
<bodyText confidence="0.999981370370371">
Confidence estimation can be viewed as a sequen-
tial labelling task in which the word sequence is
MT output and word labels can be Bad/Good or
Insertion/Substitution/Shift/Good. We first esti-
mate each individual word confidence and extend it to
the whole sentence. Arabic text is fed into an Arabic-
English SMT system and the English translation out-
puts are corrected by humans in two phases. In phase
one, a bilingual speaker corrects the MT system trans-
lation output. In phase two, another bilingual speaker
does quality checking for the correction done in phase
one. If bad corrections were spotted, they correct them
again. In this paper we use the final correction data
from phase two as the reference thus HTER can be
used as an evaluation metric. We have 75 thousand sen-
tences with 2.4 million words in total from the human
correction process described above.
We obtain training labels for each word by perform-
ing TER alignment between MT output and the phase-
two human correction. From TER alignments we ob-
served that out of total errors are 48% substitution, 28%212
deletion, 13% shift, and 11% insertion errors. Based
on the alignment, each word produced by the MT sys-
tem has a label: good, insertion, substitution and shift.
Since a deletion error occurs when it only appears in the
reference translation, not in the MT output, our model
will not predict deletion errors in the MT output.
</bodyText>
<subsectionHeader confidence="0.983258">
2.2 Word-level model
</subsectionHeader>
<bodyText confidence="0.996479166666667">
In our problem, a training instance is a word from MT
output, and its label when the MT sentence is aligned
with the human correction. Given a training instance x,
y is the true label of x; f stands for its feature vector
f(x, y); and w is feature weight vector. We define a
feature-rich classifier score(x, y) as follow
</bodyText>
<equation confidence="0.990287">
score(x, y) = w.f(x, y) (1)
</equation>
<bodyText confidence="0.999495642857143">
To obtain the label, we choose the class with the high-
est score as the predicted label for that data instance.
To learn optimized weights, we use the Margin Infused
Relaxed Algorithm or MIRA (Crammer and Singer,
2003; McDonald et al., 2005) which is an online learner
closely related to both the support vector machine and
perceptron learning framework. MIRA has been shown
to provide state-of-the-art performance for sequential
labelling task (Rozenfeld et al., 2006), and is also able
to provide an efficient mechanism to train and opti-
mize MT systems with lots of features (Watanabe et
al., 2007; Chiang et al., 2009). In general, weights are
updated at each step time t according to the following
rule:
</bodyText>
<equation confidence="0.99874">
wt+1 = arg min,,,t+1 ||wt+1 − wt||
(2)
s.t. score(x, y) &gt; score(x, y&apos;) + L(y, y&apos;)
</equation>
<bodyText confidence="0.998484">
where L(y, y&apos;) is a measure of the loss of using y&apos; in-
stead of the true label y. In this problem L(y, y&apos;) is 0-1
loss function. More specifically, for each instance xi in
the training data at a time t we find the label with the
highest score:
</bodyText>
<equation confidence="0.9071405">
y&apos; = arg max score(xi,y) (3)
y
</equation>
<bodyText confidence="0.624781">
the weight vector is updated as follow
</bodyText>
<equation confidence="0.999265">
wt+1 = wt + T(f(xi, y) − f(xi, y&apos;)) (4)
</equation>
<bodyText confidence="0.935539333333333">
T can be interpreted as a step size; when T is a large
number we want to update our weights aggressively,
otherwise weights are updated conservatively.
</bodyText>
<equation confidence="0.9984015">
T = max(0, a)
a = min fC L(y,y )−(score(x;,y)−score(xt,y&apos; ))
</equation>
<bodyText confidence="0.965002">
(5)
where C is a positive constant used to cap the maxi-
mum possible value of T. In practice, a cut-off thresh-
old n is the parameter which decides the number of
features kept (whose occurrence is at least n) during
</bodyText>
<equation confidence="0.593289">
||Ax.,y)−Ax:,y&apos;)||2
</equation>
<bodyText confidence="0.999592833333333">
training. Note that MIRA is sensitive to constant C, (a) Source phrase
the cut-off feature threshold n, and the number of iter-
ations. The final weight is typically normalized by the
number of training iterations and the number of train-
ing instances. These parameters are tuned on a devel-
opment set.
</bodyText>
<subsectionHeader confidence="0.989005">
2.3 Sentence-level model
</subsectionHeader>
<bodyText confidence="0.999979090909091">
Given the feature sets and optimized weights, we use
the Viterbi algorithm to find the best label sequence.
To estimate the confidence of a sentence S we rely on
the information from the forward-backward inference.
One approach is to directly use the conditional prob-
abilities of the whole sequence. However, this quan-
tity is the confidence measure for the label sequence
predicted by the classifier and it does not represent the
goodness of the whole MT output. Another more ap-
propriated method is to use the marginal probability of
Good label which can be defined as follow:
</bodyText>
<equation confidence="0.9557998">
ou
α(yi|S)β(yi|S)
p(yi = Good|S) = (6)
Ej α(yj  |S)β(yj |S)
p(yi = Good|S) is the marginal probability of label
</equation>
<bodyText confidence="0.99073275">
Good at position i given the MT output sentence S.
α(yi|S) and β(yi|S) are forward and backward values.
Our confidence estimation for a sentence S of k words
is defined as follow
</bodyText>
<equation confidence="0.995425333333333">
�k = Goodl S)
goodness(S) = i-1 p(yZ (7)
k
</equation>
<bodyText confidence="0.997614">
goodness(S) is ranging between 0 and 1, where 0 is
equivalent to an absolutely wrong translation and 1
is a perfect translation. Essentially, goodness(S) is
the arithmetic mean which represents the goodness of
translation per word in the whole sentence.
</bodyText>
<sectionHeader confidence="0.984005" genericHeader="method">
3 Confidence Measure Features
</sectionHeader>
<bodyText confidence="0.999878">
Features are generated from feature types: abstract
templates from which specific features are instantiated.
Features sets are often parameterized in various ways.
In this section, we describe three new feature sets intro-
duced on top of our baseline classifier which has WPP
and target POS features (Ueffing and Ney, 2007; Xiong
et al., 2010).
</bodyText>
<subsectionHeader confidence="0.997874">
3.1 Source-side features
</subsectionHeader>
<bodyText confidence="0.9998872">
From MT decoder log, we can track which source
phrases generate target phrases. Furthermore, one can
infer the alignment between source and target words
within the phrase pair using simple aligners such as
IBM Model-1 alignment.
Source phrase features: These features are designed
to capture the likelihood that source phrase and target
word co-occur with a given error label. The intuition
behind them is that if a large percentage of the source
phrase and target have often been seen together with the
</bodyText>
<page confidence="0.57925">
213
</page>
<figure confidence="0.846924">
(b) Source POS
(c) Source POS and phrase in right context
</figure>
<figureCaption confidence="0.999662">
Figure 1: Source-side features.
</figureCaption>
<bodyText confidence="0.999973166666667">
same label, then the produced target word should have
this label in the future. Figure 1a illustrates this feature
template where the first line is source POS tags, the
second line is the Buckwalter romanized source Arabic
sequence, and the third line is MT output. The source
phrase feature is defined as follow
</bodyText>
<equation confidence="0.980289666666667">
0 067 10 0 10 067
1 if source-phrase=“hdhh alamlyt”
f102(process) 0 otherwise
</equation>
<bodyText confidence="0.998865833333333">
Source POS: Source phrase features might be suscep-
tible to sparseness issues. We can generalize source
phrases based on their POS tags to reduce the number
of parameters. For example, the example in Figure 1a
is generalized as in Figure 1b and we have the follow-
ing feature:
</bodyText>
<equation confidence="0.6325465">
r 1 if source-POS=“ DT DTNN ”
f103(process) Sl 0 otherwise
</equation>
<bodyText confidence="0.996580875">
Source POS and phrase context features: This fea-
ture set allows us to look at the surrounding context
of the source phrase. For example, in Figure 1c we
have “hdhh alamlyt” generates “process”. We also
have other information such as on the right hand side
the next two phrases are “ayda” and “tshyr” or the se-
quence of source target POS on the right hand side is
“RB VBP”. An example of this type of feature is
</bodyText>
<equation confidence="0.861108666666667">
�
1 if source-POS-context=“ RB VBP ”
f104(process) 0 otherwise
</equation>
<subsectionHeader confidence="0.998964">
3.2 Alignment context features
</subsectionHeader>
<bodyText confidence="0.998872">
The IBM Model-1 feature performed relatively well in
comparison with the WPP feature as shown by Blatz et
al. (2004). In our work, we incorporate not only the
</bodyText>
<figure confidence="0.980145324324324">
Sour
VBP IN DT
wydyf an hdhh
He adds that this
BP NN
So POS
RB
Tar OS
P VBZ IN
DTNN RB VBP IN N
(c) Left target
wydyf an hdhh alamlyt ayda
(d) Source POS &amp; right tar-
get
rocess also
NN RB VBZ TO DT
(b) Right source
PN N DTNN RB VBP IN NN
g
alamlyt ayda tshyr aly a
wydyf an h alamlyt ayda tshyr aly adm
Souce
process also refers to the i
S PO B
process
MT o
also refers to th
NN RB VBZ TO D
(a) Source-Target dependency
S
VBP IN DT DTNN RB
Figure 2: Alignment context features.
He adds that this process also
PRP VBZ IN DT NN RB
(a) Left source
wydyf an h
M T ouput refersHe dds that t
</figure>
<equation confidence="0.777287333333333">
put e add that th
WPP: 1.0 0.67 .0 .0 10 0.67 ...
T POS RP BZ IN NN RB VBZ TO DT
</equation>
<bodyText confidence="0.998882483870968">
IBM Model-1 feature but also the surrounding align-
ment context. The key intuition is that collocation is a
reliable indicator for judging if a target word is gener-
ated by a particular source word (Huang, 2009). More-
over, the IBM Model-1 feature was already used in sev-
eral steps of a translation system such as word align-
ment, phrase extraction and scoring. Also the impact of
this feature alone might fade away when the MT sys-
tem is scaled up.
We obtain word-to-word alignments by applying
IBM Model-1 to bilingual phrase pairs that generated
the MT output. The IBM Model-1 assumes one
target word can only be aligned to one source word.
Therefore, given a target word we can always identify
which source word it is aligned to.
Source alignment context feature: We anchor the
target word and derive context features surround-
ing its source word. For example, in Figure 2a
and 2b we have an alignment between “tshyr” and
“refers” The source contexts “tshyr” with a window
of one word are “ayda” to the left and “aly” to the right.
Target alignment context feature: Similar to source
alignment context features, we anchor the source word
and derive context features surrounding the aligned
target word. Figure 2c shows a left target context
feature of word “refers”. Our features are derived from
a window of four words.
Combining alignment context with POS tags: In-
stead of using lexical context we have features to look
at source and target POS alignment context. For in-
stance, the feature in Figure 2d is
</bodyText>
<figure confidence="0.944556363636364">
� 1 if source-POS = “VBP”
f141(refers) = and target-context = “to”
0 otherwise
214
(b)Child-Father agreement
Structures
wydyf an hdhh alamlyt ayda tshyr aly adm qdrt almtaddt aljnsyt alqwat al
He adds that this process also refers to the inability of the multinational nava
RB VBZ TO DT NN IN DT JJ
(c) Childrn arment
Children Agreement:
</figure>
<figureCaption confidence="0.991425">
Figure 3: Dependency structures features.
</figureCaption>
<subsectionHeader confidence="0.864081">
3.3 Source and target dependency structure
features
</subsectionHeader>
<bodyText confidence="0.999087833333333">
The contextual and source information in the previous
sections only take into account surface structures of
source and target sentences. Meanwhile, dependency
structures have been extensively used in various
translation systems (Shen et al., 2008; Ma et al.,
2008; Bach et al., 2009). The adoption of dependency
structures might enable the classifier to utilize deep
structures to predict translation errors. Source and tar-
get structures are unlikely to be isomorphic as shown
in Figure 3a. However, we expect some high-level
linguistic structures are likely to transfer across certain
language pairs. For example, prepositional phrases
(PP) in Arabic and English are similar in a sense
that PPs generally appear at the end of the sentence
(after all the verbal arguments) and to a lesser extent
at its beginning (Habash and Hu, 2009). We use the
Stanford parser to obtain dependency trees and POS
tags (Marneffe et al., 2006).
</bodyText>
<listItem confidence="0.695784">
Child-Father agreement: The motivation is to take
advantage of the long distance dependency relations
between source and target words. Given an alignment
between a source word si and a target word tp A child-
</listItem>
<table confidence="0.397215333333333">
VBP IN DT DTNN RB VBP IN NN NN DTJJ DTJJ DTNNS DT
PRP VBZ IN DT NN
JJ
</table>
<bodyText confidence="0.99931652631579">
father agreement exists when sk is aligned to tl, where
sk and tl are father of si and tj in source and target
dependency trees, respectively. Figure 3b illustrates
that “tshyr” and “refers” have a child-father agreement.
To verify our intuition, we analysed 243K words of
manual aligned Arabic-English bitext. We observed
29.2% words having child-father agreements. In term
of structure types, we found 27.2% of copula verb
and 30.2% prepositional structures, including object
of a preposition, prepositional modifier, and preposi-
tional complement, are having child-father agreements.
Children agreement: In the child-father agreement
feature we look up in the dependency tree, however,
we also can look down to the dependency tree with a
similar motivation. Essentially, given an alignment be-
tween a source word si and a target word tj, how many
children of si and tj are aligned together? For exam-
ple, “tshyr” and “refers” have 2 aligned children which
are “ayda-also” and “aly-to” as shown in Figure 3c.
</bodyText>
<sectionHeader confidence="0.999885" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999782">
4.1 Arabic-English translation system
</subsectionHeader>
<bodyText confidence="0.999978566666666">
The SMT engine is a phrase-based system similar to
the description in (Tillmann, 2006), where various
features are combined within a log-linear framework.
These features include source-to-target phrase transla-
tion score, source-to-target and target-to-source word-
to-word translation scores, language model score, dis-
tortion model scores and word count. The training
data for these features are 7M Arabic-English sentence
pairs, mostly newswire and UN corpora released by
LDC. The parallel sentences have word alignment au-
tomatically generated with HMM and MaxEnt word
aligner (Ge, 2004; Ittycheriah and Roukos, 2005).
Bilingual phrase translations are extracted from these
word-aligned parallel corpora. The language model is
a 5-gram model trained on roughly 3.5 billion English
words.
Our training data contains 72k sentences Arabic-
English machine translation with human corrections
which include of 2.2M words in newswire and weblog
domains. We have a development set of 2,707 sen-
tences, 80K words (dev); an unseen test set of 2,707
sentences, 79K words (test). Feature selection and pa-
rameter tuning has been done on the development set in
which we experimented values of C, n and iterations in
range of [0.5:10], [1:5], and [50:200] respectively. The
final MIRA classifier was trained by using pocket crf
toolkit1 with 100 iterations, hyper-parameter C was 5
and cut-off feature threshold n was 1.
We use precision (P), recall (R) and F-score (F) to
evaluate the classifier performance and they are com-
</bodyText>
<footnote confidence="0.854655">
1http://pocket-crf-1.sourceforge.net/ 215
</footnote>
<bodyText confidence="0.896389333333333">
puted as follow:
P = the number of correctly tagged labels
the number of tagged labels
</bodyText>
<note confidence="0.6877315">
R = the number of correctly tagged labels (8)
the number of reference labels
</note>
<equation confidence="0.9169795">
F = 2*P*R
P+R
</equation>
<subsectionHeader confidence="0.997614">
4.2 Contribution of feature sets
</subsectionHeader>
<bodyText confidence="0.995445375">
We designed our experiments to show the impact
of each feature separately as well as their cumu-
lative impact. We trained two types of classifiers
to predict the error type of each word in MT out-
put, namely Good/Bad with a binary classifier and
Good/Insertion/Substitution/Shift with a 4-class classi-
fier. Each classifier is trained with different feature sets
as follow:
</bodyText>
<listItem confidence="0.998599">
• WPP: we reimplemented WPP calculation based
on n-best lists as described in (Ueffing and Ney,
2007).
• WPP + target POS: only WPP and target POS fea-
tures are used. This is a similar feature set used by
Xiong et al. (2010).
• Our features: the classifier has source side, align-
ment context, and dependency structure features;
WPP and target POS features are excluded.
• WPP + our features: adding our features on top of
WPP.
• WPP + target POS + our features: using all fea-
tures.
</listItem>
<table confidence="0.9981778">
binary 4-class
dev test dev test
WPP 69.3 68.7 64.4 63.7
+ source side 72.1 71.6 66.2 65.7
+ alignment context 71.4 70.9 65.7 65.3
+ dependency structures 69.9 69.5 64.9 64.3
WPP+ target POS 69.6 69.1 64.4 63.9
+ source side 72.3 71.8 66.3 65.8
+ alignment context 71.9 71.2 66 65.6
+ dependency structures 70.4 70 65.1 64.4
</table>
<tableCaption confidence="0.9097415">
Table 1: Contribution of different feature sets measure
in F-score.
</tableCaption>
<bodyText confidence="0.999853">
To evaluate the effectiveness of each feature set, we
apply them on two different baseline systems: using
WPP and WPP+target POS, respectively. We augment
each baseline with our feature sets separately. Ta-
ble 1 shows the contribution in F-score of our proposed
feature sets. Improvements are consistently obtained
when combining the proposed features with baseline
features. Experimental results also indicate that source-
side information, alignment context and dependency
</bodyText>
<figure confidence="0.994759459459459">
Pre
All-Good
WPP+terg
WPP+Ou
74
72
70
68
66
64
62
60
58
59.4
69.
BB a 88.8
88.2
85.8 &amp;quot;1 88.1
64.4 64.4
83.7 83.9
59.4
59.3
All-Good
WPP+target
WPP+Our
68
67
66
65
64
63
62
61
60
59
58
(a) Binary (b) 4-class
</figure>
<figureCaption confidence="0.9756695">
Figure 4: Performance of binary and 4-class classifiers trained with different feature sets on the development and
unseen test sets.
</figureCaption>
<bodyText confidence="0.9991216">
structures have unique and effective levers to improve
the classifier performance. Among the three proposed
feature sets, we observe the source side information
contributes the most gain, which is followed by the
alignment context and dependency structure features.
</bodyText>
<subsectionHeader confidence="0.99965">
4.3 Performance of classifiers
</subsectionHeader>
<bodyText confidence="0.999993666666667">
We trained several classifiers with our proposed feature
sets as well as baseline features. We compare their per-
formances, including a naive baseline All-Good classi-
fier, in which all words in the MT output are labelled
as good translations. Figure 4 shows the performance
of different classifiers trained with different feature sets
on development and unseen test sets. On the unseen test
set our proposed features outperform WPP and target
POS features by 2.8 and 2.4 absolute F-score respec-
tively. Improvements of our features are consistent in
development and unseen sets as well as in binary and
4-class classifiers. We reach the best performance by
combining our proposed features with WPP and target
POS features. Experiments indicate that the gaps in F-
score between our best system with the naive All-Good
system is 12.9 and 6.8 in binary and 4-class cases, re-
spectively. Table 2 presents precision, recall, and F-
score of individual class of the best binary and 4-class
classifiers. It shows that Good label is better predicted
than other labels, meanwhile, Substitution is gener-
ally easier to predict than Insertion and Shift.
</bodyText>
<subsectionHeader confidence="0.99915">
4.4 Correlation between Goodness and HTER
</subsectionHeader>
<bodyText confidence="0.999918888888889">
We estimate sentence level confidence score based
on Equation 7. Figure 5 illustrates the correla-
tion between our proposed goodness sentence level
confidence score and the human-targeted translation
edit rate (HTER). The Pearson correlation between
goodness and HTER is 0.6, while the correlation of
WPP and HTER is 0.52. This experiment shows that
goodness has a large correlation with HTER. The
black bar is the linear regression line. Blue and red
</bodyText>
<page confidence="0.59022">
216
</page>
<table confidence="0.998267375">
Label P R F
Good 74.7 80.6 77.5
Binary
Bad 68 60.1 63.8
Good 70.8 87 78.1
Insertion 37.5 16.9 23.3
Substitution 57.8 44.9 50.5
Shift 35.2 14.1 20.1
</table>
<tableCaption confidence="0.693471">
Table 2: Detailed performance in precision, recall
and F-score of binary and 4-class classifiers with
WPP+target POS+Our features on the unseen test set.
</tableCaption>
<bodyText confidence="0.99716">
bars are thresholds used to visualize good and bad sen-
tences respectively. We also experimented goodness
computation in Equation 7 using geometric mean and
harmonic mean; their Pearson correlation values are 0.5
and 0.35 respectively.
</bodyText>
<sectionHeader confidence="0.882634" genericHeader="method">
5 Improving MT quality with N-best list
reranking
</sectionHeader>
<bodyText confidence="0.999872588235294">
Experiments reporting in Section 4 indicate that the
proposed confidence measure has a high correlation
with HTER. However, it is not very clear if the core MT
system can benefit from confidence measure by provid-
ing better translations. To investigate this question we
present experimental results for the n-best list rerank-
ing task.
The MT system generates top n hypotheses and for
each hypothesis we compute sentence-level confidence
scores. The best candidate is the hypothesis with high-
est confidence score. Table 3 shows the performance of
reranking systems using goodness scores from our best
classifier in various n-best sizes. We obtained 0.7 TER
reduction and 0.4 BLEU point improvement on the de-
velopment set with a 5-best list. On the unseen test, we
obtained 0.6 TER reduction and 0.2 BLEU point im-
provement. Although, the improvement of BLEU score
</bodyText>
<figure confidence="0.997297">
4-class
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 20 40
</figure>
<figureCaption confidence="0.998419">
Figure 5: Correlation between Goodness and HTER.
</figureCaption>
<table confidence="0.999869363636364">
Dev Test
TER BLEU TER BLEU
Baseline 49.9 31.0 50.2 30.6
2-best 49.5 31.4 49.9 30.8
5-best 49.2 31.4 49.6 30.8
10-best 49.2 31.2 49.5 30.8
20-best 49.1 31.0 49.3 30.7
30-best 49.0 31.0 49.3 30.6
40-best 49.0 31.0 49.4 30.5
50-best 49.1 30.9 49.4 30.5
100-best 49.0 30.9 49.3 30.5
</table>
<tableCaption confidence="0.999953">
Table 3: Reranking performance with goodness score.
</tableCaption>
<bodyText confidence="0.999308222222222">
is not obvious, TER reductions are consistent in both
development and unseen sets. Figure 6 shows the im-
provement of reranking with goodness score. Besides,
the figure illustrates the upper and lower bound perfor-
mances with TER metric in which the lower bound is
our baseline system and the upper bound is the best hy-
pothesis in a given n-best list. Oracle scores of each n-
best list are computed by choosing the translation can-
didate with lowest TER score.
</bodyText>
<sectionHeader confidence="0.923455" genericHeader="method">
6 Visualizing translation errors
</sectionHeader>
<bodyText confidence="0.999882272727273">
Besides the application of confidence score in the n-
best list reranking task, we propose a method to visual-
ize translation error using confidence scores. Our pur-
pose is to visualize word and sentence-level confidence
scores with the following objectives 1) easy for spotting
translations errors; 2) simple and intuitive; and 3) help-
ful for post-editing productivity. We define three cate-
gories of translation quality (good/bad/decent) on both
word and sentence level. On word level, the marginal
probability of good label is used to visualize translation
errors as follow:
</bodyText>
<equation confidence="0.8488795">
good if p(yi = Good|S) &gt; 0.8
bad if p(yi = Good|S) &lt; 0.45
</equation>
<bodyText confidence="0.464839">
decent otherwise
</bodyText>
<subsectionHeader confidence="0.331185">
N-best size
</subsectionHeader>
<figureCaption confidence="0.983723">
Figure 6: A comparison between reranking and oracle
scores with different n-best size in TER metric on the
development set.
</figureCaption>
<bodyText confidence="0.6583635">
On sentence level, the goodness score is used as follow:
ess
</bodyText>
<figure confidence="0.7337068">
good if goodness(S) &gt; 0.7
bad if goodness(S) &lt; 0.5
decent otherwise
Choices Intention
big bad
Font size small good
medium decent
red bad
Colors black good
orange decent
</figure>
<tableCaption confidence="0.954035">
Table 4: Choices of layout
</tableCaption>
<bodyText confidence="0.999708444444445">
Different font sizes and colors are used to catch the
attention of post-editors whenever translation errors are
likely to appear as shown in Table 4. Colors are ap-
plied on word level, while font size is applied on both
word and sentence level. The idea of using font size
and colour to visualize translation confidence is simi-
lar to the idea of using tag/word cloud to describe the
content of websites2. The reason we are using big font
size and red color is to attract post-editors’ attention
and help them find translation errors quickly. Figure 7
shows an example of visualizing confidence scores by
font size and colours. It shows that “not to deprive
yourself”, displayed in big font and red color, is likely
to be bad translations. Meanwhile, other words, such
as “you”, “different”, “from”, and “assimilation”, dis-
played in small font and black color, are likely to be
good translation. Medium font and orange color words
are decent translations.
</bodyText>
<figure confidence="0.930798272727273">
2http://en.wikipedia.org/wiki/Tag cloud
TER
47
46
45
51
50
49
48
Oracle
Our models
Baseline
44
43
42
1 2 5 10 20 30 40 50 100
�
�
�
Li =
217
LS = I
Source c. Pe1�—
MT output c�l�l �
the pol
to intro
effects
We predict the pol
and visualize to intr
reduci
Human the su
correction to intro
climat
</figure>
<figureCaption confidence="0.988352">
Figure 7: MT errors visualization based on confidence scores.
</figureCaption>
<sectionHeader confidence="0.998376" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999844269230769">
In this paper we proposed a method to predict con-
fidence scores for machine translated words and sen-
tences based on a feature-rich classifier using linguistic
and context features. Our major contributions are three
novel feature sets including source side information,
alignment context, and dependency structures. Experi-
mental results show that by combining the source side
information, alignment context, and dependency struc-
ture features with word posterior probability and tar-
get POS context (Ueffing &amp; Ney 2007; Xiong et al.,
2010), the MT error prediction accuracy is increased
from 69.1 to 72.2 in F-score. Our framework is able to
predict error types namely insertion, substitution and
shift. The Pearson correlation with human judgement
increases from 0.52 to 0.6. Furthermore, we show that
the proposed confidence scores can help the MT sys-
tem to select better translations and as a result improve-
ments between 0.4 and 0.9 TER reduction are obtained.
Finally, we demonstrate a prototype to visualize trans-
lation errors.
This work can be expanded in several directions.
First, we plan to apply confidence estimation to per-
form a second-pass constraint decoding. After the first
pass decoding, our confidence estimation model can la-
bel which word is likely to be correctly translated. The
second-pass decoding utilizes the confidence informa-
</bodyText>
<page confidence="0.667378">
218
</page>
<bodyText confidence="0.999968285714286">
tion to constrain the search space and hopefully can
find a better hypothesis than in the first pass. This idea
is very similar to the multi-pass decoding strategy em-
ployed by speech recognition engines. Moreover, we
also intend to perform a user study on our visualiza-
tion prototype to see if it increases the productivity of
post-editors.
</bodyText>
<sectionHeader confidence="0.995577" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998974">
We would like to thank Christoph Tillmann and the
IBM machine translation team for their supports. Also,
we would like to thank anonymous reviewers, Qin Gao,
Joy Zhang, and Stephan Vogel for their helpful com-
ments.
</bodyText>
<sectionHeader confidence="0.995177" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99985132231405">
Nguyen Bach, Matthias Eck, Paisarn Charoenpornsawat,
Thilo Khler, Sebastian Stker, ThuyLinh Nguyen, Roger
Hsiao, Alex Waibel, Stephan Vogel, Tanja Schultz, and
Alan Black. 2007. The CMU TransTac 2007 Eyes-free
and Hands-free Two-way Speech-to-Speech Translation
System. In Proceedings of the IWSLT’07, Trento, Italy.
Nguyen Bach, Qin Gao, and Stephan Vogel. 2009. Source-
side dependency tree reordering models with subtree
movements and constraints. In Proceedings of the
MTSummit-XII, Ottawa, Canada, August. International
Association for Machine Translation.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence estimation for machine
translation. In The JHU Workshop Final Report, Balti-
more, Maryland, USA, April.
David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001
new features for statistical machine translation. In Pro-
ceedings of HLT-ACL, pages 218–226, Boulder, Colorado,
June. Association for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal of Ma-
chine Learning Research, 3:951–991.
Niyu Ge. 2004. Max-posterior HMM alignment for machine
translation. In Presentation given at DARPA/TIDES NIST
MT Evaluation workshop.
Nizar Habash and Jun Hu. 2009. Improving arabic-chinese
statistical machine translation using english as pivot lan-
guage. In Proceedings of the 4th Workshop on Statisti-
cal Machine Translation, pages 173–181, Morristown, NJ,
USA. Association for Computational Linguistics.
Almut Silja Hildebrand and Stephan Vogel. 2008. Combi-
nation of machine translation systems via hypothesis se-
lection from combined n-best lists. In Proceedings of the
8th Conference of the AMTA, pages 254–261, Waikiki,
Hawaii, October.
Fei Huang. 2009. Confidence measure for word align-
ment. In Proceedings of the ACL-IJCNLP ’09, pages
932–940, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A maximum
entropy word aligner for arabic-english machine transla-
tion. In Proceedings of the HTL-EMNLP’05, pages 89–
96, Morristown, NJ, USA. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL’07,
pages 177–180, Prague, Czech Republic, June.
Yanjun Ma, Sylwia Ozdowska, Yanli Sun, and Andy Way.
2008. Improving word alignment using syntactic depen-
dencies. In Proceedings of the ACL-08: HLT SSST-2,
pages 69–77, Columbus, OH.
Marie-Catherine Marneffe, Bill MacCartney, and Christopher
Manning. 2006. Generating typed dependency parses
from phrase structure parses. In Proceedings of LREC’06,
Genoa, Italy.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Flexible text segmentation with structured mul-
tilabel classification. In Proceedings of Human Lan-
guage Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pages 987–
994, Vancouver, British Columbia, Canada, October. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: A method for automatic evaluation
of machine translation. In Proceedings of ACL’02, pa�gq
311–318, Philadelphia, PA, July. 2 1
Chris Quirk. 2004. Training a sentence-level machine trans-
lation confidence measure. In Proceedings of the 4th
LREC.
Sylvain Raybaud, Caroline Lavecchia, David Langlois, and
Kamel Smaili. 2009. Error detection for statistical ma-
chine translation using linguistic features. In Proceedings
of the 13th EAMT, Barcelona, Spain, May.
Binyamin Rozenfeld, Ronen Feldman, and Moshe Fresko.
2006. A systematic cross-comparison of sequence clas-
sifiers. In Proceedings of the SDM, pages 563–567,
Bethesda, MD, USA, April.
Alberto Sanchis, Alfons Juan, and Enrique Vidal. 2007. Esti-
mation of confidence measures for machine translation. In
Proceedings of the MT Summit XI, Copenhagen, Denmark.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new
string-to-dependency machine translation algorithm with
a target dependency language model. In Proceedings of
ACL-08: HLT, pages 577–585, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Pro-
ceedings of AMTA’06, pages 223–231, August.
Radu Soricut and Abdessamad Echihabi. 2010. Trustrank:
Inducing trust in automatic translations via ranking. In
Proceedings of the 48th ACL, pages 612–621, Uppsala,
Sweden, July. Association for Computational Linguistics.
Lucia Specia, Zhuoran Wang, Marco Turchi, John Shawe-
Taylor, and Craig Saunders. 2009. Improving the con-
fidence of machine translation quality estimates. In Pro-
ceedings of the MT Summit XII, Ottawa, Canada.
Christoph Tillmann. 2006. Efficient dynamic programming
search algorithms for phrase-based SMT. In Proceedings
of the Workshop on Computationally Hard Problems and
Joint Inference in Speech and Language Processing, pages
9–16, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Nicola Ueffing and Hermann Ney. 2007. Word-level confi-
dence estimation for machine translation. Computational
Linguistics, 33(1):9–40.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statisti-
cal machine translation. In Proceedings of the EMNLP-
CoNLL, pages 764–773, Prague, Czech Republic, June.
Association for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Error de-
tection for statistical machine translation using linguistic
features. In Proceedings of the 48th ACL, pages 604–
611, Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.556154">
<title confidence="0.8704055">Goodness: A Method for Measuring Machine Translation Confidence Language Technologies</title>
<affiliation confidence="0.959867">Carnegie Mellon</affiliation>
<address confidence="0.951043">Pittsburgh, PA 15213,</address>
<email confidence="0.997964">nbach@cs.cmu.edu</email>
<author confidence="0.997104">Fei Huang</author>
<author confidence="0.997104">Yaser</author>
<affiliation confidence="0.995556">IBM T.J. Watson Research</affiliation>
<address confidence="0.913577">1101 Kitchawan Yorktown Heights, NY 10567,</address>
<abstract confidence="0.998472409090909">State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output. However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict wordlevel and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuis increased from F-score. The Pearson correlation between the proposed confidence measure and the human-targeted edit rate (HTER) is Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Matthias Eck</author>
<author>Paisarn Charoenpornsawat</author>
<author>Thilo Khler</author>
<author>Sebastian Stker</author>
<author>ThuyLinh Nguyen</author>
<author>Roger Hsiao</author>
<author>Alex Waibel</author>
<author>Stephan Vogel</author>
<author>Tanja Schultz</author>
<author>Alan Black</author>
</authors>
<title>Eyes-free and Hands-free Two-way Speech-to-Speech Translation System.</title>
<date>2007</date>
<booktitle>The CMU TransTac</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="1495" citStr="Bach et al., 2007" startWordPosition="212" endWordPosition="215">on correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity. 1 Introduction State-of-the-art Machine Translation (MT) systems are making progress to generate more usable translation outputs. In particular, statistical machine translation systems (Koehn et al., 2007; Bach et al., 2007; Shen et al., 2008) have advanced to a state that the translation quality for certain language pairs (e.g. SpanishEnglish, French-English, Iraqi-English) in certain domains (e.g. broadcasting news, force-protection, travel) is acceptable to users. However, a remaining open question is how to predict confidence scores for machine translated words and sentences. An MT system typically returns the best translation candidate from its search space, but still has no reliable way to inform users which word is likely to be correctly translated and how confident it is about the whole sentence. Such in</context>
<context position="3662" citStr="Bach et al., 2007" startWordPosition="559" endWordPosition="562">n (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the MT output into the source language to see whether the output of backward translation matches the original source sentence. However, previous studies had a few shortcomings. First, source-side features were not extensively investigated. Blatz et al.(2004) only investigated source ngram frequency statistics and source language model features, while other work mainly focused on target side features. Second, previous work attempted to incorporate more features but faced scalability issues, i.e., to train many features we need many training examples and to tr</context>
</contexts>
<marker>Bach, Eck, Charoenpornsawat, Khler, Stker, Nguyen, Hsiao, Waibel, Vogel, Schultz, Black, 2007</marker>
<rawString>Nguyen Bach, Matthias Eck, Paisarn Charoenpornsawat, Thilo Khler, Sebastian Stker, ThuyLinh Nguyen, Roger Hsiao, Alex Waibel, Stephan Vogel, Tanja Schultz, and Alan Black. 2007. The CMU TransTac 2007 Eyes-free and Hands-free Two-way Speech-to-Speech Translation System. In Proceedings of the IWSLT’07, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Sourceside dependency tree reordering models with subtree movements and constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the MTSummit-XII,</booktitle>
<institution>International Association for Machine Translation.</institution>
<location>Ottawa, Canada,</location>
<contexts>
<context position="16286" citStr="Bach et al., 2009" startWordPosition="2720" endWordPosition="2723">r agreement Structures wydyf an hdhh alamlyt ayda tshyr aly adm qdrt almtaddt aljnsyt alqwat al He adds that this process also refers to the inability of the multinational nava RB VBZ TO DT NN IN DT JJ (c) Childrn arment Children Agreement: Figure 3: Dependency structures features. 3.3 Source and target dependency structure features The contextual and source information in the previous sections only take into account surface structures of source and target sentences. Meanwhile, dependency structures have been extensively used in various translation systems (Shen et al., 2008; Ma et al., 2008; Bach et al., 2009). The adoption of dependency structures might enable the classifier to utilize deep structures to predict translation errors. Source and target structures are unlikely to be isomorphic as shown in Figure 3a. However, we expect some high-level linguistic structures are likely to transfer across certain language pairs. For example, prepositional phrases (PP) in Arabic and English are similar in a sense that PPs generally appear at the end of the sentence (after all the verbal arguments) and to a lesser extent at its beginning (Habash and Hu, 2009). We use the Stanford parser to obtain dependency</context>
</contexts>
<marker>Bach, Gao, Vogel, 2009</marker>
<rawString>Nguyen Bach, Qin Gao, and Stephan Vogel. 2009. Sourceside dependency tree reordering models with subtree movements and constraints. In Proceedings of the MTSummit-XII, Ottawa, Canada, August. International Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In The JHU Workshop Final Report,</booktitle>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="2826" citStr="Blatz et al. (2004)" startWordPosition="420" endWordPosition="423">y of machine translation in many areas. For example, a post-editor would like to quickly identify which sentences might be incorrectly translated and in need of correction. Other areas, such as cross-lingual question-answering, information extraction and retrieval, can also benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) in</context>
<context position="13356" citStr="Blatz et al. (2004)" startWordPosition="2184" endWordPosition="2187">urce POS and phrase context features: This feature set allows us to look at the surrounding context of the source phrase. For example, in Figure 1c we have “hdhh alamlyt” generates “process”. We also have other information such as on the right hand side the next two phrases are “ayda” and “tshyr” or the sequence of source target POS on the right hand side is “RB VBP”. An example of this type of feature is � 1 if source-POS-context=“ RB VBP ” f104(process) 0 otherwise 3.2 Alignment context features The IBM Model-1 feature performed relatively well in comparison with the WPP feature as shown by Blatz et al. (2004). In our work, we incorporate not only the Sour VBP IN DT wydyf an hdhh He adds that this BP NN So POS RB Tar OS P VBZ IN DTNN RB VBP IN N (c) Left target wydyf an hdhh alamlyt ayda (d) Source POS &amp; right target rocess also NN RB VBZ TO DT (b) Right source PN N DTNN RB VBP IN NN g alamlyt ayda tshyr aly a wydyf an h alamlyt ayda tshyr aly adm Souce process also refers to the i S PO B process MT o also refers to th NN RB VBZ TO D (a) Source-Target dependency S VBP IN DT DTNN RB Figure 2: Alignment context features. He adds that this process also PRP VBZ IN DT NN RB (a) Left source wydyf an h M </context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In The JHU Workshop Final Report, Baltimore, Maryland, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-ACL,</booktitle>
<pages>218--226</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="8556" citStr="Chiang et al., 2009" startWordPosition="1351" endWordPosition="1354">To obtain the label, we choose the class with the highest score as the predicted label for that data instance. To learn optimized weights, we use the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003; McDonald et al., 2005) which is an online learner closely related to both the support vector machine and perceptron learning framework. MIRA has been shown to provide state-of-the-art performance for sequential labelling task (Rozenfeld et al., 2006), and is also able to provide an efficient mechanism to train and optimize MT systems with lots of features (Watanabe et al., 2007; Chiang et al., 2009). In general, weights are updated at each step time t according to the following rule: wt+1 = arg min,,,t+1 ||wt+1 − wt|| (2) s.t. score(x, y) &gt; score(x, y&apos;) + L(y, y&apos;) where L(y, y&apos;) is a measure of the loss of using y&apos; instead of the true label y. In this problem L(y, y&apos;) is 0-1 loss function. More specifically, for each instance xi in the training data at a time t we find the label with the highest score: y&apos; = arg max score(xi,y) (3) y the weight vector is updated as follow wt+1 = wt + T(f(xi, y) − f(xi, y&apos;)) (4) T can be interpreted as a step size; when T is a large number we want to updat</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of HLT-ACL, pages 218–226, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="8152" citStr="Crammer and Singer, 2003" startWordPosition="1286" endWordPosition="1289">ict deletion errors in the MT output. 2.2 Word-level model In our problem, a training instance is a word from MT output, and its label when the MT sentence is aligned with the human correction. Given a training instance x, y is the true label of x; f stands for its feature vector f(x, y); and w is feature weight vector. We define a feature-rich classifier score(x, y) as follow score(x, y) = w.f(x, y) (1) To obtain the label, we choose the class with the highest score as the predicted label for that data instance. To learn optimized weights, we use the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003; McDonald et al., 2005) which is an online learner closely related to both the support vector machine and perceptron learning framework. MIRA has been shown to provide state-of-the-art performance for sequential labelling task (Rozenfeld et al., 2006), and is also able to provide an efficient mechanism to train and optimize MT systems with lots of features (Watanabe et al., 2007; Chiang et al., 2009). In general, weights are updated at each step time t according to the following rule: wt+1 = arg min,,,t+1 ||wt+1 − wt|| (2) s.t. score(x, y) &gt; score(x, y&apos;) + L(y, y&apos;) where L(y, y&apos;) is a measure</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
</authors>
<title>Max-posterior HMM alignment for machine translation.</title>
<date>2004</date>
<booktitle>In Presentation given at DARPA/TIDES NIST MT Evaluation workshop.</booktitle>
<contexts>
<context position="18853" citStr="Ge, 2004" startWordPosition="3129" endWordPosition="3130">ish translation system The SMT engine is a phrase-based system similar to the description in (Tillmann, 2006), where various features are combined within a log-linear framework. These features include source-to-target phrase translation score, source-to-target and target-to-source wordto-word translation scores, language model score, distortion model scores and word count. The training data for these features are 7M Arabic-English sentence pairs, mostly newswire and UN corpora released by LDC. The parallel sentences have word alignment automatically generated with HMM and MaxEnt word aligner (Ge, 2004; Ittycheriah and Roukos, 2005). Bilingual phrase translations are extracted from these word-aligned parallel corpora. The language model is a 5-gram model trained on roughly 3.5 billion English words. Our training data contains 72k sentences ArabicEnglish machine translation with human corrections which include of 2.2M words in newswire and weblog domains. We have a development set of 2,707 sentences, 80K words (dev); an unseen test set of 2,707 sentences, 79K words (test). Feature selection and parameter tuning has been done on the development set in which we experimented values of C, n and </context>
</contexts>
<marker>Ge, 2004</marker>
<rawString>Niyu Ge. 2004. Max-posterior HMM alignment for machine translation. In Presentation given at DARPA/TIDES NIST MT Evaluation workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Jun Hu</author>
</authors>
<title>Improving arabic-chinese statistical machine translation using english as pivot language.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation,</booktitle>
<pages>173--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16837" citStr="Habash and Hu, 2009" startWordPosition="2808" endWordPosition="2811">ation systems (Shen et al., 2008; Ma et al., 2008; Bach et al., 2009). The adoption of dependency structures might enable the classifier to utilize deep structures to predict translation errors. Source and target structures are unlikely to be isomorphic as shown in Figure 3a. However, we expect some high-level linguistic structures are likely to transfer across certain language pairs. For example, prepositional phrases (PP) in Arabic and English are similar in a sense that PPs generally appear at the end of the sentence (after all the verbal arguments) and to a lesser extent at its beginning (Habash and Hu, 2009). We use the Stanford parser to obtain dependency trees and POS tags (Marneffe et al., 2006). Child-Father agreement: The motivation is to take advantage of the long distance dependency relations between source and target words. Given an alignment between a source word si and a target word tp A childVBP IN DT DTNN RB VBP IN NN NN DTJJ DTJJ DTNNS DT PRP VBZ IN DT NN JJ father agreement exists when sk is aligned to tl, where sk and tl are father of si and tj in source and target dependency trees, respectively. Figure 3b illustrates that “tshyr” and “refers” have a child-father agreement. To veri</context>
</contexts>
<marker>Habash, Hu, 2009</marker>
<rawString>Nizar Habash and Jun Hu. 2009. Improving arabic-chinese statistical machine translation using english as pivot language. In Proceedings of the 4th Workshop on Statistical Machine Translation, pages 173–181, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Combination of machine translation systems via hypothesis selection from combined n-best lists.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th Conference of the AMTA,</booktitle>
<pages>254--261</pages>
<location>Waikiki, Hawaii,</location>
<contexts>
<context position="2715" citStr="Hildebrand and Vogel, 2008" startWordPosition="402" endWordPosition="405">Such information is vital * Work done during an internship at IBM T.J. Watson 211 Research Center to realize the utility of machine translation in many areas. For example, a post-editor would like to quickly identify which sentences might be incorrectly translated and in need of correction. Other areas, such as cross-lingual question-answering, information extraction and retrieval, can also benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target</context>
</contexts>
<marker>Hildebrand, Vogel, 2008</marker>
<rawString>Almut Silja Hildebrand and Stephan Vogel. 2008. Combination of machine translation systems via hypothesis selection from combined n-best lists. In Proceedings of the 8th Conference of the AMTA, pages 254–261, Waikiki, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
</authors>
<title>Confidence measure for word alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP ’09,</booktitle>
<pages>932--940</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14274" citStr="Huang, 2009" startWordPosition="2386" endWordPosition="2387"> wydyf an h alamlyt ayda tshyr aly adm Souce process also refers to the i S PO B process MT o also refers to th NN RB VBZ TO D (a) Source-Target dependency S VBP IN DT DTNN RB Figure 2: Alignment context features. He adds that this process also PRP VBZ IN DT NN RB (a) Left source wydyf an h M T ouput refersHe dds that t put e add that th WPP: 1.0 0.67 .0 .0 10 0.67 ... T POS RP BZ IN NN RB VBZ TO DT IBM Model-1 feature but also the surrounding alignment context. The key intuition is that collocation is a reliable indicator for judging if a target word is generated by a particular source word (Huang, 2009). Moreover, the IBM Model-1 feature was already used in several steps of a translation system such as word alignment, phrase extraction and scoring. Also the impact of this feature alone might fade away when the MT system is scaled up. We obtain word-to-word alignments by applying IBM Model-1 to bilingual phrase pairs that generated the MT output. The IBM Model-1 assumes one target word can only be aligned to one source word. Therefore, given a target word we can always identify which source word it is aligned to. Source alignment context feature: We anchor the target word and derive context f</context>
</contexts>
<marker>Huang, 2009</marker>
<rawString>Fei Huang. 2009. Confidence measure for word alignment. In Proceedings of the ACL-IJCNLP ’09, pages 932–940, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for arabic-english machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the HTL-EMNLP’05,</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18884" citStr="Ittycheriah and Roukos, 2005" startWordPosition="3131" endWordPosition="3134">ation system The SMT engine is a phrase-based system similar to the description in (Tillmann, 2006), where various features are combined within a log-linear framework. These features include source-to-target phrase translation score, source-to-target and target-to-source wordto-word translation scores, language model score, distortion model scores and word count. The training data for these features are 7M Arabic-English sentence pairs, mostly newswire and UN corpora released by LDC. The parallel sentences have word alignment automatically generated with HMM and MaxEnt word aligner (Ge, 2004; Ittycheriah and Roukos, 2005). Bilingual phrase translations are extracted from these word-aligned parallel corpora. The language model is a 5-gram model trained on roughly 3.5 billion English words. Our training data contains 72k sentences ArabicEnglish machine translation with human corrections which include of 2.2M words in newswire and weblog domains. We have a development set of 2,707 sentences, 80K words (dev); an unseen test set of 2,707 sentences, 79K words (test). Feature selection and parameter tuning has been done on the development set in which we experimented values of C, n and iterations in range of [0.5:10]</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for arabic-english machine translation. In Proceedings of the HTL-EMNLP’05, pages 89– 96, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="1476" citStr="Koehn et al., 2007" startWordPosition="208" endWordPosition="211">n F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity. 1 Introduction State-of-the-art Machine Translation (MT) systems are making progress to generate more usable translation outputs. In particular, statistical machine translation systems (Koehn et al., 2007; Bach et al., 2007; Shen et al., 2008) have advanced to a state that the translation quality for certain language pairs (e.g. SpanishEnglish, French-English, Iraqi-English) in certain domains (e.g. broadcasting news, force-protection, travel) is acceptable to users. However, a remaining open question is how to predict confidence scores for machine translated words and sentences. An MT system typically returns the best translation candidate from its search space, but still has no reliable way to inform users which word is likely to be correctly translated and how confident it is about the whol</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL’07, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Sylwia Ozdowska</author>
<author>Yanli Sun</author>
<author>Andy Way</author>
</authors>
<title>Improving word alignment using syntactic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT SSST-2,</booktitle>
<pages>69--77</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="16266" citStr="Ma et al., 2008" startWordPosition="2716" endWordPosition="2719">14 (b)Child-Father agreement Structures wydyf an hdhh alamlyt ayda tshyr aly adm qdrt almtaddt aljnsyt alqwat al He adds that this process also refers to the inability of the multinational nava RB VBZ TO DT NN IN DT JJ (c) Childrn arment Children Agreement: Figure 3: Dependency structures features. 3.3 Source and target dependency structure features The contextual and source information in the previous sections only take into account surface structures of source and target sentences. Meanwhile, dependency structures have been extensively used in various translation systems (Shen et al., 2008; Ma et al., 2008; Bach et al., 2009). The adoption of dependency structures might enable the classifier to utilize deep structures to predict translation errors. Source and target structures are unlikely to be isomorphic as shown in Figure 3a. However, we expect some high-level linguistic structures are likely to transfer across certain language pairs. For example, prepositional phrases (PP) in Arabic and English are similar in a sense that PPs generally appear at the end of the sentence (after all the verbal arguments) and to a lesser extent at its beginning (Habash and Hu, 2009). We use the Stanford parser </context>
</contexts>
<marker>Ma, Ozdowska, Sun, Way, 2008</marker>
<rawString>Yanjun Ma, Sylwia Ozdowska, Yanli Sun, and Andy Way. 2008. Improving word alignment using syntactic dependencies. In Proceedings of the ACL-08: HLT SSST-2, pages 69–77, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC’06,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="16929" citStr="Marneffe et al., 2006" startWordPosition="2824" endWordPosition="2827">endency structures might enable the classifier to utilize deep structures to predict translation errors. Source and target structures are unlikely to be isomorphic as shown in Figure 3a. However, we expect some high-level linguistic structures are likely to transfer across certain language pairs. For example, prepositional phrases (PP) in Arabic and English are similar in a sense that PPs generally appear at the end of the sentence (after all the verbal arguments) and to a lesser extent at its beginning (Habash and Hu, 2009). We use the Stanford parser to obtain dependency trees and POS tags (Marneffe et al., 2006). Child-Father agreement: The motivation is to take advantage of the long distance dependency relations between source and target words. Given an alignment between a source word si and a target word tp A childVBP IN DT DTNN RB VBP IN NN NN DTJJ DTJJ DTNNS DT PRP VBZ IN DT NN JJ father agreement exists when sk is aligned to tl, where sk and tl are father of si and tj in source and target dependency trees, respectively. Figure 3b illustrates that “tshyr” and “refers” have a child-father agreement. To verify our intuition, we analysed 243K words of manual aligned Arabic-English bitext. We observe</context>
</contexts>
<marker>Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine Marneffe, Bill MacCartney, and Christopher Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC’06, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Flexible text segmentation with structured multilabel classification.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>987--994</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="8176" citStr="McDonald et al., 2005" startWordPosition="1290" endWordPosition="1293"> MT output. 2.2 Word-level model In our problem, a training instance is a word from MT output, and its label when the MT sentence is aligned with the human correction. Given a training instance x, y is the true label of x; f stands for its feature vector f(x, y); and w is feature weight vector. We define a feature-rich classifier score(x, y) as follow score(x, y) = w.f(x, y) (1) To obtain the label, we choose the class with the highest score as the predicted label for that data instance. To learn optimized weights, we use the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003; McDonald et al., 2005) which is an online learner closely related to both the support vector machine and perceptron learning framework. MIRA has been shown to provide state-of-the-art performance for sequential labelling task (Rozenfeld et al., 2006), and is also able to provide an efficient mechanism to train and optimize MT systems with lots of features (Watanabe et al., 2007; Chiang et al., 2009). In general, weights are updated at each step time t according to the following rule: wt+1 = arg min,,,t+1 ||wt+1 − wt|| (2) s.t. score(x, y) &gt; score(x, y&apos;) + L(y, y&apos;) where L(y, y&apos;) is a measure of the loss of using y&apos;</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Flexible text segmentation with structured multilabel classification. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 987– 994, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02, pa�gq 311–318,</booktitle>
<volume>2</volume>
<location>Philadelphia, PA,</location>
<contexts>
<context position="4849" citStr="Papineni et al., 2002" startWordPosition="732" endWordPosition="735">ed many training examples and to train discriminatively we need to search through all possible translations of each training example. Another issue of previous work was that they are all trained with BLEU/TER score computing against Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 211–219, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the translation references which is different from predicting the human-targeted translation edit rate (HTER) which is crucial in post-editing applications (Snover et al., 2006; Papineni et al., 2002). Finally, the backtranslation approach faces a serious issue when forward and backward translation models are symmetric. In this case, back-translation will not be very informative to indicate forward translation quality. In this paper, we predict error types of each word in the MT output with a confidence score, extend it to the sentence level, then apply it to n-best list reranking task to improve MT quality, and finally design a visualization prototype. We try to answer the following questions: • Can we use a rich feature set such as sourceside information, alignment context, and dependenc</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL’02, pa�gq 311–318, Philadelphia, PA, July. 2 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
</authors>
<title>Training a sentence-level machine translation confidence measure.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th LREC.</booktitle>
<contexts>
<context position="3058" citStr="Quirk, 2004" startWordPosition="462" endWordPosition="463">extraction and retrieval, can also benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2</context>
</contexts>
<marker>Quirk, 2004</marker>
<rawString>Chris Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of the 4th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Raybaud</author>
<author>Caroline Lavecchia</author>
<author>David Langlois</author>
<author>Kamel Smaili</author>
</authors>
<title>Error detection for statistical machine translation using linguistic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th EAMT,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3102" citStr="Raybaud et al., 2009" startWordPosition="468" endWordPosition="471">benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the </context>
</contexts>
<marker>Raybaud, Lavecchia, Langlois, Smaili, 2009</marker>
<rawString>Sylvain Raybaud, Caroline Lavecchia, David Langlois, and Kamel Smaili. 2009. Error detection for statistical machine translation using linguistic features. In Proceedings of the 13th EAMT, Barcelona, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binyamin Rozenfeld</author>
<author>Ronen Feldman</author>
<author>Moshe Fresko</author>
</authors>
<title>A systematic cross-comparison of sequence classifiers.</title>
<date>2006</date>
<booktitle>In Proceedings of the SDM,</booktitle>
<pages>563--567</pages>
<location>Bethesda, MD, USA,</location>
<contexts>
<context position="8404" citStr="Rozenfeld et al., 2006" startWordPosition="1323" endWordPosition="1326">ands for its feature vector f(x, y); and w is feature weight vector. We define a feature-rich classifier score(x, y) as follow score(x, y) = w.f(x, y) (1) To obtain the label, we choose the class with the highest score as the predicted label for that data instance. To learn optimized weights, we use the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003; McDonald et al., 2005) which is an online learner closely related to both the support vector machine and perceptron learning framework. MIRA has been shown to provide state-of-the-art performance for sequential labelling task (Rozenfeld et al., 2006), and is also able to provide an efficient mechanism to train and optimize MT systems with lots of features (Watanabe et al., 2007; Chiang et al., 2009). In general, weights are updated at each step time t according to the following rule: wt+1 = arg min,,,t+1 ||wt+1 − wt|| (2) s.t. score(x, y) &gt; score(x, y&apos;) + L(y, y&apos;) where L(y, y&apos;) is a measure of the loss of using y&apos; instead of the true label y. In this problem L(y, y&apos;) is 0-1 loss function. More specifically, for each instance xi in the training data at a time t we find the label with the highest score: y&apos; = arg max score(xi,y) (3) y the w</context>
</contexts>
<marker>Rozenfeld, Feldman, Fresko, 2006</marker>
<rawString>Binyamin Rozenfeld, Ronen Feldman, and Moshe Fresko. 2006. A systematic cross-comparison of sequence classifiers. In Proceedings of the SDM, pages 563–567, Bethesda, MD, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Sanchis</author>
<author>Alfons Juan</author>
<author>Enrique Vidal</author>
</authors>
<title>Estimation of confidence measures for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the MT</booktitle>
<location>Summit XI, Copenhagen, Denmark.</location>
<contexts>
<context position="3080" citStr="Sanchis et al., 2007" startWordPosition="464" endWordPosition="467">d retrieval, can also benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it tr</context>
</contexts>
<marker>Sanchis, Juan, Vidal, 2007</marker>
<rawString>Alberto Sanchis, Alfons Juan, and Enrique Vidal. 2007. Estimation of confidence measures for machine translation. In Proceedings of the MT Summit XI, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>577--585</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1515" citStr="Shen et al., 2008" startWordPosition="216" endWordPosition="219">een the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity. 1 Introduction State-of-the-art Machine Translation (MT) systems are making progress to generate more usable translation outputs. In particular, statistical machine translation systems (Koehn et al., 2007; Bach et al., 2007; Shen et al., 2008) have advanced to a state that the translation quality for certain language pairs (e.g. SpanishEnglish, French-English, Iraqi-English) in certain domains (e.g. broadcasting news, force-protection, travel) is acceptable to users. However, a remaining open question is how to predict confidence scores for machine translated words and sentences. An MT system typically returns the best translation candidate from its search space, but still has no reliable way to inform users which word is likely to be correctly translated and how confident it is about the whole sentence. Such information is vital *</context>
<context position="16249" citStr="Shen et al., 2008" startWordPosition="2712" endWordPosition="2715"> “to” 0 otherwise 214 (b)Child-Father agreement Structures wydyf an hdhh alamlyt ayda tshyr aly adm qdrt almtaddt aljnsyt alqwat al He adds that this process also refers to the inability of the multinational nava RB VBZ TO DT NN IN DT JJ (c) Childrn arment Children Agreement: Figure 3: Dependency structures features. 3.3 Source and target dependency structure features The contextual and source information in the previous sections only take into account surface structures of source and target sentences. Meanwhile, dependency structures have been extensively used in various translation systems (Shen et al., 2008; Ma et al., 2008; Bach et al., 2009). The adoption of dependency structures might enable the classifier to utilize deep structures to predict translation errors. Source and target structures are unlikely to be isomorphic as shown in Figure 3a. However, we expect some high-level linguistic structures are likely to transfer across certain language pairs. For example, prepositional phrases (PP) in Arabic and English are similar in a sense that PPs generally appear at the end of the sentence (after all the verbal arguments) and to a lesser extent at its beginning (Habash and Hu, 2009). We use the</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577–585, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA’06,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="4825" citStr="Snover et al., 2006" startWordPosition="728" endWordPosition="731">n many features we need many training examples and to train discriminatively we need to search through all possible translations of each training example. Another issue of previous work was that they are all trained with BLEU/TER score computing against Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 211–219, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics the translation references which is different from predicting the human-targeted translation edit rate (HTER) which is crucial in post-editing applications (Snover et al., 2006; Papineni et al., 2002). Finally, the backtranslation approach faces a serious issue when forward and backward translation models are symmetric. In this case, back-translation will not be very informative to indicate forward translation quality. In this paper, we predict error types of each word in the MT output with a confidence score, extend it to the sentence level, then apply it to n-best list reranking task to improve MT quality, and finally design a visualization prototype. We try to answer the following questions: • Can we use a rich feature set such as sourceside information, alignmen</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of AMTA’06, pages 223–231, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Trustrank: Inducing trust in automatic translations via ranking.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th ACL,</booktitle>
<pages>612--621</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3153" citStr="Soricut and Echihabi (2010)" startWordPosition="476" endWordPosition="480">put. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the MT output into the source language to see whether t</context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>Radu Soricut and Abdessamad Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proceedings of the 48th ACL, pages 612–621, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Zhuoran Wang</author>
<author>Marco Turchi</author>
<author>John ShaweTaylor</author>
<author>Craig Saunders</author>
</authors>
<title>Improving the confidence of machine translation quality estimates.</title>
<date>2009</date>
<booktitle>In Proceedings of the MT Summit XII,</booktitle>
<location>Ottawa, Canada.</location>
<contexts>
<context position="3124" citStr="Specia et al., 2009" startWordPosition="472" endWordPosition="475">dence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the MT output into the sou</context>
</contexts>
<marker>Specia, Wang, Turchi, ShaweTaylor, Saunders, 2009</marker>
<rawString>Lucia Specia, Zhuoran Wang, Marco Turchi, John ShaweTaylor, and Craig Saunders. 2009. Improving the confidence of machine translation quality estimates. In Proceedings of the MT Summit XII, Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>Efficient dynamic programming search algorithms for phrase-based SMT.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18354" citStr="Tillmann, 2006" startWordPosition="3059" endWordPosition="3060">tional complement, are having child-father agreements. Children agreement: In the child-father agreement feature we look up in the dependency tree, however, we also can look down to the dependency tree with a similar motivation. Essentially, given an alignment between a source word si and a target word tj, how many children of si and tj are aligned together? For example, “tshyr” and “refers” have 2 aligned children which are “ayda-also” and “aly-to” as shown in Figure 3c. 4 Experiments 4.1 Arabic-English translation system The SMT engine is a phrase-based system similar to the description in (Tillmann, 2006), where various features are combined within a log-linear framework. These features include source-to-target phrase translation score, source-to-target and target-to-source wordto-word translation scores, language model score, distortion model scores and word count. The training data for these features are 7M Arabic-English sentence pairs, mostly newswire and UN corpora released by LDC. The parallel sentences have word alignment automatically generated with HMM and MaxEnt word aligner (Ge, 2004; Ittycheriah and Roukos, 2005). Bilingual phrase translations are extracted from these word-aligned </context>
</contexts>
<marker>Tillmann, 2006</marker>
<rawString>Christoph Tillmann. 2006. Efficient dynamic programming search algorithms for phrase-based SMT. In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, pages 9–16, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Word-level confidence estimation for machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="3423" citStr="Ueffing and Ney (2007)" startWordPosition="519" endWordPosition="522">rk of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the MT output into the source language to see whether the output of backward translation matches the original source sentence. However, previous studies had a few shortcomings. First, source-side features were not extensively investigated. Blatz et al.(2004) only investigated source ngram frequency statistics and source lan</context>
<context position="11364" citStr="Ueffing and Ney, 2007" startWordPosition="1843" endWordPosition="1846"> i-1 p(yZ (7) k goodness(S) is ranging between 0 and 1, where 0 is equivalent to an absolutely wrong translation and 1 is a perfect translation. Essentially, goodness(S) is the arithmetic mean which represents the goodness of translation per word in the whole sentence. 3 Confidence Measure Features Features are generated from feature types: abstract templates from which specific features are instantiated. Features sets are often parameterized in various ways. In this section, we describe three new feature sets introduced on top of our baseline classifier which has WPP and target POS features (Ueffing and Ney, 2007; Xiong et al., 2010). 3.1 Source-side features From MT decoder log, we can track which source phrases generate target phrases. Furthermore, one can infer the alignment between source and target words within the phrase pair using simple aligners such as IBM Model-1 alignment. Source phrase features: These features are designed to capture the likelihood that source phrase and target word co-occur with a given error label. The intuition behind them is that if a large percentage of the source phrase and target have often been seen together with the 213 (b) Source POS (c) Source POS and phrase in </context>
<context position="20500" citStr="Ueffing and Ney, 2007" startWordPosition="3392" endWordPosition="3395">e number of tagged labels R = the number of correctly tagged labels (8) the number of reference labels F = 2*P*R P+R 4.2 Contribution of feature sets We designed our experiments to show the impact of each feature separately as well as their cumulative impact. We trained two types of classifiers to predict the error type of each word in MT output, namely Good/Bad with a binary classifier and Good/Insertion/Substitution/Shift with a 4-class classifier. Each classifier is trained with different feature sets as follow: • WPP: we reimplemented WPP calculation based on n-best lists as described in (Ueffing and Ney, 2007). • WPP + target POS: only WPP and target POS features are used. This is a similar feature set used by Xiong et al. (2010). • Our features: the classifier has source side, alignment context, and dependency structure features; WPP and target POS features are excluded. • WPP + our features: adding our features on top of WPP. • WPP + target POS + our features: using all features. binary 4-class dev test dev test WPP 69.3 68.7 64.4 63.7 + source side 72.1 71.6 66.2 65.7 + alignment context 71.4 70.9 65.7 65.3 + dependency structures 69.9 69.5 64.9 64.3 WPP+ target POS 69.6 69.1 64.4 63.9 + source </context>
<context position="29270" citStr="Ueffing &amp; Ney 2007" startWordPosition="4854" endWordPosition="4857">correction to intro climat Figure 7: MT errors visualization based on confidence scores. 7 Conclusions In this paper we proposed a method to predict confidence scores for machine translated words and sentences based on a feature-rich classifier using linguistic and context features. Our major contributions are three novel feature sets including source side information, alignment context, and dependency structures. Experimental results show that by combining the source side information, alignment context, and dependency structure features with word posterior probability and target POS context (Ueffing &amp; Ney 2007; Xiong et al., 2010), the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. Our framework is able to predict error types namely insertion, substitution and shift. The Pearson correlation with human judgement increases from 0.52 to 0.6. Furthermore, we show that the proposed confidence scores can help the MT system to select better translations and as a result improvements between 0.4 and 0.9 TER reduction are obtained. Finally, we demonstrate a prototype to visualize translation errors. This work can be expanded in several directions. First, we plan to apply confidence e</context>
</contexts>
<marker>Ueffing, Ney, 2007</marker>
<rawString>Nicola Ueffing and Hermann Ney. 2007. Word-level confidence estimation for machine translation. Computational Linguistics, 33(1):9–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLPCoNLL,</booktitle>
<pages>764--773</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8534" citStr="Watanabe et al., 2007" startWordPosition="1347" endWordPosition="1350">(x, y) = w.f(x, y) (1) To obtain the label, we choose the class with the highest score as the predicted label for that data instance. To learn optimized weights, we use the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003; McDonald et al., 2005) which is an online learner closely related to both the support vector machine and perceptron learning framework. MIRA has been shown to provide state-of-the-art performance for sequential labelling task (Rozenfeld et al., 2006), and is also able to provide an efficient mechanism to train and optimize MT systems with lots of features (Watanabe et al., 2007; Chiang et al., 2009). In general, weights are updated at each step time t according to the following rule: wt+1 = arg min,,,t+1 ||wt+1 − wt|| (2) s.t. score(x, y) &gt; score(x, y&apos;) + L(y, y&apos;) where L(y, y&apos;) is a measure of the loss of using y&apos; instead of the true label y. In this problem L(y, y&apos;) is 0-1 loss function. More specifically, for each instance xi in the training data at a time t we find the label with the highest score: y&apos; = arg max score(xi,y) (3) y the weight vector is updated as follow wt+1 = wt + T(f(xi, y) − f(xi, y&apos;)) (4) T can be interpreted as a step size; when T is a large n</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the EMNLPCoNLL, pages 764–773, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Error detection for statistical machine translation using linguistic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th ACL,</booktitle>
<pages>604--611</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3399" citStr="Xiong et al., 2010" startWordPosition="515" endWordPosition="518">ation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the MT output into the source language to see whether the output of backward translation matches the original source sentence. However, previous studies had a few shortcomings. First, source-side features were not extensively investigated. Blatz et al.(2004) only investigated source ngram frequency s</context>
<context position="11385" citStr="Xiong et al., 2010" startWordPosition="1847" endWordPosition="1850">s(S) is ranging between 0 and 1, where 0 is equivalent to an absolutely wrong translation and 1 is a perfect translation. Essentially, goodness(S) is the arithmetic mean which represents the goodness of translation per word in the whole sentence. 3 Confidence Measure Features Features are generated from feature types: abstract templates from which specific features are instantiated. Features sets are often parameterized in various ways. In this section, we describe three new feature sets introduced on top of our baseline classifier which has WPP and target POS features (Ueffing and Ney, 2007; Xiong et al., 2010). 3.1 Source-side features From MT decoder log, we can track which source phrases generate target phrases. Furthermore, one can infer the alignment between source and target words within the phrase pair using simple aligners such as IBM Model-1 alignment. Source phrase features: These features are designed to capture the likelihood that source phrase and target word co-occur with a given error label. The intuition behind them is that if a large percentage of the source phrase and target have often been seen together with the 213 (b) Source POS (c) Source POS and phrase in right context Figure </context>
<context position="20622" citStr="Xiong et al. (2010)" startWordPosition="3418" endWordPosition="3421">ribution of feature sets We designed our experiments to show the impact of each feature separately as well as their cumulative impact. We trained two types of classifiers to predict the error type of each word in MT output, namely Good/Bad with a binary classifier and Good/Insertion/Substitution/Shift with a 4-class classifier. Each classifier is trained with different feature sets as follow: • WPP: we reimplemented WPP calculation based on n-best lists as described in (Ueffing and Ney, 2007). • WPP + target POS: only WPP and target POS features are used. This is a similar feature set used by Xiong et al. (2010). • Our features: the classifier has source side, alignment context, and dependency structure features; WPP and target POS features are excluded. • WPP + our features: adding our features on top of WPP. • WPP + target POS + our features: using all features. binary 4-class dev test dev test WPP 69.3 68.7 64.4 63.7 + source side 72.1 71.6 66.2 65.7 + alignment context 71.4 70.9 65.7 65.3 + dependency structures 69.9 69.5 64.9 64.3 WPP+ target POS 69.6 69.1 64.4 63.9 + source side 72.3 71.8 66.3 65.8 + alignment context 71.9 71.2 66 65.6 + dependency structures 70.4 70 65.1 64.4 Table 1: Contribu</context>
<context position="29291" citStr="Xiong et al., 2010" startWordPosition="4858" endWordPosition="4861">climat Figure 7: MT errors visualization based on confidence scores. 7 Conclusions In this paper we proposed a method to predict confidence scores for machine translated words and sentences based on a feature-rich classifier using linguistic and context features. Our major contributions are three novel feature sets including source side information, alignment context, and dependency structures. Experimental results show that by combining the source side information, alignment context, and dependency structure features with word posterior probability and target POS context (Ueffing &amp; Ney 2007; Xiong et al., 2010), the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. Our framework is able to predict error types namely insertion, substitution and shift. The Pearson correlation with human judgement increases from 0.52 to 0.6. Furthermore, we show that the proposed confidence scores can help the MT system to select better translations and as a result improvements between 0.4 and 0.9 TER reduction are obtained. Finally, we demonstrate a prototype to visualize translation errors. This work can be expanded in several directions. First, we plan to apply confidence estimation to perform </context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Error detection for statistical machine translation using linguistic features. In Proceedings of the 48th ACL, pages 604– 611, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>