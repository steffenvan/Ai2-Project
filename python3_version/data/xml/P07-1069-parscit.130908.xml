<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000229">
<title confidence="0.983099">
Generating a Table-of-Contents
</title>
<author confidence="0.99622">
S.R.K. Branavan, Pawan Deshpande and Regina Barzilay
</author>
<affiliation confidence="0.997594">
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.993782">
{branavan, pawand, regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999779823529412">
This paper presents a method for the auto-
matic generation of a table-of-contents. This
type of summary could serve as an effec-
tive navigation tool for accessing informa-
tion in long texts, such as books. To gen-
erate a coherent table-of-contents, we need
to capture both global dependencies across
different titles in the table and local con-
straints within sections. Our algorithm ef-
fectively handles these complex dependen-
cies by factoring the model into local and
global components, and incrementally con-
structing the model’s output. The results of
automatic evaluation and manual assessment
confirm the benefits of this design: our sys-
tem is consistently ranked higher than non-
hierarchical baselines.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999336875">
Current research in summarization focuses on pro-
cessing short articles, primarily in the news domain.
While in practice the existing summarization meth-
ods are not limited to this material, they are not
universal: texts in many domains and genres can-
not be summarized using these techniques. A par-
ticularly significant challenge is the summarization
of longer texts, such as books. The requirement
for high compression rates and the increased need
for the preservation of contextual dependencies be-
tween summary sentences places summarization of
such texts beyond the scope of current methods.
In this paper, we investigate the automatic gener-
ation of tables-of-contents, a type of indicative sum-
mary particularly suited for accessing information in
long texts. A typical table-of-contents lists topics
described in the source text and provides informa-
tion about their location in the text. The hierarchical
organization of information in the table further re-
fines information access by specifying the relations
between different topics and providing rich contex-
tual information during browsing. Commonly found
in books, tables-of-contents can also facilitate access
to other types of texts. For instance, this type of
summary could serve as an effective navigation tool
for understanding a long, unstructured transcript for
an academic lecture or a meeting.
Given a text, our goal is to generate a tree wherein
a node represents a segment of text and a title that
summarizes its content. This process involves two
tasks: the hierarchical segmentation of the text, and
the generation of informative titles for each segment.
The first task can be addressed by using the hier-
archical structure readily available in the text (e.g.,
chapters, sections and subsections) or by employ-
ing existing topic segmentation algorithms (Hearst,
1994). In this paper, we take the former approach.
As for the second task, a naive approach would be to
employ existing methods of title generation to each
segment, and combine the results into a tree struc-
ture.
However, the latter approach cannot guarantee
that the generated table-of-contents forms a coher-
ent representation of the entire text. Since titles of
different segments are generated in isolation, some
of the generated titles may be repetitive. Even non-
repetitive titles may not provide sufficient informa-
tion to discriminate between the content of one seg-
</bodyText>
<page confidence="0.971272">
544
</page>
<note confidence="0.925857">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 544–551,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<figure confidence="0.60622825">
Scientific computing
Remarkable recursive algorithm for multiplying matrices
Divide and conquer algorithm design
Making a recursive algorithm
Solving systems of linear equations
Computing an LUP decomposition
Forward and back substitution
Symmetric positive definite matrices and least squares approximation
</figure>
<figureCaption confidence="0.999583">
Figure 1: A fragment of a table-of-contents generated by our method.
</figureCaption>
<bodyText confidence="0.999900689655172">
ment and another. Therefore, it is essential to gen-
erate an entire table-of-contents tree in a concerted
fashion.
This paper presents a hierarchical discriminative
approach for table-of-contents generation. Figure 1
shows a fragment of a table-of-contents automat-
ically generated by this algorithm. Our method
has two important points of departure from exist-
ing techniques. First, we introduce a structured dis-
criminative model for table-of-contents generation
that accounts for a wide range of phrase-based and
collocational features. The flexibility of this model
results in improved summary quality. Second, our
model captures both global dependencies across dif-
ferent titles in the tree and local dependencies within
sections. We decompose the model into local and
global components that handle different classes of
dependencies. We further reduce the search space
through incremental construction of the model’s out-
put by considering only the promising parts of the
decision space.
We apply our method to process a 1,180 page al-
gorithms textbook. To assess the contribution of our
hierarchical model, we compare our method with
state-of-the-art methods that generate each segment
title independently.1 The results of automatic eval-
uation and manual assessment of title quality show
that the output of our system is consistently ranked
higher than that of non-hierarchical baselines.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99986425">
Although most current research in summarization
focuses on newspaper articles, a number of ap-
proaches have been developed for processing longer
texts. Most of these approaches are tailored to a par-
</bodyText>
<footnote confidence="0.991367333333333">
1The code and feature vector data for
our model and the baselines are available at
http://people.csail.mit.edu/branavan/code/toc.
</footnote>
<bodyText confidence="0.999776135135135">
ticular domain, such as medical literature or scien-
tific articles. By making strong assumptions about
the input structure and the desired format of the out-
put, these methods achieve a high compression rate
while preserving summary coherence. For instance,
Teufel and Moens (2002) summarize scientific arti-
cles by selecting rhetorical elements that are com-
monly present in scientific abstracts. Elhadad and
McKeown (2001) generate summaries of medical ar-
ticles by following a certain structural template in
content selection and realization.
Our work, however, is closer to domain-
independent methods for summarizing long texts.
Typically, these approaches employ topic segmen-
tation to identify a list of topics described in a
document, and then produce a summary for each
part (Boguraev and Neff, 2000; Angheluta et al.,
2002). In contrast to our method, these approaches
perform either sentence or phrase extraction, rather
than summary generation. Moreover, extraction for
each segment is performed in isolation, and global
constraints on the summary are not enforced.
Finally, our work is also related to research on ti-
tle generation (Banko et al., 2000; Jin and Haupt-
mann, 2001; Dorr et al., 2003). Since work in this
area focuses on generating titles for one article at a
time (e.g., newspaper reports), the issue of hierarchi-
cal generation, which is unique to our task, does not
arise. However, this is not the only novel aspect of
the proposed approach. Our model learns title gener-
ation in a fully discriminative framework, in contrast
to the commonly used noisy-channel model. Thus,
instead of independently modeling the selection and
grammaticality constraints, we learn both types of
features in a single framework. This joint training
regime supports greater flexibility in modeling fea-
ture interaction.
</bodyText>
<page confidence="0.998914">
545
</page>
<sectionHeader confidence="0.989624" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999863454545455">
We formalize the problem of table-of-contents gen-
eration as a supervised learning task where the goal
is to map a tree of text segments S to a tree of titles
T. A segment may correspond to a chapter, section
or subsection.
Since the focus of our work is on the generation
aspect of table-of-contents construction, we assume
that the hierarchical segmentation of a text is pro-
vided in the input. This division can either be au-
tomatically computed using one of the many avail-
able text segmentation algorithms (Hearst, 1994), or
it can be based on demarcations already present in
the input (e.g., paragraph markers).
During training, the algorithm is provided with a
set of pairs (Sz, Tz) for i = 1, ... , p, where Sz is
the ith tree of text segments, and Tz is the table-of-
contents for that tree. During testing, the algorithm
generates tables-of-contents for unseen trees of text
segments.
We also assume that during testing the desired
title length is provided as a parameter to the algo-
rithm.
</bodyText>
<sectionHeader confidence="0.995954" genericHeader="method">
4 Algorithm
</sectionHeader>
<bodyText confidence="0.999990333333333">
To generate a coherent table-of-contents, we need
to take into account multiple constraints: the titles
should be grammatical, they should adequately rep-
resent the content of their segments, and the table-
of-contents as a whole should clearly convey the re-
lations between the segments. Taking a discrimina-
tive approach for modeling this task would allow us
to achieve this goal: we can easily integrate a range
of constraints in a flexible manner. Since the num-
ber of possible labels (i.e., tables-of-contents) is pro-
hibitively large and the labels themselves exhibit a
rich internal structure, we employ a structured dis-
criminative model that can easily handle complex
dependencies. Our solution relies on two orthogo-
nal strategies to balance the tractability and the rich-
ness of the model. First, we factor the model into
local and global components. Second, we incremen-
tally construct the output of each component using
a search-based discriminative algorithm. Both of
these strategies have the effect of intelligently prun-
ing the decision space.
Our model factorization is driven by the different
types of dependencies which are captured by the two
components. The first model is local: for each seg-
ment, it generates a list of candidate titles ranked by
their individual likelihoods. This model focuses on
grammaticality and word selection constraints, but it
does not consider relations among different titles in
the table-of-contents. These latter dependencies are
captured in the global model that constructs a table-
of-contents by selecting titles for each segment from
the available candidates. Even after this factoriza-
tion, the decision space for each model is large: for
the local model, it is exponential in the length of the
segment title, and for the global model it is exponen-
tial in the size of the tree.
Therefore, we construct the output for each of
these models incrementally using beam search. The
algorithm maintains the most promising partial out-
put structures, which are extended at every itera-
tion. The model incorporates this decoding pro-
cedure into the training process, thereby learning
model parameters best suited for the specific decod-
ing algorithm. Similar models have been success-
fully applied in the past to other tasks including pars-
ing (Collins and Roark, 2004), chunking (Daum´e
and Marcu, 2005), and machine translation (Cowan
et al., 2006).
</bodyText>
<subsectionHeader confidence="0.999832">
4.1 Model Structure
</subsectionHeader>
<bodyText confidence="0.997703333333333">
The model takes as input a tree of text segments S.
Each segment s E S and its title z are represented
as a local feature vector Φloc(s, z). Each compo-
nent of this vector stores a numerical value. This
feature vector can track any feature of the segment s
together with its title z. For instance, the ith compo-
nent of this vector may indicate whether the bigram
(z[j]z[j + 1]) occurs in s, where z[j] is the jth word
in z:
</bodyText>
<equation confidence="0.5536225">
{ 1 if (z[j]z[j + 1]) � s
(Φloc(s, z))z = 0 otherwise
</equation>
<bodyText confidence="0.99938">
In addition, our model captures dependencies
among multiple titles that appear in the same table-
of-contents. We represent a tree of segments S
paired with titles T with the global feature vector
Φglob(S, T). The components here are also numer-
ical features. For example, the ith component of the
vector may indicate whether a title is repeated in the
table-of-contents T:
</bodyText>
<page confidence="0.942599">
546
</page>
<equation confidence="0.521120666666667">
1 repeated tie
(bglob(S,T))i = title
otherwise
</equation>
<bodyText confidence="0.981943833333333">
Our model constructs a table-of-contents in two
basic steps:
Step One The goal of this step is to generate a
list of k candidate titles for each segment s E S.
To do so, for each possible title z, the model maps
the feature vector bloc(s, z) to a real number. This
mapping can take the form of a linear model,
bloc(s, z) · αloc
where αloc is the local parameter vector.
Since the number of possible titles is exponen-
tial, we cannot consider all of them. Instead, we
prune the decision space by incrementally construct-
ing promising titles. At each iteration j, the algo-
rithm maintains a beam Q of the top k partially gen-
erated titles of length j. During iteration j + 1, a
new set of candidates is grown by appending a word
from s to the right of each member of the beam Q.
We then sort the entries in Q: z1, z2,... such that
bloc(s, zi) · αloc ? bloc(s, zi+1) · αloc, Vi. Only the
top k candidates are retained, forming the beam for
the next iteration. This process continues until a title
of the desired length is generated. Finally, the list of
k candidates is returned.
Step Two Given a set of candidate titles
z1, z2,.. . , zk for each segment s E S, our goal is
to construct a table-of-contents T by selecting the
most appropriate title from each segment’s candi-
date list. To do so, our model computes a score for
the pair (S, T) based on the global feature vector
bglob(S, T):
</bodyText>
<equation confidence="0.531365">
bglob(S, T) · αglob
</equation>
<bodyText confidence="0.9998675">
where αglob is the global parameter vector.
As with the local model (step one), the num-
ber of possible tables-of-contents is too large to be
considered exhaustively. Therefore, we incremen-
tally construct a table-of-contents by traversing the
tree of segments in a pre-order walk (i.e., the or-
der in which segments appear in the text). In this
case, the beam contains partially generated tables-
of-contents, which are expanded by one segment ti-
tle at a time. To further reduce the search space,
during decoding only the top five candidate titles for
a segment are given to the global model.
</bodyText>
<subsectionHeader confidence="0.97095">
4.2 Training the Model
</subsectionHeader>
<bodyText confidence="0.999968738095239">
Training for Step One We now describe how the
local parameter vector αloc is estimated from train-
ing data. We are given a set of training examples
(si, yi) for i = 1, ... ,l, where si is the ith text seg-
ment, and yi is the title of this segment.
This linear model is learned using a variant of
the incremental perceptron algorithm (Collins and
Roark, 2004; Daum´e and Marcu, 2005). This on-
line algorithm traverses the training set multiple
times, updating the parameter vector αloc after each
training example in case of mis-predictions. The al-
gorithm encourages a setting of the parameter vector
αloc that assigns the highest score to the feature vec-
tor associated with the correct title.
The pseudo-code of the algorithm is shown in Fig-
ure 2. Given a text segment s and the corresponding
title y, the training algorithm maintains a beam Q
containing the top k partial titles of length j. The
beam is updated on each iteration using the func-
tions GROW and PRUNE. For every word in seg-
ment s and for every partial title in Q, GROW cre-
ates a new title by appending this word to the title.
PRUNE retains only the top ranked candidates based
on the scoring function bloc(s, z) · αloc. If y[1... j]
(i.e., the prefix of y of length j) is not in the modi-
fied beam Q, then αloc is updated2 as shown in line
4 of the pseudo-code in Figure 2. In addition, Q is
replaced with a beam containing only y[1 ... j] (line
5). This process is performed jyj times. We repeat
this process for all training examples over 50 train-
ing iterations. 3
Training for Step Two To train the global param-
eter vector αglob, we are given training examples
(Si, Ti) for i = 1, ... , p, where Si is the ith tree of
text segments, and Ti is the table-of-contents for that
tree. However, we cannot directly use these tables-
of-contents for training our global model: since this
model selects one of the candidate titles zi1, ... , zik
returned by the local model, the true title of the seg-
ment may not be among these candidates. There-
fore, to determine a new target title for the segment,
we need to identify the title in the set of candidates
</bodyText>
<footnote confidence="0.99122725">
2If the word in the jth position of y does not occur in s, then
the parameter update is not performed.
3For decoding, αloe is averaged over the training iterations
as in Collins and Roark (2004).
</footnote>
<page confidence="0.974166">
547
</page>
<bodyText confidence="0.59958325">
s – segment text.
y – segment title.
y[1... j] – prefix of y of length j.
Q – beam containing partial titles.
</bodyText>
<listItem confidence="0.964772666666667">
1. for j = 1 ... |y|
2. Q = PRUNE(GROW(s, Q))
3. if y[1 ... j] ∈� Q
4. αloc = αloc + 4bloc(s, y[1 ... j])
F-zEQ �lo� QJ z)
5. Q = {y[1... j]}
</listItem>
<figureCaption confidence="0.999174">
Figure 2: The training algorithm for the local model.
</figureCaption>
<bodyText confidence="0.995261833333334">
that is closest to the true title.
We employ the L1 distance measure to compare
the content word overlap between two titles.4 For
each input (5, T), and each segment s ∈ 5, we iden-
tify the segment title closest in the L1 measure to the
true title y5:
</bodyText>
<equation confidence="0.9877815">
z* = arg min
i
</equation>
<bodyText confidence="0.999841">
Once all the training targets in the corpus have
been identified through this procedure, the global
linear model 4bglob(5, T) · αglob is learned using the
same perceptron algorithm as in step one. Rather
than maintaining the beam of partially generated ti-
tles, the beam Q holds partially generated tables-of-
contents. Also, the loop in line 1 of Figure 2 iterates
over segment titles rather than words. The global
model is trained over 200 iterations.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.999801090909091">
Local Features Our local model aims to generate
titles which adequately represent the meaning of the
segment and are grammatical. Selection and contex-
tual preferences are encoded in the local features.
The features that capture selection constraints are
specified at the word level, and contextual features
are expressed at the word sequence level.
The selection features capture the position of the
word, its TF*IDF, and part-of-speech information.
In addition, they also record whether the word oc-
curs in the body of neighboring segments. We also
</bodyText>
<footnote confidence="0.997742">
4This measure is close to ROUGE-1 which in addition con-
siders the overlap in auxiliary words.
5In the case of ties, one of the titles is picked arbitrarily.
</footnote>
<bodyText confidence="0.9997002">
Segment has the same title as its sibling
Segment has the same title as its parent
Two adjacent sibling titles have the same head
Two adjacent sibling titles start with the same word
Rank given to the title by the local model
</bodyText>
<tableCaption confidence="0.993742">
Table 1: Examples of global features.
</tableCaption>
<bodyText confidence="0.99959664516129">
generate conjunctive features by combining features
of different types.
The contextual features record the bigram and tri-
gram language model scores, both for words and for
part-of-speech tags. The trigram scores are aver-
aged over the title. The language models are trained
using the SRILM toolkit. Another type of contex-
tual feature models the collocational properties of
noun phrases in the title. This feature aims to elim-
inate generic phrases, such as “the following sec-
tion” from the generated titles.6 To achieve this ef-
fect, for each noun phrase in the title, we measure
the ratio of their frequency in the segment to their
frequency in the corpus.
Global Features Our global model describes the
interaction between different titles in the tree (See
Table 1). These interactions are encoded in three
types of global features. The first type of global
feature indicates whether titles in the tree are re-
dundant at various levels of the tree structure. The
second type of feature encourages parallel construc-
tions within the same tree. For instance, titles of ad-
joining segments may be verbalized as noun phrases
with the same head (e.g., “Bubble sort algorithm”,
“Merge sort algorithm”). We capture this property
by comparing words that appear in certain positions
in adjacent sibling titles. Finally, our global model
also uses the rank of the title provided by the local
model. This feature enables the global model to ac-
count for the preferences of the local model in the
title selection process.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="method">
6 Evaluation Set-Up
</sectionHeader>
<bodyText confidence="0.904316">
Data We apply our method to an undergraduate al-
gorithms textbook. For detailed statistics on the data
see Table 2. We split its table-of-contents into a set
6Unfortunately, we could not use more sophisticated syntac-
tic features due to the low accuracy of statistical parsers on our
corpus.
</bodyText>
<equation confidence="0.84445">
L1(zi, y)
</equation>
<page confidence="0.971863">
548
</page>
<table confidence="0.999793142857143">
Number of Titles 540
Number of Trees 39
Tree Depth 4
Number of Words 269,650
Avg. Title Length 3.64
Avg. Branching 3.29
Avg. Title Duplicates 21
</table>
<tableCaption confidence="0.933779">
Table 2: Statistics on the corpus used in the experi-
ments.
</tableCaption>
<bodyText confidence="0.995727930555556">
of independent subtrees. Given a table-of-contents
of depth n with a root branching factor of r, we gen-
erate r subtrees, with a depth of at most n − 1. We
randomly select 80% of these trees for training, and
the rest are used for testing. In our experiments, we
use ten different randomizations to compensate for
the small number of available trees.
Admittedly, this method of generating training
and testing data omits some dependencies at the
level of the table-of-contents as a whole. However,
the subtrees used in our experiments still exhibit
a sufficiently deep hierarchical structure, rich with
contextual dependencies.
Baselines As an alternative to our hierarchical dis-
criminative method, we consider three baselines that
build a table-of-contents by generating a title for
each segment individually, without taking into ac-
count the tree structure, and one hierarchical gener-
ative baseline. The first method generates a title for a
segment by selecting the noun phrase from that seg-
ment with the highest TF*IDF. This simple method
is commonly used to generate keywords for brows-
ing applications in information retrieval, and has
been shown to be effective for summarizing techni-
cal content (Wacholder et al., 2001).
The second baseline is based on the noisy-channel
generative (flat generative, FG) model proposed by
Banko et al., (2000). Similar to our local model,
this method captures both selection and grammati-
cal constraints. However, these constraints are mod-
eled separately, and then combined in a generative
framework.
We use our local model (Flat Discriminative
model, FD) as the third baseline. Like the second
baseline, this model omits global dependencies, and
only focuses on features that capture relations within
individual segments.
In the hierarchical generative (HG) baseline we
run our global model on the ranked list of titles pro-
duced for each section by the noisy-channel genera-
tive model.
The last three baselines and our algorithm are pro-
vided with the title length as a parameter. In our
experiments, the algorithms use the reference title
length.
Experimental Design: Comparison with refer-
ence tables-of-contents Reference based evalu-
ation is commonly used to assess the quality of
machine-generated headlines (Wang et al., 2005).
We compare our system’s output with the table-of-
contents from the textbook using ROUGE metrics.
We employ a publicly available software package,7
with all the parameters set to default values.
Experimental Design: Human assessment The
judges were each given 30 segments randomly se-
lected from a set of 359 test segments. For each test
segment, the judges were presented with its text, and
3 alternative titles consisting of the reference and
the titles produced by the hierarchical discriminative
model, and the best performing baseline. In addi-
tion, the judges had access to all of the segments in
the book. A total of 498 titles for 166 unique seg-
ments were ranked. The system identities were hid-
den from the judges, and the titles were presented in
random order. The judges ranked the titles based on
how well they represent the content of the segment.
Titles were ranked equal if they were judged to be
equally representative of the segment.
Six people participated in this experiment. All the
participants were graduate students in computer sci-
ence who had taken the algorithms class in the past
and were reasonably familiar with the material.
</bodyText>
<sectionHeader confidence="0.999727" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.9998735">
Figure 3 shows fragments of the tables-of-contents
generated by our method and the four baselines
along with the reference counterpart. These extracts
illustrate three general phenomena that we observed
in the test corpus. First, the titles produced by key-
word extraction exhibit a high degree of redundancy.
In fact, 40% of the titles produced by this method are
repeated more than once in the table-of-contents. In
</bodyText>
<footnote confidence="0.972875">
7http://www.isi.edu/licensed-sw/see/rouge/
</footnote>
<page confidence="0.989566">
549
</page>
<table confidence="0.799388666666667">
Reference: Flat Generative: Flat Discriminative:
hash tables linked list dictionary operations
direct address tables worst case time universe of keys
hash tables wasted space computer memory
collision resolution by chaining worst case running time element in the list
analysis of hashing with chaining to show that there are hash table with load factor
open addressing dynamic set hash table
linear probing occupied slot hash function
quadratic probing quadratic function hash function
double hashing double hashing double hashing
Keyword Extraction: Hierarchical Generative: Hierarchical Discriminative:
hash table dictionary operations dictionary operations
</table>
<bodyText confidence="0.761947375">
dynamic set worst case time direct address table
hash function wasted space computer memory
worst case worst case running time worst case running time
expected number to show that there are hash table with load factor
hash table collision resolution address table
hash function linear time hash function
hash table quadratic function quadratic probing
double hashing double hashing double hashing
</bodyText>
<figureCaption confidence="0.981189">
Figure 3: Fragments of tables-of-contents generated by our method and the four baselines along with the
corresponding reference.
</figureCaption>
<table confidence="0.999814666666667">
Rouge-1 Rouge-L Rouge-W Full Match
HD 0.256 0.249 0.216 13.5
FD 0.241 0.234 0.203 13.1
HG 0.139 0.133 0.117 5.8
FG 0.094 0.090 0.079 4.1
Keyword 0.168 0.168 0.157 6.3
</table>
<tableCaption confidence="0.995852">
Table 3: Title quality as compared to the reference
</tableCaption>
<bodyText confidence="0.998892666666667">
for the hierarchical discriminative (HD), flat dis-
criminative (FD), hierarchical generative (HG), flat
generative (FG) and Keyword models. The improve-
ment given by HD over FD in all three Rouge mea-
sures is significant at p &lt; 0.03 based on the Sign
test.
</bodyText>
<table confidence="0.887581">
better worse equal
HD vs. FD 68 32 49
Reference vs. HD 115 13 22
Reference vs. FD 123 7 20
</table>
<tableCaption confidence="0.764452">
Table 4: Overall pairwise comparisons of the rank-
</tableCaption>
<bodyText confidence="0.989806074074074">
ings given by the judges. The improvement in ti-
tle quality given by HD over FD is significant at
p &lt; 0.0002 based on the Sign test.
contrast, our method yields 5.5% of the titles as du-
plicates, as compared to 9% in the reference table-
of-contents.8
Second, the fragments show that the two discrim-
inative models — Flat and Hierarchical — have a
number of common titles. However, adding global
dependencies to rerank titles generated by the local
model changes 30% of the titles in the test set.
Comparison with reference tables-of-contents
Table 3 shows the average ROUGE scores over
the ten randomizations for the five automatic meth-
ods. The hierarchical discriminative method consis-
tently outperforms the four baselines according to
all ROUGE metrics.
At the same time, these results also show that only
a small ratio of the automatically generated titles
are identical to the reference ones. In some cases,
the machine-generated titles are very close in mean-
ing to the reference, but are verbalized differently.
Examples include pairs such as (“Minimum Span-
ning Trees”, “Spanning Tree Problem”) and (“Wal-
lace Tree”, “Multiplication Circuit”).9 While mea-
sures like ROUGE can capture the similarity in the
first pair, they cannot identify semantic proximity
</bodyText>
<footnote confidence="0.864553">
8Titles such as “Analysis” and “Chapter Outline” are re-
peated multiple times in the text.
9A Wallace Tree is a circuit that multiplies two integers.
</footnote>
<page confidence="0.99538">
550
</page>
<bodyText confidence="0.999917071428572">
between the titles in the second pair. Therefore,
we supplement the results of this experiment with
a manual assessment of title quality as described be-
low.
Human assessment We analyze the human rat-
ings by considering pairwise comparisons between
the models. Given two models, A and B, three out-
comes are possible: A is better than B, B is bet-
ter than A, or they are of equal quality. The re-
sults of the comparison are summarized in Table 4.
These results indicate that using hierarchical infor-
mation yields statistically significant improvement
(at p &lt; 0.0002 based on the Sign test) over a flat
counterpart.
</bodyText>
<sectionHeader confidence="0.985463" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989">
This paper presents a method for the automatic gen-
eration of a table-of-contents. The key strength of
our method lies in its ability to track dependencies
between generation decisions across different levels
of the tree structure. The results of automatic evalu-
ation and manual assessment confirm the benefits of
joint tree learning: our system is consistently ranked
higher than non-hierarchical baselines.
We also plan to expand our method for the task
of slide generation. Like tables-of-contents, slide
bullets are organized in a hierarchical fashion and
are written in relatively short phrases. From the
language viewpoint, however, slides exhibit more
variability and complexity than a typical table-of-
contents. To address this challenge, we will explore
more powerful generation methods that take into ac-
count syntactic information.
</bodyText>
<sectionHeader confidence="0.998374" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994192583333333">
The authors acknowledge the support of the Na-
tional Science Foundation (CAREER grant IIS-
0448168 and grant IIS-0415865). We would also
like to acknowledge the many people who took part
in human evaluations. Thanks to Michael Collins,
Benjamin Snyder, Igor Malioutov, Jacob Eisenstein,
Luke Zettlemoyer, Terry Koo, Erdong Chen, Zo-
ran Dzunic and the anonymous reviewers for helpful
comments and suggestions. Any opinions, findings,
conclusions or recommendations expressed above
are those of the authors and do not necessarily re-
flect the views of the NSF.
</bodyText>
<sectionHeader confidence="0.996412" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999939319148936">
Roxana Angheluta, Rik De Busser, and Marie-Francine
Moens. 2002. The use of topic segmentation for auto-
matic summarization. In Proceedings of the ACL-2002
Workshop on Automatic Summarization.
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statistical
translation. In Proceedings of the ACL, pages 318–
325.
Branimir Boguraev and Mary S. Neff. 2000. Discourse
segmentation in aid of document summarization. In
Proceedings of the 33rd Hawaii International Confer-
ence on System Sciences, pages 3004–3014.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the ACL, pages 111–118.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the EMNLP, pages 232–241.
Hal Daum´e and Daniel Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proceedings of the ICML,
pages 169–176.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: a parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 03 on
Text summarization workshop, pages 1–8.
Noemie Elhadad and Kathleen R. McKeown. 2001. To-
wards generating patient specific summaries of med-
ical articles. In Proceedings of NAACL Workshop on
Automatic Summarization, pages 31–39.
Marti Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of the ACL, pages 9–
16.
Rong Jin and Alexander G. Hauptmann. 2001. Auto-
matic title generation for spoken broadcast news. In
Proceedings of the HLT, pages 1–3.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409–445.
Nina Wacholder, David K. Evans, and Judith Klavans.
2001. Automatic identification and organization of in-
dex terms for interactive browsing. In JCDL, pages
126–134.
R. Wang, J. Dunnion, and J. Carthy. 2005. Machine
learning approach to augmenting news headline gen-
eration. In Proceedings of the IJCNLP.
</reference>
<page confidence="0.998058">
551
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.752654">
<title confidence="0.999692">Generating a Table-of-Contents</title>
<author confidence="0.924774">Pawan Deshpande Barzilay Branavan</author>
<affiliation confidence="0.995761">Massachusetts Institute of Technology</affiliation>
<email confidence="0.892016">pawand,</email>
<abstract confidence="0.995190888888889">This paper presents a method for the automatic generation of a table-of-contents. This type of summary could serve as an effective navigation tool for accessing information in long texts, such as books. To generate a coherent table-of-contents, we need to capture both global dependencies across different titles in the table and local constraints within sections. Our algorithm effectively handles these complex dependencies by factoring the model into local and global components, and incrementally constructing the model’s output. The results of automatic evaluation and manual assessment confirm the benefits of this design: our system is consistently ranked higher than nonhierarchical baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roxana Angheluta</author>
<author>Rik De Busser</author>
<author>Marie-Francine Moens</author>
</authors>
<title>The use of topic segmentation for automatic summarization.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-2002 Workshop on Automatic Summarization.</booktitle>
<marker>Angheluta, De Busser, Moens, 2002</marker>
<rawString>Roxana Angheluta, Rik De Busser, and Marie-Francine Moens. 2002. The use of topic segmentation for automatic summarization. In Proceedings of the ACL-2002 Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu O Mittal</author>
<author>Michael J Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>318--325</pages>
<contexts>
<context position="6739" citStr="Banko et al., 2000" startWordPosition="1004" endWordPosition="1007">on. Our work, however, is closer to domainindependent methods for summarizing long texts. Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al., 2002). In contrast to our method, these approaches perform either sentence or phrase extraction, rather than summary generation. Moreover, extraction for each segment is performed in isolation, and global constraints on the summary are not enforced. Finally, our work is also related to research on title generation (Banko et al., 2000; Jin and Hauptmann, 2001; Dorr et al., 2003). Since work in this area focuses on generating titles for one article at a time (e.g., newspaper reports), the issue of hierarchical generation, which is unique to our task, does not arise. However, this is not the only novel aspect of the proposed approach. Our model learns title generation in a fully discriminative framework, in contrast to the commonly used noisy-channel model. Thus, instead of independently modeling the selection and grammaticality constraints, we learn both types of features in a single framework. This joint training regime su</context>
<context position="21435" citStr="Banko et al., (2000)" startWordPosition="3549" endWordPosition="3552">that build a table-of-contents by generating a title for each segment individually, without taking into account the tree structure, and one hierarchical generative baseline. The first method generates a title for a segment by selecting the noun phrase from that segment with the highest TF*IDF. This simple method is commonly used to generate keywords for browsing applications in information retrieval, and has been shown to be effective for summarizing technical content (Wacholder et al., 2001). The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al., (2000). Similar to our local model, this method captures both selection and grammatical constraints. However, these constraints are modeled separately, and then combined in a generative framework. We use our local model (Flat Discriminative model, FD) as the third baseline. Like the second baseline, this model omits global dependencies, and only focuses on features that capture relations within individual segments. In the hierarchical generative (HG) baseline we run our global model on the ranked list of titles produced for each section by the noisy-channel generative model. The last three baselines</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Michele Banko, Vibhu O. Mittal, and Michael J. Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the ACL, pages 318– 325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir Boguraev</author>
<author>Mary S Neff</author>
</authors>
<title>Discourse segmentation in aid of document summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 33rd Hawaii International Conference on System Sciences,</booktitle>
<pages>3004--3014</pages>
<contexts>
<context position="6384" citStr="Boguraev and Neff, 2000" startWordPosition="949" endWordPosition="952">ieve a high compression rate while preserving summary coherence. For instance, Teufel and Moens (2002) summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts. Elhadad and McKeown (2001) generate summaries of medical articles by following a certain structural template in content selection and realization. Our work, however, is closer to domainindependent methods for summarizing long texts. Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al., 2002). In contrast to our method, these approaches perform either sentence or phrase extraction, rather than summary generation. Moreover, extraction for each segment is performed in isolation, and global constraints on the summary are not enforced. Finally, our work is also related to research on title generation (Banko et al., 2000; Jin and Hauptmann, 2001; Dorr et al., 2003). Since work in this area focuses on generating titles for one article at a time (e.g., newspaper reports), the issue of hierarchical generation, which is unique to our task, does not arise. However, </context>
</contexts>
<marker>Boguraev, Neff, 2000</marker>
<rawString>Branimir Boguraev and Mary S. Neff. 2000. Discourse segmentation in aid of document summarization. In Proceedings of the 33rd Hawaii International Conference on System Sciences, pages 3004–3014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="10727" citStr="Collins and Roark, 2004" startWordPosition="1654" endWordPosition="1657">arge: for the local model, it is exponential in the length of the segment title, and for the global model it is exponential in the size of the tree. Therefore, we construct the output for each of these models incrementally using beam search. The algorithm maintains the most promising partial output structures, which are extended at every iteration. The model incorporates this decoding procedure into the training process, thereby learning model parameters best suited for the specific decoding algorithm. Similar models have been successfully applied in the past to other tasks including parsing (Collins and Roark, 2004), chunking (Daum´e and Marcu, 2005), and machine translation (Cowan et al., 2006). 4.1 Model Structure The model takes as input a tree of text segments S. Each segment s E S and its title z are represented as a local feature vector Φloc(s, z). Each component of this vector stores a numerical value. This feature vector can track any feature of the segment s together with its title z. For instance, the ith component of this vector may indicate whether the bigram (z[j]z[j + 1]) occurs in s, where z[j] is the jth word in z: { 1 if (z[j]z[j + 1]) � s (Φloc(s, z))z = 0 otherwise In addition, our mod</context>
<context position="14099" citStr="Collins and Roark, 2004" startWordPosition="2273" endWordPosition="2276">ase, the beam contains partially generated tablesof-contents, which are expanded by one segment title at a time. To further reduce the search space, during decoding only the top five candidate titles for a segment are given to the global model. 4.2 Training the Model Training for Step One We now describe how the local parameter vector αloc is estimated from training data. We are given a set of training examples (si, yi) for i = 1, ... ,l, where si is the ith text segment, and yi is the title of this segment. This linear model is learned using a variant of the incremental perceptron algorithm (Collins and Roark, 2004; Daum´e and Marcu, 2005). This online algorithm traverses the training set multiple times, updating the parameter vector αloc after each training example in case of mis-predictions. The algorithm encourages a setting of the parameter vector αloc that assigns the highest score to the feature vector associated with the correct title. The pseudo-code of the algorithm is shown in Figure 2. Given a text segment s and the corresponding title y, the training algorithm maintains a beam Q containing the top k partial titles of length j. The beam is updated on each iteration using the functions GROW an</context>
<context position="16046" citStr="Collins and Roark (2004)" startWordPosition="2634" endWordPosition="2637"> of text segments, and Ti is the table-of-contents for that tree. However, we cannot directly use these tablesof-contents for training our global model: since this model selects one of the candidate titles zi1, ... , zik returned by the local model, the true title of the segment may not be among these candidates. Therefore, to determine a new target title for the segment, we need to identify the title in the set of candidates 2If the word in the jth position of y does not occur in s, then the parameter update is not performed. 3For decoding, αloe is averaged over the training iterations as in Collins and Roark (2004). 547 s – segment text. y – segment title. y[1... j] – prefix of y of length j. Q – beam containing partial titles. 1. for j = 1 ... |y| 2. Q = PRUNE(GROW(s, Q)) 3. if y[1 ... j] ∈� Q 4. αloc = αloc + 4bloc(s, y[1 ... j]) F-zEQ �lo� QJ z) 5. Q = {y[1... j]} Figure 2: The training algorithm for the local model. that is closest to the true title. We employ the L1 distance measure to compare the content word overlap between two titles.4 For each input (5, T), and each segment s ∈ 5, we identify the segment title closest in the L1 measure to the true title y5: z* = arg min i Once all the training </context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the ACL, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Ivona Kucerova</author>
<author>Michael Collins</author>
</authors>
<title>A discriminative model for tree-to-tree translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>232--241</pages>
<contexts>
<context position="10808" citStr="Cowan et al., 2006" startWordPosition="1666" endWordPosition="1669">or the global model it is exponential in the size of the tree. Therefore, we construct the output for each of these models incrementally using beam search. The algorithm maintains the most promising partial output structures, which are extended at every iteration. The model incorporates this decoding procedure into the training process, thereby learning model parameters best suited for the specific decoding algorithm. Similar models have been successfully applied in the past to other tasks including parsing (Collins and Roark, 2004), chunking (Daum´e and Marcu, 2005), and machine translation (Cowan et al., 2006). 4.1 Model Structure The model takes as input a tree of text segments S. Each segment s E S and its title z are represented as a local feature vector Φloc(s, z). Each component of this vector stores a numerical value. This feature vector can track any feature of the segment s together with its title z. For instance, the ith component of this vector may indicate whether the bigram (z[j]z[j + 1]) occurs in s, where z[j] is the jth word in z: { 1 if (z[j]z[j + 1]) � s (Φloc(s, z))z = 0 otherwise In addition, our model captures dependencies among multiple titles that appear in the same tableof-co</context>
</contexts>
<marker>Cowan, Kucerova, Collins, 2006</marker>
<rawString>Brooke Cowan, Ivona Kucerova, and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proceedings of the EMNLP, pages 232–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Learning as search optimization: Approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>169--176</pages>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e and Daniel Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction. In Proceedings of the ICML, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: a parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 03 on Text summarization workshop,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6784" citStr="Dorr et al., 2003" startWordPosition="1013" endWordPosition="1016">ependent methods for summarizing long texts. Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al., 2002). In contrast to our method, these approaches perform either sentence or phrase extraction, rather than summary generation. Moreover, extraction for each segment is performed in isolation, and global constraints on the summary are not enforced. Finally, our work is also related to research on title generation (Banko et al., 2000; Jin and Hauptmann, 2001; Dorr et al., 2003). Since work in this area focuses on generating titles for one article at a time (e.g., newspaper reports), the issue of hierarchical generation, which is unique to our task, does not arise. However, this is not the only novel aspect of the proposed approach. Our model learns title generation in a fully discriminative framework, in contrast to the commonly used noisy-channel model. Thus, instead of independently modeling the selection and grammaticality constraints, we learn both types of features in a single framework. This joint training regime supports greater flexibility in modeling featur</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Bonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: a parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL 03 on Text summarization workshop, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noemie Elhadad</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Towards generating patient specific summaries of medical articles.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL Workshop on Automatic Summarization,</booktitle>
<pages>31--39</pages>
<contexts>
<context position="6004" citStr="Elhadad and McKeown (2001)" startWordPosition="890" endWordPosition="893"> for processing longer texts. Most of these approaches are tailored to a par1The code and feature vector data for our model and the baselines are available at http://people.csail.mit.edu/branavan/code/toc. ticular domain, such as medical literature or scientific articles. By making strong assumptions about the input structure and the desired format of the output, these methods achieve a high compression rate while preserving summary coherence. For instance, Teufel and Moens (2002) summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts. Elhadad and McKeown (2001) generate summaries of medical articles by following a certain structural template in content selection and realization. Our work, however, is closer to domainindependent methods for summarizing long texts. Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al., 2002). In contrast to our method, these approaches perform either sentence or phrase extraction, rather than summary generation. Moreover, extraction for each segment is performed in isolation, and gl</context>
</contexts>
<marker>Elhadad, McKeown, 2001</marker>
<rawString>Noemie Elhadad and Kathleen R. McKeown. 2001. Towards generating patient specific summaries of medical articles. In Proceedings of NAACL Workshop on Automatic Summarization, pages 31–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2722" citStr="Hearst, 1994" startWordPosition="409" endWordPosition="410">mmary could serve as an effective navigation tool for understanding a long, unstructured transcript for an academic lecture or a meeting. Given a text, our goal is to generate a tree wherein a node represents a segment of text and a title that summarizes its content. This process involves two tasks: the hierarchical segmentation of the text, and the generation of informative titles for each segment. The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). In this paper, we take the former approach. As for the second task, a naive approach would be to employ existing methods of title generation to each segment, and combine the results into a tree structure. However, the latter approach cannot guarantee that the generated table-of-contents forms a coherent representation of the entire text. Since titles of different segments are generated in isolation, some of the generated titles may be repetitive. Even nonrepetitive titles may not provide sufficient information to discriminate between the content of one seg544 Proceedings of the 45th Annual M</context>
<context position="7946" citStr="Hearst, 1994" startWordPosition="1205" endWordPosition="1206">ime supports greater flexibility in modeling feature interaction. 545 3 Problem Formulation We formalize the problem of table-of-contents generation as a supervised learning task where the goal is to map a tree of text segments S to a tree of titles T. A segment may correspond to a chapter, section or subsection. Since the focus of our work is on the generation aspect of table-of-contents construction, we assume that the hierarchical segmentation of a text is provided in the input. This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g., paragraph markers). During training, the algorithm is provided with a set of pairs (Sz, Tz) for i = 1, ... , p, where Sz is the ith tree of text segments, and Tz is the table-ofcontents for that tree. During testing, the algorithm generates tables-of-contents for unseen trees of text segments. We also assume that during testing the desired title length is provided as a parameter to the algorithm. 4 Algorithm To generate a coherent table-of-contents, we need to take into account multiple constraints: the titles should be g</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings of the ACL, pages 9– 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Jin</author>
<author>Alexander G Hauptmann</author>
</authors>
<title>Automatic title generation for spoken broadcast news.</title>
<date>2001</date>
<booktitle>In Proceedings of the HLT,</booktitle>
<pages>1--3</pages>
<contexts>
<context position="6764" citStr="Jin and Hauptmann, 2001" startWordPosition="1008" endWordPosition="1012">r, is closer to domainindependent methods for summarizing long texts. Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al., 2002). In contrast to our method, these approaches perform either sentence or phrase extraction, rather than summary generation. Moreover, extraction for each segment is performed in isolation, and global constraints on the summary are not enforced. Finally, our work is also related to research on title generation (Banko et al., 2000; Jin and Hauptmann, 2001; Dorr et al., 2003). Since work in this area focuses on generating titles for one article at a time (e.g., newspaper reports), the issue of hierarchical generation, which is unique to our task, does not arise. However, this is not the only novel aspect of the proposed approach. Our model learns title generation in a fully discriminative framework, in contrast to the commonly used noisy-channel model. Thus, instead of independently modeling the selection and grammaticality constraints, we learn both types of features in a single framework. This joint training regime supports greater flexibilit</context>
</contexts>
<marker>Jin, Hauptmann, 2001</marker>
<rawString>Rong Jin and Alexander G. Hauptmann. 2001. Automatic title generation for spoken broadcast news. In Proceedings of the HLT, pages 1–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: Experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="5863" citStr="Teufel and Moens (2002)" startWordPosition="870" endWordPosition="873">. 2 Related Work Although most current research in summarization focuses on newspaper articles, a number of approaches have been developed for processing longer texts. Most of these approaches are tailored to a par1The code and feature vector data for our model and the baselines are available at http://people.csail.mit.edu/branavan/code/toc. ticular domain, such as medical literature or scientific articles. By making strong assumptions about the input structure and the desired format of the output, these methods achieve a high compression rate while preserving summary coherence. For instance, Teufel and Moens (2002) summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts. Elhadad and McKeown (2001) generate summaries of medical articles by following a certain structural template in content selection and realization. Our work, however, is closer to domainindependent methods for summarizing long texts. Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al., 2002). In contrast to our method, these approaches perform </context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Wacholder</author>
<author>David K Evans</author>
<author>Judith Klavans</author>
</authors>
<title>Automatic identification and organization of index terms for interactive browsing. In</title>
<date>2001</date>
<booktitle>JCDL,</booktitle>
<pages>126--134</pages>
<contexts>
<context position="21312" citStr="Wacholder et al., 2001" startWordPosition="3530" endWordPosition="3533">h contextual dependencies. Baselines As an alternative to our hierarchical discriminative method, we consider three baselines that build a table-of-contents by generating a title for each segment individually, without taking into account the tree structure, and one hierarchical generative baseline. The first method generates a title for a segment by selecting the noun phrase from that segment with the highest TF*IDF. This simple method is commonly used to generate keywords for browsing applications in information retrieval, and has been shown to be effective for summarizing technical content (Wacholder et al., 2001). The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al., (2000). Similar to our local model, this method captures both selection and grammatical constraints. However, these constraints are modeled separately, and then combined in a generative framework. We use our local model (Flat Discriminative model, FD) as the third baseline. Like the second baseline, this model omits global dependencies, and only focuses on features that capture relations within individual segments. In the hierarchical generative (HG) baseline we run our global m</context>
</contexts>
<marker>Wacholder, Evans, Klavans, 2001</marker>
<rawString>Nina Wacholder, David K. Evans, and Judith Klavans. 2001. Automatic identification and organization of index terms for interactive browsing. In JCDL, pages 126–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wang</author>
<author>J Dunnion</author>
<author>J Carthy</author>
</authors>
<title>Machine learning approach to augmenting news headline generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the IJCNLP.</booktitle>
<contexts>
<context position="22354" citStr="Wang et al., 2005" startWordPosition="3690" endWordPosition="3693">odel omits global dependencies, and only focuses on features that capture relations within individual segments. In the hierarchical generative (HG) baseline we run our global model on the ranked list of titles produced for each section by the noisy-channel generative model. The last three baselines and our algorithm are provided with the title length as a parameter. In our experiments, the algorithms use the reference title length. Experimental Design: Comparison with reference tables-of-contents Reference based evaluation is commonly used to assess the quality of machine-generated headlines (Wang et al., 2005). We compare our system’s output with the table-ofcontents from the textbook using ROUGE metrics. We employ a publicly available software package,7 with all the parameters set to default values. Experimental Design: Human assessment The judges were each given 30 segments randomly selected from a set of 359 test segments. For each test segment, the judges were presented with its text, and 3 alternative titles consisting of the reference and the titles produced by the hierarchical discriminative model, and the best performing baseline. In addition, the judges had access to all of the segments in</context>
</contexts>
<marker>Wang, Dunnion, Carthy, 2005</marker>
<rawString>R. Wang, J. Dunnion, and J. Carthy. 2005. Machine learning approach to augmenting news headline generation. In Proceedings of the IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>