<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.321329">
<note confidence="0.822411">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
</note>
<subsectionHeader confidence="0.785949">
Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999032375">
the glosses. The glosses do not themselves
make the sense distinctions explicit.
In fact, we believe that most of the annota-
tor disagreements were, like this example, be-
tween closely related WordNet senses with only
subtle (and often inexplicit) distinctions and
that more coarse-grained sense distinctions are
needed (Palmer et al., 2004).
</bodyText>
<sectionHeader confidence="0.963537" genericHeader="abstract">
3 Systems and Scores
</sectionHeader>
<bodyText confidence="0.999963375">
26 systems were submitted by a total of 16
teams. The system names, along with email
contacts are listed in table 3. Two sets of scores
were computed for each system.
For the first set of scores (&amp;quot;With U&amp;quot;), we as-
sumed an answer of U (untaggable) whenever
the system failed to provide a sense. Thus the
instance would be scored as correct if the answer
key also marked it as U, and incorrect otherwise.
For the second set of scores (&amp;quot;Without U&amp;quot;),
we simply skipped every instance where the sys-
tem did not provide a sense. Thus precision was
not affected by those instances, but recall was
lowered.
Even though any given team may have in-
tended their results to be interpreted one way or
the other, we have included both sets of scores
for comparative purposes. Table 1 shows the
system performance under the first interpreta-
tion of the results (&amp;quot;With U&amp;quot;). The average pre-
cision and recall is 52.2%.
Table 2 shows the system performance under
the second interpretation of the results (&amp;quot;With-
out U&amp;quot;). The average precision is 57.4% and
51.9% is the average recall.
Since comprehensive groupings of the Word-
Net senses do not yet exist, all results given are
the result of fine-grained scoring.
Although we did not compute a baseline
score, we received several baseline figures from
our participants. Deniz Yuret, of Koc Univer-
sity, computed a baseline of 60.9% precision and
recall by using the first WordNet entry for the
given word and part-of-speech. Bart Decadt,
of the University of Antwerp and submitter of
the GAMBL-AW system, provided a baseline
of 62.4% using the same method (the 1.5% dif-
ference is most likely explained by how well
the baseline systems dealt with multi-word con-
structions and hyphenated words).
</bodyText>
<sectionHeader confidence="0.990849" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9996635">
As with the SENSEVAL-2 English all-words task,
the supervised systems fared much better than
</bodyText>
<table confidence="0.999408666666667">
System Precision/Recall
GAMBL-AW-S .652
SenseLearner-S .646
Koc University-S .641
R2D2: English-all-words .626
Meaning-allwords-S .624
Meaning-simple-S .610
upv-shmm-eaw-S .609
LCCaw .607
UJAEN-S .590
IR,ST-DDD-00-U .583
University of Sussex-Prob5 .572
University of Sussex-Prob4 .554
University of Sussex-Prob3 .551
DFA-Unsup-AW-U .548
IRST-DDD-LSI-U .501
KUNLP-Eng-All-U .500
upv-unige-CIAOSENSO-eaw-U .481
merl.system3 .458
upv-unige-CIAOSENS02-eaw-U .452
merl.systeml .450
IR,ST-DDD-09-U .446
autoPS-U .436
clr04-aw .434
merl.system2 .359
autoPSNVs-U .359
DLSI-UA-all-Nosu .280
</table>
<tableCaption confidence="0.792541">
Table 1: &amp;quot;With U&amp;quot; scores; a -S or -U suffix after
the system name indicates that the system was
reported as supervised or unsupervised, respec-
tively.
</tableCaption>
<table confidence="0.999954333333334">
System Precision Recall
GAMBL-AW-S .651 .651
SenseLearner-S .651 .642
Koc University-S .648 .639
R2D2: English-all-words .626 .626
Meaning-allwords-S .625 .623
Meaning-simple-S .611 .610
LCCaw .614 .606
upv-shmm-eaw-S .616 .605
UJAEN-S .601 .588
IR,ST-DDD-00-U .583 .582
University of Sussex-Prob5 .585 .568
University of Sussex-Prob4 .575 .550
University of Sussex-Prob3 .573 .547
DFA-Unsup-AW-U .557 .546
KUNLP-Eng-All-U .510 .496
IRST-DDD-LSI-U .661 .496
upv-unige-CIAOSENSO-eaw-U .581 .480
merl.system3 .467 .456
upv-unige-CIAOSENS02-eaw-U .608 .451
merl.systeml .459 .447
IR,ST-DDD-09-U .729 .441
autoPS-U .490 .433
clr04-aw .506 .431
autoPSNVs-U .563 .354
merl.system2 .480 .352
DLSI-UA-all-Nosu .343 .275
</table>
<tableCaption confidence="0.97915575">
Table 2: &amp;quot;Without U&amp;quot; scores, sorted by recall; a
-S or -U suffix after the system name indicates
that the system was reported as supervised or
unsupervised, respectively.
</tableCaption>
<table confidence="0.999376148148148">
System Name Email Contact
autoPS dianam©sussex.ac.uk
autoPSNVs dianam©sussex.ac.uk
clr04-aw ken©clres.com
DFA-Unsup-AW david©lsi.uned.es
DLSI-UA-Nosu montoyo©dlsi.ua.es
GAMBL-AW bart.decadt©ua.ac.be
IR ST-DDD-00 strappaKfitc.it
IRST-DDD-09 strappaKfitc.it
IRST-DDD-LSI strappaKfitc.it
Koc University dyuret©ku.edu.tr
KUNLP-Eng-All hcseoKM1p.korea.ac.kr
LCCaw parker©languagecomputer.com
Meaning lluism©lsi.upc.es
Meaning simple lluism©lsi.upc.es
merl.systeml bhiksha©merl.com
merl.system2 bhiksha©merl.com
merl.system3 bhiksha©merl.com
R2D2: EAW montoyo©dlsi.ua.es
SenseLearner rada©cs.unt.edu
UJAEN mgarcia©ujaen.es
USussex-Prob3 Judita.Preiss©cl.cam.ac.uk
USussex-Prob4 Judita.Preiss©cl.cam.ac.uk
USussex-Prob5 Judita.Preiss©cl.cam.ac.uk
upv-shmm-eaw amolina©dsic.upv.es
upv-CIAOSENSO amolina©dsic.upv.es
upv-CIAOSENS02 amolina©dsic.upv.es
</table>
<tableCaption confidence="0.8909405">
Table 3: email contact for each system; sorted
alphabetically.
</tableCaption>
<bodyText confidence="0.99994935">
the unsupervised systems (Palmer et al., 2001).
In fact, all of the seven systems reported as su-
pervised scored higher than any of the nine sys-
tems reported as unsupervised in both precision
and recall (using either of the two scoring crite-
ria).
The greatest difference between these results
and those of the SENSEVAL-2 English all-words
task is that a greater number of systems have
now achieved scores at or above the baseline.
While this result is encouraging, it seems that
the best systems have a hit a wall in the 65-
70% range. This is not surprising given the
typical inter-annotator agreement of 70-75% for
this task. We believe that further significant
progress must await the development of re-
sources with coarser-grained sense distinctions
and with glosses that draw explicit contrasts be-
tween the senses — resources more suitable for
the task at hand.
</bodyText>
<sectionHeader confidence="0.997959" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9987431875">
Christiane Fellbaum, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Martha Palmer, Christiane Fellbaum, Scott
Cotton, Lauren Delfs, and Hoa Trang Dang.
2001. English tasks: All-words and verb lex-
ical sample. In Proceedings of SENSEVAL-
2: Second International Workshop on Eval-
uating Word Sense Disambiguation Systems,
Toulouse, France, July.
Martha Palmer, Olga Babko-Malaya, and
Hoa Trang Dang. 2004. Different granu-
larities for different applications. In Second
Workshop on Scalable Natural Language Un-
derstanding Systems, HLT—NAACL, Boston,
MA, May.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.905414333333333">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics</note>
<abstract confidence="0.869351095238095">the glosses. The glosses do not themselves make the sense distinctions explicit. In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004). 3 Systems and Scores 26 systems were submitted by a total of 16 teams. The system names, along with email contacts are listed in table 3. Two sets of scores were computed for each system. For the first set of scores (&amp;quot;With U&amp;quot;), we assumed an answer of U (untaggable) whenever the system failed to provide a sense. Thus the instance would be scored as correct if the answer key also marked it as U, and incorrect otherwise. For the second set of scores (&amp;quot;Without U&amp;quot;), we simply skipped every instance where the system did not provide a sense. Thus precision was not affected by those instances, but recall was lowered. Even though any given team may have intended their results to be interpreted one way or the other, we have included both sets of scores for comparative purposes. Table 1 shows the system performance under the first interpretation of the results (&amp;quot;With U&amp;quot;). The average precision and recall is 52.2%. Table 2 shows the system performance under the second interpretation of the results (&amp;quot;Without U&amp;quot;). The average precision is 57.4% and 51.9% is the average recall. Since comprehensive groupings of the Word- Net senses do not yet exist, all results given are the result of fine-grained scoring. Although we did not compute a baseline score, we received several baseline figures from our participants. Deniz Yuret, of Koc University, computed a baseline of 60.9% precision and recall by using the first WordNet entry for the given word and part-of-speech. Bart Decadt, of the University of Antwerp and submitter of the GAMBL-AW system, provided a baseline of 62.4% using the same method (the 1.5% difference is most likely explained by how well the baseline systems dealt with multi-word constructions and hyphenated words). 4 Conclusion with the all-words task, the supervised systems fared much better than System Precision/Recall GAMBL-AW-S .652 SenseLearner-S .646 Koc University-S .641 R2D2: English-all-words .626 Meaning-allwords-S .624 Meaning-simple-S .610 upv-shmm-eaw-S .609 LCCaw .607 UJAEN-S .590 IR,ST-DDD-00-U .583</abstract>
<affiliation confidence="0.816450333333333">University of Sussex-Prob5 .572 University of Sussex-Prob4 .554 University of Sussex-Prob3 .551</affiliation>
<abstract confidence="0.925263235294118">DFA-Unsup-AW-U .548 IRST-DDD-LSI-U .501 KUNLP-Eng-All-U .500 upv-unige-CIAOSENSO-eaw-U .481 merl.system3 .458 upv-unige-CIAOSENS02-eaw-U .452 merl.systeml .450 IR,ST-DDD-09-U .446 autoPS-U .436 clr04-aw .434 merl.system2 .359 autoPSNVs-U .359 DLSI-UA-all-Nosu .280 Table 1: &amp;quot;With U&amp;quot; scores; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively.</abstract>
<title confidence="0.410422">System Precision Recall</title>
<phone confidence="0.6986097">GAMBL-AW-S .651 .651 SenseLearner-S .651 .642 Koc University-S .648 .639 R2D2: English-all-words .626 .626 Meaning-allwords-S .625 .623 Meaning-simple-S .611 .610 LCCaw .614 .606 upv-shmm-eaw-S .616 .605 UJAEN-S .601 .588 IR,ST-DDD-00-U .583 .582</phone>
<affiliation confidence="0.837162666666667">University of Sussex-Prob5 .585 .568 University of Sussex-Prob4 .575 .550 University of Sussex-Prob3 .573 .547</affiliation>
<phone confidence="0.790647833333333">DFA-Unsup-AW-U .557 .546 KUNLP-Eng-All-U .510 .496 IRST-DDD-LSI-U .661 .496 upv-unige-CIAOSENSO-eaw-U .581 .480 merl.system3 .467 .456 upv-unige-CIAOSENS02-eaw-U .608 .451</phone>
<abstract confidence="0.979302133333333">merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &amp;quot;Without U&amp;quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively. System Name Email Contact autoPS dianam©sussex.ac.uk autoPSNVs dianam©sussex.ac.uk clr04-aw ken©clres.com DFA-Unsup-AW david©lsi.uned.es DLSI-UA-Nosu montoyo©dlsi.ua.es GAMBL-AW bart.decadt©ua.ac.be IR ST-DDD-00 strappaKfitc.it IRST-DDD-09 strappaKfitc.it IRST-DDD-LSI strappaKfitc.it Koc University dyuret©ku.edu.tr KUNLP-Eng-All hcseoKM1p.korea.ac.kr LCCaw parker©languagecomputer.com Meaning lluism©lsi.upc.es Meaning simple lluism©lsi.upc.es merl.systeml bhiksha©merl.com merl.system2 bhiksha©merl.com merl.system3 bhiksha©merl.com R2D2: EAW montoyo©dlsi.ua.es SenseLearner rada©cs.unt.edu UJAEN mgarcia©ujaen.es USussex-Prob3 Judita.Preiss©cl.cam.ac.uk USussex-Prob4 Judita.Preiss©cl.cam.ac.uk USussex-Prob5 Judita.Preiss©cl.cam.ac.uk upv-shmm-eaw amolina©dsic.upv.es upv-CIAOSENSO amolina©dsic.upv.es upv-CIAOSENS02 amolina©dsic.upv.es Table 3: email contact for each system; sorted alphabetically. the unsupervised systems (Palmer et al., 2001). In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria). The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline. While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range. This is not surprising given the typical inter-annotator agreement of 70-75% for this task. We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses — resources more suitable for the task at hand.</abstract>
<note confidence="0.8013755">References Fellbaum, editor. 1998.</note>
<affiliation confidence="0.781597">Electronic Lexical Database. Press,</affiliation>
<address confidence="0.591279666666667">Cambridge, MA. Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang Dang.</address>
<note confidence="0.68205">English tasks: All-words and verb lexsample. In of SENSEVAL- 2: Second International Workshop on Evaluating Word Sense Disambiguation Systems, Toulouse, France, July. Martha Palmer, Olga Babko-Malaya, and Hoa Trang Dang. 2004. Different granufor different applications. In Workshop on Scalable Natural Language Un- Systems, HLT—NAACL,</note>
<address confidence="0.616915">MA, May.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Christiane Fellbaum</author>
<author>Scott Cotton</author>
<author>Lauren Delfs</author>
<author>Hoa Trang Dang</author>
</authors>
<title>English tasks: All-words and verb lexical sample.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL2: Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="4926" citStr="Palmer et al., 2001" startWordPosition="668" endWordPosition="671">hcseoKM1p.korea.ac.kr LCCaw parker©languagecomputer.com Meaning lluism©lsi.upc.es Meaning simple lluism©lsi.upc.es merl.systeml bhiksha©merl.com merl.system2 bhiksha©merl.com merl.system3 bhiksha©merl.com R2D2: EAW montoyo©dlsi.ua.es SenseLearner rada©cs.unt.edu UJAEN mgarcia©ujaen.es USussex-Prob3 Judita.Preiss©cl.cam.ac.uk USussex-Prob4 Judita.Preiss©cl.cam.ac.uk USussex-Prob5 Judita.Preiss©cl.cam.ac.uk upv-shmm-eaw amolina©dsic.upv.es upv-CIAOSENSO amolina©dsic.upv.es upv-CIAOSENS02 amolina©dsic.upv.es Table 3: email contact for each system; sorted alphabetically. the unsupervised systems (Palmer et al., 2001). In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria). The greatest difference between these results and those of the SENSEVAL-2 English all-words task is that a greater number of systems have now achieved scores at or above the baseline. While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range. This is not surprising given the typical inter-annotator agreement of 70-75% for this task. We believe that fur</context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, Dang, 2001</marker>
<rawString>Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang Dang. 2001. English tasks: All-words and verb lexical sample. In Proceedings of SENSEVAL2: Second International Workshop on Evaluating Word Sense Disambiguation Systems, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Olga Babko-Malaya</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Different granularities for different applications.</title>
<date>2004</date>
<booktitle>In Second Workshop on Scalable Natural Language Understanding Systems, HLT—NAACL,</booktitle>
<location>Boston, MA,</location>
<marker>Palmer, Babko-Malaya, Dang, 2004</marker>
<rawString>Martha Palmer, Olga Babko-Malaya, and Hoa Trang Dang. 2004. Different granularities for different applications. In Second Workshop on Scalable Natural Language Understanding Systems, HLT—NAACL, Boston, MA, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>