<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997745">
Parsing Some Constrained Grammar
Formalisms
</title>
<author confidence="0.999928">
K. Vijay-Shanker* David J. Weirt
</author>
<affiliation confidence="0.998514">
University of Delaware University of Sussex
</affiliation>
<bodyText confidence="0.991565181818182">
In this paper we present a scheme to extend a recognition algorithm for Context-Free Gram-
mars (CFG) that can be used to derive polynomial-time recognition algorithms for a set of for-
malisms that generate a superset of languages generated by CFG. We describe the scheme by
developing a Cocke-Kasami-Younger (CKY)-like pure bottom-up recognition algorithm for Lin-
ear Indexed Grammars and show how it can be adapted to give algorithms for Tree Adjoining
Grammars and Combinatory Categorial Grammars. This is the only polynomial-time recognition
algorithm for Combinatory Categorial Grammars that we are aware of.
The main contribution of this paper is the general scheme we propose for parsing a variety of
formalisms whose derivation process is controlled by an explicit or implicit stack. The ideas pre-
sented here can be suitably modified for other parsing styles or used in the generalized framework
set out by Lang (1990).
</bodyText>
<sectionHeader confidence="0.990292" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999962571428571">
This paper presents a scheme to extend known recognition algorithms for Context-Free
Grammars (CFG) in order to obtain recognition algorithms for a class of grammatical
formalisms that generate a strict superset of the set of languages generated by CFG.
In particular, we use this scheme to give recognition algorithms for Linear Indexed
Grammars (LIG), Tree Adjoining Grammars (TAG), and a version of Combinatory
Categorial Grammars (CCG). These formalisms belong to the class of mildly context-
sensitive grammar formalisms identified by Joshi (1985) on the basis of some properties
of their generative capacity. The parsing strategy that we propose can be applied to
the formalisms listed as well as others that have similar characteristics (as outlined
below) in their derivational process. Some of the main ideas underlying our scheme
have been influenced by the observations that can be made about the constructions
used in the proofs of the equivalence of these formalisms and Head Grammars (HG)
(Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993).
There are similarities between the TAG and HG derivation processes and that of
Context-Free Grammars (CFG). This is reflected in common features of the parsing
algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the
CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen
at each step in a derivation can depend only on which of a finite set of &amp;quot;states&amp;quot; the
derivation is in (for CFG these states can be considered to be the nonterminal symbols).
This property, which we refer to as the context-freeness property, is important because
it allows one to keep only a limited amount of context during the recognition process,
</bodyText>
<footnote confidence="0.28451925">
* Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716.
E-mail: vijay@udel.edu.
t School of Cognitive and Computing Sciences, University of Sussex, Brighton BN1 9QH, U.K. E-mail:
davidw@cogs.susx.ac.uk.
</footnote>
<note confidence="0.592359">
0 1994 Association for Computational Linguistics
Computational Linguistics Volume 19, Number 4
</note>
<bodyText confidence="0.9996765">
which results in polynomial time algorithms. In the recognition algorithms mentioned
above for CFG, HG, and TAG this is reflected in the fact that the recognizer can encode
intermediate stages of the derivation with a bounded number of states. An array is
used whose entries are associated with a given component of the input. In the case of
the CKY algorithm, the presence of a particular nonterminal in an array entry is used
to encode the fact that the nonterminal derives the associated substring of the input.
The context-freeness of CFG has the consequence that there is no need to encode the
way, or ways, in which a nonterminal came to be placed in an array entry.
In this respect, the derivation processes of CCG and LIG would appear to differ
from that of CFG. In these systems unbounded stacklike structures replace the role
played by nonterminals in controlling derivation choices. This would seem to suggest
that the context-freeness property of CFG, HG, and TAG derivations no longer holds.
Unbounded stacks can encode an unbounded number of earlier derivation choices. In
fact, while the path sets&apos; of CFG, HG, and TAG derivation trees are regular languages,
the path sets of CCG and LIG are context-free languages. With respect to recognition
algorithms, this suggests that the array (whose entries contain nonterminals in the
case of CFG) would need to contain complete encodings of unbounded stacks giving
an exponential time algorithm.
However, in LIG and CCG, the use of stacks to control derivations is limited in
that different branches of a derivation cannot share stacks. Thus, despite the above
observations, the context-freeness property does in fact hold. A detailed explanation
of why this is so will be presented below. We propose a method to extend the CKY
algorithm to handle the limited use of stacks found in CCG and LIG. We have chosen to
adapt the CKY algorithm since it is the simplest form of bottom-up parsing. A similar
approach using Earley algorithm is also possible, although not considered here. Since
the use of the stacks is most explicit in the LIG formalism we describe our approach in
detail by developing a recognition algorithm for LIG (Sections 2 and 3). We then show
how the general approach suggested in the parser for LIG can be tailored to CCG (in
Section 4). In the above discussion TAG has been grouped with HG. However, TAG
can also be viewed as making use of stacks in the same way as LIG and CCG. In
Section 5 we show how the LIG algorithm presented in Section 3 can be adapted for
TAG.
</bodyText>
<sectionHeader confidence="0.92887" genericHeader="keywords">
2. Linear Indexed Grammars
</sectionHeader>
<bodyText confidence="0.998873555555556">
An Indexed Grammar (Aho 1968) can be viewed as a CFG in which objects are nonter-
minals with an associated stack of symbols. In addition to rewriting nonterminals, the
rules of the grammar can have the effect of pushing or popping symbols on top of the
stacks that are associated with each nonterminal. Gazdar (1988) discussed a restricted
form of Indexed Grammars in which the stack associated with the nonterminal on the
left of each production can only be associated with one of the occurrences of non-
terminals on the right of the production. Stacks of bounded size are associated with
other occurrences of nonterminals on the right of the production. We call this Linear
Indexed Grammars (LIG).2
</bodyText>
<footnote confidence="0.9657214">
1 The path set of a tree is the set of strings labeling paths from the root to the frontier of the tree. The
path set of a tree set is the union of path sets of trees in the set.
2 The name Linear Indexed Grammars is used by Duske and Parchmann (1984) to refer to a different
restriction on Indexed Grammars in which production was restricted to have only a single nonterminal
on their right-hand side.
</footnote>
<page confidence="0.977724">
592
</page>
<note confidence="0.777628">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<figure confidence="0.427803">
Definition 2.1
A LIG, G, is denoted by (VN, VT V1, S, P) where
VN is a finite set of nonterminals,
VT is a finite set of terminals,
VI is a finite set of indices (stack symbols),
S E VN is the start symbol, and
P is a finite set of productions.
</figure>
<bodyText confidence="0.99982">
We adopt the convention that a, 0 (with or without subscripts and primes) de-
note members of 1/7, and 7 denotes a stack symbol. As usual, A,B,C will denote
nonterminals, a,b,c will denote terminals, and u, v, w will denote members of V.
</bodyText>
<sectionHeader confidence="0.394785" genericHeader="introduction">
Definition 2.2
</sectionHeader>
<bodyText confidence="0.999891142857143">
A pair consisting of a nonterminal, say A, and a string of stack symbols, say a, will
be called an object of the grammar and will be written as A (a). Given a grammar, G,
we define the set of objects lic(G) = { A (a) IA E VN, a E 177}.
We use T to denote strings in (lic(G)UVT)*. We write A(.. a) to denote the non-
terminal A associated with an arbitrary stack a with the string on top. Also, we use
A ( ) to denote that an empty stack is associated with A. The general form of a pro-
duction in a LIG is:
</bodyText>
<equation confidence="0.688643">
A(.. a) w1A1 (ai)w2 Ai_i (ai_i) wA (• • ai) wi+1A+1 (ai+i) ... An (a)w+1 for n &gt;
0 and w1• • , Wn+1 are members of V.
Definition 2.3
</equation>
<bodyText confidence="0.946870416666667">
The derivation relation, &gt;, is defined below. If the above production is used then for
any 0 E V, Ti , T2 E (17c(G)UVT)*:
TiA (Oa) T2 (ai) W2 • • Ai-1 (ai-i) wiAi (f3a1) w +1 +1 (ai+i)
... An (an) wn-01&apos;2.
We use *&gt; as the reflexive, transitive closure of . As a result of the linearity in
the general form of the rules, we can observe that the stack Oa associated with the
object in the left-hand side of the derivation and Oa, associated with one object in
the right-hand side have the initial part 0 in common. In the derivation above, we
will say that this object A (Oat) is the distinguished child of A (Oa). Given a deriva-
tion, the distinguished descendant relation is the reflexive, transitive closure of the
distinguished child relation.
The language generated by a LIG, G,L(G)= {w So *G&gt; w}.
</bodyText>
<equation confidence="0.489609">
Example 2.1
</equation>
<bodyText confidence="0.974165">
The LIG, G =- ({ S, T }, {a,b,c}, { -ya, 7n }, S,P) generates { wcw 1w E {a,b}+ } where P
contains the following productions.
</bodyText>
<equation confidence="0.9176265">
aS(..-Ya) S(•)—&gt; bS(•-Yb) S (.. ) 7&apos;
T(.. 7a) T(.-)a T(•)b T c
</equation>
<bodyText confidence="0.984993">
A derivation tree for the string abbcabb is given in Figure 1.
</bodyText>
<page confidence="0.997128">
593
</page>
<figure confidence="0.991949428571428">
Computational Linguistics Volume 19, Number 4
SO)
a S(7a)
S(7a%)
r7a7b%)
T(7y)
IT7a;)
</figure>
<figureCaption confidence="0.6551955">
Figure 1
Derivation tree for LIG.
</figureCaption>
<bodyText confidence="0.865781666666667">
In this paper rather than adopting the general form of rules as given above, we
restrict our attention to grammars whose rules have the following form. In fact, this
can be easily seen to constitute a normal form for LIG.
</bodyText>
<listItem confidence="0.99889475">
1. A(c) —&gt; 6 where 6 E VT U {E} and length of a, len (a) &gt; 1.
2. A (• • • • • 7m) —4 Ap 1/p) As (as) where m &gt; O.
3. A (-- • • • 7m) --&gt; A, (as) Ap (• • -yp) where m &gt; O.
4. A (• • . . -yrn) —&gt; Ap (• • -yp) where m &gt; O.
</listItem>
<bodyText confidence="0.999619857142857">
We allow at most two symbols in the right-hand side of productions because we
intend to develop CKY-style algorithms. In the above rules we say that Ap (• • -yp) is
the primary constituent and A, (as) is the secondary constituent. Notice also that
in a derivation using such a rule, the primary constituent yields the distinguished
child. (In grammatical theories that use a stack of subcategorized arguments, the top
of the stack in the primary constituent determines which secondary constituent it can
combine with.)
</bodyText>
<subsectionHeader confidence="0.97501">
2.1 Terminators
</subsectionHeader>
<bodyText confidence="0.955828">
Let us consider how we may extend the CKY algorithm for the recognition of LIG.
Given a fixed grammar G and an input al . . . an, the recognition algorithm will complete
an n x n array P such that an encoding of A (a) is stored in P [i, d] if and only if A (a) &apos;&gt;
a,. The algorithm will operate bottom-up. For example, if G contains the rule
A (• • • • • 7m) Ap (• • &apos;Tp) A, (a,) and we find an encoding of Ap (ap-yp) in P [i, dp] and
an encoding of A, (a,) in P[i + dp, dd then an encoding of A (ap7i • • • -Y.) will be stored
</bodyText>
<page confidence="0.995224">
594
</page>
<note confidence="0.947321">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.997211682926829">
in P [i, dp + . What encoding scheme should be used? The most straightforward
possibility would be to store a complete encoding of A (ap7i in P [i, cip + d3].
However, in general, if an object A (a) derives a string of length d then the length of
a is 0(d).3 Hence there can be 0(kd) objects that derive a substring of the input (of
length d), for some constant k. Hence, the space and time complexity of this algorithm
is exponential in the worst case.&apos;
The inefficiency of this approach can be seen by drawing an analogy with the
following algorithm for CFG. Suppose rather than storing sets of nonterminals in each
array entry, we store a set of trees containing all derivation subtrees that yield the
corresponding substring. The problem with this is that the number of derivation trees is
exponential with respect to the length of the string spanned. However, there is no need
to store derivation trees since in considering the combination of subderivation trees
in the CFG, only the nonterminals at the root of the tree are relevant in determining
whether there is a production that licenses the combination.
Likewise because of the last-in first-out behavior in the manipulation of stacks
in LIG, we will argue that it is not necessary to store the entire stack. For instance,
consider the derivation (depicted by the tree shown in Figure 2) from the point of
view of recording the derivation in a bottom-up parser (such as CKY). Let a node ni
labeled B Uhl . . . 7k. 7„,) be a distinguished descendant of a node n labeled A (0-yi • • • -yk)
as shown in the figure. Viewing the tree bottom-up, let the node n, labeled A (07i ... 7k),
be the first node above the node ?A, labeled B (071 -yk y„,), where 7k gets exposed
as the top of the stack. Because of the last-in first-out behavior, every distinguished
descendant of n above m will have a label of the form A&apos; (071 -yka) where len (a) &gt; 1.
In order to record the derivation from A (I371 ... 7k) it would be sufficient to store A
and . ykif we could also access the entry that records the derivation from At (0- y t).
In the entry for 7), using a pointer to the entry for At (07t) would enable the recovery
of the stack below the top k symbols, 7i • However, this scheme works well only
when k &gt; 2. For instance, when k = 1, suppose we recorded only A,71, and a pointer
to entry for At (0- yt). Suppose that we are looking for the symbol below 71, i.e., the
top of 3. Then it is possible that in a similar way the latter entry could also record
just At, 7t, and a pointer to some other entry to retrieve 0. This situation can occur
arbitrarily many times.
Consider the derivation depicted in Figure 3. In this derivation we have indi-
cated the branch containing only the distinguished descendants. We will assume that
the node labeled D (1371 .. • 7k-i7i • • &apos;7,11&apos; is the closest distinguished descendant of
C (1371 ... 7k-17) such that every node between them will have a label of the form
C&apos; (0-yi al where len (al &gt; 1. Therefore, any node between that labeled
C (O&apos;Yi • • 7k_0) and B (13-yi . . . -ym) will have a label of the form C&amp;quot; (07i -yk_la&amp;quot;)
where len (a&amp;quot;) &gt; 1. Now the entries representing derivations from both A(/371 ...
71&lt;-17k) and C 071 ... 7k_11/0 could point back to the entry for the derivation from
At (Mit), whereas the entry for C&apos; (1371 • • 7k-1 -11 a&apos;) will point back to the entry for
</bodyText>
<equation confidence="0.511676">
A (071 • • • N-17k) •
</equation>
<bodyText confidence="0.959057">
We shall now formalize these notions by defining a terminator.
</bodyText>
<footnote confidence="0.888280833333333">
3 For instance, consider the grammar in Example 2.1 and the derivation in Figure 1. In general we can
have derivations of the form T (7.-1) * cab&amp;quot;. However, if there exists productions of the form
A (a) —+ e then the length of the stack in objects is not even bounded by the length of strings they
derive.
4 The CCG parsing algorithms that have been proposed so far follow this strategy (Pareschi and
Steedman 1987; Tomita 1988).
</footnote>
<page confidence="0.993974">
595
</page>
<note confidence="0.562211">
Computational Linguistics Volume 19, Number 4
</note>
<figureCaption confidence="0.912071833333333">
Figure 2
Recovering the rest of stack-1.
Figure 3
Recovering the rest of stack-2.
Figure 4
Definition of a Terminator.
</figureCaption>
<figure confidence="0.994051333333333">
B(P71-1.-1
Za,N1
(f3-q
Aarr-Lp
(-----------
yzN
B(1371-1-4,
Atom As(a)
/ \
</figure>
<page confidence="0.823398">
596
</page>
<note confidence="0.603706">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<figure confidence="0.674150888888889">
Definition 2.4
Suppose that we have the derivation tree in Figure 4 that depicts the following deriva-
tion:
A (0-yi • • .7k-17) *&gt;. uB (071 7k-17k • • • 7m) w
uAt 0370 As (as) w
&gt; uvw
or similarly:
A (071 ••• 7k-17) *&gt; uB (1371 • .7k-i7t • • • -Ym) w
uAs (as) At (0-Yt)w
</figure>
<bodyText confidence="0.982113">
where the following conditions hold
</bodyText>
<listItem confidence="0.820106166666667">
• 2 &lt; k &lt; m
• The nodes labeled B (0-y . . . 7k-17k • • . 7.) and At (070 are distinguished
descendants of the node labeled A (0-yi ...7k_iy) in the respective trees.
• For any distinguished descendent labeled C (a&apos;) between the nodes
labeled A (071 . . . -yk_i-y) and B • • • 7k-17k • -Ym), cei is of the form
/371 • .. -yka where len (a) &gt; 1. Note that the nodes labeled
</listItem>
<bodyText confidence="0.9591483">
A (071 &apos;-yk_17) and B (MI • • • &apos;Tk-17k • • • -ym) need not be different.
The node labeled At (0-yt) is the k-terminator of the node labeled A (0-Y1 • • -Yk-i-Y) •
When it is clear from context, rather than saying that a node is a terminator of
another we will assume that terminators have been defined on objects that participate
in a derivation as well. For instance, in the above derivations, we will say that At (070
is the k-terminator of A (071 -yk_o). Also when the derivation is clear from context,
we will omit the mention of the derivation (or derivation tree). Additionally, we will
say that a node (object) has a terminator, if it has a k-terminator for some k.
We will now state some properties of terminators that influence the design of our
recognition algorithm.
</bodyText>
<equation confidence="0.894271666666667">
Definition 2.5
Given a grammar, G, define MCL(G) (Maximum Change in Length) as:
MCL(G) = max { m A (-• Yi • • -Ym) TiAp (•• -yp) T2 is a production of G
</equation>
<bodyText confidence="0.963782">
Henceforth, we will write MCL since the grammar in question will always be known
from context.
</bodyText>
<equation confidence="0.547813">
Observation 2.1
</equation>
<bodyText confidence="0.932777">
In a derivation tree, if a node (say has a k-terminator (say nt) then it is a dis-
tinguished descendant of ?I. If the node n is labeled A (i3a) (where len (a) = k) then
the node nt must be labeled At (f37t) for some At E VN and -yt E Vt. Furthermore,
</bodyText>
<equation confidence="0.2668425">
2 &lt;k &lt; MCL.
Observation 2.2
</equation>
<bodyText confidence="0.730414">
In a derivation tree, if a node has a k-terminator then it has a unique terminator.
</bodyText>
<page confidence="0.98653">
597
</page>
<note confidence="0.359069">
Computational Linguistics Volume 19, Number 4
</note>
<bodyText confidence="0.901559576923077">
If n is the node in question then we are claiming here that not only does it have a
unique k-terminator but also that there does not exist k&apos; with k&apos; k such that n has a
k&apos;-terminator. To see why this is the case, let some node 77 have a k-terminator (for some
k), say rit. Using Observation 2.1 we can assume that they are labeled A (8
v-/Yi • • • 7k-i-Y)
and At (0-0, respectively, where we have (k -1) &gt; 1. From the definition of terminators
we can assume that the parent of the terminator, Tit, is a node (say in that has a label of
the form B (071 • • ..•-ym). Since (from the definition of terminators) every node
between n and 7/ (inclusive) must have a label of the form C (0-yi • • • 7k-i a&apos;) where
len (a&apos;) &gt; 1, it immediately follows that nt is the closest distinguished descendant of n
such that the length of the stack in the object labeling m is strictly less than the length
of the stack in the object labeling i. From this, the uniqueness of terminators follows.
Observation 2.3
Consider the derivation A (0&apos;-yi uAt (0&apos;Yt)w uvw where At (13-yt) is the
k-terminator of A (071 -yk_i-y). Then for any /3&apos; and v&apos;, if At (0&apos;7t) *&gt; v&apos; then we
have the derivation A („.13/71 . . . *&gt; uAt (13&apos;&apos;yt) w *&gt; uv&apos;w where At (0&apos;70 is the
k-terminator of A (13&apos;
This follows from the fact that the derivation of uAt (0-&apos;t) w from A (
is independent of 0. Therefore we can replace At (0-Y t) * v by At (0&apos;7) *&gt; v&apos;. This
is a very important property that is crucial for obtaining polynomial-time algorithm.
Note that not all nodes have terminators. For example, if a node labeled A (a) is the
parent of a node labeled a (i.e., corresponding to the use of the production A (a) a
where a is a terminal symbol) then obviously this node does not have a terminator.
Definition 2.6
Given a grammar, G, we define MTL(G) (Maximum Length in, Terminal production)
as:
</bodyText>
<equation confidence="0.708768">
MTL(G) = max { len (a) I A (a) -&gt; E is a production of G where e c VT U{E} •
</equation>
<bodyText confidence="0.980317">
As in the case of MCL, we will use MTL rather than MTL(G).
</bodyText>
<equation confidence="0.429578">
Observation 2.4
</equation>
<bodyText confidence="0.989527333333333">
In the derivation A (a) * &gt; w if len (a) &gt; MTL then A (a) has a terminator.
There must be at least two steps in the above derivation since len (a) &gt; MTL.
However, we can assume that the node (say in question labeled by the object
</bodyText>
<equation confidence="0.861012">
A (a) has a distinguished descendant, say 771, with label B (0) such that B (0) &gt;
e.
</equation>
<bodyText confidence="0.977463714285714">
Therefore, len (0) &lt; MTL and we may rewrite w as uev. Since len (a) &gt; len (0) we
can find the closest distinguished descendant of n labeled C (a&apos;) for some C, a&apos; such
that len (a&apos;) &lt; len (a). That node is the terminator of n from the arguments made in
Observation 2.2.
The above observations will be used in the following sections to explain the way
in which we represent derivations in the parsing table. We conclude this section with
an observation that has a bearing on the steps of the recognition algorithm.
</bodyText>
<page confidence="0.990976">
598
</page>
<note confidence="0.699339">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<subsectionHeader confidence="0.497751">
Observation 2.5
</subsectionHeader>
<bodyText confidence="0.572373">
Consider the following derivation.
</bodyText>
<equation confidence="0.99255275">
A (071 • • • N-1) 1 TiAp (071 • • • 7k-17k) T2
U1/1p (071 • • • 7k-17k) U2
Ui 01At (070 V2 U2
WV2U2
</equation>
<bodyText confidence="0.997687">
where Ap (071 . . . 7k-17k) is the distinguished child of A (07i ...NA and At (Mit) is the
</bodyText>
<listItem confidence="0.8270575">
k-terminator of Ap (07i 7k-i-Yk) • At (07t) is the (k - *terminator of A (071 .. .-yk_i)
if and only if k &gt; 2. If k = 2 then A (i371) has a terminator if and only if At (07r)
does. In fact, in this case, if At (07t) has a k&apos;-terminator then that terminator is also the
k&apos;-terminator of A (137t).
</listItem>
<subsectionHeader confidence="0.996089">
This can be seen by considering the derivation shown in Figure 3 and noting the
</subsectionHeader>
<bodyText confidence="0.997006">
sharing of the terminator of C (071. -yk_i-yn and A (/371 . • 7k-17k) •
</bodyText>
<sectionHeader confidence="0.738815" genericHeader="method">
3. Recognition Algorithms
</sectionHeader>
<bodyText confidence="0.9989716">
As in the CKY algorithm we will use a two-dimensional array, P, such that if A (a) * &gt;
a, . . a,+d_i then a representation of this derivation will be recorded with an encoding of
A (a) in P [i, d]. Here we assume that the given input is a1 ... an. We start our discussion
by considering the data structures we use to record such objects and derivations from
them.
</bodyText>
<subsectionHeader confidence="0.999997">
3.1 Anatomy of an Entry
</subsectionHeader>
<bodyText confidence="0.995824181818182">
We mentioned earlier that the stack in an object can be unboundedly large. We must
first find a compact way to store encodings of such objects whose size is not bounded
by the grammar. In this section we provide some motivation for the encoding scheme
used in the recognition algorithm by considering the bottom-up application of the rule
and the encoding of the primary constituent:
A (.. ... 7m) Ap (.. 7p) As (as)
The Head. An object with nonterminal Ap and top of stack -yr, will match the primary
category of this rule. Thus, the first requirement is that at least this much of the object
must be included in every entry since it is needed to determine if the rule can apply.
This component is denoted (Ap, 7p) and called the head of the entry. Thus, in general,
an entry in P [i, di with the head (A, 7) encodes derivations of ai . a,+d_i from an
object of the form A (/37) for some EV.
Terminator-pointer. An encoding of the object Ap (07p) (the primary constituent) that
derives the substring a,. a,+dp_i (of the input string ch ... an) will be stored in the array
element P [i, dp] in our CKY-style recognition algorithms. Now consider the encoding
of Ap (13-yp) for some sufficiently long /37p. While the head, (AI, 7p), of the entry is
sufficient to determine whether the object in question can match the primary category
of the rule, we will need to store more information in order that we can determine the
content of the rest of the stack. In the above production, if m = 0 then the combination
of Ap (137p) and A, (a,) results in A (0). In order to record the derivation from A (0),
we need to know the top symbol in the stack 0, i.e., the symbol below the top of
the stack associated with the primary constituent. We need to recover the identity of
</bodyText>
<page confidence="0.994403">
599
</page>
<note confidence="0.364584">
Computational Linguistics Volume 19, Number 4
</note>
<bodyText confidence="0.997537958333333">
this symbol from the encoding of the primary category. This is why we introduced
the notion of terminators. As mentioned in Section 2.1, terminators can be used to
access information about the rest of the stack. In the encoding of Ap (07), we will
store information that allows us to access the encoding of its terminator. The part of
the entry encoding the terminator will be called terminator pointer.
The Middle. Note that the object Ap (0-yp) (in the derivation Ap (137r) * a,. ai±dp_i)
can have a k-terminator where k is between 2 and MCL. Therefore, from Observa-
tion 2.1 it follows that the terminator-pointer can only be used to determine the (k+1)St
symbol from the top. Therefore, assuming that = 0&apos;71 • • 7k-1, the terminator-pointer
will allow us to access (Recall from the definition, a k-terminator of A (0&apos;71 • • • N—l&apos;Yp)
will have the form At (0&apos;-y). Thus the (k + 1)3t symbol from the top in A (13,-yp) is the
same as the symbol below the top of the stack of the terminator.) Thus, we will need
to record the string in the encoding of Ap (/(3&apos;-yi -yk-cyp) as well. This part
of the entry will be called the middle.
To summarize, the entry stored in P [i, dp] (where ,13/-yi ...-yk_17p is assumed to be
sufficiently long that we know Ap ...-yk_i-yp) is guaranteed to have a termina-
tor) will have a head, (Ar, -yp); and a tail comprised of a middle, 71 • • • N-1; and a
terminator-pointer. Note that the length of the middle must be at least one, but at
most MCL - 1, since from Observation 2.1, we know 2 &lt; k &lt; MCL. We will call an
entry of this kind a terminator-type entry.
We will now discuss what we need to store in order to point to the termina-
tor. Suppose we would like to record in P [i, d] the derivation of a, • • . from
A (1371 ryk_l-y) as shown below. We assume that At (0-n) is the terminator in this
derivation.
</bodyText>
<equation confidence="0.976607333333333">
A (0-yi .. • -rk-i-y) a1. . . at_iAt (137t)at+d, • • • ai+d-1
a, at_iat . • • at+dt—iat+d, • • • al+d-i
ai . . • ai+d-i
</equation>
<bodyText confidence="0.952245">
From Observation 2.3, it follows that it would be sufficient to use ((At, -Yt) [t, di]) as
the terminator-pointer. This is because any entry with the head (At, 7t) in P [t, dt] will
represent in general a derivation At (0&apos; 7t) &gt;- at . • • at+c11-1. This not only matches the
above case, but even if /3&apos; 0, from the Observation 2.1, we have
A (13&apos; . • • 7k-0) &gt; Cli • • • at —iAt (017t) at-l-dt • &gt; ai • • • ai+d—l•
Thus, the use of the head information (plus the two indices) in the terminator-pointer
captures the essence of Observation 2.3. It is this structure-sharing that allows us to
achieve polynomial bounds for space and time. Note that the string derived from
the terminator, at . . • at±fit-i, is a substring of a,. ai±d_i. In such a case, i.e., when
&lt; t and i + t &gt; t + dt, we will say that (t, d) &lt; (i, d). We define (t, d) &lt; (i, d) if
(t, d) &lt; (i, d) and (t, d) (i, d). Since any terminator-type entry in P [i, d] can only
have terminator-pointers of the form ((Ar, 7t) [t, di]) where (t, c11) &lt; (i, d), the number
of terminator-type entries in P [i, el] is 0(d2).
</bodyText>
<subsectionHeader confidence="0.93728">
Definition 3.1
</subsectionHeader>
<bodyText confidence="0.982392">
Given a grammar, G, define MSL(G) (Maximum Secondary constituent&apos;s stack Length)
as MSL(G) =-. max {len (as) As (as) is the secondary constituent of a production
Henceforth we will use MSL rather than MSL(G).
</bodyText>
<page confidence="0.985747">
600
</page>
<note confidence="0.706287">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.815487">
We now consider the question of when a terminator-type entry is appropriate. Of
</bodyText>
<equation confidence="0.560649">
course, if A (a) *&gt; a, . . ai±d_i we could store such an entry in P [i, dp] only when A (a)
</equation>
<bodyText confidence="0.998117466666667">
has a terminator in this derivation. From Observation 2.4 we know that if len (a) &gt;
MTL then there exists a terminator of A (a) in this derivation. However, it is possible
that for some grammar MSL &gt; MTL. Therefore even when len (a) &gt; MTL (i.e., the
object has a terminator) A (a) can still match the secondary category of a rule if len (a) &lt;
MSL. In order to verify that an object matches the secondary category of a rule we
need to consider the entire stack in the object. When A (a) &apos;&gt; a,. ai+d_i and length
of a does not exceed MSL, it would be convenient to store A as well as the entire
stack a because such an object can potentially match a secondary category of a rule.
To be certain that such an object is stored in its entirety when len (a) &lt; MSL, the
terminator-type entry can only be used when len (a) &gt; max(MSL, MTL). However,
we prefer to use the terminator-type entry for representing a derivation from A (a)
only when its terminator, say At (0), is such that len (0) &gt; max(MSL, MTL) rather than
when len (a) &gt; max(MSL, MTL). Again, we point out that this choice is made only for
convenience and because we feel it leads to a simpler algorithm. The alternate choice
could also be made, which would lead to a slightly different algorithm.
</bodyText>
<equation confidence="0.6955892">
Definition 3.2
Define the constant TTC (Terminal-Type Case) as TTC max(MSL, MTL). In a deriva-
tion A (071 • • • 7k) *&gt; w we will say that A (071 • • • 7k) has the TC-property iff it has a
k-terminator, say At (07t), such that len (07t) &gt; TTC.
If A (0-Yi • • • 7k) &gt; a1. a,±d_i, where A (0-yi • • 7k) does not have the TC-property then
</equation>
<bodyText confidence="0.97490125">
we record the object in its entirety in P [i, d]. In order for such an entry to have the
same format as the terminator-type entry, we say that the entry has a head (A, 7k); a
tail with a middle -yk_i and a nil terminator-pointer. Note that in this case the
middle can be an empty string; for instance, when we encode A (7) &apos;&gt;a,.. a,+d_i. In
</bodyText>
<equation confidence="0.7739415">
general, if a = )37 then we say top (a) =&apos;y and rest (a) -= 0. If a = c then we say that
top (a) = rest (a) =E.
</equation>
<bodyText confidence="0.871687">
To summarize, the structure of an entry in P [i, d] is described by the following
rules.
</bodyText>
<listItem confidence="0.753403538461538">
• An entry consists of a head and a tail.
• A head consists of a nonterminal and a stack symbol.
• A tail consists of a middle and a terminator-pointer. The exact nature of
the middle and the terminator-pointer are as given below.
- The terminator-pointer may be of the form ((At, 7t) • [t, di])
where Ai E VN,-Yt E V1 and (t, d) &lt; (i, d). In this case, the middle
is a string of stack symbols of length at least one. This form of a
terminator pointer is used in the encoding of a derivation from
an object if its terminator has a stack length greater than or
equal to TTC. Recall that we had called this type of an entry a
terminator-type entry.
- A terminator-pointer can be a nil. Then the middle is a (possibly
empty) string of stack symbols. However, the length of the
</listItem>
<footnote confidence="0.791187">
middle is less than TTC MCL — I. This form of a terminator
pointer is used in the encoding of a derivation from an object if
it does not satisfy the TC-property; i.e., either it has no
</footnote>
<page confidence="0.987329">
601
</page>
<note confidence="0.361902">
Computational Linguistics Volume 19, Number 4
</note>
<bodyText confidence="0.9759775">
terminator or if the terminator exists then its stack length is less
than TTC.
</bodyText>
<subsectionHeader confidence="0.999975">
3.2 Recognition Algorithms for LIG
</subsectionHeader>
<bodyText confidence="0.999393111111111">
Since the full algorithm involves a number of cases, we develop it in stages by restrict-
ing the forms of productions. The first algorithm that considers the most restricted
form of productions introduces much of what lies at the core of our approach. Next
we relax these restrictions to some degree. After giving the algorithm at this stage,
we switch to discuss how this algorithm can be adapted to yield one for CCG. Later,
in Section 5, we consider further relaxation of the restrictions on the form of LIG
productions, which can help us produce an algorithm for TAG.
Regardless of which set of restrictions we consider, in every algorithm we shall
establish that the following proposition holds.
</bodyText>
<equation confidence="0.6087765">
Proposition 3.1
• ((A, 71) ell • • 7k-1, ((Ah -Yt) [t, dr]))) E P [i, cl] if and only if for some
/3E V,
A (Mil 7k-17k) &gt; a. at-iA (0-Yr) ar+d, _1 • • • ai+d —1
</equation>
<bodyText confidence="0.501647">
where At (Mit) is the k-terminator of A (i*71 • • • 7k) and len (07t) &gt; TTC.
</bodyText>
<listItem confidence="0.468424">
• ((A, 71c) (71 • • • 7k-1 7 nil)) E P [i, di if and only if
</listItem>
<equation confidence="0.689652">
A (71 • • • 7k-17k) a, . . .*
</equation>
<bodyText confidence="0.984936">
where in this derivation A (71 . . . &apos;yk_i-yk) does not have the TC-property.
</bodyText>
<listItem confidence="0.985427166666667">
3.2.1 Algorithm 1. Recall that the general form of rules that are to be considered are
as follows.
A (a) E where E e {E} U VT, and len (a) &gt; 1.
A (.. • • • 7m) —&gt; Ap (.. 70 As (as)
3. A ( • 71 • • • 7m) —&gt; As (as) Ap (- 7p) .
4. A (. • &apos;1/1 • • • -yin) Ap (•• &apos;Tp) •
</listItem>
<bodyText confidence="0.989465222222222">
At this stage we assume that the following restrictions hold of the above rules.
In the first type of production we assume that E E VT and len (cx) &gt; 1.
Thus MTL &gt; 1.
len (as) &gt; 1 in productions of type 2 and type 3, i.e., MSL &gt; 1.
There are no productions of type 4.
We will now give the following rules that specify how entries get added in the
parsing array. The control structure of the algorithm (a CKY-style dynamic program-
ming structure) will be added later. We assume that the input given is al . an, where
n &gt; 1.
</bodyText>
<page confidence="0.991174">
602
</page>
<note confidence="0.774779">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<subsectionHeader confidence="0.957095">
Initialization Phase
</subsectionHeader>
<bodyText confidence="0.9872225">
In the initialization phase of the algorithm we store lexical objects (objects deriving a
terminal symbol in one step) entirely in a single entry. In other words,
</bodyText>
<figure confidence="0.61573">
Rule 1.L
A (a) —&gt; a a = a, 1 &lt; i &lt; n
((A, top (a)) (rest (a), nil)) E PU, 11
</figure>
<subsectionHeader confidence="0.816736">
Inductive phase
</subsectionHeader>
<bodyText confidence="0.97971725">
Here productions of type 2 and type 3 will be considered. Let us assume the presence
of the following production in the grammar: A (. • 71 • • • 7.) Ap (. • 7p) A, (a5).5
Suppose that while considering which entries are to be included in P [i, d] we find
the following for some dp, d, such that dp + d, = d.
</bodyText>
<listItem confidence="0.909391142857143">
• The entry ((A, y) (Op, tpp)) E P [i, dp] . This is consistent with the rule&apos;s
primary constituent. Regardless of whether tp = nil or not, for some
13 E Ap (013p-yp) &gt; ai . a,+dp_i. That is, when tpp = nil we have
= E.
• The entry ((A5, top (a5)) (rest (as), nil)) E p [i dp, d5]. This is consistent
with the rule&apos;s secondary object. Thus if d = dp + ds we may assume
(cEs) ai±dp • • ai+d-1.
</listItem>
<bodyText confidence="0.945236722222222">
From the presence of the two entries specified above (and the derivations they rep-
resent) we have A (0 . . . &gt; Ap (0 Op-yp) As (as) a1. This derivation
must be recorded with an entry in P [i, d] . The content of the entry depends on sev-
eral factors: the value of m; whether or not the terminator-pointer in the entry for the
primary constituent (i.e., tpp) is nil; and the length of the middle in this entry (i.e., Op )
These determine whether or not the new entry will be a terminator-type entry. We
have cases for m = 0, m 1 and m &gt; 2.
CASE WHEN m =0
The new object to be stored is A (i3N). The top of the stack in this object can be
obtained from the stack associated with the primary constituent. How this is done
depends on whether the entry encoding the primary constituent is of terminator type
or not.
When m = 0 and tpp -=- nil
This means that the primary constituent has been represented in its entirety; i.e., the
primary constituent is Ap (/3lap). Since tpp = nil the primary constituent does not satisfy
the TC-property (i.e., it does not have a terminator with a stack of length greater than
or equal to TTC), the new constituent too cannot be encoded using a terminator-type
entry. Therefore,
</bodyText>
<equation confidence="0.6961355">
Rule 2.ps.L
((Ar, 7p) (Op, nil)) E P [i, dp] (05, top (a5)) (rest (as), nil)) E P[i + dp, d — dp]
((A, top (Op)) (rest (i3p), nil)) E P [i,
5 Similar arguments can be used when we consider the production: A(., -ym) —&gt; As (as)Ap (• • -yp).
</equation>
<page confidence="0.962415">
603
</page>
<note confidence="0.317944">
Computational Linguistics Volume 19, Number 4
</note>
<bodyText confidence="0.866743">
The following rule is the counterpart of Rule2.ps.0 that corresponds to the use of the
</bodyText>
<equation confidence="0.9513316">
production A ( • ) A, (as) Ap (&apos; • -Yr) •
Rule 2.sp.L
((A5, top (as)) (rest (a,), nil)) E P [i, ds] ((Ap,-yp) (Op , nil)) E P [i + d5, d — ds]
(24, top (0p)) (rest (/3), nil)) E P [i, d]
When m = 0 and tpp nil
</equation>
<bodyText confidence="0.999949818181818">
Let the entry for the primary constituent be ((A,7) (3p, ((At, 7t) , [t, dt]))). Since the
primary constituent is Ap (33p-y) we will assume that its terminator is At (137i) where
len (13-yt) &gt; TTC. Note also that len (Op-yp) &gt; 2. The entry for the new object (A 0,30)
is determined based on whether len (13p) = 1 or len (Op) &gt; I. In the latter case the
len (i3p7) -terminator of the primary constituent is the len (3)-terminator of the new
object. This is not so in the former case, as noted in Observation 2.5.
Considering the latter case first, i.e., len (Op) &gt; 1, we may write /37, as 71 ...N-yyk
where k &gt; 2. Since in this case the new object and the primary constituent have the
same terminator and since the primary constituent has the TC-property (tpp nil),
the new object must also be encoded with a terminator-type entry. Thus we have the
following rule:
</bodyText>
<equation confidence="0.98870075">
Rule 3.ps.L
((A,,,7) .. .7ktp)) E d,,]
tpp = ((At, 7t) di]) ,k &gt; 2 ((As, top (as)) (rest (as), nil)) c P [i + d,,, d — dp]
((A, 7k) (71 • • 7k—i, tpp)) E d]
</equation>
<bodyText confidence="0.999897">
Henceforth we shall give the ps versions of the rules only and omit sp versions.
Now let us consider the case when len (i3p) = I. Rewriting Op as 71, the entries
represent derivation for ,3 EVf (len (Thi) = len (0-&apos;t) &gt; TTC).
</bodyText>
<equation confidence="0.983441333333333">
A (07i) &gt; Ap (13-top) As (as)
at . . (0-Yt) a td-dt • • • ai-Fdp—lAs (as)
at . . . at_iat . . . a t+d, _iat+dt • at+dp—iai-pdp • • aid-d-1
</equation>
<bodyText confidence="0.9999651">
where Ai (0-yt) is the 2-terminator of Ap (071-yp). From Observation 2.5 it follows that
if Ai (0-yt) has a terminator then the terminator of A (l37i) in this derivation is the
same as the terminator of k (O&apos;Yt); and if k (i37t) has no terminator then neither does
A (07i). Additionally, in this derivation A 0370 satisfies the TC-property if and only
if At (0-yt) has the TC-property. That is, we should use a terminator-type entry to
record this derivation from A (,371) if and only if a terminator-type entry has been
used for At (070. Since these two objects share the same terminator (if it exists) the
terminator-pointer must be the same when we record derivations from them. There-
fore, suppose we use the terminator-pointer of ((Ap, 7p) Op ((At, 7t) , [t, di]))) to lo-
cate an entry ((At, &apos;Yt) (13i, tpt)) E P [t, di]. This would suggest the addition of the entry
</bodyText>
<footnote confidence="0.742874333333333">
6 Here L indicates a rule we use in LIG parsing; ps indicates that the primary constituent appears before
the secondary constituent. Similarly, sp will be used to indicate that the secondary constituent appears
before the primary constituent.
</footnote>
<page confidence="0.992722">
604
</page>
<note confidence="0.724353">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.99550975">
((A, -yi) (Ot , tpt)) to P [i, d], regardless of whether or not tpt = nil. However, we give
the two cases (tpt = nil or tpt = ((Ar, 7r) , [r, dr]) for some Ar, , r, dr) in the form of
two different rules. This is because (as we shall see later) these two rules will have to
appear in different points of the control-structure of the parsing algorithm.
</bodyText>
<equation confidence="0.9657483">
Rule 4.ps.L ((At, -yr) (Oi , nil))
((At„ -yp) , ((Ai, -yr), [t, di]))) ((As, top (a,)) (rest (a,), nil)) E P [t , di]
E P [i , dp] E P[i+dp,d —dp]
((il, 71) (/3t, nil)) E P [i , d]
Rule 5.ps.L
((A, y) tp))
((Ap, -yp) ((Ar, -yt) , [t, di]))) ((As, top (a,)) (rest (as), nil)) E P [t, di]
E P [i , dp] E P[i+dp,d— dp] tpt = ((Ar, -yr) , [r, dr] )
((A, ) (73r, tp,)) E P [i, d]
CASE WHEN m = 1
</equation>
<bodyText confidence="0.999872">
The length of the stack in the new object is equal to that of the primary object. In
fact, the terminator of the primary object (if it exists) is the same as the terminator of
the new object, and when the primary object has no terminator neither does the new
object. Therefore the encoding of the new object can easily be derived from that of the
primary object by simply modifying the head (to change the top of the stack symbol).
Thus we have:
</bodyText>
<equation confidence="0.989794714285714">
Rule 6.ps.L
((Ap, &apos;ye) (i3p, nil)) E P [i, dp] ((A,, top (as)) (rest (as), nil)) E P[i + dp, d — dp]
((A, y1) (13p, nil)) E P [i, d]
Rule 7.ps.L
((Ap, 7p) (i3p, ((At, -Yt) [t, di]))) E P [i, dp] ((As, top (a,)) (rest (as), nil)) E P [i dp, d — dp]
((A, 71) (I3P, ((At, -Yt) , [t, d]))) E P [i,
CASE WHEN m &gt; 2
</equation>
<bodyText confidence="0.99762225">
If the primary constituent is Ap (0 Op-yp) then the new constituent is A (013p-yi . . . &apos;yni) . In
fact, in this case, we have the primary constituent being the m-terminator of
A (01,71 ... -ym). Of course, this does not mean that the derivation from the new object
should be recorded with the use of a terminator-type entry. We use the terminator-type
entry only when len (13p-yp) &gt; TTC. In order to determine the length of this stack we
have to use the entry for the primary constituent (i.e., ((Ap, 7p) (Op, tpp)) E P [i, dp])
and consider whether this is a terminator-type entry or not (i.e., whether tpp -=- nil or
not).
</bodyText>
<page confidence="0.984843">
605
</page>
<figure confidence="0.422778">
Computational Linguistics Volume 19, Number 4
When m &gt; 2 and tpp nil
</figure>
<bodyText confidence="0.77891175">
Therefore the length of the stack of the terminator of the primary constituent is greater
than or equal to TTC. This means that stack length of the primary constituent (the
terminator of the new object) exceeds TTC. Thus we have the following rule:
Rule 8.ps.L
((lip,-rp) 13p, tpp)) E P [i, dp]
tpp = (At, -Yr) , [t, di]) ((As, top (as)) (rest (as), nil)) e P[i + dp, d - dp]
((A, -ym) (-Yi • • • -Ym -i, ((Ap, 70 , [i, di]))) E P [i, d]
When m &gt; 2 and tpp = nil
The primary constituent (which is the terminator of the new object) should be repre-
sented in its entirety. Therefore, in order to determine whether we have to encode the
new object with a terminator-type entry or not, we have to look at the entry for the
primary constituent. Thus we obtain the following rules:
</bodyText>
<equation confidence="0.989463">
Rule 9.ps.L
len (i3p-yp) &lt; TTC
((AP, 7p) Pp, nil)) E P [i, di)] ((As, top (as)) (rest (a,), nil)) E P [i + dp, d - did
((A, -yin ) (Op-Yi • . • &apos;Yni-i , nil)) E P [i, d]
Rule 10.ps.L
len (j3p-y) &gt; TTC
((Ar, -yp) (13p, nil)) c P [i, dp] ((As, top (as)) (rest (as), nil)) E P [i + dp, d - dp]
((A, 7,n) (yi . • • -Ym -1 , ((Ap, 7p) , [i, d]))) E P [i, el]
</equation>
<bodyText confidence="0.999666333333333">
In the discussions that follow, we find it convenient to refer to the entries mentioned in
the above rules as either antecedent entries (or entries that appear in the antecedent) of
a rule or consequent entry (or entry that appears in the consequent) of a rule. For ex-
ample, ((Ap, &apos;)/p) (/3p, nil)) in P [i, dp] and ((As, top (as)) (rest (as), nil)) in P [i + dp, d - dp]
are the antecedent entries of Rule 10.ps.L and ((A, -y„,) (-yi . . . -yip-1 , ( (Ap , -yp) , [i, dp] )))
that is added to P [i, d] is the entry in the consequent of Rule 10.ps.L.
</bodyText>
<subsectionHeader confidence="0.996465">
3.3 The Control Structure
</subsectionHeader>
<bodyText confidence="0.9999595">
We will start by giving a simple control structure for the recognition algorithm that
follows the dynamic programming style used in the CKY algorithm.
</bodyText>
<page confidence="0.991738">
606
</page>
<note confidence="0.725994">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.9981606">
In this section we modify the notation for entries slightly. In the above discussion,
the terminator-pointer of a terminator-type entry contains a pair of indices repre-
senting input positions. Thus, in effect, P is a four-dimensional array. As an alter-
native to saying that ((A, 7) 03, ((A&apos;, , [t, 4))) is in P [i, d] we will sometimes say
((A, -y) (0, (A&apos; , y&apos;))) is in P [i, d][t, dt] . Also as an alternative to saying ((A, a) (0, ni0)
is in P [i, d] we will sometimes say ((A, a) (0, nil)) is in P [i, d] [0, 0] . Thus P can be
considered to be an array of size nxnx (n + 1) x (n + 1).
In the specification of the algorithm (Figure 5) we will not restate all the rules we
discussed in the previous section. Instead we will only indicate where in the control
structure each rule fits. As an example, when we state &amp;quot;Use Rule 2.ps.L with dp d&amp;quot;
within the i, d, and d&apos; loops we mean the following: for current values of i, d, and d&apos; (and
hence dp, ds) consider every production of the form A (.• 71 • • • 7m) —&gt; Ap 70 As (as)
with m = 0. For each such production, look for entries of the form ((ilp, -y,) (Op nil)) E
P [i , dp] [0,0] for some Op and ((As, top (os)) (rest (as), nil)) E P[i + dp, d — dp] [0, 0] . In the
event we find such entries, we add ((21, top (13p) ) (rest Op nil)) to P [i, [0,0] if it is
not already there.
Since the entries in P [i, d] have the form ((A, -y) (0, ((At, 7t) , [t, dt] ) ) ) (where (t, dr) &lt;
(i, d)) or the form ((A, -y) (0, WO), there are 0(d2) many entries in P [i, d] (where
1 &lt; i &lt; n and 1 &lt; d &lt; n — d). Thus space complexity of this algorithm is 0(n4).
Note that within the body within the r loop will be attempted for all possible values
of i, d, d&apos;, t, dt, r, dr. Since the range of each loop is 0(n), the time complexity is 0(n7).
The asymptotic complexity of the above algorithm can be improved to 0(n6) with
a simple rearrangement of the control structure. The key point here is that the steps
involving the use of rules 5.ps.L and 5.sp.L can be split into two parts each. Consider,
for example, the use of the Rule 5.ps.L, which is repeated below.
</bodyText>
<equation confidence="0.9863266">
Rule 5.ps.L
((A, y1) (13t, tpt))
(Op, &apos;Tp) , ((At, -y,), [t, dt]))) ((As, top (as)) (rest (as), nil)) E P [t, cid
E P [i, dp] E P[i+dp,d— dp] tpt = ((Ar, -yr) , [r, dr])
((A,71) ((A,,, -yr) , [r, dr]))) e P
</equation>
<bodyText confidence="0.998282">
This rule corresponds to the use of the production A (• • ) —&gt; Ap (- • -yr,) A (c). The values
of i, d, d&apos;, t, dt are necessary to determine the span of the substrings derived from the
primary constituent and the secondary constituent, and the values of i, d, t, dt, r, dr are
needed to locate the entry for the terminator, i.e., ((At, &apos;Yr) Pr, ((Ar, &apos;Yr) , [r, dr]))) and
to place the new entry in the appropriate parsing table element. That is, the values of
r and dr are not required for the first part and the value of d&apos; need not be known for
the second part. This indicates that the second part need not be done within the loop
for d&apos;. Therefore, we can modify the control structure in the following way. Within
the t loop (which appears within the loops for d, d&apos; , dt) we find the entries for the
primary and secondary constituents. Having found the two relevant entries, we must
record the head of the new entry A,Op) and the terminator-pointer of the primary
constituent, i.e., ((At, -yt) , [t, d t]) . We can do this by using a two-dimensional array
called TEMP where we store (A, 71,A,, 7r). Outside the d&apos; loop (and hence outside the
loops for t and dt as well), but within the loops for i and d, we can have the loops
that vary t, dt, r, dr (note (r, dr) &lt; (t, di)) in order to locate the entry for the terminator
by using the information recorded in TEMP. Finally, having found the entry for the
</bodyText>
<page confidence="0.948013">
607
</page>
<figure confidence="0.719967977777778">
Computational Linguistics Volume 19, Number 4
Algorithm 1
begin
for i:= 2 to n do
Initialization phase
Use Rule 1
for d := 2 to n do % d loop
for i := 1 to n — d + 1 do % i loop
begin
for d&apos; := 1 to d —I do % d&apos; loop
begin
Use Rule 2.ps.L, 6.ps.L, 9.ps.L, 10.ps.L with dp = d&apos;.
for dt := (d&apos; — 1) to 1 do % dt loop
for t := i to (i + d&apos; — dr) do % t&apos; loop
begin
Use Rule 3.ps.L, 4.ps.L, 7.ps.L, 8.ps.L with dp = d&apos;
for dr := dt to 1 do
for r :-= t to t + dr — dr do
begin
Use Rule 5.ps.L with dp =
end
% end of dr loop
% end of r loop
end
% end of t loop
% end of dr loop
for dr := (d — d&apos; — 1) to 1 do % dr loop
for t := (i + d&apos;) to (i + d — dt) do % t&apos; loop
begin
Use Rule 3.ps.L, 4.ps.L, 7.ps.L, 8.ps.L with d, =
for d, := dt — 1 to 1 do
for r := t to (t + dt — dr) do
begin
Use Rule 5.sp.L with ds = d&apos;
end
% end of r loop
% end of dr loop
end
% end of t loop
% end of dr loop
end
% end of d&apos; loop
end
% end of i loop
% end of d loop
</figure>
<figureCaption confidence="0.3956565">
Figure 5
Algorithm 1.
</figureCaption>
<page confidence="0.990133">
608
</page>
<note confidence="0.877632">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.9977345">
terminator we then store the resulting entry in P [i, d]. These steps are captured by the
following rules. For a specific value of (i, d) we have
</bodyText>
<equation confidence="0.993782857142857">
Rule 5.i.ps.L
((Ap, 7) (71, ((Ar, -Yr), [t, di]))) ((A,, top (a,)) (rest (a,), nil))
E P [i, dp] E P[i dp, — dp]
(A, , At, &apos;Yr) E TEMP [t, dt]
Rule 5.ii.ps.L
(A, -yi, At, E TEMP [t, dt] ((Ar, -Yr) (Or, ((Ar,&amp;quot;Yr) [r, dr]))) E P [t, dr]
((A,71) (A, ((Ar, &apos;Yr) , dr] ))) E P [i,
</equation>
<bodyText confidence="0.997521">
Similarly, we assume we have the pair Rule 5.i.sp.L and Rule 5.ii.sp.L corresponding
to Rule 5.sp.L. This leads to the algorithm given in Figure 6. In this algorithm we drop
the sp rules and specify the ps rules only for the sake of simplicity.
The correctness of Algorithm 2 can be established from the correctness of Algo-
rithm 1 (which is established in Appendix A) and the following Lemma.
</bodyText>
<subsectionHeader confidence="0.809882">
Lemma 3.1
</subsectionHeader>
<bodyText confidence="0.9268872">
Given a grammar G and an input al an an entry ((A, 7) (0, tp)) is added to P [i,
by Algorithm 1 if and only if ((A, -y) (0, tp)) is added to P [i, d] by Algorithm 2.
Outline of Proof: Using induction on d. The base case corresponding to d = 1 in-
volves only the initialization step, which is the same in the two algorithms. The
only difference between the two algorithms (apart from the control structure) is the
use of Rule 5.ps.L (and Rule 5.sp.L) by Algorithm 1 versus the use of Rule 5.i.ps.L
and Rule 5.ii.ps.L (Rule 5.i.sp.L and Rule 5.i.sp.L) in Algorithm 2. Rule 5.ps.L is
used to add entries of the form ((A, 71) (Or, ((Ar, 7r) [r dr]))) . We can establish that
((A,71) (Or, ((Ar, -Yr) , [r, dr]))) is added to P [i , d] due to the application of Rule 5.ps.L
if and only if there exist entries of the form ((A,7) (71, Mt) 7t) [t, dt] ))) in P[i, dp];
((A,, top (a,)) (rest (a,), nil)) in P [i dp, d — dp]; ((Ar, 7r) ((Ar, 7r) , [r, dr] ))) in
P [t, dt]; and the production A (. • ) —&gt; Ap ( • &apos;Tp) As (as). Using induction, we can estab-
lish that these entries exist if and only if (A, 71, At, 7t) is added to TEMP [t, dt] using
Rule 5.ps.i.L (or Rule 5.sp.i.L) and ((A,71) (Or, ((A, -Yr) , [r, dr]))) is added to P [i,
using Rule 5.ii.ps.L.
</bodyText>
<sectionHeader confidence="0.894083" genericHeader="method">
4. Combinatory Categorial Grammars
</sectionHeader>
<bodyText confidence="0.8577404">
Combinatory Categorial Grammars (CCG) (Steedman 1985, 1986) are extensions of
Classical Categorial Grammars in which both function composition and function ap-
plication are allowed. In addition, forward and backward slashes are used to place
conditions concerning the relative ordering of adjacent categories that are to be com-
bined.
</bodyText>
<subsectionHeader confidence="0.680971">
Definition 4.1
</subsectionHeader>
<bodyText confidence="0.995478333333333">
The set of categories generated from a set, VN, of atomic categories is defined as the
smallest set such that all members of VN are categories, and if c1, c2 are categories then
so are (ci /c2) and (ci \ c2).
</bodyText>
<page confidence="0.990258">
609
</page>
<table confidence="0.8170896875">
Computational Linguistics Volume 19, Number 4
Algorithm 2
begin
for i:= 1 to n do
Initialization phase
Use Rule 1
for d := 2 to n do % d loop
for i := 1 to n — d + 1 do % i loop
begin
Initialize TEMP [t, cid to (i5 for all (t, dr) &lt; (i, d)
for c4, := I to d — 1 do % cip loop
Use Rule 2.ps.L, 6.ps.L, 9.ps.L, 10.ps.L
for dt := cip — I to 1 do % dr loop
for t := i to i + dp —dr do % t loop
Use Rule 3.ps.L, 4.ps.L, 5.i.ps.L, 7.ps.L, 8.ps.L
% end of t loop
% end of dr loop
% end of dp, loop
for dt := d — 1 to 1 do % dr loop
for t := i to i + d — dt do % t loop
for d, := dr — 1 to 1 do
for r := t to t + dt — dr do
begin
Use Rule 5.ii.ps.L
end
% end of r loop
% end of dr loop
% end of dr loop
% end of t loop
end
% end of i loop
% end of d loop
</table>
<figureCaption confidence="0.815614">
Figure 6
</figureCaption>
<figure confidence="0.7800974">
Algorithm 2.
Definition 4.2
A CCG, G, is denoted by (VT, VN, S, f, R) where
VT is a finite set of terminals (lexical items),
VN is a finite set of nonterminals (atomic categories),
S is a distinguished member of VN,
f is a function that maps each element of VT to a finite set of categories,
R is a finite set of combinatory rules, where combinatory rules have the following
form.
1. A forward rule has the following form where m &gt; 0.
(x/y) (y11z112 ...Imz.) (xlizi12 ...Imz.)
2. A backward rule has the following form where m &gt; 0.
(x\y) ( I
• • • Imzin)
(Ylizil2 • • • I
</figure>
<page confidence="0.944957">
610
</page>
<note confidence="0.833749">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.967876">
Here x, y, zi , ,zm are meta-variables and I , , 17, E {\, /}. For m 0 these rules
correspond to function application and for m &gt; 0 to function composition. Note that
the set R contains a finite subset of these possible forward and backward rules; i.e.,
for a given CCG only some of the combinatory rules will be available.
</bodyText>
<subsectionHeader confidence="0.798947">
Definition 4.3
</subsectionHeader>
<bodyText confidence="0.939389357142857">
In the forward and backward rules given above, we say that (x/y) (resp. (x\y)) is the
primary constituent of the forward (resp. backward) rules and (ylizi12 Imz,n) is the
secondary constituent of the rule. The notion of a distinguished child is defined as in
the case of LIG, i.e., a category is the distinguished child of its parent if it corresponds
to the primary constituent of the rule used. As before, the distinguished descendant
is the reflexive, transitive closure of the distinguished child relation.
In discussing CCG we use the notational conventions that the variables I and
c (when used with or without primes and subscripts) range over the forward and
backward slashes and categories, respectively. We use x,y,z for meta-variables; a, f3
for strings of directional categories (i.e., a string of the form I ci12lc n from some
n &gt; 0); and A, B, C for atomic categories (i.e., members of VN).
Derivations in a CCG, G = (VT, VN, 5,f R), involve the use of the combinatory
rules in R. Let G &gt; be defined as follows, where Ti and T2 are strings of categories
and terminal symbols.
</bodyText>
<listItem confidence="0.994744">
• If c1 c2 —&gt; c is an instance of a rule in R, then Ti cT2 G &gt; T1C1C2T2.
• If c E f (a) for some a E VT and c is a category, then T1cT2 G &gt; T1aT2.
</listItem>
<bodyText confidence="0.926285">
The string languages generated by a CCG, G, L(G) = {zu j S G* &gt;zul wE V }.
</bodyText>
<subsectionHeader confidence="0.830821">
Example 4.1
</subsectionHeader>
<bodyText confidence="0.9992545">
The following CCG generates { wcw I w E {a, b} } . Let G = ({a, b, , {S, T, A, B} , S, f , R)
where
</bodyText>
<equation confidence="0.630095">
f (a) = {A, T\A/T, T\A} f (b) = {B, T\B/T, T\B} f (c) = IS /T}
The set of rules R includes the following three rules.
y (x\y) x (x/y) (y\zi/z2) (y\zi/z2) (x/y) (y\zi) (y\zi)
</equation>
<bodyText confidence="0.943968777777778">
In each of these rules, the target of the category matched with x must be S.&apos; Figure 7
shows a derivation of the string abbcabb.
We find it convenient to represent categories in a minimally parenthesized form
(i.e., without parentheses unless they are needed to override the left associativity of
the slashes), where minimally parenthesized form is defined as follows.
7 Following Steedman (1985), we allow certain very limited restrictions on the substitutions of variables
in the combinatory rules. A discussion on the use of such restrictions is given in Vijay-Shanker and
Weir (in press). However, we have not included this in the formal definition since it does not have a
significant impact on the algorithm presented.
</bodyText>
<page confidence="0.967259">
611
</page>
<figure confidence="0.997447142857143">
Computational Linguistics Volume 19, Number 4
S
S \AT
a SkATT
SNAT/T
S \AIT
sir Tr
</figure>
<figureCaption confidence="0.995721">
Figure 7
</figureCaption>
<table confidence="0.6355105">
CCG example derivation tree.
Definition 4.4
</table>
<listItem confidence="0.919937">
• A is the minimally parenthesized form of A where A E VN.
• If , cn are the minimally parenthesized forms of categories el, , cic
respectively, then (Alici 12 • • • Incn) is the minimally parenthesized form of
((&apos; • (AlicDi2 •
</listItem>
<bodyText confidence="0.992427">
A category c is in minimally parenthesized form if c is the minimally parenthesized
form of itself.
</bodyText>
<subsectionHeader confidence="0.790885">
Definition 4.5
</subsectionHeader>
<bodyText confidence="0.9983775">
Let a category c = Alici 2..• Incn be in minimally parenthesized such that n &gt; 0,
A e VN, and ci, , cn are minimally parenthesized categories.
</bodyText>
<listItem confidence="0.9903395">
• The target category of c = A11c112 • • • Incn denoted by tar (c) is A.
• The arity of c = A11c112 • • • Incn, denoted as arity (c), is n.
• The argument categories of c = Alicil2 • • • Incn denoted by
args(c)= {c1 II &lt; i &lt; n}.
</listItem>
<subsectionHeader confidence="0.993627">
4.1 CCG and LIG
</subsectionHeader>
<bodyText confidence="0.999519428571429">
Before showing how the general parsing scheme illustrated by the LIG recognition
algorithm can be instantiated as a recognition algorithm for CCG, we show that CCG
and LIG are very closely related. The details of the examination of the relationship
between CCG and LIG may be found in Weir and Joshi (1988) and Weir (1988).
A minimally parenthesized category (Alici12 • • • Incn) can be viewed as the atomic
category, A, associated with a stack of directional argument categories, lic112 • • • Incn•
The rule
</bodyText>
<equation confidence="0.934604">
(x/Y) (Y11z112 • • • Iniztn) —&gt; (xlizil2 • • • Iniztn)
</equation>
<bodyText confidence="0.753211">
has as an instance (Apricii ...rncni As) (A511c112 • • • Imcm) (Apricii • -In&apos; • • • imcm)
as well as (Apric&apos;i • • • l&apos;nen/(AslY)) (Asi&apos;clicii2 • • • Imc,n) (AprIel linc&apos;nlici 12 • • • .c.)
</bodyText>
<page confidence="0.983096">
612
</page>
<note confidence="0.476195">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.977656">
as an instance. Thus x matches the category (Ap ... c), y matches an atomic cat-
egory A, in the first example and a nonatomic category (As I &apos;c&apos;) in the second, and
each z, matches c, for 1 &lt; i &lt; m. A derivation involving the second instance (viewed
bottom-up) can be seen as popping the top directional argument /(As l&apos;c&apos;) from the
primary category and pushing the m directional arguments I ci 12 ... mcm. Thus, each
instance of the combinatory rule appears to closely resemble a LIG production. For
example, in case of the second instance we have
</bodyText>
<subsubsectionHeader confidence="0.473362">
Ap (-11c112 Imcm) —&gt; Ap (•• / (Ad&apos; c&apos;)) As (I&apos; ci • • .1,,c,n) .
</subsubsectionHeader>
<bodyText confidence="0.99937">
We now show that, like the set of stack symbols of a LIG, the set of directional argu-
ment categories that we need to be concerned with is finite.
</bodyText>
<subsectionHeader confidence="0.723657">
Definition 4.6
</subsectionHeader>
<bodyText confidence="0.998882">
Let c be a useful category with respect to a grammar G if and only if c * &gt; w for
some w E V. The set of argument categories, args (G) of a CCG, G = (VT, VN, S, f, R),
is defined as args (G) = 1.1€f (a) args (c).
</bodyText>
<subsectionHeader confidence="0.753312">
Observation 4.1
</subsectionHeader>
<bodyText confidence="0.984098333333333">
If c is a useful category then args (c) c args (G), a finite set determined by the gram-
mar, G.
This observation can be shown by an induction on the length of the derivation
of some string from c. The base case corresponds to a lexical assignment and hence
trivially args (c) c args (G). The inductive step corresponds to the use of a combination
using a rule of the form
</bodyText>
<equation confidence="0.995570333333333">
(x/y) (Ylizi12 • • • Imzm) —&gt; (xlizil2 • • • Imzm)
or
(Y11z112 • Imz,) (x \y) (xlizil2 • • • Inizm)
</equation>
<bodyText confidence="0.999911">
By inductive hypothesis, any useful category matching either (x / y), (x\y) or
(YI zi12 ... mzm) must take its arguments from args (G) (a finite set) and therefore the
resulting useful category also shares this property.
The above property makes it possible to adapt the LIG algorithm for CCG. Note
that in the CKY-style CCG recognition we only need to record the derivations from
useful categories. From Observation 4.1 it follows that the lexical category assignment,
f, determines the number of &amp;quot;stack&amp;quot; symbols we need to be concerned with. Therefore,
only one of the variables (x) in a combinatory rule is essential in the sense that the
number of categories that it can usefully match is not bound by the grammar. There-
fore, it would be possible to map each combinatory rule to an equivalent finite set of
instances in which ground categories (from args (G)) were substituted for all variables
other than x; i.e., y, zm in the combinatory rule above. This would result in a
grammar that was a slight notational variant of a LIG where the CCG variable x and
the LIG notation perform similar roles. However, for the purpose of constructing a
recognition algorithm it is both unnecessary and undesirable to expand the number of
rules in this way. We adapt the LIG algorithm so that it, in effect, constructs appropriate
instances of the combinatory rules as needed during the recognition process.
</bodyText>
<page confidence="0.993471">
613
</page>
<note confidence="0.522521">
Computational Linguistics Volume 19, Number 4
</note>
<subsectionHeader confidence="0.997529">
4.2 Recognition of CCG
</subsectionHeader>
<bodyText confidence="0.999810333333333">
The first step in modifying the LIG algorithm is to define the constants MSL and MTL
for the case of CCG. Let G (VT, VN, S, f,R) be a CCG. These definitions follow im-
mediately from the similarities between CCG combinatory rules and LIG productions.
</bodyText>
<subsectionHeader confidence="0.832073">
Observation 4.2
</subsectionHeader>
<bodyText confidence="0.7659576">
If we were to express a combinatory rule
(x/y) (Y I 1 zi • • Inizm) (xlizi • • itnzm)
in terms of LIG production
A e. • • • 7m ) Ap 7p) As (as)
then we have the following correspondences:
</bodyText>
<listItem confidence="0.999269">
• -yp with /y.
• &apos;Yi with = izfor 1 &lt;i &lt; m, i.e., -ym with
• A = AP&apos;
• A, (as) with ylizi • • • Imzm.
</listItem>
<bodyText confidence="0.946026666666667">
Given such a direct correspondence between combinatory rules and LIG productions,
we will define the following constants to be used in the the CCG algorithm with
minimal explanation.
</bodyText>
<listItem confidence="0.984833714285714">
• MTL is the maximum arity of a lexical category. Thus,
MTL = max { arity (c) I c E f(a), a E VT}.
• MSL should be the maximum arity of a useful category that can match
the secondary category of a rule. Note that a category matching
(Ylizi 12 • • •linzm) will have an arity that is the sum of m and the arity of
the category matching y. Furthermore, note that since y is an argument
of the primary category it must be bound to a member of args (G). Thus,
</listItem>
<equation confidence="0.7231235">
MSL = max {m (YilZ112 is the secondary category of a rule in
R + max { arity (c) I c E args (G)}.
</equation>
<listItem confidence="0.999802">
• Note that in the case of CCG, MCL need not be defined independently
of MSL.
• As before, we define TTC as TTC = max {MSL, MTL }.
</listItem>
<bodyText confidence="0.96171975">
Since directional categories play the same role that stack symbols have in LIG, we
revise the notions of length top () and rest () as follows. We say that the string of direc-
tional arguments categories Iicil2. • • Inc&apos;, has a length n, i.e., len (11c112 • • &apos;nen) = n. Note
that arity 12 • • lc)) len (11c112 • • Incn) = n. We define top ((11c112 =
</bodyText>
<subsubsectionHeader confidence="0.535133">
Incn and rest ((llc112 • • • Incti)) = ic1 2• • Additionally, top (6) = rest (€) = 6.
4.2.1 Terminators in CCG. We can define a k-terminator in essentially the same way
</subsubsectionHeader>
<bodyText confidence="0.6244865">
as in the case for LIG. Note that a category shares its target category with all of its
distinguished descendants.
</bodyText>
<page confidence="0.993781">
614
</page>
<figure confidence="0.7294695">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
Definition 4.7
Suppose that we have the following derivation:
,
Ai311C1 • • • lk-lCk-i1C U AOI1C1 • lk—lCk-11/cCk • • • ImCm
U AO/Cp Cpl
11C1 • • IkCk • • • ImCm
UVW
or similarly
Al311C1 • • * • lk—ICk-11C U 24011C1 • • lk-1Ck-111cCk • • • ImCm W
. • IkCk ImCm AO\Cp W
UVW
</figure>
<bodyText confidence="0.937551">
where the following conditions hold
</bodyText>
<listItem confidence="0.971216">
• 3 is a string of direction categories, i.e., 3 E ({/ ,\}args (G))*.
• k - 1 &gt; 1,
• Ai3liCi • • • lk—lCk—likCk . • . ImCm and Ail/c are distinguished descendents of
AOlici • • ik-ick_i lc
• any distinguished descendent between AO/cp and A01 ici • lk-ick_i lc can
</listItem>
<bodyText confidence="0.958717347826087">
be expressed in the form AM1c1 • • • la where len (a) &gt; 1
We say that AO/cp is the len (lici • • • ik-ick-ilc) -terminator of A/311c1 • • •
Note that cp need not be atomic. Hence if we write the secondary category as
cp Imcm we are not necessarily expressing it in minimal parenthesis form.
4.2.2 Anatomy of a CCG Entry. In the CCG algorithm we will use entries that have a
form similar to that of the entries in the LIG algorithm. The choices we make are based
on Observation 4.2. For a derivation Alici *&gt; al a,±d_i (where the input is
al . an), we will have an entry in P [i, cl] with a head (A, licj), where A E VN, E /1,
and ci E args (G).
First consider the case when a terminator-type entry is used. The terminator-
type entry is applicable when AOlici • .. lk_ick_i lc has a k-terminator, say Af3Itct where
len (01tct) &gt; TTC. As before we say that in such a case AOlici • • • ik-ick-i lc satisfies the
TC-property. Assuming the terminator derives the substring at . . . at+di -1, we can use
the terminator-pointer Oct), [t, dt] ) and a middle lk_ick_i. Notice that since the
target of the category A1,311 ci lc as well as the target of its terminator is A
and since A is already noted in the head, it is not recorded in the terminator-pointer.
For entries that are not terminator-pointer, the entire category is noted in the
entry. Such an entry has the form ((A, lie!) _1 , nil)) assuming that j &gt; I.
However, it is possible that j =- 0. In this case the category being represented is A,
and the entry will be written as ((A, €) (e, nil)) . In general, we use the non-terminator-
type entry for recording a derivation from Aa when it has no terminator or when the
terminator, say Mitct (rewriting a as Olici lk_ick_i lc) is such that len (Oltct) &lt; TTC;
i.e., when the category Aa does not satisfy the TC-property.
</bodyText>
<footnote confidence="0.966734666666667">
4.2.3 CCG Algorithm. It is straightforward to derive the rules for the CCG recognition
algorithm from those used in LIG algorithm. Using Observation 4.2, we can now give
the rules for the CCG algorithm with no explanation.
</footnote>
<page confidence="0.995284">
615
</page>
<figure confidence="0.8586948">
Computational Linguistics Volume 19, Number 4
Rule 1.0
Aa E f (a) a = ai 1 &lt; &lt; n
(Op, top (a)) (rest (a), nil)) E P[i, 1]
Assume the combinatory rule (x / y) (ylizi • • • Imzm) —&gt; (xlizi • • • Imzm)-
</figure>
<equation confidence="0.957877">
When m =- 0 and tpp = nil
Rule 2.ps.0
Asa,= cp
((Ay, /c) (13p, nil)) E P [i, did ((As, top (as)) (rest (as), nil)) E P [i dp, d — dp]
((Ay, top (Op)) (rest (13p) , nil)) E P [i,
When m = 0 and tpp 0 nil
Rule 3.ps.0
tpp = ((lict) , [t, cid ]
k &gt; 2
/cp) • • • IkCk, tPp)) E P [i, dp] ((As, top (as)) (rest (a,), nil)) E P [i dp, d — dp]
((Ar, 1kCk) (11ci • • • tpp)) E
Rule 4.ps.0 ((Ar, Itct) (A, nil))
Asa, = cp E Pk, cid
((Ap, / cp) , ((itct) , [t, d]))) ((As, top (as)) (rest (a,), nil))
E P [i, dp] E P [i + dp, d — dp]
((Ar, (Ot , nil)) E P [i,
Rule 5.ps.0
Asa, = cp tp, = ((lrcr) , [r, dr])
((Ap, / cp) , ((ltci) , [t, d1]))) ((As, top (a,)) (rest (as), nil)) ((Ap,Itct) (Or, tP
P [i , dpi E P[i + dp,d — dp] E P [t,
(Op, tPt)) E cl]
When m = 1
Rule 6.ps.0
iLas
((Ar, /c) (Op, nil)) E did ((As, top (as)) (rest (a,), nil)) E P [i dp, d — dp]
((Ar, (/3p, nil)) E P [i ,
Asa, =- cp
</equation>
<page confidence="0.693128">
616
</page>
<note confidence="0.351972">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<equation confidence="0.992755210526316">
Rule 7.ps.0
Asa, =
((Ar, /c) ((ltct) , [t, dt]))) E P [t, did ((it, top (as)) (rest (a,), nil)) E P[i + dp, d — dr]
((Ar, hci) (Or, ((ltct) , [t, d]))) E P [i,
When m &gt; 2 and tpp nil
Rule 8.ps.0
tpp = (( I tcr) , [t, dr] ) Asa, cplici • • . Inicm
(KAp, /cp) (Op, tpp)) E P [i, dp] ((As, top (a,)) (rest (a,), nil)) E P [i + dp, d — dp]
Imcm) ••• ((/ cp) , [i, did))) E P [i,
When m &gt; 2 and tpp = nil
Rule 9.ps.0
len (i3 /c) &lt; TTC Asa, cplici •
((Ar, /c) (f3p, nil)) E P [i, clp] ((A,, top (a,)) (rest (as), nil)) E P [i + dp, d — dp]
((Ap,Imcm) Ini_icm_i , nil)) E P [i,
Rule 10.ps.0
len (Op/ Cp) &gt; TTC Asa, = Imcm
(Op, /c) (i3p, nil)) E P[1, dp] ((A,, top (a,)) (rest (as), nil)) E P [i + dp, d — dp]
Imcm) (lici • • • Im —icm ((/ cp) , [i, did))) E P [i, d]
Proposition 4.1
</equation>
<bodyText confidence="0.596985">
The CCG recognition algorithm can be seen to establish the following.
</bodyText>
<listItem confidence="0.959406166666667">
• ((Ar, lc) (0, ((itct) , [t, d]))) E P [i , d] if and only if there is some a such
that Aar* *&gt; a, . ai±d_i and the (len (0) + 1)-terminator (Aaltct) of
AaOlc derives the string at. . . at+d, _1 and len (altct) &gt; TIC.
• ((Ar, top (a)) , (rest (a), nil , )) E P [i, d] if and only if Aa * &gt; a, . .
and either Aa has no terminator or its terminator, say Aa&apos; is such that
len (a&apos;) &lt; TIC.
</listItem>
<sectionHeader confidence="0.688938" genericHeader="method">
5. TAG Recognition
</sectionHeader>
<bodyText confidence="0.967689">
We begin this section by first considering how to extend our algorithm for LIG to
handle unary productions. This will be needed to show we can instantiate our scheme
to give a recognition algorithm for TAG.
</bodyText>
<subsectionHeader confidence="0.998021">
5.1 Handling Unary Productions and Epsilon Productions
</subsectionHeader>
<bodyText confidence="0.9879665">
We will now show how the LIG algorithm given earlier can be extended to consider
unary productions of the form A (•• 7,7,) -4 Ap (• • -yp) as well as e productions of
</bodyText>
<page confidence="0.985121">
617
</page>
<note confidence="0.349644">
Computational Linguistics Volume 19, Number 4
</note>
<listItem confidence="0.698169333333333">
the form: A (a) E. However, we will now assume that m &lt; 2 in productions of the
form A (• • 71 -ym) —&gt; TiAp (.• 7p) T2. Thus, henceforth MCL &lt; 2. Note that this refers
to both unary and binary productions. This additional restriction does not change
the generative power. We have introduced these restrictions in order to reduce the
number of cases we have to consider and also because we can restrict our attention
to the productions that are used in the TAG to LIG construction.
</listItem>
<bodyText confidence="0.997606125">
Consider the processing of a binary production A (.• • • • 71n) —) Ap eYp) As (as).
Since CKY-style parsers work bottom-up, we check to see if the primary and secondary
categories derive adjacent strings (say a1. aj+dp_i and ai±dp . • • ai+dp+d,, respectively)
and then we store an encoding for the new object that results from the combination.
The processing of unary productions is similar except that we do not have to consider
a secondary constituent. The rules that express the processing of such productions will
be very similar to those for the binary productions. For example, consider Rule 2.ps.L
for the binary production A (. • ) ---+ Ap (• • -yr,) As (as).
</bodyText>
<equation confidence="0.973742333333333">
Rule 2.ps.L
(Op, -yp) (13p, nil)) E P [i, dp] ((A„ top (as)) (rest (as), nil)) E P [i + dp, d — dp]
((A, top (13p)) (rest (13p) , nil)) E P [i,
</equation>
<bodyText confidence="0.8368425">
Given a unary production A (••) Ap (. • -yp) we have the Rule 2.u.L (where u stands
for unary).
</bodyText>
<equation confidence="0.950019">
Rule 2.u.L
((A, y) (Op, nil)) E P [i, dp]
((A, top (p)) (rest (0p), nil)) E P [i,
</equation>
<bodyText confidence="0.938754714285714">
In addition, with the introduction of E productions, we have to consider derivations
of strings of length d = 0. We shall assume that if A (a) * E then an encoding of
A (a) will be stored in P[i, 01 (for all i). We must also consider the possibility that the
primary constituent or the secondary constituent derive the empty string, i.e., dp = 0
or ds = 0. Processing of such cases becomes similar to that of unary productions.
To indicate the additional processing required due to the introduction of unary
productions and the possibility of the derivation of the empty string, let us consider
Rule 8.ps.L. &amp;quot;Use Rule 8.ps.L&amp;quot; can be paraphrased as follows.
If there exists a production A (.• 7„,) Ap (.• -yp) A, (as) where
m 2, el = ((Ap, &apos;YO (i3p, (A, -y))) belongs to P [i, d] [t, d] and e2 =
((As, top (as)) (rest (as) , nil)) belongs to P [i + dp, d — dp] [0,0] then add
e3 = ((A, 7,7,) (71 . . . (A, &apos;y))) to P [i, cl] [i, d] if e3 is not already
present in this array element.
If we allow E productions it is possible that ds = d—dp = 0. Consider the case where we
have A, (a5) E. That is, we expect the entry e2 to be present in P[i + d, 0] [0, 0] . This
means that the resulting entry e3 must be added to P [i, d] [i, cl] since we now have
dp = d. Note that the addition of e3 = ((A, 7,n) . . . (Ap, 7p))) (that encodes
the derivation from A (13-yi -yp) for some 13) can result in more entries being added
to the same array element P [i, cl] [i, cl] (for instance, when we have the production
B (-- -yk) A (-. -ym)). This is similar to the prediction phase in Earley&apos;s algorithm
and the state construction in LR parsing. Based on this analogy, we will define our
</bodyText>
<page confidence="0.995687">
618
</page>
<note confidence="0.870481">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.995509142857143">
notion of closure. Closure (e, i, d, t, dt) will add entries to P[1, [t, dt] or P [i, [i, d] that
result from the inclusion of the entry P [i, d] [t, dt] by considering unary productions
(or binary productions when the primary or secondary constituent derives the empty
string). Before we define Closure 0 we note that for each occurrence in the algorithm
of &amp;quot;use Rule X&amp;quot; is replaced by &amp;quot;use closure of Rule X.&amp;quot; For example, &amp;quot;Use closure of
Rule 8.ps.L&amp;quot; stands for
If we have the production A (.-Y1 • • 7m) —&gt; Ap (.• &apos;yp) As (cis) where m &gt; 2,
</bodyText>
<equation confidence="0.9506145">
= ((A, (Op, At, &apos;Yt)) belongs to P [i , [t, cid and
e2 = ((As, top (as)) (rest (as), nil)) belongs to P [i + dp, d - dp] [0,0] and
e3 = ((A, 7m) • • 7m -1 , Ap, 7p)) does not belong to P [1, d] [i, d] then
add e3 to P [i, [i, d] and then invoke Closure (e3, d, dp).
</equation>
<bodyText confidence="0.420726">
Closure is defined as follows:
</bodyText>
<figure confidence="0.662766777777778">
Closure (e,ii,d,,t,dt)
begin
use closure of Rule 2.ps.L, 6.ps.L, 7.ps.L, 8.ps.L, 9.ps.L, 10.ps.L
with d =- dp and the entry e as the primary constituent in the antecedent.
use closure of Rule 2.sp.L, 6.sp.L, 7.sp.L, 8.sp.L, 9.sp.L, 10.sp.L
with ds = d and the entry e as the secondary constituent in the antecedent.
use closure of Rule 2.u.L, 6.u.L, 7.u.L, 8.u.L, 9.u.L, 10.u.L
with d = dp and the entry e as the primary constituent in the antecedent.
end.
</figure>
<bodyText confidence="0.975908409090909">
Note Rule 3 does not apply since we have to assume MCL &lt; 2 (hence any ter-
minator is a 2-terminator and the length of the middle in a terminator-type entry is
always one). We have not included Rule 4 and Rule 5 while computing the closure.
These correspond directly to the completor step in Earley&apos;s algorithm and to the pop-
ping of stack elements and hence are not considered a part of the closure. They have
to be applied later in the control structure.
We will now consider the effect of including unary rules on the control structure of
the algorithm. Let ((i1, d1) , (ii, d2)) ( (i3, d3) , (i4, if and only if (1) (i1, d1) &lt; (i3, d3)
or (2) (i1, d1) = (i3, d3) and (i2, d2) &lt; (i4, d4). The simplicity of the loop structure in the
algorithms seen thus far stems from the fact that for any parsing rule if the entry in the
consequent is to be added to P [i3, d] [i4, d4] based on the existence of an antecedent
entry in P[il, d1] [i2, d2], then ((i1, d1) , (i2, d2)) ((is, d3) , (i4, d4)). This no longer holds
when we consider Rule 5.u.L or Rule 5.ps.L when the secondary constituent derives the
empty string. Consider the following derivation (and the presence of the productions
assumed) for a sufficiently long 0:
A() 1 &gt; (13-n) 1&gt; A2 (711&apos;2) 1 &gt; A3 (i) * ai .
Consider the addition of an entry e3 to P [i, [t, dt] (for some (t, di)) to record the
derivation from A3 (07). Closure (e3, i, d, t, dt) is invoked, resulting in the addition of
C2 (corresponding to A2 (Th172)) to P [i, [i, From Rule 5.u.L and the presence of
entry e2 and e3 we would add el (corresponding to A1 (0-y1) to P[i, [t, cid). This could
result in the need to add more entries to P [i , d] [i, d], which in turn could cause new
entries being added back to P [i, [t, dt] , and so on. Thus we have a situation where
</bodyText>
<page confidence="0.989644">
619
</page>
<figure confidence="0.993853923076923">
Computational Linguistics Volume 19, Number 4
initialization phase
for loops for d, i, d&apos; as before
begin
consider closure of Rules in Rule set I
for d, := d&apos; — 1 to 1 do
for t := i to i + d&apos; —dt do
repeat
consider closure of Rules in Rule set II
for d, := dt — 1 to 1 do
for r := t to t + dt — dr do
consider closure of Rules 5.ps.L and Rule 5.u.L
until no new entries are added to P [i, d] [t, d]
</figure>
<figureCaption confidence="0.963824">
Figure 8
</figureCaption>
<bodyText confidence="0.6223385">
Control structure with unary productions.
an antecedent entry in P [i, [t, cid ((t, dt) &lt; (i, d)) causes an entry to be added to
</bodyText>
<equation confidence="0.886294">
P [i, cl] [i, d], which, acting as an antecedent entry, causes a new entry to be added to
P [i, d][t, dt] .
</equation>
<bodyText confidence="0.991086153846154">
A simple strategy to take care of this situation would be to add another loop
within the t loop (as shown in the partial control structure given in Figure 8) that is
repeated until no new entries are added to P [i, d] [t, dt] . It is straightforward to prove
the correctness of the algorithm with this additional loop and also that the asymptotic
complexity remains the same. The latter is the case because only a bounded number
of entries can belong to P[i,d][t,dt] for any fixed value of i, d, t, dt, and hence the
repeat loop can be iterated only a bounded number of times (as determined by the
grammar). In the partially specified control structure given in Figure 8, we have not
considered the sp rules. Also we only consider the changes that need to be made to
Algorithm 1; the changes to Algorithm 2 can be made in a similar fashion. Finally, for
purposes of abbreviation, we have grouped Rules 2.ps.L, 6.ps.L, 9.ps.L, and 10.ps.L
together and called it the Rule set I, and Rules 3.ps.L, 4.ps.L, 7.ps.L, and 8.ps.L the
Rule set II.
The repeat loop shown in Figure 8 is not needed in some situations. Consider
the derivation and the sequence of addition of entries, e3, e2, el, as discussed above.
Viewing this derivation as a bottom-up recognizer would, we have a &amp;quot;prediction&amp;quot; from
entry e3 followed by a &amp;quot;completion&amp;quot; that results in the entry e1. In this case the two
entries both encode objects with the same stack length. We generalize this situation
and call such derivations auxiliary derivations (named after auxiliary trees in TAG).
A (hi) 1 &gt; TA i (172) T2 * TlUAt (131/t) WT2 *&gt; u1uAt (070 wwi
where At (r37t) is the 2-terminator of Ai (0-Yi &apos;72). We will say that this auxiliary deriva-
tion spans at least one terminal if len (al uwwi) &gt; 1. Notice that if for a particular gram-
mar every auxiliary derivation spans at least one terminal, then the extra repeat loop
added becomes unnecessary. This is because now, with this assumption, for every pars-
ing rule if the entry in the consequent is to be added to P [i3, d3][i4, 4] based on the exis-
tence of an antecedent entry in P[ii, [i2, d2] then ((ii , (i2, d2)) ((i3, d3) , d4))
</bodyText>
<page confidence="0.994587">
620
</page>
<note confidence="0.914124">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.998463">
We end this section by noting that in the case of a lexicalized TAG, we can verify
that every auxiliary derivation spans at least one terminal, and hence in the TAG
algorithm we do not have to include this additional repeat loop.
</bodyText>
<subsectionHeader confidence="0.99993">
5.2 Tree Adjoining Grammars
</subsectionHeader>
<bodyText confidence="0.99948">
Tree Adjoining Grammars (TAG) is a tree generating formalism introduced by Joshi,
Levy, and Takahashi (1975). A TAG is defined by a finite set of trees composed by
means of the operation of tree adjunction.
</bodyText>
<subsectionHeader confidence="0.957175">
Definition 5.1
</subsectionHeader>
<bodyText confidence="0.97013308">
A TAG, G, is denoted by (VN, VT, S, I, A) where
VN is a finite set of nonterminals symbols,
VT is a finite set of terminal symbols,
S E VN is the start symbol,
I is a finite set of initial trees,
A is a finite set of auxiliary trees.
An initial tree is a tree with root labeled by S and internal nodes and leaf nodes
labeled by nonterminal and terminal symbols, respectively. An auxiliary tree is a tree
that has a leaf node (the foot node) that is labeled by the same nonterminal that labels
the root node. The remaining leaf nodes are labeled by terminals and all internal nodes
labeled by nonterminals. The path from the root node to the foot node of an auxiliary
tree is called the spine of the auxiliary tree. An elementary tree is either an initial
tree or an auxiliary tree. We will use a to refer to an initial tree, and 0 to refer to
an auxiliary tree. -y may be used to refer to either an elementary tree or a tree that is
derived from an elementary tree.
We will call a node in an elementary tree an elementary node. We can give a
unique name to each elementary node by using an elementary node address. An
elementary node address is a pair composed of the name of the elementary tree to
which the node belongs and the address of the node within that tree. We will assume
the standard addressing scheme where the root node has an address e. If a node
addressed ,Lt has k children then the k children (in left to right order) have addresses
• 1,. . . , p, • k. Thus, if Al is the set of natural numbers then p, E Al&apos;. In this section
we will use to refer to addresses and n to refer to elementary node addresses. In
general, we can write 71 = (-y, ,a) where -y is an elementary tree and it E Domain (7).
We will use Domain (-y) for the set of addresses of the nodes in -y.
</bodyText>
<subsectionHeader confidence="0.96507">
Definition 5.2
</subsectionHeader>
<bodyText confidence="0.999945909090909">
Let 7 be a tree with internal node labeled by a nonterminal A. Let 0 be an auxiliary
tree with root and foot node labeled by the same nonterminal A. The tree, -y&apos;, that
results from the adjunction of 0 at the node in -y labeled A (as shown in Figure 9) is
formed by removing the subtree of -y rooted at this node, inserting 0 in its place, and
substituting it at the foot node of 13.
Each elementary node is associated with a selective adjoining (SA) constraint that
determines the set of auxiliary trees that can be adjoined at that node. In addition,
when adjunction is mandatory at a node it is said to have an obligatory adjoining
(OA) constraint. Figure 9 shows how constraints are associated with nodes in trees
derived from adjunctions. Whether 0 can be adjoined at the node (labeled by A) in
is determined by c, the SA constraint of the node. In -y&apos; the nodes contributed by 0
</bodyText>
<page confidence="0.988268">
621
</page>
<figure confidence="0.99737325">
Computational Linguistics Volume 19, Number 4
Ad
A c2
A c2
</figure>
<figureCaption confidence="0.986232">
Figure 9
</figureCaption>
<subsectionHeader confidence="0.520613">
The operation of adjoining.
</subsectionHeader>
<bodyText confidence="0.999509833333333">
have the same constraints as those associated with the corresponding nodes in 0. The
remaining nodes in 7&apos; have the constraints of the corresponding nodes in -y
Given p, E Domain (7), by LABEL(-y, p) we refer to the label of the node addressed
p in -y . If the tree in question is clear from context, we will simply use LABEL(p).
Similarly, we will use SA(-y, p) (or SA(p)) and OA(-y, p) (or 0A(p)) to refer to the SA
and OA constraints of a node addressed p in a tree 7. Finally, we will use ft (0) to
refer to the address of the foot node of an auxiliary tree i3.
To be precise, we define the adjunction of 0 at a node in -y with address it as
follows. This operation is defined when 0 is included in the SA constraints of node
addressed p in 7. If the operation is defined, we will use ADJ (-y, jt, 0) to refer to the tree
that results. Let 7&apos; ADJ (-y, 0). Then the nodes in -y&apos; and their labels and adjoining
constraints are defined as follows.
</bodyText>
<listItem confidence="0.949728785714286">
• Domain (7&apos;) = E Domain (y),IL1 /11, for some tt2 E JV-*1 U
{it &apos;Ai E Domain (0)} U fp • ft (0)• I/•j E Domain (7) , and €}
• When pi E Domain (7) such that pi • Ai for some pi E ./V*1, i.e., the
node in 7 with address pi is not equal to or dominated by the node
addressed in -y:
— LABEL(7&apos;, pi) = LABEL (-y ,
— SA(-yi, pi) = SA(-y, pi),
— 0A(7i, pi) = 0A(-y,
• when p •p E Domain (71 such that pi E Domain (0):
— LABEL (7&apos; , p, • pi) = LABEL (0, ),
— SA(7&apos;, p • = SA(/3, pi),
— OA(Y, • Ai) = OA(0 /11),
• when it • ft (0) • E Domain (-y&apos;) such that it •ILi E Domain (7) and pi E:
— LABEL (7&apos;, it ft (0) • pi) = LABEL (7, tt •
</listItem>
<page confidence="0.988953">
622
</page>
<figure confidence="0.9442938">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
a S {131,132} Pi S [32 S
a b S {131,132}
S {131432}
S a SI)
</figure>
<figureCaption confidence="0.915999">
Figure 10
</figureCaption>
<bodyText confidence="0.9289988">
Example of a TAG G.
— sA (1/ • ft (0) iti) = SA(7,11
— 0A(7&apos;, ft (0) • Pi) = 0A(7, • Ili),
In general, if /2 is the address of a node in 7 then by (7, /,/,) we refer to the elementary
node address of the node that contributes to its presence, and hence its label and
constraints.
The tree language, T(G), generated by a TAG, G, is the set of trees derived starting
from an initial tree such that no node in the resulting tree has an OA constraint. The
(string) language, L(G), generated by a TAG, G, is the set of strings that appear on the
frontier of trees in T(G).
</bodyText>
<subsectionHeader confidence="0.825239">
Example 5.1
</subsectionHeader>
<bodyText confidence="0.8066168">
Figure 10 gives a TAG, G, which generates the language {wcw I W E {a, b}±}. The
constraints associated with the root and foot of 0 specify that no auxiliary trees can
be adjoined at these nodes. This is indicated in Figure 10 by associating the empty set,
cb, with these nodes. An example derivation of the strings aca and abcab is shown in
Figure 11.
</bodyText>
<subsectionHeader confidence="0.997654">
5.3 TAG and LIG
</subsectionHeader>
<bodyText confidence="0.986750375">
In this section, we examine bottom-up recognition of a TAG. In doing so, we construct
a LIG that simulates the derivations of the TAG. Based on this construction, we derive
a recognizer for TAG from the algorithms given earlier.
Consider bottom-up TAG recognition. Having recognized the substring dominated
by an elementary node there are two possible actions: (1) move up the tree by combin-
ing this node with its siblings; or (2) consider adjunction at that node. In bottom-up
recognition, the second action (i.e., adjunction) must be considered before the first.
Therefore, there are two phases involved in the consideration of each node. On enter-
ing the bottom phase of a node, having just combined the derivations of its children,
we predict an adjunction. On entering the top phase, having just finished adjunction
at that node, we must now combine with any siblings in order to move up the tree.
Note that in the bottom phase we may also predict that there is no adjunction at the
node (if there is no OA constraint on that node) and hence move to its top phase
directly.
Figure 12 shows why, because of the nature of the adjoining operation, TAG can be
seen to involve stacking. Suppose, during recognition, the bottom phase of a node, 77,
</bodyText>
<page confidence="0.991689">
623
</page>
<figure confidence="0.9964209">
Computational Linguistics
Volume 19, Number 4
yl S Y2 S
s
a
a S {f31432}
a
S 0314321 S {131,132}
S4 b
S (I) a
</figure>
<figureCaption confidence="0.979525666666667">
Figure 11
Sample derivations in G.
Figure 12
</figureCaption>
<bodyText confidence="0.970846769230769">
Stacking in a TAG.
has been reached. When adjunction by the auxiliary tree 13 is predicted, control shifts
to the bottom phase of /3&apos;s foot node. As we move up the spine of 13 it is necessary to
remember that /3 was adjoined at n. On reaching the top phase of /3&apos;s root we must
return to (the top phase of) 77. Therefore, the adjunction point, i, must be propagated
up the spine of 0. In general, we may need to propagate a stack of adjunction points
as we move up the spine as shown in Figure 12 where 72 is obtained by adjoining )31
at a node 711 on the spine of 13. From this figure, it can be seen that the information
about the adjunction points (that must be propagated along the spine of an auxiliary
tree) follows the stack (last-in first-out) discipline. Notice also that only the nodes on
the spine participate in the propagation of adjunction points.
Consider how a LIG that simulates this process can be constructed. The details of
the equivalence between LIG and TAG can be found in Vijay-Shanker (1987). In the
</bodyText>
<figure confidence="0.645788">
a
</figure>
<page confidence="0.993583">
624
</page>
<note confidence="0.912099">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.999633363636364">
LIG, we use two nonterminals, t and b to capture the differences between the top and
bottom phases associated with a node. The stack holds an appropriate sequence of
adjunction points in the form of elementary node addresses. The top of the stack is
the elementary node address of the node that is currently being visited (thus all objects
have at least one element on the stack). Nodes that are not on the spine, or belong to
an initial tree, do not participate in the propagation of adjunction points. Therefore, the
objects for such nodes will have stacks that contain only their elementary node address.
The set of LIG productions is determined as follows. We assume that internal
elementary nodes have either a single child labeled by a terminal symbol (or E), or
exactly two children labeled by nonterminals. In this discussion below, we will use 77
for a node and its elementary node address interchangeably.
</bodyText>
<listItem confidence="0.8015316">
1. If 77 is a node that is labeled E where E e VT U{€} then we will include
2. If 77p and ns are the children of a node n such that the left sibling 77p (and
hence 7)) is on the spine then the following holds: (1) the object
corresponding to 77p can have an unboundedly large stack, whereas the
object for 77, will have a stack of size one; (2) the top of the stack in these
</listItem>
<bodyText confidence="0.955447666666667">
objects will be 77p and m; (3) combination of these two sibling nodes is
possible only after the top parts of these nodes are reached; (4) the stack
in the object for 77p must be propagated to object for 77„ except that the
top symbol ij,, is replaced by n; (5) when the two sibling nodes are
combined we reach the bottom part of 77. Hence, we include the
production b t 71p) t(7)).
</bodyText>
<listItem confidence="0.8848765">
3. If 77p, m are children of 77 as in the previous case except that 77p is the
right sibling and is on the spine, then we include the production
b (-• 77) t (77s) t (•• 77p).
4. If rip, 77, and n are as before except that neither sibling is on the spine of
</listItem>
<bodyText confidence="0.759383">
an auxiliary tree then we include the production b (•• 77) t (•• 77p) t (m).
</bodyText>
<listItem confidence="0.99291775">
5. If 77p is the only child of?) we have b(.. t (-- 71p) .
6. If n is a node where 0 can be adjoined and we are at the bottom of 77,
then, by predicting adjunction by 0, control moves to the bottom part of
777 (the foot node of 13). This is illustrated in Figure 13. In this case we
</listItem>
<bodyText confidence="0.719253">
add the production b (•• 77771) b (•• 77). When there is no OA constraint at
77 then we can predict that no adjunction takes place. This is captured
with the production t (• • b (--
7. Suppose we have reached the top part of the root node, 772, of the
auxiliary tree 0. The corresponding object has the nonterminal t with 772
on top of the stack and the node at which 13 was adjoined is immediately
below 772. Having reached the top of the root node of 0 we must return
to the top of the node where was adjoined. This is accomplished with
the production t (• • ) t (• • 772) (see Figure 13).
</bodyText>
<footnote confidence="0.4598252">
Figure 13 captures the essence of the connection between TAG and LIG—in par-
ticular the way the adjoining operation in TAG can be simulated in LIG. This figure
is also useful in order to understand the notion of terminators. As in the case of CCG,
the construction of the LIG equivalent of the given grammar is unnecessary. However,
as in the case of CCG, this discussion of the connection between TAG and LIG can be
</footnote>
<page confidence="0.981238">
625
</page>
<figure confidence="0.9960935">
Computational Linguistics Volume 19, Number 4
A
l3
&amp;quot;1
</figure>
<figureCaption confidence="0.78976">
Figure 13
TAG/LIG relationship.
</figureCaption>
<bodyText confidence="0.874832">
used to motivate the choices we make in the form of entries in TAG parser as well as
the rules in the algorithm.
</bodyText>
<subsectionHeader confidence="0.999434">
5.4 Recognition of TAG
</subsectionHeader>
<bodyText confidence="0.958679444444445">
We now give a CKY-style recognition algorithm for TAG. But first we shall consider
the LIG constructed from a given TAG as described in Section 5.3. Given this LIG
grammar, consider the objects derived and the form of entries that will be used by the
LIG algorithm.
• If n is an elementary node address of a node on the spine of an auxiliary
tree, say 0, then any object that has 71 as the top symbol of its stack must
be of the form A Oh . . . nom) where k&gt; 0, A E {t, b}, and nt is the
elementary node address of a node where 0 can be adjoined.
Furthermore, in any derivation, the terminator of A (yontn) will be b
</bodyText>
<listItem confidence="0.821527">
• For this LIG, MSL = MTL -= TTC = 1 and MCL =2. Hence it follows that
any terminator is a 2-terminator. From the discussion above, an object
A (n) (where A E {t, b} and len (p) &gt; 0) has a terminator if and only if 7/
is an elementary node address of a node on the spine of an auxiliary tree.
• Consider the forms of entries for a LIG in this form. First, the length of
the middle in a terminator-type entry will be one always, since any
terminator is a 2-terminator. Note that the terminator of A (contn) will be
b (cono. Thus, a terminator type entry in a parsing array entry, say P [i , d]
will have the form ((A, n) (Tit, ((b, 77,) , [t, dt] ))) where A E {t, b} and
(t, dt) &lt; (i, d). Note that (b, nt) in the terminator-pointer is redundant.
• From the discussion above, a non—terminator-type entry will be used to
record derivations from A (ij) where A E { t, b} and 7/ is the elementary
node address of a node that belongs to an initial tree or of a node that is
</listItem>
<page confidence="0.997114">
626
</page>
<note confidence="0.901165">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.9871517">
not on the spine of an auxiliary tree. To record this object the entry
((A, 77) ,nil) would have been used.
From the above discussion it makes sense that terminator-type entries in the TAG
parser have the form ((A, n) (nt, f, dt)) where A E { t, b}, n is an elementary node
address of a node on the spine of an auxiliary tree, say /3, and m is the elementary
node address of a node where /3 can be adjoined in. A non—terminator-type entry has
the form ((A, n) ,nil) where A E { t, b}, and 77 is an elementary node address of a node
that is not on the spine of an auxiliary tree.
Finally, consider an auxiliary derivation in the LIG obtained from a TAG as de-
scribed in Section 5.3. Recall that an auxiliary derivation has the form
</bodyText>
<equation confidence="0.861591">
A (yo-Yi) 1 GP7172) T2
TiUAt (({)&apos;7t) wT2
</equation>
<bodyText confidence="0.96367">
In this case we would have:
</bodyText>
<listItem confidence="0.9948196">
• =
• A = = t,
• At b, and
• 72 is the root of an auxiliary tree that can be adjoined at the node whose
elementary node address is given by rit.
</listItem>
<bodyText confidence="0.974896">
Since every auxiliary tree in a lexicalized TAG has at least one terminal node in its
frontier, every auxiliary derivation spans at least one terminal in the LIG we have
constructed.
</bodyText>
<subsectionHeader confidence="0.996921">
5.5 Recognition Algorithm
</subsectionHeader>
<bodyText confidence="0.715736727272727">
We begin with a description of the cases involved in TAG recognition algorithm.
• Predicting adjunction: During the recognition phase, on reaching the
bottom part of a node 77, we predict adjunction by each auxiliary tree,
that can be adjoined at n as determined by its SA constraints. As given
in Case 6 of the construction in Section 5.3, this prediction is captured
with the LIG production b (.• nm) b (.• y) where m is the foot node of
the auxiliary tree, 0. Depending on whether n is on the spine of an
auxiliary tree or not, we have the following counterparts of Rule 8.u.L
and Rule 10.u.L:
Rule 8.u.T
= (0, ft (0)) SA(Ij) ((b, t, di)) c P [i,
</bodyText>
<equation confidence="0.85798525">
((boil) (7 , cl)) E P[i,
Rule 10.u.T
= (0, ft (0)) c SA(u) ((b, 7/) ,nil) e P [i,
((b oh) (71, i, cl)) E P[i,d]
</equation>
<page confidence="0.988849">
627
</page>
<note confidence="0.625629">
Computational Linguistics Volume 19, Number 4
</note>
<bodyText confidence="0.988103333333333">
As in the second part of Case 6 of the LIG construction (i.e., when there
is no OA constraint at the node n) we have the following counterparts of
Rule 6.u.L and Rule 7.u.L:
</bodyText>
<equation confidence="0.906784166666667">
Rule 6.i.u.T
0A(n) -=false ((b,Y) (Tit, t,di)) E P [i , el]
((t, ri) (nt, t , dt)) E P [i , cl]
Rule 7.i.u.T
0A(n) = false ((I cl, 71) ,nil) E P [i, cl]
((t, n) ,nil) E P [i , cl]
</equation>
<listItem confidence="0.876469">
• Left sibling on the spine: This corresponds to Case 2 of the LIG
construction. The following rule that captures this situation corresponds
to Rule 7.ps.L.
</listItem>
<equation confidence="0.6934492">
Rule 7.ps.T
rip is left child of n
Tip is on the spine of an auxiliary tree np is right child of
((t, 7/p) (nt,t,di)) E P [i, dp] ((t, 77,) ,nil) E P [i + dp, d — dp]
((b , 77) (77t, t, dt)) E P [i, cd
</equation>
<bodyText confidence="0.889963">
The following covers Case 4 of LIG construction where the two siblings
are not on the spine or belong to an initial tree and corresponds to
Rule 6.ps.L (or Rule 6.sp.L).
Rule 6.ps.T
rip is left child of n
77 is not on the spine of any auxiliary tree Tip is right child of ti
</bodyText>
<equation confidence="0.7371095">
((t, Tip) ,nil) E P [, dp] ((t, ns) ,nil) E P [i + dp, d — dp]
((),n) ,nil) E P [i, di
</equation>
<listItem confidence="0.9946045">
• Right sibling on the spine: Corresponding to Case 3 of LIG construction
and Rule 7.sp.L we have
</listItem>
<sectionHeader confidence="0.315779" genericHeader="method">
Rule 7.sp.T
</sectionHeader>
<bodyText confidence="0.843752">
71p is right child of ii
is left child of ti 77p is on the spine of an auxiliary tree
</bodyText>
<equation confidence="0.427909">
( (t, Th) , nil) E P [i, cis] ((t, i) (Tit, t, d)) E P [i + cis, d — ds]
((I), in (Tit, t, di)) E P [i, ell
</equation>
<listItem confidence="0.9069565">
• Single child case: Corresponding to Case 5 of LIG construction,
Rule 7.u.L and Rule 6.u.L.
</listItem>
<footnote confidence="0.657512">
Rule 7.ii.u.T ((t, rip) (W, t, dr)) E P [i, di
rip is only child of n
77,, is on the spine of some auxiliary tree
((b , in (nt, t, dt)) E P [i, d]
</footnote>
<page confidence="0.988747">
628
</page>
<note confidence="0.82845">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.5297455">
Rule 6.ii.u.T
rip is only child of 77
n is not on the spine of any auxiliary tree ((t, Tip), nil) E P [i , cl]
((b, ,nil) E P [i ,
</bodyText>
<listItem confidence="0.863113666666667">
• Completing an adjunction: Corresponding to Case 7 of the construction
and depending on whether the node of adjunction is on the spine of an
auxiliary tree, we have the following counterparts of Rule 4.u.L,
</listItem>
<equation confidence="0.844887555555556">
Rule 5.u.L.
Rule 4.u.T
?it is not on the spine of any auxiliary tree
(Kt, np) t,t, di)) E P [i, el] ((b, jt) ,nil) c P [t, cid
(t, &apos;i) ,nil) E P [i , cl]
Rule 5.u.T
Yr is on the spine of an auxiliary tree
((to]p) (nt, t,di)) c P [i, ((b, ii) (77r, r, dr)) E P [t dr]
((t, 71t) (71r, r, dr)) E P
</equation>
<bodyText confidence="0.999874666666667">
From the nature of entries being created it will follow that if tip -= (0, E),
for some auxiliary tree 0, then 3 is adjoinable at tit. Similarly, if
-= (f3&apos;, it) for some auxiliary tree 0&apos;, then /3&apos; is adjoinable at nr.
</bodyText>
<listItem confidence="0.981488333333333">
• Scanning a terminal symbol: If n is a node labeled by a terminal
matching the ith input symbol, a,, then we have (corresponding to
Rule 1.L):
</listItem>
<equation confidence="0.968659333333333">
Rule 1.T
LABEL(q) 1&lt;i&lt;n
((t, n) ,nil) E PU, 11
</equation>
<listItem confidence="0.936923">
• Scanning empty string: If n is a node labeled by E, then we have
(corresponding to Rule 1.E.L):
</listItem>
<equation confidence="0.955003666666667">
Rule 1.E.T
LABEL(n) = E
((t, 77) ,nil) E 01
</equation>
<bodyText confidence="0.997713">
This concludes our discussion of the parsing rules for TAG. With the correspon-
dences with the LIG parsing rules given (via the numbering of rules), these rules may
be placed in the control structure as suggested in Section 5.1. As noted earlier, in the
case of a lexicalized TAG, since every auxiliary derivation spans at least one terminal
we do not require the repeat loop discussed in Section 5.1.
</bodyText>
<page confidence="0.99565">
629
</page>
<note confidence="0.685745">
Computational Linguistics Volume 19, Number 4
</note>
<sectionHeader confidence="0.940588" genericHeader="method">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.99981228">
In this paper we have presented a general scheme for parsing a set of grammar for-
malisms whose derivation process is controlled by (explicit or implicit) stacking ma-
chinery. We have shown how this scheme can be instantiated to give polynomial
time algorithms for LIG, CCG, and TAG. In the case of CCG, this provides the only
polynomial parsing algorithm (apart from a slight variant of this scheme given in
Vijay-Shanker and Weir (1990)) we are aware of.
The main contribution of this paper is the general recognition scheme and defi-
nitions of some notions (e.g., terminators, data structures sharing of stacks) crucial to
this scheme. We believe that these ideas can be suitably adapted in order to produce
parsing schemes based on other CFG parsing algorithms (such as Earley&apos;s algorithm).
For instance, the definition of terminator given here was tailored for pure bottom-up
parsing. In the case of Earley&apos;s algorithm, a bottom-up parser with top-down predic-
tion, an additional notion of terminator for the top-down prediction component can
be obtained in a straightforward manner.
We have also introduced a new method of representing derivations in a TAG, one
that we believe is appropriate in capturing the stacking that occurs during a TAG
derivation. The derivations themselves represented can be in another TAG that we
call the derivation grammar (see Vijay-Shanker and Weir (1993)).
We have not discussed the extraction of parses after the recognition is complete
because of space considerations. However, an algorithm to extract the parses and build
a shared forest representation of all parses for CCG was proposed in Vijay-Shanker
and Weir (1990). This scheme was based on the approach we have taken in our general
scheme. The method of extracting parses and representing them using a shared forest
given in Vijay-Shanker and Weir (1990) can be generalized in a straightforward manner
to be compatible with the generalized recognition scheme given here.
</bodyText>
<sectionHeader confidence="0.985624" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994292428571429">
This work has been partially supported by
NSF Grants IRI-8909810 and IRI-9016591.
We would like to thank A. K. Joshi, B. Lang,
Y. Schabes, S. M. Shieber, and
M. J. Steedman for many discussions. We
are grateful to the anonymous reviewers for
their numerous suggestions.
</bodyText>
<sectionHeader confidence="0.984872" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.987208209302326">
Aho, A. V. (1968). &amp;quot;Indexed grammars—An
extension to context free grammars.&amp;quot;
J. ACM, 15,647-671.
Duske, J., and Parchmann, R. (1984). &amp;quot;Linear
indexed languages.&amp;quot; Theoretical Cornput.
Sci., 32,47-60.
Gazdar, G. (1988). &amp;quot;Applicability of indexed
grammars to natural languages.&amp;quot; In
Natural Language Parsing and Linguistic
Theories, edited by U. Reyle and
C. Rohrer. D. Reidel, 69-94.
Joshi, A. K. (1985). &amp;quot;How much
context-sensitivity is necessary for
characterizing structural
descriptions—tree adjoining grammars.&amp;quot;
In Natural Language Processing—Theoretical,
Computational and Psychological Perspective,
edited by D. Dowty, L. Karttunen, and
A. Zwicky. Cambridge University Press,
206-250.
Joshi, A. K.; Levy L. S.; and Takahashi, M.
(1975). &amp;quot;Tree adjunct grammars.&amp;quot; J.
Comput. Syst. Sci., 10(1), 136-163.
Kasami, T. (1965). &amp;quot;An efficient recognition
and syntax algorithm for context-free
languages.&amp;quot; Technical Report
AF-CRL-65-758, Air Force Cambridge
Research Laboratory, Bedford, MA.
Lang, B. (1990). &amp;quot;Towards a uniform formal
framework for parsing.&amp;quot; In Current Issues
in Parsing Technology, edited by M. Tomita.
Kluwer Academic Publishers, 153-171.
Pareschi, R., and Steedman, M. J. (1987). &amp;quot;A
lazy way to chart-parse with categorial
grammars.&amp;quot; In Proceedings, 25th Meeting of
the Association for Computational Linguistics,
81-88.
Pollard, C. (1984). Generalized Phrase
Structure Grammars, Head Grammars and
Natural Language. Doctoral dissertation,
Stanford University.
Steedman, M. (1986). &amp;quot;Combinators and
grammars.&amp;quot; In Categorial Grammars and
</reference>
<page confidence="0.992543">
630
</page>
<note confidence="0.858786">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<reference confidence="0.994159340425532">
Natural Language Structures, edited by
R. Oehrle, E. Bach, and D. Wheeler. Foris,
417-442.
Steedman, M. J. (1985). &amp;quot;Dependency and
coordination in the grammar of Dutch and
English.&amp;quot; Language, 61:523-568.
Tomita, M. (1988). &amp;quot;Graph-structured stack
and natural language parsing.&amp;quot; In
Proceedings, 26th Meeting of the Association
for Computational Linguistics, 248-257.
Vijay-Shanker, K. (1987). A study of tree
adjoining grammars. Doctoral dissertation,
University of Pennsylvania, Philadelphia,
PA.
Vijay-Shanker, K., and Joshi, A. K. (1985).
&amp;quot;Some computational properties of tree
adjoining grammars.&amp;quot; In Proceedings, 23rd
Meeting of the Association for Computational
Linguistics, 82-93.
Vijay-Shanker, K., and Weir, D. J. (In press).
&amp;quot;The equivalence of four extensions of
context-free grammars.&amp;quot; Mathematical
Systems Theory.
Vijay-Shanker, K., and Weir, D. J. (1990).
&amp;quot;Polynomial parsing of combinatory
categorial grammars.&amp;quot; In Proceedings, 28th
Meeting of the Association for Computational
Linguistics, Pittsburgh, PA, 1-8.
Vijay-Shanker, K., and Weir, D. J. (1993).
&amp;quot;The use of shared forests in TAG
parsing.&amp;quot; In Proceedings, 6th Meeting of the
European Association for Computational
Linguistics, Utrecht, The Netherlands,
384-393.
Weir, D. J. (1988). Characterizing mildly
context-sensitive grammar formalisms.
Doctoral dissertation, University of
Pennsylvania, Philadelphia, PA.
Weir, D. J., and Joshi, A. K. (1988).
&amp;quot;Combinatory categorial grammars:
Generative power and relationship to
linear context-free rewriting systems.&amp;quot; In
Proceedings, 26th Meeting of the Association
for Computational Linguistics, 278-285.
Younger, D. H. (1967). &amp;quot;Recognition and
parsing of context-free languages in time
n3.&amp;quot; Inf. Control, 10(2), 189-208.
</reference>
<sectionHeader confidence="0.760952" genericHeader="method">
Appendix A: Correctness of Algorithm 1
</sectionHeader>
<bodyText confidence="0.977571">
We will now prove the correctness of Algorithm 1. In doing so, we will start by
observing some properties of the rules and the control structure used.
Firstly, given an input is al . an, we can note that every entry added by a rule
(i.e., consequents of rules) satisfies the requirements for the terminator-type and non—
terminator-type entries; viz., if ((A, -y) (13, ((Ar, -Yr) [t, cid))) is added to an array ele-
ment P [i, d] then
</bodyText>
<listItem confidence="0.959162555555556">
• A, At E VN,
• y,7tEVi,
• ,3 E VP where 1 &lt; len (3) &lt; MCL — 1 and
• (t, dt) &lt; (i, d) &lt; (1, n) where d &gt; 2.
We can also note that if ((A, 7) (i3, nil)) is added to P [i, d] then
• A E VN,
• -y E
• /3E V7 where 0 &lt; len (13) &lt; TTC + MCL — 1 and
• d &gt; 1.
</listItem>
<bodyText confidence="0.98282725">
These can be verified from noting the form of the rules and by simple induction on
(i, d). We can also observe from the control structure given that entries to P [i1, d1] [l2, d2]
are added before entries are added to P [i3, d3] d4] if and only if (ii, d1) &lt; (i3, d3) or
(ii,di)= (i3, d3) and (i2, d2) &gt; (i4, d4). This observation can be used to show that when
</bodyText>
<page confidence="0.985985">
631
</page>
<note confidence="0.366313">
Computational Linguistics Volume 19, Number 4
</note>
<bodyText confidence="0.968219">
a rule is considered for the purposes of adding an entry to P[il, d1] [i2, d2] then the
array elements specified in the antecedent of that rule would have already been filled.
Verifying these properties of the algorithm enables us to establish the correctness of
the algorithm more easily.
</bodyText>
<figure confidence="0.97725775">
Theorem A.1
•
((A, &apos;Y) (a, ((At,), [t, dt] ))) E P [i,
if and only if
A (13a-y) *&gt; a, . . at -iAt (070 at • • • a 1H-a--1
* &gt; a,. a-4-d_1
for some such that At (0-yt) is the len (a7)-terminator of A (13 cry) in this
derivation and len (3-yt) &gt; TTC.
•
((A, -y) (a, nil)) E P [i,
if and only if
A (a-y) * &gt; a, . a,+d_i
</figure>
<bodyText confidence="0.956552333333333">
where A (cry) does not have the TC-property, i.e., A (cry) has no
terminator in this derivation or the terminator, say At (0-Yt), is such that
len (13-n) &lt; TTC.
</bodyText>
<subsectionHeader confidence="0.943987">
Proof of Soundness:
</subsectionHeader>
<bodyText confidence="0.847658111111111">
We prove the soundness by inducting on d. The base case corresponds to d 1. We
have to consider only entries of the form ((A, -y) (a, nil)) in PU, 11 Such entries are
added only by the application of Rule 1. Therefore, we have A (cry) -&gt; a and a =- a,.
Hence A (cry) *&gt; a, as required.
Now, for the inductive step, let d &gt; 2. Any entry ((A, 7) (a, tp)) added to P [i,
where d &gt; 2 must be due to a rule other than Rule 1.L. This means that we have either
a production A (-• 7m) Ap (.. 7p) As (as) or A (• • 71 • • • 7m) -› As (as) Ap (• • 713). Let
us assume that the first production was used. We will discuss the cases for m = 0,
m = 1, and m &gt; 2 separately.
</bodyText>
<listItem confidence="0.7771885">
• Let m = 0. In this case the production is A (.• ) -&gt; (-• 7p) As (as). Then
the entry ((A, •7) (a, tp)) should have been added by using one of rules
1.ps.L through 5.ps.L. We take Rule 4.ps.L as a representative. If
((A, 71) Oh nil)) were to be added as a result of this rule, then we have
</listItem>
<bodyText confidence="0.90642875">
to show that A (13t7i) *&gt; ai • • ai+d_i where A (0t71) does not meet the
TC-property. Since (i, dp) &lt; (i, d), (i dp, d - d5) &lt; (i, d), and (t, dt) &lt; (i, d)
the inductive hypothesis applies to the three entries in the antecedent.
Thus, we have for some a the following derivations:
</bodyText>
<equation confidence="0.516462">
At (Oat) * at . . •
As (as) *&gt; ai±dp • • • ai+d-i
Ap (a717p) *&gt; az at-iAt (a7t) at+d, • • ai±c1,-1
at.. • ai±dp —1
</equation>
<page confidence="0.991076">
632
</page>
<note confidence="0.735084">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.9990365">
such that At (Bat) does not meet the TC-property. However, Ap (awyp)
satisfies the TC-property and furthermore At (aryt) is the 2-terminator of
Ap (a-n-yp). From Observation 2.1, we can also infer the existence of the
following derivation.
</bodyText>
<equation confidence="0.609596">
Ap (OrtrYp) &gt; ai . • at_iAt (t7t) at+di • • • at+dp —1
ai • • • ai±dp -1
</equation>
<bodyText confidence="0.7832842">
Combining this derivation with the derivation from A, (as) we have
A (0&apos;i) .&gt; Ap (i3t71yp) As (as)
at . • . at71At (Oat) at+d, • • • ai+dp—lai+dp • • •
U. . at+d_i
From Observation 2.5, we know that the terminator of At (Oat) in this
derivation is also the terminator of A (OrYi) (and if At (/3at) has no
terminator then neither does A (0t-yi)). Since At (Nyt) does not satisfy the
TC-property (i.e., it does not have a terminator with stack length greater
than or equal to TTC), A (Oryi) does not satisfy the TC-property either.
Thus we have shown the existence of the required derivation.
</bodyText>
<listItem confidence="0.596525">
• Let m 1. Therefore the production may be written as
</listItem>
<bodyText confidence="0.905330875">
A (- • Ap (. • lip) As (as). This time we will take Rule 6.ps.L as a
representative. Hence, we can assume that the entry added to P [i, di has
the form ((A, 71) (3p, nil)). Since (i, dp) &lt; d), and
(i + dp, d — ds) &lt; (i, d), the inductive hypothesis applies to the two entries
in the antecedent. Thus, we have the following derivations:
AP (0P7P) at . . . at±dp_i
As (as) at+dp . . . Cli+d_i
Therefore we have the derivation:
</bodyText>
<figure confidence="0.493723666666667">
A (/3p71) Ap (Op-yp) As (as)
&gt; ai a i+dp —101i-Fdp • • • at-Fd —1
a1. ai+d —1
</figure>
<bodyText confidence="0.987495">
Note that any terminator of Ap (3p-y1,) is also the terminator of A (001)
(and if Ap (13p7p) has no terminator then neither has A (Op-yi)). Since
Ap (3p-yp) does not meet the TC-property in this derivation (from
inductive hypothesis), neither does A (i3p-yp). Thus we have shown the
existence of the required derivation.
</bodyText>
<listItem confidence="0.879581">
• Let m &gt; 2. We will consider the application of Rule 10.ps.L as a
representative. Again, applying the inductive hypothesis we have the
following derivations:
</listItem>
<equation confidence="0.553872">
A (3 1
P v P-Y19 * a1.
</equation>
<bodyText confidence="0.789534">
As (as) *&gt; ai+dp • • • ai+d-i
</bodyText>
<page confidence="0.97504">
633
</page>
<figure confidence="0.544271666666667">
Computational Linguistics Volume 19, Number 4
where len (Op-yp) &gt; TTC. Combining the two derivations, we have:
A (1313&amp;quot;Yi • • • &apos;Ym) Ap (i3plip) As (as)
&gt; ai ai+dp—laid-cl, • • • aid-d-1
= a. .
Since in &gt; 2, Ap (Op-yp) is the m-terminator of A (Op-yi 7,7„) in the above
</figure>
<figureCaption confidence="0.366994">
derivation. Since len (N-yp) &gt; TTC, we have shown the existence of the
</figureCaption>
<bodyText confidence="0.979729">
required derivation and that A (,(3p-yi NO satisfies the TC-property.
In a similar manner we can consider other rules (including those that assume a pro-
duction of the form A(. • As (as) Ap (- -yp)) as well.
</bodyText>
<subsectionHeader confidence="0.936311">
Proof of Completeness:
</subsectionHeader>
<bodyText confidence="0.956977083333333">
We will now show the completeness of Algorithm 1. This time we use induction on
the number of steps in a derivation. Suppose A (,3) --1-=&gt;a1.. ao_d_i; we have to show
that there is a corresponding entry (as specified in Theorem A.1) in P [i, d].
The base case corresponds to 1--= 1. From the form of the productions being
considered we can assume that d = 1 and that there exists a production A (a) a,.
Rule 1 would apply and thus we have the required entry.
Let A (13) 1+1&gt; a,. ai+d_i where / &gt; 1. The first production used in this derivation
must have the form A (.• 71 • • • Yrn) Ap (• • -yp) As (as) or A (.• &apos;Yl • • . 7m) As (as) Ap (• • 7p).
We will only assume that the production is A (• • -y„,) Ap (• • -yp) As (a5). Argu-
ments similar to the one given below can be used when the production of the form
A (• • 71 . • • 7m) As (as) Ap (• • -yp) is involved as the first step of the derivation.
Case in = 0: We begin by considering the case when in = 0. Since the first production
</bodyText>
<figure confidence="0.842474666666667">
used in A (0) *&gt; a1. • • al+d-1 is A (• • ) Ap (.- 7p) As (as), we can write the derivation
as
A (0) 1 &gt; Ap (07p) A, (as)
&gt; a. . a,+dp -.Os (as)
&gt; a, . • ai±dr_iaid_dp • • • d-i
for some I &lt; dp &lt;d and Ip + Is 1. Applying the inductive hypothesis to the derivation
A, (as) &gt; a,+dp . . . ai±d_i, we can assume the existence of the entry
((As, top (as)) (rest (as), nil))
in P [i + dp, d - dp] .
</figure>
<bodyText confidence="0.7287865">
In order to show the existence of the appropriate type of entry corresponding to
the derivation of a,. ai±d_i from A (0), we need to consider whether A (f3) satisfies the
TC-property in this derivation. This could depend on whether the primary constituent
/p
Ap (i3p-yp) does. Since the inductive hypothesis applies for the derivation Ap (07p)
a,... Cli±dp_i. Let us start by assuming that A (0) satisfies the TC-property. This means
that it has a (say) (k+1)-terminator whose stack length is greater than or equal to TTC.
Expressing /3 as Orr&apos; -yk, we can then rewrite the derivation from A (3) as follows.
</bodyText>
<equation confidence="0.403827333333333">
A (A71 • • -Yk) Ap (A71 • • • -YrYp) As (as)
a1. . at _iAt (13t7t)at±dt • • • ai+cl,--lai+d, • • • ai+d-1
a1. at_iat . • • at+dt-iat+d, • . • a1+d-1
</equation>
<page confidence="0.994715">
634
</page>
<note confidence="0.73478">
K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms
</note>
<bodyText confidence="0.988670333333333">
where At (Oat) is the terminator of Ap (13t7i • .. 71(7p) . Thus, len (13at) &gt; TTC and k &gt; 1.
Now, At (Oat) is the terminator of A (01-71 • • • 7k) if and only if k &gt; 1 (from Observa-
tion 2.5).
</bodyText>
<listItem confidence="0.554097">
• Let k&gt; I. At (0t7t) is the terminator of A (Ot&apos;Yi • • 7k) and len (i3t7t) &gt; TTC.
Thus, A (0t7i • • • 7k) satisfies the TC-property. Therefore we must show
</listItem>
<bodyText confidence="0.977574789473684">
that the entry ((A, 7k) (71.. • 7k-1, ((At, 7t), cif] ))) belongs to P [i, d]. By
inductive hypothesis we may assume
((Ar,-yr) (71. • • 7k, ((At, &apos;Yt) , [t, dt]))) belongs to P[i, dr]. Now all the
conditions in the antecedent of Rule 3.ps.L have been met and thus we
have shown the existence of the appropriate entry to record the
derivation of a,. a,+d_i from A (0).
• Let k = 1. From Observation 2.5 it follows that the k&apos;-terminator of
At (Oat) (if it exists) is also the k&apos;-terminator of A (@t71), and if At (Oat)
has no terminator then neither does A (13t71). Therefore A (0) = A (0t71)
satisfies the TC-property if and only if At (Oat) does. Suppose At (13t7t)
satisfies the TC-property; then all conditions stated in the antecedent of
Rule 5.ps.L are met and the appropriate entry is added to record the
derivation from A (0). On the other hand, if At (Oat) does not satisfy the
TC-property then all conditions stated in the antecedent of Rule 4.ps.L
are met and the appropriate entry is added to record the derivation
from A (0).
Casein =1: Here we are concerned with the situation where A (. • 71) -&gt; Ap (• 7p) As (as)
is the first production used in the derivation of a, . a,+d_i from A (13). Rewriting 0 as
/31371 we have
</bodyText>
<equation confidence="0.8814875">
A (0p71) Ap (i3p7p) As (as)
&gt; ai ai+dp—lai-Fdp • • -ai+d-1
</equation>
<bodyText confidence="0.96592875">
Applying the inductive hypothesis we have
((As, top (as)) (rest (as), nil)) E P[i + dp, d - dr].
Now any k-terminator of Ap (0p-yp) is also the k-terminator of A (0pyi) (and if Ap (0p7p)
has no terminator then neither does A (Opyi)). That is, A (Op-yi) satisfies the TC-
property in this derivation if and only if Ap (0p-yp) does. If Ap (0p-yp) does not satisfy
the TC-property, then, by inductive hypothesis, we have ((A1,, p) (Op, nil)) e P [i, d1,].
Thus the entries corresponding to the antecedents of Rule 6.ps.L exist and the algo-
rithm would have added the entry ((A, -n) (op, nil)) c P[i, d] as desired. If Ap (Op-yp)
does satisfy the TC-property then Rule 7.ps.L would add the required entry to record
the derivation from A (0).
Case in &gt; 2: Finally, consider that case when in &gt; 2. The given derivation may be
expressed as
A (0p71 7m) Ap (f3p7p) As (as)
ai . . . a ±d_i
Applying the inductive hypothesis we have
((As, top (as)) (rest (a,), nil)) E P[i + dp, d - d1,].
</bodyText>
<page confidence="0.988434">
635
</page>
<note confidence="0.383174">
Computational Linguistics Volume 19, Number 4
</note>
<bodyText confidence="0.998986214285714">
Since Ap (13117p) is the m-terminator of A (13p7i . . . -y), we have to consider its length
in order to know whether A (001 ryni) satisfies the TC-property, i.e., how it must be
represented. Suppose len (13p71,) &lt; TTC, then by inductive hypothesis we have the en-
try ((Ap, 7p) (Op, nil)) E P [i , dp] . Thus all antecedents of Rule 9.ps.L have been found.
Since the terminator of A (f3p7i -yni) has a stack of length less than TTC, the required
entry, ((A, 7,0 (015,71 -ym_i , nil)), is added by the algorithm by the application of
Rule 9.ps.L. Suppose len (131,7p) &gt; TTC, then Ap (i3p-yp) may or may not be represented
as a terminator-type entry. Let us take the case where Ap (/3op) does not satisfy the
TC-property. Again by inductive hypothesis, we have the entry ((Ar, 7p) (/p, nil)) E
P [1, d,,]. Since len WO &gt; TTC and the antecedents entries of Rule 10.ps.L exist, the al-
gorithm would add ((A, 7m) (-Yl • • • 7m-i, ((Ap, 7p) , [i, d,]))) to P [i, cl] as desired. If we
had assumed At, (,(31,7p) satisfies the TC-property, then by applying the inductive hy-
pothesis we can guarantee the existence of the entries corresponding to the antecedent
of Rule 8.ps.L, and therefore the algorithm would have added
</bodyText>
<equation confidence="0.724151">
((A, 7m) (71 • • • 7m-1, ((Ap, 7p) , [i, dp] )))
to P [i , cl] as desired.
</equation>
<page confidence="0.995061">
636
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.642364">
<title confidence="0.9996095">Parsing Some Constrained Grammar Formalisms</title>
<author confidence="0.999837">K Vijay-Shanker David J Weirt</author>
<affiliation confidence="0.986191">University of Delaware University of Sussex</affiliation>
<abstract confidence="0.954948181818182">In this paper we present a scheme to extend a recognition algorithm for Context-Free Grammars (CFG) that can be used to derive polynomial-time recognition algorithms for a set of formalisms that generate a superset of languages generated by CFG. We describe the scheme by developing a Cocke-Kasami-Younger (CKY)-like pure bottom-up recognition algorithm for Linear Indexed Grammars and show how it can be adapted to give algorithms for Tree Adjoining Grammars and Combinatory Categorial Grammars. This is the only polynomial-time recognition algorithm for Combinatory Categorial Grammars that we are aware of. The main contribution of this paper is the general scheme we propose for parsing a variety of formalisms whose derivation process is controlled by an explicit or implicit stack. The ideas presented here can be suitably modified for other parsing styles or used in the generalized framework set out by Lang (1990).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
</authors>
<title>Indexed grammars—An extension to context free grammars.&amp;quot;</title>
<date>1968</date>
<journal>J. ACM,</journal>
<pages>15--647</pages>
<contexts>
<context position="5734" citStr="Aho 1968" startWordPosition="925" endWordPosition="926">also possible, although not considered here. Since the use of the stacks is most explicit in the LIG formalism we describe our approach in detail by developing a recognition algorithm for LIG (Sections 2 and 3). We then show how the general approach suggested in the parser for LIG can be tailored to CCG (in Section 4). In the above discussion TAG has been grouped with HG. However, TAG can also be viewed as making use of stacks in the same way as LIG and CCG. In Section 5 we show how the LIG algorithm presented in Section 3 can be adapted for TAG. 2. Linear Indexed Grammars An Indexed Grammar (Aho 1968) can be viewed as a CFG in which objects are nonterminals with an associated stack of symbols. In addition to rewriting nonterminals, the rules of the grammar can have the effect of pushing or popping symbols on top of the stacks that are associated with each nonterminal. Gazdar (1988) discussed a restricted form of Indexed Grammars in which the stack associated with the nonterminal on the left of each production can only be associated with one of the occurrences of nonterminals on the right of the production. Stacks of bounded size are associated with other occurrences of nonterminals on the </context>
</contexts>
<marker>Aho, 1968</marker>
<rawString>Aho, A. V. (1968). &amp;quot;Indexed grammars—An extension to context free grammars.&amp;quot; J. ACM, 15,647-671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duske</author>
<author>R Parchmann</author>
</authors>
<title>Linear indexed languages.&amp;quot;</title>
<date>1984</date>
<journal>Theoretical Cornput. Sci.,</journal>
<pages>32--47</pages>
<contexts>
<context position="6655" citStr="Duske and Parchmann (1984)" startWordPosition="1090" endWordPosition="1093">d a restricted form of Indexed Grammars in which the stack associated with the nonterminal on the left of each production can only be associated with one of the occurrences of nonterminals on the right of the production. Stacks of bounded size are associated with other occurrences of nonterminals on the right of the production. We call this Linear Indexed Grammars (LIG).2 1 The path set of a tree is the set of strings labeling paths from the root to the frontier of the tree. The path set of a tree set is the union of path sets of trees in the set. 2 The name Linear Indexed Grammars is used by Duske and Parchmann (1984) to refer to a different restriction on Indexed Grammars in which production was restricted to have only a single nonterminal on their right-hand side. 592 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Definition 2.1 A LIG, G, is denoted by (VN, VT V1, S, P) where VN is a finite set of nonterminals, VT is a finite set of terminals, VI is a finite set of indices (stack symbols), S E VN is the start symbol, and P is a finite set of productions. We adopt the convention that a, 0 (with or without subscripts and primes) denote members of 1/7, and 7 denotes a stack s</context>
</contexts>
<marker>Duske, Parchmann, 1984</marker>
<rawString>Duske, J., and Parchmann, R. (1984). &amp;quot;Linear indexed languages.&amp;quot; Theoretical Cornput. Sci., 32,47-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Applicability of indexed grammars to natural languages.&amp;quot; In Natural Language Parsing and Linguistic Theories, edited by U. Reyle</title>
<date>1988</date>
<pages>69--94</pages>
<contexts>
<context position="6020" citStr="Gazdar (1988)" startWordPosition="975" endWordPosition="976">can be tailored to CCG (in Section 4). In the above discussion TAG has been grouped with HG. However, TAG can also be viewed as making use of stacks in the same way as LIG and CCG. In Section 5 we show how the LIG algorithm presented in Section 3 can be adapted for TAG. 2. Linear Indexed Grammars An Indexed Grammar (Aho 1968) can be viewed as a CFG in which objects are nonterminals with an associated stack of symbols. In addition to rewriting nonterminals, the rules of the grammar can have the effect of pushing or popping symbols on top of the stacks that are associated with each nonterminal. Gazdar (1988) discussed a restricted form of Indexed Grammars in which the stack associated with the nonterminal on the left of each production can only be associated with one of the occurrences of nonterminals on the right of the production. Stacks of bounded size are associated with other occurrences of nonterminals on the right of the production. We call this Linear Indexed Grammars (LIG).2 1 The path set of a tree is the set of strings labeling paths from the root to the frontier of the tree. The path set of a tree set is the union of path sets of trees in the set. 2 The name Linear Indexed Grammars is</context>
</contexts>
<marker>Gazdar, 1988</marker>
<rawString>Gazdar, G. (1988). &amp;quot;Applicability of indexed grammars to natural languages.&amp;quot; In Natural Language Parsing and Linguistic Theories, edited by U. Reyle and C. Rohrer. D. Reidel, 69-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>How much context-sensitivity is necessary for characterizing structural descriptions—tree adjoining grammars.&amp;quot;</title>
<date>1985</date>
<booktitle>In Natural Language Processing—Theoretical, Computational and Psychological Perspective,</booktitle>
<pages>206--250</pages>
<publisher>Cambridge University Press,</publisher>
<note>edited by</note>
<contexts>
<context position="1611" citStr="Joshi (1985)" startWordPosition="244" endWordPosition="245">generalized framework set out by Lang (1990). 1. Introduction This paper presents a scheme to extend known recognition algorithms for Context-Free Grammars (CFG) in order to obtain recognition algorithms for a class of grammatical formalisms that generate a strict superset of the set of languages generated by CFG. In particular, we use this scheme to give recognition algorithms for Linear Indexed Grammars (LIG), Tree Adjoining Grammars (TAG), and a version of Combinatory Categorial Grammars (CCG). These formalisms belong to the class of mildly contextsensitive grammar formalisms identified by Joshi (1985) on the basis of some properties of their generative capacity. The parsing strategy that we propose can be applied to the formalisms listed as well as others that have similar characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Joshi, A. K. (1985). &amp;quot;How much context-sensitivity is necessary for characterizing structural descriptions—tree adjoining grammars.&amp;quot; In Natural Language Processing—Theoretical, Computational and Psychological Perspective, edited by D. Dowty, L. Karttunen, and A. Zwicky. Cambridge University Press, 206-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.&amp;quot;</title>
<date>1975</date>
<journal>J. Comput. Syst. Sci.,</journal>
<volume>10</volume>
<issue>1</issue>
<pages>136--163</pages>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, A. K.; Levy L. S.; and Takahashi, M. (1975). &amp;quot;Tree adjunct grammars.&amp;quot; J. Comput. Syst. Sci., 10(1), 136-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax algorithm for context-free languages.&amp;quot;</title>
<date>1965</date>
<tech>Technical Report AF-CRL-65-758,</tech>
<institution>Air Force Cambridge Research Laboratory,</institution>
<location>Bedford, MA.</location>
<contexts>
<context position="2398" citStr="Kasami 1965" startWordPosition="369" endWordPosition="370">characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of &amp;quot;states&amp;quot; the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness property, is important because it allows one to keep only a limited amount of context during the recognition process, * Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716. E-mail: vijay@udel.edu. t School of Cognitive and Computing Sciences, University of Sussex, Brighton BN1 9QH,</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Kasami, T. (1965). &amp;quot;An efficient recognition and syntax algorithm for context-free languages.&amp;quot; Technical Report AF-CRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Towards a uniform formal framework for parsing.&amp;quot; In Current Issues in Parsing Technology,</title>
<date>1990</date>
<pages>153--171</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<note>edited by</note>
<contexts>
<context position="1043" citStr="Lang (1990)" startWordPosition="160" endWordPosition="161">-like pure bottom-up recognition algorithm for Linear Indexed Grammars and show how it can be adapted to give algorithms for Tree Adjoining Grammars and Combinatory Categorial Grammars. This is the only polynomial-time recognition algorithm for Combinatory Categorial Grammars that we are aware of. The main contribution of this paper is the general scheme we propose for parsing a variety of formalisms whose derivation process is controlled by an explicit or implicit stack. The ideas presented here can be suitably modified for other parsing styles or used in the generalized framework set out by Lang (1990). 1. Introduction This paper presents a scheme to extend known recognition algorithms for Context-Free Grammars (CFG) in order to obtain recognition algorithms for a class of grammatical formalisms that generate a strict superset of the set of languages generated by CFG. In particular, we use this scheme to give recognition algorithms for Linear Indexed Grammars (LIG), Tree Adjoining Grammars (TAG), and a version of Combinatory Categorial Grammars (CCG). These formalisms belong to the class of mildly contextsensitive grammar formalisms identified by Joshi (1985) on the basis of some properties</context>
</contexts>
<marker>Lang, 1990</marker>
<rawString>Lang, B. (1990). &amp;quot;Towards a uniform formal framework for parsing.&amp;quot; In Current Issues in Parsing Technology, edited by M. Tomita. Kluwer Academic Publishers, 153-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pareschi</author>
<author>M J Steedman</author>
</authors>
<title>A lazy way to chart-parse with categorial grammars.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, 25th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="14667" citStr="Pareschi and Steedman 1987" startWordPosition="2644" endWordPosition="2647">o the entry for the derivation from At (Mit), whereas the entry for C&apos; (1371 • • 7k-1 -11 a&apos;) will point back to the entry for A (071 • • • N-17k) • We shall now formalize these notions by defining a terminator. 3 For instance, consider the grammar in Example 2.1 and the derivation in Figure 1. In general we can have derivations of the form T (7.-1) * cab&amp;quot;. However, if there exists productions of the form A (a) —+ e then the length of the stack in objects is not even bounded by the length of strings they derive. 4 The CCG parsing algorithms that have been proposed so far follow this strategy (Pareschi and Steedman 1987; Tomita 1988). 595 Computational Linguistics Volume 19, Number 4 Figure 2 Recovering the rest of stack-1. Figure 3 Recovering the rest of stack-2. Figure 4 Definition of a Terminator. B(P71-1.-1 Za,N1 (f3-q Aarr-Lp (----------- yzN B(1371-1-4, Atom As(a) / \ 596 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Definition 2.4 Suppose that we have the derivation tree in Figure 4 that depicts the following derivation: A (0-yi • • .7k-17) *&gt;. uB (071 7k-17k • • • 7m) w uAt 0370 As (as) w &gt; uvw or similarly: A (071 ••• 7k-17) *&gt; uB (1371 • .7k-i7t • • • -Ym) w uAs (as</context>
</contexts>
<marker>Pareschi, Steedman, 1987</marker>
<rawString>Pareschi, R., and Steedman, M. J. (1987). &amp;quot;A lazy way to chart-parse with categorial grammars.&amp;quot; In Proceedings, 25th Meeting of the Association for Computational Linguistics, 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
</authors>
<title>Generalized Phrase Structure Grammars, Head Grammars and Natural Language. Doctoral dissertation,</title>
<date>1984</date>
<institution>Stanford University.</institution>
<contexts>
<context position="2316" citStr="Pollard 1984" startWordPosition="355" endWordPosition="356">propose can be applied to the formalisms listed as well as others that have similar characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of &amp;quot;states&amp;quot; the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness property, is important because it allows one to keep only a limited amount of context during the recognition process, * Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716. E-mail: vijay@udel.edu. t S</context>
</contexts>
<marker>Pollard, 1984</marker>
<rawString>Pollard, C. (1984). Generalized Phrase Structure Grammars, Head Grammars and Natural Language. Doctoral dissertation, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Combinators and grammars.&amp;quot; In Categorial Grammars and Natural Language Structures, edited by</title>
<date>1986</date>
<pages>417--442</pages>
<marker>Steedman, 1986</marker>
<rawString>Steedman, M. (1986). &amp;quot;Combinators and grammars.&amp;quot; In Categorial Grammars and Natural Language Structures, edited by R. Oehrle, E. Bach, and D. Wheeler. Foris, 417-442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Steedman</author>
</authors>
<title>Dependency and coordination in the grammar of Dutch and English.&amp;quot; Language,</title>
<date>1985</date>
<pages>61--523</pages>
<contexts>
<context position="48147" citStr="Steedman 1985" startWordPosition="9334" endWordPosition="9335">i , d] due to the application of Rule 5.ps.L if and only if there exist entries of the form ((A,7) (71, Mt) 7t) [t, dt] ))) in P[i, dp]; ((A,, top (a,)) (rest (a,), nil)) in P [i dp, d — dp]; ((Ar, 7r) ((Ar, 7r) , [r, dr] ))) in P [t, dt]; and the production A (. • ) —&gt; Ap ( • &apos;Tp) As (as). Using induction, we can establish that these entries exist if and only if (A, 71, At, 7t) is added to TEMP [t, dt] using Rule 5.ps.i.L (or Rule 5.sp.i.L) and ((A,71) (Or, ((A, -Yr) , [r, dr]))) is added to P [i, using Rule 5.ii.ps.L. 4. Combinatory Categorial Grammars Combinatory Categorial Grammars (CCG) (Steedman 1985, 1986) are extensions of Classical Categorial Grammars in which both function composition and function application are allowed. In addition, forward and backward slashes are used to place conditions concerning the relative ordering of adjacent categories that are to be combined. Definition 4.1 The set of categories generated from a set, VN, of atomic categories is defined as the smallest set such that all members of VN are categories, and if c1, c2 are categories then so are (ci /c2) and (ci \ c2). 609 Computational Linguistics Volume 19, Number 4 Algorithm 2 begin for i:= 1 to n do Initializ</context>
<context position="52400" citStr="Steedman (1985)" startWordPosition="10197" endWordPosition="10198">G = ({a, b, , {S, T, A, B} , S, f , R) where f (a) = {A, T\A/T, T\A} f (b) = {B, T\B/T, T\B} f (c) = IS /T} The set of rules R includes the following three rules. y (x\y) x (x/y) (y\zi/z2) (y\zi/z2) (x/y) (y\zi) (y\zi) In each of these rules, the target of the category matched with x must be S.&apos; Figure 7 shows a derivation of the string abbcabb. We find it convenient to represent categories in a minimally parenthesized form (i.e., without parentheses unless they are needed to override the left associativity of the slashes), where minimally parenthesized form is defined as follows. 7 Following Steedman (1985), we allow certain very limited restrictions on the substitutions of variables in the combinatory rules. A discussion on the use of such restrictions is given in Vijay-Shanker and Weir (in press). However, we have not included this in the formal definition since it does not have a significant impact on the algorithm presented. 611 Computational Linguistics Volume 19, Number 4 S S \AT a SkATT SNAT/T S \AIT sir Tr Figure 7 CCG example derivation tree. Definition 4.4 • A is the minimally parenthesized form of A where A E VN. • If , cn are the minimally parenthesized forms of categories el, , cic </context>
</contexts>
<marker>Steedman, 1985</marker>
<rawString>Steedman, M. J. (1985). &amp;quot;Dependency and coordination in the grammar of Dutch and English.&amp;quot; Language, 61:523-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Graph-structured stack and natural language parsing.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 26th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>248--257</pages>
<contexts>
<context position="14681" citStr="Tomita 1988" startWordPosition="2648" endWordPosition="2649">on from At (Mit), whereas the entry for C&apos; (1371 • • 7k-1 -11 a&apos;) will point back to the entry for A (071 • • • N-17k) • We shall now formalize these notions by defining a terminator. 3 For instance, consider the grammar in Example 2.1 and the derivation in Figure 1. In general we can have derivations of the form T (7.-1) * cab&amp;quot;. However, if there exists productions of the form A (a) —+ e then the length of the stack in objects is not even bounded by the length of strings they derive. 4 The CCG parsing algorithms that have been proposed so far follow this strategy (Pareschi and Steedman 1987; Tomita 1988). 595 Computational Linguistics Volume 19, Number 4 Figure 2 Recovering the rest of stack-1. Figure 3 Recovering the rest of stack-2. Figure 4 Definition of a Terminator. B(P71-1.-1 Za,N1 (f3-q Aarr-Lp (----------- yzN B(1371-1-4, Atom As(a) / \ 596 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms Definition 2.4 Suppose that we have the derivation tree in Figure 4 that depicts the following derivation: A (0-yi • • .7k-17) *&gt;. uB (071 7k-17k • • • 7m) w uAt 0370 As (as) w &gt; uvw or similarly: A (071 ••• 7k-17) *&gt; uB (1371 • .7k-i7t • • • -Ym) w uAs (as) At (0-Yt)w w</context>
</contexts>
<marker>Tomita, 1988</marker>
<rawString>Tomita, M. (1988). &amp;quot;Graph-structured stack and natural language parsing.&amp;quot; In Proceedings, 26th Meeting of the Association for Computational Linguistics, 248-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
</authors>
<title>A study of tree adjoining grammars. Doctoral dissertation,</title>
<date>1987</date>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2081" citStr="Vijay-Shanker 1987" startWordPosition="319" endWordPosition="320">Combinatory Categorial Grammars (CCG). These formalisms belong to the class of mildly contextsensitive grammar formalisms identified by Joshi (1985) on the basis of some properties of their generative capacity. The parsing strategy that we propose can be applied to the formalisms listed as well as others that have similar characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of &amp;quot;states&amp;quot; the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness pro</context>
<context position="82591" citStr="Vijay-Shanker (1987)" startWordPosition="16135" endWordPosition="16136"> we may need to propagate a stack of adjunction points as we move up the spine as shown in Figure 12 where 72 is obtained by adjoining )31 at a node 711 on the spine of 13. From this figure, it can be seen that the information about the adjunction points (that must be propagated along the spine of an auxiliary tree) follows the stack (last-in first-out) discipline. Notice also that only the nodes on the spine participate in the propagation of adjunction points. Consider how a LIG that simulates this process can be constructed. The details of the equivalence between LIG and TAG can be found in Vijay-Shanker (1987). In the a 624 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms LIG, we use two nonterminals, t and b to capture the differences between the top and bottom phases associated with a node. The stack holds an appropriate sequence of adjunction points in the form of elementary node addresses. The top of the stack is the elementary node address of the node that is currently being visited (thus all objects have at least one element on the stack). Nodes that are not on the spine, or belong to an initial tree, do not participate in the propagation of adjunction points. Th</context>
</contexts>
<marker>Vijay-Shanker, 1987</marker>
<rawString>Vijay-Shanker, K. (1987). A study of tree adjoining grammars. Doctoral dissertation, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>A K Joshi</author>
</authors>
<title>Some computational properties of tree adjoining grammars.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>82--93</pages>
<contexts>
<context position="2355" citStr="Vijay-Shanker and Joshi 1985" startWordPosition="359" endWordPosition="362">to the formalisms listed as well as others that have similar characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of &amp;quot;states&amp;quot; the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness property, is important because it allows one to keep only a limited amount of context during the recognition process, * Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716. E-mail: vijay@udel.edu. t School of Cognitive and Computing Scienc</context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1985</marker>
<rawString>Vijay-Shanker, K., and Joshi, A. K. (1985). &amp;quot;Some computational properties of tree adjoining grammars.&amp;quot; In Proceedings, 23rd Meeting of the Association for Computational Linguistics, 82-93.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The equivalence of four extensions of context-free grammars.&amp;quot; Mathematical Systems Theory.</title>
<marker>Vijay-Shanker, Weir, </marker>
<rawString>Vijay-Shanker, K., and Weir, D. J. (In press). &amp;quot;The equivalence of four extensions of context-free grammars.&amp;quot; Mathematical Systems Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>Polynomial parsing of combinatory categorial grammars.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="93636" citStr="Vijay-Shanker and Weir (1990)" startWordPosition="18315" endWordPosition="18318">ed TAG, since every auxiliary derivation spans at least one terminal we do not require the repeat loop discussed in Section 5.1. 629 Computational Linguistics Volume 19, Number 4 6. Conclusion In this paper we have presented a general scheme for parsing a set of grammar formalisms whose derivation process is controlled by (explicit or implicit) stacking machinery. We have shown how this scheme can be instantiated to give polynomial time algorithms for LIG, CCG, and TAG. In the case of CCG, this provides the only polynomial parsing algorithm (apart from a slight variant of this scheme given in Vijay-Shanker and Weir (1990)) we are aware of. The main contribution of this paper is the general recognition scheme and definitions of some notions (e.g., terminators, data structures sharing of stacks) crucial to this scheme. We believe that these ideas can be suitably adapted in order to produce parsing schemes based on other CFG parsing algorithms (such as Earley&apos;s algorithm). For instance, the definition of terminator given here was tailored for pure bottom-up parsing. In the case of Earley&apos;s algorithm, a bottom-up parser with top-down prediction, an additional notion of terminator for the top-down prediction compon</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1990</marker>
<rawString>Vijay-Shanker, K., and Weir, D. J. (1990). &amp;quot;Polynomial parsing of combinatory categorial grammars.&amp;quot; In Proceedings, 28th Meeting of the Association for Computational Linguistics, Pittsburgh, PA, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The use of shared forests in TAG parsing.&amp;quot;</title>
<date>1993</date>
<booktitle>In Proceedings, 6th Meeting of the European Association for Computational Linguistics,</booktitle>
<pages>384--393</pages>
<location>Utrecht, The</location>
<contexts>
<context position="2122" citStr="Vijay-Shanker and Weir 1993" startWordPosition="323" endWordPosition="326"> (CCG). These formalisms belong to the class of mildly contextsensitive grammar formalisms identified by Joshi (1985) on the basis of some properties of their generative capacity. The parsing strategy that we propose can be applied to the formalisms listed as well as others that have similar characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of &amp;quot;states&amp;quot; the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness property, is important because it allows one</context>
<context position="94589" citStr="Vijay-Shanker and Weir (1993)" startWordPosition="18465" endWordPosition="18468">s (such as Earley&apos;s algorithm). For instance, the definition of terminator given here was tailored for pure bottom-up parsing. In the case of Earley&apos;s algorithm, a bottom-up parser with top-down prediction, an additional notion of terminator for the top-down prediction component can be obtained in a straightforward manner. We have also introduced a new method of representing derivations in a TAG, one that we believe is appropriate in capturing the stacking that occurs during a TAG derivation. The derivations themselves represented can be in another TAG that we call the derivation grammar (see Vijay-Shanker and Weir (1993)). We have not discussed the extraction of parses after the recognition is complete because of space considerations. However, an algorithm to extract the parses and build a shared forest representation of all parses for CCG was proposed in Vijay-Shanker and Weir (1990). This scheme was based on the approach we have taken in our general scheme. The method of extracting parses and representing them using a shared forest given in Vijay-Shanker and Weir (1990) can be generalized in a straightforward manner to be compatible with the generalized recognition scheme given here. Acknowledgments This wo</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1993</marker>
<rawString>Vijay-Shanker, K., and Weir, D. J. (1993). &amp;quot;The use of shared forests in TAG parsing.&amp;quot; In Proceedings, 6th Meeting of the European Association for Computational Linguistics, Utrecht, The Netherlands, 384-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Weir</author>
</authors>
<title>Characterizing mildly context-sensitive grammar formalisms. Doctoral dissertation,</title>
<date>1988</date>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2092" citStr="Weir 1988" startWordPosition="321" endWordPosition="322">al Grammars (CCG). These formalisms belong to the class of mildly contextsensitive grammar formalisms identified by Joshi (1985) on the basis of some properties of their generative capacity. The parsing strategy that we propose can be applied to the formalisms listed as well as others that have similar characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of &amp;quot;states&amp;quot; the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness property, is i</context>
<context position="53922" citStr="Weir (1988)" startWordPosition="10482" endWordPosition="10483"> cn are minimally parenthesized categories. • The target category of c = A11c112 • • • Incn denoted by tar (c) is A. • The arity of c = A11c112 • • • Incn, denoted as arity (c), is n. • The argument categories of c = Alicil2 • • • Incn denoted by args(c)= {c1 II &lt; i &lt; n}. 4.1 CCG and LIG Before showing how the general parsing scheme illustrated by the LIG recognition algorithm can be instantiated as a recognition algorithm for CCG, we show that CCG and LIG are very closely related. The details of the examination of the relationship between CCG and LIG may be found in Weir and Joshi (1988) and Weir (1988). A minimally parenthesized category (Alici12 • • • Incn) can be viewed as the atomic category, A, associated with a stack of directional argument categories, lic112 • • • Incn• The rule (x/Y) (Y11z112 • • • Iniztn) —&gt; (xlizil2 • • • Iniztn) has as an instance (Apricii ...rncni As) (A511c112 • • • Imcm) (Apricii • -In&apos; • • • imcm) as well as (Apric&apos;i • • • l&apos;nen/(AslY)) (Asi&apos;clicii2 • • • Imc,n) (AprIel linc&apos;nlici 12 • • • .c.) 612 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms as an instance. Thus x matches the category (Ap ... c), y matches an atomic category </context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>Weir, D. J. (1988). Characterizing mildly context-sensitive grammar formalisms. Doctoral dissertation, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Combinatory categorial grammars: Generative power and relationship to linear context-free rewriting systems.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 26th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>278--285</pages>
<contexts>
<context position="53906" citStr="Weir and Joshi (1988)" startWordPosition="10477" endWordPosition="10480">t n &gt; 0, A e VN, and ci, , cn are minimally parenthesized categories. • The target category of c = A11c112 • • • Incn denoted by tar (c) is A. • The arity of c = A11c112 • • • Incn, denoted as arity (c), is n. • The argument categories of c = Alicil2 • • • Incn denoted by args(c)= {c1 II &lt; i &lt; n}. 4.1 CCG and LIG Before showing how the general parsing scheme illustrated by the LIG recognition algorithm can be instantiated as a recognition algorithm for CCG, we show that CCG and LIG are very closely related. The details of the examination of the relationship between CCG and LIG may be found in Weir and Joshi (1988) and Weir (1988). A minimally parenthesized category (Alici12 • • • Incn) can be viewed as the atomic category, A, associated with a stack of directional argument categories, lic112 • • • Incn• The rule (x/Y) (Y11z112 • • • Iniztn) —&gt; (xlizil2 • • • Iniztn) has as an instance (Apricii ...rncni As) (A511c112 • • • Imcm) (Apricii • -In&apos; • • • imcm) as well as (Apric&apos;i • • • l&apos;nen/(AslY)) (Asi&apos;clicii2 • • • Imc,n) (AprIel linc&apos;nlici 12 • • • .c.) 612 K. Vijay-Shanker and David J. Weir Parsing Some Constrained Grammar Formalisms as an instance. Thus x matches the category (Ap ... c), y matches an </context>
</contexts>
<marker>Weir, Joshi, 1988</marker>
<rawString>Weir, D. J., and Joshi, A. K. (1988). &amp;quot;Combinatory categorial grammars: Generative power and relationship to linear context-free rewriting systems.&amp;quot; In Proceedings, 26th Meeting of the Association for Computational Linguistics, 278-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.&amp;quot;</title>
<date>1967</date>
<journal>Inf. Control,</journal>
<volume>10</volume>
<issue>2</issue>
<pages>189--208</pages>
<contexts>
<context position="2413" citStr="Younger 1967" startWordPosition="371" endWordPosition="372">cs (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of &amp;quot;states&amp;quot; the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness property, is important because it allows one to keep only a limited amount of context during the recognition process, * Department of Computer and Information Sciences, University of Delaware, Newark, DE 19716. E-mail: vijay@udel.edu. t School of Cognitive and Computing Sciences, University of Sussex, Brighton BN1 9QH, U.K. E-mail: d</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Younger, D. H. (1967). &amp;quot;Recognition and parsing of context-free languages in time n3.&amp;quot; Inf. Control, 10(2), 189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>