<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000246">
<title confidence="0.996857">
Improving Domain-Specific Word Alignment for Computer Assisted
Translation
</title>
<author confidence="0.995755">
WU Hua, WANG Haifeng
</author>
<affiliation confidence="0.987848">
Toshiba (China) Research and Development Center
</affiliation>
<address confidence="0.992535333333333">
5/F., Tower W2, Oriental Plaza
No.1, East Chang An Ave., Dong Cheng District
Beijing, China, 100738
</address>
<email confidence="0.997008">
{wuhua, wanghaifeng}@rdc.toshiba.com.cn
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999735625">
This paper proposes an approach to improve
word alignment in a specific domain, in which
only a small-scale domain-specific corpus is
available, by adapting the word alignment
information in the general domain to the
specific domain. This approach first trains two
statistical word alignment models with the
large-scale corpus in the general domain and the
small-scale corpus in the specific domain
respectively, and then improves the
domain-specific word alignment with these two
models. Experimental results show a significant
improvement in terms of both alignment
precision and recall. And the alignment results
are applied in a computer assisted translation
system to improve human translation efficiency.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962544117648">
Bilingual word alignment is first introduced as an
intermediate result in statistical machine translation
(SMT) (Brown et al., 1993). In previous alignment
methods, some researchers modeled the alignments
with different statistical models (Wu, 1997; Och and
Ney, 2000; Cherry and Lin, 2003). Some researchers
use similarity and association measures to build
alignment links (Ahrenberg et al., 1998; Tufis and
Barbu, 2002). However, All of these methods
require a large-scale bilingual corpus for training.
When the large-scale bilingual corpus is not
available, some researchers use existing dictionaries
to improve word alignment (Ker and Chang, 1997).
However, few works address the problem of
domain-specific word alignment when neither the
large-scale domain-specific bilingual corpus nor the
domain-specific translation dictionary is available.
This paper addresses the problem of word
alignment in a specific domain, where only a small
domain-specific corpus is available. In the
domain-specific corpus, there are two kinds of
words. Some are general words, which are also
frequently used in the general domain. Others are
domain-specific words, which only occur in the
specific domain. In general, it is not quite hard to
obtain a large-scale general bilingual corpus while
the available domain-specific bilingual corpus is
usually quite small. Thus, we use the bilingual
corpus in the general domain to improve word
alignments for general words and the corpus in the
specific domain for domain-specific words. In other
words, we will adapt the word alignment
information in the general domain to the specific
domain.
In this paper, we perform word alignment
adaptation from the general domain to a specific
domain (in this study, a user manual for a medical
system) with four steps. (1) We train a word
alignment model using the large-scale bilingual
corpus in the general domain; (2) We train another
word alignment model using the small-scale
bilingual corpus in the specific domain; (3) We build
two translation dictionaries according to the
alignment results in (1) and (2) respectively; (4) For
each sentence pair in the specific domain, we use the
two models to get different word alignment results
and improve the results according to the translation
dictionaries. Experimental results show that our
method improves domain-specific word alignment in
terms of both precision and recall, achieving a
21.96% relative error rate reduction.
The acquired alignment results are used in a
generalized translation memory system (GTMS, a
kind of computer assisted translation systems)
(Simard and Langlais, 2001). This kind of system
facilitates the re-use of existing translation pairs to
translate documents. When translating a new
sentence, the system tries to provide the
pre-translated examples matched with the input and
recommends a translation to the human translator,
and then the translator edits the suggestion to get a
final translation. The conventional TMS can only
recommend translation examples on the sentential
level while GTMS can work on both sentential and
sub-sentential levels by using word alignment results.
These GTMS are usually employed to translate
various documents such as user manuals, computer
operation guides, and mechanical operation manuals.
</bodyText>
<sectionHeader confidence="0.907512" genericHeader="method">
2 Word Alignment Adaptation
</sectionHeader>
<subsectionHeader confidence="0.990657">
2.1 Bi-directional Word Alignment
</subsectionHeader>
<bodyText confidence="0.9987808">
In statistical translation models (Brown et al., 1993),
only one-to-one and more-to-one word alignment
links can be found. Thus, some multi-word units
cannot be correctly aligned. In order to deal with this
problem, we perform translation in two directions
(English to Chinese, and Chinese to English) as
described in (Och and Ney, 2000). The GIZA++
toolkit 1 is used to perform statistical word
alignment.
For the general domain, we use and
</bodyText>
<equation confidence="0.623947">
SG1 SG2
</equation>
<bodyText confidence="0.999105">
to represent the alignment sets obtained with English
as the source language and Chinese as the target
language or vice versa. For alignment links in both
sets, we use i for English words and j for Chinese
words.
</bodyText>
<equation confidence="0.998381">
SG = A j j A j = a j a j &gt;_
1 {( , )  |{ }, 0
}
SG = i Ai A i = ai a i &gt;_
2 {( , )  |{ }, 0}
</equation>
<bodyText confidence="0.859104333333333">
Where, ak (k = i, j) is the position of the source
word aligned to the target word in position k. The set
Ak(k=i, j) indicates the words aligned to the same
source word k. For example, if a Chinese word in
position j is connect to an English word in position i,
then a j = i . And if a Chinese word in position j is
connect to English words in position i and k, then
Aj = {i, k} .
Based on the above two alignment sets, we
obtain their intersection set, union set 2 and
subtraction set.
Intersection: SG = SG1 n SG2
Union: PG = SG1 u SG2
Subtraction: MG = PG − SG
For the specific domain, we use and
</bodyText>
<equation confidence="0.706719">
SF1 SF2
</equation>
<bodyText confidence="0.99992375">
to represent the word alignment sets in the two
directions. The symbols SF , PF and MF
represents the intersection set, union set and the
subtraction set, respectively.
</bodyText>
<subsectionHeader confidence="0.999334">
2.2 Translation Dictionary Acquisition
</subsectionHeader>
<bodyText confidence="0.999994857142857">
When we train the statistical word alignment model
with a large-scale bilingual corpus in the general
domain, we can get two word alignment results for
the training data. By taking the intersection of the
two word alignment results, we build a new
alignment set. The alignment links in this
intersection set are extended by iteratively adding
</bodyText>
<footnote confidence="0.875116">
1 It is located at http://www.isi.edu/~och/GIZA++.html
</footnote>
<bodyText confidence="0.972444103448276">
2 In this paper, the union operation does not remove the
replicated elements. For example, if set one includes two
elements {1, 2} and set two includes two elements {1, 3}, then
the union of these two sets becomes {1, 1, 2, 3}.
word alignment links into it as described in (Och and
Ney, 2000).
Based on the extended alignment links, we build
an English to Chinese translation dictionary D1
with translation probabilities. In order to filter some
noise caused by the error alignment links, we only
retain those translation pairs whose translation
probabilities are above a threshold S1 or
co-occurring frequencies are above a threshold S2 .
When we train the IBM statistical word
alignment model with a limited bilingual corpus in
the specific domain, we build another translation
dictionary with the same method as for the
D2
dictionary D1. But we adopt a different filtering
strategy for the translation dictionary . We use
D2
log-likelihood ratio to estimate the association
strength of each translation pair because Dunning
(1993) proved that log-likelihood ratio performed
very well on small-scale data. Thus, we get the
translation dictionary by keeping those entries
D2
whose log-likelihood ratio scores are greater than a
threshold S 3.
</bodyText>
<subsectionHeader confidence="0.973762">
2.3 Word Alignment Adaptation Algorithm
</subsectionHeader>
<bodyText confidence="0.95666525">
Based on the bi-directional word alignment, we
define SI as SI = SG n SF and UG as
UG = PG u PF − SI. The word alignment links in
the set SI are very reliable. Thus, we directly
accept them as correct links and add them into the
final alignment set WA.
Input: Alignment set SIand UG
Output: Updated alignment set WA
</bodyText>
<figureCaption confidence="0.992505">
Figure 1. Word Alignment Adaptation Algorithm
</figureCaption>
<figure confidence="0.764268076923077">
(1) For alignment links in SI, we directly add
them into the final alignment set WA.
(2) For each English word i in the UG, we first
find its different alignment links, and then do
the following:
a) If there are alignment links found in
dictionary , add the link with the largest
D1
probability to WA.
b) Otherwise, if there are alignment links found
in dictionary , add the link with the
D2
largest log-likelihood ratio score to WA.
</figure>
<bodyText confidence="0.958968333333333">
c) If both a) and b) fail, but three links select the
same target words for the English word i, we
add this link into WA.
d) Otherwise, if there are two different links for
this word: one target is a single word, and
the other target is a multi-word unit and the
words in the multi-word unit have no link in
, add this multi-word alignment link to
.
</bodyText>
<equation confidence="0.7230275">
WA
WA
For each source word in the set , there are
UG
</equation>
<bodyText confidence="0.988549363636364">
two to four different alignment links. We first use
translation dictionaries to select one link among
them. We first examine the dictionary and then
D1
D2 to see whether there is at least an alignment link
of this word included in these two dictionaries. If it
is successful, we add the link with the largest
probability or the largest log-likelihood ratio score to
the final set WA. Otherwise, we use two heuristic
rules to select word alignment links. The detailed
algorithm is described in Figure 1.
</bodyText>
<figureCaption confidence="0.99954">
Figure 2. Alignment Example
</figureCaption>
<bodyText confidence="0.867319733333333">
Figure 2 shows an alignment result obtained with
the word alignment adaptation algorithm. For
example, for the English word “x-ray”, we have two
different links in UG . One is (x-ray, X) and the
other is (x-ray, X射线). And the single Chinese
words “射” and “线” have no alignment links in the
set WA. According to the rule d), we select the link
(x-ray, X 射线).
The Chinese sentences in both the training set
and the testing set are automatically segmented into
words. In order to exclude the effect of the
segmentation errors on our alignment results, we
correct the segmentation errors in our testing set.
The alignments in the testing set are manually
annotated, which includes 1,478 alignment links.
</bodyText>
<subsectionHeader confidence="0.999524">
3.2 Overall Performance
</subsectionHeader>
<bodyText confidence="0.999591333333333">
We use evaluation metrics similar to those in (Och
and Ney, 2000). However, we do not classify
alignment links into sure links and possible links.
We consider each alignment as a sure link. If we use
SG to represent the alignments identified by the
proposed methods and to denote the reference
</bodyText>
<subsubsectionHeader confidence="0.331808">
SC
</subsubsectionHeader>
<bodyText confidence="0.998746">
alignments, the methods to calculate the precision,
recall, and f-measure are shown in Equation (1), (2)
and (3). According to the definition of the alignment
error rate (AER) in (Och and Ney, 2000), AER can
be calculated with Equation (4). Thus, the higher the
f-measure is, the lower the alignment error rate is.
Thus, we will only give precision, recall and AER
values in the experimental results.
</bodyText>
<equation confidence="0.529458625">
precision =
|S S  |(1)
G ∩ C
 |S |
G
3 Evaluation recall =  |SG ∩ SC
|
SC  |(2)
</equation>
<bodyText confidence="0.999980882352941">
We compare our method with three other methods.
The first method “Gen+Spec” directly combines the
corpus in the general domain and in the specific
domain as training data. The second method “Gen”
only uses the corpus in the general domain as
training data. The third method “Spec” only uses the
domain-specific corpus as training data. With these
training data, the three methods can get their own
translation dictionaries. However, each of them can
only get one translation dictionary. Thus, only one
of the two steps a) and b) in Figure 1 can be applied
to these methods. The difference between these three
methods and our method is that, for each word, our
method has four candidate alignment links while the
other three methods only has two candidate
alignment links. Thus, the steps c) and d) in Figure 1
should not be applied to these three methods.
</bodyText>
<subsectionHeader confidence="0.998883">
3.1 Training and Testing Data
</subsectionHeader>
<bodyText confidence="0.996782888888889">
We have a sentence aligned English-Chinese
bilingual corpus in the general domain, which
includes 320,000 bilingual sentence pairs, and a
sentence aligned English-Chinese bilingual corpus in
the specific domain (a medical system manual),
which includes 546 bilingual sentence pairs. From
this domain-specific corpus, we randomly select 180
pairs as testing data. The remained 366 pairs are
used as domain-specific training data.
</bodyText>
<table confidence="0.9995416">
Method Precision Recall AER
Ours 0.8363 0.7673 0.1997
Gen+Spec 0.8276 0.6758 0.2559
Gen 0.8668 0.6428 0.2618
Spec 0.8178 0.4769 0.3974
</table>
<tableCaption confidence="0.999889">
Table 1. Word Alignment Adaptation Results
</tableCaption>
<bodyText confidence="0.999826">
We get the alignment results shown in Table 1 by
setting the translation probability threshold to
</bodyText>
<equation confidence="0.779842">
δ1 = 0. 1 , the co-occurring frequency threshold to
δ2 = 5 and log-likelihood ratio score to δ3 = 50 .
</equation>
<bodyText confidence="0.999639">
From the results, it can be seen that our approach
performs the best among others, achieving much
higher recall and comparable precision. It also
achieves a 21.96% relative error rate reduction
compared to the method “Gen+Spec”. This indicates
that separately modeling the general words and
domain-specific words can effectively improve the
word alignment in a specific domain.
</bodyText>
<figure confidence="0.981511133333333">
=1− 1SG∩SC|=1
− fmeasure (4)
AER
2*
2*  |SG ∩ SC
=
fmeasure
(3)
|
|
SG|+ |SC
|
G|
S
+|SC|
</figure>
<sectionHeader confidence="0.683267" genericHeader="method">
4 Computer Assisted Translation System
</sectionHeader>
<bodyText confidence="0.998259791666667">
A direct application of the word alignment result to
the GTMS is to get translations for sub-sequences in
the input sentence using the pre-translated examples.
For each sentence, there are many sub-sequences.
GTMS tries to find translation examples that match
the longest sub-sequences so as to cover as much of
the input sentence as possible without overlapping.
Figure 3 shows a sentence translated on the
sub-sentential level. The three panels display the
input sentence, the example translations and the
translation suggestion provided by the system,
respectively. The input sentence is segmented to
three parts. For each part, the GTMS finds one
example to get a translation fragment according to
the word alignment result. By combining the three
translation fragments, the GTMS produces a correct
translation suggestion “������ CT 4940L.”
Without the word alignment information, the
conventional TMS cannot find translations for the
input sentence because there are no examples closely
matched with it. Thus, word alignment information
can improve the translation accuracy of the GTMS,
which in turn reduces editing time of the translators
and improves translation efficiency.
</bodyText>
<figureCaption confidence="0.995411">
Figure 3. A Snapshot of the Translation System
</figureCaption>
<sectionHeader confidence="0.997951" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999982166666667">
This paper proposes an approach to improve
domain-specific word alignment through alignment
adaptation. Our contribution is that our approach
improves domain-specific word alignment by
adapting word alignment information from the
general domain to the specific domain. Our
approach achieves it by training two alignment
models with a large-scale general bilingual corpus
and a small-scale domain-specific corpus. Moreover,
with the training data, two translation dictionaries
are built to select or modify the word alignment
links and further improve the alignment results.
Experimental results indicate that our approach
achieves a precision of 83.63% and a recall of
76.73% for word alignment on a user manual of a
medical system, resulting in a relative error rate
reduction of 21.96%. Furthermore, the alignment
results are applied to a computer assisted translation
system to improve translation efficiency.
Our future work includes two aspects. First, we
will seek other adaptation methods to further
improve the domain-specific word alignment results.
Second, we will use the alignment adaptation results
in other applications.
</bodyText>
<sectionHeader confidence="0.999149" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999778230769231">
Lars Ahrenberg, Magnus Merkel and Mikael
Andersson. 1998. A Simple Hybrid Aligner for
Generating Lexical Correspondences in Parallel
Tests. In Proc. of the 36th Annual Meeting of the
Association for Computational Linguistics and the
17th International Conference on Computational
Linguistics, pages 29-35.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2): 263-311.
Colin Cherry and Dekang Lin. 2003. A Probability
Model to Improve Word Alignment. In Proc. of
the 41st Annual Meeting of the Association for
Computational Linguistics, pages 88-95.
Ted Dunning. 1993. Accurate Methods for the
Statistics of Surprise and Coincidence.
Computational Linguistics, 19(1): 61-74.
Sue J. Ker, Jason S. Chang. 1997. A Class-based
Approach to Word Alignment. Computational
Linguistics, 23(2): 313-343.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proc. of the 38th
Annual Meeting of the Association for
Computational Linguistics, pages 440-447.
Michel Simard and Philippe Langlais. 2001.
Sub-sentential Exploitation of Translation
Memories. In Proc. of MT Summit VIII, pages
335-339.
Dan Tufis and Ana Maria Barbu. 2002. Lexical
Token Alignment: Experiments, Results and
Application. In Proc. of the Third International
Conference on Language Resources and
Evaluation, pages 458-465.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel
Corpora. Computational Linguistics, 23(3):
377-403.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880889">
<title confidence="0.998686">Improving Domain-Specific Word Alignment for Computer Assisted Translation</title>
<author confidence="0.971949">WU Hua</author>
<author confidence="0.971949">WANG Haifeng</author>
<affiliation confidence="0.965365">Toshiba (China) Research and Development Center</affiliation>
<address confidence="0.988737666666667">5/F., Tower W2, Oriental Plaza No.1, East Chang An Ave., Dong Cheng District Beijing, China, 100738</address>
<email confidence="0.964237">wuhua@rdc.toshiba.com.cn</email>
<email confidence="0.964237">wanghaifeng@rdc.toshiba.com.cn</email>
<abstract confidence="0.99960105882353">This paper proposes an approach to improve word alignment in a specific domain, in which only a small-scale domain-specific corpus is available, by adapting the word alignment information in the general domain to the specific domain. This approach first trains two statistical word alignment models with the large-scale corpus in the general domain and the small-scale corpus in the specific domain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lars Ahrenberg</author>
<author>Magnus Merkel</author>
<author>Mikael Andersson</author>
</authors>
<title>A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Tests.</title>
<date>1998</date>
<booktitle>In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>29--35</pages>
<contexts>
<context position="1416" citStr="Ahrenberg et al., 1998" startWordPosition="196" endWordPosition="199"> results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. This paper addresses the problem of word alignment in a specific domain, where only a small domain-specific corpus is available. In the domain-spec</context>
</contexts>
<marker>Ahrenberg, Merkel, Andersson, 1998</marker>
<rawString>Lars Ahrenberg, Magnus Merkel and Mikael Andersson. 1998. A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Tests. In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 29-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="1151" citStr="Brown et al., 1993" startWordPosition="158" endWordPosition="161">approach first trains two statistical word alignment models with the large-scale corpus in the general domain and the small-scale corpus in the specific domain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when </context>
<context position="4419" citStr="Brown et al., 1993" startWordPosition="648" endWordPosition="651">pre-translated examples matched with the input and recommends a translation to the human translator, and then the translator edits the suggestion to get a final translation. The conventional TMS can only recommend translation examples on the sentential level while GTMS can work on both sentential and sub-sentential levels by using word alignment results. These GTMS are usually employed to translate various documents such as user manuals, computer operation guides, and mechanical operation manuals. 2 Word Alignment Adaptation 2.1 Bi-directional Word Alignment In statistical translation models (Brown et al., 1993), only one-to-one and more-to-one word alignment links can be found. Thus, some multi-word units cannot be correctly aligned. In order to deal with this problem, we perform translation in two directions (English to Chinese, and Chinese to English) as described in (Och and Ney, 2000). The GIZA++ toolkit 1 is used to perform statistical word alignment. For the general domain, we use and SG1 SG2 to represent the alignment sets obtained with English as the source language and Chinese as the target language or vice versa. For alignment links in both sets, we use i for English words and j for Chines</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A Probability Model to Improve Word Alignment.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="1309" citStr="Cherry and Lin, 2003" startWordPosition="181" endWordPosition="184">ain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. This paper addresses the problem of word</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A Probability Model to Improve Word Alignment. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics, pages 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>61--74</pages>
<contexts>
<context position="7366" citStr="Dunning (1993)" startWordPosition="1184" endWordPosition="1185">s. In order to filter some noise caused by the error alignment links, we only retain those translation pairs whose translation probabilities are above a threshold S1 or co-occurring frequencies are above a threshold S2 . When we train the IBM statistical word alignment model with a limited bilingual corpus in the specific domain, we build another translation dictionary with the same method as for the D2 dictionary D1. But we adopt a different filtering strategy for the translation dictionary . We use D2 log-likelihood ratio to estimate the association strength of each translation pair because Dunning (1993) proved that log-likelihood ratio performed very well on small-scale data. Thus, we get the translation dictionary by keeping those entries D2 whose log-likelihood ratio scores are greater than a threshold S 3. 2.3 Word Alignment Adaptation Algorithm Based on the bi-directional word alignment, we define SI as SI = SG n SF and UG as UG = PG u PF − SI. The word alignment links in the set SI are very reliable. Thus, we directly accept them as correct links and add them into the final alignment set WA. Input: Alignment set SIand UG Output: Updated alignment set WA Figure 1. Word Alignment Adaptati</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1): 61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sue J Ker</author>
<author>Jason S Chang</author>
</authors>
<title>A Class-based Approach to Word Alignment.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<pages>313--343</pages>
<contexts>
<context position="1671" citStr="Ker and Chang, 1997" startWordPosition="233" endWordPosition="236">t introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. This paper addresses the problem of word alignment in a specific domain, where only a small domain-specific corpus is available. In the domain-specific corpus, there are two kinds of words. Some are general words, which are also frequently used in the general domain. Others are domain-specific words, which only occur in the specific domain. In general, it is not quite hard to obtain a large-scale ge</context>
</contexts>
<marker>Ker, Chang, 1997</marker>
<rawString>Sue J. Ker, Jason S. Chang. 1997. A Class-based Approach to Word Alignment. Computational Linguistics, 23(2): 313-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="1286" citStr="Och and Ney, 2000" startWordPosition="177" endWordPosition="180">in the specific domain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. This paper addres</context>
<context position="4702" citStr="Och and Ney, 2000" startWordPosition="693" endWordPosition="696">th sentential and sub-sentential levels by using word alignment results. These GTMS are usually employed to translate various documents such as user manuals, computer operation guides, and mechanical operation manuals. 2 Word Alignment Adaptation 2.1 Bi-directional Word Alignment In statistical translation models (Brown et al., 1993), only one-to-one and more-to-one word alignment links can be found. Thus, some multi-word units cannot be correctly aligned. In order to deal with this problem, we perform translation in two directions (English to Chinese, and Chinese to English) as described in (Och and Ney, 2000). The GIZA++ toolkit 1 is used to perform statistical word alignment. For the general domain, we use and SG1 SG2 to represent the alignment sets obtained with English as the source language and Chinese as the target language or vice versa. For alignment links in both sets, we use i for English words and j for Chinese words. SG = A j j A j = a j a j &gt;_ 1 {( , ) |{ }, 0 } SG = i Ai A i = ai a i &gt;_ 2 {( , ) |{ }, 0} Where, ak (k = i, j) is the position of the source word aligned to the target word in position k. The set Ak(k=i, j) indicates the words aligned to the same source word k. For example</context>
<context position="6625" citStr="Och and Ney, 2000" startWordPosition="1067" endWordPosition="1070">lingual corpus in the general domain, we can get two word alignment results for the training data. By taking the intersection of the two word alignment results, we build a new alignment set. The alignment links in this intersection set are extended by iteratively adding 1 It is located at http://www.isi.edu/~och/GIZA++.html 2 In this paper, the union operation does not remove the replicated elements. For example, if set one includes two elements {1, 2} and set two includes two elements {1, 3}, then the union of these two sets becomes {1, 1, 2, 3}. word alignment links into it as described in (Och and Ney, 2000). Based on the extended alignment links, we build an English to Chinese translation dictionary D1 with translation probabilities. In order to filter some noise caused by the error alignment links, we only retain those translation pairs whose translation probabilities are above a threshold S1 or co-occurring frequencies are above a threshold S2 . When we train the IBM statistical word alignment model with a limited bilingual corpus in the specific domain, we build another translation dictionary with the same method as for the D2 dictionary D1. But we adopt a different filtering strategy for the</context>
<context position="10136" citStr="Och and Ney, 2000" startWordPosition="1676" endWordPosition="1679">e is (x-ray, X) and the other is (x-ray, X射线). And the single Chinese words “射” and “线” have no alignment links in the set WA. According to the rule d), we select the link (x-ray, X 射线). The Chinese sentences in both the training set and the testing set are automatically segmented into words. In order to exclude the effect of the segmentation errors on our alignment results, we correct the segmentation errors in our testing set. The alignments in the testing set are manually annotated, which includes 1,478 alignment links. 3.2 Overall Performance We use evaluation metrics similar to those in (Och and Ney, 2000). However, we do not classify alignment links into sure links and possible links. We consider each alignment as a sure link. If we use SG to represent the alignments identified by the proposed methods and to denote the reference SC alignments, the methods to calculate the precision, recall, and f-measure are shown in Equation (1), (2) and (3). According to the definition of the alignment error rate (AER) in (Och and Ney, 2000), AER can be calculated with Equation (4). Thus, the higher the f-measure is, the lower the alignment error rate is. Thus, we will only give precision, recall and AER val</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Philippe Langlais</author>
</authors>
<title>Sub-sentential Exploitation of Translation Memories.</title>
<date>2001</date>
<booktitle>In Proc. of MT Summit VIII,</booktitle>
<pages>335--339</pages>
<contexts>
<context position="3636" citStr="Simard and Langlais, 2001" startWordPosition="534" endWordPosition="537">n; (3) We build two translation dictionaries according to the alignment results in (1) and (2) respectively; (4) For each sentence pair in the specific domain, we use the two models to get different word alignment results and improve the results according to the translation dictionaries. Experimental results show that our method improves domain-specific word alignment in terms of both precision and recall, achieving a 21.96% relative error rate reduction. The acquired alignment results are used in a generalized translation memory system (GTMS, a kind of computer assisted translation systems) (Simard and Langlais, 2001). This kind of system facilitates the re-use of existing translation pairs to translate documents. When translating a new sentence, the system tries to provide the pre-translated examples matched with the input and recommends a translation to the human translator, and then the translator edits the suggestion to get a final translation. The conventional TMS can only recommend translation examples on the sentential level while GTMS can work on both sentential and sub-sentential levels by using word alignment results. These GTMS are usually employed to translate various documents such as user man</context>
</contexts>
<marker>Simard, Langlais, 2001</marker>
<rawString>Michel Simard and Philippe Langlais. 2001. Sub-sentential Exploitation of Translation Memories. In Proc. of MT Summit VIII, pages 335-339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufis</author>
<author>Ana Maria Barbu</author>
</authors>
<title>Lexical Token Alignment: Experiments, Results and Application.</title>
<date>2002</date>
<booktitle>In Proc. of the Third International Conference on Language Resources and Evaluation,</booktitle>
<pages>458--465</pages>
<contexts>
<context position="1440" citStr="Tufis and Barbu, 2002" startWordPosition="200" endWordPosition="203">ant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available. This paper addresses the problem of word alignment in a specific domain, where only a small domain-specific corpus is available. In the domain-specific corpus, there are t</context>
</contexts>
<marker>Tufis, Barbu, 2002</marker>
<rawString>Dan Tufis and Ana Maria Barbu. 2002. Lexical Token Alignment: Experiments, Results and Application. In Proc. of the Third International Conference on Language Resources and Evaluation, pages 458-465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<contexts>
<context position="1267" citStr="Wu, 1997" startWordPosition="175" endWordPosition="176">le corpus in the specific domain respectively, and then improves the domain-specific word alignment with these two models. Experimental results show a significant improvement in terms of both alignment precision and recall. And the alignment results are applied in a computer assisted translation system to improve human translation efficiency. 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation (SMT) (Brown et al., 1993). In previous alignment methods, some researchers modeled the alignments with different statistical models (Wu, 1997; Och and Ney, 2000; Cherry and Lin, 2003). Some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufis and Barbu, 2002). However, All of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997). However, few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): 377-403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>