<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006567">
<title confidence="0.9992215">
Improving Pronoun Resolution Using Statistics-Based
Semantic Compatibility Information
</title>
<author confidence="0.988357">
Xiaofeng Yang†‡ Jian Su† Chew Lim Tan‡
</author>
<affiliation confidence="0.981556">
†Institute for Infocomm Research ‡ Department of Computer Science
</affiliation>
<address confidence="0.9794675">
21 Heng Mui Keng Terrace, National University of Singapore,
Singapore, 119613 Singapore, 117543
</address>
<email confidence="0.989701">
Ixiaofengy,sujianl@i2r.a-star.edu.sg lyangxiao,tancll@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999308176470588">
In this paper we focus on how to improve
pronoun resolution using the statistics-
based semantic compatibility information.
We investigate two unexplored issues that
influence the effectiveness of such in-
formation: statistics source and learning
framework. Specifically, we for the first
time propose to utilize the web and the
twin-candidate model, in addition to the
previous combination of the corpus and
the single-candidate model, to compute
and apply the semantic information. Our
study shows that the semantic compatibil-
ity obtained from the web can be effec-
tively incorporated in the twin-candidate
learning model and significantly improve
the resolution of neutral pronouns.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998224">
Semantic compatibility is an important factor for
pronoun resolution. Since pronouns, especially neu-
tral pronouns, carry little semantics of their own,
the compatibility between an anaphor and its an-
tecedent candidate is commonly evaluated by ex-
amining the relationships between the candidate and
the anaphor’s context, based on the statistics that the
corresponding predicate-argument tuples occur in a
particular large corpus. Consider the example given
in the work of Dagan and Itai (1990):
</bodyText>
<footnote confidence="0.65145025">
(1) They know full well that companies held tax
money aside for collection later on the basis
that the government said it1 was going to col-
lect it2.
</footnote>
<bodyText confidence="0.999884657894737">
For anaphor it1, the candidate government should
have higher semantic compatibility than money be-
cause government collect is supposed to occur more
frequently than money collect in a large corpus. A
similar pattern could also be observed for it2.
So far, the corpus-based semantic knowledge has
been successfully employed in several anaphora res-
olution systems. Dagan and Itai (1990) proposed
a heuristics-based approach to pronoun resolu-
tion. It determined the preference of candidates
based on predicate-argument frequencies. Recently,
Bean and Riloff (2004) presented an unsupervised
approach to coreference resolution, which mined
the co-referring NP pairs with similar predicate-
arguments from a large corpus using a bootstrapping
method.
However, the utility of the corpus-based se-
mantics for pronoun resolution is often argued.
Kehler et al. (2004), for example, explored the
usage of the corpus-based statistics in supervised
learning based systems, and found that such infor-
mation did not produce apparent improvement for
the overall pronoun resolution. Indeed, existing
learning-based approaches to anaphor resolution
have performed reasonably well using limited
and shallow knowledge (e.g., Mitkov (1998),
Soon et al. (2001), Strube and Muller (2003)).
Could the relatively noisy semantic knowledge give
us further system improvement?
In this paper we focus on improving pronominal
anaphora resolution using automatically computed
semantic compatibility information. We propose to
enhance the utility of the statistics-based knowledge
from two aspects:
Statistics source. Corpus-based knowledge usu-
ally suffers from data sparseness problem. That is,
many predicate-argument tuples would be unseen
even in a large corpus. A possible solution is the
</bodyText>
<page confidence="0.982289">
165
</page>
<note confidence="0.9917055">
Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999188673469388">
web. It is believed that the size of the web is thou-
sands of times larger than normal large corpora, and
the counts obtained from the web are highly corre-
lated with the counts from large balanced corpora
for predicate-argument bi-grams (Keller and Lapata,
2003). So far the web has been utilized in nominal
anaphora resolution (Modjeska et al., 2003; Poesio
et al., 2004) to determine the semantic relation be-
tween an anaphor and candidate pair. However, to
our knowledge, using the web to help pronoun reso-
lution still remains unexplored.
Learning framework. Commonly, the predicate-
argument statistics is incorporated into anaphora res-
olution systems as a feature. What kind of learn-
ing framework is suitable for this feature? Previous
approaches to anaphora resolution adopt the single-
candidate model, in which the resolution is done on
an anaphor and one candidate at a time (Soon et al.,
2001; Ng and Cardie, 2002). However, as the pur-
pose of the predicate-argument statistics is to eval-
uate the preference of the candidates in semantics,
it is possible that the statistics-based semantic fea-
ture could be more effectively applied in the twin-
candidate (Yang et al., 2003) that focusses on the
preference relationships among candidates.
In our work we explore the acquisition of the se-
mantic compatibility information from the corpus
and the web, and the incorporation of such semantic
information in the single-candidate model and the
twin-candidate model. We systematically evaluate
the combinations of different statistics sources and
learning frameworks in terms of their effectiveness
in helping the resolution. Results on the MUC data
set show that for neutral pronoun resolution in which
an anaphor has no specific semantic category, the
web-based semantic information would be the most
effective when applied in the twin-candidate model:
Not only could such a system significantly improve
the baseline without the semantic feature, it also out-
performs the system with the combination of the cor-
pus and the single-candidate model (by 11.5% suc-
cess).
The rest of this paper is organized as follows. Sec-
tion 2 describes the acquisition of the semantic com-
patibility information from the corpus and the web.
Section 3 discusses the application of the statistics
in the single-candidate and twin-candidate learning
models. Section 4 gives the experimental results,
and finally, Section 5 gives the conclusion.
</bodyText>
<sectionHeader confidence="0.8773025" genericHeader="method">
2 Computing the Statistics-based Semantic
Compatibility
</sectionHeader>
<bodyText confidence="0.9989105">
In this section, we introduce in detail how to com-
pute the semantic compatibility, using the predicate-
argument statistics obtained from the corpus or the
web.
</bodyText>
<subsectionHeader confidence="0.996444">
2.1 Corpus-Based Semantic Compatibility
</subsectionHeader>
<bodyText confidence="0.9997185">
Three relationships, possessive-noun, subject-verb
and verb-object, are considered in our work. Be-
fore resolution a large corpus is prepared. Doc-
uments in the corpus are processed by a shallow
parser that could generate predicate-argument tuples
of the above three relationships1.
To reduce data sparseness, the following steps are
applied in each resulting tuple, automatically:
</bodyText>
<listItem confidence="0.999549285714286">
• Only the nominal or verbal heads are retained.
• Each Named-Entity (NE) is replaced by a com-
mon noun which corresponds to the seman-
tic category of the NE (e.g. “IBM” —* “com-
pany”) 2.
• All words are changed to their base morpho-
logic forms (e.g. “companies —* company”).
</listItem>
<bodyText confidence="0.9992220625">
During resolution, for an encountered anaphor,
each of its antecedent candidates is substituted with
the anaphor . According to the role and type of the
anaphor in its context, a predicate-argument tuple is
extracted and the above three steps for data-sparse
reduction are applied. Consider the sentence (1),
for example. The anaphors “it1” and “it2” indicate
a subject verb and verb object relationship, respec-
tively. Thus, the predicate-argument tuples for the
two candidates “government” and “money” would
be (collect (subject government)) and (collect (sub-
ject money)) for “it1”, and (collect (object govern-
ment)) and (collect (object money)) for “it2”.
Each extracted tuple is searched in the prepared
tuples set of the corpus, and the times the tuple oc-
curs are calculated. For each candidate, its semantic
</bodyText>
<footnote confidence="0.9999015">
1The possessive-noun relationship involves the forms like
“NP2 ofNP1” and “NP1’s NP2”.
2In our study, the semantic category of a NE is identified
automatically by the pre-processing NE recognition component.
</footnote>
<page confidence="0.998449">
166
</page>
<bodyText confidence="0.96499175">
compatibility with the anaphor could be represented
simply in terms offrequency
StatSem(candi, ana) = count(candi, ana) (1)
where count(candi, ana) is the count of the tuple
formed by candi and ana, or alternatively, in terms
of conditional probability (P (candi, ana|candi)),
where the count of the tuple is divided by the count
of the single candidate in the corpus. That is
</bodyText>
<equation confidence="0.912011">
count(candi, ana)
StatSem(candi, ana) = (2)
count(candi)
</equation>
<bodyText confidence="0.999081">
In this way, the statistics would not bias candidates
that occur frequently in isolation.
</bodyText>
<subsectionHeader confidence="0.997971">
2.2 Web-Based Semantic Compatibility
</subsectionHeader>
<bodyText confidence="0.996614511627907">
Unlike documents in normal corpora, web pages
could not be preprocessed to generate the predicate-
argument reserve. Instead, the predicate-argument
statistics has to be obtained via a web search engine
like Google and Altavista. For the three types of
predicate-argument relationships, queries are con-
structed in the forms of “NP andi VP” (for subject-
verb), “VP NP andi” (for verb-object), and “NP andi
’s NP” or “NP of NP andi” (for possessive-noun).
Consider the following sentence:
(2) Several experts suggested that IBM’s account-
ing grew much more liberal since the mid 1980s
as its business turned sour.
For the pronoun “its” and the candidate “IBM”, the
two generated queries are “business of IBM” and
“IBM’s business”.
To reduce data sparseness, in an initial query only
the nominal or verbal heads are retained. Also, each
NE is replaced by the corresponding common noun.
(e.g, “IBM’s business” —* “company’s business” and
“business ofIBM” —* “business of company”).
A set of inflected queries is generated by ex-
panding a term into all its possible morphologi-
cal forms. For example, in Sentence (1), “collect
money” becomes “collected|collecting|... money”,
and in (2) “business of company” becomes “business
of company|companies”). Besides, determiners are
inserted for every noun. If the noun is the candidate
under consideration, only the definite article the is
inserted. For other nouns, instead, a/an, the and the
empty determiners (for bare plurals) would be added
(e.g., “the|a business of the company|companies”).
Queries are submitted to a particular web search
engine (Google in our study). All queries are per-
formed as exact matching. Similar to the corpus-
based statistics, the compatibility for each candidate
and anaphor pair could be represented using either
frequency (Eq. 1) or probability (Eq. 2) metric. In
such a situation, count(candi, ana) is the hit num-
ber of the inflected queries returned by the search
engine, while count(candi) is the hit number of the
query formed with only the head of the candidate
(i.e.,“the + candi”).
</bodyText>
<sectionHeader confidence="0.967329" genericHeader="method">
3 Applying the Semantic Compatibility
</sectionHeader>
<bodyText confidence="0.999956">
In this section, we discuss how to incorporate the
statistics-based semantic compatibility for pronoun
resolution, in a machine learning framework.
</bodyText>
<subsectionHeader confidence="0.998942">
3.1 The Single-Candidate Model
</subsectionHeader>
<bodyText confidence="0.999995782608696">
One way to utilize the semantic compatibility is to
take it as a feature under the single-candidate learn-
ing model as employed by Ng and Cardie (2002).
In such a learning model, each training or testing
instance takes the form of i{C, ana}, where ana is
the possible anaphor and C is its antecedent candi-
date. An instance is associated with a feature vector
to describe their relationships.
During training, for each anaphor in a given text,
a positive instance is created by pairing the anaphor
and its closest antecedent. Also a set of negative in-
stances is formed by pairing the anaphor and each
of the intervening candidates. Based on the train-
ing instances, a binary classifier is generated using a
certain learning algorithm, like C5 (Quinlan, 1993)
in our work.
During resolution, given a new anaphor, a test in-
stance is created for each candidate. This instance is
presented to the classifier, which then returns a pos-
itive or negative result with a confidence value indi-
cating the likelihood that they are co-referent. The
candidate with the highest confidence value would
be selected as the antecedent.
</bodyText>
<subsectionHeader confidence="0.980096">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.998892">
In our study we only consider those domain-
independent features that could be obtained with low
</bodyText>
<page confidence="0.988028">
167
</page>
<bodyText confidence="0.919901615384615">
Feature Description
DefNp 1 if the candidate is a definite NP; else 0
Pron 1 if the candidate is a pronoun; else 0
NE 1 if the candidate is a named entity; else 0
SameSent 1 if the candidate and the anaphor is in the same sentence; else 0
NearestNP 1 if the candidate is nearest to the anaphor; else 0
ParalStuct 1 if the candidate has an parallel structure with ana; else 0
FirstNP 1 if the candidate is the first NP in a sentence; else 0
Reflexive 1 if the anaphor is a reflexive pronoun; else 0
Type Type of the anaphor (0: Single neuter pronoun; 1: Plural neuter pronoun; 2:
Male personal pronoun; 3: Female personal pronoun)
StatSem* the statistics-base semantic compatibility of the candidate
SemMag** the semantic compatibility difference between two competing candidates
</bodyText>
<tableCaption confidence="0.862228">
Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model
while **ed feature is only for the twin-candidate mode)
</tableCaption>
<bodyText confidence="0.979724088888889">
computational cost but with high reliability. Table 1
summarizes the features with their respective possi-
ble values. The first three features represent the lex-
ical properties of a candidate. The POS properties
could indicate whether a candidate refers to a hearer-
old entity that would have a higher preference to be
selected as the antecedent (Strube, 1998). SameSent
and NearestNP mark the distance relationships be-
tween an anaphor and the candidate, which would
significantly affect the candidate selection (Hobbs,
1978). FirstNP aims to capture the salience of the
candidate in the local discourse segment. ParalStuct
marks whether a candidate and an anaphor have sim-
ilar surrounding words, which is also a salience fac-
tor for the candidate evaluation (Mitkov, 1998).
Feature StatSem records the statistics-based se-
mantic compatibility computed, from the corpus or
the web, by either frequency or probability metric,
as described in the previous section. If a candidate
is a pronoun, this feature value would be set to that
of its closest nominal antecedent.
As described, the semantic compatibility of a can-
didate is computed under the context of the cur-
rent anaphor. Consider two occurrences of anaphors
“... it1 collected... ” and “... it2 said... ”. As “NP
collected” should occur less frequently than “NP
said”, the candidates of it1 would generally have
predicate-argument statistics lower than those of it2.
That is, a positive instance for it1 might bear a lower
semantic feature value than a negative instance for
it2. The consequence is that the learning algorithm
would think such a feature is not that ”indicative”
and reduce its salience in the resulting classifier.
One way to tackle this problem is to normalize the
feature by the frequencies of the anaphor’s context,
e.g., “count(collected)” and “count(said)”. This,
however, would require extra calculation. In fact,
as candidates of a specific anaphor share the same
anaphor context, we can just normalize the semantic
feature of a candidate by that of its competitor:
StatSem(C, ana)
StatSem�(C, ana) =
The value (0 — 1) represents the rank of the
semantic compatibility of the candidate C among
candi set(ana), the current candidates of ana.
</bodyText>
<subsectionHeader confidence="0.997547">
3.3 The Twin-Candidate Model
</subsectionHeader>
<bodyText confidence="0.999955666666667">
Yang et al. (2003) proposed an alternative twin-
candidate model for anaphora resolution task. The
strength of such a model is that unlike the single-
candidate model, it could capture the preference re-
lationships between competing candidates. In the
model, candidates for an anaphor are paired and
features from two competing candidates are put to-
gether for consideration. This property could nicely
deal with the above mentioned training problem of
different anaphor contexts, because the semantic
feature would be considered under the current can-
didate set only. In fact, as semantic compatibility is
</bodyText>
<equation confidence="0.724444">
max StatSem(ci, ana)
ciEcandi set(ana)
</equation>
<page confidence="0.980745">
168
</page>
<bodyText confidence="0.999635806451613">
a preference-based factor for anaphor resolution, it
would be incorporated in the twin-candidate model
more naturally.
In the twin-candidate model, an instance takes a
form like i{C1, C2, ana}, where C1 and C2 are two
candidates. We stipulate that C2 should be closer to
ana than C1 in distance. The instance is labelled as
“10” if C1 the antecedent, or “01” if C2 is.
During training, for each anaphor, we find its
closest antecedent, Cant,. A set of “10” instances,
i{Cant,, C, ana}, is generated by pairing Cant, and
each of the interning candidates C. Also a set of “01”
instances, i{C, Cant,, ana}, is created by pairing
Cant, with each candidate before Cant, until another
antecedent, if any, is reached.
The resulting pairwise classifier would return
“10” or “01” indicating which candidate is preferred
to the other. During resolution, candidates are paired
one by one. The score of a candidate is the total
number of the competitors that the candidate wins
over. The candidate with the highest score would be
selected as the antecedent.
Features The features for the twin-candidate
model are similar to those for the single-candidate
model except that a duplicate set of features has to
be prepared for the additional candidate. Besides,
a new feature, SemMag, is used in place of Stat-
Sem to represent the difference magnitude between
the semantic compatibility of two candidates. Let
mag = StatSem(C1, ana)/StatSem(C2, ana), feature
SemMag is defined as follows,
</bodyText>
<equation confidence="0.998433">
SemMag(C1, C2, ana) = r 1mag
mag11 : mag &lt; 1 1
</equation>
<bodyText confidence="0.99987">
The positive or negative value marks the times that
the statistics of C1 is larger or smaller than C2.
</bodyText>
<sectionHeader confidence="0.992443" genericHeader="evaluation">
4 Evaluation and Discussion
</sectionHeader>
<subsectionHeader confidence="0.980521">
4.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999982081081081">
In our study we were only concerned about the third-
person pronoun resolution. With an attempt to ex-
amine the effectiveness of the semantic feature on
different types of pronouns, the whole resolution
was divided into neutral pronoun (it &amp; they) reso-
lution and personal pronoun (he &amp; she) resolution.
The experiments were done on the newswire do-
main, using MUC corpus (Wall Street Journal ar-
ticles). The training was done on 150 documents
from MUC-6 coreference data set, while the testing
was on the 50 formal-test documents of MUC-6 (30)
and MUC-7 (20). Throughout the experiments, de-
fault learning parameters were applied to the C5 al-
gorithm. The performance was evaluated based on
success, the ratio of the number of correctly resolved
anaphors over the total number of anaphors.
An input raw text was preprocessed automati-
cally by a pipeline of NLP components. The noun
phrase identification and the predicate-argument ex-
traction were done based on the results of a chunk
tagger, which was trained for the shared task of
CoNLL-2000 and achieved 92% accuracy (Zhou et
al., 2000). The recognition of NEs as well as their
semantic categories was done by a HMM based
NER, which was trained for the MUC NE task
and obtained high F-scores of 96.9% (MUC-6) and
94.3% (MUC-7) (Zhou and Su, 2002).
For each anaphor, the markables occurring within
the current and previous two sentences were taken
as the initial candidates. Those with mismatched
number and gender agreements were filtered from
the candidate set. Also, pronouns or NEs that dis-
agreed in person with the anaphor were removed in
advance. For the training set, there are totally 645
neutral pronouns and 385 personal pronouns with
non-empty candidate set, while for the testing set,
the number is 245 and 197.
</bodyText>
<subsectionHeader confidence="0.996986">
4.2 The Corpus and the Web
</subsectionHeader>
<bodyText confidence="0.999870111111111">
The corpus for the predicate-argument statistics
computation was from the TIPSTER’s Text Re-
search Collection (v1994). Consisting of 173,252
Wall Street Journal articles from the year 1988 to
1992, the data set contained about 76 million words.
The documents were preprocessed using the same
POS tagging and NE-recognition components as in
the pronoun resolution task. Cass (Abney, 1996), a
robust chunker parser was then applied to generate
the shallow parse trees, which resulted in 353,085
possessive-noun tuples, 759,997 verb-object tuples
and 1,090,121 subject-verb tuples.
We examined the capacity of the web and the
corpus in terms of zero-count ratio and count num-
ber. On average, among the predicate-argument tu-
ples that have non-zero corpus-counts, above 93%
have also non-zero web-counts. But the ratio is only
around 40% contrariwise. And for the predicate-
</bodyText>
<page confidence="0.996681">
169
</page>
<table confidence="0.9999377">
Neutral Pron Personal Pron Overall
Learning Model System Corpus Web Corpus Web Corpus Web
baseline 65.7 86.8 75.1
+frequency 67.3 69.9 86.8 86.8 76.0 76.9
Single-Candidate +normalized frequency 66.9 67.8 86.8 86.8 75.8 76.2
+probability 65.7 65.7 86.8 86.8 75.1 75.1
+normalized probability 67.7 70.6 86.8 86.8 76.2 77.8
baseline 73.9 91.9 81.9
Twin-Candidate +frequency 76.7 79.2 91.4 91.9 83.3 84.8
+probability 75.9 78.0 91.4 92.4 82.8 84.4
</table>
<tableCaption confidence="0.998056">
Table 2: The performance of different resolution systems
</tableCaption>
<table confidence="0.9985645">
Relationship N-Pron P-Pron
Possessive-Noun 0.508 0.517
Verb-Object 0.503 0.526
Subject-Verb 0.619 0.676
</table>
<tableCaption confidence="0.86928">
Table 3: Correlation between web and corpus counts
on the seen predicate-argument tuples
</tableCaption>
<bodyText confidence="0.997206">
argument tuples that could be seen in both data
sources, the count from the web is above 2000 times
larger than that from the corpus.
Although much less sparse, the web counts are
significantly noisier than the corpus count since no
tagging, chunking and parsing could be carried out
on the web pages. However, previous study (Keller
and Lapata, 2003) reveals that the large amount of
data available for the web counts could outweigh the
noisy problems. In our study we also carried out a
correlation analysis3 to examine whether the counts
from the web and the corpus are linearly related,
on the predicate-argument tuples that can be seen
in both data sources. From the results listed in Ta-
ble 3, we observe moderately high correlation, with
coefficients ranging from 0.5 to 0.7 around, between
the counts from the web and the corpus, for both
neutral pronoun (N-Pron) and personal pronoun (P-
Pron) resolution tasks.
</bodyText>
<subsectionHeader confidence="0.999269">
4.3 System Evaluation
</subsectionHeader>
<bodyText confidence="0.996460666666667">
Table 2 summarizes the performance of the systems
with different combinations of statistics sources and
learning frameworks. The systems without the se-
</bodyText>
<footnote confidence="0.5106435">
3All the counts were log-transformed and the correlation co-
efficients were evaluated based on Pearsons’ r.
</footnote>
<bodyText confidence="0.999777266666667">
mantic feature were used as the baseline. Under the
single-candidate (SC) model, the baseline system
obtains a success of 65.7% and 86.8% for neutral
pronoun and personal pronoun resolution, respec-
tively. By contrast, the twin-candidate (TC) model
achieves a significantly (p G 0.05, by two-tailed t-
test) higher success of 73.9% and 91.9%, respec-
tively. Overall, for the whole pronoun resolution,
the baseline system under the TC model yields a
success 81.9%, 6.8% higher than SC does4. The
performance is comparable to most state-of-the-art
pronoun resolution systems on the same data set.
Web-based feature vs. Corpus-based feature
The third column of the table lists the results us-
ing the web-based compatibility feature for neutral
pronouns. Under both SC and TC models, incorpo-
ration of the web-based feature significantly boosts
the performance of the baseline: For the best sys-
tem in the SC model and the TC model, the success
rate is improved significantly by around 4.9% and
5.3%, respectively. A similar pattern of improve-
ment could be seen for the corpus-based semantic
feature. However, the increase is not as large as
using the web-based feature: Under the two learn-
ing models, the success rate of the best system with
the corpus-based feature rises by up to 2.0% and
2.8% respectively, about 2.9% and 2.5% less than
that of the counterpart systems with the web-based
feature. The larger size and the better counts of the
web against the corpus, as reported in Section 4.2,
</bodyText>
<footnote confidence="0.9947446">
4The improvement against SC is higher than that reported
in (Yang et al., 2003). It should be because we now used 150
training documents rather than 30 ones as in the previous work.
The TC model would benefit from larger training data set as it
uses more features (more than double) than SC.
</footnote>
<page confidence="0.995842">
170
</page>
<bodyText confidence="0.999603875">
should contribute to the better performance.
Single-candidate model vs. Twin-Candidate
model The difference between the SC and the TC
model is obvious from the table. For the N-Pron
and P-Pron resolution, the systems under TC could
outperform the counterpart systems under SC by
above 5% and 8% success, respectively. In addition,
the utility of the statistics-based semantic feature is
more salient under TC than under SC for N-Pron res-
olution: the best gains using the corpus-based and
the web-based semantic features under TC are 2.9%
and 5.3% respectively, higher than those under the
SC model using either un-normalized semantic fea-
tures (1.6% and 3.3%), or normalized semantic fea-
tures (2.0% and 4.9%). Although under SC, the nor-
malized semantic feature could result in a gain close
to under TC, its utility is not stable: with metric fre-
quency, using the normalized feature performs even
worse than using the un-normalized one. These re-
sults not only affirm the claim by Yang et al. (2003)
that the TC model is superior to the SC model for
pronoun resolution, but also indicate that TC is more
reliable than SC in applying the statistics-based se-
mantic feature, for N-Pron resolution.
Web+TC vs. Other combinations The above
analysis has exhibited the superiority of the web
over the corpus, and the TC model over the
SC model. The experimental results also re-
veal that using the the web-based semantic fea-
ture together with the TC model is able to further
boost the resolution performance for neutral pro-
nouns. The system with such a Web+TC combi-
nation could achieve a high success of 79.2%, de-
feating all the other possible combinations. Es-
pecially, it considerably outperforms (up to 11.5%
success) the system with the Corpus+SC combina-
tion, which is commonly adopted in previous work
(e.g., Kehler et al. (2004)).
Personal pronoun resolution vs. Neutral pro-
noun resolution Interestingly, the statistics-based
semantic feature has no effect on the resolution of
personal pronouns, as shown in the table 2. We
found in the learned decision trees such a feature
did not occur (SC) or only occurred in bottom nodes
(TC). This should be because personal pronouns
have strong restriction on the semantic category (i.e.,
human) of the candidates. A non-human candidate,
even with a high predicate-argument statistics, could
</bodyText>
<table confidence="0.999627142857143">
Feature Group Isolated Combined
SemMag (Web-based) 61.2 61.2
Type+Reflexive 53.1 61.2
ParaStruct 53.1 61.2
Pron+DefNP+InDefNP+NE 57.1 67.8
NearestNP+SameSent 53.1 70.2
FirstNP 65.3 79.2
</table>
<tableCaption confidence="0.970091">
Table 4: Results of different feature groups under
</tableCaption>
<table confidence="0.9929719">
the TC model for N-pron resolution
SameSent_1 = 0:
:..SemMag &gt; 0:
: :..Pron_2 = 0: 10 (200/23)
: :Pron_2 = 1: ...
: SemMag &lt;= 0:
: :..Pron_2 = 1: 01 (75/1)
: Pron_2 = 0:
: :..SemMag &lt;= -28: 01 (110/19)
: SemMag &gt; -28: ...
</table>
<equation confidence="0.941639333333333">
SameSent_1 = 1:
:..SameSent_2 = 0: 01 (1655/49)
SameSent_2 = 1:
:..FirstNP_2 = 1: 01 (104/1)
FirstNP_2 = 0:
:..ParaStruct_2 = 1: 01 (3)
ParaStruct_2 = 0:
:..SemMag &lt;= -151: 01 (27/2)
SemMag &gt; -151:...
</equation>
<figureCaption confidence="0.94095625">
Figure 1: Top portion of the decision tree learned
under TC model for N-pron resolution (features ended
with “ 1” are for the first candidate C1 and those with “ 2” are
for C2.)
</figureCaption>
<bodyText confidence="0.999874555555555">
not be used as the antecedent (e.g. company said in
the sentence “...the company ... he said ...”). In
fact, our analysis of the current data set reveals that
most P-Prons refer back to a P-Pron or NE candidate
whose semantic category (human) has been deter-
mined. That is, simply using features NE and Pron
is sufficient to guarantee a high success, and thus the
relatively weak semantic feature would not be taken
in the learned decision tree for resolution.
</bodyText>
<subsectionHeader confidence="0.99381">
4.4 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.999473428571429">
In our experiment we were also concerned about the
importance of the web-based compatibility feature
(using frequency metric) among the feature set. For
this purpose, we divided the features into groups,
and then trained and tested on one group at a time.
Table 4 lists the feature groups and their respective
results for N-Pron resolution under the TC model.
</bodyText>
<page confidence="0.994416">
171
</page>
<bodyText confidence="0.999897045454546">
The second column is for the systems with only the
current feature group, while the third column is with
the features combined with the existing feature set.
We see that used in isolation, the semantic compati-
bility feature is able to achieve a success up to 61%
around, just 4% lower than the best indicative fea-
ture FirstNP. In combination with other features, the
performance could be improved by as large as 18%
as opposed to being used alone.
Figure 1 shows the top portion of the pruned deci-
sion tree for N-Pron resolution under the TC model.
We could find that: (i) When comparing two can-
didates which occur in the same sentence as the
anaphor, the web-based semantic feature would be
examined in the first place, followed by the lexi-
cal property of the candidates. (ii) When two non-
pronominal candidates are both in previous sen-
tences before the anaphor, the web-based semantic
feature is still required to be examined after FirstNP
and ParaStruct. The decision tree further indicates
that the web-based feature plays an important role in
N-Pron resolution.
</bodyText>
<sectionHeader confidence="0.999112" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999992739130435">
Our research focussed on improving pronoun reso-
lution using the statistics-based semantic compati-
bility information. We explored two issues that af-
fect the utility of the semantic information: statis-
tics source and learning framework. Specifically, we
proposed to utilize the web and the twin-candidate
model, in addition to the common combination of
the corpus and single-candidate model, to compute
and apply the semantic information.
Our experiments systematically evaluated differ-
ent combinations of statistics sources and learn-
ing models. The results on the newswire domain
showed that the web-based semantic compatibility
could be the most effectively incorporated in the
twin-candidate model for the neutral pronoun res-
olution. While the utility is not obvious for per-
sonal pronoun resolution, we can still see the im-
provement on the overall performance. We believe
that the semantic information under such a config-
uration would be even more effective on technical
domains where neutral pronouns take the majority
in the pronominal anaphors. Our future work would
have a deep exploration on such domains.
</bodyText>
<sectionHeader confidence="0.996454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993625">
S. Abney. 1996. Partial parsing via finite-state cascades. In
Workshop on Robust Parsing, 8th European Summer School
in Logic, Language and Information, pages 8–15.
D. Bean and E. Riloff. 2004. Unsupervised learning of contex-
tual role knowledge for coreference resolution. In Proceed-
ings of 2004 North American chapter of the Association for
Computational Linguistics annual meeting.
I. Dagan and A. Itai. 1990. Automatic processing of large cor-
pora for the resolution of anahora references. In Proceedings
of the 13th International Conference on Computational Lin-
guistics, pages 330–332.
J. Hobbs. 1978. Resolving pronoun references. Lingua,
44:339–352.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The
(non)utility of predicate-argument frequencies for pronoun
interpretation. In Proceedings of 2004 North American
chapter of the Association for Computational Linguistics an-
nual meeting.
F. Keller and M. Lapata. 2003. Using the web to obtain
freqencies for unseen bigrams. Computational Linguistics,
29(3):459–484.
R. Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In Proceedings of the 17th Int. Conference on
Computational Linguistics, pages 869–875.
N. Modjeska, K. Markert, and M. Nissim. 2003. Using the web
in machine learning for other-anaphora resolution. In Pro-
ceedings of the Conference on Empirical Methods in Natural
Language Processing, pages 176–183.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 104–111, Philadelphia.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceedings of
42th Annual Meeting of the Association for Computational
Linguistics.
J. R. Quinlan. 1993. C4.5: Programs for machine learning.
Morgan Kaufmann Publishers, San Francisco, CA.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4):521–544.
M. Strube and C. Muller. 2003. A machine learning approach
to pronoun resolution in spoken dialogue. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 168–175, Japan.
M. Strube. 1998. Never look back: An alternative to centering.
In Proceedings of the 17th Int. Conference on Computational
Linguistics and 36th Annual Meeting of ACL, pages 1251–
1257.
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference reso-
lution using competition learning approach. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics, Japan.
G. Zhou and J. Su. 2002. Named Entity recognition using a
HMM-based chunk tagger. In Proceedings of the 40th An-
nual Meeting of the Association for Computational Linguis-
tics, Philadelphia.
G. Zhou, J. Su, and T. Tey. 2000. Hybrid text chunking. In
Proceedings of the 4th Conference on Computational Natu-
ral Language Learning, pages 163–166, Lisbon, Portugal.
</reference>
<page confidence="0.997724">
172
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.702733">
<title confidence="0.9991265">Improving Pronoun Resolution Using Statistics-Based Semantic Compatibility Information</title>
<author confidence="0.958987">Lim</author>
<affiliation confidence="0.8682705">for Infocomm Research of Computer Science 21 Heng Mui Keng Terrace, National University of Singapore,</affiliation>
<address confidence="0.996338">Singapore, 119613 Singapore, 117543</address>
<abstract confidence="0.999335888888889">In this paper we focus on how to improve pronoun resolution using the statisticsbased semantic compatibility information. We investigate two unexplored issues that influence the effectiveness of such information: statistics source and learning framework. Specifically, we for the first time propose to utilize the web and the twin-candidate model, in addition to the previous combination of the corpus and the single-candidate model, to compute and apply the semantic information. Our study shows that the semantic compatibility obtained from the web can be effectively incorporated in the twin-candidate learning model and significantly improve the resolution of neutral pronouns.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Partial parsing via finite-state cascades.</title>
<date>1996</date>
<booktitle>In Workshop on Robust Parsing, 8th European Summer School in Logic, Language and Information,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="19697" citStr="Abney, 1996" startWordPosition="3099" endWordPosition="3100">or were removed in advance. For the training set, there are totally 645 neutral pronouns and 385 personal pronouns with non-empty candidate set, while for the testing set, the number is 245 and 197. 4.2 The Corpus and the Web The corpus for the predicate-argument statistics computation was from the TIPSTER’s Text Research Collection (v1994). Consisting of 173,252 Wall Street Journal articles from the year 1988 to 1992, the data set contained about 76 million words. The documents were preprocessed using the same POS tagging and NE-recognition components as in the pronoun resolution task. Cass (Abney, 1996), a robust chunker parser was then applied to generate the shallow parse trees, which resulted in 353,085 possessive-noun tuples, 759,997 verb-object tuples and 1,090,121 subject-verb tuples. We examined the capacity of the web and the corpus in terms of zero-count ratio and count number. On average, among the predicate-argument tuples that have non-zero corpus-counts, above 93% have also non-zero web-counts. But the ratio is only around 40% contrariwise. And for the predicate169 Neutral Pron Personal Pron Overall Learning Model System Corpus Web Corpus Web Corpus Web baseline 65.7 86.8 75.1 +</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>S. Abney. 1996. Partial parsing via finite-state cascades. In Workshop on Robust Parsing, 8th European Summer School in Logic, Language and Information, pages 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bean</author>
<author>E Riloff</author>
</authors>
<title>Unsupervised learning of contextual role knowledge for coreference resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of 2004 North American chapter of the Association for Computational Linguistics annual meeting.</booktitle>
<contexts>
<context position="2267" citStr="Bean and Riloff (2004)" startWordPosition="322" endWordPosition="325"> basis that the government said it1 was going to collect it2. For anaphor it1, the candidate government should have higher semantic compatibility than money because government collect is supposed to occur more frequently than money collect in a large corpus. A similar pattern could also be observed for it2. So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems. Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument frequencies. Recently, Bean and Riloff (2004) presented an unsupervised approach to coreference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well us</context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>D. Bean and E. Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. In Proceedings of 2004 North American chapter of the Association for Computational Linguistics annual meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>A Itai</author>
</authors>
<title>Automatic processing of large corpora for the resolution of anahora references.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<pages>330--332</pages>
<contexts>
<context position="1556" citStr="Dagan and Itai (1990)" startWordPosition="212" endWordPosition="215">y incorporated in the twin-candidate learning model and significantly improve the resolution of neutral pronouns. 1 Introduction Semantic compatibility is an important factor for pronoun resolution. Since pronouns, especially neutral pronouns, carry little semantics of their own, the compatibility between an anaphor and its antecedent candidate is commonly evaluated by examining the relationships between the candidate and the anaphor’s context, based on the statistics that the corresponding predicate-argument tuples occur in a particular large corpus. Consider the example given in the work of Dagan and Itai (1990): (1) They know full well that companies held tax money aside for collection later on the basis that the government said it1 was going to collect it2. For anaphor it1, the candidate government should have higher semantic compatibility than money because government collect is supposed to occur more frequently than money collect in a large corpus. A similar pattern could also be observed for it2. So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems. Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It de</context>
</contexts>
<marker>Dagan, Itai, 1990</marker>
<rawString>I. Dagan and A. Itai. 1990. Automatic processing of large corpora for the resolution of anahora references. In Proceedings of the 13th International Conference on Computational Linguistics, pages 330–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>Resolving pronoun references.</title>
<date>1978</date>
<journal>Lingua,</journal>
<pages>44--339</pages>
<contexts>
<context position="13478" citStr="Hobbs, 1978" startWordPosition="2085" endWordPosition="2086">ly for the single-candidate model while **ed feature is only for the twin-candidate mode) computational cost but with high reliability. Table 1 summarizes the features with their respective possible values. The first three features represent the lexical properties of a candidate. The POS properties could indicate whether a candidate refers to a hearerold entity that would have a higher preference to be selected as the antecedent (Strube, 1998). SameSent and NearestNP mark the distance relationships between an anaphor and the candidate, which would significantly affect the candidate selection (Hobbs, 1978). FirstNP aims to capture the salience of the candidate in the local discourse segment. ParalStuct marks whether a candidate and an anaphor have similar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998). Feature StatSem records the statistics-based semantic compatibility computed, from the corpus or the web, by either frequency or probability metric, as described in the previous section. If a candidate is a pronoun, this feature value would be set to that of its closest nominal antecedent. As described, the semantic compatibility of a candidate is c</context>
</contexts>
<marker>Hobbs, 1978</marker>
<rawString>J. Hobbs. 1978. Resolving pronoun references. Lingua, 44:339–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
<author>D Appelt</author>
<author>L Taylor</author>
<author>A Simma</author>
</authors>
<title>The (non)utility of predicate-argument frequencies for pronoun interpretation.</title>
<date>2004</date>
<booktitle>In Proceedings of 2004 North American chapter of the Association for Computational Linguistics annual meeting.</booktitle>
<contexts>
<context position="2561" citStr="Kehler et al. (2004)" startWordPosition="365" endWordPosition="368">ved for it2. So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems. Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument frequencies. Recently, Bean and Riloff (2004) presented an unsupervised approach to coreference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic comp</context>
<context position="25679" citStr="Kehler et al. (2004)" startWordPosition="4068" endWordPosition="4071"> Other combinations The above analysis has exhibited the superiority of the web over the corpus, and the TC model over the SC model. The experimental results also reveal that using the the web-based semantic feature together with the TC model is able to further boost the resolution performance for neutral pronouns. The system with such a Web+TC combination could achieve a high success of 79.2%, defeating all the other possible combinations. Especially, it considerably outperforms (up to 11.5% success) the system with the Corpus+SC combination, which is commonly adopted in previous work (e.g., Kehler et al. (2004)). Personal pronoun resolution vs. Neutral pronoun resolution Interestingly, the statistics-based semantic feature has no effect on the resolution of personal pronouns, as shown in the table 2. We found in the learned decision trees such a feature did not occur (SC) or only occurred in bottom nodes (TC). This should be because personal pronouns have strong restriction on the semantic category (i.e., human) of the candidates. A non-human candidate, even with a high predicate-argument statistics, could Feature Group Isolated Combined SemMag (Web-based) 61.2 61.2 Type+Reflexive 53.1 61.2 ParaStru</context>
</contexts>
<marker>Kehler, Appelt, Taylor, Simma, 2004</marker>
<rawString>A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. In Proceedings of 2004 North American chapter of the Association for Computational Linguistics annual meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keller</author>
<author>M Lapata</author>
</authors>
<title>Using the web to obtain freqencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="3867" citStr="Keller and Lapata, 2003" startWordPosition="557" endWordPosition="560">nowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics web. It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). So far the web has been utilized in nominal anaphora resolution (Modjeska et al., 2003; Poesio et al., 2004) to determine the semantic relation between an anaphor and candidate pair. However, to our knowledge, using the web to help pronoun resolution still remains unexplored. Learning framework. Commonly, the predicateargument statistics is incorporated into anaphora resolution systems as a feature. What kind of learning framework is suitable for this feature? Previous approaches to anaphora resolution adopt the singlecandidate model, in which the resolution is done on an anaphor and one can</context>
<context position="21227" citStr="Keller and Lapata, 2003" startWordPosition="3333" endWordPosition="3336">.0 91.4 92.4 82.8 84.4 Table 2: The performance of different resolution systems Relationship N-Pron P-Pron Possessive-Noun 0.508 0.517 Verb-Object 0.503 0.526 Subject-Verb 0.619 0.676 Table 3: Correlation between web and corpus counts on the seen predicate-argument tuples argument tuples that could be seen in both data sources, the count from the web is above 2000 times larger than that from the corpus. Although much less sparse, the web counts are significantly noisier than the corpus count since no tagging, chunking and parsing could be carried out on the web pages. However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems. In our study we also carried out a correlation analysis3 to examine whether the counts from the web and the corpus are linearly related, on the predicate-argument tuples that can be seen in both data sources. From the results listed in Table 3, we observe moderately high correlation, with coefficients ranging from 0.5 to 0.7 around, between the counts from the web and the corpus, for both neutral pronoun (N-Pron) and personal pronoun (PPron) resolution tasks. 4.3 System Evaluation Table 2 sum</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>F. Keller and M. Lapata. 2003. Using the web to obtain freqencies for unseen bigrams. Computational Linguistics, 29(3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
</authors>
<title>Robust pronoun resolution with limited knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th Int. Conference on Computational Linguistics,</booktitle>
<pages>869--875</pages>
<contexts>
<context position="2921" citStr="Mitkov (1998)" startWordPosition="416" endWordPosition="417">reference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information. We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, p</context>
<context position="13725" citStr="Mitkov, 1998" startWordPosition="2125" endWordPosition="2126">lexical properties of a candidate. The POS properties could indicate whether a candidate refers to a hearerold entity that would have a higher preference to be selected as the antecedent (Strube, 1998). SameSent and NearestNP mark the distance relationships between an anaphor and the candidate, which would significantly affect the candidate selection (Hobbs, 1978). FirstNP aims to capture the salience of the candidate in the local discourse segment. ParalStuct marks whether a candidate and an anaphor have similar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998). Feature StatSem records the statistics-based semantic compatibility computed, from the corpus or the web, by either frequency or probability metric, as described in the previous section. If a candidate is a pronoun, this feature value would be set to that of its closest nominal antecedent. As described, the semantic compatibility of a candidate is computed under the context of the current anaphor. Consider two occurrences of anaphors “... it1 collected... ” and “... it2 said... ”. As “NP collected” should occur less frequently than “NP said”, the candidates of it1 would generally have predic</context>
</contexts>
<marker>Mitkov, 1998</marker>
<rawString>R. Mitkov. 1998. Robust pronoun resolution with limited knowledge. In Proceedings of the 17th Int. Conference on Computational Linguistics, pages 869–875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Modjeska</author>
<author>K Markert</author>
<author>M Nissim</author>
</authors>
<title>Using the web in machine learning for other-anaphora resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="3955" citStr="Modjeska et al., 2003" startWordPosition="572" endWordPosition="575">ata sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics web. It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). So far the web has been utilized in nominal anaphora resolution (Modjeska et al., 2003; Poesio et al., 2004) to determine the semantic relation between an anaphor and candidate pair. However, to our knowledge, using the web to help pronoun resolution still remains unexplored. Learning framework. Commonly, the predicateargument statistics is incorporated into anaphora resolution systems as a feature. What kind of learning framework is suitable for this feature? Previous approaches to anaphora resolution adopt the singlecandidate model, in which the resolution is done on an anaphor and one candidate at a time (Soon et al., 2001; Ng and Cardie, 2002). However, as the purpose of th</context>
</contexts>
<marker>Modjeska, Markert, Nissim, 2003</marker>
<rawString>N. Modjeska, K. Markert, and M. Nissim. 2003. Using the web in machine learning for other-anaphora resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>Philadelphia.</location>
<contexts>
<context position="4524" citStr="Ng and Cardie, 2002" startWordPosition="664" endWordPosition="667"> nominal anaphora resolution (Modjeska et al., 2003; Poesio et al., 2004) to determine the semantic relation between an anaphor and candidate pair. However, to our knowledge, using the web to help pronoun resolution still remains unexplored. Learning framework. Commonly, the predicateargument statistics is incorporated into anaphora resolution systems as a feature. What kind of learning framework is suitable for this feature? Previous approaches to anaphora resolution adopt the singlecandidate model, in which the resolution is done on an anaphor and one candidate at a time (Soon et al., 2001; Ng and Cardie, 2002). However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twincandidate (Yang et al., 2003) that focusses on the preference relationships among candidates. In our work we explore the acquisition of the semantic compatibility information from the corpus and the web, and the incorporation of such semantic information in the single-candidate model and the twin-candidate model. We systematically evaluate the combinations of different </context>
<context position="10940" citStr="Ng and Cardie (2002)" startWordPosition="1661" endWordPosition="1664">bability (Eq. 2) metric. In such a situation, count(candi, ana) is the hit number of the inflected queries returned by the search engine, while count(candi) is the hit number of the query formed with only the head of the candidate (i.e.,“the + candi”). 3 Applying the Semantic Compatibility In this section, we discuss how to incorporate the statistics-based semantic compatibility for pronoun resolution, in a machine learning framework. 3.1 The Single-Candidate Model One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). In such a learning model, each training or testing instance takes the form of i{C, ana}, where ana is the possible anaphor and C is its antecedent candidate. An instance is associated with a feature vector to describe their relationships. During training, for each anaphor in a given text, a positive instance is created by pairing the anaphor and its closest antecedent. Also a set of negative instances is formed by pairing the anaphor and each of the intervening candidates. Based on the training instances, a binary classifier is generated using a certain learning algorithm, like C5 (Quinlan, </context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 104–111, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>R Mehta</author>
<author>A Maroudas</author>
<author>J Hitzeman</author>
</authors>
<title>Learning to resolve bridging references.</title>
<date>2004</date>
<booktitle>In Proceedings of 42th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3977" citStr="Poesio et al., 2004" startWordPosition="576" endWordPosition="579"> That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics web. It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). So far the web has been utilized in nominal anaphora resolution (Modjeska et al., 2003; Poesio et al., 2004) to determine the semantic relation between an anaphor and candidate pair. However, to our knowledge, using the web to help pronoun resolution still remains unexplored. Learning framework. Commonly, the predicateargument statistics is incorporated into anaphora resolution systems as a feature. What kind of learning framework is suitable for this feature? Previous approaches to anaphora resolution adopt the singlecandidate model, in which the resolution is done on an anaphor and one candidate at a time (Soon et al., 2001; Ng and Cardie, 2002). However, as the purpose of the predicate-argument s</context>
</contexts>
<marker>Poesio, Mehta, Maroudas, Hitzeman, 2004</marker>
<rawString>M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004. Learning to resolve bridging references. In Proceedings of 42th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="11545" citStr="Quinlan, 1993" startWordPosition="1765" endWordPosition="1766">ie (2002). In such a learning model, each training or testing instance takes the form of i{C, ana}, where ana is the possible anaphor and C is its antecedent candidate. An instance is associated with a feature vector to describe their relationships. During training, for each anaphor in a given text, a positive instance is created by pairing the anaphor and its closest antecedent. Also a set of negative instances is formed by pairing the anaphor and each of the intervening candidates. Based on the training instances, a binary classifier is generated using a certain learning algorithm, like C5 (Quinlan, 1993) in our work. During resolution, given a new anaphor, a test instance is created for each candidate. This instance is presented to the classifier, which then returns a positive or negative result with a confidence value indicating the likelihood that they are co-referent. The candidate with the highest confidence value would be selected as the antecedent. 3.2 Features In our study we only consider those domainindependent features that could be obtained with low 167 Feature Description DefNp 1 if the candidate is a definite NP; else 0 Pron 1 if the candidate is a pronoun; else 0 NE 1 if the can</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for machine learning. Morgan Kaufmann Publishers, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Soon</author>
<author>H Ng</author>
<author>D Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="2941" citStr="Soon et al. (2001)" startWordPosition="418" endWordPosition="421">ution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information. We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, Ann Ar</context>
<context position="4502" citStr="Soon et al., 2001" startWordPosition="660" endWordPosition="663">as been utilized in nominal anaphora resolution (Modjeska et al., 2003; Poesio et al., 2004) to determine the semantic relation between an anaphor and candidate pair. However, to our knowledge, using the web to help pronoun resolution still remains unexplored. Learning framework. Commonly, the predicateargument statistics is incorporated into anaphora resolution systems as a feature. What kind of learning framework is suitable for this feature? Previous approaches to anaphora resolution adopt the singlecandidate model, in which the resolution is done on an anaphor and one candidate at a time (Soon et al., 2001; Ng and Cardie, 2002). However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twincandidate (Yang et al., 2003) that focusses on the preference relationships among candidates. In our work we explore the acquisition of the semantic compatibility information from the corpus and the web, and the incorporation of such semantic information in the single-candidate model and the twin-candidate model. We systematically evaluate the comb</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. Soon, H. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>C Muller</author>
</authors>
<title>A machine learning approach to pronoun resolution in spoken dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>168--175</pages>
<contexts>
<context position="2967" citStr="Strube and Muller (2003)" startWordPosition="422" endWordPosition="425">he co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information. We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, Ann Arbor, June 2005. c�2005 Ass</context>
</contexts>
<marker>Strube, Muller, 2003</marker>
<rawString>M. Strube and C. Muller. 2003. A machine learning approach to pronoun resolution in spoken dialogue. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 168–175, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
</authors>
<title>Never look back: An alternative to centering.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th Int. Conference on Computational Linguistics and 36th Annual Meeting of ACL,</booktitle>
<pages>1251--1257</pages>
<contexts>
<context position="13313" citStr="Strube, 1998" startWordPosition="2062" endWordPosition="2063">the candidate SemMag** the semantic compatibility difference between two competing candidates Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model while **ed feature is only for the twin-candidate mode) computational cost but with high reliability. Table 1 summarizes the features with their respective possible values. The first three features represent the lexical properties of a candidate. The POS properties could indicate whether a candidate refers to a hearerold entity that would have a higher preference to be selected as the antecedent (Strube, 1998). SameSent and NearestNP mark the distance relationships between an anaphor and the candidate, which would significantly affect the candidate selection (Hobbs, 1978). FirstNP aims to capture the salience of the candidate in the local discourse segment. ParalStuct marks whether a candidate and an anaphor have similar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998). Feature StatSem records the statistics-based semantic compatibility computed, from the corpus or the web, by either frequency or probability metric, as described in the previous section.</context>
</contexts>
<marker>Strube, 1998</marker>
<rawString>M. Strube. 1998. Never look back: An alternative to centering. In Proceedings of the 17th Int. Conference on Computational Linguistics and 36th Annual Meeting of ACL, pages 1251– 1257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>G Zhou</author>
<author>J Su</author>
<author>C Tan</author>
</authors>
<title>Coreference resolution using competition learning approach.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="4781" citStr="Yang et al., 2003" startWordPosition="706" endWordPosition="709">k. Commonly, the predicateargument statistics is incorporated into anaphora resolution systems as a feature. What kind of learning framework is suitable for this feature? Previous approaches to anaphora resolution adopt the singlecandidate model, in which the resolution is done on an anaphor and one candidate at a time (Soon et al., 2001; Ng and Cardie, 2002). However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twincandidate (Yang et al., 2003) that focusses on the preference relationships among candidates. In our work we explore the acquisition of the semantic compatibility information from the corpus and the web, and the incorporation of such semantic information in the single-candidate model and the twin-candidate model. We systematically evaluate the combinations of different statistics sources and learning frameworks in terms of their effectiveness in helping the resolution. Results on the MUC data set show that for neutral pronoun resolution in which an anaphor has no specific semantic category, the web-based semantic informat</context>
<context position="15219" citStr="Yang et al. (2003)" startWordPosition="2363" endWordPosition="2366">salience in the resulting classifier. One way to tackle this problem is to normalize the feature by the frequencies of the anaphor’s context, e.g., “count(collected)” and “count(said)”. This, however, would require extra calculation. In fact, as candidates of a specific anaphor share the same anaphor context, we can just normalize the semantic feature of a candidate by that of its competitor: StatSem(C, ana) StatSem�(C, ana) = The value (0 — 1) represents the rank of the semantic compatibility of the candidate C among candi set(ana), the current candidates of ana. 3.3 The Twin-Candidate Model Yang et al. (2003) proposed an alternative twincandidate model for anaphora resolution task. The strength of such a model is that unlike the singlecandidate model, it could capture the preference relationships between competing candidates. In the model, candidates for an anaphor are paired and features from two competing candidates are put together for consideration. This property could nicely deal with the above mentioned training problem of different anaphor contexts, because the semantic feature would be considered under the current candidate set only. In fact, as semantic compatibility is max StatSem(ci, an</context>
<context position="23640" citStr="Yang et al., 2003" startWordPosition="3726" endWordPosition="3729">ed significantly by around 4.9% and 5.3%, respectively. A similar pattern of improvement could be seen for the corpus-based semantic feature. However, the increase is not as large as using the web-based feature: Under the two learning models, the success rate of the best system with the corpus-based feature rises by up to 2.0% and 2.8% respectively, about 2.9% and 2.5% less than that of the counterpart systems with the web-based feature. The larger size and the better counts of the web against the corpus, as reported in Section 4.2, 4The improvement against SC is higher than that reported in (Yang et al., 2003). It should be because we now used 150 training documents rather than 30 ones as in the previous work. The TC model would benefit from larger training data set as it uses more features (more than double) than SC. 170 should contribute to the better performance. Single-candidate model vs. Twin-Candidate model The difference between the SC and the TC model is obvious from the table. For the N-Pron and P-Pron resolution, the systems under TC could outperform the counterpart systems under SC by above 5% and 8% success, respectively. In addition, the utility of the statistics-based semantic feature</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference resolution using competition learning approach. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Su</author>
</authors>
<title>Named Entity recognition using a HMM-based chunk tagger.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="18807" citStr="Zhou and Su, 2002" startWordPosition="2956" endWordPosition="2959">d based on success, the ratio of the number of correctly resolved anaphors over the total number of anaphors. An input raw text was preprocessed automatically by a pipeline of NLP components. The noun phrase identification and the predicate-argument extraction were done based on the results of a chunk tagger, which was trained for the shared task of CoNLL-2000 and achieved 92% accuracy (Zhou et al., 2000). The recognition of NEs as well as their semantic categories was done by a HMM based NER, which was trained for the MUC NE task and obtained high F-scores of 96.9% (MUC-6) and 94.3% (MUC-7) (Zhou and Su, 2002). For each anaphor, the markables occurring within the current and previous two sentences were taken as the initial candidates. Those with mismatched number and gender agreements were filtered from the candidate set. Also, pronouns or NEs that disagreed in person with the anaphor were removed in advance. For the training set, there are totally 645 neutral pronouns and 385 personal pronouns with non-empty candidate set, while for the testing set, the number is 245 and 197. 4.2 The Corpus and the Web The corpus for the predicate-argument statistics computation was from the TIPSTER’s Text Researc</context>
</contexts>
<marker>Zhou, Su, 2002</marker>
<rawString>G. Zhou and J. Su. 2002. Named Entity recognition using a HMM-based chunk tagger. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Su</author>
<author>T Tey</author>
</authors>
<title>Hybrid text chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th Conference on Computational Natural Language Learning,</booktitle>
<pages>163--166</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="18597" citStr="Zhou et al., 2000" startWordPosition="2917" endWordPosition="2920">ta set, while the testing was on the 50 formal-test documents of MUC-6 (30) and MUC-7 (20). Throughout the experiments, default learning parameters were applied to the C5 algorithm. The performance was evaluated based on success, the ratio of the number of correctly resolved anaphors over the total number of anaphors. An input raw text was preprocessed automatically by a pipeline of NLP components. The noun phrase identification and the predicate-argument extraction were done based on the results of a chunk tagger, which was trained for the shared task of CoNLL-2000 and achieved 92% accuracy (Zhou et al., 2000). The recognition of NEs as well as their semantic categories was done by a HMM based NER, which was trained for the MUC NE task and obtained high F-scores of 96.9% (MUC-6) and 94.3% (MUC-7) (Zhou and Su, 2002). For each anaphor, the markables occurring within the current and previous two sentences were taken as the initial candidates. Those with mismatched number and gender agreements were filtered from the candidate set. Also, pronouns or NEs that disagreed in person with the anaphor were removed in advance. For the training set, there are totally 645 neutral pronouns and 385 personal pronou</context>
</contexts>
<marker>Zhou, Su, Tey, 2000</marker>
<rawString>G. Zhou, J. Su, and T. Tey. 2000. Hybrid text chunking. In Proceedings of the 4th Conference on Computational Natural Language Learning, pages 163–166, Lisbon, Portugal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>