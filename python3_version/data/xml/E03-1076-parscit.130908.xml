<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014509">
<title confidence="0.994864">
Empirical Methods for Compound Splitting
</title>
<author confidence="0.995707">
Philipp Koehn
</author>
<affiliation confidence="0.997892333333333">
Information Sciences Institute
Department of Computer Science
University of Southern California
</affiliation>
<email confidence="0.997038">
koehn@isi.edu
</email>
<author confidence="0.997162">
Kevin Knight
</author>
<affiliation confidence="0.998296">
Information Sciences Institute
Department of Computer Science
University of Southern California
</affiliation>
<email confidence="0.996541">
knight@isi.edu
</email>
<sectionHeader confidence="0.998586" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.981575454545454">
Compounded words are a challenge for
NLP applications such as machine trans-
lation (MT). We introduce methods to
learn splitting rules from monolingual
and parallel corpora. We evaluate them
against a gold standard and measure
their impact on performance of statisti-
cal MT systems. Results show accuracy
of 99.1% and performance gains for MT
of 0.039 BLEU on a German-English
noun phrase translation task.
</bodyText>
<figureCaption confidence="0.998014">
Figure 1: Splitting options for the German word
</figureCaption>
<figure confidence="0.964837857142857">
Aktionsplan
Aktionsplan
Aktion
actionplan
action plan
Akt ion s plan
act ion plan
</figure>
<sectionHeader confidence="0.987058" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999904394736842">
Compounding of words is common in a number of
languages (German, Dutch, Finnish, Greek, etc.).
Since words may be joined freely, this vastly in-
creases the vocabulary size, leading to sparse data
problems. This poses challenges for a number
of NLP applications such as machine translation,
speech recognition, text classification, information
extraction, or information retrieval.
For machine translation, the splitting of an un-
known compound into its parts enables the transla-
tion of the compound by the translation of its parts.
Take the word Aktionsplan in German (see Fig-
ure 1), which was created by joining the words Ak-
tion and Plan. Breaking up this compound would
assist the translation into English as action plan.
Compound splitting is a well defined compu-
tational linguistics task. One way to define the
goal of compound splitting is to break up foreign
words, so that a one-to-one correspondence to En-
glish can be established. Note that we are looking
for a one-to-one correspondence to English con-
tent words: Say, the preferred translation of Ak-
tionsplan is plan for action. The lack of corre-
spondence for the English word for does not de-
tract from the definition of the task: We would
still like to break up the German compound into
the two parts Aktion and Plan. The insertion of
function words is not our concern.
Ultimately, the purpose of this work is to im-
prove the quality of machine translation systems.
For instance, phrase-based translation systems
[Marcu and Wong, 2002] may recover more eas-
ily from splitting regimes that do not create a
one-to-one translation correspondence. One split-
ting method may mistakenly break up the word
Aktionsplan into the three words Akt, Ion, and
Plan. But if we consistently break up the word
Aktion into Akt and Ion in our training data, such a
</bodyText>
<page confidence="0.996868">
187
</page>
<bodyText confidence="0.9984376">
system will likely learn the translation of the word
pair Akt Ion into the single English word action.
These considerations lead us to three different
objectives and therefore three different evaluation
metrics for the task of compound splitting:
</bodyText>
<listItem confidence="0.9963914">
• One-to-One correspondence
• Translation quality with a word-based trans-
lation system
• Translation quality with a phrase-based trans-
lation system
</listItem>
<bodyText confidence="0.9987932">
For the first objective, we compare the output
of our methods to a manually created gold stan-
dard. For the second and third, we provide differ-
ently prepared training corpora to statistical ma-
chine translation systems.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999974333333333">
While the linguistic properties of compounds are
widely studied [Langer, 1998], there has been only
limited work on empirical methods to split up
compounds for specific applications.
Brown [2002] proposes a approach guided by
a parallel corpus. It is limited to breaking com-
pounds into cognates and words found in a transla-
tion lexicon. This lexicon may also be acquired by
training a statistical machine translation system.
The methods leads to improved text coverage of
an example based machine translation system, but
no results on translation performance are reported.
Monz and de Rijke [2001] and Hedlund et al.
[2001] successfully use lexicon based approaches
to compound splitting for information retrieval.
Compounds are broken into either the smallest or
the biggest words that can be found in a given lex-
icon.
Larson et al. [2000] propose a data-driven
method that combines compound splitting and
word recombination for speech recognition. While
it reduces the number of out-of-vocabulary words,
it does not improve speech recognition accuracy.
Morphological analyzers such as Morphix [Fin-
kler and Neumann, 19981 usually provide a variety
of splitting options and leave it to the subsequent
application to pick the best choice.
</bodyText>
<sectionHeader confidence="0.950641" genericHeader="method">
3 Splitting Options
</sectionHeader>
<bodyText confidence="0.9997064375">
Compounds are created by joining existing words
together. Thus, to enumerate all possible splittings
of a compound, we consider all splits into known
words. Known words are words that exist in a
training corpus, in our case the European parlia-
ment proceedings consisting of 20 million words
of German [Koehn, 2002].
When joining words, filler letters may be in-
serted at the joint. These are called Fugenelemente
in German. Recall the example of Aktionsplan,
where the letter s was inserted between Aktion and
Plan. Since there are no simple rules for when
such letters may be inserted we allow them be-
tween any two words. As fillers we allow s and
es when splitting German words, which covers al-
most all cases. Other transformations at joints in-
clude dropping of letters, such as when Schweigen
and Minute are joined into Schweigeminute, drop-
ping an n. A extensive study of such transforma-
tions is carried out by Langer [1998] for German.
To summarize: We try to cover the entire length
of the compound with known words and fillers be-
tween words. An algorithm to break up words
in such a manner could be implemented using
dynamic programming, but since computational
complexity is not a problem, we employ an ex-
haustive recursive search. To speed up word
matching, we store the known words in a hash
based on the first three letters. Also, we restrict
known words to words of at least length three.
For the word Aktionsplan, we find the following
splitting options:
</bodyText>
<listItem confidence="0.999745">
• aktionsplan
• aktion—plan
• aktions—plan
• akt—ion—plan
</listItem>
<bodyText confidence="0.999849875">
We arrive at these splitting options, since all the
parts — aktionsplan, aktions, aktion, akt, ion, and
plan — have been observed as whole words in the
training corpus.
These splitting options are the basis of our
work. In the following we discuss methods that
pick one of them as the correct splitting of the
compound.
</bodyText>
<page confidence="0.997349">
188
</page>
<sectionHeader confidence="0.99087" genericHeader="method">
4 Frequency Based Metric
</sectionHeader>
<bodyText confidence="0.99978">
The more frequent a word occurs in a training
corpus, the bigger the statistical basis to esti-
mate translation probabilities, and the more likely
the correct translation probability distribution is
learned [Koehn and Knight, 20011. This insight
leads us to define a splitting metric based on word
frequency.
Given the count of words in the corpus, we pick
the split S with the highest geometric mean of
word frequencies of its parts pi (n being the num-
ber of parts):
</bodyText>
<equation confidence="0.9754725">
argmaxs ( 11 count (m)). (1)
p,ES
</equation>
<bodyText confidence="0.998916888888889">
Since this metric is purely defined in terms of
German word frequencies, there is not necessar-
ily a relationship between the selected option and
correspondence to English words. If a compound
occurs more frequently in the text than its parts,
this metric would leave the compound unbroken —
even if it is translated in parts into English.
In fact, this is the case for the example Aktions-
plan. Again, the four options:
</bodyText>
<listItem confidence="0.99722475">
• aktionsplan(852) 852
• aktion(960)—plan(710) —&gt; 825.6
• aktions(5)—plan(710) —&gt; 59.6
• akt(224)—ion(1)—plan(710) 54.2
</listItem>
<bodyText confidence="0.994560727272727">
Behind each part, we indicated its frequency in
parenthesis. On the right side is the geometric
mean score of these frequencies. The score for
the unbroken compound (852) is higher than the
preferred choice (825.6).
On the other hand, a word that has a simple one-
to-one correspondence to English may be broken
into parts that bear little relation to its meaning.
We can illustrate this on the example of Freitag
(English: Friday), which is broken into frei (En-
glish: free) and Tag (English: day):
</bodyText>
<listItem confidence="0.998531">
• frei(885)—tag(1864) 1284.4
• freitag(556) —&gt; 556
</listItem>
<sectionHeader confidence="0.525494" genericHeader="method">
5 Guidance from a Parallel Corpus
</sectionHeader>
<bodyText confidence="0.999981263157895">
As stated earlier, one of our objectives is the split-
ting of compounds into parts that have one-to-one
correspondence to English. One source of infor-
mation about word correspondence is a parallel
corpus: text in a foreign language, accompanied
by translations into English. Usually, such a cor-
pus is provided in form of sentence translation
pairs.
Going through such a corpus, we can check for
each splitting option if its parts have translations in
the English translation of the sentence. In the case
of Aktionsplan we would expect the words action
and plan on the English side, but in case of Frei-
tag we would not expect the words free and day.
This would lead us to break up Aktionsplan, but
not Freitag. See Figure 2 for illustration of this
method.
This approach requires a translation lexicon.
The easiest way to obtain a translation lexicon
is to learn it from a parallel corpus. This can
be done with the toolkit Giza [Al-Onaizan et al.,
1999], which establishes word-alignments for the
sentences in the two languages.
With this translation lexicon we can perform the
method alluded to above: For each German word,
we consider all splitting options. For each split-
ting option, we check if it has translations on the
English side.
To deal with noise in the translation table, we
demand that the translation probability of the En-
glish word given the German word be at least 0.01.
We also allow each English word to be considered
only once: If it is taken as evidence for correspon-
dence to the first part of the compound, it is ex-
cluded as evidence for the other parts. If multiple
options match the English, we select the one(s)
with the most splits and use word frequencies as
the ultimate tie-breaker.
</bodyText>
<subsectionHeader confidence="0.479898">
Second Translation Table
</subsectionHeader>
<bodyText confidence="0.999473333333333">
While this method works well for the examples
Aktionsplan and Freitag, it failed in our experi-
ments for words such as Grundrechte (English:
basic rights). This word should be broken into
the two parts Grund and Rechte. However, Grund
translates usually as reason or foundation. But
</bodyText>
<page confidence="0.993883">
189
</page>
<table confidence="0.871411571428571">
break into known German words
find correspondences
in English translation
with help from
translation lexicon
Akt ion plan
... an action plan to support ...
</table>
<figureCaption confidence="0.9826545">
Figure 2: Acquisition of splitting knowledge from a parallel corpus: The split Aktion—plan is preferred
since it has most coverage with the English (two words overlap)
</figureCaption>
<bodyText confidence="0.9997606">
here we are looking for a translation into the ad-
jective basic or fundamental. Such a translation
only occurs when Grund is used as the first part of
a compound.
To account for this, we build a second transla-
tion lexicon as follows: First, we break up German
words in the parallel corpus with the frequency
method. Then, we train a translation lexicon using
Giza from the parallel corpus with split German
and unchanged English.
Since in this corpus Grund is often broken off
from a compound, we learn the translation table
entry GrundE4basic. By joining the two transla-
tion lexicons, we can apply the same method, but
this time we correctly split Grundrechte.
By splitting all the words on the German side
of the parallel corpus, we acquire a vast amount
of splitting knowledge (for our data, this covers
75,055 different words). This knowledge contains
for instance, that Grundrechte was split up 213
times, and kept together 17 times.
When making splitting decisions for new texts,
we follow the most frequent option based on the
splitting knowledge. If the word has not been seen
before, we use the frequency method as a back-off.
</bodyText>
<sectionHeader confidence="0.909558" genericHeader="method">
6 Limitation on Part-Of-Speech
</sectionHeader>
<bodyText confidence="0.999785586206897">
A typical error of the method presented so far is
that prefixes and suffixes are often split off. For
instance, the word folgenden (English: following)
is broken off into folgen (English: consequences)
and den (English: the). While this is nonsensical,
it is easy to explain: The word the is commonly
found in English sentences, and therefore taken as
evidence for the existence of a translation for den.
Another example for this is the word Voraus-
setzung (English: condition), which is split into
vor and aussetzung. The word vor translates to
many different prepositions, which frequently oc-
cur in English.
To exclude these mistakes, we use informa-
tion about the parts-of-speech of words. We do
not want to break up a compound into parts that
are prepositions or determiners, but only content
words: nouns, adverbs, adjectives, and verbs.
To accomplish this, we tag the German cor-
pus with POS tags using the TnT tagger [Brants,
2000]. We then obtain statistics on the parts-of-
speech of words in the corpus. This allows us
to exclude words based on their POS as possible
parts of compounds. We limit possible parts of
compounds to words that occur most of the time as
one of following POS: ADJA, ADJD, ADV, NN,
NE, PTKNEG, VVFIN, VVIMP, VVINF, VVIZU,
VVPP, VAFIN, VAIMP, VAINF, VAPP, VMFIN,
VMINF, VMPP.
</bodyText>
<sectionHeader confidence="0.99877" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999267">
The training set for the experiments is a corpus
of 650,000 noun phrases and prepositional phrases
(NP/PP). For each German NP/PP, we have a En-
glish translation. This data was extracted from the
Europarl corpus [Koehn, 20021, with the help of a
German and English statistical parser. This limita-
</bodyText>
<page confidence="0.992901">
190
</page>
<table confidence="0.998960714285714">
Method Correct Wrong Metrics
split not not faulty split prec. recall acc.
raw 0 3296 202 0 0 - 0.0% 94.2%
eager 148 2901 3 51 397 24.8% 73.3% 87.1%
frequency based 175 3176 19 8 122 57.4% 86.6% 95.7%
using parallel 180 3270 13 9 27 83.3% 89.1% 98.6%
using parallel and POS 182 3287 18 2 10 93.8% 90.1% 99.1%
</table>
<tableCaption confidence="0.950366">
Table 1: Evaluation of the methods compared against a manual annotated gold standard of splits: Using
knowledge from parallel corpus and part-of-speech information gives the best accuracy (99.1%).
</tableCaption>
<bodyText confidence="0.999918">
tion is purely for computational reasons, since we
expect most compounds to be nouns. An evalua-
tion of full sentences is expected to show similar
results.
We evaluate the performance of the described
methods on a blind test set of 1000 NP/PPs, which
contain 3498 words. Following good engineering
practice, the methods have been developed with a
different development test set. This restrains us
from over-fitting to a specific test set.
</bodyText>
<subsectionHeader confidence="0.935528">
7.1 One-to-one Correspondence
</subsectionHeader>
<bodyText confidence="0.996731520833333">
Recall that our first objective is to break up Ger-
man words into parts that have a one-to-one trans-
lation correspondence to English words. To judge
this, we manually annotated the test set with cor-
rect splits. Given this gold standard, we can eval-
uate the splits proposed by the methods.
The results of this evaluation are given in Ta-
ble 1. The columns in this table mean:
correct split: words that should be split and were
split correctly
correct non: words that should not be split and
were not
wrong not: words that should be split but were
not
wrong faulty split: words that should be split,
were split, but wrongly (either too much or
too little)
wrong split: words that should not be split, but
were
precision: (correct split) / (correct split + wrong
faulty split + wrong superfluous split)
recall: (correct split) / (correct split + wrong
faulty split + wrong not split)
accuracy: (correct) / (correct + wrong)
To briefly review the methods:
raw: unprocessed data with no splits
eager: biggest split, i.e., the split into as many
parts as possible. If multiple biggest splits are
possible, the one with the highest frequency
score is taken.
frequency based: split into most frequent words,
as described in Section 4
using parallel: split guided by splitting knowl-
edge from a parallel corpus, as described in
Section 5
using parallel and POS: as previous, with an ad-
ditional restriction on the POS of split parts,
as described in Section 6
Since we developed our methods to improve
on this metric, it comes as no surprise that the
most sophisticated method that employs splitting
knowledge from a parallel corpus and information
about POS tags proves to be superior with 99.1%
accuracy. Its main remaining source of error is the
lack of training data. For instance, it fails on more
obscure words such as Passagier—auficommen (En-
glish: passenger volume), where even some of the
parts have not been seen in the training corpus.
</bodyText>
<subsectionHeader confidence="0.9971415">
7.2 Translation Quality with Word Based
Machine Translation
</subsectionHeader>
<bodyText confidence="0.9948845">
The immediate purpose of our work is to improve
the performance of statistical machine translation
</bodyText>
<page confidence="0.993736">
191
</page>
<table confidence="0.998751">
Method BLEU
raw 0.291
eager 0.222
frequency based 0.317
using parallel 0.294
using parallel and POS 0.306
</table>
<tableCaption confidence="0.8142766">
Table 2: Evaluation of the methods with a word
based statistical machine translation system (IBM
Model 4). Frequency based splitting is best, the
methods using splitting knowledge from a parallel
corpus also improve over unsplit (raw) data.
</tableCaption>
<bodyText confidence="0.964256242424242">
systems. Hence, we use the splitting methods to
prepare training and testing data to optimize the
performance of such systems.
First, we measured the impact on a word based
statistical machine translation system, the widely
studied IBM Model 4 [Brown et al., 1990], for
which training tools [Al-Onaizan et al., 19991
and decoders [Germann et al., 2001] are freely
available. We trained the system on the 650,000
NP/PPs with the Giza toolkit, and evaluated the
translation quality on the same 1000 NP/PP test
set as in the previous section. Training and testing
data was split consistently in the same way. The
translation accuracy is measured against reference
translations using the BLEU score [Papineni et al.,
2002]. Table 2 displays the results.
Somewhat surprisingly, the frequency based
method leads to better translation quality than the
more accurate methods that take advantage from
knowledge from the parallel corpus. One rea-
son for this is that the system recovers more eas-
ily from words that are split too much than from
words that are not split up sufficiently. Of course,
this has limitations: Eager splitting into as many
parts as possible fares abysmally.
7.3 Translation Quality with Phrase Based
Machine Translation
Compound words violate the bias for one-to-one
word correspondences of word based SMT sys-
tems. This is one of the motivations for phrase
based systems that translate groups of words. One
of such systems is the joint model proposed by
Marcu and Wong [2002]. We trained this sys-
</bodyText>
<table confidence="0.9914955">
Method BLEU
raw 0.305
eager 0.344
frequency based 0.342
using parallel 0.330
using parallel and POS 0.326
</table>
<tableCaption confidence="0.998371">
Table 3: Evaluation of the methods with a phrase
</tableCaption>
<bodyText confidence="0.99243475">
based statistical machine translation system. The
ability to group split words into phrases over-
comes the many mistakes of maximal (eager)
splitting of words and outperforms the more ac-
curate methods.
tem with the different flavors of our training data,
and evaluated the performance as before. Table 3
shows the results.
Here, the eager splitting method that performed
so poorly with the word based SMT system comes
out ahead. The task of deciding the granularity of
good splits is deferred to the phrase based SMT
system, which uses a statistical method to group
phrases and rejoin split words. This turns out to
be even slightly better than the frequency based
method.
</bodyText>
<sectionHeader confidence="0.99091" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999755">
We introduced various methods to split compound
words into parts. Our experimental results demon-
strate that what constitutes the optimal splitting
depends on the intended application. While one
of our method reached 99.1% accuracy compared
against a gold standard of one-to-one correspon-
dences to English, other methods show superior
results in the context of statistical machine trans-
lation. For this application, we could dramatically
improve the translation quality by up to 0.039
points as measured by the BLEU score.
The words resulting from compound splitting
could also be marked as such, and not just treated
as regular words, as they are now. Future machine
translation models that are sensitive to such lin-
guistic clues might benefit even more.
</bodyText>
<sectionHeader confidence="0.998114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9717415">
Al-Onaizan, Y., Curin, J., Jahr, M., Knight,
K., Lafferty, J., Melamed, D., Och, F.-J.,
</reference>
<page confidence="0.990367">
192
</page>
<reference confidence="0.99957723880597">
Purdy, D., Smith, N. A., and Yarowsky, D.
(1999). Statistical machine translation. Tech-
nical report, John Hopkins University Sum-
mer Workshop http://www.clsp.jhu.edu/
ws99/projects/mt/.
Brants, T. (2000). TnT - a statistical part-of-speech
tagger. In Proceedings of the Sixth Applied Nat-
ural Language Processing Conference ANLP.
Brown, P., Cocke, J., Pietra, S. A. D., Pietra, V.
J. D., Jelinek, F., Lafferty, J. D., Mercer, R. L.,
and Rossin, P. (1990). A statistical approach
to machine translation. Computational Linguis-
tics,16(2):76-85.
Brown, R. D. (2002). Corpus-driven splitting of
compound words. In Proceedings of the Ninth
International Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-2002).
Finkler, W. and Neumann, G. (1998). Morphix.
A fast realization of a classification-based ap-
proach to morphology. In 4. osterreichische
Artificial-Intelligence-Tagung. Wiener Work-
shop - Wissensbasierte Sprachverarbeitung.
Germann, U., Jahr, M., Knight, K., Marcu, D., and
Yamada, K. (2001). Fast decoding and optimal
decoding for machine translation. In Proceed-
ings of ACL 39.
Hedlund, T., Keskustalo, H., Pirkola, A., Airio,
E., and Jarvelin, K. (2001). Utaclir @ CLEF
2001 - effects of compound splitting and n-gram
techniques. In Second Workshop of the Cross-
Language Evaluation Forum (CLEF), Revised
Papers.
Koehn, P. (2002). Europarl: A multilingual
corpus for evaluation of machine translation.
Unpublished, http://www.isi.edu/,--,koehn/
publications/europarl/.
Koehn, P. and Knight, K. (2001). Knowl-
edge sources for word-level translation models.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP).
Langer, S. (1998). Zur Morphologie und Seman-
tik von Nominalkomposita. In Tagungsband
der 4. Konferenz zur Verarbeitung natiirlicher
Sprache (KONVENS).
Larson, M., Willett, D., Kohler, J., and Rigoll, G.
(2000). Compound splitting and lexical unit
recombination for improved performance of a
speech recognition system for German parlia-
mentary speeches. In 6th International Confer-
ence on Spoken Language Processing (ICSLP).
Marcu, D. and Wong, W. (2002). A phrase-
based, joint probability model for statistical ma-
chine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Monz, C. and de Rijke, M. (2001). Shallow mor-
phological analysis in monolingual information
retrieval for Dutch, German, and Italian. In Sec-
ond Workshop of the Cross-Language Evalua-
tion Forum (CLEF), Revised Papers.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL).
</reference>
<page confidence="0.9987715">
193
194
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.792079">
<title confidence="0.999731">Empirical Methods for Compound Splitting</title>
<author confidence="0.999485">Philipp Koehn</author>
<affiliation confidence="0.999813">Information Sciences Institute Department of Computer Science University of Southern California</affiliation>
<email confidence="0.999541">koehn@isi.edu</email>
<author confidence="0.99993">Kevin Knight</author>
<affiliation confidence="0.999827">Information Sciences Institute Department of Computer Science University of Southern California</affiliation>
<email confidence="0.99985">knight@isi.edu</email>
<abstract confidence="0.98481485">Compounded words are a challenge for NLP applications such as machine translation (MT). We introduce methods to learn splitting rules from monolingual and parallel corpora. We evaluate them against a gold standard and measure their impact on performance of statistical MT systems. Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task. Splitting options for the German word Aktionsplan Aktionsplan Aktion actionplan action plan Akt ion s plan act ion plan</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Y Al-Onaizan</author>
<author>J Curin</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>J Lafferty</author>
<author>D Melamed</author>
<author>F-J Och</author>
</authors>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, </marker>
<rawString>Al-Onaizan, Y., Curin, J., Jahr, M., Knight, K., Lafferty, J., Melamed, D., Och, F.-J.,</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Purdy</author>
<author>N A Smith</author>
<author>D Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>John Hopkins University Summer Workshop</institution>
<note>http://www.clsp.jhu.edu/ ws99/projects/mt/.</note>
<marker>Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Purdy, D., Smith, N. A., and Yarowsky, D. (1999). Statistical machine translation. Technical report, John Hopkins University Summer Workshop http://www.clsp.jhu.edu/ ws99/projects/mt/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT - a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Applied Natural Language Processing Conference ANLP.</booktitle>
<contexts>
<context position="12480" citStr="Brants, 2000" startWordPosition="2057" endWordPosition="2058">ences, and therefore taken as evidence for the existence of a translation for den. Another example for this is the word Voraussetzung (English: condition), which is split into vor and aussetzung. The word vor translates to many different prepositions, which frequently occur in English. To exclude these mistakes, we use information about the parts-of-speech of words. We do not want to break up a compound into parts that are prepositions or determiners, but only content words: nouns, adverbs, adjectives, and verbs. To accomplish this, we tag the German corpus with POS tags using the TnT tagger [Brants, 2000]. We then obtain statistics on the parts-ofspeech of words in the corpus. This allows us to exclude words based on their POS as possible parts of compounds. We limit possible parts of compounds to words that occur most of the time as one of following POS: ADJA, ADJD, ADV, NN, NE, PTKNEG, VVFIN, VVIMP, VVINF, VVIZU, VVPP, VAFIN, VAIMP, VAINF, VAPP, VMFIN, VMINF, VMPP. 7 Evaluation The training set for the experiments is a corpus of 650,000 noun phrases and prepositional phrases (NP/PP). For each German NP/PP, we have a English translation. This data was extracted from the Europarl corpus [Koeh</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Brants, T. (2000). TnT - a statistical part-of-speech tagger. In Proceedings of the Sixth Applied Natural Language Processing Conference ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P Rossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational</journal>
<pages>16--2</pages>
<contexts>
<context position="16843" citStr="Brown et al., 1990" startWordPosition="2789" endWordPosition="2792">ion 191 Method BLEU raw 0.291 eager 0.222 frequency based 0.317 using parallel 0.294 using parallel and POS 0.306 Table 2: Evaluation of the methods with a word based statistical machine translation system (IBM Model 4). Frequency based splitting is best, the methods using splitting knowledge from a parallel corpus also improve over unsplit (raw) data. systems. Hence, we use the splitting methods to prepare training and testing data to optimize the performance of such systems. First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available. We trained the system on the 650,000 NP/PPs with the Giza toolkit, and evaluated the translation quality on the same 1000 NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate meth</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Rossin, 1990</marker>
<rawString>Brown, P., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Lafferty, J. D., Mercer, R. L., and Rossin, P. (1990). A statistical approach to machine translation. Computational Linguistics,16(2):76-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Brown</author>
</authors>
<title>Corpus-driven splitting of compound words.</title>
<date>2002</date>
<booktitle>In Proceedings of the Ninth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-2002).</booktitle>
<marker>Brown, 2002</marker>
<rawString>Brown, R. D. (2002). Corpus-driven splitting of compound words. In Proceedings of the Ninth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Finkler</author>
<author>G Neumann</author>
</authors>
<title>Morphix. A fast realization of a classification-based approach to morphology.</title>
<date>1998</date>
<booktitle>In 4. osterreichische Artificial-Intelligence-Tagung. Wiener Workshop - Wissensbasierte Sprachverarbeitung.</booktitle>
<contexts>
<context position="4402" citStr="Finkler and Neumann, 1998" startWordPosition="689" endWordPosition="693"> translation system, but no results on translation performance are reported. Monz and de Rijke [2001] and Hedlund et al. [2001] successfully use lexicon based approaches to compound splitting for information retrieval. Compounds are broken into either the smallest or the biggest words that can be found in a given lexicon. Larson et al. [2000] propose a data-driven method that combines compound splitting and word recombination for speech recognition. While it reduces the number of out-of-vocabulary words, it does not improve speech recognition accuracy. Morphological analyzers such as Morphix [Finkler and Neumann, 19981 usually provide a variety of splitting options and leave it to the subsequent application to pick the best choice. 3 Splitting Options Compounds are created by joining existing words together. Thus, to enumerate all possible splittings of a compound, we consider all splits into known words. Known words are words that exist in a training corpus, in our case the European parliament proceedings consisting of 20 million words of German [Koehn, 2002]. When joining words, filler letters may be inserted at the joint. These are called Fugenelemente in German. Recall the example of Aktionsplan, where</context>
</contexts>
<marker>Finkler, Neumann, 1998</marker>
<rawString>Finkler, W. and Neumann, G. (1998). Morphix. A fast realization of a classification-based approach to morphology. In 4. osterreichische Artificial-Intelligence-Tagung. Wiener Workshop - Wissensbasierte Sprachverarbeitung.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Germann</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>K Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL 39.</booktitle>
<contexts>
<context position="16931" citStr="Germann et al., 2001" startWordPosition="2803" endWordPosition="2806">sing parallel and POS 0.306 Table 2: Evaluation of the methods with a word based statistical machine translation system (IBM Model 4). Frequency based splitting is best, the methods using splitting knowledge from a parallel corpus also improve over unsplit (raw) data. systems. Hence, we use the splitting methods to prepare training and testing data to optimize the performance of such systems. First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available. We trained the system on the 650,000 NP/PPs with the Giza toolkit, and evaluated the translation quality on the same 1000 NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate methods that take advantage from knowledge from the parallel corpus. One reason for this is </context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Germann, U., Jahr, M., Knight, K., Marcu, D., and Yamada, K. (2001). Fast decoding and optimal decoding for machine translation. In Proceedings of ACL 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hedlund</author>
<author>H Keskustalo</author>
<author>A Pirkola</author>
<author>E Airio</author>
<author>K Jarvelin</author>
</authors>
<title>effects of compound splitting and n-gram techniques.</title>
<date>2001</date>
<booktitle>Utaclir @ CLEF</booktitle>
<marker>Hedlund, Keskustalo, Pirkola, Airio, Jarvelin, 2001</marker>
<rawString>Hedlund, T., Keskustalo, H., Pirkola, A., Airio, E., and Jarvelin, K. (2001). Utaclir @ CLEF 2001 - effects of compound splitting and n-gram techniques. In Second Workshop of the CrossLanguage Evaluation Forum (CLEF), Revised Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2002</date>
<note>Unpublished, http://www.isi.edu/,--,koehn/ publications/europarl/.</note>
<contexts>
<context position="4852" citStr="Koehn, 2002" startWordPosition="765" endWordPosition="766">educes the number of out-of-vocabulary words, it does not improve speech recognition accuracy. Morphological analyzers such as Morphix [Finkler and Neumann, 19981 usually provide a variety of splitting options and leave it to the subsequent application to pick the best choice. 3 Splitting Options Compounds are created by joining existing words together. Thus, to enumerate all possible splittings of a compound, we consider all splits into known words. Known words are words that exist in a training corpus, in our case the European parliament proceedings consisting of 20 million words of German [Koehn, 2002]. When joining words, filler letters may be inserted at the joint. These are called Fugenelemente in German. Recall the example of Aktionsplan, where the letter s was inserted between Aktion and Plan. Since there are no simple rules for when such letters may be inserted we allow them between any two words. As fillers we allow s and es when splitting German words, which covers almost all cases. Other transformations at joints include dropping of letters, such as when Schweigen and Minute are joined into Schweigeminute, dropping an n. A extensive study of such transformations is carried out by </context>
<context position="13087" citStr="Koehn, 2002" startWordPosition="2161" endWordPosition="2162">2000]. We then obtain statistics on the parts-ofspeech of words in the corpus. This allows us to exclude words based on their POS as possible parts of compounds. We limit possible parts of compounds to words that occur most of the time as one of following POS: ADJA, ADJD, ADV, NN, NE, PTKNEG, VVFIN, VVIMP, VVINF, VVIZU, VVPP, VAFIN, VAIMP, VAINF, VAPP, VMFIN, VMINF, VMPP. 7 Evaluation The training set for the experiments is a corpus of 650,000 noun phrases and prepositional phrases (NP/PP). For each German NP/PP, we have a English translation. This data was extracted from the Europarl corpus [Koehn, 20021, with the help of a German and English statistical parser. This limita190 Method Correct Wrong Metrics split not not faulty split prec. recall acc. raw 0 3296 202 0 0 - 0.0% 94.2% eager 148 2901 3 51 397 24.8% 73.3% 87.1% frequency based 175 3176 19 8 122 57.4% 86.6% 95.7% using parallel 180 3270 13 9 27 83.3% 89.1% 98.6% using parallel and POS 182 3287 18 2 10 93.8% 90.1% 99.1% Table 1: Evaluation of the methods compared against a manual annotated gold standard of splits: Using knowledge from parallel corpus and part-of-speech information gives the best accuracy (99.1%). tion is purely for </context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Koehn, P. (2002). Europarl: A multilingual corpus for evaluation of machine translation. Unpublished, http://www.isi.edu/,--,koehn/ publications/europarl/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Knowledge sources for word-level translation models.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6641" citStr="Koehn and Knight, 2001" startWordPosition="1068" endWordPosition="1071">• aktion—plan • aktions—plan • akt—ion—plan We arrive at these splitting options, since all the parts — aktionsplan, aktions, aktion, akt, ion, and plan — have been observed as whole words in the training corpus. These splitting options are the basis of our work. In the following we discuss methods that pick one of them as the correct splitting of the compound. 188 4 Frequency Based Metric The more frequent a word occurs in a training corpus, the bigger the statistical basis to estimate translation probabilities, and the more likely the correct translation probability distribution is learned [Koehn and Knight, 20011. This insight leads us to define a splitting metric based on word frequency. Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts): argmaxs ( 11 count (m)). (1) p,ES Since this metric is purely defined in terms of German word frequencies, there is not necessarily a relationship between the selected option and correspondence to English words. If a compound occurs more frequently in the text than its parts, this metric would leave the compound unbroken — even if it is translated in parts int</context>
</contexts>
<marker>Koehn, Knight, 2001</marker>
<rawString>Koehn, P. and Knight, K. (2001). Knowledge sources for word-level translation models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Langer</author>
</authors>
<title>Zur Morphologie und Semantik von Nominalkomposita.</title>
<date>1998</date>
<booktitle>In Tagungsband der 4. Konferenz zur Verarbeitung natiirlicher Sprache (KONVENS).</booktitle>
<contexts>
<context position="3358" citStr="Langer, 1998" startWordPosition="530" endWordPosition="531">d action. These considerations lead us to three different objectives and therefore three different evaluation metrics for the task of compound splitting: • One-to-One correspondence • Translation quality with a word-based translation system • Translation quality with a phrase-based translation system For the first objective, we compare the output of our methods to a manually created gold standard. For the second and third, we provide differently prepared training corpora to statistical machine translation systems. 2 Related Work While the linguistic properties of compounds are widely studied [Langer, 1998], there has been only limited work on empirical methods to split up compounds for specific applications. Brown [2002] proposes a approach guided by a parallel corpus. It is limited to breaking compounds into cognates and words found in a translation lexicon. This lexicon may also be acquired by training a statistical machine translation system. The methods leads to improved text coverage of an example based machine translation system, but no results on translation performance are reported. Monz and de Rijke [2001] and Hedlund et al. [2001] successfully use lexicon based approaches to compound</context>
</contexts>
<marker>Langer, 1998</marker>
<rawString>Langer, S. (1998). Zur Morphologie und Semantik von Nominalkomposita. In Tagungsband der 4. Konferenz zur Verarbeitung natiirlicher Sprache (KONVENS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Larson</author>
<author>D Willett</author>
<author>J Kohler</author>
<author>G Rigoll</author>
</authors>
<title>Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches.</title>
<date>2000</date>
<booktitle>In 6th International Conference on Spoken Language Processing (ICSLP).</booktitle>
<marker>Larson, Willett, Kohler, Rigoll, 2000</marker>
<rawString>Larson, M., Willett, D., Kohler, J., and Rigoll, G. (2000). Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches. In 6th International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrasebased, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2339" citStr="Marcu and Wong, 2002" startWordPosition="364" endWordPosition="367">o that a one-to-one correspondence to English can be established. Note that we are looking for a one-to-one correspondence to English content words: Say, the preferred translation of Aktionsplan is plan for action. The lack of correspondence for the English word for does not detract from the definition of the task: We would still like to break up the German compound into the two parts Aktion and Plan. The insertion of function words is not our concern. Ultimately, the purpose of this work is to improve the quality of machine translation systems. For instance, phrase-based translation systems [Marcu and Wong, 2002] may recover more easily from splitting regimes that do not create a one-to-one translation correspondence. One splitting method may mistakenly break up the word Aktionsplan into the three words Akt, Ion, and Plan. But if we consistently break up the word Aktion into Akt and Ion in our training data, such a 187 system will likely learn the translation of the word pair Akt Ion into the single English word action. These considerations lead us to three different objectives and therefore three different evaluation metrics for the task of compound splitting: • One-to-One correspondence • Translati</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Marcu, D. and Wong, W. (2002). A phrasebased, joint probability model for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monz</author>
<author>M de Rijke</author>
</authors>
<title>Shallow morphological analysis in monolingual information retrieval for Dutch, German, and Italian.</title>
<date>2001</date>
<booktitle>In Second Workshop of the</booktitle>
<marker>Monz, de Rijke, 2001</marker>
<rawString>Monz, C. and de Rijke, M. (2001). Shallow morphological analysis in monolingual information retrieval for Dutch, German, and Italian. In Second Workshop of the Cross-Language Evaluation Forum (CLEF), Revised Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="17297" citStr="Papineni et al., 2002" startWordPosition="2862" endWordPosition="2865">e performance of such systems. First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available. We trained the system on the 650,000 NP/PPs with the Giza toolkit, and evaluated the translation quality on the same 1000 NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate methods that take advantage from knowledge from the parallel corpus. One reason for this is that the system recovers more easily from words that are split too much than from words that are not split up sufficiently. Of course, this has limitations: Eager splitting into as many parts as possible fares abysmally. 7.3 Translation Quality with Phrase Based Machine Translation Compound words violate the bias for one-to-one word correspondences of word based S</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>