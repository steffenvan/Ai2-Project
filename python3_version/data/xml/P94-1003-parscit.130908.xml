<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.5345085" genericHeader="abstract">
PRIORITY UNION AND GENERALIZATION
IN DISCOURSE GRAMMARS
</sectionHeader>
<author confidence="0.895151">
Claire Grover, Chris Brew, Suresh Manandhar, Marc Moens
</author>
<affiliation confidence="0.9088455">
HCRC Language Technology Group
The University of Edinburgh
</affiliation>
<address confidence="0.9341975">
2 Buccleuch Place
Edinburgh EH8 9LW, UK
</address>
<email confidence="0.711089">
Internet: C.Grover@ed ac .uk
</email>
<sectionHeader confidence="0.983356" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999901076923077">
We describe an implementation in Carpenter&apos;s ty-
ped feature formalism, ALE, of a discourse gram-
mar of the kind proposed by Scha, Polanyi,
et at. We examine their method for resolving
parallelism-dependent anaphora and show that
there is a coherent feature-structural rendition of
this type of grammar which uses the operations
of priority union and generalization. We describe
an augmentation of the ALE system to encompass
these operations and we show that an appropriate
choice of definition for priority union gives the de-
sired multiple output for examples of vP-ellipsis
which exhibit a strict/sloppy ambiguity.
</bodyText>
<sectionHeader confidence="0.958622" genericHeader="method">
1 Discourse Grammar
</sectionHeader>
<bodyText confidence="0.999537056603773">
Working broadly within the sign-based paradigm
exemplified by HPSG (Pollard and Sag in press)
we have been exploring computational issues for
a discourse level grammar by using the ALE sy-
stem (Carpenter 1993) to implement a discourse
grammar. Our central model of a discourse gram-
mar is the Linguistic Discourse Model (LDA4) most
often associated with Scha, Polanyi, and their co-
workers (Polanyi and Scha 1984, Scha and Polanyi
1988, Priist 1992, and most recently in Priist, Scha
and van den Berg 1994). In LDM rules are defi-
ned which are, in a broad sense, unification gram-
mar rules and which combine discourse constitu-
ent units (Dcus). These are simple clauses whose
syntax and underresolved semantics have been de-
termined by a sentence grammar but whose fully
resolved final form can only be calculated by their
integration into the current discourse and its con-
text. The rules of the discourse grammar act to
establish the rhetorical relations between constitu-
ents and to perform resolution of those anaphors
whose interpretation can be seen as a function of
discourse coherence (as opposed to those whose
interpretation relies on general knowledge).
For illustrative purposes, we focus here on Priist&apos;s
rules for building one particular type of rhetorical
relation, labelled &amp;quot;list&amp;quot; (Priist 1992). His central
thesis is that for Dcus to be combined into a list
they must exhibit a degree of syntactic-semantic
parallelism and that this parallelism will strongly
determine the way in which some kinds of anaphor
are resolved. The clearest example of this is VP-
ellipsis as in (la) but Priist also claims that the
subject and object pronouns in (lb) and (lc) are
parallelism-dependent anaphors when they occur
in list structures and must therefore be resolved to
the corresponding fully referential subject/object
in the first member of the list.
Hannah likes beetles. So does Thomas.
Hannah likes beetles. She also likes
caterpillars.
Hannah likes beetles. Thomas hates
them.
(2) is Priist&apos;s list construction rule. It is intended
to capture the idea that a list can be constructed
out of two Dcus, combined by means of connec-
tives such as and and or. The categories in Priist&apos;s
rules have features associated with them. In (2)
these features are sem (the unresolved semantic
interpretation of the category), consem (the con-
textually resolved semantic interpretation), and
schema (the semantic information that is com-
mon between the daughter categories).
</bodyText>
<figure confidence="0.741417666666667">
(2) list [ sem : R ((Ci 02) n S2 ),
schema: C1 VS2]
DCUI [ sem : Si, consem : Ci]
DCU2 [ sem : RS2, consem ((Ci V S2 ) )]
Conditions:
Ci V 52 is a characteristic generalization of Ci
</figure>
<figureCaption confidence="0.346992">
and S2; RE {and, or, ...}.
</figureCaption>
<bodyText confidence="0.991760888888889">
Priist calls the operation used to calculate the va-
lue for schema the most specific common deno-
minator (mscD, indicated by the symbol q). The
MSCD of C1 and S2 is defined as the most specific
generalization of C1 that can unify with S2. It is
essential that the result should be contentful to a
degree that confirms that the list structure is an
appropriate analysis, and to this end Priist impo-
ses the condition that the value of schema should
</bodyText>
<page confidence="0.998307">
17
</page>
<bodyText confidence="0.999872761904762">
be a characteristic generalization of the informa-
tion contributed by the two daughters. There is
no formal definition of this notion; it would re-
quire knowledge from many sources to determine
whether sufficient informativeness had been achie-
ved. However, assuming that this condition is met,
Priist uses the common information as a source for
resolution of underspecified elements in the second
daughter by encoding as the value of the second
daughter&apos;s consem the unification of the result of
MSCD with its pre-resolved semantics (the formula
((C1 S2) fl S2)). So in Prust&apos;s rule the MSCD
operation plays two distinct roles, first as a test for
parallelism (as the value of the mother&apos;s schema)
and second as a basis for resolution (in the com-
posite operation which is the value of the second
daughter&apos;s consem). There are certain problems
with MSCD which we claim stem from this attempt
to use one operation for two purposes, and our pri-
mary concern is to find alternative means of achie-
ving Priist&apos;s intended analysis.
</bodyText>
<sectionHeader confidence="0.975199" genericHeader="method">
2 An ALE Discourse Grammar
</sectionHeader>
<bodyText confidence="0.99927425">
For our initial exploration into using ALE for dis-
course grammars we have developed a small dis-
course grammar whose lexical items are complete
sentences (to circumvent the need for a sentence
grammar) and which represents the semantic con-
tent of sentences using feature structures of type
event whose sub-types are indicated in the follo-
wing part of the type hierarchy:
</bodyText>
<listItem confidence="0.669348">
(3) event
</listItem>
<bodyText confidence="0.9577584">
agentive
plus_patient prop_att
emot_att action believe assume
like hate kick catch
In addition we have a very simplified semantics of
noun phrases where we encode them as of type
entity with the subtypes indicated below:
Specifications of which features are appropriate for
which type give us the following representations of
the semantic content of the discourse units in (1):
</bodyText>
<figure confidence="0.648740117647059">
(5) Hannah likes beetles
hannah
[AGENT
PATIENT beetle
like
So does Thomas
[ AGENT thomas 1
agentive
She also likes caterpillars
female
[AGENT
PATIENT caterpillar
like
Thomas hates them
{AGENT thomas
PATIENT entity
hate
</figure>
<subsectionHeader confidence="0.994402">
2.1 Calculating Common Ground
</subsectionHeader>
<bodyText confidence="0.967622272727273">
The SCHEMA feature encodes the information that
is common between daughter Dcus and Priist uses
MSCD to calculate this information. A feature-
structural definition of MSCD would return as a
result the most specific feature structure which is
at least as general as its first argument but which
is also unifiable with its second argument. For
the example in (1c), the MSCD operation would be
given the two arguments in (5a) and (5d), and (6)
would be the result.
human I
beetle
We can contrast the MSCD operation with an
operation which is more commonly discussed in
the context of feature-based unification systems,
namely generalization. This takes two feature-
structures as input and returns a feature struc-
ture which represents the common information in
them. Unlike MSCD, generalization is not asym-
metric, i.e. the order in which the arguments are
presented does not affect the result. The genera-
lization of (5a) and (5d) is shown in (7).
</bodyText>
<figure confidence="0.932016111111111">
(6) [ AGENT
PATIENT
emot_att
(4) entity (7) [ AGENT human
animate PATIENT entity
emot_att
human
female male insect
T
</figure>
<footnote confidence="0.890968083333333">
hannah jessy thomas sam brother beetle bee cater-
pillar
It can be seen from this example that the MSCD
result contains more information than the genera-
lization result. Informally we can say that it seems
to reflect the common information between the
two inputs after the parallelism-dependent ana-
phor in the second sentence has been resolved. The
reason it is safe to use MSCD in this context is pre-
cisely because its use in a list structure guarantees
animal
Z/N
</footnote>
<page confidence="0.998618">
18
</page>
<bodyText confidence="0.999543947368421">
that the pronoun in the second sentence will be
resolved to beetle. In fact the result of MSCD in
this case is exactly the result we would get if we
were to perform the generalization of the resolved
sentences and, as a representation of what the two
have in common, it does seem that this is more de-
sirable than the generalization of the pre-resolved
forms.
If we turn to other examples, however, we discover
that MSCD does not always give the best results.
The discourse in (8) must receive a constituent
structure where the second and third clauses are
combined to form a contrast pair and then this
contrast pair combines with the first sentence to
form a list. (Priist has a separate rule to build
contrast pairs but the use of MSCD is the same as
in the list rule.)
Hannah likes ants. Thomas likes bees but
Jessy hates them.
</bodyText>
<equation confidence="0.66501725">
[
AGENT hanna]
PATIENT insect
like
</equation>
<bodyText confidence="0.99944975">
The tree in (9) demonstrates the required struc-
ture and also shows on the mother and interme-
diate nodes what the results of MSCD would be. As
we can see, where elements of the first argument
of MSCD are more specific than the corresponding
elements in the second, then the more specific one
occurs in the result. Here, this has the effect that
the structure [like, AGENT hannah, PATIENT ins-
ect] is somehow claimed to be common ground
between all three constituents even though this is
clearly not the case.
Our solution to this problem is to dispense with
the MSCD operation and to use generalization in-
stead. However, we do propose that generalization
should take inputs whose parallelism dependent
anaphors have already been resolved.&apos; In the case
of the combination of (5a) and (5d), this will give
&apos;As described in the next section, we use priority
union to resolve these anaphors in both lists and con-
trasts. The use of generalization as a step towards
checking that there is sufficient common ground is sub-
sequent to the use of priority union as the resolution
mechanism.
exactly the same result as MSCD gave (i.e. (6)),
but for the example in (8) we will get different re-
sults, as the tree in (10) shows. (Notice that the
representation of the third sentence is one where
the anaphor is resolved.) The resulting generaliza-
tion, [emot_att, AGENT human, PATIENT insect], is
a much more plausible representation of the com-
mon information between the three DCUs than the
results of MSCD.
</bodyText>
<figure confidence="0.9879585">
[AGENT huma]
PATIENT insect
emot_att
1
hannah [ AGENT human
PATIENT bee
emot_att
[AGENT thoma] [AGENT jessd
PATIENT bee PATIENT bee
like hate
</figure>
<subsectionHeader confidence="0.999562">
2.2 Resolution of Parallel Anaphors
</subsectionHeader>
<bodyText confidence="0.99745846875">
We have said that MSCD plays two roles in Priist&apos;s
rules and we have shown how its function in cal-
culating the value of SCHEMA can be better served
by using the generalization operation instead. We
turn now to the composite operation indicated in
(2) by the formula ((C1 ci S2) n S2). This com-
posite operation calculates MSCD and then unifies
it back in with the second of its arguments in or-
der to resolve any parallelism-dependent anaphors
that might occur in the second DCU. In the discus-
sion that follows, we will refer to the first DCU in
the list rule as the source and to the second DCU
as the target (because it contains a parallelism-
dependent anaphor which is the target of our at-
tempt to resolve that anaphor).
In our ALE implementation we replace Priist&apos;s
composite operation by an operation which has oc-
casionally been proposed as an addition to feature-
based unification systems and which is usually re-
ferred to either as default unification or as priority
union.2 Assumptions about the exact definition of
this operation vary but an intuitive description of
it is that it is an operation which takes two feature
structures and produces a result which is a merge
of the information in the two inputs. However,
the information in one of the feature structures is
&amp;quot;strict&amp;quot; and cannot be lost or overridden while the
information in the other is defeasible. The opera-
tion is a kind of union where the information in
the strict structure takes priority over that in the
2See, for example, Bouma (1990), Calder (1990),
Carpenter (1994), Kaplan (1987).
</bodyText>
<figure confidence="0.998302615384615">
[AGENT
PATIENT
like
hannal] [AGENT huma]
ant PATIENT bee
ernot_att
[AGENT thoma] [AGENT jessyl
PATIENT bee PATIENT entity
like hate
(10)
[AGENT
PATIENTike
ant
</figure>
<page confidence="0.992614">
19
</page>
<bodyText confidence="0.9999838">
default structure, hence our preference to refer to
it by the name priority union. Below we demon-
strate the results of priority union for the exam-
ples in (1a)—(1c). Note that the target is the strict
structure and the source is the defeasible one.
</bodyText>
<table confidence="0.998195666666667">
Hannah likes like
Source: like
Target: beetles. So does Thomas.
Priority 5a
Union: 5b
Hannah likes [ AGENT thornas
caterpillars. PATIENT beetle
Source: 5a beetles. She also likes
Target: 5c { AGENT hannah
Priority PATIENT caterpillar
Union: beetles. Thomas hates them.
Hannah likes
</table>
<bodyText confidence="0.9996797">
For these examples priority union gives us exactly
the same results as Priist&apos;s composite operation.
We use a definition of priority union provided by
Carpenter (1994) (although note that his name for
the operation is &amp;quot;credulous default unification&amp;quot;).
It is discussed in more detail in Section 3. The pri-
ority union of a target T and a source S is defined
as a two step process: first calculate a maximal
feature structure S&apos; such that S&apos; E S, and then
unify the new feature structure with T.
This is very similar to Priist&apos;s composite opera-
tion but there is a significant difference, however.
For Priist there is a requirement that there should
always be a unique MSCD since he also uses MSCD
to calculate the common ground as a test for par-
allelism and there must only be one result for that
purpose. By contrast, we have taken Carpenter&apos;s
definition of credulous default unification and this
can return more than one result. We have strong
reasons for choosing this definition even though
Carpenter does define a &amp;quot;skeptical default unifi-
cation&amp;quot; operation which returns only one result.
Our reasons for preferring the credulous version
arise from examples of vP-ellipsis which exhibit an
ambiguity whereby both a &amp;quot;strict&amp;quot; and a &amp;quot;sloppy&amp;quot;
reading are possible. For example, the second sen-
tence in (14) has two possible readings which can
be glossed as &amp;quot;Hannah likes Jessy&apos;s brother&amp;quot; (the
strict reading) and &amp;quot;Hannah likes her own bro-
ther&amp;quot; (the sloppy reading).
</bodyText>
<listItem confidence="0.518282">
(14) Jessy likes her brother. So does Hannah.
</listItem>
<bodyText confidence="0.999897142857143">
The situations where the credulous version of the
operation will return more than one result arise
from structure sharing in the defeasible feature
structure and it turns out that these are exactly
the places where we would need to get more than
one result in order to get the strict/sloppy ambi-
guities. We illustrate below:
</bodyText>
<subsubsectionHeader confidence="0.360759">
(15) Jessy likes her brother. So does Hannah.
</subsubsectionHeader>
<bodyText confidence="0.996604">
Here priority union returns two results, one where
the structure-sharing information in the source has
been preserved and one where it has not. As the
example demonstrates, this gives the two readings
required. By contrast, Carpenter&apos;s skeptical de-
fault unification operation and Priist&apos;s composite
operation return only one result.
</bodyText>
<subsectionHeader confidence="0.99869">
2.3 Higher Order Unification
</subsectionHeader>
<bodyText confidence="0.999969125">
There are similarities between our implementa-
tion of Priist&apos;s grammar and the account of vi&apos;-
ellipsis described by Dalrymple, Shieber and Pe-
reira (1991) (henceforth DSP). DSP gives an
equational characterization of the problem of vi&apos;-
ellipsis where the interpretation of the target
phrase follows from an initial step of solving an
equation with respect to the source phrase. If a
function can be found such that applying that fun-
ction to the source subject results in the source in-
terpretation, then an application of that function
to the target subject will yield the resolved inter-
pretation for the target. The method for solving
such equations is &amp;quot;higher order unification&amp;quot;. (16)
shows all the components of the interpretation of
the example in (11).
</bodyText>
<figure confidence="0.948831135135135">
Source: AGENT
PATIENT
like
ii jessy
{ BROTHER-OF
brother
Target:
Priority
Union:
[ AGENT hannah
agentive
AGENT
PATIENT
like
AGENT
PATIENT
like
ii hannah
[ BROTHER-OF
brother
hannah
[ BROTHER-OF jessy]
brother
Source: 5a
Target: 5d
Priority
Union:
[ AGENT thomas
PATIENT beetle
hate
20
(16) Hannah likes beetles. So does Thomas.
Source: like(hannah, beetle)
Target (T): P(thomas)
Equation: P(hannah) = like(hannah,beetle)
Solution: P = Ax.like(x,beetle)
Apply to T: like(thornas,beetle)
</figure>
<bodyText confidence="0.994487875">
A prerequisite to the DSP procedure is the esta-
blishment of parallelism between source and target
and the identification of parallel subparts. For ex-
ample, for (16) it is necessary both that the two
clauses Hannah likes beetles and So does Thomas
should be parallel and that the element hannah
should be identified as a parallel element. DSP
indicate parallel elements in the source by means
of underlines as shown in (16). An underlined ele-
ment in the source is termed a &apos;primary occur-
rence&apos; and DSP place a constraint on solutions to
equations requiring that primary occurrences be
abstracted. Without the identification of hannah
as a primary occurrence in (16), other equations
deriving from the source might be possible, for ex-
ample (17):
</bodyText>
<equation confidence="0.668642">
(17) a. P(beetle) = like(hannah, beetle)
b. P(like) = like(hannah, beetle)
</equation>
<bodyText confidence="0.997638111111111">
The DSP analysis of our strict/sloppy example in
(14) is shown in (18). The ambiguity follows from
the fact that there are two possible solutions to the
equation on the source: the first solution involves
abstraction of just the primary occurrence of jessy,
while the second solution involves abstraction of
both the primary and the secondary occurrences.
When applied to the target these solutions yield
the two different interpretations:
</bodyText>
<equation confidence="0.902717125">
(18) Jessy likes her brother. So does Hannah.
Source: like(jessy, brother-of (jessy))
Target: P(hannah)
Equation: P(jessy) like(jessy, brother-of (jessy))
So1.1 (Si): P = Ax.like(x, brother-of (jessy))
So1.2 (S2): P = Ax .like(x , brother-of (x))
Apply Si: like(hannah, brother-of (jessy))
Apply S2: like(hannah, brother-of (hannah))
</equation>
<bodyText confidence="0.999404615384616">
DSP claim that a significant attribute of their ac-
count is that they can provide the two readings in
strict/sloppy ambiguities without having to postu-
late ambiguity in the source. They claim this as
a virtue which is matched by few other accounts
of VP-ellipsis. We have shown here, however, that
an account which uses priority union also has no
need to treat the source as ambiguous.
Our results and DSP&apos;s also converge where the
treatment of cascaded ellipsis is concerned. For
the example in (19) both accounts find six rea-
dings although two of these are either extremely
implausible or even impossible.
</bodyText>
<listItem confidence="0.5156905">
(19) John revised his paper before the teacher
did, and Bill did too.
</listItem>
<bodyText confidence="0.998816393939394">
DSP consider ways of reducing the number of
readings and, similarly, we are currently explo-
ring a potential solution whereby some of the re-
entrancies in the source are required to be trans-
mitted to the result of priority union.
There are also similarities between our account
and the DSP account with respect to the esta-
blishment of parallelism. In the DSP analysis the
determination of parallelism is separate from and
a prerequisite to the resolution of ellipsis. Howe-
ver, they do not actually formulate how paralle-
lism is to be determined. In our modification of
Priist&apos;s account we have taken the same step as
DSP in that we separate out the part of the fea-
ture structure used to determine parallelism from
the part used to resolve ellipsis. In the general
spirit of Priist&apos;s analysis, however, we have taken
one step further down the line towards determi-
ning parallelism by postulating that calculating
the generalization of the source and target is a
first step towards showing that parallelism exists.
The further condition that Priist imposes, that the
common ground should be a characteristic genera-
lization, would conclude the establishment of par-
allelism. We are currently not able to define the
notion of characteristic generalization, so like DSP
we do not have enough in our theory to fully imple-
ment the parallelism requirement. In contrast to
the DSP account, however, our feature structural
approach does not involve us having to explicitly
pair up the component parts of source and target,
nor does it require us to distinguish primary from
secondary occurrences.
</bodyText>
<subsectionHeader confidence="0.966079">
2.4 Parallelism
</subsectionHeader>
<bodyText confidence="0.999853692307692">
In the DSP approach to vP-ellipsis and in our ap-
proach too, the emphasis has been on semantic
parallelism. It has often been pointed out, howe-
ver, that there can be an additional requirement of
syntactic parallelism (see for example, Kehler 1993
and Asher 1993). Kehler (1993) provides a use-
ful discussion of the issue and argues convincingly
that whether syntactic parallelism is required de-
pends on the coherence relation involved. As the
examples in (20) and (21) demonstrate, semantic
parallelism is sufficient to establish a relation like
contrast but it is not sufficient for building a co-
herent list.
</bodyText>
<listItem confidence="0.91776075">
(20) The problem was looked into by John, but
no-one else did.
(21) *This problem was looked into by John,
and Bill did too.
</listItem>
<bodyText confidence="0.7963895">
For a list to be well-formed both syntactic and
semantic parallelism are required:
</bodyText>
<page confidence="0.997391">
21
</page>
<bodyText confidence="0.983078347826087">
(22) John looked into this problem, and Bill did
too.
In the light of Kehler&apos;s claims, it would seem that
a more far-reaching implementation of our prio-
rity union account would need to specify how the
constraint of syntactic parallelism might be imple-
mented for those constructions which require it.
An HPSG-style sign, containing as it does all types
of linguistic information within the same feature
structure, would lend itself well to an account of
syntactic parallelism. If we consider that the DTRS
feature in the sign for the source clause contains
the entire parse tree including the node for the
VP which is the syntactic antecedent, then ways
to bring together the source VP and the target be-
gin to suggest themselves. We have at our disposal
both unification to achieve re-entrancy and the op-
tion to use priority union over syntactic subparts
of the sign. In the light of this, we are confident
that it would be possible to articulate a more ela-
borate account of vP-ellipis within our framework
and that priority union would remain the opera-
tion of choice to achieve the resolution.
</bodyText>
<sectionHeader confidence="0.959988" genericHeader="method">
3 Extensions to ALE
</sectionHeader>
<bodyText confidence="0.999977">
In the previous sections we showed that Priist&apos;s
MSCD operation would more appropriately be re-
placed by the related operations of generalization
and priority union. We have added generalization
and priority union to the ALE system and in this
section we discuss our implementation. We have
provided the new operations as a complement to
the definite clause component of ALE. We chose
this route because we wanted to give the gram-
mar writer explicit control of the point at which
the operations were invoked. ALE adopts a sim-
ple PROLOG-like execution strategy rather than
the more sophisticated control schemes of systems
like CUF and TFS (Manandhar 1993). In princi-
ple it might be preferable to allow the very gene-
ral deduction strategies which these other systems
support, since they have the potential to support a
more declarative style of grammar-writing. Unfor-
tunately, priority union is a non-monotonic ope-
ration, and the consequences of embedding such
operations in a system providing for flexible exe-
cution strategies are largely unexplored. At least
at the outset it seems preferable to work within a
framework in which the grammar writer is requi-
red to take some of the responsibility for the order
in which operations are carried out. Ultimately we
would hope that much of this load could be taken
by the system, but as a tool for exploration ALE
certainly suffices.
</bodyText>
<subsectionHeader confidence="0.999">
3.1 Priority Union in ALE
</subsectionHeader>
<bodyText confidence="0.9997275">
We use the following definition of priority union,
based on Carpenter&apos;s definition of credulous de-
</bodyText>
<equation confidence="0.765890666666667">
fault unification:
(23) punion(T,S) = funify(T,S&apos;) I S&apos; S
is maximal such that unify(T,S&apos;) is defined}
</equation>
<bodyText confidence="0.999913222222222">
punion(T,S) computes the priority union of T (tar-
get; the strict feature structure) with S (source;
the defeasible feature structure). This definition
relies on Moshier&apos;s (1988) definition of atomic fea-
ture structures, and on the technical result that
any feature structure can be decomposed into a
unification of a unique set of atomic feature struc-
tures. Our implementation is a simple procedura-
lization of Carpenter&apos;s declarative definition. First
we decompose the default feature structure into a
set of atomic feature structures, then we search for
the maximal subsets required by the definition.
We illustrate our implementation of priority union
in ALE with the example in (15): Source is the de-
fault input, and Target is the strict input. The
hierarchy we assume is the same as shown in (3)
and (4). Information about how features are asso-
ciated with types is as follows:
</bodyText>
<listItem confidence="0.951904125">
• The type agentive introduces the feature AGENT
with range type human.
• The type plus-patient introduces the feature PA-
TIENT with range type human.
• The type brother introduces the feature
BROTHER-OF with range type human.
• The types jessy and hannah introduce no fea-
tures.
</listItem>
<bodyText confidence="0.999882090909091">
In order to show the decomposition into ato-
mic feature structures we need a notation to re-
present paths and types. We show paths like
this: PATIENTIBROTHER-OF and in order to sti-
pulate that the PATIENT feature leads to a struc-
ture of type brother, we include type informa-
tion in this way: (PATIENT/ brother)1(BRoTHER-
OF/human). We introduce a special feature (*)
to allow specification of the top level type of the
structure. The structures in (15) decompose into
the following atomic components.
</bodyText>
<equation confidence="0.966034625">
(24) Default input:
(AGENT/ jessy) (DI)
(PATIENT/ brother) I (BROTHER- OF/jessy) (D2)
AGENT = PATIENT I BROTHER-OF (D3)
(*/like) (D4)
Strict input:
(AGENT/hannah) (Si)
(* 1 agentive) (S2)
</equation>
<bodyText confidence="0.9985315">
Given the type hierarchy the expressions above ex-
pand to the following typed feature structures:
</bodyText>
<page confidence="0.994012">
22
</page>
<figure confidence="0.978794125">
(25)
plus-patient
[AGENT human I
PATIENT entity
like
Strict input:
[ AGENT hannah
agentive
</figure>
<bodyText confidence="0.973073">
We can now carry out the following steps in order
to generate the priority union.
</bodyText>
<listItem confidence="0.9353041">
1. Add (D4) to the strict input. It cannot conflict.
2. Note that it is impossible to add (D1) to the
strict input.
3. Non-deterministically add either (D2) or (D3)
to the strict input.
4. Note that the results are maximal in each case
because it is impossible to add both (D2) and
(D3) without causing a clash between the dis-
joint atomic types hannah and jessy.
5. Assemble the results into feature structures. If
</listItem>
<bodyText confidence="0.710103">
we have added (D3) the result will be (26) and
if we have added (D2) the result will be (27).
</bodyText>
<figure confidence="0.8730589">
(26) Result 1:
AGENT
hannah
PATIENT
like
(27) Result 2:
[ AGENT hannah
PATIENT { BROTHER-OF jessy]
brother
like
</figure>
<bodyText confidence="0.999945638888889">
In order to make this step-by-step description into
an algorithm we have used a breadth-first search
routine with the property that the largest sets are
generated first. We collect answers in the order in
which the search comes upon them and carry out
subsumption checks to ensure that all the answers
which will be returned are maximal. These checks
reduce to checks on subset inclusion, which can be
reasonably efficient with suitable set representati-
ons. Consistency checking is straightforward be-
cause the ALE system manages type information
in a manner which is largely transparent to the
user. Unification of ALE terms is defined in such a
way that if adding a feature to a term results in a
term of a new type, then the representation of the
structure is specialized to reflect this. Since prio-
rity union is non-deterministic we will finish with
a set of maximal consistent subsets. Each of these
subsets can be converted directly into ALE terms
using ALE&apos;s built-in predicate add_to/5. The re-
sulting set of ALE terms is the (disjunctive) result
of priority union.
In general we expect priority union to be a corn-
putationally expensive operation, since we cannot
exclude pathological cases in which the system has
to search an exponential number of subsets in the
search for the maximal consistent elements which
are required. In the light of this it is fortunate
that our current discourse grammars do not re-
quire frequent use of priority union. Because of
the inherent complexity of the task we have fa-
voured correctness and clarity at the possible ex-
pense of efficiency. Once it becomes established
that priority union is a useful operation we can
begin to explore the possibilities for faster imple-
mentations.
</bodyText>
<subsectionHeader confidence="0.989304">
3.2 Generalization in ALE
</subsectionHeader>
<bodyText confidence="0.999785272727273">
The abstract definition of generalization stipulates
that the generalization of two categories is the lar-
gest category which subsumes both of them. Mos-
hier (1988) has shown that generalization can be
defined as the intersection of sets of atomic fea-
ture structures. In the previous section we outli-
ned how an ALE term can be broken up into atomic
feature structures. All that is now required is the
set intersection operation with the addition that
we also need to cater for the possibility that ato-
mic types may have a consistent generalization.
</bodyText>
<listItem confidence="0.996728571428571">
1. For P and Q complex feature structures
Gen(P,Q) =df {Path: C jPath :AEP
and Path : B E Q } where C is the most
specific type which subsumes both A and B.
2. For A and B atomic types Gen(A, B) =df C
where C is the most specific type which subsu-
mes both A and B.
</listItem>
<bodyText confidence="0.999958">
In ALE there is always a unique type for the gene-
ralization. We have made a small extension to the
ALE compiler to generate a table of type genera-
lizations to assist in the (relatively) efficient com-
putation of generalization. To illustrate, we show
how the generalization of the two feature structu-
res in (28) and (29) is calculated.
</bodyText>
<figure confidence="0.916333764705882">
Default input:
[ AGENT jessy
agentive
AGENT human
PATIENT [ BROTHER-OF jessy]
brother
plus-patient
AGENT
PATIENT
II human
[ BROTHER-OF
brother
(S1,S2)
{ BROTHER-OF
brother
23
(28) Hannah likes ants.
[AGENT hannah
PATIENT ant
Hub Priist. Andreas SchOter helped with the imple-
mentation work. The Human Communication Rese-
arch Centre (HCRC) is supported by the Economic
and Social Research Council (UK).
like
(29) Jessy laughs.
[ AGENT jessy
laugh
These decompose into the atomic components
shown in (30) and (31) respectively.
(30) (*/like)
(AGENT/hannah)
(PATIENT/ant)
(31) (*/laugh)
(AGENT jessy)
</figure>
<bodyText confidence="0.9957532">
These have only the AGENT path in common alt-
hough with different values and therefore the ge-
neralization is the feature structure corresponding
to this path but with the generalization of the ato-
mic types hannah and jessy as value:
</bodyText>
<figure confidence="0.475741">
(32) [ AGENT female ]
agentive
</figure>
<sectionHeader confidence="0.990861" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.998916421052632">
In this paper we have reported on an implemen-
tation of a discourse grammar in a sign-based for-
malism, using Carpenter&apos;s Attribute Logic Engine
(ALE). We extended the discourse grammar and
ALE to incorporate the operations of priority union
and generalization, operations which we use for
resolving parallelism dependent anaphoric expres-
sions. We also reported on a resolution mecha-
nism for verb phrase ellipsis which yields sloppy
and strict readings through priority union, and we
claimed some advantages of this approach over the
use of higher-order unification.
The outstanding unsolved problem is that of esta-
blishing parallelism. While we believe that gene-
ralization is an appropriate formal operation to
assist in this, we still stand in dire need of a con-
vincing criterion for judging whether the genera-
lization of two categories is sufficiently informative
to successfully establish parallelism.
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.496809833333333">
This work was supported by the EC-funded project
LRE-61-062 &amp;quot;Towards a Declarative Theory of Dis-
course&amp;quot; and a longer version of the paper is available
in Brew et a/ (1994). We have profited from discus-
sions with Jo Calder, Dick Crouch, Joke Dorrepaal,
Claire Gardent, Janet Hitzeman, David Millward and
</bodyText>
<sectionHeader confidence="0.948586" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856396226415">
Asher, N. (1993) Reference to Abstract Objects in Di-
scourse. Dordrecht: Kluwer.
Bouma, G. (1990) Defaults in Unification Grammar.
In Proceedings of the 28th ACL, pp. 165-172, Uni-
versity of Pittsburgh.
Brew, C. et al (1994) Discourse Representation. De-
liverable B+ of LRE-61-062: Toward a Declarative
Theory of Discourse.
Calder, J. H. R. (1990) An Interpretation of Paradig-
matic Morphology. PhD thesis, Centre for Cognitive
Science, University of Edinburgh.
Carpenter, B. (1993) ALE. The Attribute Logic En-
gine user&apos;s guide, version (3. Laboratory for Com-
putational Linguistics, Carnegie Mellon University,
Pittsburgh, Pa.
Carpenter, B. (1994) Skeptical and credulous default
unification with applications to templates and inhe-
ritance. In T. Briscoe et al, eds., Inheritance, De-
faults, and the Lexicon, pp. 13-37. Cambridge: Cam-
bridge University Press.
Dalrymple, M., S. Shieber and F. Pereira (1991) El-
lipsis and higher-order unification. Linguistics and
Philosophy 14(4), 399-452.
Kaplan, R. M. (1987) Three seductions of computa-
tional psycholinguistics. In P. J. Whitelock et al,
eds., Linguistic Theory and Computer Applications,
pp. 149-188. London: Academic Press.
Kehler, A. (1993) The effect of establishing coherence
in ellipsis and anaphora resolution. In Proceedings
of the 31st ACL, pp. 62-69, Ohio State University.
Manandhar, S. (1993) CUF in context. In J. DOrre, ed.,
Computational Aspects of Constraint-Based Lingui-
stics Description. DYANA-2 Deliverable.
Moshier, D. (1988) Extensions to Unification Gram-
mar for the Description of Programming Languages.
PhD thesis, Department of Mathematics, University
of California, Los Angeles.
Polanyi, L. and R. Scha (1984) A syntactic approach
to discourse semantics. In Proceedings of the 10th
Coling and the 22nd ACL, pp. 413-419, Stanford
University.
Pollard, C. and I. A. Sag (in press) Head-Driven
Phrase Structure Grammar. Chicago, Ill.: Univer-
sity of Chicago Press and CSLI Publications.
Priist, H. (1992) On Discourse Structuring, VP Ana-
phora and Gapping. PhD thesis, Universiteit van
Amsterdam, Amsterdam.
Priist, H., R. Scha and M. van den Berg (1994) Dis-
course grammar and verb phrase anaphora. Lingui-
stics and Philosophy. To appear.
Scha, R. and L. Polanyi (1988) An augmented context
free grammar for discourse. In Proceedings of the
12th Coling, pp. 573-577, Budapest.
</reference>
<page confidence="0.999178">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9894815">PRIORITY UNION AND GENERALIZATION IN DISCOURSE GRAMMARS</title>
<author confidence="0.999349">Claire Grover</author>
<author confidence="0.999349">Chris Brew</author>
<author confidence="0.999349">Suresh Manandhar</author>
<author confidence="0.999349">Marc Moens</author>
<affiliation confidence="0.933955666666667">HCRC Language Technology Group The University of Edinburgh 2 Buccleuch Place</affiliation>
<address confidence="0.967083">Edinburgh EH8 9LW, UK</address>
<note confidence="0.550757">C.Grover@ed .uk</note>
<abstract confidence="0.977282006711412">We describe an implementation in Carpenter&apos;s tyfeature formalism, a discourse grammar of the kind proposed by Scha, Polanyi, at. We their method for resolving parallelism-dependent anaphora and show that there is a coherent feature-structural rendition of this type of grammar which uses the operations union describe an augmentation of the ALE system to encompass these operations and we show that an appropriate choice of definition for priority union gives the desired multiple output for examples of vP-ellipsis which exhibit a strict/sloppy ambiguity. 1 Discourse Grammar Working broadly within the sign-based paradigm by and Sag in press) we have been exploring computational issues for discourse level grammar by using the system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDA4) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha van den Berg 1994). In are defined which are, in a broad sense, unification gramrules and which combine constituunits These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anaphors whose interpretation can be seen as a function of discourse coherence (as opposed to those whose interpretation relies on general knowledge). For illustrative purposes, we focus here on Priist&apos;s rules for building one particular type of rhetorical labelled 1992). His central is that for Dcus to be combined into a they must exhibit a degree of syntactic-semantic parallelism and that this parallelism will strongly determine the way in which some kinds of anaphor resolved. The clearest example of this is VPellipsis as in (la) but Priist also claims that the subject and object pronouns in (lb) and (lc) are parallelism-dependent anaphors when they occur and must therefore be resolved to the corresponding fully referential subject/object the first member of the Hannah likes beetles. So does Thomas. Hannah likes beetles. She also likes caterpillars. Hannah likes beetles. Thomas hates them. is Priist&apos;s rule. It is intended capture the idea that a be constructed out of two Dcus, combined by means of connecsuch as and categories in Priist&apos;s rules have features associated with them. In (2) these features are sem (the unresolved semantic of the category), contextually resolved semantic interpretation), and semantic information that is common between the daughter categories). list [ : R ((Ci n S2 ), schema: C1 VS2] : : Ci] [ sem : RS2, consem V S2 ) )] Conditions: V 52 is characteristic generalization of Ci S2; or, ...}. Priist calls the operation used to calculate the vafor the specific common denoindicated by the symbol C1 and defined as the most specific of C1 that can unify with is essential that the result should be contentful to a that confirms that the is an appropriate analysis, and to this end Priist impothe condition that the value of should 17 a generalization the information contributed by the two daughters. There is no formal definition of this notion; it would require knowledge from many sources to determine whether sufficient informativeness had been achieved. However, assuming that this condition is met, Priist uses the common information as a source for resolution of underspecified elements in the second daughter by encoding as the value of the second daughter&apos;s consem the unification of the result of its pre-resolved semantics (the formula S2) fl S2)). So in Prust&apos;s rule the MSCD operation plays two distinct roles, first as a test for (as the value of the mother&apos;s and second as a basis for resolution (in the composite operation which is the value of the second There are certain problems we claim stem from this attempt to use one operation for two purposes, and our primary concern is to find alternative means of achieving Priist&apos;s intended analysis. 2 An ALE Discourse Grammar our initial exploration into using discourse grammars we have developed a small discourse grammar whose lexical items are complete sentences (to circumvent the need for a sentence grammar) and which represents the semantic content of sentences using feature structures of type sub-types are indicated in the following part of the type hierarchy: agentive plus_patient prop_att emot_att action believe assume like hate kick catch In addition we have a very simplified semantics of noun phrases where we encode them as of type the subtypes indicated below: Specifications of which features are appropriate for which type give us the following representations of the semantic content of the discourse units in (1): Hannah likes beetles hannah like So does Thomas AGENT 1 agentive She also likes caterpillars female PATIENT caterpillar like Thomas hates them hate 2.1 Calculating Common Ground encodes the information that is common between daughter Dcus and Priist uses calculate this information. A featuredefinition of return as a result the most specific feature structure which is at least as general as its first argument but which is also unifiable with its second argument. For example in (1c), the would be given the two arguments in (5a) and (5d), and (6) would be the result. human I beetle can contrast the with an operation which is more commonly discussed in the context of feature-based unification systems, takes two featurestructures as input and returns a feature structure which represents the common information in Unlike is not asymmetric, i.e. the order in which the arguments are presented does not affect the result. The generalization of (5a) and (5d) is shown in (7). (6) [ AGENT PATIENT emot_att (4) entity [ AGENT PATIENT human entity animate emot_att human female male insect T jessy thomas sam brother beetle bee caterpillar can be seen from this example that the result contains more information than the generalization result. Informally we can say that it seems to reflect the common information between the inputs the parallelism-dependent anasecond sentence has been resolved. it is safe to use this context is prebecause its use in a guarantees animal 18 that the pronoun in the second sentence will be to fact the result of this case is exactly the result we would get if we were to perform the generalization of the resolved sentences and, as a representation of what the two have in common, it does seem that this is more desirable than the generalization of the pre-resolved forms. If we turn to other examples, however, we discover not always give the best results. The discourse in (8) must receive a constituent structure where the second and third clauses are to form a and then this combines with the first sentence to a has a separate rule to build but the use of the same as the Hannah likes ants. Thomas likes bees but Jessy hates them. [ like The tree in (9) demonstrates the required structure and also shows on the mother and intermenodes what the results of be. As we can see, where elements of the first argument more specific than the corresponding elements in the second, then the more specific one occurs in the result. Here, this has the effect that structure inssomehow claimed to be common ground between all three constituents even though this is clearly not the case. Our solution to this problem is to dispense with and to use generalization instead. However, we do propose that generalization should take inputs whose parallelism dependent anaphors have already been resolved.&apos; In the case of the combination of (5a) and (5d), this will give &apos;As described in the next section, we use priority to resolve these anaphors in both conuse of generalization as a step towards checking that there is sufficient common ground is subsequent to the use of priority union as the resolution mechanism. the same result as (i.e. but for the example in (8) we will get different results, as the tree in (10) shows. (Notice that the representation of the third sentence is one where the anaphor is resolved.) The resulting generalizaa much more plausible representation of the cominformation between the three the of emot_att 1 [ emot_att like hate 2.2 Resolution of Parallel Anaphors have said that two roles in Priist&apos;s rules and we have shown how its function in calthe value of be better served by using the generalization operation instead. We turn now to the composite operation indicated in by the formula ((C1 ci S2) This comoperation calculates then unifies it back in with the second of its arguments in order to resolve any parallelism-dependent anaphors might occur in the second the discusthat follows, we will refer to the first as the to the second the it contains a parallelismdependent anaphor which is the target of our attempt to resolve that anaphor). our we replace Priist&apos;s composite operation by an operation which has occasionally been proposed as an addition to featurebased unification systems and which is usually reto either as unification Assumptions about the exact definition of this operation vary but an intuitive description of it is that it is an operation which takes two feature structures and produces a result which is a merge of the information in the two inputs. However, the information in one of the feature structures is &amp;quot;strict&amp;quot; and cannot be lost or overridden while the information in the other is defeasible. The operation is a kind of union where the information in the strict structure takes priority over that in the for example, Bouma [AGENT PATIENT like bee ernot_att [AGENT PATIENT like jessyl entity (10) [AGENT ant 19 default structure, hence our preference to refer to it by the name priority union. Below we demonstrate the results of priority union for the examples in (1a)—(1c). Note that the target is the strict structure and the source is the defeasible one. Hannah likes like Source: like Target: beetles. So does Thomas. Priority Union: 5a likes 5b caterpillars. AGENT For these examples priority union gives us exactly the same results as Priist&apos;s composite operation. We use a definition of priority union provided by Carpenter (1994) (although note that his name for the operation is &amp;quot;credulous default unification&amp;quot;). It is discussed in more detail in Section 3. The priunion of a target a source defined as a two step process: first calculate a maximal structure that E then the new feature structure with This is very similar to Priist&apos;s composite operation but there is a significant difference, however. For Priist there is a requirement that there should be a unique he also uses to calculate the common ground as a test for parallelism and there must only be one result for that purpose. By contrast, we have taken Carpenter&apos;s definition of credulous default unification and this can return more than one result. We have strong reasons for choosing this definition even though Carpenter does define a &amp;quot;skeptical default unification&amp;quot; operation which returns only one result. Our reasons for preferring the credulous version arise from examples of vP-ellipsis which exhibit an ambiguity whereby both a &amp;quot;strict&amp;quot; and a &amp;quot;sloppy&amp;quot; reading are possible. For example, the second sentence in (14) has two possible readings which can be glossed as &amp;quot;Hannah likes Jessy&apos;s brother&amp;quot; (the strict reading) and &amp;quot;Hannah likes her own brother&amp;quot; (the sloppy reading). (14) Jessy likes her brother. So does Hannah. The situations where the credulous version of the operation will return more than one result arise from structure sharing in the defeasible feature structure and it turns out that these are exactly the places where we would need to get more than one result in order to get the strict/sloppy ambiguities. We illustrate below: likes So does Hannah. Here priority union returns two results, one where the structure-sharing information in the source has been preserved and one where it has not. As the example demonstrates, this gives the two readings required. By contrast, Carpenter&apos;s skeptical default unification operation and Priist&apos;s composite operation return only one result. 2.3 Higher Order Unification There are similarities between our implementation of Priist&apos;s grammar and the account of vi&apos;ellipsis described by Dalrymple, Shieber and Pereira (1991) (henceforth DSP). DSP gives an characterization of the problem of vi&apos;ellipsis where the interpretation of the target phrase follows from an initial step of solving an equation with respect to the source phrase. If a function can be found such that applying that function to the source subject results in the source interpretation, then an application of that function to the target subject will yield the resolved interpretation for the target. The method for solving such equations is &amp;quot;higher order unification&amp;quot;. (16) shows all the components of the interpretation of the example in (11). Source: AGENT PATIENT like { BROTHER-OF brother Target: Priority Union: [ AGENT hannah agentive AGENT PATIENT like AGENT PATIENT like [ BROTHER-OF brother hannah BROTHER-OF brother Source: 5a Target: 5d Priority Union: AGENT hate 20 (16) Hannah likes beetles. So does Thomas. beetle) (T): P(hannah) = = Ax.like(x,beetle) to T: A prerequisite to the DSP procedure is the establishment of parallelism between source and target and the identification of parallel subparts. For example, for (16) it is necessary both that the two likes beetles does Thomas should be parallel and that the element hannah should be identified as a parallel element. DSP indicate parallel elements in the source by means of underlines as shown in (16). An underlined element in the source is termed a &apos;primary occurrence&apos; and DSP place a constraint on solutions to equations requiring that primary occurrences be Without the identification of as a primary occurrence in (16), other equations deriving from the source might be possible, for example (17): a. = like(hannah, beetle) = like(hannah, beetle) The DSP analysis of our strict/sloppy example in (14) is shown in (18). The ambiguity follows from the fact that there are two possible solutions to the equation on the source: the first solution involves of just the primary occurrence of while the second solution involves abstraction of both the primary and the secondary occurrences. When applied to the target these solutions yield the two different interpretations: (18) Jessy likes her brother. So does Hannah. brother-of (jessy)) Target: P(hannah) like(jessy, brother-of (jessy)) = Ax.like(x, brother-of (jessy)) (S2): = Ax .like(x , brother-of (x)) Si: like(hannah, (jessy)) S2: brother-of (hannah)) DSP claim that a significant attribute of their account is that they can provide the two readings in strict/sloppy ambiguities without having to postulate ambiguity in the source. They claim this as a virtue which is matched by few other accounts of VP-ellipsis. We have shown here, however, that an account which uses priority union also has no need to treat the source as ambiguous. Our results and DSP&apos;s also converge where the treatment of cascaded ellipsis is concerned. For the example in (19) both accounts find six readings although two of these are either extremely implausible or even impossible. (19) John revised his paper before the teacher did, and Bill did too. DSP consider ways of reducing the number of readings and, similarly, we are currently exploring a potential solution whereby some of the reentrancies in the source are required to be transmitted to the result of priority union. There are also similarities between our account and the DSP account with respect to the establishment of parallelism. In the DSP analysis the determination of parallelism is separate from and a prerequisite to the resolution of ellipsis. However, they do not actually formulate how parallelism is to be determined. In our modification of account we have taken the same DSP in that we separate out the part of the feature structure used to determine parallelism from the part used to resolve ellipsis. In the general spirit of Priist&apos;s analysis, however, we have taken one step further down the line towards determining parallelism by postulating that calculating the generalization of the source and target is a first step towards showing that parallelism exists. The further condition that Priist imposes, that the common ground should be a characteristic generalization, would conclude the establishment of parallelism. We are currently not able to define the notion of characteristic generalization, so like DSP we do not have enough in our theory to fully implement the parallelism requirement. In contrast to the DSP account, however, our feature structural approach does not involve us having to explicitly pair up the component parts of source and target, nor does it require us to distinguish primary from secondary occurrences. 2.4 Parallelism the to and in our approach too, the emphasis has been on semantic parallelism. It has often been pointed out, however, that there can be an additional requirement of syntactic parallelism (see for example, Kehler 1993 and Asher 1993). Kehler (1993) provides a useof the issue and argues convincingly that whether syntactic parallelism is required depends on the coherence relation involved. As the examples in (20) and (21) demonstrate, semantic parallelism is sufficient to establish a relation like it is not sufficient for building a co- (20) The problem was looked into by John, but did. (21) *This problem was looked into by John, and Bill did too. a be well-formed both and are required: 21 (22) John looked into this problem, and Bill did too. In the light of Kehler&apos;s claims, it would seem that a more far-reaching implementation of our priority union account would need to specify how the constraint of syntactic parallelism might be implemented for those constructions which require it. containing as it does all types of linguistic information within the same feature structure, would lend itself well to an account of parallelism. If we consider that the feature in the sign for the source clause contains the entire parse tree including the node for the is the syntactic antecedent, then ways bring together the source the target begin to suggest themselves. We have at our disposal both unification to achieve re-entrancy and the option to use priority union over syntactic subparts of the sign. In the light of this, we are confident that it would be possible to articulate a more elaborate account of vP-ellipis within our framework and that priority union would remain the operation of choice to achieve the resolution. 3 Extensions to ALE In the previous sections we showed that Priist&apos;s would more appropriately be replaced by the related operations of generalization and priority union. We have added generalization priority union to the and in this section we discuss our implementation. We have provided the new operations as a complement to definite clause component of chose this route because we wanted to give the grammar writer explicit control of the point at which operations were invoked. a simple PROLOG-like execution strategy rather than the more sophisticated control schemes of systems 1993). In principle it might be preferable to allow the very general deduction strategies which these other systems support, since they have the potential to support a more declarative style of grammar-writing. Unfortunately, priority union is a non-monotonic operation, and the consequences of embedding such operations in a system providing for flexible execution strategies are largely unexplored. At least at the outset it seems preferable to work within a framework in which the grammar writer is required to take some of the responsibility for the order in which operations are carried out. Ultimately we would hope that much of this load could be taken the system, but as a tool for exploration certainly suffices. Union in ALE We use the following definition of priority union, on Carpenter&apos;s definition of credulous default unification: punion(T,S) = funify(T,S&apos;) IS&apos; S is maximal such that unify(T,S&apos;) is defined} punion(T,S) computes the priority union of T (target; the strict feature structure) with S (source; the defeasible feature structure). This definition on Moshier&apos;s (1988) definition of feastructures, the technical result that any feature structure can be decomposed into a unification of a unique set of atomic feature structures. Our implementation is a simple proceduralization of Carpenter&apos;s declarative definition. First we decompose the default feature structure into a set of atomic feature structures, then we search for the maximal subsets required by the definition. We illustrate our implementation of priority union the example in (15): the deinput, and the strict input. The hierarchy we assume is the same as shown in (3) and (4). Information about how features are associated with types is as follows: The type the feature range type The type the feature PArange type human. The type the feature range type The types no features. In order to show the decomposition into atomic feature structures we need a notation to represent paths and types. We show paths like in order to stithat leads to a strucof type include type informain this way: brother)1(BRoTHER- OF/human). We introduce a special feature (*) to allow specification of the top level type of the structure. The structures in (15) decompose into the following atomic components. (24) Default input: (DI) I = PATIENT I BROTHER-OF Strict input: (AGENT/hannah) (Si) 1 agentive) Given the type hierarchy the expressions above expand to the following typed feature structures: 22 (25) plus-patient human I like Strict input: [ AGENT hannah agentive We can now carry out the following steps in order to generate the priority union. 1. Add (D4) to the strict input. It cannot conflict. 2. Note that it is impossible to add (D1) to the strict input. 3. Non-deterministically add either (D2) or (D3) to the strict input. 4. Note that the results are maximal in each case because it is impossible to add both (D2) and (D3) without causing a clash between the disatomic types 5. Assemble the results into feature structures. If have added (D3) the result (26) and if we have added (D2) the result will be (27). (26) Result 1: AGENT hannah PATIENT like (27) Result 2: [ AGENT hannah { BROTHER-OF brother like In order to make this step-by-step description into an algorithm we have used a breadth-first search routine with the property that the largest sets are generated first. We collect answers in the order in which the search comes upon them and carry out subsumption checks to ensure that all the answers which will be returned are maximal. These checks reduce to checks on subset inclusion, which can be reasonably efficient with suitable set representations. Consistency checking is straightforward bethe manages type information in a manner which is largely transparent to the Unification of is defined in such a way that if adding a feature to a term results in a term of a new type, then the representation of the structure is specialized to reflect this. Since priority union is non-deterministic we will finish with a set of maximal consistent subsets. Each of these can be converted directly into ALE&apos;s built-in predicate reset of is the (disjunctive) result of priority union. In general we expect priority union to be a cornputationally expensive operation, since we cannot exclude pathological cases in which the system has to search an exponential number of subsets in the search for the maximal consistent elements which are required. In the light of this it is fortunate that our current discourse grammars do not require frequent use of priority union. Because of the inherent complexity of the task we have favoured correctness and clarity at the possible expense of efficiency. Once it becomes established that priority union is a useful operation we can begin to explore the possibilities for faster implementations. 3.2 Generalization in ALE abstract definition of that the generalization of two categories is the largest category which subsumes both of them. Moshier (1988) has shown that generalization can be intersection of sets of atomic feature structures. In the previous section we outlihow an can be broken up into atomic feature structures. All that is now required is the set intersection operation with the addition that we also need to cater for the possibility that atomic types may have a consistent generalization. For feature structures C jPath where the most type which subsumes both For A types B) the most specific type which subsuboth and is always a unique type for the generalization. We have made a small extension to the to generate a table of type generalizations to assist in the (relatively) efficient computation of generalization. To illustrate, we show how the generalization of the two feature structures in (28) and (29) is calculated. Default input: AGENT agentive AGENT human [ BROTHER-OF brother AGENT PATIENT [ BROTHER-OF brother (S1,S2) brother 23 Hannah ants. PATIENT ant Hub Priist. Andreas SchOter helped with the implementation work. The Human Communication Research Centre (HCRC) is supported by the Economic and Social Research Council (UK). like (29) Jessy laughs. AGENT laugh These decompose into the atomic components shown in (30) and (31) respectively. (30) (AGENT/hannah) (PATIENT/ant) (31) have only the in common although with different values and therefore the generalization is the feature structure corresponding to this path but with the generalization of the atotypes hannah and value: [ AGENT ] agentive 4 Conclusion In this paper we have reported on an implementation of a discourse grammar in a sign-based formalism, using Carpenter&apos;s Attribute Logic Engine (ALE). We extended the discourse grammar and incorporate the operations of priority union and generalization, operations which we use for resolving parallelism dependent anaphoric expressions. We also reported on a resolution mechanism for verb phrase ellipsis which yields sloppy and strict readings through priority union, and we claimed some advantages of this approach over the use of higher-order unification. The outstanding unsolved problem is that of establishing parallelism. While we believe that generalization is an appropriate formal operation to assist in this, we still stand in dire need of a convincing criterion for judging whether the generalization of two categories is sufficiently informative to successfully establish parallelism.</abstract>
<note confidence="0.756896">Acknowledgements This work was supported by the EC-funded project LRE-61-062 &amp;quot;Towards a Declarative Theory of Discourse&amp;quot; and a longer version of the paper is available Brew (1994). We have profited from discussions with Jo Calder, Dick Crouch, Joke Dorrepaal, Claire Gardent, Janet Hitzeman, David Millward and References N. (1993) Reference Abstract Objects Di- Kluwer. Bouma, G. (1990) Defaults in Unification Grammar. of the 28th ACL, 165-172, University of Pittsburgh. C. al Representation. Deliverable B+ of LRE-61-062: Toward a Declarative Theory of Discourse. J. H. R. (1990) An Interpretation Paradig- Morphology. thesis, Centre for Cognitive</note>
<affiliation confidence="0.975244">Science, University of Edinburgh.</affiliation>
<address confidence="0.917129">Carpenter, B. (1993) ALE. The Attribute Logic En-</address>
<email confidence="0.421552">user'sguide,versionforCom-</email>
<affiliation confidence="0.985994">putational Linguistics, Carnegie Mellon University,</affiliation>
<address confidence="0.81726">Pittsburgh, Pa. Carpenter, B. (1994) Skeptical and credulous default</address>
<note confidence="0.791537783783784">unification with applications to templates and inhe- In T. Briscoe et al, eds., Deand the Lexicon, 13-37. Cambridge: Cambridge University Press. Dalrymple, M., S. Shieber and F. Pereira (1991) Eland higher-order unification. 399-452. Kaplan, R. M. (1987) Three seductions of computational psycholinguistics. In P. J. Whitelock et al, Theory and Computer Applications, pp. 149-188. London: Academic Press. Kehler, A. (1993) The effect of establishing coherence ellipsis and anaphora resolution. In the 31st ACL, 62-69, Ohio State University. Manandhar, S. (1993) CUF in context. In J. DOrre, ed., Computational Aspects of Constraint-Based Lingui- Description. Deliverable. D. (1988) to Unification Gramthe Description of Programming Languages. PhD thesis, Department of Mathematics, University of California, Los Angeles. Polanyi, L. and R. Scha (1984) A syntactic approach discourse semantics. In of the 10th 413-419, Stanford University. C. and I. A. Sag (in press) Grammar. Chicago, Ill.: University of Chicago Press and CSLI Publications. H. (1992) Discourse Structuring, VP Anaphora and Gapping. PhD thesis, Universiteit van Amsterdam, Amsterdam. Priist, H., R. Scha and M. van den Berg (1994) Disgrammar and verb phrase anaphora. Linguiappear. Scha, R. and L. Polanyi (1988) An augmented context grammar for discourse. In of the Coling, 573-577, Budapest.</note>
<intro confidence="0.531793">24</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Asher</author>
</authors>
<title>Reference to Abstract Objects in Discourse.</title>
<date>1993</date>
<publisher>Kluwer.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="19977" citStr="Asher 1993" startWordPosition="3313" endWordPosition="3314">so like DSP we do not have enough in our theory to fully implement the parallelism requirement. In contrast to the DSP account, however, our feature structural approach does not involve us having to explicitly pair up the component parts of source and target, nor does it require us to distinguish primary from secondary occurrences. 2.4 Parallelism In the DSP approach to vP-ellipsis and in our approach too, the emphasis has been on semantic parallelism. It has often been pointed out, however, that there can be an additional requirement of syntactic parallelism (see for example, Kehler 1993 and Asher 1993). Kehler (1993) provides a useful discussion of the issue and argues convincingly that whether syntactic parallelism is required depends on the coherence relation involved. As the examples in (20) and (21) demonstrate, semantic parallelism is sufficient to establish a relation like contrast but it is not sufficient for building a coherent list. (20) The problem was looked into by John, but no-one else did. (21) *This problem was looked into by John, and Bill did too. For a list to be well-formed both syntactic and semantic parallelism are required: 21 (22) John looked into this problem, and Bi</context>
</contexts>
<marker>Asher, 1993</marker>
<rawString>Asher, N. (1993) Reference to Abstract Objects in Discourse. Dordrecht: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
</authors>
<title>Defaults in Unification Grammar.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th ACL,</booktitle>
<pages>165--172</pages>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="11623" citStr="Bouma (1990)" startWordPosition="1959" endWordPosition="1960">hich is usually referred to either as default unification or as priority union.2 Assumptions about the exact definition of this operation vary but an intuitive description of it is that it is an operation which takes two feature structures and produces a result which is a merge of the information in the two inputs. However, the information in one of the feature structures is &amp;quot;strict&amp;quot; and cannot be lost or overridden while the information in the other is defeasible. The operation is a kind of union where the information in the strict structure takes priority over that in the 2See, for example, Bouma (1990), Calder (1990), Carpenter (1994), Kaplan (1987). [AGENT PATIENT like hannal] [AGENT huma] ant PATIENT bee ernot_att [AGENT thoma] [AGENT jessyl PATIENT bee PATIENT entity like hate (10) [AGENT PATIENTike ant 19 default structure, hence our preference to refer to it by the name priority union. Below we demonstrate the results of priority union for the examples in (1a)—(1c). Note that the target is the strict structure and the source is the defeasible one. Hannah likes like Source: like Target: beetles. So does Thomas. Priority 5a Union: 5b Hannah likes [ AGENT thornas caterpillars. PATIENT bee</context>
</contexts>
<marker>Bouma, 1990</marker>
<rawString>Bouma, G. (1990) Defaults in Unification Grammar. In Proceedings of the 28th ACL, pp. 165-172, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brew</author>
</authors>
<title>Discourse Representation. Deliverable B+ of LRE-61-062: Toward a Declarative Theory of Discourse.</title>
<date>1994</date>
<marker>Brew, 1994</marker>
<rawString>Brew, C. et al (1994) Discourse Representation. Deliverable B+ of LRE-61-062: Toward a Declarative Theory of Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H R Calder</author>
</authors>
<title>An Interpretation of Paradigmatic Morphology.</title>
<date>1990</date>
<tech>PhD thesis,</tech>
<institution>Centre for Cognitive Science, University of Edinburgh.</institution>
<contexts>
<context position="11638" citStr="Calder (1990)" startWordPosition="1961" endWordPosition="1962">y referred to either as default unification or as priority union.2 Assumptions about the exact definition of this operation vary but an intuitive description of it is that it is an operation which takes two feature structures and produces a result which is a merge of the information in the two inputs. However, the information in one of the feature structures is &amp;quot;strict&amp;quot; and cannot be lost or overridden while the information in the other is defeasible. The operation is a kind of union where the information in the strict structure takes priority over that in the 2See, for example, Bouma (1990), Calder (1990), Carpenter (1994), Kaplan (1987). [AGENT PATIENT like hannal] [AGENT huma] ant PATIENT bee ernot_att [AGENT thoma] [AGENT jessyl PATIENT bee PATIENT entity like hate (10) [AGENT PATIENTike ant 19 default structure, hence our preference to refer to it by the name priority union. Below we demonstrate the results of priority union for the examples in (1a)—(1c). Note that the target is the strict structure and the source is the defeasible one. Hannah likes like Source: like Target: beetles. So does Thomas. Priority 5a Union: 5b Hannah likes [ AGENT thornas caterpillars. PATIENT beetle Source: 5a </context>
</contexts>
<marker>Calder, 1990</marker>
<rawString>Calder, J. H. R. (1990) An Interpretation of Paradigmatic Morphology. PhD thesis, Centre for Cognitive Science, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>ALE. The Attribute Logic Engine user&apos;s guide, version (3.</title>
<date>1993</date>
<institution>Laboratory for Computational Linguistics, Carnegie Mellon University,</institution>
<location>Pittsburgh, Pa.</location>
<contexts>
<context position="1092" citStr="Carpenter 1993" startWordPosition="164" endWordPosition="165">here is a coherent feature-structural rendition of this type of grammar which uses the operations of priority union and generalization. We describe an augmentation of the ALE system to encompass these operations and we show that an appropriate choice of definition for priority union gives the desired multiple output for examples of vP-ellipsis which exhibit a strict/sloppy ambiguity. 1 Discourse Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDA4) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (Dcus). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated b</context>
</contexts>
<marker>Carpenter, 1993</marker>
<rawString>Carpenter, B. (1993) ALE. The Attribute Logic Engine user&apos;s guide, version (3. Laboratory for Computational Linguistics, Carnegie Mellon University, Pittsburgh, Pa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>Skeptical and credulous default unification with applications to templates and inheritance. In</title>
<date>1994</date>
<booktitle>Inheritance, Defaults, and the Lexicon,</booktitle>
<pages>13--37</pages>
<editor>T. Briscoe et al, eds.,</editor>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge:</location>
<contexts>
<context position="11656" citStr="Carpenter (1994)" startWordPosition="1963" endWordPosition="1964">ither as default unification or as priority union.2 Assumptions about the exact definition of this operation vary but an intuitive description of it is that it is an operation which takes two feature structures and produces a result which is a merge of the information in the two inputs. However, the information in one of the feature structures is &amp;quot;strict&amp;quot; and cannot be lost or overridden while the information in the other is defeasible. The operation is a kind of union where the information in the strict structure takes priority over that in the 2See, for example, Bouma (1990), Calder (1990), Carpenter (1994), Kaplan (1987). [AGENT PATIENT like hannal] [AGENT huma] ant PATIENT bee ernot_att [AGENT thoma] [AGENT jessyl PATIENT bee PATIENT entity like hate (10) [AGENT PATIENTike ant 19 default structure, hence our preference to refer to it by the name priority union. Below we demonstrate the results of priority union for the examples in (1a)—(1c). Note that the target is the strict structure and the source is the defeasible one. Hannah likes like Source: like Target: beetles. So does Thomas. Priority 5a Union: 5b Hannah likes [ AGENT thornas caterpillars. PATIENT beetle Source: 5a beetles. She also </context>
</contexts>
<marker>Carpenter, 1994</marker>
<rawString>Carpenter, B. (1994) Skeptical and credulous default unification with applications to templates and inheritance. In T. Briscoe et al, eds., Inheritance, Defaults, and the Lexicon, pp. 13-37. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dalrymple</author>
<author>S Shieber</author>
<author>F Pereira</author>
</authors>
<title>Ellipsis and higher-order unification.</title>
<date>1991</date>
<journal>Linguistics and Philosophy</journal>
<volume>14</volume>
<issue>4</issue>
<pages>399--452</pages>
<marker>Dalrymple, Shieber, Pereira, 1991</marker>
<rawString>Dalrymple, M., S. Shieber and F. Pereira (1991) Ellipsis and higher-order unification. Linguistics and Philosophy 14(4), 399-452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
</authors>
<title>Three seductions of computational psycholinguistics.</title>
<date>1987</date>
<booktitle>Linguistic Theory and Computer Applications,</booktitle>
<pages>149--188</pages>
<editor>In P. J. Whitelock et al, eds.,</editor>
<publisher>Academic Press.</publisher>
<location>London:</location>
<contexts>
<context position="11671" citStr="Kaplan (1987)" startWordPosition="1965" endWordPosition="1966">nification or as priority union.2 Assumptions about the exact definition of this operation vary but an intuitive description of it is that it is an operation which takes two feature structures and produces a result which is a merge of the information in the two inputs. However, the information in one of the feature structures is &amp;quot;strict&amp;quot; and cannot be lost or overridden while the information in the other is defeasible. The operation is a kind of union where the information in the strict structure takes priority over that in the 2See, for example, Bouma (1990), Calder (1990), Carpenter (1994), Kaplan (1987). [AGENT PATIENT like hannal] [AGENT huma] ant PATIENT bee ernot_att [AGENT thoma] [AGENT jessyl PATIENT bee PATIENT entity like hate (10) [AGENT PATIENTike ant 19 default structure, hence our preference to refer to it by the name priority union. Below we demonstrate the results of priority union for the examples in (1a)—(1c). Note that the target is the strict structure and the source is the defeasible one. Hannah likes like Source: like Target: beetles. So does Thomas. Priority 5a Union: 5b Hannah likes [ AGENT thornas caterpillars. PATIENT beetle Source: 5a beetles. She also likes Target: 5</context>
</contexts>
<marker>Kaplan, 1987</marker>
<rawString>Kaplan, R. M. (1987) Three seductions of computational psycholinguistics. In P. J. Whitelock et al, eds., Linguistic Theory and Computer Applications, pp. 149-188. London: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kehler</author>
</authors>
<title>The effect of establishing coherence in ellipsis and anaphora resolution.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st ACL,</booktitle>
<pages>62--69</pages>
<institution>Ohio State University.</institution>
<contexts>
<context position="19961" citStr="Kehler 1993" startWordPosition="3310" endWordPosition="3311">generalization, so like DSP we do not have enough in our theory to fully implement the parallelism requirement. In contrast to the DSP account, however, our feature structural approach does not involve us having to explicitly pair up the component parts of source and target, nor does it require us to distinguish primary from secondary occurrences. 2.4 Parallelism In the DSP approach to vP-ellipsis and in our approach too, the emphasis has been on semantic parallelism. It has often been pointed out, however, that there can be an additional requirement of syntactic parallelism (see for example, Kehler 1993 and Asher 1993). Kehler (1993) provides a useful discussion of the issue and argues convincingly that whether syntactic parallelism is required depends on the coherence relation involved. As the examples in (20) and (21) demonstrate, semantic parallelism is sufficient to establish a relation like contrast but it is not sufficient for building a coherent list. (20) The problem was looked into by John, but no-one else did. (21) *This problem was looked into by John, and Bill did too. For a list to be well-formed both syntactic and semantic parallelism are required: 21 (22) John looked into this</context>
</contexts>
<marker>Kehler, 1993</marker>
<rawString>Kehler, A. (1993) The effect of establishing coherence in ellipsis and anaphora resolution. In Proceedings of the 31st ACL, pp. 62-69, Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Manandhar</author>
</authors>
<title>CUF in context. In</title>
<date>1993</date>
<booktitle>Computational Aspects of Constraint-Based Linguistics Description. DYANA-2 Deliverable.</booktitle>
<editor>J. DOrre, ed.,</editor>
<contexts>
<context position="22303" citStr="Manandhar 1993" startWordPosition="3704" endWordPosition="3705"> Priist&apos;s MSCD operation would more appropriately be replaced by the related operations of generalization and priority union. We have added generalization and priority union to the ALE system and in this section we discuss our implementation. We have provided the new operations as a complement to the definite clause component of ALE. We chose this route because we wanted to give the grammar writer explicit control of the point at which the operations were invoked. ALE adopts a simple PROLOG-like execution strategy rather than the more sophisticated control schemes of systems like CUF and TFS (Manandhar 1993). In principle it might be preferable to allow the very general deduction strategies which these other systems support, since they have the potential to support a more declarative style of grammar-writing. Unfortunately, priority union is a non-monotonic operation, and the consequences of embedding such operations in a system providing for flexible execution strategies are largely unexplored. At least at the outset it seems preferable to work within a framework in which the grammar writer is required to take some of the responsibility for the order in which operations are carried out. Ultimate</context>
</contexts>
<marker>Manandhar, 1993</marker>
<rawString>Manandhar, S. (1993) CUF in context. In J. DOrre, ed., Computational Aspects of Constraint-Based Linguistics Description. DYANA-2 Deliverable.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moshier</author>
</authors>
<title>Extensions to Unification Grammar for the Description of Programming Languages.</title>
<date>1988</date>
<tech>PhD thesis,</tech>
<institution>Department of Mathematics, University of California,</institution>
<location>Los Angeles.</location>
<contexts>
<context position="27899" citStr="Moshier (1988)" startWordPosition="4637" endWordPosition="4639">onsistent elements which are required. In the light of this it is fortunate that our current discourse grammars do not require frequent use of priority union. Because of the inherent complexity of the task we have favoured correctness and clarity at the possible expense of efficiency. Once it becomes established that priority union is a useful operation we can begin to explore the possibilities for faster implementations. 3.2 Generalization in ALE The abstract definition of generalization stipulates that the generalization of two categories is the largest category which subsumes both of them. Moshier (1988) has shown that generalization can be defined as the intersection of sets of atomic feature structures. In the previous section we outlined how an ALE term can be broken up into atomic feature structures. All that is now required is the set intersection operation with the addition that we also need to cater for the possibility that atomic types may have a consistent generalization. 1. For P and Q complex feature structures Gen(P,Q) =df {Path: C jPath :AEP and Path : B E Q } where C is the most specific type which subsumes both A and B. 2. For A and B atomic types Gen(A, B) =df C where C is the</context>
</contexts>
<marker>Moshier, 1988</marker>
<rawString>Moshier, D. (1988) Extensions to Unification Grammar for the Description of Programming Languages. PhD thesis, Department of Mathematics, University of California, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>R Scha</author>
</authors>
<title>A syntactic approach to discourse semantics.</title>
<date>1984</date>
<booktitle>In Proceedings of the 10th Coling and the 22nd ACL,</booktitle>
<pages>413--419</pages>
<institution>Stanford University.</institution>
<contexts>
<context position="1293" citStr="Polanyi and Scha 1984" startWordPosition="195" endWordPosition="198">these operations and we show that an appropriate choice of definition for priority union gives the desired multiple output for examples of vP-ellipsis which exhibit a strict/sloppy ambiguity. 1 Discourse Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDA4) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (Dcus). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anap</context>
</contexts>
<marker>Polanyi, Scha, 1984</marker>
<rawString>Polanyi, L. and R. Scha (1984) A syntactic approach to discourse semantics. In Proceedings of the 10th Coling and the 22nd ACL, pp. 413-419, Stanford University.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Pollard</author>
<author>I A</author>
</authors>
<title>Sag (in press) Head-Driven Phrase Structure Grammar.</title>
<publisher>University of Chicago Press and CSLI Publications.</publisher>
<location>Chicago, Ill.:</location>
<marker>Pollard, A, </marker>
<rawString>Pollard, C. and I. A. Sag (in press) Head-Driven Phrase Structure Grammar. Chicago, Ill.: University of Chicago Press and CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Priist</author>
</authors>
<title>On Discourse Structuring, VP Anaphora and Gapping.</title>
<date>1992</date>
<tech>PhD thesis,</tech>
<institution>Universiteit van</institution>
<location>Amsterdam, Amsterdam.</location>
<contexts>
<context position="1329" citStr="Priist 1992" startWordPosition="203" endWordPosition="204">te choice of definition for priority union gives the desired multiple output for examples of vP-ellipsis which exhibit a strict/sloppy ambiguity. 1 Discourse Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDA4) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (Dcus). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anaphors whose interpretation can be see</context>
</contexts>
<marker>Priist, 1992</marker>
<rawString>Priist, H. (1992) On Discourse Structuring, VP Anaphora and Gapping. PhD thesis, Universiteit van Amsterdam, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Priist</author>
<author>R Scha</author>
<author>M van den Berg</author>
</authors>
<title>Discourse grammar and verb phrase anaphora. Linguistics and Philosophy.</title>
<date>1994</date>
<note>To appear.</note>
<marker>Priist, Scha, van den Berg, 1994</marker>
<rawString>Priist, H., R. Scha and M. van den Berg (1994) Discourse grammar and verb phrase anaphora. Linguistics and Philosophy. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Scha</author>
<author>L Polanyi</author>
</authors>
<title>An augmented context free grammar for discourse.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th Coling,</booktitle>
<pages>573--577</pages>
<location>Budapest.</location>
<contexts>
<context position="1316" citStr="Scha and Polanyi 1988" startWordPosition="199" endWordPosition="202"> show that an appropriate choice of definition for priority union gives the desired multiple output for examples of vP-ellipsis which exhibit a strict/sloppy ambiguity. 1 Discourse Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDA4) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (Dcus). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anaphors whose interpretati</context>
</contexts>
<marker>Scha, Polanyi, 1988</marker>
<rawString>Scha, R. and L. Polanyi (1988) An augmented context free grammar for discourse. In Proceedings of the 12th Coling, pp. 573-577, Budapest.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>