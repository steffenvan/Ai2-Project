<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9983255">
Pivot Language Approach for Phrase-Based Statistical Machine
Translation
</title>
<author confidence="0.976552">
Hua Wu and Haifeng Wang
</author>
<affiliation confidence="0.968873">
Toshiba (China) Research and Development Center
</affiliation>
<address confidence="0.9706965">
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District
Beijing, 100738, China
</address>
<email confidence="0.999598">
{wuhua,wanghaifeng}@rdc.toshiba.com.cn
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945521739131">
This paper proposes a novel method for
phrase-based statistical machine translation
by using pivot language. To conduct trans-
lation between languages Lf and Le with a
small bilingual corpus, we bring in a third
language Lp, which is named the pivot lan-
guage. For Lf-Lp and Lp-Le, there exist
large bilingual corpora. Using only Lf-Lp
and Lp-Le bilingual corpora, we can build a
translation model for Lf-Le. The advantage
of this method lies in that we can perform
translation between Lf and Le even if there
is no bilingual corpus available for this
language pair. Using BLEU as a metric,
our pivot language method achieves an ab-
solute improvement of 0.06 (22.13% rela-
tive) as compared with the model directly
trained with 5,000 Lf-Le sentence pairs for
French-Spanish translation. Moreover, with
a small Lf-Le bilingual corpus available,
our method can further improve the transla-
tion quality by using the additional Lf-Lp
and Lp-Le bilingual corpora.
</bodyText>
<sectionHeader confidence="0.999321" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.975848386363636">
For statistical machine translation (SMT), phrase-
based methods (Koehn et al., 2003; Och and Ney,
2004) and syntax-based methods (Wu, 1997; Al-
shawi et al. 2000; Yamada and Knignt, 2001;
Melamed, 2004; Chiang, 2005; Quick et al., 2005;
Mellebeek et al., 2006) outperform word-based
methods (Brown et al., 1993). These methods need
large bilingual corpora. However, for some lan-
guages pairs, only a small bilingual corpus is
available, which will degrade the performance of
statistical translation systems.
To solve this problem, this paper proposes a
novel method for phrase-based SMT by using a
pivot language. To perform translation between
languages Lf and Le, we bring in a pivot language
Lp, for which there exist large bilingual corpora for
language pairs Lf-Lp and Lp-Le. With the Lf-Lp and
Lp-Le bilingual corpora, we can build a translation
model for Lf-Le by using Lp as the pivot language.
We name the translation model pivot model. The
advantage of this method lies in that we can con-
duct translation between Lf and Le even if there is
no bilingual corpus available for this language pair.
Moreover, if a small corpus is available for Lf-Le,
we build another translation model, which is
named standard model. Then, we build an interpo-
lated model by performing linear interpolation on
the standard model and the pivot model. Thus, the
interpolated model can employ both the small Lf-
Le corpus and the large Lf-Lp and Lp-Le corpora.
We perform experiments on the Europarl corpus
(Koehn, 2005). Using BLEU (Papineni et al., 2002)
as a metric, our method achieves an absolute im-
provement of 0.06 (22.13% relative) as compared
with the standard model trained with 5,000 Lf-Le
sentence pairs for French-Spanish translation. The
translation quality is comparable with that of the
model trained with a bilingual corpus of 30,000 Lf-
Le sentence pairs. Moreover, translation quality is
further boosted by using both the small Lf-Le bilin-
gual corpus and the large Lf-Lp and Lp-Le corpora.
Experimental results on Chinese-Japanese trans-
lation also indicate that our method achieves satis-
factory results using English as the pivot language.
</bodyText>
<page confidence="0.992761">
856
</page>
<note confidence="0.925644">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 856–863,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998521571428571">
The remainder of this paper is organized as fol-
lows. In section 2, we describe the related work.
Section 3 briefly introduces phrase-based SMT.
Section 4 and Section 5 describes our method for
phrase-based SMT using pivot language. We de-
scribe the experimental results in sections 6 and 7.
Lastly, we conclude in section 8.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99980716">
Our method is mainly related to two kinds of
methods: those using pivot language and those
using a small bilingual corpus or scarce resources.
For the first kind, pivot languages are employed
to translate queries in cross-language information
retrieval (CLIR) (Gollins and Sanderson, 2001;
Kishida and Kando, 2003). These methods only
used the available dictionaries to perform word by
word translation. In addition, NTCIR 4 workshop
organized a shared task for CLIR using pivot lan-
guage. Machine translation systems are used to
translate queries into pivot language sentences, and
then into target sentences (Sakai et al., 2004).
Callison-Burch et al. (2006) used pivot lan-
guages for paraphrase extraction to handle the un-
seen phrases for phrase-based SMT. Borin (2000)
and Wang et al. (2006) used pivot languages to
improve word alignment. Borin (2000) used multi-
lingual corpora to increase alignment coverage.
Wang et al. (2006) induced alignment models by
using two additional bilingual corpora to improve
word alignment quality. Pivot Language methods
were also used for translation dictionary induction
(Schafer and Yarowsky, 2002), word sense disam-
biguation (Diab and Resnik, 2002), and so on.
For the second kind, Niessen and Ney (2004)
used morpho-syntactic information for translation
between language pairs with scarce resources.
Vandeghinste et al. (2006) used translation dic-
tionaries and shallow analysis tools for translation
between the language pair with low resources. A
shared task on word alignment was organized as
part of the ACL 2005 Workshop on Building and
Using Parallel Texts (Martin et al., 2005). This
task focused on languages with scarce resources.
For the subtask of unlimited resources, some re-
searchers (Aswani and Gaizauskas, 2005; Lopez
and Resnik, 2005; Tufis et al., 2005) used lan-
guage-dependent resources such as dictionary, the-
saurus, and dependency parser to improve word
alignment results.
In this paper, we address the translation problem
for language pairs with scarce resources by bring-
ing in a pivot language, via which we can make
use of large bilingual corpora. Our method does
not need language-dependent resources or deep
linguistic processing. Thus, the method is easy to
be adapted to any language pair where a pivot lan-
guage and corresponding large bilingual corpora
are available.
</bodyText>
<sectionHeader confidence="0.99858" genericHeader="method">
3 Phrase-Based SMT
</sectionHeader>
<bodyText confidence="0.9938445">
According to the translation model presented in
(Koehn et al., 2003), given a source sentence f ,
the best target translation ebest can be obtained
according to the following model
</bodyText>
<equation confidence="0.99159932">
e = arg max
best e
(f
e)
LM (e)
|
ω
e p
p
Where the tran
slation model p(f  |e) can be
decomposed into
I I
p(f1 |
I
= ∏ φ (  |) (
f e d a b
−
i i i i −
i=1
φ(f
|ei)
(ai−bi− 1)
pw
i|ei
</equation>
<bodyText confidence="0.812663428571428">
λ is the strength of the lexical weight.
Where
i
and d
denote phrase
translation probability and distortion probability,
respectively.
</bodyText>
<equation confidence="0.7800645">
(f
, a) is the lexical weight,
</equation>
<bodyText confidence="0.954162352941177">
4Phrase-Based SMT Via Pivot Language
This section will introduce the method that per-
forms phrase-based SMT for the language pair Lf-
Le by using the two bilingual corpora of
and
With the two additional bilingual corpora,
we train two translation models for
and
respectively. Based on these two models, we build
a pivot translation model for
with
as a
pivot language.
According to equation (2), the phrase translation
probability and the lexical weight are language
dependent. We will introduce them in sections 4.1
an
</bodyText>
<equation confidence="0.7426225">
(2)
and
Lf-Lp
Lp-Le.
Lf-Lp
Lp-Le,
Lf-Le,
Lp
</equation>
<bodyText confidence="0.603991">
d 4.2, respectively.
</bodyText>
<subsectionHeader confidence="0.994509">
4.1 Phrase Translation Probability
</subsectionHeader>
<bodyText confidence="0.9953725">
Using the Lf-Lp and Lp-Le bilingual corpora, we
train two phrase translation probabilities
</bodyText>
<equation confidence="0.999471">
e f
p (  |)
= arg max
(e)
(1)
length
)
e1
λ
) p
1 w
(fi  |ei, a)
</equation>
<page confidence="0.986512">
857
</page>
<bodyText confidence="0.99896425">
φ(f i  |pi) and φ(pi  |ei), where pi is the phrase
in the pivot language Lp. Given the phrase
translation probabilities φ(f i  |pi) and φ(pi  |ei) ,
we obtain the phrase translation probability
</bodyText>
<equation confidence="0.98077">
φ(f i  |ei) according to the following model.
φff i  |ei)=∑φ(fi  |pi,ei)φ(pi  |ei) (3)
pi
</equation>
<bodyText confidence="0.99800025">
The phrase translation probability φ(f i  |pi , ei )
does not depend on the phase ei in the language Le,
since it is estimated from the Lf-Lp bilingual corpus.
Thus, equation (3) can be rewritten as
</bodyText>
<equation confidence="0.942161">
φ(fi  |ei) = ∑φ(fi  |pi)φ(pi  |ei) (4)
pi
</equation>
<subsectionHeader confidence="0.983071">
4.2 Lexical Weight
</subsectionHeader>
<bodyText confidence="0.999987">
Given a phrase pair (f , e) and a word alignment
a between the source word positions i =1,..., n
and the target word positions j =1,... , m , the
lexical weight can be estimated according to the
following method (Koehn et al., 2003).
</bodyText>
<figureCaption confidence="0.997418">
Figure 1. Alignment Information Induction
</figureCaption>
<bodyText confidence="0.919241">
and a2 represent the word alignment in-
respectively, then the alignment information a
inside (f , e) can be obtained as shown in (6). An
example is shown in Figure 1.
</bodyText>
<equation confidence="0.606672">
a={(f,e)
:
,
)
,e)
a1
|∃p
f
p
∈
1
p
∈
2
</equation>
<bodyText confidence="0.8845144">
om the induced phrase pairs. We
name this method phrase method. If we use K to
denote the number of the induced phrase pairs, we
estimate the co-occurring frequency of the word
pair (f , e) according to the following model.
</bodyText>
<equation confidence="0.990992375">
count
K
n (7)
= ∑ φ f e f f e e
∑ δ δ
k (  |) ( , ) ( , )
i a i
= 1 i = 1
</equation>
<bodyText confidence="0.919897">
Where φk (f  |e) is the phrase translation probabil-
ity for phrase pair k . δ(x, y) =1 if x = y; other-
wise, δ(x, y) = 0 . Thus, lexical translation prob-
ability can be estimated as in (8).
&amp;(
} (6)
With the induced alignment information, this
paper proposes a method to estimate the probabil-
ity directly fr
ability w(f
e) using the method described in
(Wang et al., 2006), which is shown in (9). We
named it word method in this paper.
</bodyText>
<equation confidence="0.926783285714286">
|
w f e
(  |)=∑wf p w p e sim f e p
(  |) (  |) ( , ; ) (9)
p
p) and w(p
e) are two lexical
</equation>
<bodyText confidence="0.959650333333334">
probabilities, and sim(f , e; p) is the cross-
language word similari
|
|
ty.
In order to estimate the lexical weight, we first
need to obtain the alignment information a be-
tween the two phrases f and e , andthen estimate
the lexical translation probability w(f
e) accord-
ing to the alignment information. The alignment
information of the phrase pair (f , e) can be in-
duced fr
|
om the two phrase pairs (f , p) and (p, e) .
</bodyText>
<page confidence="0.79589">
858
</page>
<equation confidence="0.441885">
(8)
</equation>
<bodyText confidence="0.9655755">
We also estimate the lexical translation prob-
Where w(f
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="method">
5 Interpolated Model
</sectionHeader>
<bodyText confidence="0.969145">
If we have a small Lf-Le bilingual corpus, we can
employ this corpus to estimate a translation model
as described in section 3. However, this model may
perform poorly due to the sparseness of the data. In
order to improve its performance, we can employ
the additional
and
bilingual corpora.
Moreover, we can use more than one pivot lan-
guage to improve the translation performance if the
corresponding bilingual corpora exist. Different
pivot lan
</bodyText>
<equation confidence="0.9570626">
Lf-Lp
Lp-Le
guages may catch different linguistic phe-
p f e a
W (  |,
n 1
= ∏ ∑
j i j a
 |( , ) ∈ ∀ ∈
i = 1 ( , )
</equation>
<bodyText confidence="0.349987">
i j a
</bodyText>
<figure confidence="0.925422821428571">
(5)
w f i e j
(  |)
Let
w f e
(  |) =
&apos;
,
f
∑
count
(
f
&apos;
formation inside the phrase pairs (f, p) and (p, e)
e)
,
count
e
(
f
)
,
e
(
f
)
k
</figure>
<bodyText confidence="0.9975347">
nomena, and improve translation quality for the
desired language pair Lf-Le in different ways.
If we include n pivot languages, n pivot mod-
els can be estimated using the method as described
in section 4. In order to combine these n pivot
models with the standard model trained with the
Lf-Le corpus, we use the linear interpolation
method. The phrase translation probability and the
lexical weight are estimated as shown in (10) and
(11), respectively.
</bodyText>
<equation confidence="0.99676225">
n
φ(  |) α φ (  |)
f e = ∑ i i f e (10)
i=0
n
,, (  |, ) β ,,,
f e a = ∑ ip
i=0
</equation>
<bodyText confidence="0.993425142857143">
Where φ0 (f  |e) and p,,,0 (f  |e, a) denote the
phrase translation probability and lexical weight
trained with the Lf-Le bilingual corpus, respec-
tively. φi (f  |e) and p ,,,i (f  |e, a) ( i =1,... , n ) are
the phrase translation probability and lexical
weight estimated by using the pivot languages. αi
and βi are the interpolation coefficients.
</bodyText>
<sectionHeader confidence="0.965742" genericHeader="method">
6 Experiments on the Europarl Corpus
</sectionHeader>
<subsectionHeader confidence="0.945964">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999879863636363">
A shared task to evaluate machine translation per-
formance was organized as part of the
NAACL/HLT 2006 Workshop on Statistical Ma-
chine Translation (Koehn and Monz, 2006). The
shared task used the Europarl corpus (Koehn,
2005), in which four languages are involved: Eng-
lish, French, Spanish, and German. The shared task
performed translation between English and the
other three languages. In our work, we perform
translation from French to the other three lan-
guages. We select French to Spanish and French to
German translation that are not in the shared task
because we want to use English as the pivot lan-
guage. In general, for most of the languages, there
exist bilingual corpora between these languages
and English since English is an internationally
used language.
Table 1 shows the information about the bilin-
gual training data. In the table, &amp;quot;Fr&amp;quot;, &amp;quot;En&amp;quot;, &amp;quot;Es&amp;quot;,
and &amp;quot;De&amp;quot; denotes &amp;quot;French&amp;quot;, &amp;quot;English&amp;quot;, &amp;quot;Spanish&amp;quot;,
and &amp;quot;German&amp;quot;, respectively. For the language pairs
Lf-Le not including English, the bilingual corpus is
</bodyText>
<table confidence="0.999953875">
Language Sentence Source Target
Pairs Pairs Words Words
Fr-En 688,031 15,323,737 13,808,104
Fr-Es 640,661 14,148,926 13,134,411
Fr-De 639,693 14,215,058 12,155,876
Es-En 730,740 15,676,710 15,222,105
De-En 751,088 15,256,793 16,052,269
De-Es 672,813 13,246,255 14,362,615
</table>
<tableCaption confidence="0.999857">
Table 1. Training Corpus for European Languages
</tableCaption>
<bodyText confidence="0.990728875">
extracted from Lf-English and English-Le since
Europarl corpus is a multilingual corpus.
For the language models, we use the same data
provided in the shared task. We also use the same
development set and test set provided by the shared
task. The in-domain test set includes 2,000 sen-
tences and the out-of-domain test set includes
1,064 sentences for each language.
</bodyText>
<subsectionHeader confidence="0.987649">
6.2 Translation System and Evaluation
Method
</subsectionHeader>
<bodyText confidence="0.999954090909091">
To perform phrase-based SMT, we use Koehn&apos;s
training scripts1 and the Pharaoh decoder (Koehn,
2004). We run the decoder with its default settings
and then use Koehn&apos;s implementation of minimum
error rate training (Och, 2003) to tune the feature
weights on the development set.
The translation quality was evaluated using a
well-established automatic measure: BLEU score
(Papineni et al., 2002). And we also use the tool
provided in the NAACL/HLT 2006 shared task on
SMT to calculate the BLEU scores.
</bodyText>
<subsectionHeader confidence="0.999383">
6.3 Comparison of Different Lexical Weights
</subsectionHeader>
<bodyText confidence="0.9999895">
As described in section 4, we employ two methods
to estimate the lexical weight in the translation
model. In order to compare the two methods, we
translate from French to Spanish, using English as
the pivot language. We use the French-English and
English-Spanish corpora described in Table 1 as
training data. During training, before estimating
the Spanish to French phrase translation probabil-
ity, we filter those French-English and English-
Spanish phrase pairs whose translation probabili-
ties are below a fixed threshold 0.001.2 The trans-
lation results are shown in Table 2.
</bodyText>
<footnote confidence="0.99677125">
1 It is located at http://www.statmt.org/wmt06/shared-
task/baseline.htm
2 In the following experiments using pivot languages, we use
the same filtering threshold for all of the language pairs.
</footnote>
<equation confidence="0.987896">
p
i f e a
(  |, ) (11)
</equation>
<page confidence="0.993751">
859
</page>
<bodyText confidence="0.999829333333333">
The phrase method proposed in this paper per-
forms better than the word method proposed in
(Wang et al., 2006). This is because our method
uses phrase translation probability as a confidence
weight to estimate the lexical translation probabil-
ity. It strengthens the frequently aligned pairs and
weakens the infrequently aligned pairs. Thus, the
following sections will use the phrase method to
estimate the lexical weight.
</bodyText>
<table confidence="0.995062333333333">
Method In-Domain Out-of-Domain
Phrase 0.3212 0.2098
Word 0.2583 0.1672
</table>
<tableCaption confidence="0.999978">
Table 2. Results with Different Lexical Weights
</tableCaption>
<subsectionHeader confidence="0.929739">
6.4 Results of Using One Pivot Language
</subsectionHeader>
<bodyText confidence="0.9999566">
This section describes the translation results by
using only one pivot language. For the language
pair French and Spanish, we use English as the
pivot language. The entire French-English and
English-Spanish corpora as described in section 4
are used to train a pivot model for French-Spanish.
As described in section 5, if we have a small Lf-
Le bilingual corpus and large Lf-Lp and Lp-Le bilin-
gual corpora, we can obtain interpolated models.
In order to conduct the experiments, we ran-
domly select 5K, 10K, 20K, 30K, 40K, 50K, and
100K sentence pairs from the French-Spanish cor-
pus. Using each of these corpora, we train a stan-
dard translation model.
For each standard model, we interpolate it with
the pivot model to get an interpolated model. The
interpolation weights are tuned using the develop-
ment set. For all the interpolated models, we set
α0 = 0.9 , α1 = 0. 1 , ,Q0 = 0.9 , and ,Q1 = 0. 1. We
test the three kinds of models on both the in-
domain and out-of-domain test sets. The results are
shown in Figures 2 and 3.
The pivot model achieves BLEU scores of
0.3212 and 0.2098 on the in-domain and out-of-
domain test set, respectively. It achieves an abso-
lute improvement of 0.05 on both test sets (16.92%
and 35.35% relative) over the standard model
trained with 5,000 French-Spanish sentence pairs.
And the performance of the pivot models are com-
parable with that of the standard models trained
with 20,000 and 30,000 sentence pairs on the in-
domain and out-of-domain test set, respectively.
When the French-Spanish training corpus is in-
creased, the standard models quickly outperform
the pivot model.
</bodyText>
<figure confidence="0.9635745">
5 10 20 30 40 50 100
Fr-Es Data (k pairs)
</figure>
<figureCaption confidence="0.987726">
Figure 2. In-Domain French-Spanish Results
</figureCaption>
<figure confidence="0.975524">
5 10 20 30 40 50 100
Fr-Es Data (K pairs)
</figure>
<figureCaption confidence="0.961308">
Figure 3. Out-of-Domain French-Spanish Results
</figureCaption>
<figure confidence="0.974208">
5 10 20 30 40 50 100
Fr-En Data (k Pairs)
</figure>
<figureCaption confidence="0.980313">
Figure 4. In-Domain French-English Results
</figureCaption>
<figure confidence="0.937355">
5 10 20 30 40 50 100
Fr-De Data (k Pairs)
</figure>
<figureCaption confidence="0.999848">
Figure 5. In-Domain French-German Results
</figureCaption>
<bodyText confidence="0.999966727272727">
When only a very small French-Spanish bilin-
gual corpus is available, the interpolated method
can greatly improve the translation quality. For
example, when only 5,000 French-Spanish sen-
tence pairs are available, the interpolated model
outperforms the standard model by achieving a
relative improvement of 17.55%, with the BLEU
score improved from 0.2747 to 0.3229. With
50,000 French-Spanish sentence pairs available,
the interpolated model significantly3 improves the
translation quality by achieving an absolute im-
</bodyText>
<footnote confidence="0.7743065">
3 We conduct the significance test using the same method as
described in (Koehn and Monz, 2006).
</footnote>
<page confidence="0.59135525">
37
35
33
31
</page>
<figure confidence="0.993895785714285">
29
27
25
Interpolated
Standard
Pivot
BLEU (%)
26
24
22
20
18
16
14
Interpolated
Standard
Pivot
BLEU (%)
28
26
24
22
20
30
18
Interpolated
Standard
Pivot
BLEU (%)
17
16
15
14
13
12
11
10
9
Interpolated
Standard
Pivot
BLEU (%)
</figure>
<page confidence="0.992634">
860
</page>
<bodyText confidence="0.999926714285714">
provement of 0.01 BLEU. When the French-
Spanish training corpus increases to 100,000 sen-
tence pairs, the interpolated model achieves almost
the same result as the standard model. This indi-
cates that our pivot language method is suitable for
the language pairs with small quantities of training
data available.
Besides experiments on French-Spanish transla-
tion, we also conduct translation from French to
English and French to German, using German and
English as the pivot language, respectively. The
results on the in-domain test set4 are shown in Fig-
ures 4 and 5. The tendency of the results is similar
to that in Figure 2.
</bodyText>
<subsectionHeader confidence="0.9215025">
6.5 Results of Using More Than One Pivot
Language
</subsectionHeader>
<bodyText confidence="0.999880931034483">
For French to Spanish translation, we also intro-
duce German as a pivot language besides English.
Using these two pivot languages, we build two dif-
ferent pivot models, and then perform linear inter-
polation on them. The interpolation weights for the
English pivot model and the German pivot model
are set to 0.6 and 0.4 respectively5. The translation
results on the in-domain test set are 0.3212, 0.3077,
and 0.3355 for the pivot models using English,
German, and both German and English as pivot
languages, respectively.
With the pivot model using both English and
German as pivot languages, we interpolate it with
the standard models trained with French-Spanish
corpora of different sizes as described in the above
section. The comparison of the translation results
among the interpolated models, standard models,
and the pivot model are shown in Figure 6.
It can be seen that the translation results can be
further improved by using more than one pivot
language. The pivot model &amp;quot;Pivot-En+De&amp;quot; using
two pivot languages achieves an absolute im-
provement of 0.06 (22.13% relative) as compared
with the standard model trained with 5,000 sen-
tence pairs. And it achieves comparable translation
result as compared with the standard model trained
with 30,000 French-Spanish sentence pairs.
The results in Figure 6 also indicate the interpo-
lated models using two pivot languages achieve the
</bodyText>
<page confidence="0.447032">
4 The results on the out-of-domain test set are similar to that in
</page>
<figureCaption confidence="0.95899">
Figure 3. We only show the in-domain translation results in all
of the following experiments because of space limit.
</figureCaption>
<page confidence="0.432218">
5 The weights are tuned on the development set.
</page>
<bodyText confidence="0.9994462">
best results of all. Significance test shows that the
interpolated models using two pivot languages sig-
nificantly outperform those using one pivot lan-
guage when less than 50,000 French-Spanish sen-
tence pairs are available.
</bodyText>
<figure confidence="0.906959">
5 10 20 30 40 50 100
Fr-Es Data (k Pairs)
</figure>
<figureCaption confidence="0.9776135">
Figure 6. In-Domain French-Spanish Translation
Results by Using Two Pivot Languages
</figureCaption>
<subsectionHeader confidence="0.9874645">
6.6 Results by Using Pivot Language Related
Corpora of Different Sizes
</subsectionHeader>
<bodyText confidence="0.999935266666667">
In all of the above results, the corpora used to train
the pivot models are not changed. In order to ex-
amine the effect of the size of the pivot corpora,
we decrease the French-English and English-
French corpora. We randomly select 200,000 and
400,000 sentence pairs from both of them to train
two pivot models, respectively. The translation
results on the in-domain test set are 0.2376, 0.2954,
and 0.3212 for the pivot models trained with
200,000, 400,000, and the entire French-English
and English-Spanish corpora, respectively. The
results of the interpolated models and the standard
models are shown in Figure 7. The results indicate
that the larger the training corpora used to train the
pivot model are, the better the translation quality is.
</bodyText>
<figure confidence="0.910764">
5 10 20 30 40 50 100
Fr-Es Data (k pairs)
</figure>
<figureCaption confidence="0.995703">
Figure 7. In-Domain French-Spanish Results by
Using Lf-Lp and Lp-Le Corpora of Different Sizes
</figureCaption>
<figure confidence="0.951882060606061">
BLEU (%)
29
28
27
37
36
35
34
33
32
31
30
Interpolated-En+De
Interpolated-En
Interpolated-De
Standard
Pivot-En+De
BLEU (%)
29
28
27
37
36
35
34
33
32
31
30
Interpolated-All
interpolated-400k
Interpolated-200k
Standard
</figure>
<page confidence="0.989607">
861
</page>
<sectionHeader confidence="0.9759175" genericHeader="method">
7 Experiments on Chinese to Japanese
Translation
</sectionHeader>
<bodyText confidence="0.9988526">
In section 6, translation results on the Europarl
multilingual corpus indicate the effectiveness of
our method. To investigate the effectiveness of our
method by using independently sourced parallel
corpora, we conduct Chinese-Japanese translation
using English as a pivot language in this section,
where the training data are not limited to a specific
domain.
The data used for this experiment is the same as
those used in (Wang et al., 2006). There are 21,977,
329,350, and 160,535 sentence pairs for the lan-
guage pairs Chinese-Japanese, Chinese-English,
and English-Japanese, respectively. The develop-
ment data and testing data include 500 and 1,000
Chinese sentences respectively, with one reference
for each sentence. For Japanese language model
training, we use about 100M bytes Japanese corpus.
The translation result is shown in Figure 8. The
pivot model only outperforms the standard model
trained with 2,500 sentence pairs. This is because
(1) the corpora used to train the pivot model are
smaller as compared with the Europarl corpus; (2)
the training data and the testing data are not limited
to a specific domain; (3) The languages are not
closely related.
</bodyText>
<figureCaption confidence="0.9964">
Figure 8. Chinese-Japanese Translation Results
</figureCaption>
<bodyText confidence="0.999965857142857">
The interpolated models significantly outper-
form the other models. When only 5,000 sentence
pairs are available, the BLEU score increases rela-
tively by 20.53%. With the entire (21,977 pairs)
Chinese-Japanese available, the interpolated model
relatively increases the BLEU score by 5.62%,
from 0.1708 to 0.1804.
</bodyText>
<sectionHeader confidence="0.997983" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999982515151515">
This paper proposed a novel method for phrase-
based SMT on language pairs with a small bilin-
gual corpus by bringing in pivot languages. To per-
form translation between Lf and Le, we bring in a
pivot language Lp, via which the large corpora of
Lf-Lp and Lp-Le can be used to induce a translation
model for Lf-Le. The advantage of this method is
that it can perform translation between the lan-
guage pair Lf-Le even if no bilingual corpus for this
pair is available. Using BLEU as a metric, our
method achieves an absolute improvement of 0.06
(22.13% relative) as compared with the model di-
rectly trained with 5,000 sentence pairs for French-
Spanish translation. And the translation quality is
comparable with that of the model directly trained
with 30,000 French-Spanish sentence pairs. The
results also indicate that using more pivot lan-
guages leads to better translation quality.
With a small bilingual corpus available for Lf-Le,
we built a translation model, and interpolated it
with the pivot model trained with the large Lf-Lp
and Lp-Le bilingual corpora. The results on both
the Europarl corpus and Chinese-Japanese transla-
tion indicate that the interpolated models achieve
the best results. Results also indicate that our pivot
language approach is suitable for translation on
language pairs with a small bilingual corpus. The
less the Lf-Le bilingual corpus is, the bigger the
improvement is.
We also performed experiments using Lf-Lp and
Lp-Le corpora of different sizes. The results indi-
cate that using larger training corpora to train the
pivot model leads to better translation quality.
</bodyText>
<sectionHeader confidence="0.998581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9975">
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning Dependency Translation Models as
Collections of Finite-State Head Transducers. Com-
putational Linguistics, 26(1):45-60.
Niraj Aswani and Robert Gaizauskas. 2005. Aligning
Words in English-Hindi Parallel Corpora. In Proc. of
the ACL 2005 Workshop on Building and Using Par-
allel Texts: Data-driven Machine Translation and
Beyond, pages 115-118.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 19(2):
263-311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Transla-
</reference>
<figure confidence="0.992650076923077">
Interpolated
Standard
Pivot
14
12
10
8
6
2.5 5 10 21.9
Chinese-Japanese Data (k pairs)
BLEU (%)
18
16
</figure>
<page confidence="0.971958">
862
</page>
<reference confidence="0.999946887755102">
tion Using Paraphrases. In Proc. of NAACL-2006,
pages 17-24.
Lars Borin. 2000. You&apos;ll Take the High Road and I&apos;ll
Take the Low Road: Using a Third Language to Im-
prove Bilingual Word Alignment. In Proc. of COL-
ING-2000, pages 97-103.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
of ACL-2005, pages 263-270.
Mona Diab and Philip Resnik. 2002. An Unsupervised
Method for Word Sense Tagging using Parallel Cor-
pora. In Proc. of ACL-2002, pages 255-262.
Tim Gollins and Mark Sanderson. 2001. Improving
Cross Language Information Retrieval with Triangu-
lated Translation. In Proc. of ACM SIGIR-2001,
pages 90-95.
Kazuaki Kishida and Noriko Kando. 2003. Two-Stage
Refinement of Query Translation in a Pivot Lan-
guage Approach to Cross-Lingual Information Re-
trieval: An Experiment at CLEF 2003. In Proc. of
CLEF-2003. pages 253-262.
Philipp Koehn. 2004. Pharaoh: A Beam Search Decoder
for Phrase-Based Statistical Machine Translation
Models. In Proc. of AMTA-2004, pages 115-124.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proc. of MT
Summit X, pages 79-86.
Philipp Koehn and Christof Monz. 2006. Manual and
Automatic Evaluation of Machine Translation be-
tween European Languages. In Proc. of the 2006
HLT-NAACL Workshop on Statistical Machine
Translation, pages 102-121.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of HLT-NAAC- 2003, pages 127-133.
Adam Lopez and Philip Resnik. 2005. Improved HMM
Alignment Models for Languages with Scarce Re-
sources. In Proc. of the ACL-2005 Work-shop on
Building and Using Parallel Texts: Data-driven Ma-
chine Translation and Beyond, pages 83-86.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word Alignment for Languages with Scarce Re-
sources. In Proc. of the ACL-2005 Workshop on
Building and Using Parallel Texts: Data-driven Ma-
chine Translation and Beyond, pages 65-74.
Dan Melamed. 2004. Statistical Machine Translation by
Parsing. In Proc. of ACL-2004, pages 653-660.
Bart Mellebeek, Karolina Owczarzak, Declan Groves,
Josef Van Genabith, and Andy Way. 2006. A Syntac-
tic Skeleton for Statistical Machine Translation. In
Proc. of EAMT-2006, pages 195-202.
Sonja Niessen and Hermann Ney. 2004. Statistical
Machine Translation with Scarce Resources Using
Morpho-Syntactic Information. Computational
linguistics, 30(2): 181-204.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of ACL-
2003, pages 160-167.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine
Translation. Computational Linguistics, 30(4):417-
449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proc. of ACL-
2002, pages 311-318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proc. of ACL-2005, pages
271-279.
Tetsuya Sakai, Makoto Koyama, Akira Kumano, and
Toshihiko Manabe. 2004. Toshiba BRIDJE at
NTCIR-4 CLIR: Monolingual/Bilingual IR and
Flexible Feedback. In Proc. of NTCIR 4.
Charles Schafer and David Yarowsky. 2002. Inducing
Translation Lexicons via Diverse Similarity Meas-
ures and Bridge Languages. In Proc. of CoNLL-2002,
pages 1-7.
Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word
Alignment for Languages with Scarce Resources Us-
ing Bilingual Corpora of Other Language Pairs. In
Proc. of COLING/ACL-2006 Main Conference
Poster Sessions, pages 874-881.
Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan Ste-
fanescu. 2005. Combined Word Alignments. In Proc.
of the ACL-2005 Workshop on Building and Using
Parallel Texts: Data-driven Machine Translation and
Beyond, pages 107-110.
Vincent Vandeghinste, Ineka Schuurman, Michael Carl,
Stella Markantonatou, and Toni Badia. 2006.
METIS-II: Machine Translation for Low-Resource
Languages. In Proc. of LREC-2006, pages 1284-1289.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377-403.
Kenji Yamada and Kevin Knight. 2001. A Syntax Based
Statistical Translation Model. In Proc. of ACL-2001,
pages 523-530.
</reference>
<page confidence="0.999185">
863
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.863360">
<title confidence="0.9992945">Pivot Language Approach for Phrase-Based Statistical Machine Translation</title>
<author confidence="0.984737">Hua Wu</author>
<author confidence="0.984737">Haifeng Wang</author>
<affiliation confidence="0.975113">Toshiba (China) Research and Development Center</affiliation>
<address confidence="0.9905355">5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District Beijing, 100738, China</address>
<abstract confidence="0.99570925">This paper proposes a novel method for phrase-based statistical machine translation by using pivot language. To conduct transbetween languages with a small bilingual corpus, we bring in a third which is named the lan- For and there exist bilingual corpora. Using only bilingual corpora, we can build a model for The advantage of this method lies in that we can perform between even if there is no bilingual corpus available for this language pair. Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly with 5,000 sentence pairs for French-Spanish translation. Moreover, with small bilingual corpus available, our method can further improve the translaquality by using the additional bilingual corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Learning Dependency Translation Models as Collections of Finite-State Head Transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--1</pages>
<contexts>
<context position="1420" citStr="Alshawi et al. 2000" startWordPosition="215" endWordPosition="219"> is no bilingual corpus available for this language pair. Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. Moreover, with a small Lf-Le bilingual corpus available, our method can further improve the translation quality by using the additional Lf-Lp and Lp-Le bilingual corpora. 1 Introduction For statistical machine translation (SMT), phrasebased methods (Koehn et al., 2003; Och and Ney, 2004) and syntax-based methods (Wu, 1997; Alshawi et al. 2000; Yamada and Knignt, 2001; Melamed, 2004; Chiang, 2005; Quick et al., 2005; Mellebeek et al., 2006) outperform word-based methods (Brown et al., 1993). These methods need large bilingual corpora. However, for some languages pairs, only a small bilingual corpus is available, which will degrade the performance of statistical translation systems. To solve this problem, this paper proposes a novel method for phrase-based SMT by using a pivot language. To perform translation between languages Lf and Le, we bring in a pivot language Lp, for which there exist large bilingual corpora for language pair</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning Dependency Translation Models as Collections of Finite-State Head Transducers. Computational Linguistics, 26(1):45-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niraj Aswani</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Aligning Words in English-Hindi Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL 2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond,</booktitle>
<pages>115--118</pages>
<contexts>
<context position="5693" citStr="Aswani and Gaizauskas, 2005" startWordPosition="889" endWordPosition="892"> word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According </context>
</contexts>
<marker>Aswani, Gaizauskas, 2005</marker>
<rawString>Niraj Aswani and Robert Gaizauskas. 2005. Aligning Words in English-Hindi Parallel Corpora. In Proc. of the ACL 2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond, pages 115-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="1570" citStr="Brown et al., 1993" startWordPosition="239" endWordPosition="242">22.13% relative) as compared with the model directly trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. Moreover, with a small Lf-Le bilingual corpus available, our method can further improve the translation quality by using the additional Lf-Lp and Lp-Le bilingual corpora. 1 Introduction For statistical machine translation (SMT), phrasebased methods (Koehn et al., 2003; Och and Ney, 2004) and syntax-based methods (Wu, 1997; Alshawi et al. 2000; Yamada and Knignt, 2001; Melamed, 2004; Chiang, 2005; Quick et al., 2005; Mellebeek et al., 2006) outperform word-based methods (Brown et al., 1993). These methods need large bilingual corpora. However, for some languages pairs, only a small bilingual corpus is available, which will degrade the performance of statistical translation systems. To solve this problem, this paper proposes a novel method for phrase-based SMT by using a pivot language. To perform translation between languages Lf and Le, we bring in a pivot language Lp, for which there exist large bilingual corpora for language pairs Lf-Lp and Lp-Le. With the Lf-Lp and Lp-Le bilingual corpora, we can build a translation model for Lf-Le by using Lp as the pivot language. We name t</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved Statistical Machine Translation Using Paraphrases.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL-2006,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="4587" citStr="Callison-Burch et al. (2006)" startWordPosition="720" endWordPosition="723"> two kinds of methods: those using pivot language and those using a small bilingual corpus or scarce resources. For the first kind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-sy</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved Statistical Machine Translation Using Paraphrases. In Proc. of NAACL-2006, pages 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Borin</author>
</authors>
<title>You&apos;ll Take the High Road and I&apos;ll Take the Low Road: Using a Third Language to Improve Bilingual Word Alignment.</title>
<date>2000</date>
<booktitle>In Proc. of COLING-2000,</booktitle>
<pages>97--103</pages>
<contexts>
<context position="4698" citStr="Borin (2000)" startWordPosition="740" endWordPosition="741">ind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) us</context>
</contexts>
<marker>Borin, 2000</marker>
<rawString>Lars Borin. 2000. You&apos;ll Take the High Road and I&apos;ll Take the Low Road: Using a Third Language to Improve Bilingual Word Alignment. In Proc. of COLING-2000, pages 97-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL-2005,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1474" citStr="Chiang, 2005" startWordPosition="226" endWordPosition="227">ng BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. Moreover, with a small Lf-Le bilingual corpus available, our method can further improve the translation quality by using the additional Lf-Lp and Lp-Le bilingual corpora. 1 Introduction For statistical machine translation (SMT), phrasebased methods (Koehn et al., 2003; Och and Ney, 2004) and syntax-based methods (Wu, 1997; Alshawi et al. 2000; Yamada and Knignt, 2001; Melamed, 2004; Chiang, 2005; Quick et al., 2005; Mellebeek et al., 2006) outperform word-based methods (Brown et al., 1993). These methods need large bilingual corpora. However, for some languages pairs, only a small bilingual corpus is available, which will degrade the performance of statistical translation systems. To solve this problem, this paper proposes a novel method for phrase-based SMT by using a pivot language. To perform translation between languages Lf and Le, we bring in a pivot language Lp, for which there exist large bilingual corpora for language pairs Lf-Lp and Lp-Le. With the Lf-Lp and Lp-Le bilingual </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proc. of ACL-2005, pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An Unsupervised Method for Word Sense Tagging using Parallel Corpora.</title>
<date>2002</date>
<booktitle>In Proc. of ACL-2002,</booktitle>
<pages>255--262</pages>
<contexts>
<context position="5116" citStr="Diab and Resnik, 2002" startWordPosition="800" endWordPosition="803"> sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 200</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An Unsupervised Method for Word Sense Tagging using Parallel Corpora. In Proc. of ACL-2002, pages 255-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Gollins</author>
<author>Mark Sanderson</author>
</authors>
<title>Improving Cross Language Information Retrieval with Triangulated Translation.</title>
<date>2001</date>
<booktitle>In Proc. of ACM SIGIR-2001,</booktitle>
<pages>90--95</pages>
<contexts>
<context position="4216" citStr="Gollins and Sanderson, 2001" startWordPosition="663" endWordPosition="666">The remainder of this paper is organized as follows. In section 2, we describe the related work. Section 3 briefly introduces phrase-based SMT. Section 4 and Section 5 describes our method for phrase-based SMT using pivot language. We describe the experimental results in sections 6 and 7. Lastly, we conclude in section 8. 2 Related Work Our method is mainly related to two kinds of methods: those using pivot language and those using a small bilingual corpus or scarce resources. For the first kind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to incr</context>
</contexts>
<marker>Gollins, Sanderson, 2001</marker>
<rawString>Tim Gollins and Mark Sanderson. 2001. Improving Cross Language Information Retrieval with Triangulated Translation. In Proc. of ACM SIGIR-2001, pages 90-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuaki Kishida</author>
<author>Noriko Kando</author>
</authors>
<title>Two-Stage Refinement of Query Translation in a Pivot Language Approach to Cross-Lingual Information Retrieval: An Experiment at CLEF</title>
<date>2003</date>
<booktitle>In Proc. of CLEF-2003.</booktitle>
<pages>253--262</pages>
<contexts>
<context position="4242" citStr="Kishida and Kando, 2003" startWordPosition="667" endWordPosition="670">s organized as follows. In section 2, we describe the related work. Section 3 briefly introduces phrase-based SMT. Section 4 and Section 5 describes our method for phrase-based SMT using pivot language. We describe the experimental results in sections 6 and 7. Lastly, we conclude in section 8. 2 Related Work Our method is mainly related to two kinds of methods: those using pivot language and those using a small bilingual corpus or scarce resources. For the first kind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. W</context>
</contexts>
<marker>Kishida, Kando, 2003</marker>
<rawString>Kazuaki Kishida and Noriko Kando. 2003. Two-Stage Refinement of Query Translation in a Pivot Language Approach to Cross-Lingual Information Retrieval: An Experiment at CLEF 2003. In Proc. of CLEF-2003. pages 253-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proc. of AMTA-2004,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="13393" citStr="Koehn, 2004" startWordPosition="2326" endWordPosition="2327">56,793 16,052,269 De-Es 672,813 13,246,255 14,362,615 Table 1. Training Corpus for European Languages extracted from Lf-English and English-Le since Europarl corpus is a multilingual corpus. For the language models, we use the same data provided in the shared task. We also use the same development set and test set provided by the shared task. The in-domain test set includes 2,000 sentences and the out-of-domain test set includes 1,064 sentences for each language. 6.2 Translation System and Evaluation Method To perform phrase-based SMT, we use Koehn&apos;s training scripts1 and the Pharaoh decoder (Koehn, 2004). We run the decoder with its default settings and then use Koehn&apos;s implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. 6.3 Comparison of Different Lexical Weights As described in section 4, we employ two methods to estimate the lexical weight in the translation model. In order to compare the two methods, we translate</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models. In Proc. of AMTA-2004, pages 115-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proc. of MT Summit X,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="2761" citStr="Koehn, 2005" startWordPosition="439" endWordPosition="440">t language. We name the translation model pivot model. The advantage of this method lies in that we can conduct translation between Lf and Le even if there is no bilingual corpus available for this language pair. Moreover, if a small corpus is available for Lf-Le, we build another translation model, which is named standard model. Then, we build an interpolated model by performing linear interpolation on the standard model and the pivot model. Thus, the interpolated model can employ both the small LfLe corpus and the large Lf-Lp and Lp-Le corpora. We perform experiments on the Europarl corpus (Koehn, 2005). Using BLEU (Papineni et al., 2002) as a metric, our method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the standard model trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. The translation quality is comparable with that of the model trained with a bilingual corpus of 30,000 LfLe sentence pairs. Moreover, translation quality is further boosted by using both the small Lf-Le bilingual corpus and the large Lf-Lp and Lp-Le corpora. Experimental results on Chinese-Japanese translation also indicate that our method achieves satisfactory results </context>
<context position="11769" citStr="Koehn, 2005" startWordPosition="2079" endWordPosition="2080">ere φ0 (f |e) and p,,,0 (f |e, a) denote the phrase translation probability and lexical weight trained with the Lf-Le bilingual corpus, respectively. φi (f |e) and p ,,,i (f |e, a) ( i =1,... , n ) are the phrase translation probability and lexical weight estimated by using the pivot languages. αi and βi are the interpolation coefficients. 6 Experiments on the Europarl Corpus 6.1 Data A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The shared task used the Europarl corpus (Koehn, 2005), in which four languages are involved: English, French, Spanish, and German. The shared task performed translation between English and the other three languages. In our work, we perform translation from French to the other three languages. We select French to Spanish and French to German translation that are not in the shared task because we want to use English as the pivot language. In general, for most of the languages, there exist bilingual corpora between these languages and English since English is an internationally used language. Table 1 shows the information about the bilingual traini</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proc. of MT Summit X, pages 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and Automatic Evaluation of Machine Translation between European Languages.</title>
<date>2006</date>
<booktitle>In Proc. of the 2006 HLT-NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="11713" citStr="Koehn and Monz, 2006" startWordPosition="2068" endWordPosition="2071">|) f e = ∑ i i f e (10) i=0 n ,, ( |, ) β ,,, f e a = ∑ ip i=0 Where φ0 (f |e) and p,,,0 (f |e, a) denote the phrase translation probability and lexical weight trained with the Lf-Le bilingual corpus, respectively. φi (f |e) and p ,,,i (f |e, a) ( i =1,... , n ) are the phrase translation probability and lexical weight estimated by using the pivot languages. αi and βi are the interpolation coefficients. 6 Experiments on the Europarl Corpus 6.1 Data A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The shared task used the Europarl corpus (Koehn, 2005), in which four languages are involved: English, French, Spanish, and German. The shared task performed translation between English and the other three languages. In our work, we perform translation from French to the other three languages. We select French to Spanish and French to German translation that are not in the shared task because we want to use English as the pivot language. In general, for most of the languages, there exist bilingual corpora between these languages and English since English is an internationally used language. </context>
<context position="17774" citStr="Koehn and Monz, 2006" startWordPosition="3045" endWordPosition="3048">n Results When only a very small French-Spanish bilingual corpus is available, the interpolated method can greatly improve the translation quality. For example, when only 5,000 French-Spanish sentence pairs are available, the interpolated model outperforms the standard model by achieving a relative improvement of 17.55%, with the BLEU score improved from 0.2747 to 0.3229. With 50,000 French-Spanish sentence pairs available, the interpolated model significantly3 improves the translation quality by achieving an absolute im3 We conduct the significance test using the same method as described in (Koehn and Monz, 2006). 37 35 33 31 29 27 25 Interpolated Standard Pivot BLEU (%) 26 24 22 20 18 16 14 Interpolated Standard Pivot BLEU (%) 28 26 24 22 20 30 18 Interpolated Standard Pivot BLEU (%) 17 16 15 14 13 12 11 10 9 Interpolated Standard Pivot BLEU (%) 860 provement of 0.01 BLEU. When the FrenchSpanish training corpus increases to 100,000 sentence pairs, the interpolated model achieves almost the same result as the standard model. This indicates that our pivot language method is suitable for the language pairs with small quantities of training data available. Besides experiments on French-Spanish translatio</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and Automatic Evaluation of Machine Translation between European Languages. In Proc. of the 2006 HLT-NAACL Workshop on Statistical Machine Translation, pages 102-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAAC-</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1344" citStr="Koehn et al., 2003" startWordPosition="202" endWordPosition="205">hod lies in that we can perform translation between Lf and Le even if there is no bilingual corpus available for this language pair. Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. Moreover, with a small Lf-Le bilingual corpus available, our method can further improve the translation quality by using the additional Lf-Lp and Lp-Le bilingual corpora. 1 Introduction For statistical machine translation (SMT), phrasebased methods (Koehn et al., 2003; Och and Ney, 2004) and syntax-based methods (Wu, 1997; Alshawi et al. 2000; Yamada and Knignt, 2001; Melamed, 2004; Chiang, 2005; Quick et al., 2005; Mellebeek et al., 2006) outperform word-based methods (Brown et al., 1993). These methods need large bilingual corpora. However, for some languages pairs, only a small bilingual corpus is available, which will degrade the performance of statistical translation systems. To solve this problem, this paper proposes a novel method for phrase-based SMT by using a pivot language. To perform translation between languages Lf and Le, we bring in a pivot </context>
<context position="6351" citStr="Koehn et al., 2003" startWordPosition="994" endWordPosition="997"> 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation model presented in (Koehn et al., 2003), given a source sentence f , the best target translation ebest can be obtained according to the following model e = arg max best e (f e) LM (e) | ω e p p Where the tran slation model p(f |e) can be decomposed into I I p(f1 | I = ∏ φ ( |) ( f e d a b − i i i i − i=1 φ(f |ei) (ai−bi− 1) pw i|ei λ is the strength of the lexical weight. Where i and d denote phrase translation probability and distortion probability, respectively. (f , a) is the lexical weight, 4Phrase-Based SMT Via Pivot Language This section will introduce the method that performs phrase-based SMT for the language pair LfLe by us</context>
<context position="8350" citStr="Koehn et al., 2003" startWordPosition="1371" endWordPosition="1374">we obtain the phrase translation probability φ(f i |ei) according to the following model. φff i |ei)=∑φ(fi |pi,ei)φ(pi |ei) (3) pi The phrase translation probability φ(f i |pi , ei ) does not depend on the phase ei in the language Le, since it is estimated from the Lf-Lp bilingual corpus. Thus, equation (3) can be rewritten as φ(fi |ei) = ∑φ(fi |pi)φ(pi |ei) (4) pi 4.2 Lexical Weight Given a phrase pair (f , e) and a word alignment a between the source word positions i =1,..., n and the target word positions j =1,... , m , the lexical weight can be estimated according to the following method (Koehn et al., 2003). Figure 1. Alignment Information Induction and a2 represent the word alignment inrespectively, then the alignment information a inside (f , e) can be obtained as shown in (6). An example is shown in Figure 1. a={(f,e) : , ) ,e) a1 |∃p f p ∈ 1 p ∈ 2 om the induced phrase pairs. We name this method phrase method. If we use K to denote the number of the induced phrase pairs, we estimate the co-occurring frequency of the word pair (f , e) according to the following model. count K n (7) = ∑ φ f e f f e e ∑ δ δ k ( |) ( , ) ( , ) i a i = 1 i = 1 Where φk (f |e) is the phrase translation probability</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of HLT-NAAC- 2003, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Philip Resnik</author>
</authors>
<title>Improved HMM Alignment Models for Languages with Scarce Resources.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL-2005 Work-shop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond,</booktitle>
<pages>83--86</pages>
<contexts>
<context position="5717" citStr="Lopez and Resnik, 2005" startWordPosition="893" endWordPosition="896">iab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation model</context>
</contexts>
<marker>Lopez, Resnik, 2005</marker>
<rawString>Adam Lopez and Philip Resnik. 2005. Improved HMM Alignment Models for Languages with Scarce Resources. In Proc. of the ACL-2005 Work-shop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond, pages 83-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Martin</author>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>Word Alignment for Languages with Scarce Resources.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond,</booktitle>
<pages>65--74</pages>
<contexts>
<context position="5552" citStr="Martin et al., 2005" startWordPosition="868" endWordPosition="871">prove word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to </context>
</contexts>
<marker>Martin, Mihalcea, Pedersen, 2005</marker>
<rawString>Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005. Word Alignment for Languages with Scarce Resources. In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond, pages 65-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Statistical Machine Translation by Parsing.</title>
<date>2004</date>
<booktitle>In Proc. of ACL-2004,</booktitle>
<pages>653--660</pages>
<contexts>
<context position="1460" citStr="Melamed, 2004" startWordPosition="224" endWordPosition="225">guage pair. Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. Moreover, with a small Lf-Le bilingual corpus available, our method can further improve the translation quality by using the additional Lf-Lp and Lp-Le bilingual corpora. 1 Introduction For statistical machine translation (SMT), phrasebased methods (Koehn et al., 2003; Och and Ney, 2004) and syntax-based methods (Wu, 1997; Alshawi et al. 2000; Yamada and Knignt, 2001; Melamed, 2004; Chiang, 2005; Quick et al., 2005; Mellebeek et al., 2006) outperform word-based methods (Brown et al., 1993). These methods need large bilingual corpora. However, for some languages pairs, only a small bilingual corpus is available, which will degrade the performance of statistical translation systems. To solve this problem, this paper proposes a novel method for phrase-based SMT by using a pivot language. To perform translation between languages Lf and Le, we bring in a pivot language Lp, for which there exist large bilingual corpora for language pairs Lf-Lp and Lp-Le. With the Lf-Lp and Lp</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>Dan Melamed. 2004. Statistical Machine Translation by Parsing. In Proc. of ACL-2004, pages 653-660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Mellebeek</author>
<author>Karolina Owczarzak</author>
<author>Declan Groves</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>A Syntactic Skeleton for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of EAMT-2006,</booktitle>
<pages>195--202</pages>
<marker>Mellebeek, Owczarzak, Groves, Van Genabith, Way, 2006</marker>
<rawString>Bart Mellebeek, Karolina Owczarzak, Declan Groves, Josef Van Genabith, and Andy Way. 2006. A Syntactic Skeleton for Statistical Machine Translation. In Proc. of EAMT-2006, pages 195-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niessen</author>
<author>Hermann Ney</author>
</authors>
<date>2004</date>
<booktitle>Statistical Machine Translation with Scarce Resources Using Morpho-Syntactic Information. Computational linguistics,</booktitle>
<volume>30</volume>
<issue>2</issue>
<pages>181--204</pages>
<contexts>
<context position="5172" citStr="Niessen and Ney (2004)" startWordPosition="811" endWordPosition="814">, 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources</context>
</contexts>
<marker>Niessen, Ney, 2004</marker>
<rawString>Sonja Niessen and Hermann Ney. 2004. Statistical Machine Translation with Scarce Resources Using Morpho-Syntactic Information. Computational linguistics, 30(2): 181-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL2003,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="13518" citStr="Och, 2003" startWordPosition="2346" endWordPosition="2347"> and English-Le since Europarl corpus is a multilingual corpus. For the language models, we use the same data provided in the shared task. We also use the same development set and test set provided by the shared task. The in-domain test set includes 2,000 sentences and the out-of-domain test set includes 1,064 sentences for each language. 6.2 Translation System and Evaluation Method To perform phrase-based SMT, we use Koehn&apos;s training scripts1 and the Pharaoh decoder (Koehn, 2004). We run the decoder with its default settings and then use Koehn&apos;s implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. 6.3 Comparison of Different Lexical Weights As described in section 4, we employ two methods to estimate the lexical weight in the translation model. In order to compare the two methods, we translate from French to Spanish, using English as the pivot language. We use the French-English and English-Spanish corpora described</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of ACL2003, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--4</pages>
<contexts>
<context position="1364" citStr="Och and Ney, 2004" startWordPosition="206" endWordPosition="209">can perform translation between Lf and Le even if there is no bilingual corpus available for this language pair. Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. Moreover, with a small Lf-Le bilingual corpus available, our method can further improve the translation quality by using the additional Lf-Lp and Lp-Le bilingual corpora. 1 Introduction For statistical machine translation (SMT), phrasebased methods (Koehn et al., 2003; Och and Ney, 2004) and syntax-based methods (Wu, 1997; Alshawi et al. 2000; Yamada and Knignt, 2001; Melamed, 2004; Chiang, 2005; Quick et al., 2005; Mellebeek et al., 2006) outperform word-based methods (Brown et al., 1993). These methods need large bilingual corpora. However, for some languages pairs, only a small bilingual corpus is available, which will degrade the performance of statistical translation systems. To solve this problem, this paper proposes a novel method for phrase-based SMT by using a pivot language. To perform translation between languages Lf and Le, we bring in a pivot language Lp, for whi</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL2002,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="2797" citStr="Papineni et al., 2002" startWordPosition="443" endWordPosition="446">anslation model pivot model. The advantage of this method lies in that we can conduct translation between Lf and Le even if there is no bilingual corpus available for this language pair. Moreover, if a small corpus is available for Lf-Le, we build another translation model, which is named standard model. Then, we build an interpolated model by performing linear interpolation on the standard model and the pivot model. Thus, the interpolated model can employ both the small LfLe corpus and the large Lf-Lp and Lp-Le corpora. We perform experiments on the Europarl corpus (Koehn, 2005). Using BLEU (Papineni et al., 2002) as a metric, our method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the standard model trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. The translation quality is comparable with that of the model trained with a bilingual corpus of 30,000 LfLe sentence pairs. Moreover, translation quality is further boosted by using both the small Lf-Le bilingual corpus and the large Lf-Lp and Lp-Le corpora. Experimental results on Chinese-Japanese translation also indicate that our method achieves satisfactory results using English as the pivot language.</context>
<context position="13687" citStr="Papineni et al., 2002" startWordPosition="2369" endWordPosition="2372">me development set and test set provided by the shared task. The in-domain test set includes 2,000 sentences and the out-of-domain test set includes 1,064 sentences for each language. 6.2 Translation System and Evaluation Method To perform phrase-based SMT, we use Koehn&apos;s training scripts1 and the Pharaoh decoder (Koehn, 2004). We run the decoder with its default settings and then use Koehn&apos;s implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. 6.3 Comparison of Different Lexical Weights As described in section 4, we employ two methods to estimate the lexical weight in the translation model. In order to compare the two methods, we translate from French to Spanish, using English as the pivot language. We use the French-English and English-Spanish corpora described in Table 1 as training data. During training, before estimating the Spanish to French phrase translation probability, we filter those French-English and EnglishSpanish </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proc. of ACL2002, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proc. of ACL-2005,</booktitle>
<pages>271--279</pages>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proc. of ACL-2005, pages 271-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
<author>Makoto Koyama</author>
<author>Akira Kumano</author>
<author>Toshihiko Manabe</author>
</authors>
<date>2004</date>
<booktitle>Toshiba BRIDJE at NTCIR-4 CLIR: Monolingual/Bilingual IR and Flexible Feedback. In Proc. of NTCIR 4.</booktitle>
<contexts>
<context position="4557" citStr="Sakai et al., 2004" startWordPosition="716" endWordPosition="719"> is mainly related to two kinds of methods: those using pivot language and those using a small bilingual corpus or scarce resources. For the first kind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen</context>
</contexts>
<marker>Sakai, Koyama, Kumano, Manabe, 2004</marker>
<rawString>Tetsuya Sakai, Makoto Koyama, Akira Kumano, and Toshihiko Manabe. 2004. Toshiba BRIDJE at NTCIR-4 CLIR: Monolingual/Bilingual IR and Flexible Feedback. In Proc. of NTCIR 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing Translation Lexicons via Diverse Similarity Measures and Bridge Languages.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL-2002,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="5065" citStr="Schafer and Yarowsky, 2002" startWordPosition="792" endWordPosition="795">ystems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers </context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing Translation Lexicons via Diverse Similarity Measures and Bridge Languages. In Proc. of CoNLL-2002, pages 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
<author>Zhanyi Liu</author>
</authors>
<title>Word Alignment for Languages with Scarce Resources Using Bilingual Corpora of Other Language Pairs.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL-2006 Main Conference Poster Sessions,</booktitle>
<pages>874--881</pages>
<contexts>
<context position="4721" citStr="Wang et al. (2006)" startWordPosition="743" endWordPosition="746">ges are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot language. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictiona</context>
<context position="9265" citStr="Wang et al., 2006" startWordPosition="1567" endWordPosition="1570">d phrase method. If we use K to denote the number of the induced phrase pairs, we estimate the co-occurring frequency of the word pair (f , e) according to the following model. count K n (7) = ∑ φ f e f f e e ∑ δ δ k ( |) ( , ) ( , ) i a i = 1 i = 1 Where φk (f |e) is the phrase translation probability for phrase pair k . δ(x, y) =1 if x = y; otherwise, δ(x, y) = 0 . Thus, lexical translation probability can be estimated as in (8). &amp;( } (6) With the induced alignment information, this paper proposes a method to estimate the probability directly fr ability w(f e) using the method described in (Wang et al., 2006), which is shown in (9). We named it word method in this paper. | w f e ( |)=∑wf p w p e sim f e p ( |) ( |) ( , ; ) (9) p p) and w(p e) are two lexical probabilities, and sim(f , e; p) is the crosslanguage word similari | | ty. In order to estimate the lexical weight, we first need to obtain the alignment information a between the two phrases f and e , andthen estimate the lexical translation probability w(f e) according to the alignment information. The alignment information of the phrase pair (f , e) can be induced fr | om the two phrase pairs (f , p) and (p, e) . 858 (8) We also estimate t</context>
<context position="14741" citStr="Wang et al., 2006" startWordPosition="2543" endWordPosition="2546">able 1 as training data. During training, before estimating the Spanish to French phrase translation probability, we filter those French-English and EnglishSpanish phrase pairs whose translation probabilities are below a fixed threshold 0.001.2 The translation results are shown in Table 2. 1 It is located at http://www.statmt.org/wmt06/sharedtask/baseline.htm 2 In the following experiments using pivot languages, we use the same filtering threshold for all of the language pairs. p i f e a ( |, ) (11) 859 The phrase method proposed in this paper performs better than the word method proposed in (Wang et al., 2006). This is because our method uses phrase translation probability as a confidence weight to estimate the lexical translation probability. It strengthens the frequently aligned pairs and weakens the infrequently aligned pairs. Thus, the following sections will use the phrase method to estimate the lexical weight. Method In-Domain Out-of-Domain Phrase 0.3212 0.2098 Word 0.2583 0.1672 Table 2. Results with Different Lexical Weights 6.4 Results of Using One Pivot Language This section describes the translation results by using only one pivot language. For the language pair French and Spanish, we us</context>
<context position="22326" citStr="Wang et al., 2006" startWordPosition="3798" endWordPosition="3801">ndard Pivot-En+De BLEU (%) 29 28 27 37 36 35 34 33 32 31 30 Interpolated-All interpolated-400k Interpolated-200k Standard 861 7 Experiments on Chinese to Japanese Translation In section 6, translation results on the Europarl multilingual corpus indicate the effectiveness of our method. To investigate the effectiveness of our method by using independently sourced parallel corpora, we conduct Chinese-Japanese translation using English as a pivot language in this section, where the training data are not limited to a specific domain. The data used for this experiment is the same as those used in (Wang et al., 2006). There are 21,977, 329,350, and 160,535 sentence pairs for the language pairs Chinese-Japanese, Chinese-English, and English-Japanese, respectively. The development data and testing data include 500 and 1,000 Chinese sentences respectively, with one reference for each sentence. For Japanese language model training, we use about 100M bytes Japanese corpus. The translation result is shown in Figure 8. The pivot model only outperforms the standard model trained with 2,500 sentence pairs. This is because (1) the corpora used to train the pivot model are smaller as compared with the Europarl corpu</context>
</contexts>
<marker>Wang, Wu, Liu, 2006</marker>
<rawString>Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word Alignment for Languages with Scarce Resources Using Bilingual Corpora of Other Language Pairs. In Proc. of COLING/ACL-2006 Main Conference Poster Sessions, pages 874-881.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufis</author>
<author>Radu Ion</author>
<author>Alexandru Ceausu</author>
<author>Dan Stefanescu</author>
</authors>
<title>Combined Word Alignments.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond,</booktitle>
<pages>107--110</pages>
<contexts>
<context position="5738" citStr="Tufis et al., 2005" startWordPosition="897" endWordPosition="900">nd so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. In this paper, we address the translation problem for language pairs with scarce resources by bringing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot language and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation model presented in (Koehn </context>
</contexts>
<marker>Tufis, Ion, Ceausu, Stefanescu, 2005</marker>
<rawString>Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan Stefanescu. 2005. Combined Word Alignments. In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond, pages 107-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Vandeghinste</author>
<author>Ineka Schuurman</author>
<author>Michael Carl</author>
<author>Stella Markantonatou</author>
<author>Toni Badia</author>
</authors>
<title>METIS-II: Machine Translation for Low-Resource Languages.</title>
<date>2006</date>
<booktitle>In Proc. of LREC-2006,</booktitle>
<pages>1284--1289</pages>
<contexts>
<context position="5295" citStr="Vandeghinste et al. (2006)" startWordPosition="826" endWordPosition="829">hrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multilingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dictionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some researchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used language-dependent resources such as dictionary, thesaurus, and dependency parser to improve word alignment results. In this paper, we address the tran</context>
</contexts>
<marker>Vandeghinste, Schuurman, Carl, Markantonatou, Badia, 2006</marker>
<rawString>Vincent Vandeghinste, Ineka Schuurman, Michael Carl, Stella Markantonatou, and Toni Badia. 2006. METIS-II: Machine Translation for Low-Resource Languages. In Proc. of LREC-2006, pages 1284-1289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="1399" citStr="Wu, 1997" startWordPosition="213" endWordPosition="214">n if there is no bilingual corpus available for this language pair. Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. Moreover, with a small Lf-Le bilingual corpus available, our method can further improve the translation quality by using the additional Lf-Lp and Lp-Le bilingual corpora. 1 Introduction For statistical machine translation (SMT), phrasebased methods (Koehn et al., 2003; Och and Ney, 2004) and syntax-based methods (Wu, 1997; Alshawi et al. 2000; Yamada and Knignt, 2001; Melamed, 2004; Chiang, 2005; Quick et al., 2005; Mellebeek et al., 2006) outperform word-based methods (Brown et al., 1993). These methods need large bilingual corpora. However, for some languages pairs, only a small bilingual corpus is available, which will degrade the performance of statistical translation systems. To solve this problem, this paper proposes a novel method for phrase-based SMT by using a pivot language. To perform translation between languages Lf and Le, we bring in a pivot language Lp, for which there exist large bilingual corp</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A Syntax Based Statistical Translation Model.</title>
<date>2001</date>
<booktitle>In Proc. of ACL-2001,</booktitle>
<pages>523--530</pages>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A Syntax Based Statistical Translation Model. In Proc. of ACL-2001, pages 523-530.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>