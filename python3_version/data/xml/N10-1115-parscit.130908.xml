<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.995153">
An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing
</title>
<author confidence="0.99298">
Yoav Goldberg* and Michael Elhadad
</author>
<affiliation confidence="0.9972145">
Ben Gurion University of the Negev
Department of Computer Science
</affiliation>
<address confidence="0.886565">
POB 653 Be’er Sheva, 84105, Israel
</address>
<email confidence="0.998353">
{yoavg|elhadad}@cs.bgu.ac.il
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999336363636364">
We present a novel deterministic dependency pars-
ing algorithm that attempts to create the easiest arcs
in the dependency structure first in a non-directional
manner. Traditional deterministic parsing algorithms
are based on a shift-reduce framework: they traverse
the sentence from left-to-right and, at each step, per-
form one of a possible set of actions, until a complete
tree is built. A drawback of this approach is that
it is extremely local: while decisions can be based
on complex structures on the left, they can look only
at a few words to the right. In contrast, our algo-
rithm builds a dependency tree by iteratively select-
ing the best pair of neighbours to connect at each
parsing step. This allows incorporation of features
from already built structures both to the left and to the
right of the attachment point. The parser learns both
the attachment preferences and the order in which
they should be performed. The result is a determin-
istic, best-first, O(nlogn) parser, which is signifi-
cantly more accurate than best-first transition based
parsers, and nears the performance of globally opti-
mized parsing models.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999145857142857">
Dependency parsing has been a topic of active re-
search in natural language processing in the last sev-
eral years. An important part of this research effort
are the CoNLL 2006 and 2007 shared tasks (Buch-
holz and Marsi, 2006; Nivre et al., 2007), which al-
lowed for a comparison of many algorithms and ap-
proaches for this task on many languages.
</bodyText>
<footnote confidence="0.891683">
*Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
</footnote>
<bodyText confidence="0.999806117647059">
Current dependency parsers can be categorized
into three families: local-and-greedy transition-
based parsers (e.g., MALTPARSER (Nivre et al.,
2006)), globally optimized graph-based parsers
(e.g., MSTPARSER (McDonald et al., 2005)), and
hybrid systems (e.g., (Sagae and Lavie, 2006b;
Nivre and McDonald, 2008)), which combine the
output of various parsers into a new and improved
parse, and which are orthogonal to our approach.
Transition-based parsers scan the input from left
to right, are fast (O(n)), and can make use of rich
feature sets, which are based on all the previously
derived structures. However, all of their decisions
are very local, and the strict left-to-right order im-
plies that, while the feature set can use rich struc-
tural information from the left of the current attach-
ment point, it is also very restricted in information
to the right of the attachment point: traditionally,
only the next two or three input tokens are avail-
able to the parser. This limited look-ahead window
leads to error propagation and worse performance on
root and long distant dependencies relative to graph-
based parsers (McDonald and Nivre, 2007).
Graph-based parsers, on the other hand, are glob-
ally optimized. They perform an exhaustive search
over all possible parse trees for a sentence, and find
the highest scoring tree. In order to make the search
tractable, the feature set needs to be restricted to fea-
tures over single edges (first-order models) or edges
pairs (higher-order models, e.g. (McDonald and
Pereira, 2006; Carreras, 2007)). There are several
attempts at incorporating arbitrary tree-based fea-
tures but these involve either solving an ILP prob-
lem (Riedel and Clarke, 2006) or using computa-
</bodyText>
<page confidence="0.948196">
742
</page>
<note confidence="0.755624">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.997537888888889">
(1) ATTACHRIGHT(2)
-157 -68 -197 -152 231
a brown fox jumped with joy
-27 403 -47 -243 3
(2) ATTACHRIGHT(1) (3) ATTACHRIGHT(1)
fox with
fox fox joy
a brownjoy
a brown a brown
</figure>
<figureCaption confidence="0.99897">
Figure 1: Parsing the sentence “a brown fox jumped with joy”. Rounded arcs represent possible actions.
</figureCaption>
<figure confidence="0.989197428571429">
brown a brown
(4) ATTACHLEFT(2) 186 (5) ATTACHLEFT(1) (6)
-161 430 jumped
314
0
-146
12
fox
jumped with joy
246
-149
-133
270
10
-154
-52
a fox
-159
-176
246
jumped with joy
with
with joy
jumped
jumped
-2
-232
-435
</figure>
<bodyText confidence="0.99631836">
tionally intensive sampling-based methods (Naka-
gawa, 2007). As a result, these models, while accu-
rate, are slow (O(n3) for projective, first-order mod-
els, higher polynomials for higher-order models, and
worse for richer tree-feature models).
We propose a new category of dependency pars-
ing algorithms, inspired by (Shen et al., 2007): non-
directional easy-first parsing. This is a greedy, de-
terministic parsing approach, which relaxes the left-
to-right processing order of transition-based pars-
ing algorithms. By doing so, we allow the ex-
plicit incorporation of rich structural features de-
rived from both sides of the attachment point, and
implicitly take into account the entire previously de-
rived structure of the whole sentence. This exten-
sion allows the incorporation of much richer features
than those available to transition- and especially to
graph-based parsers, and greatly reduces the local-
ity of transition-based algorithm decisions. On the
other hand, it is still a greedy, best-first algorithm
leading to an efficient implementation.
We present a concrete O(nlogn) parsing algo-
rithm, which significantly outperforms state-of-the-
art transition-based parsers, while closing the gap to
graph-based parsers.
</bodyText>
<sectionHeader confidence="0.976732" genericHeader="introduction">
2 Easy-first parsing
</sectionHeader>
<bodyText confidence="0.999891523809524">
When humans comprehend a natural language sen-
tence, they arguably do it in an incremental, left-to-
right manner. However, when humans consciously
annotate a sentence with syntactic structure, they
hardly ever work in fixed left-to-right order. Rather,
they start by building several isolated constituents
by making easy and local attachment decisions and
only then combine these constituents into bigger
constituents, jumping back-and-forth over the sen-
tence and proceeding from easy to harder phenom-
ena to analyze. When getting to the harder decisions
a lot of structure is already in place, and this struc-
ture can be used in deciding a correct attachment.
Our parser follows a similar kind of annotation
process: starting from easy attachment decisions,
and proceeding to harder and harder ones. When
making later decisions, the parser has access to the
entire structure built in earlier stages. During the
training process, the parser learns its own notion of
easy and hard, and learns to defer specific kinds of
decisions until more structure is available.
</bodyText>
<sectionHeader confidence="0.932278" genericHeader="method">
3 Parsing algorithm
</sectionHeader>
<bodyText confidence="0.998995166666667">
Our (projective) parsing algorithm builds the parse
tree bottom up, using two kinds of actions: AT-
TACHLEFT(i) and ATTACHRIGHT(i) . These
actions are applied to a list of partial structures
p1, ... , pk, called pending, which is initialized with
the n words of the sentence w1, ... , wn. Each ac-
</bodyText>
<page confidence="0.997649">
743
</page>
<bodyText confidence="0.995602769230769">
tion connects the heads of two neighbouring struc-
tures, making one of them the parent of the other,
and removing the daughter from the list of partial
structures. ATTACHLEFT(i) adds a dependency
edge (pi, pi+1) and removes pi+1 from the list. AT-
TACHRIGHT(i) adds a dependency edge (pi+1, pi)
and removes pi from the list. Each action shortens
the list of partial structures by 1, and after n−1 such
actions, the list contains the root of a connected pro-
jective tree over the sentence.
Figure 1 shows an example of parsing the sen-
tence “a brown fox jumped with joy”. The pseu-
docode of the algorithm is given in Algorithm 1.
</bodyText>
<equation confidence="0.662401052631579">
Algorithm 1: Non-directional Parsing
Input: a sentence= w1 ... wn
Output: a set of dependency arcs over the
sentence (Arcs)
1 Acts = {ATTACHLEFT, ATTACHRIGHT}
2 Arcs � {}
3 pending = p1 ... pn +— w1 ... wn
4 while length(pending) &gt; 1 do
best arg max score(act(i))
actEActs
5 1&lt;i&lt;len(pending)
6 (parent, child) +— edgeFor(best)
7 Arcs.add( (parent, child) )
8 pending.remove(child)
9 end
10 return Arcs
edgeFor(act(i)) =�(pi+1,pi)
(pi, pi+1) ATTACHLEFT(i)
ATTACHRIGHT(i)
</equation>
<bodyText confidence="0.986292034482759">
At each step the algorithm chooses a spe-
cific action/location pair using a function
score(ACTION(i)), which assign scores to ac-
tion/location pairs based on the partially built
structures headed by pi and pi+1, as well as neigh-
bouring structures. The score() function is learned
from data. This scoring function reflects not only
the correctness of an attachment, but also the order
in which attachments should be made. For example,
consider the attachments (brown,fox) and (joy,with)
in Figure (1.1). While both are correct, the scoring
function prefers the (adjective,noun) attachment
over the (prep,noun) attachment. Moreover, the
attachment (jumped,with), while correct, receives
a negative score for the bare preposition “with”
(Fig. (1.1) - (1.4) ), and a high score once the verb
has its subject and the PP “with joy” is built (Fig.
(1.5) ). Ideally, we would like to score easy and
reliable attachments higher than harder less likely
attachments, thus performing attachments in order
of confidence. This strategy allows us both to limit
the extent of error propagation, and to make use of
richer contextual information in the later, harder
attachments. Unfortunately, this kind of ordering
information is not directly encoded in the data. We
must, therefore, learn how to order the decisions.
We first describe the learning algorithm (Section
4) and a feature representation (Section 5) which en-
ables us to learn an effective scoring function.
</bodyText>
<sectionHeader confidence="0.982808" genericHeader="method">
4 Learning Algorithm
</sectionHeader>
<bodyText confidence="0.955665823529412">
We use a linear model score(x) = w� · O(x), where
O(x) is a feature representation and w� is a weight
vector. We write Oact(i) to denote the feature repre-
sentation extracted for action act at location i. The
model is trained using a variant of the structured per-
ceptron (Collins, 2002), similar to the algorithm of
(Shen et al., 2007; Shen and Joshi, 2008). As usual,
we use parameter averaging to prevent the percep-
tron from overfitting.
The training algorithm is initialized with a zero
parameter vector w. The algorithm makes several
passes over the data. At each pass, we apply the
training procedure given in Algorithm 2 to every
sentence in the training set.
At training time, each sentence is parsed using the
parsing algorithm and the current w. Whenever an
invalid action is chosen by the parsing algorithm, it
is not performed (line 6). Instead, we update the pa-
rameter vector w� by decreasing the weights of the
features associated with the invalid action, and in-
creasing the weights for the currently highest scor-
ing valid action.1 We then proceed to parse the sen-
tence with the updated values. The process repeats
until a valid action is chosen.
Note that each single update does not guarantee
that the next chosen action is valid, or even different
than the previously selected action. Yet, this is still
an aggressive update procedure: we do not leave a
sentence until our parameters vector parses it cor-
1We considered 3 variants of this scheme: (1) using the high-
est scoring valid action, (2) using the leftmost valid action, and
(3) using a random valid action. The 3 variants achieved nearly
identical accuracy, while (1) converged somewhat faster than
the other two.
</bodyText>
<page confidence="0.991475">
744
</page>
<bodyText confidence="0.9991403">
rectly, and we do not proceed from one partial parse
to the next until w� predicts a correct location/action
pair. However, as the best ordering, and hence the
best attachment point is not known to us, we do not
perform a single aggressive update step. Instead, our
aggressive update is performed incrementally in a
series of smaller steps, each pushing w� away from
invalid attachments and toward valid ones. This way
we integrate the search of confident attachments into
the learning process.
</bodyText>
<figure confidence="0.806738727272727">
Algorithm 2: Structured perceptron training
for direction-less parser, over one sentence.
Input: sentence,gold arcs,current w,feature
representation O
Output: weight vector w�
1 Arcs +— {}
2 pending +— sent
3 while length(pending) &gt; 1 do
4 allowed +— {act(i)JisV alid(act(i),Gold, Arcs)}
choice arg max
actEActs
</figure>
<equation confidence="0.9130812">
5 1&lt;i&lt;len(pending)
Function isValid(action,Gold,Arcs)
1 (p, c) edgeFor(action)
2 if (V : (c, c&apos;) E Gold n (c, c&apos;) E� Arcs)
V (p, c) E� Gold then
</equation>
<sectionHeader confidence="0.607029" genericHeader="method">
3 return false
4 return true
</sectionHeader>
<bodyText confidence="0.999354875">
The function isValid(act(i), gold, arcs) (line 4)
is used to decide if the chosen action/location pair
is valid. It returns True if two conditions apply: (a)
(pi, pj) is present in gold, (b) all edges (❑, pj) in
gold are also in arcs. In words, the function verifies
that the proposed edge is indeed present in the gold
parse and that the suggested daughter already found
all its own daughters.2
</bodyText>
<footnote confidence="0.856991">
2This is in line with the Arc-Standard parsing strategy of
shift-reduce dependency parsers (Nivre, 2004). We are cur-
rently experimenting also with an Arc-Eager variant of the non-
</footnote>
<sectionHeader confidence="0.970621" genericHeader="method">
5 Feature Representation
</sectionHeader>
<bodyText confidence="0.998668">
The feature representation for an action can take
into account the original sentence, as well as
the entire parse history: Oact(i) above is actually
O(act(i), sentence, Arcs, pending).
We use binary valued features, and each feature is
conjoined with the type of action.
When designing the feature representation, we
keep in mind that our features should not only di-
rect the parser toward desired actions and away from
undesired actions, but also provide the parser with
means of choosing between several desired actions.
We want the parser to be able to defer some desired
actions until more structure is available and a more
informed prediction can be made. This desire is re-
flected in our choice of features: some of our fea-
tures are designed to signal to the parser the pres-
ence of possibly “incomplete” structures, such as an
incomplete phrase, a coordinator without conjuncts,
and so on.
When considering an action ACTION(i), we limit
ourselves to features of partial structures around the
attachment point: pi−2, pi−1, pi, pi+1, pi+2, pi+s,
that is the two structures which are to be attached by
the action (pi and pi+1), and the two neighbouring
structures on each side3.
While these features encode local context, it is lo-
cal in terms of syntactic structure, and not purely in
terms of sentence surface form. This let us capture
some, though not all, long-distance relations.
For a partial structure p, we use wp to refer to
the head word form, tp to the head word POS tag,
and lcp and rcp to the POS tags of the left-most and
right-most child of p respectively.
All our prepositions (IN) and coordinators (CC)
are lexicalized: for them, tp is in fact wptp.
We define structural, unigram, bigram and pp-
attachment features.
The structural features are: the length of the
structures (lenp), whether the structure is a word
(contains no children: ncp), and the surface distance
between structure heads (Apipj). The unigram and
bigram features are adapted from the feature set for
left-to-right Arc-Standard dependency parsing de-
</bodyText>
<footnote confidence="0.781675">
directional algorithm.
3Our sentences are padded from each side with sentence de-
limiter tokens.
</footnote>
<figure confidence="0.977307764705882">
11
6 if choice E allowed then
7 (parent, child) +— edgeFor(choice)
8 Arcs.add( (parent, child) )
9 pending.remove(child)
10 else
good +— arg max
act(j)Eallowed
w�+—
12
w + Ogood − Ochoice
13
end
14
return w�
w &apos; Oact(j)
w &apos; Oact(i)
</figure>
<page confidence="0.827502">
745
</page>
<table confidence="0.794799266666667">
Structural
for pin pi−2,pi−1,pi,pi+1,pi+2,pi+3 lenp , ncp
for p,q in (pi−2,pi−1),(pi−1,pi),(pi,pi+1),(pi+1, pi + 2),(pi+2,pi+3) Aqp , Aqptptq
Unigram
for pin pi−2,pi−1,pi,pi+1,pi+2,pi+3 tp , wp , tplcp , tprcp , tprcplcp
Bigram
for p,q in (pi,pi+1),(pi,pi+2),(pi−1,pi),(pi−1,pi+2),(pi+1,pi+2) tptq , wpwq , tpwq , wptq
tptqlcplcq , tptqrcplcq
tptqlcprcq , tptqrcprcq
PP-Attachment
if pi is a preposition wpi_1wpircpi , tpi_1wpircwpi
if pi+1 is apreposition wpi_1wpi+1rcpi+1 , tpi_1wpi+1rcwpi+1
wpiwpi+1rcpi+1 , tpiwpi+1rcwpi+1
if pi+2 is apreposition wpi+1wpi+2rcpi+2 , tpi+1wpi+2rcwpi+2
wpiwpi+2rcpi+2 , tpiwpi+2rcwpi+2
</table>
<figureCaption confidence="0.980149">
Figure 2: Feature Templates
</figureCaption>
<bodyText confidence="0.990597052631579">
scribed in (Huang et al., 2009). We extended that
feature set to include the structure on both sides of
the proposed attachment point.
In the case of unigram features, we added features
that specify the POS of a word and its left-most and
right-most children. These features provide the non-
directional model with means to prefer some attach-
ment points over others based on the types of struc-
tures already built. In English, the left- and right-
most POS-tags are good indicators of constituency.
The pp-attachment features are similar to the bi-
gram features, but fire only when one of the struc-
tures is headed by a preposition (IN). These features
are more lexicalized than the regular bigram fea-
tures, and include also the word-form of the right-
most child of the PP (rcwp). This should help the
model learn lexicalized attachment preferences such
as (hit, with-bat).
Figure 2 enumerate the feature templates we use.
</bodyText>
<sectionHeader confidence="0.991899" genericHeader="method">
6 Computational Complexity and Efficient
Implementation
</sectionHeader>
<bodyText confidence="0.99927975">
The parsing algorithm (Algorithm 1) begins with
n+1 disjoint structures (the words of the sentence +
ROOT symbol), and terminates with one connected
structure. Each iteration of the main loop connects
two structures and removes one of them, and so the
loop repeats for exactly n times.
The argmax in line 5 selects the maximal scoring
action/location pair. At iteration i, there are n − i
locations to choose from, and a naive computation of
the argmax is O(n), resulting in an O(n2) algorithm.
Each performed action changes the partial struc-
tures and with it the extracted features and the com-
puted scores. However, these changes are limited
to a fixed local context around the attachment point
of the action. Thus, we observe that the feature ex-
traction and score calculation can be performed once
for each action/location pair in a given sentence, and
reused throughout all the iterations. After each iter-
ation we need to update the extracted features and
calculated scores for only k locations, where k is a
fixed number depending on the window size used in
the feature extraction, and usually k « n.
Using this technique, we perform only (k + 1)n
feature extractions and score calculations for each
sentence, that is O(n) feature-extraction operations
per sentence.
Given the scores for each location, the argmax can
then be computed in O(logn) time using a heap,
resulting in an O(nlogn) algorithm: n iterations,
where the first iteration involves n feature extrac-
tion operations and n heap insertions, and each sub-
sequent iteration involves k feature extractions and
heap updates.
We note that the dominating factor in polynomial-
time discriminative parsers, is by far the feature-
extraction and score calculation. It makes sense to
compare parser complexity in terms of these opera-
tions only.4 Table 1 compares the complexity of our
4Indeed, in our implementation we do not use a heap, and
opt instead to find the argmax using a simple O(n) max oper-
ation. This O(n2) algorithm is faster in practice than the heap
based one, as both are dominated by the O(n) feature extrac-
tion, while the cost of the O(n) max calculationis negligible
compared to the constants involved in heap maintenance.
</bodyText>
<page confidence="0.996523">
746
</page>
<table confidence="0.822898428571429">
parser to other dependency parsing frameworks.
Parser Runtime Features / Scoring
MALT O(n) O(n)
MST O(n3) O(n2)
MST2 O(n3) O(n3)
BEAM O(n * beam) O(n * beam)
NONDIR (This Work) O(nlogn) O(n)
</table>
<tableCaption confidence="0.98801125">
Table 1: Complexity of different parsing frameworks.
MST: first order MST parser, MST2: second order MST
parser, MALT: shift-reduce left-to-right parsing. BEAM:
beam search parser, as in (Zhang and Clark, 2008)
</tableCaption>
<bodyText confidence="0.999880071428571">
In terms of feature extraction and score calcula-
tion operations, our algorithm has the same cost as
traditional shift-reduce (MALT) parsers, and is an
order of magnitude more efficient than graph-based
(MST) parsers. Beam-search decoding for left-to-
right parsers (Zhang and Clark, 2008) is also linear,
but has an additional linear dependence on the beam-
size. The reported results in (Zhang and Clark,
2008) use a beam size of 64, compared to our con-
stant of k = 6.
Our Python-based implementation5 (the percep-
tron is implemented in a C extension module) parses
about 40 tagged sentences per second on an Intel
based MacBook laptop.
</bodyText>
<sectionHeader confidence="0.994202" genericHeader="evaluation">
7 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999977846153846">
We evaluate the parser using the WSJ Treebank. The
trees were converted to dependency structures with
the Penn2Malt conversion program,6 using the head-
finding rules from (Yamada and Matsumoto, 2003).7
We use Sections 2-21 for training, Section 22 for
development, and Section 23 as the final test set.
The text is automatically POS tagged using a trigram
HMM based POS tagger prior to training and pars-
ing. Each section is tagged after training the tagger
on all other sections. The tagging accuracy of the
tagger is 96.5 for the training set and 96.8 for the
test set. While better taggers exist, we believe that
the simpler HMM tagger overfits less, and is more
</bodyText>
<footnote confidence="0.942443166666667">
5http://www.cs.bgu.ac.il/—yoavg/software/
6http://w3.msi.vxu.se/—nivre/research/Penn2Malt.html
7While other and better conversions exist (see, e.g., (Johans-
son and Nugues, 2007; Sangati and Mazza, 2009)), this con-
version heuristic is still the most widely used. Using the same
conversion facilitates comparison with previous works.
</footnote>
<bodyText confidence="0.995408777777778">
representative of the tagging performance on non-
WSJ corpus texts.
Parsers We evaluate our parser against the
transition-based MALT parser and the graph-based
MST parser. We use version 1.2 of MALT parser8,
with the settings used for parsing English in the
CoNLL 2007 shared task. For the MST parser9,
we use the default first-order, projective parser set-
tings, which provide state-of-the-art results for En-
glish. All parsers are trained and tested on the same
data. Our parser is trained for 20 iterations.
Evaluation Measures We evaluate the parsers using
three common measures:
(unlabeled) Accuracy: percentage of tokens which
got assigned their correct parent.
Root: The percentage of sentences in which the
ROOT attachment is correct.
Complete: the percentage of sentences in which all
tokens were assigned their correct parent.
Unlike most previous work on English dependency
parsing, we do not exclude punctuation marks from
the evaluation.
Results are presented in Table 2. Our non-
directional easy-first parser significantly outper-
forms the left-to-right greedy MALT parser in terms
of accuracy and root prediction, and significantly
outperforms both parsers in terms of exact match.
The globally optimized MST parser is better in root-
prediction, and slightly better in terms of accuracy.
We evaluated the parsers also on the English
dataset from the CoNLL 2007 shared task. While
this dataset is also derived from the WSJ Treebank, it
differs from the previous dataset in two important as-
pects: it is much smaller in size, and it is created us-
ing a different conversion procedure, which is more
linguistically adequate. For these experiments, we
use the dataset POS tags, and the same parameters as
in the previous set of experiments: we train the non-
directional parser for 20 iterations, with the same
feature set. The CoNLL dataset contains some non-
projective constructions. MALT and MST deal with
non-projectivity. For the non-directional parser, we
projectivize the training set prior to training using
the procedure described in (Carreras, 2007).
Results are presented in Table 3.
</bodyText>
<footnote confidence="0.9999415">
8http://maltparser.org/dist/1.2/malt-1.2.tar.gz
9http://sourceforge.net/projects/mstparser/
</footnote>
<page confidence="0.983234">
747
</page>
<table confidence="0.999726">
Parser Accuracy Root Complete
MALT 88.36 87.04 34.14
MST 90.05 93.95 34.64
NONDIR (this work) 89.70 91.50 37.50
</table>
<tableCaption confidence="0.854164">
Table 2: Unlabeled dependency accuracy on PTB Section
23, automatic POS-tags, including punctuation.
</tableCaption>
<table confidence="0.99993525">
Parser Accuracy Root Complete
MALT 85.82 87.85 24.76
MST 89.08 93.45 24.76
NONDIR (this work) 88.34 91.12 29.43
</table>
<tableCaption confidence="0.9980925">
Table 3: Unlabeled dependency accuracy on CoNLL
2007 English test set, including punctuation.
</tableCaption>
<bodyText confidence="0.999800058823529">
While all models suffer from the move to the
smaller dataset and the more challenging annotation
scheme, the overall story remains the same: the non-
directional parser is better than MALT but not as
good as MST in terms of parent-accuracy and root
prediction, and is better than both MALT and MST
in terms of producing complete correct parses.
That the non-directional parser has lower accu-
racy but more exact matches than the MST parser
can be explained by it being a deterministic parser,
and hence still vulnerable to error propagation: once
it erred once, it is likely to do so again, result-
ing in low accuracies for some sentences. How-
ever, due to the easy-first policy, it manages to parse
many sentences without a single error, which lead
to higher exact-match scores. The non-directional
parser avoids error propagation by not making the
initial error. On average, the non-directional parser
manages to assign correct heads to over 60% of the
tokens before making its first error.
The MST parser would have ranked 5th in the
shared task, and NONDIR would have ranked 7th.
The better ranking systems in the shared task
are either higher-order global models, beam-search
based systems, or ensemble-based systems, all of
which are more complex and less efficient than the
NONDIR parser.
Parse Diversity The parses produced by the non-
directional parser are different than the parses pro-
duced by the graph-based and left-to-right parsers.
To demonstrate this difference, we performed an Or-
acle experiment, in which we combine the output of
several parsers by choosing, for each sentence, the
parse with the highest score. Results are presented
</bodyText>
<table confidence="0.999604090909091">
Combination Accuracy Complete
Penn2Malt, Train 2-21, Test 23
MALT+MST 92.29 44.03
NONDIR+MALT 92.19 45.48
NONDIR+MST 92.53 44.41
NONDIR+MST+MALT 93.54 49.79
CoNLL 2007
MALT+MST 91.50 33.64
NONDIR+MALT 91.02 34.11
NONDIR+MST 91.90 34.11
NONDIR+MST+MALT 92.70 38.31
</table>
<tableCaption confidence="0.914789">
Table 4: Parser combination with Oracle, choosing the
highest scoring parse for each sentence of the test-set.
</tableCaption>
<bodyText confidence="0.997492142857143">
in Table 4.
A non-oracle blending of MALT+MST+NONDIR
using Sagae and Lavie’s (2006) simplest combina-
tion method assigning each component the same
weight, yield an accuracy of 90.8 on the CoNLL
2007 English dataset, making it the highest scoring
system among the participants.
</bodyText>
<subsectionHeader confidence="0.751778">
7.1 Error Analysis / Limitations
</subsectionHeader>
<bodyText confidence="0.999986884615385">
When we investigate the POS category of mistaken
instances, we see that for all parsers, nodes with
structures of depth 2 and more which are assigned
an incorrect head are predominantly PPs (headed
by ’IN’), followed by NPs (headed by ’NN’). All
parsers have a hard time dealing with PP attachment,
but MST parser is better at it than NONDIR, and both
are better than MALT.
Looking further at the mistaken instances, we no-
tice a tendency of the PP mistakes of the NONDIR
parser to involve, before the PP, an NP embedded
in a relative clause. This reveals a limitation of our
parser: recall that for an edge to be built, the child
must first acquire all its own children. This means
that in case of relative clauses such as “I saw the
boy [who ate the pizza] with my eyes”, the parser
must decide if the PP “with my eyes” should be at-
tached to “the pizza” or not before it is allowed to
build parts of the outer NP (“the boy who... ”). In
this case, the verb “saw” and the noun “boy” are
both outside of the sight of the parser when decid-
ing on the PP attachment, and it is forced to make a
decision in ignorance, which, in many cases, leads
to mistakes. The globally optimized MST does not
suffer as much from such cases. We plan to address
this deficiency in future work.
</bodyText>
<page confidence="0.996306">
748
</page>
<sectionHeader confidence="0.999774" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.9999065">
Deterministic shift-reduce parsers are restricted by a
strict left-to-right processing order. Such parsers can
rely on rich syntactic information on the left, but not
on the right, of the decision point. They are forced
to commit early, and suffer from error propagation.
Our non-directional parser addresses these deficien-
cies by discarding the strict left-to-right processing
order, and attempting to make easier decisions be-
fore harder ones. Other methods of dealing with
these deficiencies were proposed over the years:
Several Passes Yamada and Matsumoto’s (2003)
pioneering work introduces a shift-reduce parser
which makes several left-to-right passes over a sen-
tence. Each pass adds structure, which can then be
used in subsequent passes. Sagae and Lavie (2006b)
extend this model to alternate between left-to-right
and right-to-left passes. This model is similar to
ours, in that it attempts to defer harder decisions to
later passes over the sentence, and allows late deci-
sions to make use of rich syntactic information (built
in earlier passes) on both sides of the decision point.
However, the model is not explicitly trained to op-
timize attachment ordering, has an O(n2) runtime
complexity, and produces results which are inferior
to current single-pass shift-reduce parsers.
Beam Search Several researchers dealt with the
early-commitment and error propagation of deter-
ministic parsers by extending the greedy decisions
with various flavors of beam-search (Sagae and
Lavie, 2006a; Zhang and Clark, 2008; Titov and
Henderson, 2007). This approach works well and
produces highly competitive results. Beam search
can be incorporated into our parser as well. We leave
this investigation to future work.
Strict left-to-right ordering is also prevalent in se-
quence tagging. Indeed, one major influence on
our work is Shen et.al.’s bi-directional POS-tagging
algorithm (Shen et al., 2007), which combines a
perceptron learning procedure similar to our own
with beam search to produce a state-of-the-art POS-
tagger, which does not rely on left-to-right process-
ing. Shen and Joshi (2008) extends the bidirectional
tagging algorithm to LTAG parsing, with good re-
sults. We build on top of that work and present a
concrete and efficient greedy non-directional depen-
dency parsing algorithm.
Structure Restrictions Eisner and Smith (2005)
propose to improve the efficiency of a globally op-
timized parser by posing hard constraints on the
lengths of arcs it can produce. Such constraints
pose an explicit upper bound on parser accuracy.10
Our parsing model does not pose such restrictions.
Shorter edges are arguably easier to predict, and our
parses builds them early in time. However, it is
also capable of producing long dependencies at later
stages in the parsing process. Indeed, the distribu-
tion of arc lengths produced by our parser is similar
to those produced by the MALT and MST parsers.
</bodyText>
<sectionHeader confidence="0.998703" genericHeader="conclusions">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999974419354839">
We presented a non-directional deterministic depen-
dency parsing algorithm, which is not restricted by
the left-to-right parsing order of other deterministic
parsers. Instead, it works in an easy-first order. This
strategy allows using more context at each decision.
The parser learns both what and when to connect.
We show that this parsing algorithm significantly
outperforms a left-to-right deterministic algorithm.
While it still lags behind globally optimized pars-
ing algorithms in terms of accuracy and root pre-
diction, it is much better in terms of exact match,
and much faster. As our parsing framework can eas-
ily and efficiently utilize more structural information
than globally optimized parsers, we believe that with
some enhancements and better features, it can out-
perform globally optimized algorithms, especially
when more structural information is needed, such as
for morphologically rich languages.
Moreover, we show that our parser produces
different structures than those produced by both
left-to-right and globally optimized parsers, mak-
ing it a good candidate for inclusion in an ensem-
ble system. Indeed, a simple combination scheme
of graph-based, left-to-right and non-directional
parsers yields state-of-the-art results on English de-
pendency parsing on the CoNLL 2007 dataset.
We hope that further work on this non-directional
parsing framework will pave the way to better under-
standing of an interesting cognitive question: which
kinds of parsing decisions are hard to make, and
which linguistic constructs are hard to analyze?
</bodyText>
<footnote confidence="0.876198">
10In (Dreyer et al., 2006), constraints are chosen “to be the
minimum value that will allow recovery of 90% of the left
(right) dependencies in the training corpus”.
</footnote>
<page confidence="0.998217">
749
</page>
<sectionHeader confidence="0.995857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999554849315069">
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL
Shared Task, EMNLP-CoNLL.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc of EMNLP.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In Proc. of CoNLL, pages 201–
205, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Jason Eisner and Noah A. Smith. 2005. arsing with soft
and hard constraints on dependency length. In Proc.
of IWPT.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc of EMNLP.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc of NODALIDA.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proc. of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proc of EACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc of ACL.
Tetsuji Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proc. of EMNLP-CoNLL.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proc. of ACL, pages 950–958, Columbus, Ohio,
June. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, and Jens Nillson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proc. of LREC.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mcdon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of EMNLP-CoNLL.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Incremental Parsing: Bringing
Engineering and Cognition Together, ACL-Workshop.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proc. of EMNLP 2006, July.
Kenji Sagae and Alon Lavie. 2006a. A best-first proba-
bilistic shift-reduce parser. In Proc of ACL.
Kenji Sagae and Alon Lavie. 2006b. Parser combination
by reparsing. In Proc of NAACL.
Federico Sangati and Chiara Mazza. 2009. An english
dependency treebank a` la tesni`ere. In Proc of TLT8.
Libin Shen and Aravind K. Joshi. 2008. Ltag depen-
dency parsing with bidirectional incremental construc-
tion. In Proc of EMNLP.
Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proc of ACL.
Ivan Titov and James Henderson. 2007. Fast and robust
multilingual dependency parsing with a generative la-
tent variable model. In Proc. of EMNLP-CoNLL.
Yamada and Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of
IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proc of EMNLP.
</reference>
<page confidence="0.997642">
750
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.453649">
<title confidence="0.8558145">An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing and</title>
<author confidence="0.67782">Ben Gurion University of the</author>
<affiliation confidence="0.963865">Department of Computer</affiliation>
<address confidence="0.804251">POB 653 Be’er Sheva, 84105,</address>
<abstract confidence="0.999099173913043">We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a determinbest-first, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="1610" citStr="Buchholz and Marsi, 2006" startWordPosition="252" endWordPosition="256">m already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models. 1 Introduction Dependency parsing has been a topic of active research in natural language processing in the last several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. *Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., MALTPARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., MSTPARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse,</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. of CoNLL Shared Task, EMNLP-CoNLL.</booktitle>
<contexts>
<context position="3364" citStr="Carreras, 2007" startWordPosition="534" endWordPosition="535">e input tokens are available to the parser. This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics (1) ATTACHRIGHT(2) -157 -68 -197 -152 231 a brown fox jumped with joy -27 403 -47 -243 3 (2) ATTACHRIGHT(1) (3) ATTACHRIGHT(1) fox with fox fox joy a brownjoy a brown a brown Figure 1: Parsing the sentence “a brown fox jumped with </context>
<context position="22976" citStr="Carreras, 2007" startWordPosition="3705" endWordPosition="3706">differs from the previous dataset in two important aspects: it is much smaller in size, and it is created using a different conversion procedure, which is more linguistically adequate. For these experiments, we use the dataset POS tags, and the same parameters as in the previous set of experiments: we train the nondirectional parser for 20 iterations, with the same feature set. The CoNLL dataset contains some nonprojective constructions. MALT and MST deal with non-projectivity. For the non-directional parser, we projectivize the training set prior to training using the procedure described in (Carreras, 2007). Results are presented in Table 3. 8http://maltparser.org/dist/1.2/malt-1.2.tar.gz 9http://sourceforge.net/projects/mstparser/ 747 Parser Accuracy Root Complete MALT 88.36 87.04 34.14 MST 90.05 93.95 34.64 NONDIR (this work) 89.70 91.50 37.50 Table 2: Unlabeled dependency accuracy on PTB Section 23, automatic POS-tags, including punctuation. Parser Accuracy Root Complete MALT 85.82 87.85 24.76 MST 89.08 93.45 24.76 NONDIR (this work) 88.34 91.12 29.43 Table 3: Unlabeled dependency accuracy on CoNLL 2007 English test set, including punctuation. While all models suffer from the move to the smal</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proc. of CoNLL Shared Task, EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc of EMNLP.</booktitle>
<contexts>
<context position="9690" citStr="Collins, 2002" startWordPosition="1552" endWordPosition="1553">der attachments. Unfortunately, this kind of ordering information is not directly encoded in the data. We must, therefore, learn how to order the decisions. We first describe the learning algorithm (Section 4) and a feature representation (Section 5) which enables us to learn an effective scoring function. 4 Learning Algorithm We use a linear model score(x) = w� · O(x), where O(x) is a feature representation and w� is a weight vector. We write Oact(i) to denote the feature representation extracted for action act at location i. The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008). As usual, we use parameter averaging to prevent the perceptron from overfitting. The training algorithm is initialized with a zero parameter vector w. The algorithm makes several passes over the data. At each pass, we apply the training procedure given in Algorithm 2 to every sentence in the training set. At training time, each sentence is parsed using the parsing algorithm and the current w. Whenever an invalid action is chosen by the parsing algorithm, it is not performed (line 6). Instead, we update the parameter vecto</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Vine parsing and minimum risk reranking for speed and precision.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>201--205</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Dreyer, Smith, Smith, 2006</marker>
<rawString>Markus Dreyer, David A. Smith, and Noah A. Smith. 2006. Vine parsing and minimum risk reranking for speed and precision. In Proc. of CoNLL, pages 201– 205, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Noah A Smith</author>
</authors>
<title>arsing with soft and hard constraints on dependency length.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="29486" citStr="Eisner and Smith (2005)" startWordPosition="4742" endWordPosition="4745">trict left-to-right ordering is also prevalent in sequence tagging. Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing. Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG parsing, with good results. We build on top of that work and present a concrete and efficient greedy non-directional dependency parsing algorithm. Structure Restrictions Eisner and Smith (2005) propose to improve the efficiency of a globally optimized parser by posing hard constraints on the lengths of arcs it can produce. Such constraints pose an explicit upper bound on parser accuracy.10 Our parsing model does not pose such restrictions. Shorter edges are arguably easier to predict, and our parses builds them early in time. However, it is also capable of producing long dependencies at later stages in the parsing process. Indeed, the distribution of arc lengths produced by our parser is similar to those produced by the MALT and MST parsers. 9 Discussion We presented a non-direction</context>
</contexts>
<marker>Eisner, Smith, 2005</marker>
<rawString>Jason Eisner and Noah A. Smith. 2005. arsing with soft and hard constraints on dependency length. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proc of EMNLP.</booktitle>
<contexts>
<context position="15721" citStr="Huang et al., 2009" startWordPosition="2534" endWordPosition="2537"> p,q in (pi−2,pi−1),(pi−1,pi),(pi,pi+1),(pi+1, pi + 2),(pi+2,pi+3) Aqp , Aqptptq Unigram for pin pi−2,pi−1,pi,pi+1,pi+2,pi+3 tp , wp , tplcp , tprcp , tprcplcp Bigram for p,q in (pi,pi+1),(pi,pi+2),(pi−1,pi),(pi−1,pi+2),(pi+1,pi+2) tptq , wpwq , tpwq , wptq tptqlcplcq , tptqrcplcq tptqlcprcq , tptqrcprcq PP-Attachment if pi is a preposition wpi_1wpircpi , tpi_1wpircwpi if pi+1 is apreposition wpi_1wpi+1rcpi+1 , tpi_1wpi+1rcwpi+1 wpiwpi+1rcpi+1 , tpiwpi+1rcwpi+1 if pi+2 is apreposition wpi+1wpi+2rcpi+2 , tpi+1wpi+2rcwpi+2 wpiwpi+2rcpi+2 , tpiwpi+2rcwpi+2 Figure 2: Feature Templates scribed in (Huang et al., 2009). We extended that feature set to include the structure on both sides of the proposed attachment point. In the case of unigram features, we added features that specify the POS of a word and its left-most and right-most children. These features provide the nondirectional model with means to prefer some attachment points over others based on the types of structures already built. In English, the left- and rightmost POS-tags are good indicators of constituency. The pp-attachment features are similar to the bigram features, but fire only when one of the structures is headed by a preposition (IN). </context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proc of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In Proc of NODALIDA.</booktitle>
<contexts>
<context position="20762" citStr="Johansson and Nugues, 2007" startWordPosition="3356" endWordPosition="3360">ns 2-21 for training, Section 22 for development, and Section 23 as the final test set. The text is automatically POS tagged using a trigram HMM based POS tagger prior to training and parsing. Each section is tagged after training the tagger on all other sections. The tagging accuracy of the tagger is 96.5 for the training set and 96.8 for the test set. While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more 5http://www.cs.bgu.ac.il/—yoavg/software/ 6http://w3.msi.vxu.se/—nivre/research/Penn2Malt.html 7While other and better conversions exist (see, e.g., (Johansson and Nugues, 2007; Sangati and Mazza, 2009)), this conversion heuristic is still the most widely used. Using the same conversion facilitates comparison with previous works. representative of the tagging performance on nonWSJ corpus texts. Parsers We evaluate our parser against the transition-based MALT parser and the graph-based MST parser. We use version 1.2 of MALT parser8, with the settings used for parsing English in the CoNLL 2007 shared task. For the MST parser9, we use the default first-order, projective parser settings, which provide state-of-the-art results for English. All parsers are trained and tes</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proc of NODALIDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2968" citStr="McDonald and Nivre, 2007" startWordPosition="469" endWordPosition="472">use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point: traditionally, only the next two or three input tokens are available to the parser. This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The </context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc of EACL.</booktitle>
<contexts>
<context position="3347" citStr="McDonald and Pereira, 2006" startWordPosition="530" endWordPosition="533">y, only the next two or three input tokens are available to the parser. This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics (1) ATTACHRIGHT(2) -157 -68 -197 -152 231 a brown fox jumped with joy -27 403 -47 -243 3 (2) ATTACHRIGHT(1) (3) ATTACHRIGHT(1) fox with fox fox joy a brownjoy a brown a brown Figure 1: Parsing the sentence “a brown</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="2054" citStr="McDonald et al., 2005" startWordPosition="318" endWordPosition="321">ve research in natural language processing in the last several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. *Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., MALTPARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., MSTPARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricte</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Multilingual dependency parsing using global features.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="4284" citStr="Nakagawa, 2007" startWordPosition="686" endWordPosition="688">ia, June 2010. c�2010 Association for Computational Linguistics (1) ATTACHRIGHT(2) -157 -68 -197 -152 231 a brown fox jumped with joy -27 403 -47 -243 3 (2) ATTACHRIGHT(1) (3) ATTACHRIGHT(1) fox with fox fox joy a brownjoy a brown a brown Figure 1: Parsing the sentence “a brown fox jumped with joy”. Rounded arcs represent possible actions. brown a brown (4) ATTACHLEFT(2) 186 (5) ATTACHLEFT(1) (6) -161 430 jumped 314 0 -146 12 fox jumped with joy 246 -149 -133 270 10 -154 -52 a fox -159 -176 246 jumped with joy with with joy jumped jumped -2 -232 -435 tionally intensive sampling-based methods (Nakagawa, 2007). As a result, these models, while accurate, are slow (O(n3) for projective, first-order models, higher polynomials for higher-order models, and worse for richer tree-feature models). We propose a new category of dependency parsing algorithms, inspired by (Shen et al., 2007): nondirectional easy-first parsing. This is a greedy, deterministic parsing approach, which relaxes the leftto-right processing order of transition-based parsing algorithms. By doing so, we allow the explicit incorporation of rich structural features derived from both sides of the attachment point, and implicitly take into</context>
</contexts>
<marker>Nakagawa, 2007</marker>
<rawString>Tetsuji Nakagawa. 2007. Multilingual dependency parsing using global features. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>950--958</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2133" citStr="Nivre and McDonald, 2008" startWordPosition="330" endWordPosition="333">ortant part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. *Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., MALTPARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., MSTPARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point: traditionally, only the </context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proc. of ACL, pages 950–958, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nillson</author>
</authors>
<title>MaltParser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="1972" citStr="Nivre et al., 2006" startWordPosition="308" endWordPosition="311">ized parsing models. 1 Introduction Dependency parsing has been a topic of active research in natural language processing in the last several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. *Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., MALTPARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., MSTPARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural in</context>
</contexts>
<marker>Nivre, Hall, Nillson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nillson. 2006. MaltParser: A data-driven parser-generator for dependency parsing. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan Mcdonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, Mcdonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mcdonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>ACL-Workshop.</pages>
<contexts>
<context position="12576" citStr="Nivre, 2004" startWordPosition="2035" endWordPosition="2036"> edgeFor(action) 2 if (V : (c, c&apos;) E Gold n (c, c&apos;) E� Arcs) V (p, c) E� Gold then 3 return false 4 return true The function isValid(act(i), gold, arcs) (line 4) is used to decide if the chosen action/location pair is valid. It returns True if two conditions apply: (a) (pi, pj) is present in gold, (b) all edges (❑, pj) in gold are also in arcs. In words, the function verifies that the proposed edge is indeed present in the gold parse and that the suggested daughter already found all its own daughters.2 2This is in line with the Arc-Standard parsing strategy of shift-reduce dependency parsers (Nivre, 2004). We are currently experimenting also with an Arc-Eager variant of the non5 Feature Representation The feature representation for an action can take into account the original sentence, as well as the entire parse history: Oact(i) above is actually O(act(i), sentence, Arcs, pending). We use binary valued features, and each feature is conjoined with the type of action. When designing the feature representation, we keep in mind that our features should not only direct the parser toward desired actions and away from undesired actions, but also provide the parser with means of choosing between seve</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Incremental Parsing: Bringing Engineering and Cognition Together, ACL-Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>James Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP 2006, July. Kenji Sagae and Alon Lavie.</booktitle>
<contexts>
<context position="3514" citStr="Riedel and Clarke, 2006" startWordPosition="555" endWordPosition="558">distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics (1) ATTACHRIGHT(2) -157 -68 -197 -152 231 a brown fox jumped with joy -27 403 -47 -243 3 (2) ATTACHRIGHT(1) (3) ATTACHRIGHT(1) fox with fox fox joy a brownjoy a brown a brown Figure 1: Parsing the sentence “a brown fox jumped with joy”. Rounded arcs represent possible actions. brown a brown (4) ATTACHLEFT(2) 186 (5) ATTACHLEFT(1) (6) -161 430 jumped 314 0 -146 12 fox jumped with</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>Sebastian Riedel and James Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proc. of EMNLP 2006, July. Kenji Sagae and Alon Lavie. 2006a. A best-first probabilistic shift-reduce parser. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proc of NAACL.</booktitle>
<contexts>
<context position="2105" citStr="Sagae and Lavie, 2006" startWordPosition="326" endWordPosition="329">st several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. *Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., MALTPARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., MSTPARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment poi</context>
<context position="27917" citStr="Sagae and Lavie (2006" startWordPosition="4503" endWordPosition="4506">n the left, but not on the right, of the decision point. They are forced to commit early, and suffer from error propagation. Our non-directional parser addresses these deficiencies by discarding the strict left-to-right processing order, and attempting to make easier decisions before harder ones. Other methods of dealing with these deficiencies were proposed over the years: Several Passes Yamada and Matsumoto’s (2003) pioneering work introduces a shift-reduce parser which makes several left-to-right passes over a sentence. Each pass adds structure, which can then be used in subsequent passes. Sagae and Lavie (2006b) extend this model to alternate between left-to-right and right-to-left passes. This model is similar to ours, in that it attempts to defer harder decisions to later passes over the sentence, and allows late decisions to make use of rich syntactic information (built in earlier passes) on both sides of the decision point. However, the model is not explicitly trained to optimize attachment ordering, has an O(n2) runtime complexity, and produces results which are inferior to current single-pass shift-reduce parsers. Beam Search Several researchers dealt with the early-commitment and error propa</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006b. Parser combination by reparsing. In Proc of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federico Sangati</author>
<author>Chiara Mazza</author>
</authors>
<title>An english dependency treebank a` la tesni`ere.</title>
<date>2009</date>
<booktitle>In Proc of TLT8.</booktitle>
<contexts>
<context position="20788" citStr="Sangati and Mazza, 2009" startWordPosition="3361" endWordPosition="3364">n 22 for development, and Section 23 as the final test set. The text is automatically POS tagged using a trigram HMM based POS tagger prior to training and parsing. Each section is tagged after training the tagger on all other sections. The tagging accuracy of the tagger is 96.5 for the training set and 96.8 for the test set. While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more 5http://www.cs.bgu.ac.il/—yoavg/software/ 6http://w3.msi.vxu.se/—nivre/research/Penn2Malt.html 7While other and better conversions exist (see, e.g., (Johansson and Nugues, 2007; Sangati and Mazza, 2009)), this conversion heuristic is still the most widely used. Using the same conversion facilitates comparison with previous works. representative of the tagging performance on nonWSJ corpus texts. Parsers We evaluate our parser against the transition-based MALT parser and the graph-based MST parser. We use version 1.2 of MALT parser8, with the settings used for parsing English in the CoNLL 2007 shared task. For the MST parser9, we use the default first-order, projective parser settings, which provide state-of-the-art results for English. All parsers are trained and tested on the same data. Our </context>
</contexts>
<marker>Sangati, Mazza, 2009</marker>
<rawString>Federico Sangati and Chiara Mazza. 2009. An english dependency treebank a` la tesni`ere. In Proc of TLT8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>Ltag dependency parsing with bidirectional incremental construction.</title>
<date>2008</date>
<booktitle>In Proc of EMNLP.</booktitle>
<contexts>
<context position="9761" citStr="Shen and Joshi, 2008" startWordPosition="1563" endWordPosition="1566">n is not directly encoded in the data. We must, therefore, learn how to order the decisions. We first describe the learning algorithm (Section 4) and a feature representation (Section 5) which enables us to learn an effective scoring function. 4 Learning Algorithm We use a linear model score(x) = w� · O(x), where O(x) is a feature representation and w� is a weight vector. We write Oact(i) to denote the feature representation extracted for action act at location i. The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008). As usual, we use parameter averaging to prevent the perceptron from overfitting. The training algorithm is initialized with a zero parameter vector w. The algorithm makes several passes over the data. At each pass, we apply the training procedure given in Algorithm 2 to every sentence in the training set. At training time, each sentence is parsed using the parsing algorithm and the current w. Whenever an invalid action is chosen by the parsing algorithm, it is not performed (line 6). Instead, we update the parameter vector w� by decreasing the weights of the features associated with the inva</context>
<context position="29240" citStr="Shen and Joshi (2008)" startWordPosition="4705" endWordPosition="4708">(Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007). This approach works well and produces highly competitive results. Beam search can be incorporated into our parser as well. We leave this investigation to future work. Strict left-to-right ordering is also prevalent in sequence tagging. Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing. Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG parsing, with good results. We build on top of that work and present a concrete and efficient greedy non-directional dependency parsing algorithm. Structure Restrictions Eisner and Smith (2005) propose to improve the efficiency of a globally optimized parser by posing hard constraints on the lengths of arcs it can produce. Such constraints pose an explicit upper bound on parser accuracy.10 Our parsing model does not pose such restrictions. Shorter edges are arguably easier to predict, and our parses builds them early in time. However, it is </context>
</contexts>
<marker>Shen, Joshi, 2008</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2008. Ltag dependency parsing with bidirectional incremental construction. In Proc of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind K Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="4559" citStr="Shen et al., 2007" startWordPosition="728" endWordPosition="731"> brown fox jumped with joy”. Rounded arcs represent possible actions. brown a brown (4) ATTACHLEFT(2) 186 (5) ATTACHLEFT(1) (6) -161 430 jumped 314 0 -146 12 fox jumped with joy 246 -149 -133 270 10 -154 -52 a fox -159 -176 246 jumped with joy with with joy jumped jumped -2 -232 -435 tionally intensive sampling-based methods (Nakagawa, 2007). As a result, these models, while accurate, are slow (O(n3) for projective, first-order models, higher polynomials for higher-order models, and worse for richer tree-feature models). We propose a new category of dependency parsing algorithms, inspired by (Shen et al., 2007): nondirectional easy-first parsing. This is a greedy, deterministic parsing approach, which relaxes the leftto-right processing order of transition-based parsing algorithms. By doing so, we allow the explicit incorporation of rich structural features derived from both sides of the attachment point, and implicitly take into account the entire previously derived structure of the whole sentence. This extension allows the incorporation of much richer features than those available to transition- and especially to graph-based parsers, and greatly reduces the locality of transition-based algorithm d</context>
<context position="9738" citStr="Shen et al., 2007" startWordPosition="1559" endWordPosition="1562">ordering information is not directly encoded in the data. We must, therefore, learn how to order the decisions. We first describe the learning algorithm (Section 4) and a feature representation (Section 5) which enables us to learn an effective scoring function. 4 Learning Algorithm We use a linear model score(x) = w� · O(x), where O(x) is a feature representation and w� is a weight vector. We write Oact(i) to denote the feature representation extracted for action act at location i. The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008). As usual, we use parameter averaging to prevent the perceptron from overfitting. The training algorithm is initialized with a zero parameter vector w. The algorithm makes several passes over the data. At each pass, we apply the training procedure given in Algorithm 2 to every sentence in the training set. At training time, each sentence is parsed using the parsing algorithm and the current w. Whenever an invalid action is chosen by the parsing algorithm, it is not performed (line 6). Instead, we update the parameter vector w� by decreasing the weights of the features a</context>
<context position="29044" citStr="Shen et al., 2007" startWordPosition="4674" endWordPosition="4677">ce parsers. Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007). This approach works well and produces highly competitive results. Beam search can be incorporated into our parser as well. We leave this investigation to future work. Strict left-to-right ordering is also prevalent in sequence tagging. Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing. Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG parsing, with good results. We build on top of that work and present a concrete and efficient greedy non-directional dependency parsing algorithm. Structure Restrictions Eisner and Smith (2005) propose to improve the efficiency of a globally optimized parser by posing hard constraints on the lengths of arcs it can produce. Such constraints pose an e</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007. Guided learning for bidirectional sequence classification. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Fast and robust multilingual dependency parsing with a generative latent variable model.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="28693" citStr="Titov and Henderson, 2007" startWordPosition="4621" endWordPosition="4624">ecisions to later passes over the sentence, and allows late decisions to make use of rich syntactic information (built in earlier passes) on both sides of the decision point. However, the model is not explicitly trained to optimize attachment ordering, has an O(n2) runtime complexity, and produces results which are inferior to current single-pass shift-reduce parsers. Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007). This approach works well and produces highly competitive results. Beam search can be incorporated into our parser as well. We leave this investigation to future work. Strict left-to-right ordering is also prevalent in sequence tagging. Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing. Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG </context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007. Fast and robust multilingual dependency parsing with a generative latent variable model. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yamada</author>
<author>Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="20120" citStr="Yamada and Matsumoto, 2003" startWordPosition="3259" endWordPosition="3262">ecoding for left-toright parsers (Zhang and Clark, 2008) is also linear, but has an additional linear dependence on the beamsize. The reported results in (Zhang and Clark, 2008) use a beam size of 64, compared to our constant of k = 6. Our Python-based implementation5 (the perceptron is implemented in a C extension module) parses about 40 tagged sentences per second on an Intel based MacBook laptop. 7 Experiments and Results We evaluate the parser using the WSJ Treebank. The trees were converted to dependency structures with the Penn2Malt conversion program,6 using the headfinding rules from (Yamada and Matsumoto, 2003).7 We use Sections 2-21 for training, Section 22 for development, and Section 23 as the final test set. The text is automatically POS tagged using a trigram HMM based POS tagger prior to training and parsing. Each section is tagged after training the tagger on all other sections. The tagging accuracy of the tagger is 96.5 for the training set and 96.8 for the test set. While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more 5http://www.cs.bgu.ac.il/—yoavg/software/ 6http://w3.msi.vxu.se/—nivre/research/Penn2Malt.html 7While other and better conversions exi</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada and Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beamsearch.</title>
<date>2008</date>
<booktitle>In Proc of EMNLP.</booktitle>
<contexts>
<context position="19262" citStr="Zhang and Clark, 2008" startWordPosition="3120" endWordPosition="3123">thm is faster in practice than the heap based one, as both are dominated by the O(n) feature extraction, while the cost of the O(n) max calculationis negligible compared to the constants involved in heap maintenance. 746 parser to other dependency parsing frameworks. Parser Runtime Features / Scoring MALT O(n) O(n) MST O(n3) O(n2) MST2 O(n3) O(n3) BEAM O(n * beam) O(n * beam) NONDIR (This Work) O(nlogn) O(n) Table 1: Complexity of different parsing frameworks. MST: first order MST parser, MST2: second order MST parser, MALT: shift-reduce left-to-right parsing. BEAM: beam search parser, as in (Zhang and Clark, 2008) In terms of feature extraction and score calculation operations, our algorithm has the same cost as traditional shift-reduce (MALT) parsers, and is an order of magnitude more efficient than graph-based (MST) parsers. Beam-search decoding for left-toright parsers (Zhang and Clark, 2008) is also linear, but has an additional linear dependence on the beamsize. The reported results in (Zhang and Clark, 2008) use a beam size of 64, compared to our constant of k = 6. Our Python-based implementation5 (the perceptron is implemented in a C extension module) parses about 40 tagged sentences per second </context>
<context position="28665" citStr="Zhang and Clark, 2008" startWordPosition="4617" endWordPosition="4620">empts to defer harder decisions to later passes over the sentence, and allows late decisions to make use of rich syntactic information (built in earlier passes) on both sides of the decision point. However, the model is not explicitly trained to optimize attachment ordering, has an O(n2) runtime complexity, and produces results which are inferior to current single-pass shift-reduce parsers. Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007). This approach works well and produces highly competitive results. Beam search can be incorporated into our parser as well. We leave this investigation to future work. Strict left-to-right ordering is also prevalent in sequence tagging. Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing. Shen and Joshi (2008) extends the bidirectiona</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graph-based and transition-based dependency parsing using beamsearch. In Proc of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>