<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000045">
<title confidence="0.998839">
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the `0-norm
</title>
<author confidence="0.995877">
Ashish Vaswani Liang Huang David Chiang
</author>
<affiliation confidence="0.9841095">
University of Southern California
Information Sciences Institute
</affiliation>
<email confidence="0.998708">
{avaswani,lhuang,chiang}@isi.edu
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921333333333">
Two decades after their invention, the IBM
word-based translation models, widely avail-
able in the GIZA++ toolkit, remain the dom-
inant approach to word alignment and an in-
tegral part of many statistical translation sys-
tems. Although many models have surpassed
them in accuracy, none have supplanted them
in practice. In this paper, we propose a simple
extension to the IBM models: an t0 prior to en-
courage sparsity in the word-to-word transla-
tion model. We explain how to implement this
extension efficiently for large-scale data (also
released as a modification to GIZA++) and
demonstrate, in experiments on Czech, Ara-
bic, Chinese, and Urdu to English translation,
significant improvements over IBM Model 4
in both word alignment (up to +6.7 F1) and
translation quality (up to +1.4 B ).
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999075">
Automatic word alignment is a vital component of
nearly all current statistical translation pipelines. Al-
though state-of-the-art translation models use rules
that operate on units bigger than words (like phrases
or tree fragments), they nearly always use word
alignments to drive extraction of those translation
rules. The dominant approach to word alignment has
been the IBM models (Brown et al., 1993) together
with the HMM model (Vogel et al., 1996). These
models are unsupervised, making them applicable
to any language pair for which parallel text is avail-
able. Moreover, they are widely disseminated in the
open-source GIZA++ toolkit (Och and Ney, 2004).
These properties make them the default choice for
most statistical MT systems.
In the decades since their invention, many mod-
els have surpassed them in accuracy, but none has
supplanted them in practice. Some of these models
are partially supervised, combining unlabeled paral-
lel text with manually-aligned parallel text (Moore,
2005; Taskar et al., 2005; Riesa and Marcu, 2010).
Although manually-aligned data is very valuable, it
is only available for a small number of language
pairs. Other models are unsupervised like the IBM
models (Liang et al., 2006; Grac¸a et al., 2010; Dyer
et al., 2011), but have not been as widely adopted as
GIZA++ has.
In this paper, we propose a simple extension to
the IBM/HMM models that is unsupervised like the
IBM models, is as scalable as GIZA++ because it is
implemented on top of GIZA++, and provides sig-
nificant improvements in both alignment and trans-
lation quality. It extends the IBM/HMM models by
incorporating an t0 prior, inspired by the princi-
ple of minimum description length (Barron et al.,
1998), to encourage sparsity in the word-to-word
translation model (Section 2.2). This extension fol-
lows our previous work on unsupervised part-of-
speech tagging (Vaswani et al., 2010), but enables
it to scale to the large datasets typical in word
alignment, using an efficient training method based
on projected gradient descent (Section 2.3). Ex-
periments on Czech-, Arabic-, Chinese- and Urdu-
English translation (Section 3) demonstrate consis-
tent significant improvements over IBM Model 4 in
both word alignment (up to +6.7 F1) and transla-
tion quality (up to +1.4 B ). Our implementation
has been released as a simple modification to the
GIZA++ toolkit that can be used as a drop-in re-
placement for GIZA++ in any existing MT pipeline.
</bodyText>
<page confidence="0.983815">
311
</page>
<note confidence="0.986114">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311–319,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.98271" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.99989125">
We start with a brief review of the IBM and HMM
word alignment models, then describe how to extend
them with a smoothed `0 prior and how to efficiently
train them.
</bodyText>
<subsectionHeader confidence="0.92476">
2.1 IBM Models and HMM
</subsectionHeader>
<bodyText confidence="0.9998441">
Given a French string f = f1 · · · fj · · · fm and an
English string e = e1 · · · ei · · · e`, these models de-
scribe the process by which the French string is
generated by the English string via the alignment
a = a1,... , aj, ... , am. Each aj is a hidden vari-
ables, indicating which English word eaj the French
word fj is aligned to.
In IBM Model 1–2 and the HMM model, the joint
probability of the French sentence and alignment
given the English sentence is
</bodyText>
<equation confidence="0.997047">
m
P(f, a  |e) = H d(aj  |aj−1, j)t(fj  |eaj). (1)
j=1
</equation>
<bodyText confidence="0.999242153846154">
The parameters of these models are the distortion
probabilities d(aj  |aj−1, j) and the translation prob-
abilities t(fj  |eaj). The three models differ in their
estimation of d, but the differences do not concern us
here. All three models, as well as IBM Models 3–5,
share the same t. For further details of these models,
the reader is referred to the original papers describ-
ing them (Brown et al., 1993; Vogel et al., 1996).
Let θ stand for all the parameters of the model.
The standard training procedure is to find the param-
eter values that maximize the likelihood, or, equiv-
alently, minimize the negative log-likelihood of the
observed data:
</bodyText>
<equation confidence="0.973092">
(− log P(f  |e, θ)) (2)
</equation>
<bodyText confidence="0.99977635">
(Moore, 2004), aligning to many unrelated words.
This hurts alignment precision and rule-extraction
recall. Previous attempted remedies include early
stopping, smoothing (Moore, 2004), and posterior
regularization (Grac¸a et al., 2010).
We have previously proposed another simple
remedy to overfitting in the context of unsuper-
vised part-of-speech tagging (Vaswani et al., 2010),
which is to minimize the size of the model using a
smoothed `0 prior. Applying this prior to an HMM
improves tagging accuracy for both Italian and En-
glish.
Here, our goal is to apply a similar prior in a
word-alignment model to the word-to-word transla-
tion probabilities t(f  |e). We leave the distortion
models alone, since they are not very large, and there
is not much reason to believe that we can profit from
compacting them.
With the addition of the `0 prior, the MAP (maxi-
mum a posteriori) objective function is
</bodyText>
<equation confidence="0.944143">
θˆ = arg min (− log P(f  |e, θ)P(θ)) (4)
θ
</equation>
<bodyText confidence="0.56353">
where
</bodyText>
<equation confidence="0.999684">
P(θ) ∝ exp (−αkθkro) (5)
</equation>
<bodyText confidence="0.5715395">
and
exp
</bodyText>
<equation confidence="0.981154666666667">
−t(f  |e)1 (6)
1 − J
β
</equation>
<bodyText confidence="0.9995992">
is a smoothed approximation of the `0-norm. The
hyperparameter β controls the tightness of the ap-
proximation, as illustrated in Figure 1. Substituting
back into (4) and dropping constant terms, we get
the following optimization problem: minimize
</bodyText>
<equation confidence="0.995682333333333">
θˆ = arg min
θ
E
</equation>
<bodyText confidence="0.800810571428571">
kθkβ 0 =
e,f
= arg min −log E ������ E exp −t(f  |e) (7)
θ l a P(f, a  |e, θ) � (3) −log P(f  |e, θ) − α β
e, f
This is done using the Expectation-Maximization
(EM) algorithm (Dempster et al., 1977).
</bodyText>
<subsectionHeader confidence="0.999203">
2.2 MAP-EM with the `0-norm
</subsectionHeader>
<bodyText confidence="0.9480368">
Maximum likelihood training is prone to overfitting,
especially in models with many parameters. In word
alignment, one well-known manifestation of overfit-
ting is that rare words can act as “garbage collectors”
subject to the constraints
</bodyText>
<equation confidence="0.965078">
E t(f  |e) = 1 for all e. (8)
f
</equation>
<bodyText confidence="0.958518666666667">
We can carry out the optimization in (7) with the
MAP-EM algorithm (Bishop, 2006). EM and MAP-
EM share the same E-step; the difference lies in the
</bodyText>
<page confidence="0.994793">
312
</page>
<figure confidence="0.998813714285714">
1
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
</figure>
<figureCaption confidence="0.982708">
Figure 1: The `0-norm (top curve) and smoothed approx-
imations (below) for β = 0.05, 0.1, 0.2.
</figureCaption>
<bodyText confidence="0.568173">
M-step. For vanilla EM, the M-step is:
</bodyText>
<equation confidence="0.832294333333333">
��������
E[C(e, f)] log t(f  |e) (9)
�
</equation>
<bodyText confidence="0.961732615384615">
again subject to the constraints (8). The count
C(e, f) is the number of times that f occurs aligned
to e. For MAP-EM, it is:
(10); we seek to minimize this function. As in pre-
vious work (Vaswani et al., 2010), we optimize each
set of parameters {t(·  |e)} separately for each En-
glish word type e. The inputs to the PGD are the
expected counts E[C(e, f)] and the current word-to-
word conditional probabilities θ. We run PGD for K
iterations, producing a sequence of intermediate pa-
rameter vectors θ1, ... , θk, ... , θK. Each iteration has
two steps, a projection step and a line search.
Projection step In this step, we compute:
</bodyText>
<equation confidence="0.97938">
�A
θk = �θk − s∇F(θk) (11)
</equation>
<bodyText confidence="0.988952833333333">
This moves θ in the direction of steepest descent
(∇F) with step size s, and then the function [·]A
projects the resulting point onto the simplex; that
is, it finds the nearest point that satisfies the con-
straints (8).
The gradient ∇F(θk) is
</bodyText>
<equation confidence="0.868061">
−t(f  |e) (12)
β
</equation>
<bodyText confidence="0.997894">
In contrast to Schoenemann (2011b), we use an
O(n log n) algorithm for the projection step due to
Duchi et. al. (2008), shown in Pseudocode 1.
</bodyText>
<equation confidence="0.9642440625">
θˆ = arg min ����������−
θ e,f
∂F
∂t(f  |e) =
E[C(f, e)]
t(f  |e)
α
+ exp
β
θˆ = arg min E E[C(e, f)] logt(f  |e) −
θ e,f
Pseudocode 1 Project input vector u ∈ Rn onto the
(10) probability simplex.
�
−t( f  |e)
β
</equation>
<bodyText confidence="0.999927125">
This optimization problem is non-convex, and we
do not know of a closed-form solution. Previously
(Vaswani et al., 2010), we used ALGENCAN, a non-
linear optimization toolkit, but this solution does not
scale well to the number of parameters involved in
word alignment models. Instead, we use a simpler
and more scalable method which we describe in the
next section.
</bodyText>
<subsectionHeader confidence="0.988586">
2.3 Projected gradient descent
</subsectionHeader>
<bodyText confidence="0.999823714285714">
Following Schoenemann (2011b), we use projected
gradient descent (PGD) to solve the M-step (but
with the `0-norm instead of the `1-norm). Gradient
projection methods are attractive solutions to con-
strained optimization problems, particularly when
the constraints on the parameters are simple (Bert-
sekas, 1999). Let F(θ) be the objective function in
</bodyText>
<equation confidence="0.9545935">
v = u sorted in non-increasing order
ρ = 0
for i = 1 to n do
if vi − 1 ��i �
r=1 vr − 1 &gt; 0 then
i
ρ = i
end if
end for
η ρ (ZP=1 vr − 1)
wr = max{vr − η, 0} for 1 ≤ r ≤ n
return w
</equation>
<bodyText confidence="0.9882995">
Line search Next, we move to a point between θk
and θk that satisfies the Armijo condition,
</bodyText>
<equation confidence="0.975155">
� �
F(θk + δm) ≤ F(θk) + σ ∇F(θk) · δm (13)
</equation>
<bodyText confidence="0.946165666666667">
where δm = γm(θk − θk) and σ and γ are both con-
stants in (0, 1). We try values m = 1, 2,... until the
Armijo condition (13) is satisfied or the limit m = 20
exp
�α
e,f
</bodyText>
<page confidence="0.978406">
313
</page>
<bodyText confidence="0.7891505">
Pseudocode 2 Find a point between θk and θk that
satisfies the Armijo condition.
</bodyText>
<equation confidence="0.993431266666667">
Fmin = F(θk)
θmin = θk
for m = 1 to 20 do
δm = γm (θk − θk)
if F(θk + δm) &lt; Fmin then
Fmin = F(θk + δm)
θmin = θk + δm
end if
� �
if F(θk + δm) ≤ F(θk) + σ ∇F(θk) · δm then
break
end if
end for
θk+1 = θmin
return θk+1
</equation>
<bodyText confidence="0.998036210526316">
is reached. (Note that we don’t allow m = 0 because
this can cause θk + δm to land on the boundary of
the probability simplex, where the objective func-
tion is undefined.) Then we set θk+1 to the point in
{θk} ∪ {θk + δm  |1 ≤ m ≤ 20} that minimizes F.
The line search algorithm is summarized in Pseu-
docode 2.
In our implementation, we set γ = 0.5 and σ �
0.5. We keep s fixed for all PGD iterations; we ex-
perimented with s ∈ {0.1, 0.5} and did not observe
significant changes in F-score. We run the projection
step and line search alternately for at most K itera-
tions, terminating early if there is no change in θk
from one iteration to the next. We set K = 35 for the
large Arabic-English experiment; for all other con-
ditions, we set K = 50. These choices were made to
balance efficiency and accuracy. We found that val-
ues of K between 30 and 75 were generally reason-
able.
</bodyText>
<sectionHeader confidence="0.999798" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999918636363636">
To demonstrate the effect of the `0-norm on the IBM
models, we performed experiments on four trans-
lation tasks: Arabic-English, Chinese-English, and
Urdu-English from the NIST Open MT Evaluation,
and the Czech-English translation from the Work-
shop on Machine Translation (WMT) shared task.
We measured the accuracy of word alignments gen-
erated by GIZA++ with and without the `0-norm,
and also translation accuracy of systems trained us-
ing the word alignments. Across all tests, we found
strong improvements from adding the `0-norm.
</bodyText>
<subsectionHeader confidence="0.994192">
3.1 Training
</subsectionHeader>
<bodyText confidence="0.999848538461538">
We have implemented our algorithm as an open-
source extension to GIZA++.1 Usage of the exten-
sion is identical to standard GIZA++, except that the
user can switch the `0 prior on or off, and adjust the
hyperparameters α and β.
For vanilla EM, we ran five iterations of Model 1,
five iterations of HMM, and ten iterations of
Model 4. For our approach, we first ran one iter-
ation of Model 1, followed by four iterations of
Model 1 with smoothed `0, followed by five itera-
tions of HMM with smoothed `0. Finally, we ran ten
iterations of Model 4.2
We used the following parallel data:
</bodyText>
<listItem confidence="0.999338214285714">
• Chinese-English: selected data from the con-
strained task of the NIST 2009 Open MT Eval-
uation.3
• Arabic-English: all available data for the
constrained track of NIST 2009, excluding
United Nations proceedings (LDC2004E13),
ISI Automatically Extracted Parallel Text
(LDC2007E08), and Ummah newswire text
(LDC2004T18), for a total of 5.4+4.3 mil-
lion words. We also experimented on a larger
Arabic-English parallel text of 44+37 million
words from the DARPA GALE program.
• Urdu-English: all available data for the con-
strained track of NIST 2009.
</listItem>
<footnote confidence="0.5568175">
1The code can be downloaded from the first author’s website
at http://www.isi.edu/˜avaswani/giza-pp-l0.html.
</footnote>
<bodyText confidence="0.835132111111111">
2GIZA++ allows changing some heuristic parameters for
efficient training. Currently, we set two of these to zero:
mincountincrease and probcutoff. In the default setting,
both are set to 10−7. We set probcutoff to 0 because we would
like the optimization to learn the parameter values. For a fair
comparison, we applied the same setting to our vanilla EM
training as well. To test, we ran GIZA++ with the default set-
ting on the smaller of our two Arabic-English datasets with the
same number of iterations and found no change in F-score.
</bodyText>
<footnote confidence="0.855307">
3LDC catalog numbers LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34,
LDC2006E85, LDC2006E86, LDC2006E92, and
LDC2006E93.
</footnote>
<page confidence="0.996044">
314
</page>
<figure confidence="0.997965230769231">
✥✥✥✉✥✉✥✥✥✥✥✥✥✥✥✥✥waijiao
✥✥✥✥✥✉✥✥✥✥✥✥✥✥✥✥xuehui✉✥✥✥✥✥✥✥✥✥✥✥✥✥✥✥huizhang
✥✥✥✥✥✥✥✉✥✥✥✥✥✥✥✥li�u
✥✥✥✥✥✥✉✥✉✥✥✥✥✥✥✥✥shuqing
✥✥✥✥✥✥✥✥✥✥✥✥✥✉✥✥huijian
✥✥✥✥✥✥✥✥✥✥✥✥✥✥✥shi✥✥✥✥✥✥✥✥✥✉✥✉✥✉✥✥✉✥✥zaizuo
✥✥✥✥✥✥✥✥✥✥✥✥✥✥✉✥.
✥✥✥✥✉✥✥✉✥✥✥✥✥✥zhongwai
✥✥✉✥✥✥✥✥✥✥✥✥✥laibin
✥✉✥✥✥✥✥✥✥✥✥✥✥siqian
✉✥✉✥✥✥✥✥✥✥✥✥✥✥duo
✥✥✥✥✥✥✥✥✥✥✥✥ren
✥✥✥✥✥✥✥✉✥✥✥✥✥ch5xi✥✥✥✥✥✥✥✉✥✥✥✥✥le
✥✥✥✥✥✥✥✥✥✉✥✉✥✥kaimiishi✥✥✥✥✥✥✥✥✥✥✥✉✥.
(a) (b)
✉✥✥✥✥✥✥✥✥✥✥ruguo
✥✥✥✥✉✥✥✥✥✥✥yao
✥✥✥✥✥✥✥✉✥✉✥✥lulu
✥✥✥✥✥✥✥✥✥✥zhuan
✥✥✥✥✥✉✥✉✥✥✥✥q�u
✥✥✥✥✥✥✥✥✥✥dehua
✥✥✥✥✥✥✥✥✥✥ne
✥✥✥✥✥✥✥✥✥✉✥,
✥✉✥✥✥✥✥✥✥✥✥✥✥✥zhege
✥✥✥✥✉✥✥✥✥✥✥✥✥✥chul��
✥✥✥✥✥✥✥✥✥✥✥✥✥wan
✉✥✥✥✥✥✥✥✥✥✥✥✥✥yihou
✥✥✥✥✥✥✥✥✥✥✥✥✥ne
✥✥✥✥✥✥✉✥✥✥✥✥✥✥,
✥✥✥✥✥✥✥✥✥✥✥✥✥hai
✥✥✥✥✥✥✥✥✥✥✉✥✥✥zha
✥✥✉✥✥✥✥✥✥✥✥hen ✥✥✥✥✥✥✥✥✥✥✥✉✥✥le
✥✥✉✥✥✥✥✥✥✥✥hen ✥✥✥✥✥✥✥✉✥✥✥✥✥✥sige
✥✥✉✥✥✥✥✥✥✥✥hen ✥✥✥✥✥✥✥✥✉✥✥✥✥✥diaobao
✥✥✉✥✥✥✥✥✥✥✥hen ✥✥✥✥✥✥✥✥✥✥✥✥✉✥.
✥✥✥✉✥✥✥✥✥✥✥mafan
✥✥✥✥✥✥✥✥✥✥de
✥✥✥✥✥✥✥✥✥✉✥,
(c) (d)
</figure>
<figureCaption confidence="0.997806">
Figure 2: Smoothed-`0 alignments (red circles) correct many errors in the baseline GIZA++ alignments (black
</figureCaption>
<bodyText confidence="0.955625">
squares), as shown in four Chinese-English examples (the red circles are almost perfect for these examples, except
for minor mistakes such as liu-sh¯uqYng and meeting-z`aizu`o in (a) and .-, in (c)). In particular, the baseline system
demonstrates typical “garbage-collection” phenomena in proper name “shuqing” in both languages in (a), number
“4000” and word “l´aibYn” (lit. “guest”) in (b), word “troublesome” and “l`ul`u” (lit. “land-route”) in (c), and “block-
houses” and “di¯aobˇao” (lit. “bunker”) in (d). We found this garbage-collection behavior to be especially common with
proper names, numbers, and uncommon words in both languages. Most interestingly, in (c), our smoothed-`0 system
correctly aligns “extremely” to “hˇen hˇen hˇen hˇen” (lit. “very very very very”) which is rare in the bitext.
</bodyText>
<page confidence="0.998025">
315
</page>
<table confidence="0.999571764705882">
task data (M) system align F1 (%) word trans (M) ˜φsing. B (%) 2010
2008 2009
Chi-Eng 9.6+12 baseline 73.2 3.5 6.2 28.7
`0-norm 76.5 2.0 3.3 29.5
difference +3.3 −43% −47% +0.8
baseline 65.0 3.1 4.5 39.8 42.5
Ara-Eng 5.4+4.3 `0-norm 70.8 1.8 1.8 41.1 43.7
difference +5.9 −39% −60% +1.3 +1.2
baseline 66.2 15 5.0 41.6 44.9
Ara-Eng 44+37 `0-norm 71.8 7.9 1.8 42.5 45.3
difference +5.6 −47% −64% +0.9 +0.4
baseline 1.7 4.5 25.3* 29.8
Urd-Eng 1.7+1.5 `0-norm 1.2 2.2 25.9* 31.2
difference −29% −51% +0.6* +1.4
baseline 65.6 1.5 3.0 17.3 18.0
Cze-Eng 2.1+2.3 `0-norm 72.3 1.0 1.4 17.9 18.4
difference +6.7 −33% −53% +0.6 +0.4
</table>
<tableCaption confidence="0.896664">
Table 1: Adding the `0-norm to the IBM models improves both alignment and translation accuracy across four different
language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the
</tableCaption>
<bodyText confidence="0.730187666666667">
lexical weighting table) is reduced. The ˜φsing. column shows the average fertility of once-seen source words. For
Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open
MT Evaluation. *Half of this test set was also used for tuning feature weights.
</bodyText>
<listItem confidence="0.978359">
• Czech-English: A corpus of 4 million words of
Czech-English data from the News Commen-
tary corpus.4
</listItem>
<bodyText confidence="0.99999">
We set the hyperparameters α and β by tuning
on gold-standard word alignments (to maximize F1)
when possible. For Arabic-English and Chinese-
English, we used 346 and 184 hand-aligned sen-
tences from LDC2006E86 and LDC2006E93. Sim-
ilarly, for Czech-English, 515 hand-aligned sen-
tences were available (Bojar and Prokopov´a, 2006).
But for Urdu-English, since we did not have any
gold alignments, we used α = 10 and β = 0.05. We
did not choose a large α, as the dataset was small,
and we chose a conservative value for β.
We ran word alignment in both directions and
symmetrized using grow-diag-final (Koehn et al.,
2003). For models with the smoothed `0 prior, we
tuned α and β separately in each direction.
</bodyText>
<subsectionHeader confidence="0.997543">
3.2 Alignment
</subsectionHeader>
<bodyText confidence="0.9983565">
First, we evaluated alignment accuracy directly by
comparing against gold-standard word alignments.
</bodyText>
<footnote confidence="0.930957">
4This data is available at http://statmt.org/wmt10.
</footnote>
<bodyText confidence="0.999709708333333">
The results are shown in the alignment F1 col-
umn of Table 1. We used balanced F-measure rather
than alignment error rate as our metric (Fraser and
Marcu, 2007).
Following Dyer et al. (2011), we also measured
the average fertility, ˜φsing., of once-seen source
words in the symmetrized alignments. Our align-
ments show smaller fertility for once-seen words,
suggesting that they suffer from “garbage collec-
tion” effects less than the baseline alignments do.
The fact that we had to use hand-aligned data to
tune the hyperparameters α and β means that our
method is no longer completely unsupervised. How-
ever, our observation is that alignment accuracy is
actually fairly robust to the choice of these hyperpa-
rameters, as shown in Table 2. As we will see below,
we still obtained strong improvements in translation
quality when hand-aligned data was unavailable.
We also tried generating 50 word classes using
the tool provided in GIZA++. We found that adding
word classes improved alignment quality a little, but
more so for the baseline system (see Table 3). We
used the alignments generated by training with word
classes for our translation experiments.
</bodyText>
<page confidence="0.997285">
316
</page>
<table confidence="0.9999775625">
� model 0 10 25 α 75 100 250 500 750
50
– HMM 47.5
M4 52.1
0.5 HMM 46.3 48.4 52.8 55.7 57.5 61.5 62.6 62.7
M4 51.7 53.7 56.4 58.6 59.8 63.3 64.4 64.8
0.1 HMM 55.6 60.4 61.6 62.1 61.9 61.8 60.2 60.1
M4 58.2 62.4 64.0 64.4 64.8 65.5 65.6 65.9
0.05 HMM 59.1 61.4 62.4 62.5 62.3 60.8 58.7 57.7
M4 61.0 63.5 64.6 65.3 65.3 65.4 65.7 65.7
0.01 HMM 59.7 61.6 60.0 59.5 58.7 56.9 55.7 54.7
M4 62.9 65.0 65.1 65.2 65.1 65.4 65.3 65.4
0.005 HMM 58.1 59.0 58.3 57.6 57.0 55.9 53.9 51.7
M4 62.0 64.1 64.5 64.5 64.5 65.0 64.8 64.6
0.001 HMM 51.7 52.1 51.4 49.3 50.4 46.8 45.4 44.0
M4 59.8 61.3 61.5 61.0 61.8 61.2 61.0 61.2
</table>
<tableCaption confidence="0.786738">
Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM model
for Arabic-English alignment (α = 0).
</tableCaption>
<table confidence="0.999647875">
word classes?
direction system no yes
baseline 49.0 52.1
P(f I e) `0-norm 63.9 65.9
difference +14.9 +13.8
baseline 64.3 65.2
P(e I f) `0-norm 69.2 70.3
difference +4.9 +5.1
</table>
<tableCaption confidence="0.968818">
Table 3: Adding word classes improves the F-score in
both directions for Arabic-English alignment by a little,
for the baseline system more so than ours.
</tableCaption>
<bodyText confidence="0.998384">
Figure 2 shows four examples of Chinese-
English alignment, comparing the baseline with our
smoothed-`0 method. In all four cases, the base-
line produces incorrect extra alignments that prevent
good translation rules from being extracted while
the smoothed-`0 results are correct. In particular, the
baseline system demonstrates typical “garbage col-
lection” behavior (Moore, 2004) in all four exam-
ples.
</bodyText>
<subsectionHeader confidence="0.998592">
3.3 Translation
</subsectionHeader>
<bodyText confidence="0.9968244">
We then tested the effect of word alignments on
translation quality using the hierarchical phrase-
based translation system Hiero (Chiang, 2007). We
used a fairly standard set of features: seven in-
herited from Pharaoh (Koehn et al., 2003), a sec-
</bodyText>
<table confidence="0.998812166666667">
setting align F1 (%) B (%)
t(f I e) t(e  |f) 2008 2009
1st 1st 70.8 41.1 43.7
1st 2nd 70.7 41.1 43.8
2nd 1st 70.7 40.7 44.1
2nd 2nd 70.9 41.1 44.2
</table>
<tableCaption confidence="0.936558">
Table 4: Optimizing hyperparameters on alignment F1
score does not necessarily lead to optimal B . The
first two columns indicate whether we used the first- or
second-best alignments in each direction (according to
F1); the third column shows the F1 of the symmetrized
</tableCaption>
<bodyText confidence="0.976170823529412">
alignments, whose corresponding B scores are shown
in the last two columns.
ond language model, and penalties for the glue
rule, identity rules, unknown-word rules, and two
kinds of number/name rules. The feature weights
were discriminatively trained using MIRA (Chi-
ang et al., 2008). We used two 5-gram language
models, one on the combined English sides of
the NIST 2009 Arabic-English and Chinese-English
constrained tracks (385M words), and another on
2 billion words of English.
For each language pair, we extracted grammar
rules from the same data that were used for word
alignment. The development data that were used for
discriminative training were: for Chinese-English
and Arabic-English, data from the NIST 2004 and
NIST 2006 test sets, plus newsgroup data from the
</bodyText>
<page confidence="0.994396">
317
</page>
<bodyText confidence="0.999869516129032">
GALE program (LDC2006E92); for Urdu-English,
half of the NIST 2008 test set; for Czech-English,
a training set of 2051 sentences provided by the
WMT10 translation workshop.
The results are shown in the B column of Ta-
ble 1. We used case-insensitive IBM B (closest
reference length) as our metric. Significance test-
ing was carried out using bootstrap resampling with
1000 samples (Koehn, 2004; Zhang et al., 2004).
All of the tests showed significant improvements
(p &lt; 0.01), ranging from +0.4 B to +1.4 B .
For Urdu, even though we didn’t have manual align-
ments to tune hyperparameters, we got significant
gains over a good baseline. This is promising for lan-
guages that do not have any manually aligned data.
Ideally, one would want to tune α and f3 to max-
imize B . However, this is prohibitively expen-
sive, especially if we must tune them separately
in each alignment direction before symmetrization.
We ran some contrastive experiments to investi-
gate the impact of hyperparameter tuning on trans-
lation quality. For the smaller Arabic-English cor-
pus, we symmetrized all combinations of the two
top-scoring alignments (according to F1) in each di-
rection, yielding four sets of alignments. Table 4
shows B scores for translation models learned
from these alignments. Unfortunately, we find that
optimizing F1 is not optimal for B —using the
second-best alignments yields a further improve-
ment of 0.5 B on the NIST 2009 data, which is
statistically significant (p &lt; 0.05).
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999971476190476">
Schoenemann (2011a), taking inspiration from Bo-
drumlu et al. (2009), uses integer linear program-
ming to optimize IBM Model 1–2 and the HMM
with the `0-norm. This method, however, does not
outperform GIZA++. In later work, Schoenemann
(2011b) used projected gradient descent for the f1-
norm. Here, we have adopted his use of projected
gradient descent, but using a smoothed `0-norm.
Liang et al. (2006) show how to train IBM mod-
els in both directions simultaneously by adding a
term to the log-likelihood that measures the agree-
ment between the two directions. Grac¸a et al. (2010)
explore modifications to the HMM model that en-
courage bijectivity and symmetry. The modifications
take the form of constraints on the posterior dis-
tribution over alignments that is computed during
the E-step. Mermer and Sarac¸lar (2011) explore a
Bayesian version of IBM Model 1, applying sparse
Dirichlet priors to t. However, because this method
requires the use of Monte Carlo methods, it is not
clear how well it can scale to larger datasets.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999924210526316">
We have extended the IBM models and HMM model
by the addition of an t0 prior to the word-to-word
translation model, which compacts the word-to-
word translation table, reducing overfitting, and, in
particular, the “garbage collection” effect. We have
shown how to perform MAP-EM with this prior
efficiently, even for large datasets. The method is
implemented as a modification to the open-source
toolkit GIZA++, and we have shown that it signif-
icantly improves translation quality across four dif-
ferent language pairs. Even though we have used a
small set of gold-standard alignments to tune our
hyperparameters, we found that performance was
fairly robust to variation in the hyperparameters, and
translation performance was good even when gold-
standard alignments were unavailable. We hope that
our method, due to its simplicity, generality, and ef-
fectiveness, will find wide application for training
better statistical translation systems.
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999112">
We are indebted to Thomas Schoenemann for ini-
tial discussions and pilot experiments that led to
this work, and to the anonymous reviewers for
their valuable comments. We thank Jason Riesa for
providing the Arabic-English and Chinese-English
hand-aligned data and the alignment visualization
tool, and Chris Dyer for the Czech-English hand-
aligned data. This research was supported in part
by DARPA under contract DOI-NBC D11AP00244
and a Google Faculty Research Award to L. H.
</bodyText>
<page confidence="0.998502">
318
</page>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999287">
Andrew Barron, Jorma Rissanen, and Bin Yu. 1998. The
minimum description length principle in coding and
modeling. IEEE Transactions on Information Theory,
44(6):2743–2760.
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A new objective function for word alignment. In Pro-
ceedings of the NAACL HLT Workshop on Integer Lin-
ear Programming for Natural Language Processing.
Ondˇrej Bojar and Magdalena Prokopov´a. 2006. Czech-
English word alignment. In Proceedings of LREC.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19:263–311.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–208.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Computational Linguistics, 39(4):1–38.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
f1-ball for learning in high dimensions. In Proceed-
ings of ICML.
Chris Dyer, Jonathan H. Clark, Alon Lavie, and Noah A.
Smith. 2011. Unsupervised word alignment with ar-
bitrary features. In Proceedings ofACL.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293–303.
Jo˜ao V. Grac¸a, Kuzman Ganchev, and Ben Taskar.
2010. Learning tractable word alignment models
with complex constraints. Computational Linguistics,
36(3):481–504.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Cos¸kun Mermer and Murat Sarac¸lar. 2011. Bayesian
word alignment for statistical machine translation. In
Proceedings of ACL HLT.
Robert C. Moore. 2004. Improving IBM word-
alignment Model 1. In Proceedings of ACL.
Robert Moore. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of HLT-
EMNLP.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417–449.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proceedings of ACL.
Thomas Schoenemann. 2011a. Probabilistic word align-
ment under the L0-norm. In Proceedings of CoNLL.
Thomas Schoenemann. 2011b. Regularizing mono- and
bi-word models for word alignment. In Proceedings
of IJCNLP.
Ben Taskar, Lacoste-Julien Simon, and Klein Dan. 2005.
A discriminative matching approach to word align-
ment. In Proceedings of HLT-EMNLP.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of ACL.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC.
</reference>
<page confidence="0.999356">
319
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798912">
<title confidence="0.972636">Smaller Alignment Models for Better Word Alignment with the</title>
<author confidence="0.998962">Ashish Vaswani Liang Huang David Chiang</author>
<affiliation confidence="0.999915">University of Southern Information Sciences Institute</affiliation>
<email confidence="0.999185">avaswani@isi.edu</email>
<email confidence="0.999185">lhuang@isi.edu</email>
<email confidence="0.999185">chiang@isi.edu</email>
<abstract confidence="0.989275526315789">Two decades after their invention, the IBM word-based translation models, widely availin the remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple to the IBM models: an to encourage sparsity in the word-to-word translation model. We explain how to implement this for large-scale data (also as a modification to and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 both word alignment (up to F1) and quality (up to B ).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew Barron</author>
<author>Jorma Rissanen</author>
<author>Bin Yu</author>
</authors>
<title>The minimum description length principle in coding and modeling.</title>
<date>1998</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>44</volume>
<issue>6</issue>
<contexts>
<context position="2752" citStr="Barron et al., 1998" startWordPosition="429" endWordPosition="432">, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an t0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previous work on unsupervised part-ofspeech tagging (Vaswani et al., 2010), but enables it to scale to the large datasets typical in word alignment, using an efficient training method based on projected gradient descent (Section 2.3). Experiments on Czech-, Arabic-, Chinese- and UrduEnglish translation (Section 3) demonstrate consistent significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ). Our implementation has been release</context>
</contexts>
<marker>Barron, Rissanen, Yu, 1998</marker>
<rawString>Andrew Barron, Jorma Rissanen, and Bin Yu. 1998. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743–2760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri P Bertsekas</author>
</authors>
<title>Nonlinear Programming. Athena Scientific.</title>
<date>1999</date>
<contexts>
<context position="9142" citStr="Bertsekas, 1999" startWordPosition="1573" endWordPosition="1575">eviously (Vaswani et al., 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models. Instead, we use a simpler and more scalable method which we describe in the next section. 2.3 Projected gradient descent Following Schoenemann (2011b), we use projected gradient descent (PGD) to solve the M-step (but with the `0-norm instead of the `1-norm). Gradient projection methods are attractive solutions to constrained optimization problems, particularly when the constraints on the parameters are simple (Bertsekas, 1999). Let F(θ) be the objective function in v = u sorted in non-increasing order ρ = 0 for i = 1 to n do if vi − 1 ��i � r=1 vr − 1 &gt; 0 then i ρ = i end if end for η ρ (ZP=1 vr − 1) wr = max{vr − η, 0} for 1 ≤ r ≤ n return w Line search Next, we move to a point between θk and θk that satisfies the Armijo condition, � � F(θk + δm) ≤ F(θk) + σ ∇F(θk) · δm (13) where δm = γm(θk − θk) and σ and γ are both constants in (0, 1). We try values m = 1, 2,... until the Armijo condition (13) is satisfied or the limit m = 20 exp �α e,f 313 Pseudocode 2 Find a point between θk and θk that satisfies the Armijo c</context>
</contexts>
<marker>Bertsekas, 1999</marker>
<rawString>Dimitri P. Bertsekas. 1999. Nonlinear Programming. Athena Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="6892" citStr="Bishop, 2006" startWordPosition="1163" endWordPosition="1164">ion problem: minimize θˆ = arg min θ E kθkβ 0 = e,f = arg min −log E ������ E exp −t(f |e) (7) θ l a P(f, a |e, θ) � (3) −log P(f |e, θ) − α β e, f This is done using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). 2.2 MAP-EM with the `0-norm Maximum likelihood training is prone to overfitting, especially in models with many parameters. In word alignment, one well-known manifestation of overfitting is that rare words can act as “garbage collectors” subject to the constraints E t(f |e) = 1 for all e. (8) f We can carry out the optimization in (7) with the MAP-EM algorithm (Bishop, 2006). EM and MAPEM share the same E-step; the difference lies in the 312 1 0.8 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 Figure 1: The `0-norm (top curve) and smoothed approximations (below) for β = 0.05, 0.1, 0.2. M-step. For vanilla EM, the M-step is: �������� E[C(e, f)] log t(f |e) (9) � again subject to the constraints (8). The count C(e, f) is the number of times that f occurs aligned to e. For MAP-EM, it is: (10); we seek to minimize this function. As in previous work (Vaswani et al., 2010), we optimize each set of parameters {t(· |e)} separately for each English word type e. The inputs to the PGD a</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tugba Bodrumlu</author>
<author>Kevin Knight</author>
<author>Sujith Ravi</author>
</authors>
<title>A new objective function for word alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing.</booktitle>
<contexts>
<context position="22870" citStr="Bodrumlu et al. (2009)" startWordPosition="3847" endWordPosition="3851">iments to investigate the impact of hyperparameter tuning on translation quality. For the smaller Arabic-English corpus, we symmetrized all combinations of the two top-scoring alignments (according to F1) in each direction, yielding four sets of alignments. Table 4 shows B scores for translation models learned from these alignments. Unfortunately, we find that optimizing F1 is not optimal for B —using the second-best alignments yields a further improvement of 0.5 B on the NIST 2009 data, which is statistically significant (p &lt; 0.05). 4 Related Work Schoenemann (2011a), taking inspiration from Bodrumlu et al. (2009), uses integer linear programming to optimize IBM Model 1–2 and the HMM with the `0-norm. This method, however, does not outperform GIZA++. In later work, Schoenemann (2011b) used projected gradient descent for the f1- norm. Here, we have adopted his use of projected gradient descent, but using a smoothed `0-norm. Liang et al. (2006) show how to train IBM models in both directions simultaneously by adding a term to the log-likelihood that measures the agreement between the two directions. Grac¸a et al. (2010) explore modifications to the HMM model that encourage bijectivity and symmetry. The m</context>
</contexts>
<marker>Bodrumlu, Knight, Ravi, 2009</marker>
<rawString>Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009. A new objective function for word alignment. In Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Magdalena Prokopov´a</author>
</authors>
<title>CzechEnglish word alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Bojar, Prokopov´a, 2006</marker>
<rawString>Ondˇrej Bojar and Magdalena Prokopov´a. 2006. CzechEnglish word alignment. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1447" citStr="Brown et al., 1993" startWordPosition="215" endWordPosition="218">te, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ). 1 Introduction Automatic word alignment is a vital component of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar </context>
<context position="4788" citStr="Brown et al., 1993" startWordPosition="793" endWordPosition="796">ord eaj the French word fj is aligned to. In IBM Model 1–2 and the HMM model, the joint probability of the French sentence and alignment given the English sentence is m P(f, a |e) = H d(aj |aj−1, j)t(fj |eaj). (1) j=1 The parameters of these models are the distortion probabilities d(aj |aj−1, j) and the translation probabilities t(fj |eaj). The three models differ in their estimation of d, but the differences do not concern us here. All three models, as well as IBM Models 3–5, share the same t. For further details of these models, the reader is referred to the original papers describing them (Brown et al., 1993; Vogel et al., 1996). Let θ stand for all the parameters of the model. The standard training procedure is to find the parameter values that maximize the likelihood, or, equivalently, minimize the negative log-likelihood of the observed data: (− log P(f |e, θ)) (2) (Moore, 2004), aligning to many unrelated words. This hurts alignment precision and rule-extraction recall. Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Grac¸a et al., 2010). We have previously proposed another simple remedy to overfitting in the context of unsupervised p</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="20820" citStr="Chiang et al., 2008" startWordPosition="3511" endWordPosition="3515">st 2nd 70.7 41.1 43.8 2nd 1st 70.7 40.7 44.1 2nd 2nd 70.9 41.1 44.2 Table 4: Optimizing hyperparameters on alignment F1 score does not necessarily lead to optimal B . The first two columns indicate whether we used the first- or second-best alignments in each direction (according to F1); the third column shows the F1 of the symmetrized alignments, whose corresponding B scores are shown in the last two columns. ond language model, and penalties for the glue rule, identity rules, unknown-word rules, and two kinds of number/name rules. The feature weights were discriminatively trained using MIRA (Chiang et al., 2008). We used two 5-gram language models, one on the combined English sides of the NIST 2009 Arabic-English and Chinese-English constrained tracks (385M words), and another on 2 billion words of English. For each language pair, we extracted grammar rules from the same data that were used for word alignment. The development data that were used for discriminative training were: for Chinese-English and Arabic-English, data from the NIST 2004 and NIST 2006 test sets, plus newsgroup data from the 317 GALE program (LDC2006E92); for Urdu-English, half of the NIST 2008 test set; for Czech-English, a train</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="20021" citStr="Chiang, 2007" startWordPosition="3374" endWordPosition="3375">y a little, for the baseline system more so than ours. Figure 2 shows four examples of ChineseEnglish alignment, comparing the baseline with our smoothed-`0 method. In all four cases, the baseline produces incorrect extra alignments that prevent good translation rules from being extracted while the smoothed-`0 results are correct. In particular, the baseline system demonstrates typical “garbage collection” behavior (Moore, 2004) in all four examples. 3.3 Translation We then tested the effect of word alignments on translation quality using the hierarchical phrasebased translation system Hiero (Chiang, 2007). We used a fairly standard set of features: seven inherited from Pharaoh (Koehn et al., 2003), a secsetting align F1 (%) B (%) t(f I e) t(e |f) 2008 2009 1st 1st 70.8 41.1 43.7 1st 2nd 70.7 41.1 43.8 2nd 1st 70.7 40.7 44.1 2nd 2nd 70.9 41.1 44.2 Table 4: Optimizing hyperparameters on alignment F1 score does not necessarily lead to optimal B . The first two columns indicate whether we used the first- or second-best alignments in each direction (according to F1); the third column shows the F1 of the symmetrized alignments, whose corresponding B scores are shown in the last two columns. ond lang</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="6513" citStr="Dempster et al., 1977" startWordPosition="1097" endWordPosition="1100"> prior, the MAP (maximum a posteriori) objective function is θˆ = arg min (− log P(f |e, θ)P(θ)) (4) θ where P(θ) ∝ exp (−αkθkro) (5) and exp −t(f |e)1 (6) 1 − J β is a smoothed approximation of the `0-norm. The hyperparameter β controls the tightness of the approximation, as illustrated in Figure 1. Substituting back into (4) and dropping constant terms, we get the following optimization problem: minimize θˆ = arg min θ E kθkβ 0 = e,f = arg min −log E ������ E exp −t(f |e) (7) θ l a P(f, a |e, θ) � (3) −log P(f |e, θ) − α β e, f This is done using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). 2.2 MAP-EM with the `0-norm Maximum likelihood training is prone to overfitting, especially in models with many parameters. In word alignment, one well-known manifestation of overfitting is that rare words can act as “garbage collectors” subject to the constraints E t(f |e) = 1 for all e. (8) f We can carry out the optimization in (7) with the MAP-EM algorithm (Bishop, 2006). EM and MAPEM share the same E-step; the difference lies in the 312 1 0.8 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 Figure 1: The `0-norm (top curve) and smoothed approximations (below) for β = 0.05, 0.1, 0.2. M-step. For vanill</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Computational Linguistics, 39(4):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Tushar Chandra</author>
</authors>
<title>Efficient projections onto the f1-ball for learning in high dimensions.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<marker>Duchi, Shalev-Shwartz, Singer, Chandra, 2008</marker>
<rawString>John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. 2008. Efficient projections onto the f1-ball for learning in high dimensions. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Jonathan H Clark</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Unsupervised word alignment with arbitrary features.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2303" citStr="Dyer et al., 2011" startWordPosition="352" endWordPosition="355">Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an t0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previous work on unsupervised part-ofspeech tag</context>
<context position="17405" citStr="Dyer et al. (2011)" startWordPosition="2925" endWordPosition="2928">large α, as the dataset was small, and we chose a conservative value for β. We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al., 2003). For models with the smoothed `0 prior, we tuned α and β separately in each direction. 3.2 Alignment First, we evaluated alignment accuracy directly by comparing against gold-standard word alignments. 4This data is available at http://statmt.org/wmt10. The results are shown in the alignment F1 column of Table 1. We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007). Following Dyer et al. (2011), we also measured the average fertility, ˜φsing., of once-seen source words in the symmetrized alignments. Our alignments show smaller fertility for once-seen words, suggesting that they suffer from “garbage collection” effects less than the baseline alignments do. The fact that we had to use hand-aligned data to tune the hyperparameters α and β means that our method is no longer completely unsupervised. However, our observation is that alignment accuracy is actually fairly robust to the choice of these hyperparameters, as shown in Table 2. As we will see below, we still obtained strong impro</context>
</contexts>
<marker>Dyer, Clark, Lavie, Smith, 2011</marker>
<rawString>Chris Dyer, Jonathan H. Clark, Alon Lavie, and Noah A. Smith. 2011. Unsupervised word alignment with arbitrary features. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="17375" citStr="Fraser and Marcu, 2007" startWordPosition="2920" endWordPosition="2923"> and β = 0.05. We did not choose a large α, as the dataset was small, and we chose a conservative value for β. We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al., 2003). For models with the smoothed `0 prior, we tuned α and β separately in each direction. 3.2 Alignment First, we evaluated alignment accuracy directly by comparing against gold-standard word alignments. 4This data is available at http://statmt.org/wmt10. The results are shown in the alignment F1 column of Table 1. We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007). Following Dyer et al. (2011), we also measured the average fertility, ˜φsing., of once-seen source words in the symmetrized alignments. Our alignments show smaller fertility for once-seen words, suggesting that they suffer from “garbage collection” effects less than the baseline alignments do. The fact that we had to use hand-aligned data to tune the hyperparameters α and β means that our method is no longer completely unsupervised. However, our observation is that alignment accuracy is actually fairly robust to the choice of these hyperparameters, as shown in Table 2. As we will see below, </context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Computational Linguistics, 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo˜ao V Grac¸a</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
</authors>
<title>Learning tractable word alignment models with complex constraints.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<marker>Grac¸a, Ganchev, Taskar, 2010</marker>
<rawString>Jo˜ao V. Grac¸a, Kuzman Ganchev, and Ben Taskar. 2010. Learning tractable word alignment models with complex constraints. Computational Linguistics, 36(3):481–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="16962" citStr="Koehn et al., 2003" startWordPosition="2855" endWordPosition="2858">s.4 We set the hyperparameters α and β by tuning on gold-standard word alignments (to maximize F1) when possible. For Arabic-English and ChineseEnglish, we used 346 and 184 hand-aligned sentences from LDC2006E86 and LDC2006E93. Similarly, for Czech-English, 515 hand-aligned sentences were available (Bojar and Prokopov´a, 2006). But for Urdu-English, since we did not have any gold alignments, we used α = 10 and β = 0.05. We did not choose a large α, as the dataset was small, and we chose a conservative value for β. We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al., 2003). For models with the smoothed `0 prior, we tuned α and β separately in each direction. 3.2 Alignment First, we evaluated alignment accuracy directly by comparing against gold-standard word alignments. 4This data is available at http://statmt.org/wmt10. The results are shown in the alignment F1 column of Table 1. We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007). Following Dyer et al. (2011), we also measured the average fertility, ˜φsing., of once-seen source words in the symmetrized alignments. Our alignments show smaller fertility for once-se</context>
<context position="20115" citStr="Koehn et al., 2003" startWordPosition="3389" endWordPosition="3392">hineseEnglish alignment, comparing the baseline with our smoothed-`0 method. In all four cases, the baseline produces incorrect extra alignments that prevent good translation rules from being extracted while the smoothed-`0 results are correct. In particular, the baseline system demonstrates typical “garbage collection” behavior (Moore, 2004) in all four examples. 3.3 Translation We then tested the effect of word alignments on translation quality using the hierarchical phrasebased translation system Hiero (Chiang, 2007). We used a fairly standard set of features: seven inherited from Pharaoh (Koehn et al., 2003), a secsetting align F1 (%) B (%) t(f I e) t(e |f) 2008 2009 1st 1st 70.8 41.1 43.7 1st 2nd 70.7 41.1 43.8 2nd 1st 70.7 40.7 44.1 2nd 2nd 70.9 41.1 44.2 Table 4: Optimizing hyperparameters on alignment F1 score does not necessarily lead to optimal B . The first two columns indicate whether we used the first- or second-best alignments in each direction (according to F1); the third column shows the F1 of the symmetrized alignments, whose corresponding B scores are shown in the last two columns. ond language model, and penalties for the glue rule, identity rules, unknown-word rules, and two kinds</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="21707" citStr="Koehn, 2004" startWordPosition="3656" endWordPosition="3657">ere used for word alignment. The development data that were used for discriminative training were: for Chinese-English and Arabic-English, data from the NIST 2004 and NIST 2006 test sets, plus newsgroup data from the 317 GALE program (LDC2006E92); for Urdu-English, half of the NIST 2008 test set; for Czech-English, a training set of 2051 sentences provided by the WMT10 translation workshop. The results are shown in the B column of Table 1. We used case-insensitive IBM B (closest reference length) as our metric. Significance testing was carried out using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). All of the tests showed significant improvements (p &lt; 0.01), ranging from +0.4 B to +1.4 B . For Urdu, even though we didn’t have manual alignments to tune hyperparameters, we got significant gains over a good baseline. This is promising for languages that do not have any manually aligned data. Ideally, one would want to tune α and f3 to maximize B . However, this is prohibitively expensive, especially if we must tune them separately in each alignment direction before symmetrization. We ran some contrastive experiments to investigate the impact of hyperparameter tuning o</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2262" citStr="Liang et al., 2006" startWordPosition="344" endWordPosition="347">nated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an t0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previo</context>
<context position="23205" citStr="Liang et al. (2006)" startWordPosition="3903" endWordPosition="3906">ortunately, we find that optimizing F1 is not optimal for B —using the second-best alignments yields a further improvement of 0.5 B on the NIST 2009 data, which is statistically significant (p &lt; 0.05). 4 Related Work Schoenemann (2011a), taking inspiration from Bodrumlu et al. (2009), uses integer linear programming to optimize IBM Model 1–2 and the HMM with the `0-norm. This method, however, does not outperform GIZA++. In later work, Schoenemann (2011b) used projected gradient descent for the f1- norm. Here, we have adopted his use of projected gradient descent, but using a smoothed `0-norm. Liang et al. (2006) show how to train IBM models in both directions simultaneously by adding a term to the log-likelihood that measures the agreement between the two directions. Grac¸a et al. (2010) explore modifications to the HMM model that encourage bijectivity and symmetry. The modifications take the form of constraints on the posterior distribution over alignments that is computed during the E-step. Mermer and Sarac¸lar (2011) explore a Bayesian version of IBM Model 1, applying sparse Dirichlet priors to t. However, because this method requires the use of Monte Carlo methods, it is not clear how well it can</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cos¸kun Mermer</author>
<author>Murat Sarac¸lar</author>
</authors>
<title>Bayesian word alignment for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL HLT.</booktitle>
<marker>Mermer, Sarac¸lar, 2011</marker>
<rawString>Cos¸kun Mermer and Murat Sarac¸lar. 2011. Bayesian word alignment for statistical machine translation. In Proceedings of ACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving IBM wordalignment Model 1.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5067" citStr="Moore, 2004" startWordPosition="843" endWordPosition="844">j |aj−1, j) and the translation probabilities t(fj |eaj). The three models differ in their estimation of d, but the differences do not concern us here. All three models, as well as IBM Models 3–5, share the same t. For further details of these models, the reader is referred to the original papers describing them (Brown et al., 1993; Vogel et al., 1996). Let θ stand for all the parameters of the model. The standard training procedure is to find the parameter values that maximize the likelihood, or, equivalently, minimize the negative log-likelihood of the observed data: (− log P(f |e, θ)) (2) (Moore, 2004), aligning to many unrelated words. This hurts alignment precision and rule-extraction recall. Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Grac¸a et al., 2010). We have previously proposed another simple remedy to overfitting in the context of unsupervised part-of-speech tagging (Vaswani et al., 2010), which is to minimize the size of the model using a smoothed `0 prior. Applying this prior to an HMM improves tagging accuracy for both Italian and English. Here, our goal is to apply a similar prior in a word-alignment model to the w</context>
<context position="19840" citStr="Moore, 2004" startWordPosition="3346" endWordPosition="3347">e +14.9 +13.8 baseline 64.3 65.2 P(e I f) `0-norm 69.2 70.3 difference +4.9 +5.1 Table 3: Adding word classes improves the F-score in both directions for Arabic-English alignment by a little, for the baseline system more so than ours. Figure 2 shows four examples of ChineseEnglish alignment, comparing the baseline with our smoothed-`0 method. In all four cases, the baseline produces incorrect extra alignments that prevent good translation rules from being extracted while the smoothed-`0 results are correct. In particular, the baseline system demonstrates typical “garbage collection” behavior (Moore, 2004) in all four examples. 3.3 Translation We then tested the effect of word alignments on translation quality using the hierarchical phrasebased translation system Hiero (Chiang, 2007). We used a fairly standard set of features: seven inherited from Pharaoh (Koehn et al., 2003), a secsetting align F1 (%) B (%) t(f I e) t(e |f) 2008 2009 1st 1st 70.8 41.1 43.7 1st 2nd 70.7 41.1 43.8 2nd 1st 70.7 40.7 44.1 2nd 2nd 70.9 41.1 44.2 Table 4: Optimizing hyperparameters on alignment F1 score does not necessarily lead to optimal B . The first two columns indicate whether we used the first- or second-best </context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving IBM wordalignment Model 1. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP.</booktitle>
<contexts>
<context position="2038" citStr="Moore, 2005" startWordPosition="309" endWordPosition="310">(Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM </context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert Moore. 2005. A discriminative framework for bilingual word alignment. In Proceedings of HLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="1703" citStr="Och and Ney, 2004" startWordPosition="256" endWordPosition="259">nent of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011)</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Daniel Marcu</author>
</authors>
<title>Hierarchical search for word alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2083" citStr="Riesa and Marcu, 2010" startWordPosition="315" endWordPosition="318"> the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an t0 prior, inspired</context>
</contexts>
<marker>Riesa, Marcu, 2010</marker>
<rawString>Jason Riesa and Daniel Marcu. 2010. Hierarchical search for word alignment. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Schoenemann</author>
</authors>
<title>Probabilistic word alignment under the L0-norm.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="8117" citStr="Schoenemann (2011" startWordPosition="1396" endWordPosition="1397">xpected counts E[C(e, f)] and the current word-toword conditional probabilities θ. We run PGD for K iterations, producing a sequence of intermediate parameter vectors θ1, ... , θk, ... , θK. Each iteration has two steps, a projection step and a line search. Projection step In this step, we compute: �A θk = �θk − s∇F(θk) (11) This moves θ in the direction of steepest descent (∇F) with step size s, and then the function [·]A projects the resulting point onto the simplex; that is, it finds the nearest point that satisfies the constraints (8). The gradient ∇F(θk) is −t(f |e) (12) β In contrast to Schoenemann (2011b), we use an O(n log n) algorithm for the projection step due to Duchi et. al. (2008), shown in Pseudocode 1. θˆ = arg min ����������− θ e,f ∂F ∂t(f |e) = E[C(f, e)] t(f |e) α + exp β θˆ = arg min E E[C(e, f)] logt(f |e) − θ e,f Pseudocode 1 Project input vector u ∈ Rn onto the (10) probability simplex. � −t( f |e) β This optimization problem is non-convex, and we do not know of a closed-form solution. Previously (Vaswani et al., 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models. Inste</context>
<context position="22820" citStr="Schoenemann (2011" startWordPosition="3842" endWordPosition="3843">symmetrization. We ran some contrastive experiments to investigate the impact of hyperparameter tuning on translation quality. For the smaller Arabic-English corpus, we symmetrized all combinations of the two top-scoring alignments (according to F1) in each direction, yielding four sets of alignments. Table 4 shows B scores for translation models learned from these alignments. Unfortunately, we find that optimizing F1 is not optimal for B —using the second-best alignments yields a further improvement of 0.5 B on the NIST 2009 data, which is statistically significant (p &lt; 0.05). 4 Related Work Schoenemann (2011a), taking inspiration from Bodrumlu et al. (2009), uses integer linear programming to optimize IBM Model 1–2 and the HMM with the `0-norm. This method, however, does not outperform GIZA++. In later work, Schoenemann (2011b) used projected gradient descent for the f1- norm. Here, we have adopted his use of projected gradient descent, but using a smoothed `0-norm. Liang et al. (2006) show how to train IBM models in both directions simultaneously by adding a term to the log-likelihood that measures the agreement between the two directions. Grac¸a et al. (2010) explore modifications to the HMM mo</context>
</contexts>
<marker>Schoenemann, 2011</marker>
<rawString>Thomas Schoenemann. 2011a. Probabilistic word alignment under the L0-norm. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Schoenemann</author>
</authors>
<title>Regularizing mono- and bi-word models for word alignment.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="8117" citStr="Schoenemann (2011" startWordPosition="1396" endWordPosition="1397">xpected counts E[C(e, f)] and the current word-toword conditional probabilities θ. We run PGD for K iterations, producing a sequence of intermediate parameter vectors θ1, ... , θk, ... , θK. Each iteration has two steps, a projection step and a line search. Projection step In this step, we compute: �A θk = �θk − s∇F(θk) (11) This moves θ in the direction of steepest descent (∇F) with step size s, and then the function [·]A projects the resulting point onto the simplex; that is, it finds the nearest point that satisfies the constraints (8). The gradient ∇F(θk) is −t(f |e) (12) β In contrast to Schoenemann (2011b), we use an O(n log n) algorithm for the projection step due to Duchi et. al. (2008), shown in Pseudocode 1. θˆ = arg min ����������− θ e,f ∂F ∂t(f |e) = E[C(f, e)] t(f |e) α + exp β θˆ = arg min E E[C(e, f)] logt(f |e) − θ e,f Pseudocode 1 Project input vector u ∈ Rn onto the (10) probability simplex. � −t( f |e) β This optimization problem is non-convex, and we do not know of a closed-form solution. Previously (Vaswani et al., 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models. Inste</context>
<context position="22820" citStr="Schoenemann (2011" startWordPosition="3842" endWordPosition="3843">symmetrization. We ran some contrastive experiments to investigate the impact of hyperparameter tuning on translation quality. For the smaller Arabic-English corpus, we symmetrized all combinations of the two top-scoring alignments (according to F1) in each direction, yielding four sets of alignments. Table 4 shows B scores for translation models learned from these alignments. Unfortunately, we find that optimizing F1 is not optimal for B —using the second-best alignments yields a further improvement of 0.5 B on the NIST 2009 data, which is statistically significant (p &lt; 0.05). 4 Related Work Schoenemann (2011a), taking inspiration from Bodrumlu et al. (2009), uses integer linear programming to optimize IBM Model 1–2 and the HMM with the `0-norm. This method, however, does not outperform GIZA++. In later work, Schoenemann (2011b) used projected gradient descent for the f1- norm. Here, we have adopted his use of projected gradient descent, but using a smoothed `0-norm. Liang et al. (2006) show how to train IBM models in both directions simultaneously by adding a term to the log-likelihood that measures the agreement between the two directions. Grac¸a et al. (2010) explore modifications to the HMM mo</context>
</contexts>
<marker>Schoenemann, 2011</marker>
<rawString>Thomas Schoenemann. 2011b. Regularizing mono- and bi-word models for word alignment. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Lacoste-Julien Simon</author>
<author>Klein Dan</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="2059" citStr="Taskar et al., 2005" startWordPosition="311" endWordPosition="314">, 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporati</context>
</contexts>
<marker>Taskar, Simon, Dan, 2005</marker>
<rawString>Ben Taskar, Lacoste-Julien Simon, and Klein Dan. 2005. A discriminative matching approach to word alignment. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Adam Pauls</author>
<author>David Chiang</author>
</authors>
<title>Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2930" citStr="Vaswani et al., 2010" startWordPosition="455" endWordPosition="458">have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an t0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previous work on unsupervised part-ofspeech tagging (Vaswani et al., 2010), but enables it to scale to the large datasets typical in word alignment, using an efficient training method based on projected gradient descent (Section 2.3). Experiments on Czech-, Arabic-, Chinese- and UrduEnglish translation (Section 3) demonstrate consistent significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ). Our implementation has been released as a simple modification to the GIZA++ toolkit that can be used as a drop-in replacement for GIZA++ in any existing MT pipeline. 311 Proceedings of the 50th Annual Meeting of t</context>
<context position="5432" citStr="Vaswani et al., 2010" startWordPosition="890" endWordPosition="893">Let θ stand for all the parameters of the model. The standard training procedure is to find the parameter values that maximize the likelihood, or, equivalently, minimize the negative log-likelihood of the observed data: (− log P(f |e, θ)) (2) (Moore, 2004), aligning to many unrelated words. This hurts alignment precision and rule-extraction recall. Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Grac¸a et al., 2010). We have previously proposed another simple remedy to overfitting in the context of unsupervised part-of-speech tagging (Vaswani et al., 2010), which is to minimize the size of the model using a smoothed `0 prior. Applying this prior to an HMM improves tagging accuracy for both Italian and English. Here, our goal is to apply a similar prior in a word-alignment model to the word-to-word translation probabilities t(f |e). We leave the distortion models alone, since they are not very large, and there is not much reason to believe that we can profit from compacting them. With the addition of the `0 prior, the MAP (maximum a posteriori) objective function is θˆ = arg min (− log P(f |e, θ)P(θ)) (4) θ where P(θ) ∝ exp (−αkθkro) (5) and exp</context>
<context position="7381" citStr="Vaswani et al., 2010" startWordPosition="1260" endWordPosition="1263"> to the constraints E t(f |e) = 1 for all e. (8) f We can carry out the optimization in (7) with the MAP-EM algorithm (Bishop, 2006). EM and MAPEM share the same E-step; the difference lies in the 312 1 0.8 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 Figure 1: The `0-norm (top curve) and smoothed approximations (below) for β = 0.05, 0.1, 0.2. M-step. For vanilla EM, the M-step is: �������� E[C(e, f)] log t(f |e) (9) � again subject to the constraints (8). The count C(e, f) is the number of times that f occurs aligned to e. For MAP-EM, it is: (10); we seek to minimize this function. As in previous work (Vaswani et al., 2010), we optimize each set of parameters {t(· |e)} separately for each English word type e. The inputs to the PGD are the expected counts E[C(e, f)] and the current word-toword conditional probabilities θ. We run PGD for K iterations, producing a sequence of intermediate parameter vectors θ1, ... , θk, ... , θK. Each iteration has two steps, a projection step and a line search. Projection step In this step, we compute: �A θk = �θk − s∇F(θk) (11) This moves θ in the direction of steepest descent (∇F) with step size s, and then the function [·]A projects the resulting point onto the simplex; that is</context>
</contexts>
<marker>Vaswani, Pauls, Chiang, 2010</marker>
<rawString>Ashish Vaswani, Adam Pauls, and David Chiang. 2010. Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1496" citStr="Vogel et al., 1996" startWordPosition="224" endWordPosition="227"> Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ). 1 Introduction Automatic word alignment is a vital component of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although ma</context>
<context position="4809" citStr="Vogel et al., 1996" startWordPosition="797" endWordPosition="800">ord fj is aligned to. In IBM Model 1–2 and the HMM model, the joint probability of the French sentence and alignment given the English sentence is m P(f, a |e) = H d(aj |aj−1, j)t(fj |eaj). (1) j=1 The parameters of these models are the distortion probabilities d(aj |aj−1, j) and the translation probabilities t(fj |eaj). The three models differ in their estimation of d, but the differences do not concern us here. All three models, as well as IBM Models 3–5, share the same t. For further details of these models, the reader is referred to the original papers describing them (Brown et al., 1993; Vogel et al., 1996). Let θ stand for all the parameters of the model. The standard training procedure is to find the parameter values that maximize the likelihood, or, equivalently, minimize the negative log-likelihood of the observed data: (− log P(f |e, θ)) (2) (Moore, 2004), aligning to many unrelated words. This hurts alignment precision and rule-extraction recall. Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Grac¸a et al., 2010). We have previously proposed another simple remedy to overfitting in the context of unsupervised part-of-speech tagging</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="21728" citStr="Zhang et al., 2004" startWordPosition="3658" endWordPosition="3661">word alignment. The development data that were used for discriminative training were: for Chinese-English and Arabic-English, data from the NIST 2004 and NIST 2006 test sets, plus newsgroup data from the 317 GALE program (LDC2006E92); for Urdu-English, half of the NIST 2008 test set; for Czech-English, a training set of 2051 sentences provided by the WMT10 translation workshop. The results are shown in the B column of Table 1. We used case-insensitive IBM B (closest reference length) as our metric. Significance testing was carried out using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). All of the tests showed significant improvements (p &lt; 0.01), ranging from +0.4 B to +1.4 B . For Urdu, even though we didn’t have manual alignments to tune hyperparameters, we got significant gains over a good baseline. This is promising for languages that do not have any manually aligned data. Ideally, one would want to tune α and f3 to maximize B . However, this is prohibitively expensive, especially if we must tune them separately in each alignment direction before symmetrization. We ran some contrastive experiments to investigate the impact of hyperparameter tuning on translation quality</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings of LREC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>