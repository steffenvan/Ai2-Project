<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000035">
<title confidence="0.984962">
Cluster-based Prediction of User Ratings for Stylistic Surface Realisation
</title>
<author confidence="0.989639">
Nina Dethlefs, Heriberto Cuay´ahuitl, Helen Hastie, Verena Rieser and Oliver Lemon
</author>
<affiliation confidence="0.957244">
Heriot-Watt University, Mathematical and Computer Sciences, Edinburgh
</affiliation>
<email confidence="0.995975">
n.s.dethlefs@hw.ac.uk
</email>
<sectionHeader confidence="0.995611" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999574782608696">
Surface realisations typically depend on
their target style and audience. A challenge
in estimating a stylistic realiser from data is
that humans vary significantly in their sub-
jective perceptions of linguistic forms and
styles, leading to almost no correlation be-
tween ratings of the same utterance. We ad-
dress this problem in two steps. First, we
estimate a mapping function between the
linguistic features of a corpus of utterances
and their human style ratings. Users are
partitioned into clusters based on the sim-
ilarity of their ratings, so that ratings for
new utterances can be estimated, even for
new, unknown users. In a second step, the
estimated model is used to re-rank the out-
puts of a number of surface realisers to pro-
duce stylistically adaptive output. Results
confirm that the generated styles are recog-
nisable to human judges and that predictive
models based on clusters of users lead to
better rating predictions than models based
on an average population of users.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999763306122449">
Stylistic surface realisation aims not only to find
the best realisation candidate for a semantic input
based on some underlying trained model, but also
aims to adapt its output to properties of the user,
such as their age, social group, or location, among
others. One of the first systems to address stylis-
tic variation in generation was Hovy (1988)’s
PAULINE, which generated texts that reflect dif-
ferent speaker attitudes towards events based on
multiple, adjustable features. Stylistic variation
in such contexts can often be modelled systemat-
ically as a multidimensional variation space with
several continuous dimensions, so that varying
stylistic scores indicate the strength of each di-
mension in a realisation candidate. Here, we fo-
cus on the dimensions of colloquialism, politeness
and naturalness. Assuming a target score on one
or more dimensions, candidate outputs of a data-
driven realiser can then be ranked according to
their predicted affinity with the target scores.
In this paper, we aim for an approach to stylis-
tic surface realisation which is on the one hand
based on natural human data so as to reflect stylis-
tic variation that is as natural as possible. On the
other hand, we aim to minimise the amount of
annotation and human engineering that informs
the design of the system. To this end, we esti-
mate a mapping function between automatically
identifiable shallow linguistic features character-
istic of an utterance and its human-assigned style
ratings. In addition, we aim to address the high
degree of variability that is often encountered in
subjective rating studies, such as assessments of
recommender systems (O’Mahony et al., 2006;
Amatriain et al., 2009), sentiment analysis (Pang
and Lee, 2005), or surface realisations, where user
ratings have been shown to differ significantly
(p&lt;0.001) for the same utterance (Walker et al.,
2007). Such high variability can affect the per-
formance of systems which are trained from an
average population of user ratings. However, we
are not aware of any work that has addressed this
problem principally by estimating ratings for both
known users, for whom ratings exists, and un-
known users, for whom no prior ratings exist. To
achieve this, we propose to partition users into
clusters of individuals who assign similar ratings
to linguistically similar utterances, so that their
ratings can be estimated more accurately than
</bodyText>
<page confidence="0.954289">
702
</page>
<note confidence="0.993031">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702–711,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99983075">
based on an average population of users. This is
similar to Janarthanam and Lemon (2014), who
show that clustering users and adapting to their
level of domain expertise can significantly im-
prove task success and user ratings. Our resulting
model is evaluated with realisers not originally
built to deal with stylistic variation, and produces
natural variation recognisable by humans.
</bodyText>
<sectionHeader confidence="0.923122" genericHeader="introduction">
2 Architecture and Domain
</sectionHeader>
<bodyText confidence="0.999990411764706">
We aim to with generating restaurant recommen-
dations as part of an interactive system. To do
this, we assume that a generator input is provided
by a preceding module, e.g. the interaction man-
ager, and that the task of the surface realiser is
to find a suitable stylistically appropriate realisa-
tion. An example input is inform(food=Italian,
name=Roma), which could be expressed as The
restaurant Roma serves Italian food. A further
aspect is that users are initially unknown to the
system, but that it should adapt to them over time
by discovering their stylistic preferences. Fu-
ture work involves integrating the surface realiser
into the PARLANCE1 (Hastie et al., 2013) spo-
ken dialogue system with a method for triggering
the different styles. Here, we leave the question
of when different styles are appropriate as future
work and focus on being able to generate them.
The architecture of our model is shown in Fig-
ure 1. Training of the regression model from sty-
listically-rated human corpora is shown in the top-
left box (grey). Utterance ratings from human
judges are used to extract shallow linguistic fea-
tures as well as to estimate user clusters. Both
types of information inform the resulting stylis-
tic regression model. For surface realisation (top-
right box, blue), a semantic input from a preced-
ing model is given as input to a surface realiser.
Any realiser is suitable that returns a ranked list of
output candidates. The resulting list is re-ranked
according to stylistic scores estimated by the re-
gressor, so that the utterance which most closely
reflects the target score is ranked highest. The re-
ranking process is shown in the lower box (red).
</bodyText>
<sectionHeader confidence="0.99999" genericHeader="related work">
3 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999904">
3.1 Stylistic Variation in Surface Realisation
</subsectionHeader>
<bodyText confidence="0.9411485">
Our approach is most closely related to work by
Paiva and Evans (2005) and Mairesse and Walker
</bodyText>
<footnote confidence="0.982539">
1http://parlance-project.eu
</footnote>
<figureCaption confidence="0.934440166666667">
Figure 1: Architecture of stylistic realisation model.
Top left: user clusters are estimated from corpus ut-
terances described by linguistic features and ratings.
Top right: surface realisation ranks a list of output can-
didates based on a semantic input. These are ranked
stylistically given a trained regressor.
</figureCaption>
<bodyText confidence="0.99982034375">
(2011), discussed in turn here. Paiva and Evans
(2005) present an approach that uses multivari-
ate linear regression to map individual linguistic
features to distinguishable styles of text. The ap-
proach works in three steps. First, a factor anal-
ysis is used to determine the relevant stylistic di-
mensions from a corpus of human text using shal-
low linguistic features. Second, a hand-crafted
generator is used to produce a large set of ut-
terances, keeping traces of each generator deci-
sion, and obtaining style scores for each output
based on the estimated factor model. The result
is a dataset of &lt;generator decision, style score&gt;
pairs which can be used in a correlation analy-
sis to identify the predictors of particular output
styles. During generation, the correlation equa-
tions inform the generator at each choice point so
as to best express the desired style. Unfortunately,
no human evaluation of the model is presented so
that it remains unclear to what extent the gener-
ated styles are perceivable by humans.
Closely related is work by Mairesse and Walker
(2011) who present the PERSONAGE system,
which aims to generate language reflecting par-
ticular personalities. Instead of choosing genera-
tor decisions by considering their predicted style
scores, however, Mairesse and Walker (2011) di-
rectly predict generator decisions based on tar-
get personality scores. To obtain the generator,
the authors first generate a corpus of utterances
which differ randomly in their linguistic choices.
All utterances are rated by humans indicating the
</bodyText>
<page confidence="0.997706">
703
</page>
<bodyText confidence="0.99924325">
extent to which they reflect different personality
traits. The best predictive model is then chosen in
a comparison of several classifiers and regressors.
Mairesse and Walker (2011) are the first to evalu-
ate their generator with humans and show that the
generated personalities are indeed recognisable.
Approaches on replicating personalities in re-
alisations include Gill and Oberlander (2002) and
Isard et al. (2006). Porayska-Pomsta and Mellish
(2004) and Gupta et al. (2007) are approaches to
politeness in generation, based on the notion of
face and politeness theory, respectively.
</bodyText>
<subsectionHeader confidence="0.999841">
3.2 User Preferences in Surface Realisation
</subsectionHeader>
<bodyText confidence="0.999979916666667">
Taking users’ individual content preferences into
account for training generation systems can
positively affect their performance (Jordan and
Walker, 2005; Dale and Viethen, 2009). We are
interested in individual user perceptions concern-
ing the surface realisation of system output and
the way they relate to different stylistic dimen-
sions. Walker et al. (2007) were the first to show
that individual preferences exist for the perceived
quality of realisations and that these can be mod-
elled in trainable generation. They train two ver-
sions of a rank-and-boost generator, a first version
of which is trained on the average population of
user ratings, whereas a second one is trained on
the ratings of individual users. The authors show
statistically that ratings from different users are
drawn from different distributions (p&lt;0.001) and
that significantly better performance is achieved
when training and testing on data of individual
users. In fact, training a model on one user’s rat-
ings and testing it on another’s performs as badly
as a random baseline. However, no previous work
has modelled the individual preferences of unseen
users–for whom no training data exists.
</bodyText>
<sectionHeader confidence="0.758044" genericHeader="method">
4 Estimation of Style Prediction Models
</sectionHeader>
<subsectionHeader confidence="0.996436">
4.1 Corpora and Style Dimensions
</subsectionHeader>
<bodyText confidence="0.999954333333333">
Our domain of interest is the automatic generation
of restaurant recommendations that differ with re-
spect to their colloquialism and politeness and are
as natural as possible. All three stylistic dimen-
sion were identified from a qualitative analysis of
human domain data. To estimate the strength of
each of them in a single utterance, we collect user
ratings for three data sets that were collected un-
der different conditions and are freely available.
</bodyText>
<table confidence="0.99746125">
Corpus Colloquial Natural Polite
LIST 3.38 f 1.5 4.06 f 1.2 4.35 f 0.8
MAI 3.95 f 1.2 4.32 f 1.0 4.27 f 0.8
CLASSIC 4.29 f 1.1 4.20 f 1.2 3.64 f 1.3
</table>
<tableCaption confidence="0.988530333333333">
Table 1: Average ratings with standard deviations.
Ratings between datasets (except one) differ signifi-
cantly at p&lt;0.01, using the Wilcoxon signed-rank test.
</tableCaption>
<listItem confidence="0.989089307692308">
• LIST is a corpus of restaurant recommenda-
tions from the website The List.2 It consists
of professionally written reviews. An exam-
ple is “Located in the heart ofBarnwell, Bel-
uga is an excellent restaurant with a smart
menu of modern Italian cuisine.”
• MAI is a dataset collected by Mairesse et
al. (2010),3 using Amazon Mechanical Turk.
Turkers typed in recommendations for vari-
ous specified semantics; e.g. “I recommend
the restaurant Beluga near the cathedral.”
• CLASSIC is a dataset of transcribed spoken
user utterances from the CLASSiC project.4
</listItem>
<bodyText confidence="0.9481601">
The utterances consist of user queries for
restaurants, such as “I need an Italian
restaurant with a moderate price range.”
Our joint dataset consists of 1, 361 human ut-
terances, 450 from the LIST, 334 from MAI,
and 577 from CLASSIC. We asked users on the
CrowdFlower crowdsourcing platform5 to read
utterances and rate their colloquialism, politeness
and naturalness on a 1-5 scale (the higher the bet-
ter). The following questions were asked.
</bodyText>
<listItem confidence="0.99997525">
• Colloquialism: The utterance is colloquial,
i.e. could have been spoken.
• Politeness: The utterance is polite / friendly.
• Naturalness: The utterance is natural, i.e.
</listItem>
<bodyText confidence="0.9534914">
could have been produced by a human.
The question on naturalness can be seen as a gen-
eral quality check for our training set. We do
not aim to generate unnatural utterances. 167
users took part in our rating study leading to a
rated dataset of altogether 3,849 utterances. All
users were from the USA. The average ratings per
dataset and stylistic dimension are summarised
in Table 1. From this, we can see that LIST ut-
terances were perceived as the least natural and
</bodyText>
<footnote confidence="0.997174">
2http://www.list.co.uk/
3http://people.csail.mit.edu/francois/
research/bagel/
4http://www.classic-project.org/
5http://crowdflower.com/
</footnote>
<page confidence="0.997134">
704
</page>
<bodyText confidence="0.999996103448276">
colloquial, but as the most polite. CLASSIC ut-
terances were perceived as the most colloquial,
but the least polite, and MAI utterances were rated
as the most natural. Differences between ratings
for each dimension and dataset are significant at
p&lt;0.01, using the Wilcoxon signed-rank test, ex-
cept the naturalness for MAI and CLASSIC.
Since we are mainly interested in the lexical
and syntactic features of utterances here, the fact
that CLASSIC utterances are spoken, whereas the
other two corpora are written, should not affect
the quality of the resulting model. Similarly, some
stylistic categories may seem closely related, such
as colloquialism and naturalness, or orthogonal
to each other, such as politeness and colloqui-
alism. However, while ratings for colloquialism
and naturalness are very close for the CLASSIC
dataset, they vary significantly for the two other
datasets (p&lt;0.01). Also, the ratings for colloqui-
alim and politeness show a weak positive corre-
lation of 0.23, i.e. are not perceived as orthogo-
nal by users. These results suggest that all in all
our three stylistic categories are perceived as suf-
ficiently different from each other and suitable for
training to predict a spectrum of different styles.
Another interesting aspect is that individual
user ratings vary significantly, leading to a high
degree of variability for identical utterances. This
will be the focus of the following sections.
</bodyText>
<subsectionHeader confidence="0.989736">
4.2 Feature Estimation
</subsectionHeader>
<bodyText confidence="0.999887125">
Table 2 shows the feature set we will use in our
regression experiments. We started from a larger
subset including 45 lexical and syntactic features
as well as unigrams and bigrams, all of which
could be identified from the corpus without man-
ual annotation. The only analysis tool we used
was the Stanford Parser,6 which identified certain
types of words (pronouns, wh-words) or the depth
of syntactic embedding. A step-wise regression
analysis was then carried out to identify those
features that contributed significantly (at p&lt;0.01)
to the overall regression equation obtained per
stylistic dimension. Of all lexical features (uni-
grams and bigrams), the word with was the only
contributor. A related feature was the average tf-
idf score of the content words in an utterance.
</bodyText>
<footnote confidence="0.934514">
6http://nlp.stanford.edu/software/
lex-parser.shtml
</footnote>
<table confidence="0.973010333333333">
Feature Type
Length of utterance num
Presence of personal pronouns bool
Presence of WH words bool
with cue word bool
Presence of negation bool
</table>
<tableCaption confidence="0.8765555">
Average length of content words num
Ave tf-idf score of content words num
Depth of syntactic embedding num
Table 2: Features used for regression, which were
identified as significant contributors (p&lt;0.01) from a
larger feature set in a step-wise regression analysis.
</tableCaption>
<subsectionHeader confidence="0.998375">
4.3 Regression Experiments
</subsectionHeader>
<bodyText confidence="0.999949882352941">
Based on the features identified in Section 4.2, we
train a separate regressor for each stylistic dimen-
sion. The task of the regressor is to predict, based
on the extracted linguistic features of an utterance,
a score in the range of 1-5 for colloquialism, po-
liteness and naturalness. We compare: (1) a mul-
tivariate multiple regressor (MMR), (2) an M5P
decision tree regressor, (3) a support vector ma-
chine (SVM) with linear kernel, and (4) a ZeroR
classifier, which serves as a majority baseline. We
used the R statistics toolkit7 for the MMR and the
Weka toolkit8 for the remaining models.
Average User Ratings The regressors were first
trained to predict the average user ratings of an ut-
terance and evaluated in a 10-fold cross validation
experiment. Table 3 shows the results. Here, r
denotes the Pearson correlation coefficient, which
indicates the correlation between the predicted
and the actual user scores; R2 is the coefficient of
determination, which provides a measure of how
well the learnt model fits the data; and RMSE
refers to the Root Mean Squared Error, the error
between the predicted and actual user ratings.
We can observe that MMR achieves the best
performance for predicting colloquialism and nat-
uralness, whereas M5P best predicts politeness.
Unfortunately, all regressors achieve at best a
moderate correlation with human ratings. Based
on these results, we ran a correlation analysis for
all utterances for which more than 20 original
user ratings were available. The purpose was to
find out to what extent human raters agree with
each other. The results showed that user agree-
ment in fact ranges from a high positive corre-
</bodyText>
<footnote confidence="0.9994225">
7http://www.r-project.org/
8http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<page confidence="0.973696">
705
</page>
<table confidence="0.999944461538462">
Model r R2 RMSE
Colloquial MMR 0.50 0.25 0.85
SVM 0.47 0.22 0.86
M5P 0.48 0.23 0.85
ZeroR -0.08 0.006 0.97
Natural MMR 0.30 0.09 0.78
SVM 0.24 0.06 0.81
M5P 0.27 0.07 0.78
ZeroR -0.09 0.008 0.81
Polite MMR 0.33 0.11 0.71
SVM 0.31 0.09 0.73
M5P 0.42 0.18 0.69
ZeroR -0.09 0.008 0.76
</table>
<tableCaption confidence="0.702741333333333">
Table 3: Comparison of regression models per dimen-
sion using average user ratings. The best model is
indicated in bold-face for the correlation coefficient.
</tableCaption>
<table confidence="0.999979769230769">
Model r R2 RMSE
Colloquial MMR 0.61 0.37 1.05
SVM 0.36 0.13 1.3
M5P 0.56 0.31 1.07
ZeroR -0.06 0.004 1.3
Natural MMR 0.55 0.30 0.96
SVM 0.36 0.13 1.13
M5P 0.49 0.24 0.99
ZeroR -0.08 0.06 1.13
Polite MMR 0.69 0.48 0.76
SVM 0.54 0.30 0.92
M5P 0.71 0.50 0.73
ZeroR -0.04 0.002 1.04
</table>
<tableCaption confidence="0.765888666666667">
Table 4: Comparison of regression models per dimen-
sion using individual user ratings. The best model is
indicated in bold-face for the correlation coefficient.
</tableCaption>
<bodyText confidence="0.999955282051282">
lation of 0.79 to a moderate negative correlation
of −0.55. The average is 0.04 (SD=0.95), i.e.
indicating no correlation between user ratings,
even for the same utterance. This observation is
partially in line with related work that has found
high diversity in subjective user ratings. Yeh and
Mellish (1997) report only 70% agreement of hu-
man judges on the best choice of referring ex-
pression. Amatriain et al. (2009) report incon-
sistencies in user ratings in recommender systems
with an RMSE range of 0.55 to 0.81 and argue
that this constitutes a lower bound for system per-
formance. This inconsistency is exacerbated by
raters recruited via crowdsourcing platforms as
in our study (Koller et al., 2010; Rieser et al.,
2011). However, while crowdsourced data have
been shown to contain substantially more noise
than data collected in a lab environment, they do
tend to reflect the general tendency of their more
controlled counterparts (Gosling et al., 2004).
Individual User Ratings Given that individual
preferences exist for surface realisation (Walker
et al., 2007), we included the user’s ID as a re-
gression feature and re-ran the experiments. The
hypothesis was that if users differ in their pref-
erences for realisation candidates, they may also
differ in terms of their perceptions of linguistic
styles. The results shown in Table 4 support this:
the obtained correlations are significantly higher
(p&lt;0.001, using the Fisher r-to-z transformation)
than those without the user’s ID (though we are
still not able to model the full variation observed
in ratings). Importantly, this shows that user rat-
ings are intrinsically coherent (not random) and
that variation exists mainly for inter-user agree-
ment. This model performs satisfactorily for a
known population of users. However, it does not
allow the prediction of ratings of unknown users,
who we mostly encounter in generation.
</bodyText>
<sectionHeader confidence="0.964707" genericHeader="method">
5 Clustering User Rating Behaviour
</sectionHeader>
<subsectionHeader confidence="0.977408">
5.1 Spectral Clustering
</subsectionHeader>
<bodyText confidence="0.999914727272727">
The goal of this section is to find a number of k
clusters which partition our data set of user rat-
ings in a way that users in one cluster rate ut-
terances with particular linguistic properties most
similarly to each other, while rating them most
dissimilarly to users in other clusters. We as-
sume a set of n data points x1 ... xn, which
in our case correspond to an individual user or
group of users, characterised in terms of word
bigrams, POS tag bigrams, and assigned rat-
ings of the utterance they rated. An example
is Beluga NNP serves VBZ Italian JJ food NN;
[col=5.0, nat=5.0, pol=4.0]. Features were cho-
sen as a subset of relevant features from the larger
set used for regression above.
Using spectral clustering (von Luxburg, 2007),
clusters can be identified from a set of eigenvec-
tors of an affinity matrix S derived from pair-wise
similarities between data points sij = s(xi, xj)
using a symmetric and non-negative similarity
function. To do that, we use a cumulative simi-
larity based on the Kullback-Leibler divergence,
</bodyText>
<equation confidence="0.9142244">
pilo92(pi
qi ) +
j
,
2
</equation>
<bodyText confidence="0.999804333333333">
where P is a distribution of words, POS tags or
ratings in data point xi; and Q a similar distribu-
tion in data point xj. The lower the cumulative di-
</bodyText>
<figure confidence="0.693338333333333">
D(P, Q) =
�
i
qjlo92(qjpj )
706
Number of Clusters
</figure>
<figureCaption confidence="0.984947333333333">
Figure 2: Average correlation coefficient for different
numbers of clusters. For comparison, results from av-
erage and individual user ratings are also shown.
</figureCaption>
<bodyText confidence="0.999942724137931">
vergence between two data sets, the more similar
they are. To find clusters of similar users from the
affinity matrix 5, we use the algorithm described
in Ng et al. (2001). It derives clusters by choosing
the k largest eigenvectors u1, u2, ... , uk from the
Laplacian matrix L = D1/2 −5D1/2 (where D is
a diagonal matrix), arranging them into columns
in a matrix U = [u1u2 ... uk] and then normalis-
ing them for length. The result is a new matrix T,
obtained through tij = uij/(Ek u2ik)1/2. The set
of clusters C1,... Ck can then be obtained from T
using the K-means algorithm, where each row in
T serves as an individual data point. Finally, each
original data point xi (row i of T) is assigned to a
cluster Cj. In comparison to other clustering algo-
rithms, experiments by Ng et al. (2001) show that
spectral clustering is robust for convex and non-
convex data sets. The authors also demonstrate
why using K-means only is often not sufficient.
The main clusters obtained describe surface
realisation preferences by particular groups of
users. An example is the realisation of the loca-
tion of a restaurant as a prepositional phrase or as
a relative clause as in restaurant in the city centre
vs. restaurant located in the city centre; or the re-
alisation of the food type as an adjective, an Ital-
ian restaurant, vs. a clause, this restaurant serves
Italian food. Clusters can then be characterised as
different combinations of such preferences.
</bodyText>
<subsectionHeader confidence="0.974032">
5.2 Results: Predicting Stylistic Ratings
</subsectionHeader>
<bodyText confidence="0.9998485">
Figure 2 shows the average correlation coefficient
r across dimensions in relation to the number
of clusters, in comparison to the results obtained
with average and individual user ratings. We can
see that the baseline without user information is
outperformed with as few as three clusters. From
30 clusters on, a medium correlation is obtained
until another performance jump occurs around 90
clusters. Evidently, the best performance would
be achieved by obtaining one cluster per user, i.e.
167 clusters, but nothing would be gained in this
way, and we can see that useful generalisations
can be made from much fewer clusters. Based on
the clusters found, we will now predict the ratings
of known and unknown users.
Known Users For known users, first of all, Fig-
ure 3 shows the correlations between the predicted
and actual ratings for colloquialism, politeness
and naturalness based on 90 user clusters. Cor-
relation coefficients were obtained using an MMR
regressor. We can see that a medium correlation is
achieved for naturalness and (nearly) strong cor-
relations are achieved for politeness and colloqui-
alism. This confirms that clustering users can help
to better predict their ratings than based on shal-
low linguistic features alone, but that more gener-
alisation is achieved than based on individual user
ratings that include the user’s ID as a regression
feature. The performance gain in comparison to
predicting average ratings is significant (p&lt;0.01)
from as few as three clusters onwards.
Unknown Users We initially sort unknown
users into the majority cluster and then aim to
make more accurate cluster allocations as more
information becomes available. For example, af-
ter a user has assigned their first rating, we can
take it into account to re-estimate their cluster
more accurately. Clusters are re-estimated with
each new rating, based on our trained regression
model. While estimating a user cluster based on
linguistic features alone yields an average corre-
lation of 0.38, an estimation based on linguistic
features and a single rating alone already yields an
average correlation of 0.45. From around 30 rat-
ings, the average correlation coefficients achieved
are as good as for known users. More importantly,
though, estimations based on a single rating alone
significantly outperform ratings based on the av-
</bodyText>
<figure confidence="0.999329666666667">
1 3 5 7 9 20 40 60 80 100 167
0.3 0.4 0.5 0.6
Individual
Clusters
Average
Correlation Coefficient
</figure>
<page confidence="0.699988">
707
</page>
<figureCaption confidence="0.916992">
Figure 3: Correlations per dimension between actual and predicted user ratings based on 90 user clusters: (a)
Colloquialism (r = 0.57, p&lt;0.001), (b) Naturalness (r = 0.49, p&lt;0.001) and (c) Politeness (r = 0.59, p&lt;0.001).
</figureCaption>
<figure confidence="0.998295777777778">
(a)
Actual Ratings
(b)
Actual Ratings
(c)
Actual Ratings
Correlation: Colloquialism
1 2 3 4 5
2 3 4 5
Predicted Ratings
Correlation: Naturalness
1 2 3 4 5
Correlation: Politeness
1 2 3 4 5
1 2 3 4 5
Predicted Ratings
1 2 3 4 5
Predicted Ratings
</figure>
<bodyText confidence="0.992332">
erage population of users (p&lt;0.001). Fig. 4 shows
this process. It shows the correlation between pre-
dicted and actual user ratings for unknown users
over time. This is useful in interactive scenarios,
where system behaviour is refined as more infor-
mation becomes available (Cuay´ahuitl and Deth-
lefs, 2011; Gaˇsi´c et al., 2011), or for incremental
systems (Skantze and Hjalmarsson, 2010; Deth-
lefs et al., 2012b; Dethlefs et al., 2012a).
</bodyText>
<subsectionHeader confidence="0.449735">
Number of Ratings
</subsectionHeader>
<figureCaption confidence="0.993742666666667">
Figure 4: Average correlation coefficient for unknown
users with an increasing number of ratings. Results
from 90 clusters and average ratings are also shown.
</figureCaption>
<sectionHeader confidence="0.988309" genericHeader="method">
6 Evaluation: Stylistically-Aware
</sectionHeader>
<subsectionHeader confidence="0.678318">
Surface Realisation
</subsectionHeader>
<bodyText confidence="0.999966375">
To evaluate the applicability of our regression
model for stylistically-adaptive surface realisa-
tion, this section describes work that compares
four different surface realisers, which were not
originally developed to produce stylistic variation.
To do that, we first obtain the cluster for each in-
put sentence s: c* = argminc∈C &amp;x D(Psx |Qxc),
where x refers to n-grams, POS tags or ratings
(see Section 5.1); P refers to a discrete probability
distribution of sentence s; and Q refers to a dis-
crete probability distribution of cluster c. The best
cluster is used to compute the style score of sen-
tence s using: score(s) = Eni Oifi(s), c* E F,
where Oi are the weights estimated by the regres-
sor, and fi are the features of sentence s; see Table
2. The idea is that if well-phrased utterances can
be generated, whose stylistic variation is recog-
nisable to human judges, then our regressor can
be used in combination with any statistical sur-
face realiser. Note however that the stylistic vari-
ation observed depends on the stylistic spectrum
that each realiser covers. Here, our goal is mainly
to show that whatever stylistic variation exists in
a realiser can be recognised by our model.
</bodyText>
<subsectionHeader confidence="0.999858">
6.1 Overview of Surface Realisers
</subsectionHeader>
<bodyText confidence="0.999965285714286">
In a human rating study, we compare four surface
realisers (ordered alphabetically), all of which
are able to return a ranked list of candidate re-
alisations for a semantic input. Please refer to
the references given for details of each system.
The BAGEL and SPaRKy realisers were compared
based on published ranked output lists.9
</bodyText>
<listItem confidence="0.974521333333333">
• BAGEL is a surface realiser based on dy-
namic Bayes Nets originally trained using
Active Learning by Mairesse et al. (2010).
It was shown to generate well-phrased utter-
ances from unseen semantic inputs.
• CRF (global) treats surface realisation as a
</listItem>
<footnote confidence="0.941580666666667">
9Available from http://people.csail.mit.
edu/francois/research/bagel and http://
users.soe.ucsc.edu/˜maw/downloads.html.
</footnote>
<figure confidence="0.849688166666667">
1 2 3 4 5 6 7 8 9 10 15 20 30
Correlation Coefficient
0.3 0.4 0.5 0.6
90 Clusters
Ratings
Average
</figure>
<page confidence="0.848107">
708
</page>
<subsectionHeader confidence="0.774012">
System Utterance
</subsectionHeader>
<bodyText confidence="0.442315">
BAGEL Beluga is a moderately priced
restaurant in the city centre area.
Col = 4.0, Pol = 4.0, Nat = 4.0
</bodyText>
<footnote confidence="0.625556416666667">
CRF (global) Set in the city centre, Beluga is a
moderately priced location for the
celebration of the Italian spirit.
Col = 2.0, Pol = 5.0, Nat = 2.0
pCRU Beluga is located in the city centre
and serves cheap Italian food.
Col = 4.0, Pol = 3.0, Nat = 5.0
SPaRKy Beluga has the best overall quality
among the selected restaurants
since this Italian restaurant has
good decor, with good service.
Col = 3.0, Pol = 4.0, Nat = 5.0
</footnote>
<tableCaption confidence="0.991453">
Table 5: Example utterances for the BAGEL, CRF
(global), pCRU and SPaRKy realisers shown to users.
Sample ratings from individual users are also shown.
</tableCaption>
<bodyText confidence="0.845958">
sequence labelling task: given a set of (ob-
served) linguistic features, it aims to find the
best (hidden) sequence of phrases realising a
semantic input (Dethlefs et al., 2013).
</bodyText>
<listItem confidence="0.998106777777778">
• pCRU is based on probabilistic context-
free grammars and generation is done using
Viterbi search, sampling (used here), or ran-
dom search. It is based on Belz (2008).
• SPaRKy is based on a rank-and-boost ap-
proach. It learns a mapping between the lin-
guistic features of a target utterance and its
predicted user ratings and ranks candidates
accordingly (Walker et al., 2007).
</listItem>
<subsectionHeader confidence="0.993272">
6.2 Results: Recognising Stylistic Variation
</subsectionHeader>
<bodyText confidence="0.9983205">
242 users from the USA took part in a rating study
on the CrowdFlower platform and rated altogether
1, 702 utterances, from among the highest-ranked
surface realisations above. For each utterance
they read, they rated the colloquialism, natura-
less and politeness based on the same questions
as in Section 4.1, used to obtain the training data.
Based on this, we compare the perceived strength
of each stylistic dimension in an utterance to the
one predicted by the regressor. Example utter-
ances and ratings are shown in Table 5. Results
are shown in Table 6 and confirm our observa-
tions: ratings for known users can be estimated
with a medium (or high) correlation based on
clusters of users who assign similar ratings to ut-
terances with similar linguistic features. We can
also see that such estimations do not depend on a
particular data set or realiser.
</bodyText>
<table confidence="0.9994248">
System Colloquial Polite Natural
BAGEL 0.78 0.66 0.69
CRF global 0.58 0.63 0.63
pCRU 0.67 0.42 0.77
SPaRKy 0.87 0.56 0.81
</table>
<tableCaption confidence="0.993827666666667">
Table 6: Correlation coefficients between subjective
user ratings and ratings predicted by the regressor for
known users across data-driven surface realisers.
</tableCaption>
<bodyText confidence="0.999967611111111">
A novel aspect of our technique in compari-
son to previous work on stylistic realisation is
that it does not depend on the time- and resource-
intensive design of a hand-coded generator, as in
Paiva and Evans (2005) and Mairesse and Walker
(2011). Instead, it can be applied in conjunc-
tion with any system designer’s favourite realiser
and preserves the realiser’s original features by
re-ranking only its top n (e.g. 10) output candi-
dates. Our method is therefore able to strike a
balance between highly-ranked and well-phrased
utterances and stylistic adaptation. A current lim-
itation of our model is that some ratings can still
not be predicted with a high correlation with hu-
man judgements. However, even the medium cor-
relations achieved have been shown to be signif-
icantly better than estimations based on the aver-
age population of users (Section 5.2).
</bodyText>
<sectionHeader confidence="0.987884" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999956142857143">
We have presented a model of stylistic realisation
that is able to adapt its output along several stylis-
tic dimensions. Results show that the variation is
recognisable by humans and that user ratings can
be predicted for known as well as unknown users.
A model which clusters individual users based
on their ratings of linguistically similar utterances
achieves significantly higher performance than a
model trained on the average population of rat-
ings. These results may also play a role in other
domains in which users display variability in their
subjective ratings, e.g. recommender systems,
sentiment analysis, or emotion generation. Future
work may explore the use of additional cluster-
ing features as a more scalable alternative to re-
ranking. It also needs to determine how user feed-
back can be obtained during an interaction, where
asking users for ratings may be disruptive. Possi-
bilities include to infer user ratings from their next
dialogue move, or from multimodal information
such as hesitations or eye-tracking.
</bodyText>
<page confidence="0.994668">
709
</page>
<bodyText confidence="0.99979">
Acknowledgements This research was funded
by the EC FP7 programme FP7/2011-14 under
grant agreements no. 270019 (SPACEBOOK)
and no. 287615 (PARLANCE).
</bodyText>
<sectionHeader confidence="0.990309" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999906102803738">
Xavier Amatriain, Josep M. Pujol, and Nuria Oliver.
2009. I like It... I Like It Not: Evaluating User Rat-
ings Noise in Recommender Systems. In In the 17th
International Conference on User Modelling, Adap-
tation, and Personalisation (UMAP), pages 247–
258, Trento, Italy. Springer-Verlag.
Anja Belz. 2008. Automatic Generation of Weather
Forecast Texts Using Comprehensive Probabilistic
Generation-Space Models. Natural Language En-
gineering, 14(4):431–455.
Penelope Brown and Stephen Levinson. 1987. Some
Universals in Language Usage. Cambridge Univer-
sity Press, Cambridge, UK.
Heriberto Cuay´ahuitl and Nina Dethlefs. 2011. Op-
timizing Situated Dialogue Management in Un-
known Environments. In INTERSPEECH, pages
1009–1012.
Robert Dale and Jette Viethen. 2009. Referring
Expression Generation Through Attribute-Based
Heuristics. In Proceedings of the 12th Euro-
pean Workshop on Natural Language Generation
(ENLG), Athens, Greece.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the Interna-
tional Conference on Natural Language Generation
(INLG), Chicago, Illinois, USA.
Nina Dethlefs, Helen Hastie, Heriberto Cuay´ahuitl,
and Oliver Lemon. 2013. Conditional Random
Fields for Responsive Surface Realisation Using
Global Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Sofia, Bulgaria.
Michael Fleischman and Eduard Hovy. 2002. Emo-
tional Variation in Speech-Based Natural Language
Generation. In Proceedings of the 2nd International
Natural Language Generation Conference.
Milica Gaˇsi´c, Filip Jurˇciˇcek, Blaise Thomson, Kai Yu,
and Steve Young. 2011. On-Line Policy Optimi-
sation of Spoken Dialogue Systems via Interaction
with Human Subjects. In Proceedings of the IEEE
Automatic Speech Recognition and Understanding
(ASRU) Workshop.
Alastair Gill and Jon Oberlander. 2002. Taking Care
of the Linguistic Features of Extraversion. In Pro-
ceedings of the 24th Annual Conference of the Cog-
nitive Science Society, pages 363–368, Fairfax, VA.
Samuel Gosling, Simine Vazire, Sanjay Srivastava,
and Oliver John. 2004. Should We Trust Web-
Based Studies? A Comparative Analysis of Six Pre-
conceptions About Internet Questionnaires. Ameri-
can Psychologist, 59(2):93–104.
Swati Gupta, Marilyn Walker, and Daniela Romano.
2007. How Rude Are You? Evaluating Politeness
and Affect in Interaction. In Proceedings of the
2nd International Conference on Affective Comput-
ing and Intelligent Interaction.
Helen Hastie, Marie-Aude Aufaure, Panos Alex-
opoulos, Heriberto Cuayhuitl, Nina Dethlefs,
James Henderson Milica Gasic, Oliver Lemon,
Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis,
Yves Vanrompay, Boris Villazon-Terrazas, and
Steve Young. 2013. Demonstration of the PAR-
LANCE System: A Data-Driven, Incremental, Spo-
ken Dialogue System for Interactive Search. In Pro-
ceedings of the 14th Annual Meeting of the Special
Interest Group on Discourse and Dialogue (SIG-
dial).
Eduard Hovy. 1988. Generating Natural Language
under Pragmatic Constraints. Lawrence Erlbaum
Associates, Hillsdale, NJ.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and Alignment in Generated
Dialogues. In Proceedings of the 4th International
Natural Language Generation Conference (INLG),
Sydney, Australia.
Srini Janarthanam and Oliver Lemon. 2014. Adaptive
generation in dialogue systems using dynamic user
modeling. Computational Linguistics. (in press).
Pamela Jordan and Marilyn Walker. 2005. Learning
Content Selection Rules for Generating Object De-
scriptions in Dialogue. Journal ofArtificial Intelli-
gence Research, 24:157–194.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, and Johanna Moore.
2010. The First Challenge on Generating Instruc-
tions in Virtual Environments. In M. Theune and
E. Krahmer, editors, Empirical Methods in Natu-
ral Language Generation, pages 337–361. Springer
Verlag, Berlin/Heidelberg.
Franc¸ois Mairesse and Marilyn Walker. 2011. Con-
trolling User Perceptions of Linguistic Style: Train-
able Generation of Personality Traits. Computa-
tional Linguistics, 37(3):455–488, September.
Franc¸ois Mairesse, Milica Gaˇsi´c, Filip Jurˇciˇcek, Si-
mon Keizer, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Phrase-based statistical language
generation using graphical models and active learn-
ing. In Proceedings of the Annual Meeting of the
</reference>
<page confidence="0.962156">
710
</page>
<reference confidence="0.999449170212766">
Association for Computational Linguistics (ACL),
pages 1552–1561.
Andrew Ng, Michael Jordan, and Yair Weiss. 2001.
On Spectral Clustering: Analysis and an Algorithm.
In Advances in Neural Information Processing Sys-
tems, pages 849–856. MIT Press.
Michael O’Mahony, Neil Hurley, and Gu´enol´e Sil-
vestre. 2006. Detecting Noise in Recommender
System Databases. In Proceedings of the Inter-
national Conference on Intelligent User Interfaces
(IUI)s. ACM Press.
Daniel Paiva and Roger Evans. 2005. Empirically-
Based Control of Natural Language Generation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL),
Ann Arbor, Michigan, USA.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploit-
ing Class Relationships for Sentiment Categoriza-
tion with Respect to Rating Scales. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL).
Kaska Porayska-Pomsta and Chris Mellish. 2004.
Modelling Politness in Natural Language Gener-
ation. In Proceedings of the 3rd International
Natural Language Generation Conference (INLG),
Brighton, UK.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual Sig-
Dial Meeting on Discourse and Dialogue, Tokyo,
Japan.
Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing, 17(4).
Marilyn Walker, Amanda Stent, Franc¸ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413–456.
Ching-long Yeh and Chris Mellish. 1997. An Empir-
ical Study on the Generation of Anaphora in Chi-
nese. Computational Linguistics, 23:169–190.
</reference>
<page confidence="0.997956">
711
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983151">
<title confidence="0.998811">Cluster-based Prediction of User Ratings for Stylistic Surface Realisation</title>
<author confidence="0.997065">Nina Dethlefs</author>
<author confidence="0.997065">Heriberto Cuay´ahuitl</author>
<author confidence="0.997065">Helen Hastie</author>
<author confidence="0.997065">Verena Rieser</author>
<author confidence="0.997065">Oliver</author>
<affiliation confidence="0.998335">Heriot-Watt University, Mathematical and Computer Sciences,</affiliation>
<email confidence="0.994702">n.s.dethlefs@hw.ac.uk</email>
<abstract confidence="0.999739">Surface realisations typically depend on their target style and audience. A challenge in estimating a stylistic realiser from data is that humans vary significantly in their subjective perceptions of linguistic forms and styles, leading to almost no correlation between ratings of the same utterance. We address this problem in two steps. First, we estimate a mapping function between the linguistic features of a corpus of utterances and their human style ratings. Users are partitioned into clusters based on the similarity of their ratings, so that ratings for new utterances can be estimated, even for In a second step, the estimated model is used to re-rank the outputs of a number of surface realisers to produce stylistically adaptive output. Results confirm that the generated styles are recognisable to human judges and that predictive models based on clusters of users lead to better rating predictions than models based on an average population of users.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Amatriain</author>
<author>Josep M Pujol</author>
<author>Nuria Oliver</author>
</authors>
<title>I like It... I Like It Not: Evaluating User Ratings Noise in Recommender Systems.</title>
<date>2009</date>
<booktitle>In In the 17th International Conference on User Modelling, Adaptation, and Personalisation (UMAP),</booktitle>
<pages>247--258</pages>
<publisher>Springer-Verlag.</publisher>
<location>Trento, Italy.</location>
<contexts>
<context position="2944" citStr="Amatriain et al., 2009" startWordPosition="459" endWordPosition="462">s on the one hand based on natural human data so as to reflect stylistic variation that is as natural as possible. On the other hand, we aim to minimise the amount of annotation and human engineering that informs the design of the system. To this end, we estimate a mapping function between automatically identifiable shallow linguistic features characteristic of an utterance and its human-assigned style ratings. In addition, we aim to address the high degree of variability that is often encountered in subjective rating studies, such as assessments of recommender systems (O’Mahony et al., 2006; Amatriain et al., 2009), sentiment analysis (Pang and Lee, 2005), or surface realisations, where user ratings have been shown to differ significantly (p&lt;0.001) for the same utterance (Walker et al., 2007). Such high variability can affect the performance of systems which are trained from an average population of user ratings. However, we are not aware of any work that has addressed this problem principally by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign simil</context>
<context position="18124" citStr="Amatriain et al. (2009)" startWordPosition="2878" endWordPosition="2881">2 M5P 0.71 0.50 0.73 ZeroR -0.04 0.002 1.04 Table 4: Comparison of regression models per dimension using individual user ratings. The best model is indicated in bold-face for the correlation coefficient. lation of 0.79 to a moderate negative correlation of −0.55. The average is 0.04 (SD=0.95), i.e. indicating no correlation between user ratings, even for the same utterance. This observation is partially in line with related work that has found high diversity in subjective user ratings. Yeh and Mellish (1997) report only 70% agreement of human judges on the best choice of referring expression. Amatriain et al. (2009) report inconsistencies in user ratings in recommender systems with an RMSE range of 0.55 to 0.81 and argue that this constitutes a lower bound for system performance. This inconsistency is exacerbated by raters recruited via crowdsourcing platforms as in our study (Koller et al., 2010; Rieser et al., 2011). However, while crowdsourced data have been shown to contain substantially more noise than data collected in a lab environment, they do tend to reflect the general tendency of their more controlled counterparts (Gosling et al., 2004). Individual User Ratings Given that individual preference</context>
</contexts>
<marker>Amatriain, Pujol, Oliver, 2009</marker>
<rawString>Xavier Amatriain, Josep M. Pujol, and Nuria Oliver. 2009. I like It... I Like It Not: Evaluating User Ratings Noise in Recommender Systems. In In the 17th International Conference on User Modelling, Adaptation, and Personalisation (UMAP), pages 247– 258, Trento, Italy. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Automatic Generation of Weather Forecast Texts Using Comprehensive Probabilistic Generation-Space Models.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="29181" citStr="Belz (2008)" startWordPosition="4724" endWordPosition="4725">lected restaurants since this Italian restaurant has good decor, with good service. Col = 3.0, Pol = 4.0, Nat = 5.0 Table 5: Example utterances for the BAGEL, CRF (global), pCRU and SPaRKy realisers shown to users. Sample ratings from individual users are also shown. sequence labelling task: given a set of (observed) linguistic features, it aims to find the best (hidden) sequence of phrases realising a semantic input (Dethlefs et al., 2013). • pCRU is based on probabilistic contextfree grammars and generation is done using Viterbi search, sampling (used here), or random search. It is based on Belz (2008). • SPaRKy is based on a rank-and-boost approach. It learns a mapping between the linguistic features of a target utterance and its predicted user ratings and ranks candidates accordingly (Walker et al., 2007). 6.2 Results: Recognising Stylistic Variation 242 users from the USA took part in a rating study on the CrowdFlower platform and rated altogether 1, 702 utterances, from among the highest-ranked surface realisations above. For each utterance they read, they rated the colloquialism, naturaless and politeness based on the same questions as in Section 4.1, used to obtain the training data. </context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>Anja Belz. 2008. Automatic Generation of Weather Forecast Texts Using Comprehensive Probabilistic Generation-Space Models. Natural Language Engineering, 14(4):431–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Brown</author>
<author>Stephen Levinson</author>
</authors>
<title>Some Universals in Language Usage.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Brown, Levinson, 1987</marker>
<rawString>Penelope Brown and Stephen Levinson. 1987. Some Universals in Language Usage. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heriberto Cuay´ahuitl</author>
<author>Nina Dethlefs</author>
</authors>
<title>Optimizing Situated Dialogue Management in Unknown Environments.</title>
<date>2011</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1009--1012</pages>
<marker>Cuay´ahuitl, Dethlefs, 2011</marker>
<rawString>Heriberto Cuay´ahuitl and Nina Dethlefs. 2011. Optimizing Situated Dialogue Management in Unknown Environments. In INTERSPEECH, pages 1009–1012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Jette Viethen</author>
</authors>
<title>Referring Expression Generation Through Attribute-Based Heuristics.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="8792" citStr="Dale and Viethen, 2009" startWordPosition="1378" endWordPosition="1381">he first to evaluate their generator with humans and show that the generated personalities are indeed recognisable. Approaches on replicating personalities in realisations include Gill and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) were the first to show that individual preferences exist for the perceived quality of realisations and that these can be modelled in trainable generation. They train two versions of a rank-and-boost generator, a first version of which is trained on the average population of user ratings, whereas a second one is trained on the ratings of individual users. The authors show statistically that ratings from different u</context>
</contexts>
<marker>Dale, Viethen, 2009</marker>
<rawString>Robert Dale and Jette Viethen. 2009. Referring Expression Generation Through Attribute-Based Heuristics. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Helen Hastie</author>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-CoNLL),</booktitle>
<location>Jeju, South</location>
<contexts>
<context position="25854" citStr="Dethlefs et al., 2012" startWordPosition="4167" endWordPosition="4171">ings (c) Actual Ratings Correlation: Colloquialism 1 2 3 4 5 2 3 4 5 Predicted Ratings Correlation: Naturalness 1 2 3 4 5 Correlation: Politeness 1 2 3 4 5 1 2 3 4 5 Predicted Ratings 1 2 3 4 5 Predicted Ratings erage population of users (p&lt;0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. 6 Evaluation: Stylistically-Aware Surface Realisation To evaluate the applicability of our regression model for stylistically-adaptive surface realisation, this section describes work that compares four different surface realisers, which were not originally developed to produce stylistic variation. To do that, we first obtain the cluster for each input sentence s: c* = argminc∈C &amp;x D(Psx |Qxc</context>
</contexts>
<marker>Dethlefs, Hastie, Rieser, Lemon, 2012</marker>
<rawString>Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver Lemon. 2012a. Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-CoNLL), Jeju, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Helen Hastie</author>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Optimising Incremental Generation for Spoken Dialogue Systems: Reducing the Need for Fillers.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Natural Language Generation (INLG),</booktitle>
<location>Chicago, Illinois, USA.</location>
<contexts>
<context position="25854" citStr="Dethlefs et al., 2012" startWordPosition="4167" endWordPosition="4171">ings (c) Actual Ratings Correlation: Colloquialism 1 2 3 4 5 2 3 4 5 Predicted Ratings Correlation: Naturalness 1 2 3 4 5 Correlation: Politeness 1 2 3 4 5 1 2 3 4 5 Predicted Ratings 1 2 3 4 5 Predicted Ratings erage population of users (p&lt;0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. 6 Evaluation: Stylistically-Aware Surface Realisation To evaluate the applicability of our regression model for stylistically-adaptive surface realisation, this section describes work that compares four different surface realisers, which were not originally developed to produce stylistic variation. To do that, we first obtain the cluster for each input sentence s: c* = argminc∈C &amp;x D(Psx |Qxc</context>
</contexts>
<marker>Dethlefs, Hastie, Rieser, Lemon, 2012</marker>
<rawString>Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver Lemon. 2012b. Optimising Incremental Generation for Spoken Dialogue Systems: Reducing the Need for Fillers. In Proceedings of the International Conference on Natural Language Generation (INLG), Chicago, Illinois, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Dethlefs</author>
<author>Helen Hastie</author>
<author>Heriberto Cuay´ahuitl</author>
<author>Oliver Lemon</author>
</authors>
<title>Conditional Random Fields for Responsive Surface Realisation Using Global Features.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>Dethlefs, Hastie, Cuay´ahuitl, Lemon, 2013</marker>
<rawString>Nina Dethlefs, Helen Hastie, Heriberto Cuay´ahuitl, and Oliver Lemon. 2013. Conditional Random Fields for Responsive Surface Realisation Using Global Features. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>Emotional Variation in Speech-Based Natural Language Generation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Natural Language Generation Conference.</booktitle>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>Michael Fleischman and Eduard Hovy. 2002. Emotional Variation in Speech-Based Natural Language Generation. In Proceedings of the 2nd International Natural Language Generation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milica Gaˇsi´c</author>
<author>Filip Jurˇciˇcek</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>On-Line Policy Optimisation of Spoken Dialogue Systems via Interaction with Human Subjects.</title>
<date>2011</date>
<booktitle>In Proceedings of the IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop.</booktitle>
<marker>Gaˇsi´c, Jurˇciˇcek, Thomson, Yu, Young, 2011</marker>
<rawString>Milica Gaˇsi´c, Filip Jurˇciˇcek, Blaise Thomson, Kai Yu, and Steve Young. 2011. On-Line Policy Optimisation of Spoken Dialogue Systems via Interaction with Human Subjects. In Proceedings of the IEEE Automatic Speech Recognition and Understanding (ASRU) Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alastair Gill</author>
<author>Jon Oberlander</author>
</authors>
<title>Taking Care of the Linguistic Features of Extraversion.</title>
<date>2002</date>
<booktitle>In Proceedings of the 24th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>363--368</pages>
<location>Fairfax, VA.</location>
<contexts>
<context position="8375" citStr="Gill and Oberlander (2002)" startWordPosition="1318" endWordPosition="1321">erator decisions based on target personality scores. To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of several classifiers and regressors. Mairesse and Walker (2011) are the first to evaluate their generator with humans and show that the generated personalities are indeed recognisable. Approaches on replicating personalities in realisations include Gill and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) </context>
</contexts>
<marker>Gill, Oberlander, 2002</marker>
<rawString>Alastair Gill and Jon Oberlander. 2002. Taking Care of the Linguistic Features of Extraversion. In Proceedings of the 24th Annual Conference of the Cognitive Science Society, pages 363–368, Fairfax, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Gosling</author>
<author>Simine Vazire</author>
<author>Sanjay Srivastava</author>
<author>Oliver John</author>
</authors>
<title>Should We Trust WebBased Studies? A Comparative Analysis of Six Preconceptions About Internet Questionnaires.</title>
<date>2004</date>
<journal>American Psychologist,</journal>
<volume>59</volume>
<issue>2</issue>
<contexts>
<context position="18666" citStr="Gosling et al., 2004" startWordPosition="2965" endWordPosition="2968">uman judges on the best choice of referring expression. Amatriain et al. (2009) report inconsistencies in user ratings in recommender systems with an RMSE range of 0.55 to 0.81 and argue that this constitutes a lower bound for system performance. This inconsistency is exacerbated by raters recruited via crowdsourcing platforms as in our study (Koller et al., 2010; Rieser et al., 2011). However, while crowdsourced data have been shown to contain substantially more noise than data collected in a lab environment, they do tend to reflect the general tendency of their more controlled counterparts (Gosling et al., 2004). Individual User Ratings Given that individual preferences exist for surface realisation (Walker et al., 2007), we included the user’s ID as a regression feature and re-ran the experiments. The hypothesis was that if users differ in their preferences for realisation candidates, they may also differ in terms of their perceptions of linguistic styles. The results shown in Table 4 support this: the obtained correlations are significantly higher (p&lt;0.001, using the Fisher r-to-z transformation) than those without the user’s ID (though we are still not able to model the full variation observed in </context>
</contexts>
<marker>Gosling, Vazire, Srivastava, John, 2004</marker>
<rawString>Samuel Gosling, Simine Vazire, Sanjay Srivastava, and Oliver John. 2004. Should We Trust WebBased Studies? A Comparative Analysis of Six Preconceptions About Internet Questionnaires. American Psychologist, 59(2):93–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swati Gupta</author>
<author>Marilyn Walker</author>
<author>Daniela Romano</author>
</authors>
<title>How Rude Are You? Evaluating Politeness and Affect in Interaction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2nd International Conference on Affective Computing and Intelligent Interaction.</booktitle>
<contexts>
<context position="8459" citStr="Gupta et al. (2007)" startWordPosition="1332" endWordPosition="1335">irst generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of several classifiers and regressors. Mairesse and Walker (2011) are the first to evaluate their generator with humans and show that the generated personalities are indeed recognisable. Approaches on replicating personalities in realisations include Gill and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) were the first to show that individual preferences exist for the perceived quality o</context>
</contexts>
<marker>Gupta, Walker, Romano, 2007</marker>
<rawString>Swati Gupta, Marilyn Walker, and Daniela Romano. 2007. How Rude Are You? Evaluating Politeness and Affect in Interaction. In Proceedings of the 2nd International Conference on Affective Computing and Intelligent Interaction.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Helen Hastie</author>
</authors>
<title>Marie-Aude Aufaure, Panos Alexopoulos, Heriberto Cuayhuitl, Nina Dethlefs, James Henderson Milica Gasic, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, and Steve Young.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGdial).</booktitle>
<marker>Hastie, 2013</marker>
<rawString>Helen Hastie, Marie-Aude Aufaure, Panos Alexopoulos, Heriberto Cuayhuitl, Nina Dethlefs, James Henderson Milica Gasic, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, and Steve Young. 2013. Demonstration of the PARLANCE System: A Data-Driven, Incremental, Spoken Dialogue System for Interactive Search. In Proceedings of the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGdial).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
</authors>
<title>Generating Natural Language under Pragmatic Constraints. Lawrence Erlbaum Associates,</title>
<date>1988</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="1610" citStr="Hovy (1988)" startWordPosition="250" endWordPosition="251">tylistically adaptive output. Results confirm that the generated styles are recognisable to human judges and that predictive models based on clusters of users lead to better rating predictions than models based on an average population of users. 1 Introduction Stylistic surface realisation aims not only to find the best realisation candidate for a semantic input based on some underlying trained model, but also aims to adapt its output to properties of the user, such as their age, social group, or location, among others. One of the first systems to address stylistic variation in generation was Hovy (1988)’s PAULINE, which generated texts that reflect different speaker attitudes towards events based on multiple, adjustable features. Stylistic variation in such contexts can often be modelled systematically as a multidimensional variation space with several continuous dimensions, so that varying stylistic scores indicate the strength of each dimension in a realisation candidate. Here, we focus on the dimensions of colloquialism, politeness and naturalness. Assuming a target score on one or more dimensions, candidate outputs of a datadriven realiser can then be ranked according to their predicted </context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Eduard Hovy. 1988. Generating Natural Language under Pragmatic Constraints. Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy Isard</author>
<author>Carsten Brockmann</author>
<author>Jon Oberlander</author>
</authors>
<title>Individuality and Alignment in Generated Dialogues.</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th International Natural Language Generation Conference (INLG),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="8399" citStr="Isard et al. (2006)" startWordPosition="1323" endWordPosition="1326">t personality scores. To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of several classifiers and regressors. Mairesse and Walker (2011) are the first to evaluate their generator with humans and show that the generated personalities are indeed recognisable. Approaches on replicating personalities in realisations include Gill and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) were the first to show t</context>
</contexts>
<marker>Isard, Brockmann, Oberlander, 2006</marker>
<rawString>Amy Isard, Carsten Brockmann, and Jon Oberlander. 2006. Individuality and Alignment in Generated Dialogues. In Proceedings of the 4th International Natural Language Generation Conference (INLG), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>Adaptive generation in dialogue systems using dynamic user modeling. Computational Linguistics.</title>
<date>2014</date>
<note>(in press).</note>
<contexts>
<context position="3956" citStr="Janarthanam and Lemon (2014)" startWordPosition="614" endWordPosition="617">ly by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign similar ratings to linguistically similar utterances, so that their ratings can be estimated more accurately than 702 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702–711, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics based on an average population of users. This is similar to Janarthanam and Lemon (2014), who show that clustering users and adapting to their level of domain expertise can significantly improve task success and user ratings. Our resulting model is evaluated with realisers not originally built to deal with stylistic variation, and produces natural variation recognisable by humans. 2 Architecture and Domain We aim to with generating restaurant recommendations as part of an interactive system. To do this, we assume that a generator input is provided by a preceding module, e.g. the interaction manager, and that the task of the surface realiser is to find a suitable stylistically app</context>
</contexts>
<marker>Janarthanam, Lemon, 2014</marker>
<rawString>Srini Janarthanam and Oliver Lemon. 2014. Adaptive generation in dialogue systems using dynamic user modeling. Computational Linguistics. (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Jordan</author>
<author>Marilyn Walker</author>
</authors>
<title>Learning Content Selection Rules for Generating Object Descriptions in Dialogue.</title>
<date>2005</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>24--157</pages>
<contexts>
<context position="8767" citStr="Jordan and Walker, 2005" startWordPosition="1374" endWordPosition="1377">e and Walker (2011) are the first to evaluate their generator with humans and show that the generated personalities are indeed recognisable. Approaches on replicating personalities in realisations include Gill and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) were the first to show that individual preferences exist for the perceived quality of realisations and that these can be modelled in trainable generation. They train two versions of a rank-and-boost generator, a first version of which is trained on the average population of user ratings, whereas a second one is trained on the ratings of individual users. The authors show statistically that</context>
</contexts>
<marker>Jordan, Walker, 2005</marker>
<rawString>Pamela Jordan and Marilyn Walker. 2005. Learning Content Selection Rules for Generating Object Descriptions in Dialogue. Journal ofArtificial Intelligence Research, 24:157–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
<author>Donna Byron</author>
<author>Justine Cassell</author>
<author>Robert Dale</author>
<author>Johanna Moore</author>
</authors>
<title>The First Challenge on Generating Instructions in Virtual Environments.</title>
<date>2010</date>
<booktitle>Empirical Methods in Natural Language Generation,</booktitle>
<pages>337--361</pages>
<editor>In M. Theune and E. Krahmer, editors,</editor>
<publisher>Springer Verlag, Berlin/Heidelberg.</publisher>
<contexts>
<context position="18410" citStr="Koller et al., 2010" startWordPosition="2925" endWordPosition="2928">95), i.e. indicating no correlation between user ratings, even for the same utterance. This observation is partially in line with related work that has found high diversity in subjective user ratings. Yeh and Mellish (1997) report only 70% agreement of human judges on the best choice of referring expression. Amatriain et al. (2009) report inconsistencies in user ratings in recommender systems with an RMSE range of 0.55 to 0.81 and argue that this constitutes a lower bound for system performance. This inconsistency is exacerbated by raters recruited via crowdsourcing platforms as in our study (Koller et al., 2010; Rieser et al., 2011). However, while crowdsourced data have been shown to contain substantially more noise than data collected in a lab environment, they do tend to reflect the general tendency of their more controlled counterparts (Gosling et al., 2004). Individual User Ratings Given that individual preferences exist for surface realisation (Walker et al., 2007), we included the user’s ID as a regression feature and re-ran the experiments. The hypothesis was that if users differ in their preferences for realisation candidates, they may also differ in terms of their perceptions of linguistic</context>
</contexts>
<marker>Koller, Striegnitz, Byron, Cassell, Dale, Moore, 2010</marker>
<rawString>Alexander Koller, Kristina Striegnitz, Donna Byron, Justine Cassell, Robert Dale, and Johanna Moore. 2010. The First Challenge on Generating Instructions in Virtual Environments. In M. Theune and E. Krahmer, editors, Empirical Methods in Natural Language Generation, pages 337–361. Springer Verlag, Berlin/Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Mairesse</author>
<author>Marilyn Walker</author>
</authors>
<title>Controlling User Perceptions of Linguistic Style: Trainable Generation of Personality Traits.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="7504" citStr="Mairesse and Walker (2011)" startWordPosition="1190" endWordPosition="1193">es, keeping traces of each generator decision, and obtaining style scores for each output based on the estimated factor model. The result is a dataset of &lt;generator decision, style score&gt; pairs which can be used in a correlation analysis to identify the predictors of particular output styles. During generation, the correlation equations inform the generator at each choice point so as to best express the desired style. Unfortunately, no human evaluation of the model is presented so that it remains unclear to what extent the generated styles are perceivable by humans. Closely related is work by Mairesse and Walker (2011) who present the PERSONAGE system, which aims to generate language reflecting particular personalities. Instead of choosing generator decisions by considering their predicted style scores, however, Mairesse and Walker (2011) directly predict generator decisions based on target personality scores. To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of sev</context>
<context position="30818" citStr="Mairesse and Walker (2011)" startWordPosition="4992" endWordPosition="4995">c features. We can also see that such estimations do not depend on a particular data set or realiser. System Colloquial Polite Natural BAGEL 0.78 0.66 0.69 CRF global 0.58 0.63 0.63 pCRU 0.67 0.42 0.77 SPaRKy 0.87 0.56 0.81 Table 6: Correlation coefficients between subjective user ratings and ratings predicted by the regressor for known users across data-driven surface realisers. A novel aspect of our technique in comparison to previous work on stylistic realisation is that it does not depend on the time- and resourceintensive design of a hand-coded generator, as in Paiva and Evans (2005) and Mairesse and Walker (2011). Instead, it can be applied in conjunction with any system designer’s favourite realiser and preserves the realiser’s original features by re-ranking only its top n (e.g. 10) output candidates. Our method is therefore able to strike a balance between highly-ranked and well-phrased utterances and stylistic adaptation. A current limitation of our model is that some ratings can still not be predicted with a high correlation with human judgements. However, even the medium correlations achieved have been shown to be significantly better than estimations based on the average population of users (Se</context>
</contexts>
<marker>Mairesse, Walker, 2011</marker>
<rawString>Franc¸ois Mairesse and Marilyn Walker. 2011. Controlling User Perceptions of Linguistic Style: Trainable Generation of Personality Traits. Computational Linguistics, 37(3):455–488, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Mairesse</author>
<author>Milica Gaˇsi´c</author>
<author>Filip Jurˇciˇcek</author>
<author>Simon Keizer</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Phrase-based statistical language generation using graphical models and active learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1552--1561</pages>
<marker>Mairesse, Gaˇsi´c, Jurˇciˇcek, Keizer, Thomson, Yu, Young, 2010</marker>
<rawString>Franc¸ois Mairesse, Milica Gaˇsi´c, Filip Jurˇciˇcek, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. 2010. Phrase-based statistical language generation using graphical models and active learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1552–1561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
<author>Yair Weiss</author>
</authors>
<title>On Spectral Clustering: Analysis and an Algorithm.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>849--856</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="21237" citStr="Ng et al. (2001)" startWordPosition="3399" endWordPosition="3402"> cumulative similarity based on the Kullback-Leibler divergence, pilo92(pi qi ) + j , 2 where P is a distribution of words, POS tags or ratings in data point xi; and Q a similar distribution in data point xj. The lower the cumulative diD(P, Q) = � i qjlo92(qjpj ) 706 Number of Clusters Figure 2: Average correlation coefficient for different numbers of clusters. For comparison, results from average and individual user ratings are also shown. vergence between two data sets, the more similar they are. To find clusters of similar users from the affinity matrix 5, we use the algorithm described in Ng et al. (2001). It derives clusters by choosing the k largest eigenvectors u1, u2, ... , uk from the Laplacian matrix L = D1/2 −5D1/2 (where D is a diagonal matrix), arranging them into columns in a matrix U = [u1u2 ... uk] and then normalising them for length. The result is a new matrix T, obtained through tij = uij/(Ek u2ik)1/2. The set of clusters C1,... Ck can then be obtained from T using the K-means algorithm, where each row in T serves as an individual data point. Finally, each original data point xi (row i of T) is assigned to a cluster Cj. In comparison to other clustering algorithms, experiments b</context>
</contexts>
<marker>Ng, Jordan, Weiss, 2001</marker>
<rawString>Andrew Ng, Michael Jordan, and Yair Weiss. 2001. On Spectral Clustering: Analysis and an Algorithm. In Advances in Neural Information Processing Systems, pages 849–856. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael O’Mahony</author>
<author>Neil Hurley</author>
<author>Gu´enol´e Silvestre</author>
</authors>
<title>Detecting Noise in Recommender System Databases.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Intelligent User Interfaces (IUI)s.</booktitle>
<publisher>ACM Press.</publisher>
<marker>O’Mahony, Hurley, Silvestre, 2006</marker>
<rawString>Michael O’Mahony, Neil Hurley, and Gu´enol´e Silvestre. 2006. Detecting Noise in Recommender System Databases. In Proceedings of the International Conference on Intelligent User Interfaces (IUI)s. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Paiva</author>
<author>Roger Evans</author>
</authors>
<title>EmpiricallyBased Control of Natural Language Generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="6071" citStr="Paiva and Evans (2005)" startWordPosition="961" endWordPosition="964">ormation inform the resulting stylistic regression model. For surface realisation (topright box, blue), a semantic input from a preceding model is given as input to a surface realiser. Any realiser is suitable that returns a ranked list of output candidates. The resulting list is re-ranked according to stylistic scores estimated by the regressor, so that the utterance which most closely reflects the target score is ranked highest. The reranking process is shown in the lower box (red). 3 Related Work 3.1 Stylistic Variation in Surface Realisation Our approach is most closely related to work by Paiva and Evans (2005) and Mairesse and Walker 1http://parlance-project.eu Figure 1: Architecture of stylistic realisation model. Top left: user clusters are estimated from corpus utterances described by linguistic features and ratings. Top right: surface realisation ranks a list of output candidates based on a semantic input. These are ranked stylistically given a trained regressor. (2011), discussed in turn here. Paiva and Evans (2005) present an approach that uses multivariate linear regression to map individual linguistic features to distinguishable styles of text. The approach works in three steps. First, a fa</context>
<context position="30787" citStr="Paiva and Evans (2005)" startWordPosition="4987" endWordPosition="4990">nces with similar linguistic features. We can also see that such estimations do not depend on a particular data set or realiser. System Colloquial Polite Natural BAGEL 0.78 0.66 0.69 CRF global 0.58 0.63 0.63 pCRU 0.67 0.42 0.77 SPaRKy 0.87 0.56 0.81 Table 6: Correlation coefficients between subjective user ratings and ratings predicted by the regressor for known users across data-driven surface realisers. A novel aspect of our technique in comparison to previous work on stylistic realisation is that it does not depend on the time- and resourceintensive design of a hand-coded generator, as in Paiva and Evans (2005) and Mairesse and Walker (2011). Instead, it can be applied in conjunction with any system designer’s favourite realiser and preserves the realiser’s original features by re-ranking only its top n (e.g. 10) output candidates. Our method is therefore able to strike a balance between highly-ranked and well-phrased utterances and stylistic adaptation. A current limitation of our model is that some ratings can still not be predicted with a high correlation with human judgements. However, even the medium correlations achieved have been shown to be significantly better than estimations based on the </context>
</contexts>
<marker>Paiva, Evans, 2005</marker>
<rawString>Daniel Paiva and Roger Evans. 2005. EmpiricallyBased Control of Natural Language Generation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2985" citStr="Pang and Lee, 2005" startWordPosition="465" endWordPosition="468"> so as to reflect stylistic variation that is as natural as possible. On the other hand, we aim to minimise the amount of annotation and human engineering that informs the design of the system. To this end, we estimate a mapping function between automatically identifiable shallow linguistic features characteristic of an utterance and its human-assigned style ratings. In addition, we aim to address the high degree of variability that is often encountered in subjective rating studies, such as assessments of recommender systems (O’Mahony et al., 2006; Amatriain et al., 2009), sentiment analysis (Pang and Lee, 2005), or surface realisations, where user ratings have been shown to differ significantly (p&lt;0.001) for the same utterance (Walker et al., 2007). Such high variability can affect the performance of systems which are trained from an average population of user ratings. However, we are not aware of any work that has addressed this problem principally by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign similar ratings to linguistically similar utte</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaska Porayska-Pomsta</author>
<author>Chris Mellish</author>
</authors>
<title>Modelling Politness in Natural Language Generation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd International Natural Language Generation Conference (INLG),</booktitle>
<location>Brighton, UK.</location>
<contexts>
<context position="8435" citStr="Porayska-Pomsta and Mellish (2004)" startWordPosition="1327" endWordPosition="1330"> To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of several classifiers and regressors. Mairesse and Walker (2011) are the first to evaluate their generator with humans and show that the generated personalities are indeed recognisable. Approaches on replicating personalities in realisations include Gill and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) were the first to show that individual preferences exist for</context>
</contexts>
<marker>Porayska-Pomsta, Mellish, 2004</marker>
<rawString>Kaska Porayska-Pomsta and Chris Mellish. 2004. Modelling Politness in Natural Language Generation. In Proceedings of the 3rd International Natural Language Generation Conference (INLG), Brighton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Simon Keizer</author>
<author>Xingkun Liu</author>
<author>Oliver Lemon</author>
</authors>
<title>Adaptive Information Presentation for Spoken Dialogue Systems: Evaluation with Human Subjects.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="18432" citStr="Rieser et al., 2011" startWordPosition="2929" endWordPosition="2932">no correlation between user ratings, even for the same utterance. This observation is partially in line with related work that has found high diversity in subjective user ratings. Yeh and Mellish (1997) report only 70% agreement of human judges on the best choice of referring expression. Amatriain et al. (2009) report inconsistencies in user ratings in recommender systems with an RMSE range of 0.55 to 0.81 and argue that this constitutes a lower bound for system performance. This inconsistency is exacerbated by raters recruited via crowdsourcing platforms as in our study (Koller et al., 2010; Rieser et al., 2011). However, while crowdsourced data have been shown to contain substantially more noise than data collected in a lab environment, they do tend to reflect the general tendency of their more controlled counterparts (Gosling et al., 2004). Individual User Ratings Given that individual preferences exist for surface realisation (Walker et al., 2007), we included the user’s ID as a regression feature and re-ran the experiments. The hypothesis was that if users differ in their preferences for realisation candidates, they may also differ in terms of their perceptions of linguistic styles. The results s</context>
</contexts>
<marker>Rieser, Keizer, Liu, Lemon, 2011</marker>
<rawString>Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver Lemon. 2011. Adaptive Information Presentation for Spoken Dialogue Systems: Evaluation with Human Subjects. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>Anna Hjalmarsson</author>
</authors>
<title>Towards Incremental Speech Generation in Dialogue Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual SigDial Meeting on Discourse and Dialogue,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="25831" citStr="Skantze and Hjalmarsson, 2010" startWordPosition="4163" endWordPosition="4166">) Actual Ratings (b) Actual Ratings (c) Actual Ratings Correlation: Colloquialism 1 2 3 4 5 2 3 4 5 Predicted Ratings Correlation: Naturalness 1 2 3 4 5 Correlation: Politeness 1 2 3 4 5 1 2 3 4 5 Predicted Ratings 1 2 3 4 5 Predicted Ratings erage population of users (p&lt;0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. 6 Evaluation: Stylistically-Aware Surface Realisation To evaluate the applicability of our regression model for stylistically-adaptive surface realisation, this section describes work that compares four different surface realisers, which were not originally developed to produce stylistic variation. To do that, we first obtain the cluster for each input sentence s: c* = </context>
</contexts>
<marker>Skantze, Hjalmarsson, 2010</marker>
<rawString>Gabriel Skantze and Anna Hjalmarsson. 2010. Towards Incremental Speech Generation in Dialogue Systems. In Proceedings of the 11th Annual SigDial Meeting on Discourse and Dialogue, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike von Luxburg</author>
</authors>
<title>A Tutorial on Spectral Clustering.</title>
<date>2007</date>
<journal>Statistics and Computing,</journal>
<volume>17</volume>
<issue>4</issue>
<marker>von Luxburg, 2007</marker>
<rawString>Ulrike von Luxburg. 2007. A Tutorial on Spectral Clustering. Statistics and Computing, 17(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Amanda Stent</author>
<author>Franc¸ois Mairesse</author>
<author>Rashmi Prasad</author>
</authors>
<title>Individual and Domain Adaptation in Sentence Planning for Dialogue.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="3125" citStr="Walker et al., 2007" startWordPosition="486" endWordPosition="489">man engineering that informs the design of the system. To this end, we estimate a mapping function between automatically identifiable shallow linguistic features characteristic of an utterance and its human-assigned style ratings. In addition, we aim to address the high degree of variability that is often encountered in subjective rating studies, such as assessments of recommender systems (O’Mahony et al., 2006; Amatriain et al., 2009), sentiment analysis (Pang and Lee, 2005), or surface realisations, where user ratings have been shown to differ significantly (p&lt;0.001) for the same utterance (Walker et al., 2007). Such high variability can affect the performance of systems which are trained from an average population of user ratings. However, we are not aware of any work that has addressed this problem principally by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign similar ratings to linguistically similar utterances, so that their ratings can be estimated more accurately than 702 Proceedings of the 14th Conference of the European Chapter of the As</context>
<context position="8974" citStr="Walker et al. (2007)" startWordPosition="1407" endWordPosition="1410">and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) were the first to show that individual preferences exist for the perceived quality of realisations and that these can be modelled in trainable generation. They train two versions of a rank-and-boost generator, a first version of which is trained on the average population of user ratings, whereas a second one is trained on the ratings of individual users. The authors show statistically that ratings from different users are drawn from different distributions (p&lt;0.001) and that significantly better performance is achieved when training and testing on data of individual users. In fact, training a</context>
<context position="18777" citStr="Walker et al., 2007" startWordPosition="2980" endWordPosition="2983">atings in recommender systems with an RMSE range of 0.55 to 0.81 and argue that this constitutes a lower bound for system performance. This inconsistency is exacerbated by raters recruited via crowdsourcing platforms as in our study (Koller et al., 2010; Rieser et al., 2011). However, while crowdsourced data have been shown to contain substantially more noise than data collected in a lab environment, they do tend to reflect the general tendency of their more controlled counterparts (Gosling et al., 2004). Individual User Ratings Given that individual preferences exist for surface realisation (Walker et al., 2007), we included the user’s ID as a regression feature and re-ran the experiments. The hypothesis was that if users differ in their preferences for realisation candidates, they may also differ in terms of their perceptions of linguistic styles. The results shown in Table 4 support this: the obtained correlations are significantly higher (p&lt;0.001, using the Fisher r-to-z transformation) than those without the user’s ID (though we are still not able to model the full variation observed in ratings). Importantly, this shows that user ratings are intrinsically coherent (not random) and that variation </context>
<context position="29390" citStr="Walker et al., 2007" startWordPosition="4757" endWordPosition="4760">hown to users. Sample ratings from individual users are also shown. sequence labelling task: given a set of (observed) linguistic features, it aims to find the best (hidden) sequence of phrases realising a semantic input (Dethlefs et al., 2013). • pCRU is based on probabilistic contextfree grammars and generation is done using Viterbi search, sampling (used here), or random search. It is based on Belz (2008). • SPaRKy is based on a rank-and-boost approach. It learns a mapping between the linguistic features of a target utterance and its predicted user ratings and ranks candidates accordingly (Walker et al., 2007). 6.2 Results: Recognising Stylistic Variation 242 users from the USA took part in a rating study on the CrowdFlower platform and rated altogether 1, 702 utterances, from among the highest-ranked surface realisations above. For each utterance they read, they rated the colloquialism, naturaless and politeness based on the same questions as in Section 4.1, used to obtain the training data. Based on this, we compare the perceived strength of each stylistic dimension in an utterance to the one predicted by the regressor. Example utterances and ratings are shown in Table 5. Results are shown in Tab</context>
</contexts>
<marker>Walker, Stent, Mairesse, Prasad, 2007</marker>
<rawString>Marilyn Walker, Amanda Stent, Franc¸ois Mairesse, and Rashmi Prasad. 2007. Individual and Domain Adaptation in Sentence Planning for Dialogue. Journal of Artificial Intelligence Research, 30(1):413–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-long Yeh</author>
<author>Chris Mellish</author>
</authors>
<date>1997</date>
<booktitle>An Empirical Study on the Generation of Anaphora in Chinese. Computational Linguistics,</booktitle>
<pages>23--169</pages>
<contexts>
<context position="18014" citStr="Yeh and Mellish (1997)" startWordPosition="2858" endWordPosition="2861"> 0.96 SVM 0.36 0.13 1.13 M5P 0.49 0.24 0.99 ZeroR -0.08 0.06 1.13 Polite MMR 0.69 0.48 0.76 SVM 0.54 0.30 0.92 M5P 0.71 0.50 0.73 ZeroR -0.04 0.002 1.04 Table 4: Comparison of regression models per dimension using individual user ratings. The best model is indicated in bold-face for the correlation coefficient. lation of 0.79 to a moderate negative correlation of −0.55. The average is 0.04 (SD=0.95), i.e. indicating no correlation between user ratings, even for the same utterance. This observation is partially in line with related work that has found high diversity in subjective user ratings. Yeh and Mellish (1997) report only 70% agreement of human judges on the best choice of referring expression. Amatriain et al. (2009) report inconsistencies in user ratings in recommender systems with an RMSE range of 0.55 to 0.81 and argue that this constitutes a lower bound for system performance. This inconsistency is exacerbated by raters recruited via crowdsourcing platforms as in our study (Koller et al., 2010; Rieser et al., 2011). However, while crowdsourced data have been shown to contain substantially more noise than data collected in a lab environment, they do tend to reflect the general tendency of their</context>
</contexts>
<marker>Yeh, Mellish, 1997</marker>
<rawString>Ching-long Yeh and Chris Mellish. 1997. An Empirical Study on the Generation of Anaphora in Chinese. Computational Linguistics, 23:169–190.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>