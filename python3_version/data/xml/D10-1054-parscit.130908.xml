<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.9446545">
Maximum Entropy Based Phrase Reordering
for Hierarchical Phrase-based Translation
</title>
<author confidence="0.913213">
Zhongjun He Yao Meng Hao Yu
</author>
<affiliation confidence="0.714931">
Fujitsu R&amp;D Center CO., LTD.
</affiliation>
<address confidence="0.758299">
15/F, Tower A, Ocean International Center, 56 Dongsihuan Zhong Rd.
Chaoyang District, Beijing, 100025, China
</address>
<email confidence="0.994932">
{hezhongjun, mengyao, yu}@cn.fujitsu.com
</email>
<sectionHeader confidence="0.996629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99974625">
Hierarchical phrase-based (HPB) translation
provides a powerful mechanism to capture
both short and long distance phrase reorder-
ings. However, the phrase reorderings lack of
contextual information in conventional HPB
systems. This paper proposes a context-
dependent phrase reordering approach that
uses the maximum entropy (MaxEnt) model
to help the HPB decoder select appropriate re-
ordering patterns. We classify translation rules
into several reordering patterns, and build a
MaxEnt model for each pattern based on var-
ious contextual features. We integrate the
MaxEnt models into the HPB model. Ex-
perimental results show that our approach
achieves significant improvements over a stan-
dard HPB system on large-scale translation
tasks. On Chinese-to-English translation,
the absolute improvements in BLEU (case-
insensitive) range from 1.2 to 2.1.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999715166666667">
The hierarchical phrase-based (HPB) model (Chi-
ang, 2005; Chiang, 2007) has been widely adopted
in statistical machine translation (SMT). It utilizes
synchronous context free grammar (SCFG) rules
to perform translation. Typically, there are three
types of rules (see Table 1): phrasal rule, a phrase
pair consisting of consecutive words; hierarchical
rule, a hierarchical phrase pair consisting of both
words and variables; and glue rule, which is used to
merge phrases serially. Phrasal rule captures short
distance reorderings within phrases, while hierar-
chical rule captures long distance reorderings be-
</bodyText>
<table confidence="0.99384">
Type Constituent Examples
Word Variable
PR - X — (Z �, one of)
HR X — (XM, ofX)
GR - � 5 — (5X, 5X)
</table>
<tableCaption confidence="0.955919666666667">
Table 1: A classification of grammar rules for the HPB
model. PR = phrasal rule, HR = hierarchical rule, GR =
glue rule.
</tableCaption>
<bodyText confidence="0.9984445">
tween phrases. Therefore, the HPB model outper-
forms conventional phrase-based models on phrase
reorderings.
However, HPB translation suffers from a limita-
tion, in that the phrase reorderings lack of contex-
tual information, such as the surrounding words of
a phrase and the content of sub-phrases that rep-
resented by variables. Consider the following two
hierarchical rules in translating a Chinese sentence
into English:
</bodyText>
<equation confidence="0.999971">
X — (X1 M X2, X1 ’s X2) (1)
X — (X1 M X2, X2X1) (2)
</equation>
<bodyText confidence="0.882154">
� with Russia ’s talks
talks with Russia
Both pattern-match the source sentence, but pro-
duce quite different phrase reorderings. The first
rule generates a monotone translation, while the sec-
ond rule swaps the source phrases covered by X1
and X2 on the target side. During decoding, the first
</bodyText>
<page confidence="0.970699">
555
</page>
<note confidence="0.81813">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 555–563,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999922666666667">
rule is more likely to be used, as it occurs more fre-
quently in a training corpus. However, the exam-
ple is not a noun possessive case because the sub-
phrase covered by X1 is not a noun but a preposi-
tional phrase. Thus, without considering informa-
tion of sub-phrases, the decoder may make errors on
phrase reordering.
Contextual information has been widely used to
improve translation performance. It is helpful to re-
duce ambiguity, thus guide the decoder to choose
correct translation for a source text. Several re-
searchers observed that word sense disambiguation
improves translation quality on lexical translation
(Carpuat and Wu, 2007; Chan et al., 2007). These
methods utilized contextual features to determine
the correct meaning of a source word, thus help an
SMT system choose an appropriate target transla-
tion.
Zens and Ney (2006) and Xiong et al. (2006)
utilized contextual information to improve phrase
reordering. They addressed phrase reordering as
a two-class classification problem that translating
neighboring phrases serially or inversely. They built
a maximum entropy (MaxEnt) classifier based on
boundary words to predict the order of neighboring
phrases.
He et al. (2008) presented a lexicalized rule selec-
tion model to improve both lexical translation and
phrase reordering for HPB translation. They built
a MaxEnt model for each ambiguous source side
based on contextual features. The method was also
successfully applied to improve syntax-based SMT
translation (Liu et al., 2008), using more sophisti-
cated syntactical features. Shen et al. (2008) inte-
grated various contextual and linguistic features into
an HPB system, using surrounding words and de-
pendency information for building context and de-
pendency language models, respectively.
In this paper, we focus on improving phrase re-
ordering for HPB translation. We classify SCFG
rules into several reordering patterns consisting of
two variables X and F (or E) 1, such as X1FX2
and X2EX1. We treat phrase reordering as a classi-
fication problem and build a MaxEnt model for each
source reordering pattern based on various contex-
</bodyText>
<footnote confidence="0.50938">
1We use F and E to represent source and target words, re-
spectively.
</footnote>
<bodyText confidence="0.9872785">
tual features. We propose a method to integrate the
MaxEnt models into an HPB system. Specifically:
</bodyText>
<listItem confidence="0.99270325">
• For hierarchical rules, we classify the source-
side and the target-side into 7 and 17 reordering
patterns, respectively. Target reordering pat-
terns are treated as possible labels. We then
build a classifier for each source pattern to pre-
dict phrase reorderings. This is different from
He et al. (2008), in which they built a clas-
sifier for each ambiguous hierarchical source-
side. Therefore, the training examples for each
MaxEnt model is small and the model maybe
unstable. Here, we classify source hierarchical
phrases into 7 reordering patterns according to
the arrangement of words and variables. We
can obtain sufficient samples for each MaxEnt
model from large-scale bilingual corpus.
• For glue rules, we extend the HPB model by
using bracketing transduction grammar (BTG)
(Wu, 1996) instead of the monotone glue rule.
By doing this, there are two options for the de-
coder to merge phrases: serial or inverse. We
then build a classifier for glue rules to predict
reorderings of neighboring phrases, analogous
to Xiong et al. (2006).
• We integrate the MaxEnt based phrase reorder-
ing models as features into the HPB model
(Chiang, 2005). The feature weights can be
tuned together with other feature functions by
MERT algorithm (Och, 2003).
</listItem>
<bodyText confidence="0.997716538461538">
Experimental results show that the presented method
achieves significant improvement over the baseline.
On Chinese-to-English translation tasks of NIST
evluation, improvements in BLEU (case-insensitive)
are 1.2 on MT06 GALE set, 1.8 on MT06 NIST set,
and 2.1 on MT08.
The rest of the paper is structured as follows: Sec-
tion 2 describes the MaxEnt based phrase reorder-
ing method. Section 3 integrates the MaxEnt mod-
els into the translation model. In Section 4, we re-
port experimental results. We analyze the presented
method and experimental results in Section 5 and
conclude in Section 6.
</bodyText>
<page confidence="0.995597">
556
</page>
<figure confidence="0.930467625">
Source phrase
X *p
Target phrase
X and
with X
between X and
Source pattern
XF
FX
FXF
Target pattern
XE
EX
EXE
Table 2: A classification of the source side and the target
side for the hierarchical rule that contains one variable.
</figure>
<figureCaption confidence="0.976833">
Figure 1: A source hierarchical phrase and its corre-
sponding target translation.
</figureCaption>
<sectionHeader confidence="0.679954" genericHeader="method">
2 MaxEnt based Phrase Reordering
</sectionHeader>
<bodyText confidence="0.999712357142857">
We regard phrase reordering as a pattern classifica-
tion problem. A reordering pattern indicates an ar-
rangement of words and variables. Although there
are a large amount of hierarchical rules may be ex-
tracted from bilingual corpus, these rules can be
classified into several reordering patterns (Section
2.1). In addition, we extend the HPB model with
BTG, that adding an inverted glue rule to merge
phrases inversely (Section 2.2). Therefore, the glue
rules are classified into two patterns: serial or in-
verse. We then build a MaxEnt phrase reordering
(MEPR) classifier for each source reordering pattern
(Section 2.3). In Section 2.4, we describe contextual
features.
</bodyText>
<figure confidence="0.523021333333333">
Source pattern Target pattern
X1EX2
X2EX1
X1X2E
X2X1E
EX1X2
X1FX2 EX2X1
X1FX2F X1EX2E
FX1FX2 X2EX1E
FX1FX2F EX1X2E
EX2X1E
EX1EX2
EX2EX1
EX1EX2E
EX2EX1E
</figure>
<subsectionHeader confidence="0.9944125">
2.1 Reordering Pattern Classification for
Hierarchical Rule
</subsectionHeader>
<bodyText confidence="0.999966714285714">
Hierarchical rule, consisting of both words and vari-
ables, is of great importance for the HPB model.
During decoding, words are used for lexical trans-
lation, and variables capture phrase reordering. We
may learn millions of hierarchical rules from a bilin-
gual corpus. Although these rules are different from
each other, they can be classified into several re-
ordering patterns according to the arrangement of
variables and words.
In this paper, we follow the constraint as de-
scribed in (Chiang, 2005) that a hierarchical rule
can have at most two variables and they cannot be
adjacent on the source side. We use “X” to rep-
resent the variable, and “F” and “E” to represent
word strings in source and target language, respec-
tively. Therefore, in a hierarchical rule, E is the lex-
ical translation of F, while the order of X and E
contains phrase reordering information.
For the hierarchical rule that contains one vari-
able (see Figure 1 for example), both the source and
the target phrases can be classified into three pat-
</bodyText>
<tableCaption confidence="0.987729">
Table 3: A classification of the source side and the target
side for the hierarchical rule that contains two variables.
</tableCaption>
<bodyText confidence="0.999959647058823">
terns (Table 2). To reduce the complexity of clas-
sification, we do not distinguish the order of word
strings. For example, we consider “e1Xe2” and
“e2Xe1” as the same pattern “EXE”, because the
target words are determined by lexical translation of
source words. Our focus is the order between X and
E. During decoding the phrases covered by X are
dynamically changed and the contextual information
of these phrases is ignored for pattern-matching of
hierarchical rules.
Analogously, for the hierarchical rule that con-
tains two variables, the source phrases are classified
into 4 patterns, while the target phrases are classified
into 14 patterns, as shown in Table 3. The pattern
number on the source side is less than that on the
target side, because on the source side, “X1” always
appears before “X2”, and they cannot be adjacent.
</bodyText>
<page confidence="0.980157">
557
</page>
<subsectionHeader confidence="0.9869195">
2.2 Reordering Pattern Classification for Glue
Rule
</subsectionHeader>
<bodyText confidence="0.9999151">
The HPB model used glue rule to combine phrases
serially. The reason is that in some cases, there are
no valid translation rules that cover a source span.
Therefore, the glue rule provides a default monotone
combination of phrases in order to complete a trans-
lation. This is not sufficient because in certain cases,
the order of phrases may be inverted on the target-
side.
In this paper, we extend the glue rule with BTG
(Wu, 1996), which consists of three types of rules:
</bodyText>
<equation confidence="0.999929666666667">
X —* (f, e)
X —* (X1X2, X1X2)
X —* (X1X2, X2X1)
</equation>
<bodyText confidence="0.984783833333333">
Rule 3 is a phrasal rule that translates a source
phrase f into a target phrase E. Rule 4 merges two
consecutive phrases in monotone order, while Rule
5 merges them in inverted order. During decod-
ing, the decoder first uses Rule 3 to produce phrase
translation, and then iteratively uses Rule 4 and 5 to
merge two neighboring phrases into a larger phrase
until the whole sentence is covered.
We replace the original glue rules in the HPB
model with BTG rules (see Table 4). We believe
that the extended HPB model can benefit from BTG
in the following aspects:
</bodyText>
<listItem confidence="0.876823142857143">
• In the HPB model, as we mentioned, hierarchi-
cal rules are constrained in that nonterminals
cannot be adjacent on the source side, i.e., the
source side cannot contain “X1X2”. One rea-
son is that it will heavily increase the rule table
size. The other reason is that it can cause a spu-
rious ambiguity problem (Chiang, 2005). The
inverted glue rule in BTG, however, can solve
this problem.
• In the HPB model, only a monotone glue rule
is provided to merge phrases serially. In the ex-
tended HPB model, the combination of phrases
is classified into two types: monotone and in-
verse.
</listItem>
<bodyText confidence="0.550606">
Analogous to Xiong et al. (2006), to perform
context-dependent phrase reordering, we build a
</bodyText>
<table confidence="0.9996315">
Glue Rule Extended Glue Rule
S —* (X, X) S —* (X, X)
S —* (SX, SX) X —* (X1X2, X1X2)
- X —* (X1X2,X2X1)
</table>
<tableCaption confidence="0.984767">
Table 4: Extending the glue rules in the HPB model with
BTG.
</tableCaption>
<bodyText confidence="0.997333">
MaxEnt based classifier for glue rules to predict the
order of two neighboring phrases. In this paper, we
utilize more contextual features.
</bodyText>
<subsectionHeader confidence="0.952856">
2.3 The MaxEnt based Phrase Reordering
Classifier
</subsectionHeader>
<bodyText confidence="0.996043428571429">
As described above, we classified phrase reorderings
into several patterns. Therefore, phrase reordering
can be regarded as a classification problem: for each
source reordering pattern, we treat the correspond-
ing target reordering patterns as labels.
We build a general classification model within the
MaxEnt framework:
</bodyText>
<equation confidence="0.998136">
Pme(Tγ|Tα, α, γ) _
exp(Ei λihi(γ, α, f(X), e(X)) (6)
</equation>
<bodyText confidence="0.9581595">
ET, exp(Ei λihi(γ, α, f (X), e(X ))
where, α and γ are the source and target side, re-
spectively. Tα/Tγ is the reordering pattern of α/γ.
f(X) and e(X) are the phrases that covered by X
one the source and target side, respectively. Given
a source phrase, the model predicts a target reorder-
ing pattern, considering various contextual features
(Section 2.4).
According to the classification of reordering pat-
terns, there are 3 kinds of classifiers:
</bodyText>
<listItem confidence="0.97189">
• Phr1
</listItem>
<bodyText confidence="0.870458333333333">
me includes 3 classifiers for the hierarchical
rules that contain 1 variable. Each of the clas-
sifier has 3 labels;
</bodyText>
<listItem confidence="0.970777">
• Phr2
</listItem>
<bodyText confidence="0.689485625">
me includes 4 classifiers for the hierarchical
rules that contain 2 variables. Each of the clas-
sifier has 14 labels;
• Pgrme includes 1 classifier for the glue rules. The
classifier has 2 labels that predict a monotone
or inverse order for two neighboring phrases.
This classifier is analogous to (Xiong et al.,
2006).
</bodyText>
<page confidence="0.991094">
558
</page>
<bodyText confidence="0.999958090909091">
There are 8 classifiers in total. This is much fewer
than the classifiers in He et al. (2008), in which a
classifier was built for each ambiguous hierarchical
source side. In this way, a classifier may face the
risk that there are not enough samples for training a
stable MaxEnt model. While our approach is more
generic, rather than training a MaxEnt model for a
specific hierarchical source side, we train a model
for a source reordering pattern. Thus, we reduce the
number of classifiers and can extract large training
examples for each classifier.
</bodyText>
<subsectionHeader confidence="0.987255">
2.4 Feature definition
</subsectionHeader>
<bodyText confidence="0.998703666666667">
For a reordering pattern pair (Tα, Tγ), we design
three feature functions for phrase reordering classi-
fiers:
</bodyText>
<listItem confidence="0.800226727272727">
• Source lexical feature, including boundary
words and neighboring words. Boundary
words are the left and right word of the source
phrases covered by f(X), while neighboring
words are the words that immediately to the left
and right of a source phrase f(α);
• Part-of-Speech (POS) feature, POS tags of the
boundary and neighboring words on the source
side.
• Target lexical feature, the boundary words of
the target phrases covered by e(X).
</listItem>
<bodyText confidence="0.998994428571429">
These features can be extracted together with
translation rules from bilingual corpus. However,
since the hierarchical rule does not allow for adja-
cent variables on the source side, we extract features
for Pgrme by using the method described in Xiong et
al. (2006). We train the classifiers with a MaxEnt
trainer (Zhang, 2004).
</bodyText>
<sectionHeader confidence="0.9513255" genericHeader="method">
3 Integrating the MEPR Classifier into the
HPB Model
</sectionHeader>
<bodyText confidence="0.99916">
The HPB model is built within the standard log-
linear framework (Och and Ney, 2002):
</bodyText>
<equation confidence="0.884532">
Pr(e|f) a � Aihi(α,-y) (7)
i
</equation>
<bodyText confidence="0.914463363636364">
where hi(α, -y) is a feature function and Ai is the
weight of hi. The HPB model has the following fea-
tures: translation probabilities p(-y|α) and p(α|-y),
lexical weights pw(-y|α) and pw(α|-y), word penalty,
phrase penalty, glue rule penalty, and a target n-
gram language model.
To integrate the MEPR classifiers into the transla-
tion model, the features of the log-linear model are
changed as follows:
• We add the MEPR classifier as a feature func-
tion to predict reordering pattern:
</bodyText>
<equation confidence="0.935593">
�
hme(Tγ|Tα) � Pme(Tγ|Tα, α, -y) (8)
</equation>
<bodyText confidence="0.999615285714286">
During decoding, we first classify each source
phrase into one of the 8 source reordering pat-
terns and then use the corresponding MEPR
classifier to predict the possible target reorder-
ing pattern. Therefore, the contextual informa-
tion guides the decoder to perform phrase re-
ordering.
</bodyText>
<listItem confidence="0.5377446">
• We split the “glue rule penalty” into two fea-
tures: monotone glue rule number and inverted
glue rule number. These features reflect pref-
erence of the decoder for using monotone or
inverted glue rules.
</listItem>
<bodyText confidence="0.999815235294118">
The advantage of our extension method is that the
weights of the new features can be tuned together
with the other features by MERT algorithm (Och,
2003).
We utilize a standard CKY algorithm for decod-
ing. Given a source sentence, the decoder searches
the best derivation from the bottom to top. For a
source span [j1, j2], the decoder uses three kinds of
rules: translation rules produce lexical translation
and phrase reordering (for hierarchical rules), mono-
tone rule merges any neighboring sub-spans [j1, k]
and [k + 1, j2] serially, and inverted rule swap them.
Note that when the decoder uses the monotone and
inverted glue rule to combine sub-spans, it merges
phrases that do not contain variables. Because the
CKY algorithm guarantees that the sub spans [j1, k]
and [k + 1, j2] have been translated before [j1, j2].
</bodyText>
<page confidence="0.998687">
559
</page>
<sectionHeader confidence="0.9989" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99791">
We carried out experiments on four systems:
</bodyText>
<listItem confidence="0.973757777777778">
• HPB: replication of the Hiero system (Chiang,
2005);
• HPB+MEHR: HPB with MaxEnt based classi-
fier for hierarchical rules, as described in Sec-
tion 2.1;
• HPB+MEGR: HPB with MaxEnt based classi-
fier for glue rules, as described in Section 2.2;
• HPB+MER: HPB with MaxEnt based classifier
for both hierarchical and glue rules.
</listItem>
<bodyText confidence="0.999350454545454">
All systems were tuned on NIST MT03 and tested
on MT06 and MT08. The evaluation metric was
BLEU (Papineni et al., 2002) with case-insensitive
matching of n-grams, where n = 4.
We evaluated our approach on Chinese-to-
English translation. The training data contained
77M Chinese words and 81M English words.
These data come from 17 corpora: LDC2002E18,
LDC2002L27, LDC2002T01, LDC2003E07,
LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34,
LDC2006E24, LDC2006E26, LDC2006E34,
LDC2006E86, LDC2006E92, LDC2006E93,
LDC2004T08 (HK News, HK Hansards).
To obtain word alignments, we first ran GIZA++
(Och and Ney, 2000) in both translation directions
and then refined the results using the “grow-diag-
final” method (Koehn et al., 2003). For the lan-
guage model, we used the SRI Language Modeling
Toolkit (Stolcke, 2002) to train two 4-gram models
on the Xinhua portion of the GigaWord corpus and
the English side of the training corpus.
</bodyText>
<subsectionHeader confidence="0.9685425">
4.1 Statistical Information of Rules
Hierarchical Rules
</subsectionHeader>
<bodyText confidence="0.881261571428571">
We extracted 162M translation rules from the train-
ing corpus. Among them, there were 127M hi-
erarchical rules, which contained 85M hierarchical
source phrases. We classified these source phrases
into 7 patterns as described in Section 2.1. Table
5 shows the statistical information. We observed
that the most frequent source pattern is “FXF”,
</bodyText>
<table confidence="0.988481875">
Source Pattern Percentage (%)
XF 9.7
FX 9.7
FXF 46.1
X1FX2 3.7
X1FX2F 11.9
FX1FX2 11.8
FX1FX2F 7.1
</table>
<tableCaption confidence="0.9677975">
Table 5: Statistical information of reordering pattern clas-
sification for hierarchical source phrases.
</tableCaption>
<table confidence="0.9998872">
# Source
Target (%) FX XF FXF
EX 82.8 7 4.6
XE 6.4 82.4 2.9
EXE 10.8 10.6 92.5
</table>
<tableCaption confidence="0.9974325">
Table 6: Percentage of target reordering pattern for each
source pattern containing one variable.
</tableCaption>
<bodyText confidence="0.999961222222222">
which accounted for 46.1% of the total. Interest-
ingly, “X1FX2”, accounting for 3.7%, was the least
frequent pattern. Table 6 and Table 7 show the
distributions of reordering patterns for hierarchical
source phrases that contain one and two variables,
respectively. From both the tables, we observed
that for Chinese-to-English translation, the most fre-
quent “reordering” pattern for a source phrase is
monotone translation (bold font in the tables).
</bodyText>
<subsectionHeader confidence="0.899868">
Glue Rules
</subsectionHeader>
<bodyText confidence="0.999951571428571">
To train a MaxEnt classifier for glue rules, we ex-
tracted 65.8M reordering (monotone and inverse)
instances from the training data, using the algo-
rithm described in Xiong et al. (2006). There were
63M monotone instances, accounting for 95.7%. Al-
though instances of inverse reordering accounted for
4.3%, they are important for phrase reordering.
</bodyText>
<subsectionHeader confidence="0.580546">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.99530675">
Table 8 shows the BLEU scores and decoding speed
of the four systems on MT06 (GALE set and NIST
set) and MT08. From the table, we made the follow-
ing observations:
</bodyText>
<page confidence="0.988469">
560
</page>
<table confidence="0.999948375">
# Source
Target (%) FX1FX2 FX1FX2F X1FX2 X1FX2F
EX1EX2 78.1 3.6 4.6 1.2
EX1EX2E 2.1 75.9 0.1 1.6
EX1X2 6.8 0.1 2.8 0.1
EX1X2E 1.8 11.2 0.1 2
EX2EX1 2.8 1.4 2 1.2
EX2EX1E 1.4 2.3 0.7 1.1
EX2X1 0.9 0.1 2.2 0.2
EX2X1E 1 1.1 0.9 1.0
X1EX2 1.9 0.1 71.2 3.3
X1EX2E 0.7 2.1 6 78.4
X1X2E 0.1 0.1 2.8 5.9
X2EX1 0.9 0.4 1.6 0.7
X2EX1E 1.5 1.5 2.6 2.4
X2X1E 0.1 0.04 2.2 0.8
</table>
<tableCaption confidence="0.992312">
Table 7: Percentage of target reordering pattern for each source pattern containing two variables.
</tableCaption>
<table confidence="0.999687285714286">
System Speed
Test Data
06G 06N 08
HPB 14.19 33.93 25.85 8.7
HPB+MEHR 14.76 34.95 26.56 3.2
HPB+MEGR 15.09 35.72 27.34 2.7
HPB+MER 15.42 35.80 27.94 1.7
</table>
<tableCaption confidence="0.915610333333333">
Table 8: BLEU percentage scores and translation speed (words/second) on test data. G=GALE set, N=NIST set. All
improvements are statistically significant (p &lt; 0.01). Note that MT06G has one reference for each source sentence,
while the MT06N and MT08 have four references.
</tableCaption>
<listItem confidence="0.996956625">
• The HPB+MEHR system achieved significant
improvements on all test sets compared to the
HPB system. The absolute increases in BLEU
scores ranging from 0.6 (on 06G) to 1.0 (on
06N) percentage points. This indicates that the
ME based reordering for hierarchical rules im-
proves translation performance.
• The HPB+MEGR system achieved significant
</listItem>
<bodyText confidence="0.977222222222222">
improvements over the HPB system. The ab-
solute increases in BLEU scores ranging from
0.9 (on 06G) to 1.8 (on 06N) percentage points.
The HPB+MEGR system overcomes the short-
coming of the HPB system by using both
monotone glue rule and inverted glue rule,
which merging phrases serially and inversely,
respectively. Furthermore, the HPB+MEGR
system outperformed the HPB+MEHR system.
</bodyText>
<listItem confidence="0.985960125">
• The HPB+MER system achieved the best per-
formances on all test sets, with absolute in-
creases of BLEU scores ranging from 1.2 (on
06G) to 2.1 (on 08). The system combin-
ing with ME based reordering for both hier-
archical and glue rules, outperformed both the
HPB+MEHR and HPB+MEGR systems.
• In addition, we found that the decoder takes
</listItem>
<bodyText confidence="0.997396111111111">
more time after adding the MEPR models (the
speed column of Table 8). The average transla-
tion speed of HPB+MER (1.7 words/second) is
about 5 times slower than the HPB system (8.7
words/second). One reason is that the MEPR
models utilized contextual information to com-
pute classification scores. Another reason is
that adding inverted glue rules increases search
space.
</bodyText>
<page confidence="0.996672">
561
</page>
<sectionHeader confidence="0.99546" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999985888888889">
Experiments showed that the presented approach
achieved significant gains on BLEU scores. Further-
more, we sought to explore what would happen af-
ter integrating the MEPR classifiers into the transla-
tion model. We compared the outputs of HPB and
HPB+MER and observed that the translation perfor-
mance are improved on phrase reordering. For ex-
ample, the translations of a source sentence in MT08
are as follows 2:
</bodyText>
<listItem confidence="0.978862454545455">
• Src: ¸I1 4?2 ±‡A3 A4 3fA`n5 A146
Xt7 V08 JWr 40)710 n11 )�*12 M13 ffl
Ï14 i+115
• Ref: At the end4 of last3 month3, the
South1 Korean1 government2 began5 a plan15
to provider 400,00010 tonnes11 of rice12 as
aid14 to North8 Korea8
• HPB: South Korean government late last
month to start with 400,000 tons of rice aid to
the DPRK
• HPB+MER: Start at the end of last month,
</listItem>
<bodyText confidence="0.97619925">
South Korean government plans to provide
400,000 tons of rice in aid to the DPRK
The most obvious error that the baseline system
makes is the order of the time expression “�‡�
A, the end of last month”, which should be either
at the beginning or the end on target side. However,
the baseline produced a monotone translation by us-
ing the rule “¸I O? X1, South Korean govern-
ment X1”. The HPB+MER system, however, moved
the time expression to the beginning of the sentence
by using the rule “¸I O? X1, X1 South Ko-
rean government’. The reason is that the MaxEnt
phrase reordering classifier uses the contextual fea-
tures (e.g. the boundary words) of the phrase cov-
ered by X1 to predict the phrase reordering as X1E
for the source phrase FX1.
</bodyText>
<footnote confidence="0.856082">
2The co-indexes of the words in the source and reference
sentence indicate word alignments.
</footnote>
<sectionHeader confidence="0.959248" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999958956521739">
In this paper, we have proposed a MaxEnt based
phrase reordering approach to help the HPB decoder
select reordering patterns. We classified hierarchical
rules into 7 reordering patterns on the source side
and 17 reordering patterns on the target side. In ad-
dition, we introduced BTG to enhance the reorder-
ing of neighboring phrases and classified the glue
rules into two patterns. We trained a MaxEnt clas-
sifier for each reordering pattern and integrated it
into a standard HPB system. Experimental results
showed that the proposed approach achieved signif-
icant improvements over the baseline. The absolute
improvements in BLEU range from 1.2 to 2.1.
MaxEnt based phrase reordering provides a mech-
anism to incorporate various features into the trans-
lation model. In this paper, we only use a few fea-
ture sets based on standard contextual word and POS
tags. We believe that additional features will fur-
ther improve translation performance. Such features
could include syntactical features (Chiang et al.,
2009). In the future, we will carry out experiments
on deeper features and evaluate the effects of differ-
ent feature sets.
</bodyText>
<sectionHeader confidence="0.998733" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999585166666667">
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of EMNLP-CoNLL 2007, pages
61–72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, pages 33–40.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics - Hu-
man Language Technologies 2009 Conference, page
218õ226.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263–270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 33(2):201–
228.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
</reference>
<page confidence="0.970884">
562
</page>
<reference confidence="0.999720389830508">
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 321–328.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the 2003 Conference ofthe North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 48–54.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, page
89õ97.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440–447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295–302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311–318.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2008. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 72–80.
Andreas Stolcke. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Processing,
volume 2, pages 901–904.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of the
Thirty-Fourth Annual Meeting of the Association for
Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, pages 521–528.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 55–63.
Le Zhang. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
</reference>
<page confidence="0.998937">
563
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647266">
<title confidence="0.99971">Maximum Entropy Based Phrase for Hierarchical Phrase-based Translation</title>
<author confidence="0.993761">Zhongjun He Yao Meng Hao</author>
<affiliation confidence="0.99505">Fujitsu R&amp;D Center CO.,</affiliation>
<address confidence="0.993559">15/F, Tower A, Ocean International Center, 56 Dongsihuan Zhong Chaoyang District, Beijing, 100025,</address>
<email confidence="0.97365">mengyao,</email>
<abstract confidence="0.98070419047619">Hierarchical phrase-based (HPB) translation provides a powerful mechanism to capture both short and long distance phrase reorderings. However, the phrase reorderings lack of contextual information in conventional HPB systems. This paper proposes a contextdependent phrase reordering approach that uses the maximum entropy (MaxEnt) model to help the HPB decoder select appropriate reordering patterns. We classify translation rules into several reordering patterns, and build a MaxEnt model for each pattern based on various contextual features. We integrate the MaxEnt models into the HPB model. Experimental results show that our approach achieves significant improvements over a standard HPB system on large-scale translation tasks. On Chinese-to-English the absolute improvements in BLEU (caseinsensitive) range from 1.2 to 2.1.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<pages>61--72</pages>
<contexts>
<context position="3586" citStr="Carpuat and Wu, 2007" startWordPosition="554" endWordPosition="557">ely to be used, as it occurs more frequently in a training corpus. However, the example is not a noun possessive case because the subphrase covered by X1 is not a noun but a prepositional phrase. Thus, without considering information of sub-phrases, the decoder may make errors on phrase reordering. Contextual information has been widely used to improve translation performance. It is helpful to reduce ambiguity, thus guide the decoder to choose correct translation for a source text. Several researchers observed that word sense disambiguation improves translation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model t</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of EMNLP-CoNLL 2007, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="3606" citStr="Chan et al., 2007" startWordPosition="558" endWordPosition="561">occurs more frequently in a training corpus. However, the example is not a noun possessive case because the subphrase covered by X1 is not a noun but a prepositional phrase. Thus, without considering information of sub-phrases, the decoder may make errors on phrase reordering. Contextual information has been widely used to improve translation performance. It is helpful to reduce ambiguity, thus guide the decoder to choose correct translation for a source text. Several researchers observed that word sense disambiguation improves translation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexic</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Wei Wang</author>
<author>Kevin Knight</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies 2009 Conference,</booktitle>
<pages>218--226</pages>
<marker>Chiang, Wang, Knight, 2009</marker>
<rawString>David Chiang, Wei Wang, and Kevin Knight. 2009. 11,001 new features for statistical machine translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies 2009 Conference, page 218õ226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1213" citStr="Chiang, 2005" startWordPosition="168" endWordPosition="170">he maximum entropy (MaxEnt) model to help the HPB decoder select appropriate reordering patterns. We classify translation rules into several reordering patterns, and build a MaxEnt model for each pattern based on various contextual features. We integrate the MaxEnt models into the HPB model. Experimental results show that our approach achieves significant improvements over a standard HPB system on large-scale translation tasks. On Chinese-to-English translation, the absolute improvements in BLEU (caseinsensitive) range from 1.2 to 2.1. 1 Introduction The hierarchical phrase-based (HPB) model (Chiang, 2005; Chiang, 2007) has been widely adopted in statistical machine translation (SMT). It utilizes synchronous context free grammar (SCFG) rules to perform translation. Typically, there are three types of rules (see Table 1): phrasal rule, a phrase pair consisting of consecutive words; hierarchical rule, a hierarchical phrase pair consisting of both words and variables; and glue rule, which is used to merge phrases serially. Phrasal rule captures short distance reorderings within phrases, while hierarchical rule captures long distance reorderings beType Constituent Examples Word Variable PR - X — (</context>
<context position="6360" citStr="Chiang, 2005" startWordPosition="998" endWordPosition="999">reordering patterns according to the arrangement of words and variables. We can obtain sufficient samples for each MaxEnt model from large-scale bilingual corpus. • For glue rules, we extend the HPB model by using bracketing transduction grammar (BTG) (Wu, 1996) instead of the monotone glue rule. By doing this, there are two options for the decoder to merge phrases: serial or inverse. We then build a classifier for glue rules to predict reorderings of neighboring phrases, analogous to Xiong et al. (2006). • We integrate the MaxEnt based phrase reordering models as features into the HPB model (Chiang, 2005). The feature weights can be tuned together with other feature functions by MERT algorithm (Och, 2003). Experimental results show that the presented method achieves significant improvement over the baseline. On Chinese-to-English translation tasks of NIST evluation, improvements in BLEU (case-insensitive) are 1.2 on MT06 GALE set, 1.8 on MT06 NIST set, and 2.1 on MT08. The rest of the paper is structured as follows: Section 2 describes the MaxEnt based phrase reordering method. Section 3 integrates the MaxEnt models into the translation model. In Section 4, we report experimental results. We a</context>
<context position="8780" citStr="Chiang, 2005" startWordPosition="1384" endWordPosition="1385">E FX1FX2F EX1X2E EX2X1E EX1EX2 EX2EX1 EX1EX2E EX2EX1E 2.1 Reordering Pattern Classification for Hierarchical Rule Hierarchical rule, consisting of both words and variables, is of great importance for the HPB model. During decoding, words are used for lexical translation, and variables capture phrase reordering. We may learn millions of hierarchical rules from a bilingual corpus. Although these rules are different from each other, they can be classified into several reordering patterns according to the arrangement of variables and words. In this paper, we follow the constraint as described in (Chiang, 2005) that a hierarchical rule can have at most two variables and they cannot be adjacent on the source side. We use “X” to represent the variable, and “F” and “E” to represent word strings in source and target language, respectively. Therefore, in a hierarchical rule, E is the lexical translation of F, while the order of X and E contains phrase reordering information. For the hierarchical rule that contains one variable (see Figure 1 for example), both the source and the target phrases can be classified into three patTable 3: A classification of the source side and the target side for the hierarch</context>
<context position="11712" citStr="Chiang, 2005" startWordPosition="1898" endWordPosition="1899"> iteratively uses Rule 4 and 5 to merge two neighboring phrases into a larger phrase until the whole sentence is covered. We replace the original glue rules in the HPB model with BTG rules (see Table 4). We believe that the extended HPB model can benefit from BTG in the following aspects: • In the HPB model, as we mentioned, hierarchical rules are constrained in that nonterminals cannot be adjacent on the source side, i.e., the source side cannot contain “X1X2”. One reason is that it will heavily increase the rule table size. The other reason is that it can cause a spurious ambiguity problem (Chiang, 2005). The inverted glue rule in BTG, however, can solve this problem. • In the HPB model, only a monotone glue rule is provided to merge phrases serially. In the extended HPB model, the combination of phrases is classified into two types: monotone and inverse. Analogous to Xiong et al. (2006), to perform context-dependent phrase reordering, we build a Glue Rule Extended Glue Rule S —* (X, X) S —* (X, X) S —* (SX, SX) X —* (X1X2, X1X2) - X —* (X1X2,X2X1) Table 4: Extending the glue rules in the HPB model with BTG. MaxEnt based classifier for glue rules to predict the order of two neighboring phrase</context>
<context position="17253" citStr="Chiang, 2005" startWordPosition="2831" endWordPosition="2832">e decoder uses three kinds of rules: translation rules produce lexical translation and phrase reordering (for hierarchical rules), monotone rule merges any neighboring sub-spans [j1, k] and [k + 1, j2] serially, and inverted rule swap them. Note that when the decoder uses the monotone and inverted glue rule to combine sub-spans, it merges phrases that do not contain variables. Because the CKY algorithm guarantees that the sub spans [j1, k] and [k + 1, j2] have been translated before [j1, j2]. 559 4 Experiments We carried out experiments on four systems: • HPB: replication of the Hiero system (Chiang, 2005); • HPB+MEHR: HPB with MaxEnt based classifier for hierarchical rules, as described in Section 2.1; • HPB+MEGR: HPB with MaxEnt based classifier for glue rules, as described in Section 2.2; • HPB+MER: HPB with MaxEnt based classifier for both hierarchical and glue rules. All systems were tuned on NIST MT03 and tested on MT06 and MT08. The evaluation metric was BLEU (Papineni et al., 2002) with case-insensitive matching of n-grams, where n = 4. We evaluated our approach on Chinese-toEnglish translation. The training data contained 77M Chinese words and 81M English words. These data come from 17</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--2</pages>
<contexts>
<context position="1228" citStr="Chiang, 2007" startWordPosition="171" endWordPosition="172">ropy (MaxEnt) model to help the HPB decoder select appropriate reordering patterns. We classify translation rules into several reordering patterns, and build a MaxEnt model for each pattern based on various contextual features. We integrate the MaxEnt models into the HPB model. Experimental results show that our approach achieves significant improvements over a standard HPB system on large-scale translation tasks. On Chinese-to-English translation, the absolute improvements in BLEU (caseinsensitive) range from 1.2 to 2.1. 1 Introduction The hierarchical phrase-based (HPB) model (Chiang, 2005; Chiang, 2007) has been widely adopted in statistical machine translation (SMT). It utilizes synchronous context free grammar (SCFG) rules to perform translation. Typically, there are three types of rules (see Table 1): phrasal rule, a phrase pair consisting of consecutive words; hierarchical rule, a hierarchical phrase pair consisting of both words and variables; and glue rule, which is used to merge phrases serially. Phrasal rule captures short distance reorderings within phrases, while hierarchical rule captures long distance reorderings beType Constituent Examples Word Variable PR - X — (Z �, one of) HR</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, pages 33(2):201– 228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="4139" citStr="He et al. (2008)" startWordPosition="637" endWordPosition="640">ranslation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation. They built a MaxEnt model for each ambiguous source side based on contextual features. The method was also successfully applied to improve syntax-based SMT translation (Liu et al., 2008), using more sophisticated syntactical features. Shen et al. (2008) integrated various contextual and linguistic features into an HPB system, using surrounding words and dependency information for building context and dependency language models, respectively. In this paper, we focus on im</context>
<context position="5521" citStr="He et al. (2008)" startWordPosition="859" endWordPosition="862"> X2EX1. We treat phrase reordering as a classification problem and build a MaxEnt model for each source reordering pattern based on various contex1We use F and E to represent source and target words, respectively. tual features. We propose a method to integrate the MaxEnt models into an HPB system. Specifically: • For hierarchical rules, we classify the sourceside and the target-side into 7 and 17 reordering patterns, respectively. Target reordering patterns are treated as possible labels. We then build a classifier for each source pattern to predict phrase reorderings. This is different from He et al. (2008), in which they built a classifier for each ambiguous hierarchical sourceside. Therefore, the training examples for each MaxEnt model is small and the model maybe unstable. Here, we classify source hierarchical phrases into 7 reordering patterns according to the arrangement of words and variables. We can obtain sufficient samples for each MaxEnt model from large-scale bilingual corpus. • For glue rules, we extend the HPB model by using bracketing transduction grammar (BTG) (Wu, 1996) instead of the monotone glue rule. By doing this, there are two options for the decoder to merge phrases: seria</context>
<context position="13782" citStr="He et al. (2008)" startWordPosition="2250" endWordPosition="2253">assification of reordering patterns, there are 3 kinds of classifiers: • Phr1 me includes 3 classifiers for the hierarchical rules that contain 1 variable. Each of the classifier has 3 labels; • Phr2 me includes 4 classifiers for the hierarchical rules that contain 2 variables. Each of the classifier has 14 labels; • Pgrme includes 1 classifier for the glue rules. The classifier has 2 labels that predict a monotone or inverse order for two neighboring phrases. This classifier is analogous to (Xiong et al., 2006). 558 There are 8 classifiers in total. This is much fewer than the classifiers in He et al. (2008), in which a classifier was built for each ambiguous hierarchical source side. In this way, a classifier may face the risk that there are not enough samples for training a stable MaxEnt model. While our approach is more generic, rather than training a MaxEnt model for a specific hierarchical source side, we train a model for a source reordering pattern. Thus, we reduce the number of classifiers and can extract large training examples for each classifier. 2.4 Feature definition For a reordering pattern pair (Tα, Tγ), we design three feature functions for phrase reordering classifiers: • Source </context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference ofthe North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="18271" citStr="Koehn et al., 2003" startWordPosition="2982" endWordPosition="2985">se-insensitive matching of n-grams, where n = 4. We evaluated our approach on Chinese-toEnglish translation. The training data contained 77M Chinese words and 81M English words. These data come from 17 corpora: LDC2002E18, LDC2002L27, LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24, LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92, LDC2006E93, LDC2004T08 (HK News, HK Hansards). To obtain word alignments, we first ran GIZA++ (Och and Ney, 2000) in both translation directions and then refined the results using the “grow-diagfinal” method (Koehn et al., 2003). For the language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train two 4-gram models on the Xinhua portion of the GigaWord corpus and the English side of the training corpus. 4.1 Statistical Information of Rules Hierarchical Rules We extracted 162M translation rules from the training corpus. Among them, there were 127M hierarchical rules, which contained 85M hierarchical source phrases. We classified these source phrases into 7 patterns as described in Section 2.1. Table 5 shows the statistical information. We observed that the most frequent source pattern is “FXF”, S</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference ofthe North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qun Liu</author>
<author>Zhongjun He</author>
<author>Yang Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based rule selection model for syntax-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>89--97</pages>
<contexts>
<context position="4450" citStr="Liu et al., 2008" startWordPosition="684" endWordPosition="687">nformation to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation. They built a MaxEnt model for each ambiguous source side based on contextual features. The method was also successfully applied to improve syntax-based SMT translation (Liu et al., 2008), using more sophisticated syntactical features. Shen et al. (2008) integrated various contextual and linguistic features into an HPB system, using surrounding words and dependency information for building context and dependency language models, respectively. In this paper, we focus on improving phrase reordering for HPB translation. We classify SCFG rules into several reordering patterns consisting of two variables X and F (or E) 1, such as X1FX2 and X2EX1. We treat phrase reordering as a classification problem and build a MaxEnt model for each source reordering pattern based on various conte</context>
</contexts>
<marker>Liu, He, Liu, Lin, 2008</marker>
<rawString>Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 2008. Maximum entropy based rule selection model for syntax-based statistical machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, page 89õ97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="18156" citStr="Och and Ney, 2000" startWordPosition="2964" endWordPosition="2967">ere tuned on NIST MT03 and tested on MT06 and MT08. The evaluation metric was BLEU (Papineni et al., 2002) with case-insensitive matching of n-grams, where n = 4. We evaluated our approach on Chinese-toEnglish translation. The training data contained 77M Chinese words and 81M English words. These data come from 17 corpora: LDC2002E18, LDC2002L27, LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24, LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92, LDC2006E93, LDC2004T08 (HK News, HK Hansards). To obtain word alignments, we first ran GIZA++ (Och and Ney, 2000) in both translation directions and then refined the results using the “grow-diagfinal” method (Koehn et al., 2003). For the language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train two 4-gram models on the Xinhua portion of the GigaWord corpus and the English side of the training corpus. 4.1 Statistical Information of Rules Hierarchical Rules We extracted 162M translation rules from the training corpus. Among them, there were 127M hierarchical rules, which contained 85M hierarchical source phrases. We classified these source phrases into 7 patterns as described in Se</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="15277" citStr="Och and Ney, 2002" startWordPosition="2496" endWordPosition="2499">re, POS tags of the boundary and neighboring words on the source side. • Target lexical feature, the boundary words of the target phrases covered by e(X). These features can be extracted together with translation rules from bilingual corpus. However, since the hierarchical rule does not allow for adjacent variables on the source side, we extract features for Pgrme by using the method described in Xiong et al. (2006). We train the classifiers with a MaxEnt trainer (Zhang, 2004). 3 Integrating the MEPR Classifier into the HPB Model The HPB model is built within the standard loglinear framework (Och and Ney, 2002): Pr(e|f) a � Aihi(α,-y) (7) i where hi(α, -y) is a feature function and Ai is the weight of hi. The HPB model has the following features: translation probabilities p(-y|α) and p(α|-y), lexical weights pw(-y|α) and pw(α|-y), word penalty, phrase penalty, glue rule penalty, and a target ngram language model. To integrate the MEPR classifiers into the translation model, the features of the log-linear model are changed as follows: • We add the MEPR classifier as a feature function to predict reordering pattern: � hme(Tγ|Tα) � Pme(Tγ|Tα, α, -y) (8) During decoding, we first classify each source ph</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="6462" citStr="Och, 2003" startWordPosition="1014" endWordPosition="1015"> for each MaxEnt model from large-scale bilingual corpus. • For glue rules, we extend the HPB model by using bracketing transduction grammar (BTG) (Wu, 1996) instead of the monotone glue rule. By doing this, there are two options for the decoder to merge phrases: serial or inverse. We then build a classifier for glue rules to predict reorderings of neighboring phrases, analogous to Xiong et al. (2006). • We integrate the MaxEnt based phrase reordering models as features into the HPB model (Chiang, 2005). The feature weights can be tuned together with other feature functions by MERT algorithm (Och, 2003). Experimental results show that the presented method achieves significant improvement over the baseline. On Chinese-to-English translation tasks of NIST evluation, improvements in BLEU (case-insensitive) are 1.2 on MT06 GALE set, 1.8 on MT06 NIST set, and 2.1 on MT08. The rest of the paper is structured as follows: Section 2 describes the MaxEnt based phrase reordering method. Section 3 integrates the MaxEnt models into the translation model. In Section 4, we report experimental results. We analyze the presented method and experimental results in Section 5 and conclude in Section 6. 556 Sourc</context>
<context position="16468" citStr="Och, 2003" startWordPosition="2699" endWordPosition="2700">sify each source phrase into one of the 8 source reordering patterns and then use the corresponding MEPR classifier to predict the possible target reordering pattern. Therefore, the contextual information guides the decoder to perform phrase reordering. • We split the “glue rule penalty” into two features: monotone glue rule number and inverted glue rule number. These features reflect preference of the decoder for using monotone or inverted glue rules. The advantage of our extension method is that the weights of the new features can be tuned together with the other features by MERT algorithm (Och, 2003). We utilize a standard CKY algorithm for decoding. Given a source sentence, the decoder searches the best derivation from the bottom to top. For a source span [j1, j2], the decoder uses three kinds of rules: translation rules produce lexical translation and phrase reordering (for hierarchical rules), monotone rule merges any neighboring sub-spans [j1, k] and [k + 1, j2] serially, and inverted rule swap them. Note that when the decoder uses the monotone and inverted glue rule to combine sub-spans, it merges phrases that do not contain variables. Because the CKY algorithm guarantees that the su</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="17644" citStr="Papineni et al., 2002" startWordPosition="2897" endWordPosition="2900">cause the CKY algorithm guarantees that the sub spans [j1, k] and [k + 1, j2] have been translated before [j1, j2]. 559 4 Experiments We carried out experiments on four systems: • HPB: replication of the Hiero system (Chiang, 2005); • HPB+MEHR: HPB with MaxEnt based classifier for hierarchical rules, as described in Section 2.1; • HPB+MEGR: HPB with MaxEnt based classifier for glue rules, as described in Section 2.2; • HPB+MER: HPB with MaxEnt based classifier for both hierarchical and glue rules. All systems were tuned on NIST MT03 and tested on MT06 and MT08. The evaluation metric was BLEU (Papineni et al., 2002) with case-insensitive matching of n-grams, where n = 4. We evaluated our approach on Chinese-toEnglish translation. The training data contained 77M Chinese words and 81M English words. These data come from 17 corpora: LDC2002E18, LDC2002L27, LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24, LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92, LDC2006E93, LDC2004T08 (HK News, HK Hansards). To obtain word alignments, we first ran GIZA++ (Och and Ney, 2000) in both translation directions and then refined the results using the “grow-diagfinal” </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="4517" citStr="Shen et al. (2008)" startWordPosition="694" endWordPosition="697">rdering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation. They built a MaxEnt model for each ambiguous source side based on contextual features. The method was also successfully applied to improve syntax-based SMT translation (Liu et al., 2008), using more sophisticated syntactical features. Shen et al. (2008) integrated various contextual and linguistic features into an HPB system, using surrounding words and dependency information for building context and dependency language models, respectively. In this paper, we focus on improving phrase reordering for HPB translation. We classify SCFG rules into several reordering patterns consisting of two variables X and F (or E) 1, such as X1FX2 and X2EX1. We treat phrase reordering as a classification problem and build a MaxEnt model for each source reordering pattern based on various contex1We use F and E to represent source and target words, respectively</context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2008. Effective use of linguistic and contextual information for statistical machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="18354" citStr="Stolcke, 2002" startWordPosition="2998" endWordPosition="2999">English translation. The training data contained 77M Chinese words and 81M English words. These data come from 17 corpora: LDC2002E18, LDC2002L27, LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24, LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92, LDC2006E93, LDC2004T08 (HK News, HK Hansards). To obtain word alignments, we first ran GIZA++ (Och and Ney, 2000) in both translation directions and then refined the results using the “grow-diagfinal” method (Koehn et al., 2003). For the language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train two 4-gram models on the Xinhua portion of the GigaWord corpus and the English side of the training corpus. 4.1 Statistical Information of Rules Hierarchical Rules We extracted 162M translation rules from the training corpus. Among them, there were 127M hierarchical rules, which contained 85M hierarchical source phrases. We classified these source phrases into 7 patterns as described in Section 2.1. Table 5 shows the statistical information. We observed that the most frequent source pattern is “FXF”, Source Pattern Percentage (%) XF 9.7 FX 9.7 FXF 46.1 X1FX2 3.7 X1FX2F 11.9 FX1FX2 11</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of the International Conference on Spoken language Processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6009" citStr="Wu, 1996" startWordPosition="937" endWordPosition="938">We then build a classifier for each source pattern to predict phrase reorderings. This is different from He et al. (2008), in which they built a classifier for each ambiguous hierarchical sourceside. Therefore, the training examples for each MaxEnt model is small and the model maybe unstable. Here, we classify source hierarchical phrases into 7 reordering patterns according to the arrangement of words and variables. We can obtain sufficient samples for each MaxEnt model from large-scale bilingual corpus. • For glue rules, we extend the HPB model by using bracketing transduction grammar (BTG) (Wu, 1996) instead of the monotone glue rule. By doing this, there are two options for the decoder to merge phrases: serial or inverse. We then build a classifier for glue rules to predict reorderings of neighboring phrases, analogous to Xiong et al. (2006). • We integrate the MaxEnt based phrase reordering models as features into the HPB model (Chiang, 2005). The feature weights can be tuned together with other feature functions by MERT algorithm (Och, 2003). Experimental results show that the presented method achieves significant improvement over the baseline. On Chinese-to-English translation tasks o</context>
<context position="10739" citStr="Wu, 1996" startWordPosition="1720" endWordPosition="1721">ss than that on the target side, because on the source side, “X1” always appears before “X2”, and they cannot be adjacent. 557 2.2 Reordering Pattern Classification for Glue Rule The HPB model used glue rule to combine phrases serially. The reason is that in some cases, there are no valid translation rules that cover a source span. Therefore, the glue rule provides a default monotone combination of phrases in order to complete a translation. This is not sufficient because in certain cases, the order of phrases may be inverted on the targetside. In this paper, we extend the glue rule with BTG (Wu, 1996), which consists of three types of rules: X —* (f, e) X —* (X1X2, X1X2) X —* (X1X2, X2X1) Rule 3 is a phrasal rule that translates a source phrase f into a target phrase E. Rule 4 merges two consecutive phrases in monotone order, while Rule 5 merges them in inverted order. During decoding, the decoder first uses Rule 3 to produce phrase translation, and then iteratively uses Rule 4 and 5 to merge two neighboring phrases into a larger phrase until the whole sentence is covered. We replace the original glue rules in the HPB model with BTG rules (see Table 4). We believe that the extended HPB mod</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<contexts>
<context position="3811" citStr="Xiong et al. (2006)" startWordPosition="592" endWordPosition="595">rmation of sub-phrases, the decoder may make errors on phrase reordering. Contextual information has been widely used to improve translation performance. It is helpful to reduce ambiguity, thus guide the decoder to choose correct translation for a source text. Several researchers observed that word sense disambiguation improves translation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation. They built a MaxEnt model for each ambiguous source side based on contextual features. The method was also successfully applied to improve syntax-b</context>
<context position="6256" citStr="Xiong et al. (2006)" startWordPosition="978" endWordPosition="981">each MaxEnt model is small and the model maybe unstable. Here, we classify source hierarchical phrases into 7 reordering patterns according to the arrangement of words and variables. We can obtain sufficient samples for each MaxEnt model from large-scale bilingual corpus. • For glue rules, we extend the HPB model by using bracketing transduction grammar (BTG) (Wu, 1996) instead of the monotone glue rule. By doing this, there are two options for the decoder to merge phrases: serial or inverse. We then build a classifier for glue rules to predict reorderings of neighboring phrases, analogous to Xiong et al. (2006). • We integrate the MaxEnt based phrase reordering models as features into the HPB model (Chiang, 2005). The feature weights can be tuned together with other feature functions by MERT algorithm (Och, 2003). Experimental results show that the presented method achieves significant improvement over the baseline. On Chinese-to-English translation tasks of NIST evluation, improvements in BLEU (case-insensitive) are 1.2 on MT06 GALE set, 1.8 on MT06 NIST set, and 2.1 on MT08. The rest of the paper is structured as follows: Section 2 describes the MaxEnt based phrase reordering method. Section 3 int</context>
<context position="12001" citStr="Xiong et al. (2006)" startWordPosition="1948" endWordPosition="1951">pects: • In the HPB model, as we mentioned, hierarchical rules are constrained in that nonterminals cannot be adjacent on the source side, i.e., the source side cannot contain “X1X2”. One reason is that it will heavily increase the rule table size. The other reason is that it can cause a spurious ambiguity problem (Chiang, 2005). The inverted glue rule in BTG, however, can solve this problem. • In the HPB model, only a monotone glue rule is provided to merge phrases serially. In the extended HPB model, the combination of phrases is classified into two types: monotone and inverse. Analogous to Xiong et al. (2006), to perform context-dependent phrase reordering, we build a Glue Rule Extended Glue Rule S —* (X, X) S —* (X, X) S —* (SX, SX) X —* (X1X2, X1X2) - X —* (X1X2,X2X1) Table 4: Extending the glue rules in the HPB model with BTG. MaxEnt based classifier for glue rules to predict the order of two neighboring phrases. In this paper, we utilize more contextual features. 2.3 The MaxEnt based Phrase Reordering Classifier As described above, we classified phrase reorderings into several patterns. Therefore, phrase reordering can be regarded as a classification problem: for each source reordering pattern</context>
<context position="13683" citStr="Xiong et al., 2006" startWordPosition="2231" endWordPosition="2234"> target reordering pattern, considering various contextual features (Section 2.4). According to the classification of reordering patterns, there are 3 kinds of classifiers: • Phr1 me includes 3 classifiers for the hierarchical rules that contain 1 variable. Each of the classifier has 3 labels; • Phr2 me includes 4 classifiers for the hierarchical rules that contain 2 variables. Each of the classifier has 14 labels; • Pgrme includes 1 classifier for the glue rules. The classifier has 2 labels that predict a monotone or inverse order for two neighboring phrases. This classifier is analogous to (Xiong et al., 2006). 558 There are 8 classifiers in total. This is much fewer than the classifiers in He et al. (2008), in which a classifier was built for each ambiguous hierarchical source side. In this way, a classifier may face the risk that there are not enough samples for training a stable MaxEnt model. While our approach is more generic, rather than training a MaxEnt model for a specific hierarchical source side, we train a model for a source reordering pattern. Thus, we reduce the number of classifiers and can extract large training examples for each classifier. 2.4 Feature definition For a reordering pa</context>
<context position="15078" citStr="Xiong et al. (2006)" startWordPosition="2462" endWordPosition="2465">ds are the left and right word of the source phrases covered by f(X), while neighboring words are the words that immediately to the left and right of a source phrase f(α); • Part-of-Speech (POS) feature, POS tags of the boundary and neighboring words on the source side. • Target lexical feature, the boundary words of the target phrases covered by e(X). These features can be extracted together with translation rules from bilingual corpus. However, since the hierarchical rule does not allow for adjacent variables on the source side, we extract features for Pgrme by using the method described in Xiong et al. (2006). We train the classifiers with a MaxEnt trainer (Zhang, 2004). 3 Integrating the MEPR Classifier into the HPB Model The HPB model is built within the standard loglinear framework (Och and Ney, 2002): Pr(e|f) a � Aihi(α,-y) (7) i where hi(α, -y) is a feature function and Ai is the weight of hi. The HPB model has the following features: translation probabilities p(-y|α) and p(α|-y), lexical weights pw(-y|α) and pw(α|-y), word penalty, phrase penalty, glue rule penalty, and a target ngram language model. To integrate the MEPR classifiers into the translation model, the features of the log-linear</context>
<context position="19894" citStr="Xiong et al. (2006)" startWordPosition="3242" endWordPosition="3245">of the total. Interestingly, “X1FX2”, accounting for 3.7%, was the least frequent pattern. Table 6 and Table 7 show the distributions of reordering patterns for hierarchical source phrases that contain one and two variables, respectively. From both the tables, we observed that for Chinese-to-English translation, the most frequent “reordering” pattern for a source phrase is monotone translation (bold font in the tables). Glue Rules To train a MaxEnt classifier for glue rules, we extracted 65.8M reordering (monotone and inverse) instances from the training data, using the algorithm described in Xiong et al. (2006). There were 63M monotone instances, accounting for 95.7%. Although instances of inverse reordering accounted for 4.3%, they are important for phrase reordering. 4.2 Results Table 8 shows the BLEU scores and decoding speed of the four systems on MT06 (GALE set and NIST set) and MT08. From the table, we made the following observations: 560 # Source Target (%) FX1FX2 FX1FX2F X1FX2 X1FX2F EX1EX2 78.1 3.6 4.6 1.2 EX1EX2E 2.1 75.9 0.1 1.6 EX1X2 6.8 0.1 2.8 0.1 EX1X2E 1.8 11.2 0.1 2 EX2EX1 2.8 1.4 2 1.2 EX2EX1E 1.4 2.3 0.7 1.1 EX2X1 0.9 0.1 2.2 0.2 EX2X1E 1 1.1 0.9 1.0 X1EX2 1.9 0.1 71.2 3.3 X1EX2E </context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics, pages 521–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative reordering models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<contexts>
<context position="3787" citStr="Zens and Ney (2006)" startWordPosition="587" endWordPosition="590">without considering information of sub-phrases, the decoder may make errors on phrase reordering. Contextual information has been widely used to improve translation performance. It is helpful to reduce ambiguity, thus guide the decoder to choose correct translation for a source text. Several researchers observed that word sense disambiguation improves translation quality on lexical translation (Carpuat and Wu, 2007; Chan et al., 2007). These methods utilized contextual features to determine the correct meaning of a source word, thus help an SMT system choose an appropriate target translation. Zens and Ney (2006) and Xiong et al. (2006) utilized contextual information to improve phrase reordering. They addressed phrase reordering as a two-class classification problem that translating neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring phrases. He et al. (2008) presented a lexicalized rule selection model to improve both lexical translation and phrase reordering for HPB translation. They built a MaxEnt model for each ambiguous source side based on contextual features. The method was also successfully app</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative reordering models for statistical machine translation. In Proceedings of the Workshop on Statistical Machine Translation, pages 55–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum entropy modeling toolkit for python and c++. available at http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.</title>
<date>2004</date>
<marker>Le Zhang, 2004</marker>
<rawString>Le Zhang. 2004. Maximum entropy modeling toolkit for python and c++. available at http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>