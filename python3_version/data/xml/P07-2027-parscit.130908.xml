<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006002">
<title confidence="0.976053">
Disambiguating Between Generic and Referential “You” in Dialog*
</title>
<author confidence="0.9616">
Surabhi Gupta
</author>
<affiliation confidence="0.974232">
Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.877">
Stanford, CA 94305, US
</address>
<email confidence="0.994566">
surabhi@cs.stanford.edu
</email>
<author confidence="0.933626">
Matthew Purver
</author>
<affiliation confidence="0.907722">
Center for the Study
of Language and Information
Stanford University
</affiliation>
<address confidence="0.906118">
Stanford, CA 94305, US
</address>
<email confidence="0.997742">
mpurver@stanford.edu
</email>
<author confidence="0.995433">
Dan Jurafsky
</author>
<affiliation confidence="0.9843925">
Department of Linguistics
Stanford University
</affiliation>
<address confidence="0.892792">
Stanford, CA 94305, US
</address>
<email confidence="0.998772">
jurafsky@stanford.edu
</email>
<sectionHeader confidence="0.993881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999489625">
We describe an algorithm for a novel task: disam-
biguating the pronoun you in conversation. You can
be generic or referential; finding referential you is im-
portant for tasks such as addressee identification or
extracting ‘owners’ of action items. Our classifier
achieves 84% accuracy in two-person conversations;
an initial study shows promising performance even on
more complex multi-party meetings.
</bodyText>
<sectionHeader confidence="0.98385" genericHeader="categories and subject descriptors">
1 Introduction and Background
</sectionHeader>
<bodyText confidence="0.9948592">
This paper describes an algorithm for disambiguat-
ing the generic and referential senses of the pronoun
you.
Our overall aim is the extraction of action items
from multi-party human-human conversations, con-
crete decisions in which one (or more) individuals
take on a group commitment to perform a given task
(Purver et al., 2006). Besides identifying the task it-
self, it is crucial to determine the owner, or person
responsible. Occasionally, the name of the responsi-
ble party is mentioned explicitly. More usually, the
owner is addressed directly and therefore referred to
using a second-person pronoun, as in example (1).1
A: and um if you can get that binding point also
maybe with a nice example that would be helpful
for Johno and me.
B: Oh yeah uh O K.
It can also be important to distinguish between
singular and plural reference, as in example (2)
where the task is assigned to more than one person:
</bodyText>
<listItem confidence="0.823865">
A: So y- so you guys will send to the rest of us um a
(2) version of um, this, and - the - uh, description -
B: With sugge- yeah, suggested improvements and -
</listItem>
<bodyText confidence="0.91511695">
Use of “you” might therefore help us both in de-
*This work was supported by the CALO project
(DARPA grant NBCH-D-03-0010) and ONR (MURI award
N000140510388). The authors also thank John Niekrasz for
annotating our test data.
1(1,2) are taken from the ICSI Meeting Corpus (Shriberg et
al., 2004); (3,4) from Switchboard (Godfrey et al., 1992).
tecting the fact that a task is being assigned, and in
identifying the owner. While there is an increas-
ing body of work concerning addressee identifica-
tion (Katzenmaier et al., 2004; Jovanovic et al.,
2006), there is very little investigating the problem
of second-person pronoun resolution, and it is this
that we address here. Most cases of “you” do not in
fact refer to the addressee but are generic, as in ex-
ample (3); automatic referentiality classification is
therefore very important.
B: Well, usually what you do is just wait until you
think it’s stopped,
and then you patch them up.
</bodyText>
<sectionHeader confidence="0.99967" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999971875">
Previous linguistic work has recognized that “you”
is not always addressee-referring, differentiating be-
tween generic and referential uses (Holmes, 1998;
Meyers, 1990) as well as idiomatic cases of “you
know”. For example, (Jurafsky et al., 2002) found
that “you know” covered 47% of cases, the referen-
tial class 22%, and the generic class 27%, with no
significant differences in surface form (duration or
vowel reduction) between the different cases.
While there seems to be no previous work investi-
gating automatic classification, there is related work
on classifying “it”, which also takes various referen-
tial and non-referential readings: (M¨uller, 2006) use
lexical and syntactic features in a rule-based clas-
sifier to detect non-referential uses, achieving raw
accuracies around 74-80% and F-scores 63-69%.
</bodyText>
<sectionHeader confidence="0.997577" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9992142">
We used the Switchboard corpus of two-party tele-
phone conversations (Godfrey et al., 1992), and an-
notated the data with four classes: generic, referen-
tial singular, referential plural and a reported refer-
ential class, for mention in reported speech of an
</bodyText>
<equation confidence="0.886382">
(1)
(3)
</equation>
<page confidence="0.973735">
105
</page>
<note confidence="0.4573845">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 105–108,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<table confidence="0.999314714285714">
Training Testing
Generic 360 79
Referential singular 287 92
Referential plural 17 3
Reported referential 5 1
Ambiguous 4 1
Total 673 176
</table>
<tableCaption confidence="0.999871">
Table 1: Number of cases found.
</tableCaption>
<bodyText confidence="0.986796391304348">
originally referential use (as the original addressee
may not be the current addressee – see example (4)).
We allowed a separate class for genuinely ambigu-
ous cases. Switchboard explicitly tags “you know”
when used as a discourse marker; as this (generic)
case is common and seems trivial we removed it
from our data.
B: Well, uh, I guess probably the last one I went to I
met so many people that I had not seen in proba-
bly ten, over ten years.
It was like, don’t you remember me.
And I am like no.
A: Am I related to you?
To test inter-annotator agreement, two people an-
notated 4 conversations, yielding 85 utterances con-
taining “you”; the task was reported to be easy, and
the kappa was 100%.
We then annotated a total of 42 conversations for
training and 13 for testing. Different labelers an-
notated the training and test sets; none of the au-
thors were involved in labeling the test set. Table 1
presents information about the number of instances
of each of these classes found.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.998879222222222">
All features used for classifier experiments were
extracted from the Switchboard LDC Treebank 3
release, which includes transcripts, part of speech
information using the Penn tagset (Marcus et al.,
1994) and dialog act tags (Jurafsky et al., 1997).
Features fell into four main categories:2 senten-
tial features which capture lexical features of the
utterance itself; part-of-speech features which cap-
ture shallow syntactic patterns; dialog act features
capturing the discourse function of the current ut-
terance and surrounding context; and context fea-
tures which give oracle information (i.e., the cor-
rect generic/referential label) about preceding uses
2Currently, features are all based on perfect transcriptions.
of “you”. We also investigated using the presence
of a question mark in the transcription as a feature,
as a possible replacement for some dialog act fea-
tures. Table 2 presents our features in detail.
</bodyText>
<table confidence="0.999283074074074">
N Features
Sentential Features (Sent)
2 you, you know, you guys
N number of you, your, yourself
2 you (say|said|tell|told|mention(ed)|mean(t)|sound(ed))
2 you (hear|heard)
2 (do|does|did|have|has|had|are|could|should|n’t) you
2 “if you”
2 (which|what|where|when|how) you
Part of Speech Features (POS)
2 Comparative JJR tag
2 you (VB*)
2 (I|we) (VB*)
2 (PRP*) you
Dialog Act Features (DA)
46 DA tag of current utterance i
46 DA tag of previous utterance i − 1
46 DA tag of utterance i − 2
2 Presence of any question DA tag (Q DA)
2 Presence of elaboration DA tag
Oracle Context Features (Ctxt)
3 Class of utterance i − 1
3 Class of utterance i − 2
3 Class of previous utterance by same speaker
3 Class of previous labeled utterance
Other Features (QM)
2 Question mark
</table>
<tableCaption confidence="0.986332666666667">
Table 2: Features investigated. N indicates the num-
ber of possible values (there are 46 DA tags; context
features can be generic, referential or N/A).
</tableCaption>
<sectionHeader confidence="0.994412" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.915664214285714">
As Table 1 shows, there are very few occurrences
of the referential plural, reported referential and am-
biguous classes. We therefore decided to model our
problem as a two way classification task, predicting
generic versus referential (collapsing referential sin-
gular and plural as one category). Note that we ex-
pect this to be the major useful distinction for our
overall action-item detection task.
Baseline A simple baseline involves predicting the
dominant class (in the test set, referential). This
gives 54.59% accuracy (see Table 1).3
SVM Results We used LIBSVM (Chang and Lin,
2001), a support vector machine classifier trained
using an RBF kernel. Table 3 presents results for
</bodyText>
<equation confidence="0.3173705">
3Precision and recall are of course 54.59% and 100%.
(4)
</equation>
<page confidence="0.953564">
106
</page>
<table confidence="0.9997319">
Features Accuracy F-Score
Ctxt 45.66% 0%
Baseline 54.59% 70.63%
Sent 67.05% 57.14%
Sent + Ctxt + POS 67.05% 57.14%
Sent + Ctxt + POS + QM 76.30% 72.84%
Sent + Ctxt + POS + Q DA 79.19% 77.50%
DA 80.92% 79.75%
Sent + Ctxt + POS + 84.39% 84.21%
QM + DA
</table>
<tableCaption confidence="0.999876">
Table 3: SVM results: generic versus referential
</tableCaption>
<bodyText confidence="0.99576934375">
various selected sets of features. The best set of fea-
tures gave accuracy of 84.39% and f-score 84.21%.
Discussion Overall performance is respectable;
precision was consistently high (94% for the
highest-accuracy result). Perhaps surprisingly, none
of the context or part-of-speech features were found
to be useful; however, dialog act features proved
very useful – using these features alone give us
an accuracy of 80.92% – with the referential class
strongly associated with question dialog acts.
We used manually produced dialog act tags, and
automatic labeling accuracy with this fine-grained
tagset will be low; we would therefore prefer to
use more robust features if possible. We found that
one such heuristic feature, the presence of ques-
tion mark, cannot entirely substitute: accuracy is
reduced to 76.3%. However, using only the binary
Q DA feature (which clusters together all the dif-
ferent kinds of question DAs) does better (79.19%).
Although worse than performance with a full tagset,
this gives hope that using a coarse-grained set of
tags might allow reasonable results. As (Stolcke et
al., 2000) report good accuracy (87%) for statement
vs. question classification on manual Switchboard
transcripts, such coarse-grained information might
be reliably available.
Surprisingly, using the oracle context features (the
correct classification for the previous you) alone per-
forms worse than the baseline; and adding these fea-
tures to sentential features gives no improvement.
This suggests that the generic/referential status of
each you may be independent of previous yous.
</bodyText>
<table confidence="0.9984922">
Features Accuracy F-Score
Prosodic only 46.66% 44.31%
Baseline 54.59% 70.63%
Sent + Ctxt + POS + 84.39% 84.21%
QM + DA + Prosodic
</table>
<tableCaption confidence="0.986428">
Table 4: SVM results: prosodic features
</tableCaption>
<table confidence="0.9998096">
Category Referential Generic
Count 294 340
Pitch (Hz) 156.18 143.98
Intensity (dB) 60.06 59.41
Duration (msec) 139.50 136.84
</table>
<tableCaption confidence="0.998374">
Table 5: Prosodic feature analysis
</tableCaption>
<sectionHeader confidence="0.978188" genericHeader="method">
6 Prosodic Features
</sectionHeader>
<bodyText confidence="0.99998745">
We next checked a set of prosodic features, test-
ing the hypothesis that generics are prosodically re-
duced. Mean pitch, intensity and duration were ex-
tracted using Praat, both averaged over the entire
utterance and just for the word “you”. Classifi-
cation results are shown in Table 4. Using only
prosodic features performs below the baseline; in-
cluding prosodic features with the best-performing
feature set from Table 3 gives identical performance
to that with lexical and contextual features alone.
To see why the prosodic features did not help, we
examined the difference between the average pitch,
intensity and duration for referential versus generic
cases (Table 5). A one-sided t-test shows no signif-
icant differences between the average intensity and
duration (confirming the results of (Jurafsky et al.,
2002), who found no significant change in duration).
The difference in the average pitch was found to be
significant (p=0.2) – but not enough for this feature
alone to cause an increase in overall accuracy.
</bodyText>
<sectionHeader confidence="0.997465" genericHeader="method">
7 Error Analysis
</sectionHeader>
<bodyText confidence="0.999203444444445">
We performed an error analysis on our best classi-
fier output on the training set; accuracy was 94.53%,
giving a total of 36 errors.
Half of the errors (18 of 36) were ambiguous even
for humans (the authors), if looking at the sentence
alone without the neighboring context from the ac-
tual conversation – see (5a). Treating these exam-
ples thus needs a detailed model of dialog context.
The other major class of errors requires detailed
</bodyText>
<page confidence="0.997509">
107
</page>
<bodyText confidence="0.999389625">
knowledge about sentential semantics and/or the
world – see e.g. (5b,c), which we can tell are ref-
erential because they predicate inter-personal com-
parison or communication.
In addition, as questions are such a useful feature
(see above), the classifier tends to label all question
cases as referential. However, generic uses do occur
within questions (5d), especially if rhetorical (5e):
</bodyText>
<reference confidence="0.9268565">
(5) a. so uh and if you don’t have the money then use a
credit card
b. I’m probably older than you
c. although uh I will personally tell you I used to work
at a bank
d. Do they survive longer if you plant them in the winter
time?
e. my question I guess are they really your peers?
</reference>
<sectionHeader confidence="0.98959" genericHeader="method">
8 Initial Multi-Party Experiments
</sectionHeader>
<bodyText confidence="0.999953115384615">
The experiments above used two-person dialog data:
we expect that multi-party data is more complex. We
performed an initial exploratory study, applying the
same classes and features to multi-party meetings.
Two annotators labeled one meeting from the
AMI corpus (Carletta et al., 2006), giving a total of
52 utterances containing “you” on which to assess
agreement: kappa was 87.18% for two way clas-
sification of generic versus referential. One of the
authors then labeled a testing set of 203 utterances;
104 are generic and 99 referential, giving a baseline
accuracy of 51.23% (and F-score of 67.65%).
We performed experiments for the same task: de-
tecting generic versus referential uses. Due to the
small amount of data, we trained the classifier on the
Switchboard training set from section 3 (i.e. on two-
party rather than multi-party data). Lacking part-of-
speech or dialog act features (since the dialog act
tagset differs from the Switchboard tagset), we used
only the sentential, context and question mark fea-
tures described in Table 2.
However, the classifier still achieves an accuracy
of 73.89% and F-score of 74.15%, comparable to the
results on Switchboard without dialog act features
(accuracy 76.30%). Precision is lower, though (both
precision and recall are 73-75%).
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999907090909091">
We have presented results on two person and multi-
party data for the task of generic versus referential
“you” detection. We have seen that the problem is
a real one: in both datasets the distribution of the
classes is approximately 50/50, and baseline accu-
racy is low. Classifier accuracy on two-party data is
reasonable, and we see promising results on multi-
party data with a basic set of features. We expect the
accuracy to go up once we train and test on same-
genre data and also add features that are more spe-
cific to multi-party data.
</bodyText>
<sectionHeader confidence="0.998539" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902404255319">
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot,
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal,
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post,
D. Reidsma, and P. Wellner. 2006. The AMI meeting cor-
pus. In MLMI2005, Revised Selected Papers.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for
Support Vector Machines. Software available at http:
//www.csie.ntu.edu.tw/—cjlin/libsvm.
J. J. Godfrey, E. Holliman, and J. McDaniel. 1992. SWITCH-
BOARD: Telephone speech corpus for research and devel-
opment. In Proceedings ofIEEE ICASSP-92.
J. Holmes. 1998. Generic pronouns in the Wellington corpus
of spoken New Zealand English. K¯otare, 1(1).
N. Jovanovic, R. op den Akker, and A. Nijholt. 2006. Ad-
dressee identification in face-to-face meetings. In Proceed-
ings of the 11th Conference of the EACL.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL shallow-discourse-function annota-
tion coders manual, draft 13. Technical Report 97-02, Uni-
versity of Colorado, Boulder.
D. Jurafsky, A. Bell, and C. Girand. 2002. The role of the
lemma in form variation. In C. Gussenhoven and N. Warner,
editors, Papers in Laboratory Phonology VII, pages 1–34.
M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004. Iden-
tifying the addressee in human-human-robot interactions
based on head pose and speech. In Proceedings of the 6th
International Conference on Multimodal Interfaces.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies,
M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn
treebank: Annotating predicate argument structure. In ARPA
Human Language Technology Workshop.
M. W. Meyers. 1990. Current generic pronoun usage. Ameri-
can Speech, 65(3):228–237.
C. M¨uller. 2006. Automatic detection of nonreferential It in
spoken multi-party dialog. In Proceedings of the 11th Con-
ference of the EACL.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting action
items in multi-party meetings: Annotation and initial exper-
iments. In MLMI2006, Revised Selected Papers.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The ICSI Meeting Recorder Dialog Act (MRDA) Corpus. In
Proceedings of the 5th SIGdial Workshop.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Juraf-
sky, P. Taylor, C. V. Ess-Dykema, R. Martin, and M. Meteer.
2000. Dialogue act modeling for automatic tagging and
recognition of conversational speech. Computational Lin-
guistics, 26(3):339–373.
</reference>
<page confidence="0.998366">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550767">
<title confidence="0.99823">Between Generic and Referential</title>
<author confidence="0.999144">Surabhi Gupta</author>
<affiliation confidence="0.999951">Department of Computer Science Stanford University</affiliation>
<address confidence="0.996457">Stanford, CA 94305, US</address>
<email confidence="0.999159">surabhi@cs.stanford.edu</email>
<author confidence="0.964036">Matthew Purver</author>
<affiliation confidence="0.786704666666667">Center for the Study of Language and Information Stanford University</affiliation>
<address confidence="0.990933">Stanford, CA 94305, US</address>
<email confidence="0.99936">mpurver@stanford.edu</email>
<author confidence="0.999803">Dan Jurafsky</author>
<affiliation confidence="0.999914">Department of Linguistics Stanford University</affiliation>
<address confidence="0.997528">Stanford, CA 94305, US</address>
<email confidence="0.99943">jurafsky@stanford.edu</email>
<abstract confidence="0.998873666666667">We describe an algorithm for a novel task: disamthe pronoun conversation. generic or referential; finding referential important for tasks such as addressee identification or extracting ‘owners’ of action items. Our classifier achieves 84% accuracy in two-person conversations; an initial study shows promising performance even on more complex multi-party meetings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>a</author>
</authors>
<title>so uh and if you don’t have the money then use a credit card b. I’m probably older than you</title>
<marker>a, </marker>
<rawString>(5) a. so uh and if you don’t have the money then use a credit card b. I’m probably older than you</rawString>
</citation>
<citation valid="false">
<authors>
<author>c</author>
</authors>
<title>although uh I will personally tell you I used to work at a bank</title>
<marker>c, </marker>
<rawString>c. although uh I will personally tell you I used to work at a bank</rawString>
</citation>
<citation valid="false">
<authors>
<author>d Do</author>
</authors>
<title>they survive longer if you plant them in the winter time?</title>
<marker>Do, </marker>
<rawString>d. Do they survive longer if you plant them in the winter time?</rawString>
</citation>
<citation valid="true">
<authors>
<author>e</author>
</authors>
<title>my question I guess are they really your</title>
<date>2006</date>
<marker>e, 2006</marker>
<rawString>e. my question I guess are they really your peers? J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2006. The AMI meeting corpus. In MLMI2005, Revised Selected Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>LIBSVM: a library for Support Vector Machines. Software available at http: //www.csie.ntu.edu.tw/—cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="7698" citStr="Chang and Lin, 2001" startWordPosition="1246" endWordPosition="1249">/A). 5 Experiments and Results As Table 1 shows, there are very few occurrences of the referential plural, reported referential and ambiguous classes. We therefore decided to model our problem as a two way classification task, predicting generic versus referential (collapsing referential singular and plural as one category). Note that we expect this to be the major useful distinction for our overall action-item detection task. Baseline A simple baseline involves predicting the dominant class (in the test set, referential). This gives 54.59% accuracy (see Table 1).3 SVM Results We used LIBSVM (Chang and Lin, 2001), a support vector machine classifier trained using an RBF kernel. Table 3 presents results for 3Precision and recall are of course 54.59% and 100%. (4) 106 Features Accuracy F-Score Ctxt 45.66% 0% Baseline 54.59% 70.63% Sent 67.05% 57.14% Sent + Ctxt + POS 67.05% 57.14% Sent + Ctxt + POS + QM 76.30% 72.84% Sent + Ctxt + POS + Q DA 79.19% 77.50% DA 80.92% 79.75% Sent + Ctxt + POS + 84.39% 84.21% QM + DA Table 3: SVM results: generic versus referential various selected sets of features. The best set of features gave accuracy of 84.39% and f-score 84.21%. Discussion Overall performance is respec</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for Support Vector Machines. Software available at http: //www.csie.ntu.edu.tw/—cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proceedings ofIEEE ICASSP-92.</booktitle>
<contexts>
<context position="2253" citStr="Godfrey et al., 1992" startWordPosition="356" endWordPosition="359"> distinguish between singular and plural reference, as in example (2) where the task is assigned to more than one person: A: So y- so you guys will send to the rest of us um a (2) version of um, this, and - the - uh, description - B: With sugge- yeah, suggested improvements and - Use of “you” might therefore help us both in de*This work was supported by the CALO project (DARPA grant NBCH-D-03-0010) and ONR (MURI award N000140510388). The authors also thank John Niekrasz for annotating our test data. 1(1,2) are taken from the ICSI Meeting Corpus (Shriberg et al., 2004); (3,4) from Switchboard (Godfrey et al., 1992). tecting the fact that a task is being assigned, and in identifying the owner. While there is an increasing body of work concerning addressee identification (Katzenmaier et al., 2004; Jovanovic et al., 2006), there is very little investigating the problem of second-person pronoun resolution, and it is this that we address here. Most cases of “you” do not in fact refer to the addressee but are generic, as in example (3); automatic referentiality classification is therefore very important. B: Well, usually what you do is just wait until you think it’s stopped, and then you patch them up. 2 Rela</context>
<context position="3772" citStr="Godfrey et al., 1992" startWordPosition="597" endWordPosition="600">, the referential class 22%, and the generic class 27%, with no significant differences in surface form (duration or vowel reduction) between the different cases. While there seems to be no previous work investigating automatic classification, there is related work on classifying “it”, which also takes various referential and non-referential readings: (M¨uller, 2006) use lexical and syntactic features in a rule-based classifier to detect non-referential uses, achieving raw accuracies around 74-80% and F-scores 63-69%. 3 Data We used the Switchboard corpus of two-party telephone conversations (Godfrey et al., 1992), and annotated the data with four classes: generic, referential singular, referential plural and a reported referential class, for mention in reported speech of an (1) (3) 105 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 105–108, Prague, June 2007. c�2007 Association for Computational Linguistics Training Testing Generic 360 79 Referential singular 287 92 Referential plural 17 3 Reported referential 5 1 Ambiguous 4 1 Total 673 176 Table 1: Number of cases found. originally referential use (as the original addressee may not be the current addressee – see example (4)). We allowed</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. J. Godfrey, E. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings ofIEEE ICASSP-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Holmes</author>
</authors>
<title>Generic pronouns in the Wellington corpus of spoken New Zealand English.</title>
<date>1998</date>
<journal>K¯otare,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="3014" citStr="Holmes, 1998" startWordPosition="482" endWordPosition="483">fication (Katzenmaier et al., 2004; Jovanovic et al., 2006), there is very little investigating the problem of second-person pronoun resolution, and it is this that we address here. Most cases of “you” do not in fact refer to the addressee but are generic, as in example (3); automatic referentiality classification is therefore very important. B: Well, usually what you do is just wait until you think it’s stopped, and then you patch them up. 2 Related Work Previous linguistic work has recognized that “you” is not always addressee-referring, differentiating between generic and referential uses (Holmes, 1998; Meyers, 1990) as well as idiomatic cases of “you know”. For example, (Jurafsky et al., 2002) found that “you know” covered 47% of cases, the referential class 22%, and the generic class 27%, with no significant differences in surface form (duration or vowel reduction) between the different cases. While there seems to be no previous work investigating automatic classification, there is related work on classifying “it”, which also takes various referential and non-referential readings: (M¨uller, 2006) use lexical and syntactic features in a rule-based classifier to detect non-referential uses,</context>
</contexts>
<marker>Holmes, 1998</marker>
<rawString>J. Holmes. 1998. Generic pronouns in the Wellington corpus of spoken New Zealand English. K¯otare, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Jovanovic</author>
<author>R op den Akker</author>
<author>A Nijholt</author>
</authors>
<title>Addressee identification in face-to-face meetings.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the EACL.</booktitle>
<marker>Jovanovic, den Akker, Nijholt, 2006</marker>
<rawString>N. Jovanovic, R. op den Akker, and A. Nijholt. 2006. Addressee identification in face-to-face meetings. In Proceedings of the 11th Conference of the EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>E Shriberg</author>
<author>D Biasca</author>
</authors>
<title>Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft 13.</title>
<date>1997</date>
<tech>Technical Report 97-02,</tech>
<institution>University of Colorado,</institution>
<location>Boulder.</location>
<contexts>
<context position="5495" citStr="Jurafsky et al., 1997" startWordPosition="893" endWordPosition="896"> containing “you”; the task was reported to be easy, and the kappa was 100%. We then annotated a total of 42 conversations for training and 13 for testing. Different labelers annotated the training and test sets; none of the authors were involved in labeling the test set. Table 1 presents information about the number of instances of each of these classes found. 4 Features All features used for classifier experiments were extracted from the Switchboard LDC Treebank 3 release, which includes transcripts, part of speech information using the Penn tagset (Marcus et al., 1994) and dialog act tags (Jurafsky et al., 1997). Features fell into four main categories:2 sentential features which capture lexical features of the utterance itself; part-of-speech features which capture shallow syntactic patterns; dialog act features capturing the discourse function of the current utterance and surrounding context; and context features which give oracle information (i.e., the correct generic/referential label) about preceding uses 2Currently, features are all based on perfect transcriptions. of “you”. We also investigated using the presence of a question mark in the transcription as a feature, as a possible replacement f</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft 13. Technical Report 97-02, University of Colorado, Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>A Bell</author>
<author>C Girand</author>
</authors>
<title>The role of the lemma in form variation.</title>
<date>2002</date>
<booktitle>Papers in Laboratory Phonology VII,</booktitle>
<pages>1--34</pages>
<editor>In C. Gussenhoven and N. Warner, editors,</editor>
<contexts>
<context position="3108" citStr="Jurafsky et al., 2002" startWordPosition="496" endWordPosition="499">vestigating the problem of second-person pronoun resolution, and it is this that we address here. Most cases of “you” do not in fact refer to the addressee but are generic, as in example (3); automatic referentiality classification is therefore very important. B: Well, usually what you do is just wait until you think it’s stopped, and then you patch them up. 2 Related Work Previous linguistic work has recognized that “you” is not always addressee-referring, differentiating between generic and referential uses (Holmes, 1998; Meyers, 1990) as well as idiomatic cases of “you know”. For example, (Jurafsky et al., 2002) found that “you know” covered 47% of cases, the referential class 22%, and the generic class 27%, with no significant differences in surface form (duration or vowel reduction) between the different cases. While there seems to be no previous work investigating automatic classification, there is related work on classifying “it”, which also takes various referential and non-referential readings: (M¨uller, 2006) use lexical and syntactic features in a rule-based classifier to detect non-referential uses, achieving raw accuracies around 74-80% and F-scores 63-69%. 3 Data We used the Switchboard co</context>
<context position="10907" citStr="Jurafsky et al., 2002" startWordPosition="1754" endWordPosition="1757">tire utterance and just for the word “you”. Classification results are shown in Table 4. Using only prosodic features performs below the baseline; including prosodic features with the best-performing feature set from Table 3 gives identical performance to that with lexical and contextual features alone. To see why the prosodic features did not help, we examined the difference between the average pitch, intensity and duration for referential versus generic cases (Table 5). A one-sided t-test shows no significant differences between the average intensity and duration (confirming the results of (Jurafsky et al., 2002), who found no significant change in duration). The difference in the average pitch was found to be significant (p=0.2) – but not enough for this feature alone to cause an increase in overall accuracy. 7 Error Analysis We performed an error analysis on our best classifier output on the training set; accuracy was 94.53%, giving a total of 36 errors. Half of the errors (18 of 36) were ambiguous even for humans (the authors), if looking at the sentence alone without the neighboring context from the actual conversation – see (5a). Treating these examples thus needs a detailed model of dialog conte</context>
</contexts>
<marker>Jurafsky, Bell, Girand, 2002</marker>
<rawString>D. Jurafsky, A. Bell, and C. Girand. 2002. The role of the lemma in form variation. In C. Gussenhoven and N. Warner, editors, Papers in Laboratory Phonology VII, pages 1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Katzenmaier</author>
<author>R Stiefelhagen</author>
<author>T Schultz</author>
</authors>
<title>Identifying the addressee in human-human-robot interactions based on head pose and speech.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th International Conference on Multimodal Interfaces.</booktitle>
<contexts>
<context position="2436" citStr="Katzenmaier et al., 2004" startWordPosition="387" endWordPosition="390">version of um, this, and - the - uh, description - B: With sugge- yeah, suggested improvements and - Use of “you” might therefore help us both in de*This work was supported by the CALO project (DARPA grant NBCH-D-03-0010) and ONR (MURI award N000140510388). The authors also thank John Niekrasz for annotating our test data. 1(1,2) are taken from the ICSI Meeting Corpus (Shriberg et al., 2004); (3,4) from Switchboard (Godfrey et al., 1992). tecting the fact that a task is being assigned, and in identifying the owner. While there is an increasing body of work concerning addressee identification (Katzenmaier et al., 2004; Jovanovic et al., 2006), there is very little investigating the problem of second-person pronoun resolution, and it is this that we address here. Most cases of “you” do not in fact refer to the addressee but are generic, as in example (3); automatic referentiality classification is therefore very important. B: Well, usually what you do is just wait until you think it’s stopped, and then you patch them up. 2 Related Work Previous linguistic work has recognized that “you” is not always addressee-referring, differentiating between generic and referential uses (Holmes, 1998; Meyers, 1990) as wel</context>
</contexts>
<marker>Katzenmaier, Stiefelhagen, Schultz, 2004</marker>
<rawString>M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004. Identifying the addressee in human-human-robot interactions based on head pose and speech. In Proceedings of the 6th International Conference on Multimodal Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M Marcinkiewicz</author>
<author>R MacIntyre</author>
<author>A Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The Penn treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="5451" citStr="Marcus et al., 1994" startWordPosition="885" endWordPosition="888">ed 4 conversations, yielding 85 utterances containing “you”; the task was reported to be easy, and the kappa was 100%. We then annotated a total of 42 conversations for training and 13 for testing. Different labelers annotated the training and test sets; none of the authors were involved in labeling the test set. Table 1 presents information about the number of instances of each of these classes found. 4 Features All features used for classifier experiments were extracted from the Switchboard LDC Treebank 3 release, which includes transcripts, part of speech information using the Penn tagset (Marcus et al., 1994) and dialog act tags (Jurafsky et al., 1997). Features fell into four main categories:2 sentential features which capture lexical features of the utterance itself; part-of-speech features which capture shallow syntactic patterns; dialog act features capturing the discourse function of the current utterance and surrounding context; and context features which give oracle information (i.e., the correct generic/referential label) about preceding uses 2Currently, features are all based on perfect transcriptions. of “you”. We also investigated using the presence of a question mark in the transcripti</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn treebank: Annotating predicate argument structure. In ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Meyers</author>
</authors>
<title>Current generic pronoun usage.</title>
<date>1990</date>
<journal>American Speech,</journal>
<volume>65</volume>
<issue>3</issue>
<contexts>
<context position="3029" citStr="Meyers, 1990" startWordPosition="484" endWordPosition="485">enmaier et al., 2004; Jovanovic et al., 2006), there is very little investigating the problem of second-person pronoun resolution, and it is this that we address here. Most cases of “you” do not in fact refer to the addressee but are generic, as in example (3); automatic referentiality classification is therefore very important. B: Well, usually what you do is just wait until you think it’s stopped, and then you patch them up. 2 Related Work Previous linguistic work has recognized that “you” is not always addressee-referring, differentiating between generic and referential uses (Holmes, 1998; Meyers, 1990) as well as idiomatic cases of “you know”. For example, (Jurafsky et al., 2002) found that “you know” covered 47% of cases, the referential class 22%, and the generic class 27%, with no significant differences in surface form (duration or vowel reduction) between the different cases. While there seems to be no previous work investigating automatic classification, there is related work on classifying “it”, which also takes various referential and non-referential readings: (M¨uller, 2006) use lexical and syntactic features in a rule-based classifier to detect non-referential uses, achieving raw </context>
</contexts>
<marker>Meyers, 1990</marker>
<rawString>M. W. Meyers. 1990. Current generic pronoun usage. American Speech, 65(3):228–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M¨uller</author>
</authors>
<title>Automatic detection of nonreferential It in spoken multi-party dialog.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the EACL.</booktitle>
<marker>M¨uller, 2006</marker>
<rawString>C. M¨uller. 2006. Automatic detection of nonreferential It in spoken multi-party dialog. In Proceedings of the 11th Conference of the EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>P Ehlen</author>
<author>J Niekrasz</author>
</authors>
<title>Detecting action items in multi-party meetings: Annotation and initial experiments. In MLMI2006, Revised Selected Papers.</title>
<date>2006</date>
<contexts>
<context position="1175" citStr="Purver et al., 2006" startWordPosition="163" endWordPosition="166">rential you is important for tasks such as addressee identification or extracting ‘owners’ of action items. Our classifier achieves 84% accuracy in two-person conversations; an initial study shows promising performance even on more complex multi-party meetings. 1 Introduction and Background This paper describes an algorithm for disambiguating the generic and referential senses of the pronoun you. Our overall aim is the extraction of action items from multi-party human-human conversations, concrete decisions in which one (or more) individuals take on a group commitment to perform a given task (Purver et al., 2006). Besides identifying the task itself, it is crucial to determine the owner, or person responsible. Occasionally, the name of the responsible party is mentioned explicitly. More usually, the owner is addressed directly and therefore referred to using a second-person pronoun, as in example (1).1 A: and um if you can get that binding point also maybe with a nice example that would be helpful for Johno and me. B: Oh yeah uh O K. It can also be important to distinguish between singular and plural reference, as in example (2) where the task is assigned to more than one person: A: So y- so you guys </context>
</contexts>
<marker>Purver, Ehlen, Niekrasz, 2006</marker>
<rawString>M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting action items in multi-party meetings: Annotation and initial experiments. In MLMI2006, Revised Selected Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>J Ang</author>
<author>H Carvey</author>
</authors>
<title>The ICSI Meeting Recorder Dialog Act (MRDA) Corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th SIGdial Workshop.</booktitle>
<contexts>
<context position="2206" citStr="Shriberg et al., 2004" startWordPosition="349" endWordPosition="352">. B: Oh yeah uh O K. It can also be important to distinguish between singular and plural reference, as in example (2) where the task is assigned to more than one person: A: So y- so you guys will send to the rest of us um a (2) version of um, this, and - the - uh, description - B: With sugge- yeah, suggested improvements and - Use of “you” might therefore help us both in de*This work was supported by the CALO project (DARPA grant NBCH-D-03-0010) and ONR (MURI award N000140510388). The authors also thank John Niekrasz for annotating our test data. 1(1,2) are taken from the ICSI Meeting Corpus (Shriberg et al., 2004); (3,4) from Switchboard (Godfrey et al., 1992). tecting the fact that a task is being assigned, and in identifying the owner. While there is an increasing body of work concerning addressee identification (Katzenmaier et al., 2004; Jovanovic et al., 2006), there is very little investigating the problem of second-person pronoun resolution, and it is this that we address here. Most cases of “you” do not in fact refer to the addressee but are generic, as in example (3); automatic referentiality classification is therefore very important. B: Well, usually what you do is just wait until you think i</context>
</contexts>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004. The ICSI Meeting Recorder Dialog Act (MRDA) Corpus. In Proceedings of the 5th SIGdial Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>C V Ess-Dykema</author>
<author>R Martin</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="9266" citStr="Stolcke et al., 2000" startWordPosition="1505" endWordPosition="1508">g acts. We used manually produced dialog act tags, and automatic labeling accuracy with this fine-grained tagset will be low; we would therefore prefer to use more robust features if possible. We found that one such heuristic feature, the presence of question mark, cannot entirely substitute: accuracy is reduced to 76.3%. However, using only the binary Q DA feature (which clusters together all the different kinds of question DAs) does better (79.19%). Although worse than performance with a full tagset, this gives hope that using a coarse-grained set of tags might allow reasonable results. As (Stolcke et al., 2000) report good accuracy (87%) for statement vs. question classification on manual Switchboard transcripts, such coarse-grained information might be reliably available. Surprisingly, using the oracle context features (the correct classification for the previous you) alone performs worse than the baseline; and adding these features to sentential features gives no improvement. This suggests that the generic/referential status of each you may be independent of previous yous. Features Accuracy F-Score Prosodic only 46.66% 44.31% Baseline 54.59% 70.63% Sent + Ctxt + POS + 84.39% 84.21% QM + DA + Proso</context>
</contexts>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Ess-Dykema, Martin, Meteer, 2000</marker>
<rawString>A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, C. V. Ess-Dykema, R. Martin, and M. Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–373.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>