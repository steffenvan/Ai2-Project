<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.119043">
<title confidence="0.978438">
Persistent Information State in a Data-Centric Architecture*
</title>
<author confidence="0.999726">
Sebastian Varges, Giuseppe Riccardi, Silvia Quarteroni
</author>
<affiliation confidence="0.998525">
Department of Information Engineering and Computer Science
University of Trento
</affiliation>
<address confidence="0.714498">
38050 Povo di Trento, Italy
</address>
<email confidence="0.998845">
{varges|riccardi|silviaq}@disi.unitn.it
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996571">
We present the ADAMACH data centric dia-
log system, that allows to perform on- and off-
line mining of dialog context, speech recog-
nition results and other system-generated rep-
resentations, both within and across dialogs.
The architecture implements a “fat pipeline”
for speech and language processing. We detail
how the approach integrates domain knowl-
edge and evolving empirical data, based on a
user study in the University Helpdesk domain.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997947823529412">
In this paper, we argue that the ability to store and
query large amounts of data is a key requirement
for data-driven dialog systems, in which the data is
generated by the spoken dialog system (SDS) com-
ponents (spoken language understanding (SLU), di-
alog management (DM), natural language genera-
tion (NLG) etc.) and the world it is interacting
with (news streams, ambient sensors etc.). We
describe an SDS that is built around a database
management system (DBMS), uses the web ser-
vice paradigm (in contrast to the architecture de-
scribed in (Varges and Riccardi, 2007)), and em-
ploys a Voice XML (VXML) server for interfac-
ing with Automatic Speech Recognition (ASR) and
Text-to-Speech (TTS) components. We would like
to emphasize upfront that this does not mean that
we follow a VXML dialog model.
</bodyText>
<footnote confidence="0.95850125">
&apos;This work was partially supported by the European Com-
mission Marie Curie Excellence Grant for the ADAMACH
project (contract No. 022593) and by LUNA STREP project
(contract no33549).
</footnote>
<page confidence="0.996378">
68
</page>
<bodyText confidence="0.999733275862069">
The data centric architecture we adopt has sev-
eral advantages: first, the database concentrates het-
erogeneous types of information allowing to uni-
formly query the evolving data at any time, e.g. by
performing queries across various types of infor-
mation. Second, the architecture facilitates dialog
evaluation, data mining and online learning because
data is available for querying as soon as it has been
stored. Third, multiple systems/applications can be
made available on the same infrastructure due to a
clean separation of its processing modules (SLU,
DM, NLG etc.) from data storage and persistency
(DBMS), and monitoring/analysis/visualization and
annotation tools. Fourth, there is no need for sep-
arate ‘logging’ mechanisms: the state of the SDS
is contained in the database, and is therefore persis-
tently available for analysis after the dialog ends.
As opposed to the presented architecture, the
Open Agent Architecture (OAA) (Martin et al.,
1999) and DARPA Communicator (Seneff et al.,
1998) treat data as peripheral: they were not specif-
ically designed to handle large volumes of data, and
data is not automatically persistent. In contrast to
the CSLI-DM (Mirkovic and Cavedon, 2005), and
TrindiKit (Larsson and Traum, 2000), but similar
to Communicator, the ADAMACH architecture is
server-based, thus enabling continuous operation.
To prove our concept, we test it on a University
helpdesk application (section 4).
</bodyText>
<sectionHeader confidence="0.965842" genericHeader="method">
2 Dialog System Architecture
</sectionHeader>
<bodyText confidence="0.973472666666667">
Figure 1 shows our vision for the architecture of the
ADAMACH system. We implemented and evalu-
ated the speech modality based core of this system
</bodyText>
<note confidence="0.4909">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 68–71,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999671">
Figure 1: Architecture vision
</figureCaption>
<bodyText confidence="0.995732962962963">
(figure 2). A typical interaction is initiated by a
phone call that arrives at an telephony server which
routes it to a VXML platform. A VXML page is
continuously rewritten by the dialog manager, con-
taining the system utterance and other TTS param-
eters, and the ASR recognition parameters for the
next user utterance. Thus, VXML is used as a low-
level interface to the ASR and TTS engines, but not
for representing dialog strategies. Once a user utter-
ance is recognized, a web service request is issued
to a dialog management server.
All communication between the above-mentioned
components is stored in the DBMS: ASR recogni-
tion results, TTS parameters and ASR recognition
parameters reside in separate tables. The dialog
manager uses the basic tables as its communication
protocol with ASR and TTS engines, and addition-
ally stores its Information State (IS) in the database.
This means that the IS is automatically persistent,
and that dialog management becomes a function that
maps ASR results and old IS to the TTS and ASR
parameters and a new IS. The tables of the database
are organized into turns, several of which belong to a
call (dialog), thus resulting in a tree structure that is
enforced by foreign key constraints in the relational
database.
The VXML standard is based on the web infras-
tructure. In particular, a VXML platform can issue
HTTP requests that can be served by a web server
just like any (HTML) page. The VXML server only
sees the generated VXML page, the ‘return value’
of the HTTP request. This allows us to organize the
processing modules of the dialog system (SLU, DM,
VXML generator) as web services that are invoked
by the HTTP request. As a consequence, each sys-
tem turn of a dialog is a separate, stateless request.
The state of the dialog is stored in the database.
Furthermore, by threading the VXML session ID
through the processing loop (including the VXML
pages generated on-the-fly) and distinguishing en-
tries in the DB by sessions, the SDS is inherently
parallelizable, just as a conventional web server can
serve many users in parallel. Figure 2 shows how
information is processed for each turn. The HTTP
requests that invoke the processing modules pass on
various IDs and parameters, but the actual data is
stored in the DB and retrieved only if a processing
module requires it. This effectively implements a
‘fat pipeline’: each speech, language and DM mod-
ule has access to the database for rescoring and mod-
eling (i.e. data within and across dialogs). At the im-
plementation level, this balances a lightweight com-
munication protocol downstream with data flowing
laterally towards the database.
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="method">
3 Dialog Management
</sectionHeader>
<bodyText confidence="0.9998926">
Dialog management works in two stages: retriev-
ing and preprocessing facts (tuples) taken from the
database, and inferencing over those facts to gen-
erate a system response. We distinguish between
the ‘context model’ of the first phase and the ‘dialog
move engine’ (DME) of the second phase (Larsson
and Traum, 2000).
The first stage entails retrieving from the persis-
tent Information State the following information:
all open questions for the current dialog from the
database, any application information already pro-
vided by the user (including their grounding status),
the ASR recognition results of last user turn, and
confidence and other thresholds. The context model
that is applied when retrieving the relevant dialog
history from the database can be characterized as a
‘linear default model’: application parameters pro-
vided by the user, such as student ID, are overrid-
den if the user provides a new value, for example to
correct a previous misunderstanding. Task bound-
aries are detected and prevent an application param-
eter from carrying over directly to the new task.
The second stage employs an inference engine
to determine the system action and response: SLU
rules match the user utterance to open questions.
</bodyText>
<page confidence="0.997674">
69
</page>
<figureCaption confidence="0.999646">
Figure 2: Turn-level information flow
</figureCaption>
<bodyText confidence="0.998218285714286">
This may result in the decision to verify the applica-
tion parameter in question, and the action is verbal-
ized by language generation rules. If the parameter
is accepted, application dependent task rules deter-
mine the next parameter to be acquired, resulting in
the generation of an appropriate request. For reasons
of space, we cannot provide more details here.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999979240740741">
Our current application is a University helpdesk
in Italian which students call to perform 5 tasks:
receive information about exams (times, rooms
... ), subscribe/cancel subscriptions to exams, obtain
exam mark, or request to talk to an operator. Follow-
ing experimentations, we annotated the dialogs and
conducted performance statistics using the system’s
built-in annotation tool.
Two Italian mothertongues were in charge of
manually annotating a total of 423 interactions.
Each annotator independently annotated each dialog
turn according to whether one of the five available
tasks was being requested or completed in it. To
compute inter-annotator agreement, 24 dialogs were
processed by both annotators; the remaining ones
were partitioned equally among them.
We computed agreement at both turn and dialog
level. Turn level agreement is concerned with which
tasks are requested and completed at a given dia-
log turn according to each annotator. An agree-
ment matrix is compiled where rows and columns
correspond to the five task types in our application.
Cohen’s n (Cohen, 1960), computed over the turn
matrix, gave a turn agreement of 0.72 resp. 0.77
for requests resp. completions, exceeding the rec-
ommended 0.7 threshold. While turn-level agree-
ment refers to which tasks occurred and at what
turn, dialog level agreement refers to how many task
requests/completions occurred. Also at the dialog
level, the n statistic gave good results (0.71 for re-
quests and 0.9 for completions).
General dialog statistics The average duration of
the 423 annotated dialogs is 63.1 seconds, with an
average of 7.43 turn (i.e. adjacency) pairs. 356 of
the dialogs contained at least one task; the majority
(338) contained exactly one, 17 dialogs contained 2
tasks, and one dialog contained 3. In the remain-
ing 67 dialogs, no tasks were detected: from the
audio files, it seems that these generally happened
by accident or in noisy environments, hence noin-
put/hangup events occurred shortly after the initial
system prompt.
Furthermore, relative frequencies of task requests
and task completions are reported in Table 1. In to-
tal, according to the two annotators, there were 375
task requests and 234 task completions. Among the
requested tasks, the vast majority was composed by
“Get exam mark” –a striking 96%– while “Exam
withdrawal” never occurred and the three others
were barely performed. Indeed, it seems that stu-
dents preferred to use the system to carry on “in-
formative” tasks such as obtaining exam marks and
general information rather than “active” tasks such
as exam subscription and withdrawal.
</bodyText>
<tableCaption confidence="0.9995">
Table 1: Task request and completion frequencies (%)
</tableCaption>
<table confidence="0.996284571428572">
Task Request Completion
Get exam mark 96 (360) 96.6 (226)
Info on exam 1.9 (7) 1.7 (4)
Exam subscription 1.1 (4) 0.4 (1)
Exam withdrawal 0.0 (0) 0.0 (0)
Talk to operator 1.1 (4) 1.3 (3)
Total 100 (375) 100 (234)
</table>
<bodyText confidence="0.998046333333333">
Task and dialog success Based on the annotation
of task requests and completions, we defined task
success as a binary measure of whether the request
of a given task type is eventually followed by a task
completion of the same type. Table 2 reports the av-
erage success of each task type according to the an-
</bodyText>
<page confidence="0.995346">
70
</page>
<bodyText confidence="0.9344485">
notators1. Our results show that the most frequently
requested type, “Get exam mark”, has a 64.64% suc-
cess rate (it seems that failure was mostly due to the
system’s inability to recognize student IDs).
</bodyText>
<tableCaption confidence="0.9921132">
Table 2: Top: annotator (srM) and automatic (srA) task
success rates. Mean ± binomial proportion confidence
interval on the average task success (α= 95%) is reported.
Bottom: mean annotator (dsrM) and automatic (dsrA)
dialog success rates ± normal law c.i. (α= 95%).
</tableCaption>
<table confidence="0.998694555555556">
Task srM(%) srA(%)
Get exam mark 64.64 77.97
Info on exam 57.14 71.43
Exam subscription 25 100
Exam withdrawal - -
Talk to operator 75 75
Average 64.17±4.96 78.06±4.28
Dialog dsrM(%) dsrA(%)
Average 64.47±4.95 88.31±9.2
</table>
<bodyText confidence="0.99993356">
In fact, while it is straightforward to obtain task
success information using the manual annotation of
dialogs, when the dialog system cannot rely on hu-
man judgments, unsupervised approaches must be
defined for a rapid (on-line or off-line) evaluation.
For this purpose, an automatic approximation of the
“manual” task success estimation has been defined
using a set of database queries associated to each
task type. For instance, the task success query as-
sociated to “Info on exam” checks that two condi-
tions are met in the current dialog: 1) it includes
a turn where an action is requested the interpreta-
tion of which contains “information”; 2) it contains
a turn where the concept Exam Name is in focus.
Automatic task success rates have been computed
on the same dialogs for which manual task success
rates were available and are reported in Table 2, col.
2. The comparison shows that the automatic metric
srA is more “optimistic” than the manual one srM.
Indeed, automatic estimators rely on “punctual” in-
dicators (such as the occurrence of confirmations of
a given value) in the whole dialog, regardless of the
task they appear in (this information is only avail-
able from human annotation) and also of the order
with which such indicators appear in the dialog.
</bodyText>
<footnote confidence="0.925444666666667">
1As several task types occur seldom, we only report the con-
fidence intervals on the means relating to the overall (“Aver-
age”) task success, computed according to the normal law.
</footnote>
<bodyText confidence="0.998302">
As a by-product of task success evaluation, we de-
fined dialog success rate (dsr) as the average success
</bodyText>
<equation confidence="0.67255">
EtzET sr(ti)
</equation>
<bodyText confidence="0.9991711">
rate of the tasks in a dialog: dsr T ,
T being the set of requested tasks. Dependi=ng on
whether srM or srA is used, we obtain two metrics,
dsrM resp. dsrA.
Our dialog success results (last row of Table 2) are
comparable to the task success ones; also, the differ-
ence between the automatic and manual estimators
of dialog success is similar to their difference at the
the task level. This is not surprising when consider-
ing that most of the dialogs contained only one task.
</bodyText>
<sectionHeader confidence="0.999826" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999989285714286">
We have presented a data-centric Spoken Dialog
System whose novel aspect is the storage and re-
trieval of dialog management state, ASR results and
other information in a database. As a consequence,
dialog management can be lightweight and operate
on a turn-by-turn basis, and dialog system evaluation
and logging are facilitated.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999976333333333">
We would like to thank Pierluigi Roberti for helping
with the speech platform and annotation tools, and
LOQUENDO for providing the VXML platform.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999420428571428">
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37–46.
S. Larsson and D. Traum. 2000. Information State and
dialogue management in the TRINDI Dialogue Move
Engine Toolkit. Natural Language Engineering, 6(3–
4):323–340.
D. L. Martin, A. J. Cheyer, and D. B. Moran. 1999. The
Open Agent Architecture: A framework for building
distributed software systems. Applied Artificial Intel-
ligence: An International Journal, 13(1-2):91–128.
D. Mirkovic and L. Cavedon. 2005. Practical Plug-and-
Play Dialogue Management. In Proceedings of PA-
CLING, Tokyo, Japan.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. GALAXY-II: A reference architecture
for conversational system development. In Proc. of
ICSLP 1998, Sydney, Australia.
S. Varges and G. Riccardi. 2007. A data-centric archi-
tecture for data-driven spoken dialogue systems. In
Proceedings of ASRU, Kyoto, Japan.
</reference>
<page confidence="0.999144">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972579">
<title confidence="0.999853">Information State in a Data-Centric</title>
<author confidence="0.998754">Sebastian Varges</author>
<author confidence="0.998754">Giuseppe Riccardi</author>
<author confidence="0.998754">Silvia</author>
<affiliation confidence="0.999229">Department of Information Engineering and Computer University of</affiliation>
<address confidence="0.996981">38050 Povo di Trento, Italy</address>
<abstract confidence="0.997982272727273">We present the ADAMACH data centric dialog system, that allows to perform onand offline mining of dialog context, speech recognition results and other system-generated representations, both within and across dialogs. The architecture implements a “fat pipeline” for speech and language processing. We detail how the approach integrates domain knowledge and evolving empirical data, based on a user study in the University Helpdesk domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="8868" citStr="Cohen, 1960" startWordPosition="1404" endWordPosition="1405">tions. Each annotator independently annotated each dialog turn according to whether one of the five available tasks was being requested or completed in it. To compute inter-annotator agreement, 24 dialogs were processed by both annotators; the remaining ones were partitioned equally among them. We computed agreement at both turn and dialog level. Turn level agreement is concerned with which tasks are requested and completed at a given dialog turn according to each annotator. An agreement matrix is compiled where rows and columns correspond to the five task types in our application. Cohen’s n (Cohen, 1960), computed over the turn matrix, gave a turn agreement of 0.72 resp. 0.77 for requests resp. completions, exceeding the recommended 0.7 threshold. While turn-level agreement refers to which tasks occurred and at what turn, dialog level agreement refers to how many task requests/completions occurred. Also at the dialog level, the n statistic gave good results (0.71 for requests and 0.9 for completions). General dialog statistics The average duration of the 423 annotated dialogs is 63.1 seconds, with an average of 7.43 turn (i.e. adjacency) pairs. 356 of the dialogs contained at least one task; </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Larsson</author>
<author>D Traum</author>
</authors>
<title>Information State and dialogue management in the TRINDI Dialogue Move Engine Toolkit.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>4--323</pages>
<contexts>
<context position="2947" citStr="Larsson and Traum, 2000" startWordPosition="449" endWordPosition="452"> and monitoring/analysis/visualization and annotation tools. Fourth, there is no need for separate ‘logging’ mechanisms: the state of the SDS is contained in the database, and is therefore persistently available for analysis after the dialog ends. As opposed to the presented architecture, the Open Agent Architecture (OAA) (Martin et al., 1999) and DARPA Communicator (Seneff et al., 1998) treat data as peripheral: they were not specifically designed to handle large volumes of data, and data is not automatically persistent. In contrast to the CSLI-DM (Mirkovic and Cavedon, 2005), and TrindiKit (Larsson and Traum, 2000), but similar to Communicator, the ADAMACH architecture is server-based, thus enabling continuous operation. To prove our concept, we test it on a University helpdesk application (section 4). 2 Dialog System Architecture Figure 1 shows our vision for the architecture of the ADAMACH system. We implemented and evaluated the speech modality based core of this system Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 68–71, Columbus, June 2008. c�2008 Association for Computational Linguistics Figure 1: Architecture vision (figure 2). A typical interaction is initiated by a ph</context>
<context position="6464" citStr="Larsson and Traum, 2000" startWordPosition="1027" endWordPosition="1030">s a ‘fat pipeline’: each speech, language and DM module has access to the database for rescoring and modeling (i.e. data within and across dialogs). At the implementation level, this balances a lightweight communication protocol downstream with data flowing laterally towards the database. 3 Dialog Management Dialog management works in two stages: retrieving and preprocessing facts (tuples) taken from the database, and inferencing over those facts to generate a system response. We distinguish between the ‘context model’ of the first phase and the ‘dialog move engine’ (DME) of the second phase (Larsson and Traum, 2000). The first stage entails retrieving from the persistent Information State the following information: all open questions for the current dialog from the database, any application information already provided by the user (including their grounding status), the ASR recognition results of last user turn, and confidence and other thresholds. The context model that is applied when retrieving the relevant dialog history from the database can be characterized as a ‘linear default model’: application parameters provided by the user, such as student ID, are overridden if the user provides a new value, </context>
</contexts>
<marker>Larsson, Traum, 2000</marker>
<rawString>S. Larsson and D. Traum. 2000. Information State and dialogue management in the TRINDI Dialogue Move Engine Toolkit. Natural Language Engineering, 6(3– 4):323–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Martin</author>
<author>A J Cheyer</author>
<author>D B Moran</author>
</authors>
<title>The Open Agent Architecture: A framework for building distributed software systems.</title>
<date>1999</date>
<journal>Applied Artificial Intelligence: An International Journal,</journal>
<pages>13--1</pages>
<contexts>
<context position="2668" citStr="Martin et al., 1999" startWordPosition="405" endWordPosition="408">rning because data is available for querying as soon as it has been stored. Third, multiple systems/applications can be made available on the same infrastructure due to a clean separation of its processing modules (SLU, DM, NLG etc.) from data storage and persistency (DBMS), and monitoring/analysis/visualization and annotation tools. Fourth, there is no need for separate ‘logging’ mechanisms: the state of the SDS is contained in the database, and is therefore persistently available for analysis after the dialog ends. As opposed to the presented architecture, the Open Agent Architecture (OAA) (Martin et al., 1999) and DARPA Communicator (Seneff et al., 1998) treat data as peripheral: they were not specifically designed to handle large volumes of data, and data is not automatically persistent. In contrast to the CSLI-DM (Mirkovic and Cavedon, 2005), and TrindiKit (Larsson and Traum, 2000), but similar to Communicator, the ADAMACH architecture is server-based, thus enabling continuous operation. To prove our concept, we test it on a University helpdesk application (section 4). 2 Dialog System Architecture Figure 1 shows our vision for the architecture of the ADAMACH system. We implemented and evaluated t</context>
</contexts>
<marker>Martin, Cheyer, Moran, 1999</marker>
<rawString>D. L. Martin, A. J. Cheyer, and D. B. Moran. 1999. The Open Agent Architecture: A framework for building distributed software systems. Applied Artificial Intelligence: An International Journal, 13(1-2):91–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mirkovic</author>
<author>L Cavedon</author>
</authors>
<title>Practical Plug-andPlay Dialogue Management.</title>
<date>2005</date>
<booktitle>In Proceedings of</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2906" citStr="Mirkovic and Cavedon, 2005" startWordPosition="443" endWordPosition="446">.) from data storage and persistency (DBMS), and monitoring/analysis/visualization and annotation tools. Fourth, there is no need for separate ‘logging’ mechanisms: the state of the SDS is contained in the database, and is therefore persistently available for analysis after the dialog ends. As opposed to the presented architecture, the Open Agent Architecture (OAA) (Martin et al., 1999) and DARPA Communicator (Seneff et al., 1998) treat data as peripheral: they were not specifically designed to handle large volumes of data, and data is not automatically persistent. In contrast to the CSLI-DM (Mirkovic and Cavedon, 2005), and TrindiKit (Larsson and Traum, 2000), but similar to Communicator, the ADAMACH architecture is server-based, thus enabling continuous operation. To prove our concept, we test it on a University helpdesk application (section 4). 2 Dialog System Architecture Figure 1 shows our vision for the architecture of the ADAMACH system. We implemented and evaluated the speech modality based core of this system Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 68–71, Columbus, June 2008. c�2008 Association for Computational Linguistics Figure 1: Architecture vision (figure 2). A</context>
</contexts>
<marker>Mirkovic, Cavedon, 2005</marker>
<rawString>D. Mirkovic and L. Cavedon. 2005. Practical Plug-andPlay Dialogue Management. In Proceedings of PACLING, Tokyo, Japan. S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and V. Zue. 1998. GALAXY-II: A reference architecture for conversational system development. In Proc. of ICSLP 1998, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Varges</author>
<author>G Riccardi</author>
</authors>
<title>A data-centric architecture for data-driven spoken dialogue systems.</title>
<date>2007</date>
<booktitle>In Proceedings of ASRU, Kyoto,</booktitle>
<contexts>
<context position="1297" citStr="Varges and Riccardi, 2007" startWordPosition="192" endWordPosition="195">r study in the University Helpdesk domain. 1 Introduction In this paper, we argue that the ability to store and query large amounts of data is a key requirement for data-driven dialog systems, in which the data is generated by the spoken dialog system (SDS) components (spoken language understanding (SLU), dialog management (DM), natural language generation (NLG) etc.) and the world it is interacting with (news streams, ambient sensors etc.). We describe an SDS that is built around a database management system (DBMS), uses the web service paradigm (in contrast to the architecture described in (Varges and Riccardi, 2007)), and employs a Voice XML (VXML) server for interfacing with Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) components. We would like to emphasize upfront that this does not mean that we follow a VXML dialog model. &apos;This work was partially supported by the European Commission Marie Curie Excellence Grant for the ADAMACH project (contract No. 022593) and by LUNA STREP project (contract no33549). 68 The data centric architecture we adopt has several advantages: first, the database concentrates heterogeneous types of information allowing to uniformly query the evolving data at any t</context>
</contexts>
<marker>Varges, Riccardi, 2007</marker>
<rawString>S. Varges and G. Riccardi. 2007. A data-centric architecture for data-driven spoken dialogue systems. In Proceedings of ASRU, Kyoto, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>