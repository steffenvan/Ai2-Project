<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.994948">
Profile Based Cross-Document Coreference
Using Kernelized Fuzzy Relational Clustering
</title>
<author confidence="0.953173">
Jian Huang† Sarah M. Taylor‡ Jonathan L. Smith‡ Konstantinos A. Fotiadis‡ C. Lee Giles††College of Information Sciences and Technology
</author>
<affiliation confidence="0.999208">
Pennsylvania State University, University Park, PA 16802, USA
</affiliation>
<email confidence="0.968366">
{jhuang, giles}@ist.psu.edu
</email>
<affiliation confidence="0.481039">
‡Advanced Technology Office, Lockheed Martin IS&amp;GS, Arlington, VA 22203, USA
</affiliation>
<email confidence="0.987731">
{sarah.m.taylor, jonathan.l.smith, konstantinos.a.fotiadis}@lmco.com
</email>
<sectionHeader confidence="0.994779" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876846153846">
Coreferencing entities across documents
in a large corpus enables advanced
document understanding tasks such as
question answering. This paper presents
a novel cross document coreference
approach that leverages the profiles
of entities which are constructed by
using information extraction tools and
reconciled by using a within-document
coreference module. We propose to
match the profiles by using a learned
ensemble distance function comprised
of a suite of similarity specialists. We
develop a kernelized soft relational
clustering algorithm that makes use of
the learned distance function to partition
the entities into fuzzy sets of identities.
We compare the kernelized clustering
method with a popular fuzzy relation
clustering algorithm (FRC) and show 5%
improvement in coreference performance.
Evaluation of our proposed methods
on a large benchmark disambiguation
collection shows that they compare
favorably with the top runs in the
SemEval evaluation.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911096153846">
A named entity that represents a person, an or-
ganization or a geo-location may appear within
and across documents in different forms. Cross
document coreference (CDC) is the task of con-
solidating named entities that appear in multiple
documents according to their real referents. CDC
is a stepping stone for achieving intelligent in-
formation access to vast and heterogeneous text
corpora, which includes advanced NLP techniques
such as document summarization and question an-
swering. A related and well studied task is within
document coreference (WDC), which limits the
scope of disambiguation to within the boundary of
a document. When namesakes appear in an article,
the author can explicitly help to disambiguate, us-
ing titles and suffixes (as in the example, “George
Bush Sr. ... the younger Bush”) besides other
means. Cross document coreference, on the other
hand, is a more challenging task because these
linguistics cues and sentence structures no longer
apply, given the wide variety of context and styles
in different documents.
Cross document coreference research has re-
cently become more popular due to the increasing
interests in the web person search task (Artiles
et al., 2007). Here, a search query for a person
name is entered into a search engine and the
desired outputs are documents clustered according
to the identities of the entities in question. In
our work, we propose to drill down to the sub-
document mention level and construct an entity
profile with the support of information extraction
tools and reconciled with WDC methods. Hence
our IE based approach has access to accurate
information such as a person’s mentions and geo-
locations for disambiguation. Simple IR based
CDC approaches (e.g. (Gooi and Allan, 2004)), on
the other hand, may simply use all the terms and
this can be detrimental to accuracy. For example, a
biography of John F. Kennedy is likely to mention
members of his family with related positions,
besides references to other political figures. Even
with careful word selection, these textual features
can still confuse the disambiguation system about
the true identity of the person.
We propose to handle the CDC task using a
novel kernelized fuzzy relational clustering algo-
rithm, which allows probabilistic cluster mem-
bership assignment. This not only addresses the
intrinsic uncertainty nature of the CDC problem,
but also yields additional performance improve-
ment. We propose to use a specialist ensemble
</bodyText>
<page confidence="0.976533">
414
</page>
<note confidence="0.999613">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 414–422,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999789375">
learning approach to aggregate the diverse set of
similarities in comparing attributes and relation-
ships in entity profiles. Our approach is first fully
described in Section 2. The effectiveness of the
proposed method is demonstrated using real world
benchmark test sets in Section 3. We review
related work in cross document coreference and
conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.999418" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.999628">
2.1 Document Level and Profile Based CDC
</subsectionHeader>
<bodyText confidence="0.999957545454546">
We make distinctions between document level and
profile based cross document coreference. Docu-
ment level CDC makes a simplifying assumption
that a named entity (and its variants) in a document
has one underlying real identity. The assump-
tion is generally acceptable but may be violated
when a document refers to namesakes at the same
time (e.g. George W. Bush and George H. W.
Bush referred to as George or President Bush).
Furthermore, the context surrounding the person
NE President Clinton can be counterproductive
for disambiguating the NE Senator Clinton, with
both entities likely to appear in a document at the
same time. The simplified document level CDC
has nevertheless been used in the WePS evaluation
(Artiles et al., 2007), called the web people task.
In this work, we advocate profile based disam-
biguation that aims to leverage the advances in
NLP techniques. Rather than treating a document
as simply a bag of words, an information extrac-
tion tool first extracts NE’s and their relationships.
For the NE’s of interest (i.e. persons in this work),
a within-document coreference (WDC) module
then links the entities deemed as referring to
the same underlying identity into a WDC chain.
This process includes both anaphora resolution
(resolving ‘He’ and its antecedent ‘President Clin-
ton’) and entity tracking (resolving ‘Bill’ and
‘President Clinton’). Let £ = {e1,..., eN} denote
the set of N chained entities (each corresponding
to a WDC chain), provided as input to the CDC
system. We intentionally do not distinguish which
document each ej belongs to, as profile based
CDC can potentially rectify WDC errors by lever-
aging information across document boundaries.
Each ei is represented as a profile which contains
the NE, its attributes and associated relationships,
i.e. ej =&lt; ej,1, ..., ej,L &gt; (ej,l can be a textual
attribute or a pointer to another entity). The profile
based CDC method generates a partition of £,
represented by a partition matrix U (where uij
denotes the membership of an entity ej to the i-
th identity cluster). Therefore, the chained entities
placed in a name cluster are deemed as coreferent.
Profile based CDC addresses a finer grained
coreference problem in the mention level, enabled
by the recent advances in IE and WDC techniques.
In addition, profile based CDC facilitates user
information consumption with structured informa-
tion and short summary passages. Next, we focus
on the relational clustering algorithm that lies at
the core of the profile based CDC system. We then
turn our attention to the specialist learning algo-
rithm for the distance function used in clustering,
capable of leveraging the available training data.
</bodyText>
<subsectionHeader confidence="0.97498">
2.2 CDC Using Fuzzy Relational Clustering
2.2.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.968512416666667">
Traditionally, hard clustering algorithms (where
uij E {0,1}) such as complete linkage hierarchi-
cal agglomerative clustering (Mann and Yarowsky,
2003) have been applied to the disambiguation
problem. In this work, we propose to use fuzzy
clustering methods (relaxing the membership con-
dition to uij E [0, 1]) as a better way of handling
uncertainty in cross document coreference. First,
consider the following motivating example,
Example. The named entity President Bush is
extracted from the sentence “President Bush ad-
dressed the nation from the Oval Office Monday.”
</bodyText>
<listItem confidence="0.82138675">
• Without additional cues, a hard clustering
algorithm has to arbitrarily assign the
mention “President Bush” to either the NE
“George W. Bush” or “George H. W. Bush”.
• A soft clustering algorithm, on the other
hand, can assign equal probability to the two
identities, indicating low entropy or high
uncertainty in the solution. Additionally, the
soft clustering algorithm can assign lower
probability to the identity “Governor Jeb
Bush”, reflecting a less likely (though not
impossible) coreference decision.
</listItem>
<bodyText confidence="0.998133666666667">
We first formalize the cross document corefer-
ence problem as a soft clustering problem, which
minimizes the following objective function:
</bodyText>
<equation confidence="0.997618666666667">
C N
JC(£) =
i=1 E umija2(ej,vi) (1)
C N
s.t. uij = 1 and E uij &gt; 0, uij E [0, 1]
i=1 j=1
</equation>
<page confidence="0.986788">
415
</page>
<bodyText confidence="0.999991637931035">
where vi is a virtual (implicit) prototype of the i-th
cluster (ej, vi E D) and m controls the fuzziness
of the solution (m &gt; 1; the solution approaches
hard clustering as m approaches 1). We will
further explain the generic distance function d :
D x D —* R in the next subsection. The goal
of the optimization is to minimize the sum of
deviations of patterns to the cluster prototypes.
The clustering solution is a fuzzy partition Pθ =
{Ci}, where ej E Ci if and only if uij &gt; 0.
We note from the outset that the optimization
functional has the same form as the classical
Fuzzy C-Means (FCM) algorithm (Bezdek, 1981),
but major differences exist. FCM, as most ob-
ject clustering algorithms, deals with object data
represented in a vectorial form. In our case, the
data is purely relational and only the mutual rela-
tionships between entities can be determined. To
be exact, we can define the similarity/dissimilarity
between a pair of attributes or relationships of
the same type l between entities ej and ek as
s(l)(ej, ek). For instance, the similarity between
the occupations ‘President’ and ‘Commander in
Chief’ can be computed using the JC semantic
distance (Jiang and Conrath, 1997) with WordNet;
the similarity of co-occurrence with other people
can be measured by the Jaccard coefficient. In the
next section, we propose to compute the relation
strength r(·, ·) from the component similarities
using aggregation weights learned from training
data. Hence the N chained entities to be clustered
can be represented as relational data using an nxn
matrix R, where rj,k = r(ej, ek). The Any Rela-
tion Clustering Algorithm (ARCA) (Corsini et al.,
2005; Cimino et al., 2006) represents relational
data as object data using their mutual relation
strength and uses FCM for clustering. We adopt
this approach to transform (objectify) a relational
pattern ej into an N dimensional vector rj (i.e.
the j-th row in the matrix R) using a mapping
Θ : D —* RN. In other words, each chained entity
is represented as a vector of its relation strengths
with all the entities. Fuzzy clusters can then
be obtained by grouping closely related patterns
using object clustering algorithm.
Furthermore, it is well known that FCM
is a spherical clustering algorithm and thus
is not generally applicable to relational data
which may yield relational clusters of arbitrary
and complicated shapes. Also, the distance in
the transformed space may be non-Euclidean,
rendering many clustering algorithms ineffective
(many FCM extensions theoretically require
the underlying distance to satisfy certain metric
properties). In this work, we propose kernelized
ARCA (called KARC) which uses a kernel-
induced metric to handle the objectified relational
data, as we introduce next.
</bodyText>
<subsectionHeader confidence="0.547124">
2.2.2 Kernelized Fuzzy Clustering
</subsectionHeader>
<bodyText confidence="0.999979884615385">
Kernelization (Sch¨olkopf and Smola, 2002) is a
machine learning technique to transform patterns
in the data space to a high-dimensional feature
space so that the structure of the data can be more
easily and adequately discovered. Specifically, a
nonlinear transformation Φ maps data in RN to
H of possibly infinite dimensions (Hilbert space).
The key idea is the kernel trick – without explicitly
specifying Φ and H, the inner product in H can
be computed by evaluating a kernel function K in
the data space, i.e. &lt; Φ(ri), Φ(rj) &gt;= K(ri, rj)
(one of the most frequently used kernel func-
tions is the Gaussian RBF kernel: K(rj, rk) =
exp(−AIIrj − rkII2)). This technique has been
successfully applied to SVMs to classify non-
linearly separable data (Vapnik, 1995). Kerneliza-
tion preserves the simplicity in the formalism of
the underlying clustering algorithm, meanwhile it
yields highly nonlinear boundaries so that spheri-
cal clustering algorithms can apply (e.g. (Zhang
and Chen, 2003) developed a kernelized object
clustering algorithm based on FCM).
Let wi denote the objectified virtual cluster vi,
i.e. wi = Θ(vi). Using the kernel trick, the
squared distance between Φ(rj) and Φ(wi) in the
feature space H can be computed as:
</bodyText>
<equation confidence="0.994167142857143">
IIΦ(rj) − Φ(wi)II2 (2)
H
= &lt; Φ(rj) − Φ(wi),Φ(rj) − Φ(wi) &gt;
= &lt; Φ(rj), Φ(rj) &gt; −2 &lt; Φ(rj), Φ(wi) &gt;
+ &lt; Φ(wi), Φ(wi) &gt;
= 2 − 2K(rj, wi) (3)
assuming K(r, r) = 1. The KARC algorithm
</equation>
<bodyText confidence="0.993037">
defines the generic distance d as d2(ej, vi) def =
IIΦ(rj) − Φ(wi)II2H = IIΦ(Θ(ej)) − Φ(Θ(vi))II2H
(we also use d2ji as a notational shorthand).
Using Lagrange Multiplier as in FCM, the opti-
mal solution for Equation (1) is:
</bodyText>
<equation confidence="0.979598647058823">
L1 o (d
2.P J J,P 0)
1 E\ , (d2ji = 0)
(4)
�
��
��
uij =
416
N
P
k=1
umikΦ(rk)
Φ(wi) = N
k=1
P uik
m
</equation>
<bodyText confidence="0.99823175">
Since Φ is an implicit mapping, Eq. (5) can
not be explicitly evaluated. On the other hand,
plugging Eq. (5) into Eq. (3), d2ji can be explicitly
represented by using the kernel matrix,
</bodyText>
<equation confidence="0.988185142857143">
d2ji = 2 − 2 · N (6)
P umikK(rj, rk)
k=1
N
P uik
m
k=1
</equation>
<bodyText confidence="0.98312385">
With the derivation, the kernelized fuzzy clus-
tering algorithm KARC works as follows. The
chained entities £ are first objectified into the
relation strength matrix R using SEG, the details
of which are described in the following section.
The Gram matrix K is then computed based on
the relation strength vectors using the kernel func-
tion. For a given number of clusters C, the
initialization step is done by randomly picking C
patterns as cluster centers, equivalently, C indices
{n1,.., nC} are randomly picked from {1, .., N}.
D0 is initialized by setting d2ji = 2 − 2K(rj, rni).
KARC alternately updates the membership matrix
U and the kernel distance matrix D until conver-
gence or running more than maxIter iterations
(Algorithm 1). Finally, the soft partition is gen-
erated based on the membership matrix U, which
is the desired cross document coreference result.
Algorithm 1 KARC Alternating Optimization
Input: Gram matrix K; #Clusters C; threshold B
</bodyText>
<equation confidence="0.9626746875">
initialize D0
t +— 0
repeat
t +— t + 1
// 1– Update membership matrix Ut:
uij = PC 2 −M−1
h=1 (djh) 1
// 2– Update kernel distance matrix Dt:
d2ji = 2 − 2 · N
P
uik
M
k=1
until (t &gt; maxIter) or
(t &gt; 1 and IUt − Ut−1I &lt; c)
Pθ +— Generate soft partition(Ut,B)
</equation>
<bodyText confidence="0.161757">
Output: Fuzzy partition Pθ
</bodyText>
<subsubsectionHeader confidence="0.759745">
2.2.3 Cluster Validation
</subsubsectionHeader>
<bodyText confidence="0.999903095238095">
In the CDC setting, the number of true underlying
identities may vary depending on the entities’ level
of ambiguity (e.g. name frequency). Selecting the
optimal number of clusters is in general a hard
research question in clustering1. We adopt the
Xie-Beni Index (XBI) (Xie and Beni, 1991) as in
ARCA, which is one of the most popular cluster
validities for fuzzy clustering algorithms. Xie-
Beni Index (XBI) measures the goodness of clus-
tering using the ratio of the intra-cluster variation
and the inter-cluster separation. We measure the
kernelized XBI (KXBI) in the feature space as,
where the nominator is readily computed using D
and the inter-cluster separation in the denominator
can be evaluated using the similar kernel trick
above (details omitted). Note that KXBI is only
defined for C &gt; 1. Thus we pick the C that
corresponds to the first minimum of KXBI, and
then compare its objective function value JC with
the cluster variance (J1 for C = 1). The optimal
C is chosen from the minimum of the two2.
</bodyText>
<subsectionHeader confidence="0.995478">
2.3 Specialist Ensemble Learning of Relation
Strengths between Entities
</subsectionHeader>
<bodyText confidence="0.999836529411765">
One remaining element in the overall CDC ap-
proach is how the relation strength rj,k between
two entities is computed. In (Cohen et al., 2003),
a binary SVM model is trained and its confidence
in predicting the non-coreferent class is used as
the distance metric. In our case of using in-
formation extraction results for disambiguation,
however, only some of the similarity features are
present based on the available relationships in two
profiles. In this work, we propose to treat each
similarity function as a specialist that specializes
in computing the similarity of a particular type
of relationship. Indeed, the similarity function
between a pair of attributes or relationships may in
itself be a sophisticated component algorithm. We
utilize the specialist ensemble learning framework
(Freund et al., 1997) to combine these component
</bodyText>
<footnote confidence="0.9982974">
1In particular, clustering algorithms that regularize the
optimization with cluster size are not applicable in our case.
2In practice, the entities to be disambiguated tend to be
dominated by several major identities. Hence performance
generally does not vary much in the range of large C values.
</footnote>
<equation confidence="0.992067625">
(5)
1
(d2ji) M−1
N
P
k=1
uMikKjk
um ij
IIΦ(rj) − Φ(wi)II2H
N · min IIΦ(wi) − Φ(wj)II2H
1&lt;i&lt;j&lt;C
KXBI =
PC
i=1
PN
j=1
</equation>
<page confidence="0.970264">
417
</page>
<bodyText confidence="0.95574575">
similarities into the relation strength for clustering.
Here, a specialist is awakened for prediction only
when the same type of relationships are present in
both chained entities. A specialist can choose not
to make a prediction if it is not confident enough
for an instance. These aspects contrast with the
traditional insomniac ensemble learning methods,
where each component learner is always available
for prediction (Freund et al., 1997). Also, spe-
cialists have different weights (in addition to their
prediction) on the final relation strength, e.g. a
match in a family relationship is considered more
important than in a co-occurrence relationship.
Algorithm 2 SEG (Freund et al., 1997)
Input: Initial weight distribution pl;
learning rate η &gt; 0; training set {&lt; st, yt &gt;}
</bodyText>
<listItem confidence="0.9519175">
1: for t=1 to T do
2: Predict using:
</listItem>
<equation confidence="0.930280666666667">
tt
�t = Pi∈Et pisi P (7)
i∈Et pti
</equation>
<listItem confidence="0.867711333333333">
3: Observe the true label yt and incur square
loss L(yt, yt) = (yt _ yt)2
4: Update weight distribution: for i E Et
</listItem>
<equation confidence="0.9967865">
X t
t −277xa(yt−yt) P�
j∈Et (8)
pje jEEt
Otherwise:pt+1
i = pt i
</equation>
<listItem confidence="0.490226">
5: end for
Output: Model p
</listItem>
<bodyText confidence="0.999949612903226">
The ensemble relation strength model is learned
as follows. Given training data, the set of chained
entities £train is extracted as described earlier. For
a pair of entities ej and ek, a similarity vector
s is computed using the component similarity
functions for the respective attributes and rela-
tionships, and the true label is defined as y =
I{ej and ek are coreferent}. The instances are
subsampled to yield a balanced pairwise train-
ing set {&lt; st, yt &gt;}. We adopt the Special-
ist Exponentiated Gradient (SEG) (Freund et al.,
1997) algorithm to learn the mixing weights of the
specialists’ prediction (Algorithm 2) in an online
manner. In each training iteration, an instance
&lt; st, yt &gt; is presented to the learner (with Et
denoting the set of indices of awake specialists in
st). The SEG algorithm first predicts the value yt
based on the awake specialists’ decisions. The true
value yt is then revealed and the learner incurs a
square loss between the predicted and the true val-
ues. The current weight distribution p is updated
to minimize square loss: awake specialists are
promoted or demoted in their weights according to
the difference between the predicted and the true
value. The learning iterations can run a few passes
till convergence, and the model is learned in linear
time with respect to T and is thus very efficient. In
prediction time, let E(jk) denote the set of active
specialists for the pair of entities ej and ek, and
s(jk) denote the computed similarity vector. The
predicted relation strength rj,k is,
</bodyText>
<equation confidence="0.9927815">
k
j
Pi∈E(jk) pisi (9)
Pi∈E(jk) pi
</equation>
<subsectionHeader confidence="0.85694">
2.4 Remarks
</subsectionHeader>
<bodyText confidence="0.999983636363636">
Before we conclude this section, we make several
comments on using fuzzy clustering for cross
document coreference. First, instead of conduct-
ing CDC for all entities concurrently (which can
be computationally intensive with a large cor-
pus), chained entities are first distributed into non-
overlapping blocks. Clustering is performed for
each block which is a drastically smaller problem
space, while entities from different blocks are
unlikely to be coreferent. Our CDC system uses
phonetic blocking on the full name, so that name
variations arising from translation, transliteration
and abbreviation can be accommodated. Ad-
ditional link constraints checking is also imple-
mented to improve scalability though these are not
the main focus of the paper.
There are several additional benefits in using
a fuzzy clustering method besides the capabil-
ity of probabilistic membership assignments in
the CDC solution. In the clustered web search
context, splitting a true identity into two clusters
is perceived as a more severe error than putting
irrelevant records in a cluster, as it is more difficult
for the user to collect records in different clusters
(to reconstruct the real underlying identity) than
to prune away noisy records. While there is no
universal way to handle this with hard clustering,
soft clustering algorithms can more easily avoid
the false negatives by allowing records to prob-
abilistically appear in different clusters (subject
to the sum of 1) using a more lenient threshold.
Also, while there is no real prototypical elements
in relational clustering, soft relational clustering
</bodyText>
<equation confidence="0.9988115">
pt+1
i =
ptie−2ηxti(˜yt−yt)
rj,k =
</equation>
<page confidence="0.983813">
418
</page>
<bodyText confidence="0.99893275">
methods can naturally rank the profiles within
a cluster according to their membership levels,
which is an additional advantage for enhancing
user consumption of the disambiguation results.
</bodyText>
<sectionHeader confidence="0.999064" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999700333333333">
In this section, we first formally define the evalu-
ation metrics, followed by the introduction to the
benchmark test sets and the system’s performance.
</bodyText>
<subsectionHeader confidence="0.994924">
3.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999942428571429">
We benchmarked our method using the standard
purity and inverse purity clustering metrics as in
the WePS evaluation. Let a set of clusters P =
{Ci} denote the system’s partition as aforemen-
tioned and a set of categories Q = {Dj} be the
gold standard. The precision of a cluster Ci with
respect to a category Dj is defined as,
</bodyText>
<equation confidence="0.9917325">
Precision(Ci, Dj) = |Ci ∩ Dj|
|Ci|
</equation>
<bodyText confidence="0.966072277777778">
Purity is in turn defined as the weighted average
of the maximum precision achieved by the clusters
on one of the categories,
Precision(Ci, Dj)
where n = E |Ci|. Hence purity penalizes putting
noise chained entities in a cluster. Trivially, the
maximum purity (i.e. 1) can be achieved by
making one cluster per chained entity (referred to
as the one-in-one baseline). Reversing the role of
clusters and categories, Inverse purity(P, Q) def =
Purity(Q, P). Inverse Purity penalizes splitting
chained entities belonging to the same category
into different clusters. The maximum inverse
purity can be similarly achieved by putting all
entities into one cluster (all-in-one baseline).
Purity and inverse purity are similar to the
precision and recall measures commonly used in
IR. The F score, F = 1/(α 1
</bodyText>
<equation confidence="0.978147">
Purity + (1 −
α) 1
</equation>
<bodyText confidence="0.98688075">
InversePurity ), is used in performance evalua-
tion. α = 0.2 is used to give more weight to
inverse purity, with the justification for the web
person search mentioned earlier.
</bodyText>
<subsectionHeader confidence="0.994149">
3.2 Dataset
</subsectionHeader>
<bodyText confidence="0.999765307692308">
We evaluate our methods using the benchmark
test collection from the ACL SemEval-2007 web
person search task (WePS) (Artiles et al., 2007).
The test collection consists of three sets of 10
different names, sampled from ambiguous names
from English Wikipedia (famous people), partici-
pants of the ACL 2006 conference (computer sci-
entists) and common names from the US Census
data, respectively. For each name, the top 100
documents retrieved from the Yahoo! Search API
were annotated, yielding on average 45 real world
identities per set and about 3k documents in total.
As we note in the beginning of Section 2, the
human markup for the entities corresponding to
the search queries is on the document level. The
profile-based CDC approach, however, is to merge
the mention-level entities. In our evaluation, we
adopt the document label (and the person search
query) to annotate the entity profiles that corre-
sponds to the person name search query. Despite
the difference, the results of the one-in-one and
all-in-one baselines are almost identical to those
reported in the WePS evaluation (F = 0.52, 0.58
respectively). Hence the performance reported
here is comparable to the official evaluation results
(Artiles et al., 2007).
</bodyText>
<subsectionHeader confidence="0.999828">
3.3 Information Extraction and Similarities
</subsectionHeader>
<bodyText confidence="0.999970653846154">
We use an information extraction tool AeroText
(Taylor, 2004) to construct the entity profiles.
AeroText extracts two types of information for
an entity. First, the attribute information about
the person named entity includes first/middle/last
names, gender, mention, etc. In addition,
AeroText extracts relationship information
between named entities, such as Family, List,
Employment, Ownership, Citizen-Resident-
Religion-Ethnicity and so on, as specified in the
ACE evaluation. AeroText resolves the references
of entities within a document and produces the
entity profiles, used as input to the CDC system.
Note that alternative IE or WDC tools, as well
as additional attributes or relationships, can be
readily used in the CDC methods we proposed.
A suite of similarity functions is designed to
determine if the attributes relationships in a pair
of entity profiles match or not:
Text similarity. To decide whether two names
in the co-occurrence or family relationship match,
we use the SoftTFIDF measure (Cohen et al.,
2003), which is a hybrid matching scheme that
combines the token-based TFIDF with the Jaro-
Winkler string distance metric. This permits in-
exact matching of named entities due to name
</bodyText>
<equation confidence="0.99847">
Purity(P, Q) =
C |Ci |max
i=1 n j
</equation>
<page confidence="0.995432">
419
</page>
<bodyText confidence="0.9992443125">
variations, typos, etc.
Semantic similarity. Text or syntactic similarity
is not always sufficient for matching relationships.
WordNet and the information theoretic semantic
distance (Jiang and Conrath, 1997) are used to
measure the semantic similarity between concepts
in relationships such as mention, employment,
ownership, etc.
Other rule-based similarity. Several other
cases require special treatment. For example,
the employment relationships of Senator and
D-N.Y. should match based on domain knowledge.
Also, we design dictionary-based similarity
functions to handle nicknames (Bill and William),
acronyms (COLING for International Conference
on Computational Linguistics), and geo-locations.
</bodyText>
<subsectionHeader confidence="0.989611">
3.4 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.9950875">
From the WePS training data, we generated a
training set of around 32k pairwise instances as
previously stated in Section 2.3. We then used
the SEG algorithm to learn the weight distribution
model. We tuned the parameters in the KARC
algorithm using the training set with discrete grid
search and chose m = 1.6 and 0 = 0.3. The RBF
kernel (Gaussian) is used with -y = 0.015.
</bodyText>
<tableCaption confidence="0.881375">
Table 1: Cross document coreference performance
(I. Purity denotes inverse purity).
</tableCaption>
<table confidence="0.9879825">
Method Purity I. Purity F
KARC-S 0.657 0.795 0.740
KARC-H 0.662 0.762 0.710
FRC 0.484 0.840 0.697
One-in-one 1.000 0.482 0.524
All-in-one 0.279 1.000 0.571
</table>
<bodyText confidence="0.995427">
The macro-averaged cross document corefer-
ence on the WePS test sets are reported in Table
1. The F score of our CDC system (KARC-
S) is 0.740, comparable to the test results of the
first tier systems in the official evaluation. The
two baselines are also included. Since different
feature sets, NLP tools, etc are used in different
benchmarked systems, we are also interested in
comparing the proposed algorithm with differ-
ent soft relational clustering variants. First, we
‘harden’ the fuzzy partition produced by KARC
by allowing an entity to appear in the cluster
with highest membership value (KARC-H). Purity
improves because of the removal of noise entities,
though at the sacrifice of inverse purity and the
</bodyText>
<tableCaption confidence="0.9595025">
Table 2: Cross document coreference performance
on subsets (I. Purity denotes inverse purity).
</tableCaption>
<table confidence="0.9534112">
Test set Identity Purity I. Purity F
Wikipedia 56.5 0.666 0.752 0.717
ACL-06 31.0 0.783 0.771 0.773
US Census 50.3 0.554 0.889 0.754
F score deteriorates. We also implement a pop-
</table>
<bodyText confidence="0.99451452">
ular fuzzy relational clustering algorithm called
FRC (Dave and Sen, 2002), whose optimization
functional directly minimizes with respect to the
relation matrix. With the same feature sets and
distance function, KARC-S outperforms FRC in F
score by about 5%. Because the test set is very am-
biguous (on average only two documents per real
world entity), the baselines have relatively high F
score as observed in the WePS evaluation (Artiles
et al., 2007). Table 2 further analyzes KARC-
S’s result on the three subsets Wikipedia, ACL06
and US Census. The F score is higher in the
less ambiguous (the average number of identities)
dataset and lower in the more ambiguous one, with
a spread of 6%.
We study how the cross document coreference
performance changes as we vary the fuzziness in
the solution (controlled by m). In Figure 1, as
m increases from 1.4 to 1.9, purity improves by
10% to 0.67, which indicates that more correct
coreference decisions (true positives) can be made
in a softer configuration. The complimentary is
true for inverse purity, though to a lesser extent.
In this case, more false negatives, corresponding
to the entities of different coreferents incorrectly
</bodyText>
<figure confidence="0.878354">
KARC performance with different m
1.4 1.5 1.6 1.7 1.8 1.9
m
</figure>
<figureCaption confidence="0.886784">
Figure 1: Purity, inverse purity and F score with
different fuzzifiers m.
</figureCaption>
<figure confidence="0.974871466666667">
0.85
0.75
0.65
0.55
0.8
0.7
0.6
0.5
purity
inverse purity
F
420
KARC performance with different 0
0.1 0.2 0.3 0.4 0.5 0.6
0
</figure>
<figureCaption confidence="0.999852">
Figure 2: CDC performance with different 0.
</figureCaption>
<bodyText confidence="0.9986164">
linked, are made in a softer partition. The F
score peaks at 0.74 (m = 1.6) and then slightly
decreases, as the gain in purity is outweighed by
the loss in inverse purity.
Figure 2 evaluates the impact of the different
settings of 0 (the threshold of including a chained
entity in the fuzzy cluster) on the coreference
performance. We observe that as we increase
0, purity improves indicating less ‘noise’ entities
are included in the solution. On the other hand,
inverse purity decreases meaning more coreferent
entities are not linked due to the stricter threshold.
Overall, the changes in the two metrics offset each
other and the F score is relatively stable across a
broad range of 0 settings.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999986571428572">
The original work in (Bagga and Baldwin, 1998)
proposed a CDC system by first performing WDC
and then disambiguating based on the summary
sentences of the chains. This is similar to ours in
that mentions rather than documents are clustered,
leveraging the advances in state-of-the-art WDC
methods developed in NLP, e.g. (Ng and Cardie,
2001; Yang et al., 2008). On the other hand, our
work goes beyond the simple bag-of-word features
and vector space model in (Bagga and Baldwin,
1998; Gooi and Allan, 2004) with IE results. (Wan
et al., 2005) describes a person resolution system
WebHawk that clusters web pages using some
extracted personal information including person
name, title, organization, email and phone number,
besides lexical features. (Mann and Yarowsky,
2003) extracts biographical information, which is
relatively scarce in web data, for disambiguation.
With the support of state-of-the-art information
extraction tools, the profiles of entities in this work
covers a broader range of relational information.
(Niu et al., 2004) also leveraged IE support, but
their approach was evaluated on a small artificial
corpus. Also, the pairwise distance model is
insomniac (i.e. all similarity specialists are awake
for prediction) and our work extends this with a
specialist learning framework.
Prior work has largely relied on using hier-
archical clustering methods for CDC, with the
threshold for stopping the merging set using the
training data, e.g. (Mann and Yarowsky, 2003;
Chen and Martin, 2007; Baron and Freedman,
2008). The fuzzy relational clustering method
proposed in this paper we believe better addresses
the uncertainty aspect of the CDC problem.
There are also orthogonal research directions
for the CDC problem. (Li et al., 2004) solved the
CDC problem by adopting a probabilistic view on
how documents are generated and how names are
sprinkled into them. (Bunescu and Pasca, 2006)
showed that external information from Wikipedia
can improve the disambiguation performance.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99998272">
We have presented a profile-based Cross Docu-
ment Coreference (CDC) approach based on a
novel fuzzy relational clustering algorithm KARC.
In contrast to traditional hard clustering methods,
KARC produces fuzzy sets of identities which
better reflect the intrinsic uncertainty of the CDC
problem. Kernelization, as used in KARC, enables
the optimization of clustering that is spherical
in nature to apply to relational data that tend to
have complicated shapes. KARC partitions named
entities based on their profiles constructed by an
information extraction tool. To match the pro-
files, a specialist ensemble algorithm predicts the
pairwise distance by aggregating the similarities of
the attributes and relationships in the profiles. We
evaluated the proposed methods with experiments
on a large benchmark collection and demonstrate
that the proposed methods compare favorably with
the top runs in the SemEval evaluation.
The focus of this work is on the novel learning
and clustering methods for coreference. Future
research directions include developing rich feature
sets and using corpus level or external informa-
tion. We believe that such efforts can further im-
prove cross document coreference performance.
</bodyText>
<figure confidence="0.998688555555555">
0.85
0.75
0.65
0.8
0.7
0.6
purity
inverse purity
F
</figure>
<page confidence="0.996549">
421
</page>
<sectionHeader confidence="0.992972" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999765953703704">
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007. The SemEval-2007 WePS evaluation:
Establishing a benchmark for the web people search
task. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-
2007), pages 64–69.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of 36th International
Conference On Computational Linguistics (ACL)
and 17th international conference on Computational
linguistics (COLING), pages 79–85.
Alex Baron and Marjorie Freedman. 2008. Who
is who and what is what: Experiments in cross-
document co-reference. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 274–283.
J. C. Bezdek. 1981. Pattern Recognition with Fuzzy
Objective Function Algoritms. Plenum Press, NY.
Razvan Bunescu and Marius Pasca. 2006. Using
encyclopedic knowledge for named entity disam-
biguation. In Proceedings of the 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 9–16.
Ying Chen and James Martin. 2007. Towards
robust unsupervised personal name disambiguation.
In Proc. of 2007 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning.
Mario G. C. A. Cimino, Beatrice Lazzerini, and
Francesco Marcelloni. 2006. A novel approach
to fuzzy clustering based on a dissimilarity relation
extracted from data using a TS system. Pattern
Recognition, 39(11):2077–2091.
William W. Cohen, Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks.
In Proceedings of IJCAI Workshop on Information
Integration on the Web.
Paolo Corsini, Beatrice Lazzerini, and Francesco
Marcelloni. 2005. Anew fuzzy relational clustering
algorithm based on the fuzzy c-means algorithm.
Soft Computing, 9(6):439 – 447.
Rajesh N. Dave and Sumit Sen. 2002. Robust fuzzy
clustering of relational data. IEEE Transactions on
Fuzzy Systems, 10(6):713–727.
Yoav Freund, Robert E. Schapire, Yoram Singer, and
Manfred K. Warmuth. 1997. Using and combining
predictors that specialize. In Proceedings of the
twenty-ninth annual ACM symposium on Theory of
computing (STOC), pages 334–343.
Chung H. Gooi and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics
(NAACL), pages 9–16.
Jay J. Jiang and David W. Conrath. 1997.
Semantic similarity based on corpus statistics and
lexical taxonomy. In Proceedings of International
Conference Research on Computational Linguistics.
Xin Li, Paul Morie, and Dan Roth. 2004. Robust
reading: Identification and tracing of ambiguous
names. In Proceedings of the Human Language
Technology Conference and the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 17–24.
Gideon S. Mann and David Yarowsky. 2003.
Unsupervised personal name disambiguation. In
Conference on Computational Natural Language
Learning (CoNLL), pages 33–40.
Vincent Ng and Claire Cardie. 2001. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 104–111.
Cheng Niu, Wei Li, and Rohini K. Srihari. 2004.
Weakly supervised learning for cross-document
person name disambiguation supported by infor-
mation extraction. In Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics (ACL), pages 597–604.
Bernhard Sch¨olkopf and Alex Smola. 2002. Learning
with Kernels. MIT Press, Cambridge, MA.
Sarah M. Taylor. 2004. Information extraction tools:
Deciphering human language. IT Professional,
6(6):28 – 34.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag New York.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong
Ding. 2005. Person resolution in person search
results: WebHawk. In Proceedings of the 14th
ACM international conference on Information and
knowledge management (CIKM), pages 163–170.
Xuanli Lisa Xie and Gerardo Beni. 1991. A validity
measure for fuzzy clustering. IEEE Transactions
on Pattern Analysis and Machine Intelligence,
13(8):841 – 847.
Xiaofeng Yang, Jian Su, Jun Lang, Chew L. Tan,
Ting Liu, and Sheng Li. 2008. An entity-
mention model for coreference resolution with
inductive logic programming. In Proceedings of
the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 843–851.
Dao-Qiang Zhang and Song-Can Chen. 2003.
Clustering incomplete data using kernel-based fuzzy
c-means algorithm. Neural Processing Letters,
18(3):155 – 162.
</reference>
<page confidence="0.998512">
422
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.638441">
<title confidence="0.998939">Profile Based Cross-Document Coreference Using Kernelized Fuzzy Relational Clustering</title>
<author confidence="0.972614">M L A Lee of Information Sciences</author>
<author confidence="0.972614">Technology</author>
<address confidence="0.9703385">Pennsylvania State University, University Park, PA 16802, USA Technology Office, Lockheed Martin IS&amp;GS, Arlington, VA 22203, USA</address>
<email confidence="0.990878">jonathan.l.smith,</email>
<abstract confidence="0.988869185185185">Coreferencing entities across documents in a large corpus enables advanced document understanding tasks such as question answering. This paper presents a novel cross document coreference approach that leverages the profiles of entities which are constructed by using information extraction tools and reconciled by using a within-document coreference module. We propose to match the profiles by using a learned ensemble distance function comprised of a suite of similarity specialists. We develop a kernelized soft relational clustering algorithm that makes use of the learned distance function to partition the entities into fuzzy sets of identities. We compare the kernelized clustering method with a popular fuzzy relation clustering algorithm (FRC) and show 5% improvement in coreference performance. Evaluation of our proposed methods on a large benchmark disambiguation collection shows that they compare favorably with the top runs in the SemEval evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Julio Gonzalo</author>
<author>Satoshi Sekine</author>
</authors>
<title>The SemEval-2007 WePS evaluation: Establishing a benchmark for the web people search task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval2007),</booktitle>
<pages>64--69</pages>
<contexts>
<context position="2638" citStr="Artiles et al., 2007" startWordPosition="376" endWordPosition="379">cope of disambiguation to within the boundary of a document. When namesakes appear in an article, the author can explicitly help to disambiguate, using titles and suffixes (as in the example, “George Bush Sr. ... the younger Bush”) besides other means. Cross document coreference, on the other hand, is a more challenging task because these linguistics cues and sentence structures no longer apply, given the wide variety of context and styles in different documents. Cross document coreference research has recently become more popular due to the increasing interests in the web person search task (Artiles et al., 2007). Here, a search query for a person name is entered into a search engine and the desired outputs are documents clustered according to the identities of the entities in question. In our work, we propose to drill down to the subdocument mention level and construct an entity profile with the support of information extraction tools and reconciled with WDC methods. Hence our IE based approach has access to accurate information such as a person’s mentions and geolocations for disambiguation. Simple IR based CDC approaches (e.g. (Gooi and Allan, 2004)), on the other hand, may simply use all the terms</context>
<context position="5213" citStr="Artiles et al., 2007" startWordPosition="792" endWordPosition="795"> a simplifying assumption that a named entity (and its variants) in a document has one underlying real identity. The assumption is generally acceptable but may be violated when a document refers to namesakes at the same time (e.g. George W. Bush and George H. W. Bush referred to as George or President Bush). Furthermore, the context surrounding the person NE President Clinton can be counterproductive for disambiguating the NE Senator Clinton, with both entities likely to appear in a document at the same time. The simplified document level CDC has nevertheless been used in the WePS evaluation (Artiles et al., 2007), called the web people task. In this work, we advocate profile based disambiguation that aims to leverage the advances in NLP techniques. Rather than treating a document as simply a bag of words, an information extraction tool first extracts NE’s and their relationships. For the NE’s of interest (i.e. persons in this work), a within-document coreference (WDC) module then links the entities deemed as referring to the same underlying identity into a WDC chain. This process includes both anaphora resolution (resolving ‘He’ and its antecedent ‘President Clinton’) and entity tracking (resolving ‘B</context>
<context position="23053" citStr="Artiles et al., 2007" startWordPosition="3774" endWordPosition="3777"> same category into different clusters. The maximum inverse purity can be similarly achieved by putting all entities into one cluster (all-in-one baseline). Purity and inverse purity are similar to the precision and recall measures commonly used in IR. The F score, F = 1/(α 1 Purity + (1 − α) 1 InversePurity ), is used in performance evaluation. α = 0.2 is used to give more weight to inverse purity, with the justification for the web person search mentioned earlier. 3.2 Dataset We evaluate our methods using the benchmark test collection from the ACL SemEval-2007 web person search task (WePS) (Artiles et al., 2007). The test collection consists of three sets of 10 different names, sampled from ambiguous names from English Wikipedia (famous people), participants of the ACL 2006 conference (computer scientists) and common names from the US Census data, respectively. For each name, the top 100 documents retrieved from the Yahoo! Search API were annotated, yielding on average 45 real world identities per set and about 3k documents in total. As we note in the beginning of Section 2, the human markup for the entities corresponding to the search queries is on the document level. The profile-based CDC approach,</context>
<context position="28210" citStr="Artiles et al., 2007" startWordPosition="4577" endWordPosition="4580">dentity Purity I. Purity F Wikipedia 56.5 0.666 0.752 0.717 ACL-06 31.0 0.783 0.771 0.773 US Census 50.3 0.554 0.889 0.754 F score deteriorates. We also implement a popular fuzzy relational clustering algorithm called FRC (Dave and Sen, 2002), whose optimization functional directly minimizes with respect to the relation matrix. With the same feature sets and distance function, KARC-S outperforms FRC in F score by about 5%. Because the test set is very ambiguous (on average only two documents per real world entity), the baselines have relatively high F score as observed in the WePS evaluation (Artiles et al., 2007). Table 2 further analyzes KARCS’s result on the three subsets Wikipedia, ACL06 and US Census. The F score is higher in the less ambiguous (the average number of identities) dataset and lower in the more ambiguous one, with a spread of 6%. We study how the cross document coreference performance changes as we vary the fuzziness in the solution (controlled by m). In Figure 1, as m increases from 1.4 to 1.9, purity improves by 10% to 0.67, which indicates that more correct coreference decisions (true positives) can be made in a softer configuration. The complimentary is true for inverse purity, t</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007. The SemEval-2007 WePS evaluation: Establishing a benchmark for the web people search task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval2007), pages 64–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Entity-based cross-document coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proceedings of 36th International Conference On Computational Linguistics (ACL) and 17th international conference on Computational linguistics (COLING),</booktitle>
<pages>79--85</pages>
<contexts>
<context position="30001" citStr="Bagga and Baldwin, 1998" startWordPosition="4885" endWordPosition="4888">ghed by the loss in inverse purity. Figure 2 evaluates the impact of the different settings of 0 (the threshold of including a chained entity in the fuzzy cluster) on the coreference performance. We observe that as we increase 0, purity improves indicating less ‘noise’ entities are included in the solution. On the other hand, inverse purity decreases meaning more coreferent entities are not linked due to the stricter threshold. Overall, the changes in the two metrics offset each other and the F score is relatively stable across a broad range of 0 settings. 4 Related Work The original work in (Bagga and Baldwin, 1998) proposed a CDC system by first performing WDC and then disambiguating based on the summary sentences of the chains. This is similar to ours in that mentions rather than documents are clustered, leveraging the advances in state-of-the-art WDC methods developed in NLP, e.g. (Ng and Cardie, 2001; Yang et al., 2008). On the other hand, our work goes beyond the simple bag-of-word features and vector space model in (Bagga and Baldwin, 1998; Gooi and Allan, 2004) with IE results. (Wan et al., 2005) describes a person resolution system WebHawk that clusters web pages using some extracted personal inf</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Entity-based cross-document coreferencing using the vector space model. In Proceedings of 36th International Conference On Computational Linguistics (ACL) and 17th international conference on Computational linguistics (COLING), pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Baron</author>
<author>Marjorie Freedman</author>
</authors>
<title>Who is who and what is what: Experiments in crossdocument co-reference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>274--283</pages>
<contexts>
<context position="31491" citStr="Baron and Freedman, 2008" startWordPosition="5114" endWordPosition="5117">rmation extraction tools, the profiles of entities in this work covers a broader range of relational information. (Niu et al., 2004) also leveraged IE support, but their approach was evaluated on a small artificial corpus. Also, the pairwise distance model is insomniac (i.e. all similarity specialists are awake for prediction) and our work extends this with a specialist learning framework. Prior work has largely relied on using hierarchical clustering methods for CDC, with the threshold for stopping the merging set using the training data, e.g. (Mann and Yarowsky, 2003; Chen and Martin, 2007; Baron and Freedman, 2008). The fuzzy relational clustering method proposed in this paper we believe better addresses the uncertainty aspect of the CDC problem. There are also orthogonal research directions for the CDC problem. (Li et al., 2004) solved the CDC problem by adopting a probabilistic view on how documents are generated and how names are sprinkled into them. (Bunescu and Pasca, 2006) showed that external information from Wikipedia can improve the disambiguation performance. 5 Conclusions We have presented a profile-based Cross Document Coreference (CDC) approach based on a novel fuzzy relational clustering a</context>
</contexts>
<marker>Baron, Freedman, 2008</marker>
<rawString>Alex Baron and Marjorie Freedman. 2008. Who is who and what is what: Experiments in crossdocument co-reference. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 274–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Bezdek</author>
</authors>
<title>Pattern Recognition with Fuzzy Objective Function Algoritms.</title>
<date>1981</date>
<publisher>Plenum Press, NY.</publisher>
<contexts>
<context position="9143" citStr="Bezdek, 1981" startWordPosition="1440" endWordPosition="1441">e vi is a virtual (implicit) prototype of the i-th cluster (ej, vi E D) and m controls the fuzziness of the solution (m &gt; 1; the solution approaches hard clustering as m approaches 1). We will further explain the generic distance function d : D x D —* R in the next subsection. The goal of the optimization is to minimize the sum of deviations of patterns to the cluster prototypes. The clustering solution is a fuzzy partition Pθ = {Ci}, where ej E Ci if and only if uij &gt; 0. We note from the outset that the optimization functional has the same form as the classical Fuzzy C-Means (FCM) algorithm (Bezdek, 1981), but major differences exist. FCM, as most object clustering algorithms, deals with object data represented in a vectorial form. In our case, the data is purely relational and only the mutual relationships between entities can be determined. To be exact, we can define the similarity/dissimilarity between a pair of attributes or relationships of the same type l between entities ej and ek as s(l)(ej, ek). For instance, the similarity between the occupations ‘President’ and ‘Commander in Chief’ can be computed using the JC semantic distance (Jiang and Conrath, 1997) with WordNet; the similarity </context>
</contexts>
<marker>Bezdek, 1981</marker>
<rawString>J. C. Bezdek. 1981. Pattern Recognition with Fuzzy Objective Function Algoritms. Plenum Press, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="31862" citStr="Bunescu and Pasca, 2006" startWordPosition="5173" endWordPosition="5176"> learning framework. Prior work has largely relied on using hierarchical clustering methods for CDC, with the threshold for stopping the merging set using the training data, e.g. (Mann and Yarowsky, 2003; Chen and Martin, 2007; Baron and Freedman, 2008). The fuzzy relational clustering method proposed in this paper we believe better addresses the uncertainty aspect of the CDC problem. There are also orthogonal research directions for the CDC problem. (Li et al., 2004) solved the CDC problem by adopting a probabilistic view on how documents are generated and how names are sprinkled into them. (Bunescu and Pasca, 2006) showed that external information from Wikipedia can improve the disambiguation performance. 5 Conclusions We have presented a profile-based Cross Document Coreference (CDC) approach based on a novel fuzzy relational clustering algorithm KARC. In contrast to traditional hard clustering methods, KARC produces fuzzy sets of identities which better reflect the intrinsic uncertainty of the CDC problem. Kernelization, as used in KARC, enables the optimization of clustering that is spherical in nature to apply to relational data that tend to have complicated shapes. KARC partitions named entities ba</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Chen</author>
<author>James Martin</author>
</authors>
<title>Towards robust unsupervised personal name disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="31464" citStr="Chen and Martin, 2007" startWordPosition="5110" endWordPosition="5113">f state-of-the-art information extraction tools, the profiles of entities in this work covers a broader range of relational information. (Niu et al., 2004) also leveraged IE support, but their approach was evaluated on a small artificial corpus. Also, the pairwise distance model is insomniac (i.e. all similarity specialists are awake for prediction) and our work extends this with a specialist learning framework. Prior work has largely relied on using hierarchical clustering methods for CDC, with the threshold for stopping the merging set using the training data, e.g. (Mann and Yarowsky, 2003; Chen and Martin, 2007; Baron and Freedman, 2008). The fuzzy relational clustering method proposed in this paper we believe better addresses the uncertainty aspect of the CDC problem. There are also orthogonal research directions for the CDC problem. (Li et al., 2004) solved the CDC problem by adopting a probabilistic view on how documents are generated and how names are sprinkled into them. (Bunescu and Pasca, 2006) showed that external information from Wikipedia can improve the disambiguation performance. 5 Conclusions We have presented a profile-based Cross Document Coreference (CDC) approach based on a novel fu</context>
</contexts>
<marker>Chen, Martin, 2007</marker>
<rawString>Ying Chen and James Martin. 2007. Towards robust unsupervised personal name disambiguation. In Proc. of 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario G C A Cimino</author>
<author>Beatrice Lazzerini</author>
<author>Francesco Marcelloni</author>
</authors>
<title>A novel approach to fuzzy clustering based on a dissimilarity relation extracted from data using a TS system.</title>
<date>2006</date>
<journal>Pattern Recognition,</journal>
<volume>39</volume>
<issue>11</issue>
<contexts>
<context position="10199" citStr="Cimino et al., 2006" startWordPosition="1608" endWordPosition="1611"> between the occupations ‘President’ and ‘Commander in Chief’ can be computed using the JC semantic distance (Jiang and Conrath, 1997) with WordNet; the similarity of co-occurrence with other people can be measured by the Jaccard coefficient. In the next section, we propose to compute the relation strength r(·, ·) from the component similarities using aggregation weights learned from training data. Hence the N chained entities to be clustered can be represented as relational data using an nxn matrix R, where rj,k = r(ej, ek). The Any Relation Clustering Algorithm (ARCA) (Corsini et al., 2005; Cimino et al., 2006) represents relational data as object data using their mutual relation strength and uses FCM for clustering. We adopt this approach to transform (objectify) a relational pattern ej into an N dimensional vector rj (i.e. the j-th row in the matrix R) using a mapping Θ : D —* RN. In other words, each chained entity is represented as a vector of its relation strengths with all the entities. Fuzzy clusters can then be obtained by grouping closely related patterns using object clustering algorithm. Furthermore, it is well known that FCM is a spherical clustering algorithm and thus is not generally a</context>
</contexts>
<marker>Cimino, Lazzerini, Marcelloni, 2006</marker>
<rawString>Mario G. C. A. Cimino, Beatrice Lazzerini, and Francesco Marcelloni. 2006. A novel approach to fuzzy clustering based on a dissimilarity relation extracted from data using a TS system. Pattern Recognition, 39(11):2077–2091.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Pradeep Ravikumar</author>
<author>Stephen E Fienberg</author>
</authors>
<title>A comparison of string distance metrics for name-matching tasks.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="15779" citStr="Cohen et al., 2003" startWordPosition="2574" endWordPosition="2577">is readily computed using D and the inter-cluster separation in the denominator can be evaluated using the similar kernel trick above (details omitted). Note that KXBI is only defined for C &gt; 1. Thus we pick the C that corresponds to the first minimum of KXBI, and then compare its objective function value JC with the cluster variance (J1 for C = 1). The optimal C is chosen from the minimum of the two2. 2.3 Specialist Ensemble Learning of Relation Strengths between Entities One remaining element in the overall CDC approach is how the relation strength rj,k between two entities is computed. In (Cohen et al., 2003), a binary SVM model is trained and its confidence in predicting the non-coreferent class is used as the distance metric. In our case of using information extraction results for disambiguation, however, only some of the similarity features are present based on the available relationships in two profiles. In this work, we propose to treat each similarity function as a specialist that specializes in computing the similarity of a particular type of relationship. Indeed, the similarity function between a pair of attributes or relationships may in itself be a sophisticated component algorithm. We u</context>
<context position="25216" citStr="Cohen et al., 2003" startWordPosition="4107" endWordPosition="4110">dentReligion-Ethnicity and so on, as specified in the ACE evaluation. AeroText resolves the references of entities within a document and produces the entity profiles, used as input to the CDC system. Note that alternative IE or WDC tools, as well as additional attributes or relationships, can be readily used in the CDC methods we proposed. A suite of similarity functions is designed to determine if the attributes relationships in a pair of entity profiles match or not: Text similarity. To decide whether two names in the co-occurrence or family relationship match, we use the SoftTFIDF measure (Cohen et al., 2003), which is a hybrid matching scheme that combines the token-based TFIDF with the JaroWinkler string distance metric. This permits inexact matching of named entities due to name Purity(P, Q) = C |Ci |max i=1 n j 419 variations, typos, etc. Semantic similarity. Text or syntactic similarity is not always sufficient for matching relationships. WordNet and the information theoretic semantic distance (Jiang and Conrath, 1997) are used to measure the semantic similarity between concepts in relationships such as mention, employment, ownership, etc. Other rule-based similarity. Several other cases requ</context>
</contexts>
<marker>Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>William W. Cohen, Pradeep Ravikumar, and Stephen E. Fienberg. 2003. A comparison of string distance metrics for name-matching tasks. In Proceedings of IJCAI Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Corsini</author>
<author>Beatrice Lazzerini</author>
<author>Francesco Marcelloni</author>
</authors>
<title>Anew fuzzy relational clustering algorithm based on the fuzzy c-means algorithm.</title>
<date>2005</date>
<journal>Soft Computing,</journal>
<volume>9</volume>
<issue>6</issue>
<pages>447</pages>
<contexts>
<context position="10177" citStr="Corsini et al., 2005" startWordPosition="1604" endWordPosition="1607">stance, the similarity between the occupations ‘President’ and ‘Commander in Chief’ can be computed using the JC semantic distance (Jiang and Conrath, 1997) with WordNet; the similarity of co-occurrence with other people can be measured by the Jaccard coefficient. In the next section, we propose to compute the relation strength r(·, ·) from the component similarities using aggregation weights learned from training data. Hence the N chained entities to be clustered can be represented as relational data using an nxn matrix R, where rj,k = r(ej, ek). The Any Relation Clustering Algorithm (ARCA) (Corsini et al., 2005; Cimino et al., 2006) represents relational data as object data using their mutual relation strength and uses FCM for clustering. We adopt this approach to transform (objectify) a relational pattern ej into an N dimensional vector rj (i.e. the j-th row in the matrix R) using a mapping Θ : D —* RN. In other words, each chained entity is represented as a vector of its relation strengths with all the entities. Fuzzy clusters can then be obtained by grouping closely related patterns using object clustering algorithm. Furthermore, it is well known that FCM is a spherical clustering algorithm and t</context>
</contexts>
<marker>Corsini, Lazzerini, Marcelloni, 2005</marker>
<rawString>Paolo Corsini, Beatrice Lazzerini, and Francesco Marcelloni. 2005. Anew fuzzy relational clustering algorithm based on the fuzzy c-means algorithm. Soft Computing, 9(6):439 – 447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajesh N Dave</author>
<author>Sumit Sen</author>
</authors>
<title>Robust fuzzy clustering of relational data.</title>
<date>2002</date>
<journal>IEEE Transactions on Fuzzy Systems,</journal>
<volume>10</volume>
<issue>6</issue>
<contexts>
<context position="27831" citStr="Dave and Sen, 2002" startWordPosition="4515" endWordPosition="4518">clustering variants. First, we ‘harden’ the fuzzy partition produced by KARC by allowing an entity to appear in the cluster with highest membership value (KARC-H). Purity improves because of the removal of noise entities, though at the sacrifice of inverse purity and the Table 2: Cross document coreference performance on subsets (I. Purity denotes inverse purity). Test set Identity Purity I. Purity F Wikipedia 56.5 0.666 0.752 0.717 ACL-06 31.0 0.783 0.771 0.773 US Census 50.3 0.554 0.889 0.754 F score deteriorates. We also implement a popular fuzzy relational clustering algorithm called FRC (Dave and Sen, 2002), whose optimization functional directly minimizes with respect to the relation matrix. With the same feature sets and distance function, KARC-S outperforms FRC in F score by about 5%. Because the test set is very ambiguous (on average only two documents per real world entity), the baselines have relatively high F score as observed in the WePS evaluation (Artiles et al., 2007). Table 2 further analyzes KARCS’s result on the three subsets Wikipedia, ACL06 and US Census. The F score is higher in the less ambiguous (the average number of identities) dataset and lower in the more ambiguous one, wi</context>
</contexts>
<marker>Dave, Sen, 2002</marker>
<rawString>Rajesh N. Dave and Sumit Sen. 2002. Robust fuzzy clustering of relational data. IEEE Transactions on Fuzzy Systems, 10(6):713–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
<author>Manfred K Warmuth</author>
</authors>
<title>Using and combining predictors that specialize.</title>
<date>1997</date>
<booktitle>In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing (STOC),</booktitle>
<pages>334--343</pages>
<contexts>
<context position="16450" citStr="Freund et al., 1997" startWordPosition="2677" endWordPosition="2680">in predicting the non-coreferent class is used as the distance metric. In our case of using information extraction results for disambiguation, however, only some of the similarity features are present based on the available relationships in two profiles. In this work, we propose to treat each similarity function as a specialist that specializes in computing the similarity of a particular type of relationship. Indeed, the similarity function between a pair of attributes or relationships may in itself be a sophisticated component algorithm. We utilize the specialist ensemble learning framework (Freund et al., 1997) to combine these component 1In particular, clustering algorithms that regularize the optimization with cluster size are not applicable in our case. 2In practice, the entities to be disambiguated tend to be dominated by several major identities. Hence performance generally does not vary much in the range of large C values. (5) 1 (d2ji) M−1 N P k=1 uMikKjk um ij IIΦ(rj) − Φ(wi)II2H N · min IIΦ(wi) − Φ(wj)II2H 1&lt;i&lt;j&lt;C KXBI = PC i=1 PN j=1 417 similarities into the relation strength for clustering. Here, a specialist is awakened for prediction only when the same type of relationships are present </context>
<context position="18489" citStr="Freund et al., 1997" startWordPosition="3030" endWordPosition="3033">−277xa(yt−yt) P� j∈Et (8) pje jEEt Otherwise:pt+1 i = pt i 5: end for Output: Model p The ensemble relation strength model is learned as follows. Given training data, the set of chained entities £train is extracted as described earlier. For a pair of entities ej and ek, a similarity vector s is computed using the component similarity functions for the respective attributes and relationships, and the true label is defined as y = I{ej and ek are coreferent}. The instances are subsampled to yield a balanced pairwise training set {&lt; st, yt &gt;}. We adopt the Specialist Exponentiated Gradient (SEG) (Freund et al., 1997) algorithm to learn the mixing weights of the specialists’ prediction (Algorithm 2) in an online manner. In each training iteration, an instance &lt; st, yt &gt; is presented to the learner (with Et denoting the set of indices of awake specialists in st). The SEG algorithm first predicts the value yt based on the awake specialists’ decisions. The true value yt is then revealed and the learner incurs a square loss between the predicted and the true values. The current weight distribution p is updated to minimize square loss: awake specialists are promoted or demoted in their weights according to the </context>
</contexts>
<marker>Freund, Schapire, Singer, Warmuth, 1997</marker>
<rawString>Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. 1997. Using and combining predictors that specialize. In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing (STOC), pages 334–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung H Gooi</author>
<author>James Allan</author>
</authors>
<title>Crossdocument coreference on a large scale corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="3188" citStr="Gooi and Allan, 2004" startWordPosition="467" endWordPosition="470">creasing interests in the web person search task (Artiles et al., 2007). Here, a search query for a person name is entered into a search engine and the desired outputs are documents clustered according to the identities of the entities in question. In our work, we propose to drill down to the subdocument mention level and construct an entity profile with the support of information extraction tools and reconciled with WDC methods. Hence our IE based approach has access to accurate information such as a person’s mentions and geolocations for disambiguation. Simple IR based CDC approaches (e.g. (Gooi and Allan, 2004)), on the other hand, may simply use all the terms and this can be detrimental to accuracy. For example, a biography of John F. Kennedy is likely to mention members of his family with related positions, besides references to other political figures. Even with careful word selection, these textual features can still confuse the disambiguation system about the true identity of the person. We propose to handle the CDC task using a novel kernelized fuzzy relational clustering algorithm, which allows probabilistic cluster membership assignment. This not only addresses the intrinsic uncertainty natu</context>
<context position="30462" citStr="Gooi and Allan, 2004" startWordPosition="4961" endWordPosition="4964">metrics offset each other and the F score is relatively stable across a broad range of 0 settings. 4 Related Work The original work in (Bagga and Baldwin, 1998) proposed a CDC system by first performing WDC and then disambiguating based on the summary sentences of the chains. This is similar to ours in that mentions rather than documents are clustered, leveraging the advances in state-of-the-art WDC methods developed in NLP, e.g. (Ng and Cardie, 2001; Yang et al., 2008). On the other hand, our work goes beyond the simple bag-of-word features and vector space model in (Bagga and Baldwin, 1998; Gooi and Allan, 2004) with IE results. (Wan et al., 2005) describes a person resolution system WebHawk that clusters web pages using some extracted personal information including person name, title, organization, email and phone number, besides lexical features. (Mann and Yarowsky, 2003) extracts biographical information, which is relatively scarce in web data, for disambiguation. With the support of state-of-the-art information extraction tools, the profiles of entities in this work covers a broader range of relational information. (Niu et al., 2004) also leveraged IE support, but their approach was evaluated on </context>
</contexts>
<marker>Gooi, Allan, 2004</marker>
<rawString>Chung H. Gooi and James Allan. 2004. Crossdocument coreference on a large scale corpus. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of International Conference Research on Computational Linguistics.</booktitle>
<contexts>
<context position="9713" citStr="Jiang and Conrath, 1997" startWordPosition="1529" endWordPosition="1532">classical Fuzzy C-Means (FCM) algorithm (Bezdek, 1981), but major differences exist. FCM, as most object clustering algorithms, deals with object data represented in a vectorial form. In our case, the data is purely relational and only the mutual relationships between entities can be determined. To be exact, we can define the similarity/dissimilarity between a pair of attributes or relationships of the same type l between entities ej and ek as s(l)(ej, ek). For instance, the similarity between the occupations ‘President’ and ‘Commander in Chief’ can be computed using the JC semantic distance (Jiang and Conrath, 1997) with WordNet; the similarity of co-occurrence with other people can be measured by the Jaccard coefficient. In the next section, we propose to compute the relation strength r(·, ·) from the component similarities using aggregation weights learned from training data. Hence the N chained entities to be clustered can be represented as relational data using an nxn matrix R, where rj,k = r(ej, ek). The Any Relation Clustering Algorithm (ARCA) (Corsini et al., 2005; Cimino et al., 2006) represents relational data as object data using their mutual relation strength and uses FCM for clustering. We ad</context>
<context position="25639" citStr="Jiang and Conrath, 1997" startWordPosition="4173" endWordPosition="4176">elationships in a pair of entity profiles match or not: Text similarity. To decide whether two names in the co-occurrence or family relationship match, we use the SoftTFIDF measure (Cohen et al., 2003), which is a hybrid matching scheme that combines the token-based TFIDF with the JaroWinkler string distance metric. This permits inexact matching of named entities due to name Purity(P, Q) = C |Ci |max i=1 n j 419 variations, typos, etc. Semantic similarity. Text or syntactic similarity is not always sufficient for matching relationships. WordNet and the information theoretic semantic distance (Jiang and Conrath, 1997) are used to measure the semantic similarity between concepts in relationships such as mention, employment, ownership, etc. Other rule-based similarity. Several other cases require special treatment. For example, the employment relationships of Senator and D-N.Y. should match based on domain knowledge. Also, we design dictionary-based similarity functions to handle nicknames (Bill and William), acronyms (COLING for International Conference on Computational Linguistics), and geo-locations. 3.4 Evaluation Results From the WePS training data, we generated a training set of around 32k pairwise ins</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of International Conference Research on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Paul Morie</author>
<author>Dan Roth</author>
</authors>
<title>Robust reading: Identification and tracing of ambiguous names.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>17--24</pages>
<contexts>
<context position="31710" citStr="Li et al., 2004" startWordPosition="5148" endWordPosition="5151"> the pairwise distance model is insomniac (i.e. all similarity specialists are awake for prediction) and our work extends this with a specialist learning framework. Prior work has largely relied on using hierarchical clustering methods for CDC, with the threshold for stopping the merging set using the training data, e.g. (Mann and Yarowsky, 2003; Chen and Martin, 2007; Baron and Freedman, 2008). The fuzzy relational clustering method proposed in this paper we believe better addresses the uncertainty aspect of the CDC problem. There are also orthogonal research directions for the CDC problem. (Li et al., 2004) solved the CDC problem by adopting a probabilistic view on how documents are generated and how names are sprinkled into them. (Bunescu and Pasca, 2006) showed that external information from Wikipedia can improve the disambiguation performance. 5 Conclusions We have presented a profile-based Cross Document Coreference (CDC) approach based on a novel fuzzy relational clustering algorithm KARC. In contrast to traditional hard clustering methods, KARC produces fuzzy sets of identities which better reflect the intrinsic uncertainty of the CDC problem. Kernelization, as used in KARC, enables the op</context>
</contexts>
<marker>Li, Morie, Roth, 2004</marker>
<rawString>Xin Li, Paul Morie, and Dan Roth. 2004. Robust reading: Identification and tracing of ambiguous names. In Proceedings of the Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>In Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>33--40</pages>
<contexts>
<context position="7365" citStr="Mann and Yarowsky, 2003" startWordPosition="1133" endWordPosition="1136">C techniques. In addition, profile based CDC facilitates user information consumption with structured information and short summary passages. Next, we focus on the relational clustering algorithm that lies at the core of the profile based CDC system. We then turn our attention to the specialist learning algorithm for the distance function used in clustering, capable of leveraging the available training data. 2.2 CDC Using Fuzzy Relational Clustering 2.2.1 Preliminaries Traditionally, hard clustering algorithms (where uij E {0,1}) such as complete linkage hierarchical agglomerative clustering (Mann and Yarowsky, 2003) have been applied to the disambiguation problem. In this work, we propose to use fuzzy clustering methods (relaxing the membership condition to uij E [0, 1]) as a better way of handling uncertainty in cross document coreference. First, consider the following motivating example, Example. The named entity President Bush is extracted from the sentence “President Bush addressed the nation from the Oval Office Monday.” • Without additional cues, a hard clustering algorithm has to arbitrarily assign the mention “President Bush” to either the NE “George W. Bush” or “George H. W. Bush”. • A soft clus</context>
<context position="30729" citStr="Mann and Yarowsky, 2003" startWordPosition="4999" endWordPosition="5002"> the chains. This is similar to ours in that mentions rather than documents are clustered, leveraging the advances in state-of-the-art WDC methods developed in NLP, e.g. (Ng and Cardie, 2001; Yang et al., 2008). On the other hand, our work goes beyond the simple bag-of-word features and vector space model in (Bagga and Baldwin, 1998; Gooi and Allan, 2004) with IE results. (Wan et al., 2005) describes a person resolution system WebHawk that clusters web pages using some extracted personal information including person name, title, organization, email and phone number, besides lexical features. (Mann and Yarowsky, 2003) extracts biographical information, which is relatively scarce in web data, for disambiguation. With the support of state-of-the-art information extraction tools, the profiles of entities in this work covers a broader range of relational information. (Niu et al., 2004) also leveraged IE support, but their approach was evaluated on a small artificial corpus. Also, the pairwise distance model is insomniac (i.e. all similarity specialists are awake for prediction) and our work extends this with a specialist learning framework. Prior work has largely relied on using hierarchical clustering methods</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>Gideon S. Mann and David Yarowsky. 2003. Unsupervised personal name disambiguation. In Conference on Computational Natural Language Learning (CoNLL), pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>104--111</pages>
<contexts>
<context position="30295" citStr="Ng and Cardie, 2001" startWordPosition="4932" endWordPosition="4935">solution. On the other hand, inverse purity decreases meaning more coreferent entities are not linked due to the stricter threshold. Overall, the changes in the two metrics offset each other and the F score is relatively stable across a broad range of 0 settings. 4 Related Work The original work in (Bagga and Baldwin, 1998) proposed a CDC system by first performing WDC and then disambiguating based on the summary sentences of the chains. This is similar to ours in that mentions rather than documents are clustered, leveraging the advances in state-of-the-art WDC methods developed in NLP, e.g. (Ng and Cardie, 2001; Yang et al., 2008). On the other hand, our work goes beyond the simple bag-of-word features and vector space model in (Bagga and Baldwin, 1998; Gooi and Allan, 2004) with IE results. (Wan et al., 2005) describes a person resolution system WebHawk that clusters web pages using some extracted personal information including person name, title, organization, email and phone number, besides lexical features. (Mann and Yarowsky, 2003) extracts biographical information, which is relatively scarce in web data, for disambiguation. With the support of state-of-the-art information extraction tools, the</context>
</contexts>
<marker>Ng, Cardie, 2001</marker>
<rawString>Vincent Ng and Claire Cardie. 2001. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheng Niu</author>
<author>Wei Li</author>
<author>Rohini K Srihari</author>
</authors>
<title>Weakly supervised learning for cross-document person name disambiguation supported by information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>597--604</pages>
<contexts>
<context position="30998" citStr="Niu et al., 2004" startWordPosition="5037" endWordPosition="5040">eatures and vector space model in (Bagga and Baldwin, 1998; Gooi and Allan, 2004) with IE results. (Wan et al., 2005) describes a person resolution system WebHawk that clusters web pages using some extracted personal information including person name, title, organization, email and phone number, besides lexical features. (Mann and Yarowsky, 2003) extracts biographical information, which is relatively scarce in web data, for disambiguation. With the support of state-of-the-art information extraction tools, the profiles of entities in this work covers a broader range of relational information. (Niu et al., 2004) also leveraged IE support, but their approach was evaluated on a small artificial corpus. Also, the pairwise distance model is insomniac (i.e. all similarity specialists are awake for prediction) and our work extends this with a specialist learning framework. Prior work has largely relied on using hierarchical clustering methods for CDC, with the threshold for stopping the merging set using the training data, e.g. (Mann and Yarowsky, 2003; Chen and Martin, 2007; Baron and Freedman, 2008). The fuzzy relational clustering method proposed in this paper we believe better addresses the uncertainty</context>
</contexts>
<marker>Niu, Li, Srihari, 2004</marker>
<rawString>Cheng Niu, Wei Li, and Rohini K. Srihari. 2004. Weakly supervised learning for cross-document person name disambiguation supported by information extraction. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL), pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Alex Smola</author>
</authors>
<title>Learning with Kernels.</title>
<date>2002</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Sch¨olkopf, Smola, 2002</marker>
<rawString>Bernhard Sch¨olkopf and Alex Smola. 2002. Learning with Kernels. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah M Taylor</author>
</authors>
<title>Information extraction tools: Deciphering human language.</title>
<date>2004</date>
<journal>IT Professional,</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="24248" citStr="Taylor, 2004" startWordPosition="3964" endWordPosition="3965">ased CDC approach, however, is to merge the mention-level entities. In our evaluation, we adopt the document label (and the person search query) to annotate the entity profiles that corresponds to the person name search query. Despite the difference, the results of the one-in-one and all-in-one baselines are almost identical to those reported in the WePS evaluation (F = 0.52, 0.58 respectively). Hence the performance reported here is comparable to the official evaluation results (Artiles et al., 2007). 3.3 Information Extraction and Similarities We use an information extraction tool AeroText (Taylor, 2004) to construct the entity profiles. AeroText extracts two types of information for an entity. First, the attribute information about the person named entity includes first/middle/last names, gender, mention, etc. In addition, AeroText extracts relationship information between named entities, such as Family, List, Employment, Ownership, Citizen-ResidentReligion-Ethnicity and so on, as specified in the ACE evaluation. AeroText resolves the references of entities within a document and produces the entity profiles, used as input to the CDC system. Note that alternative IE or WDC tools, as well as a</context>
</contexts>
<marker>Taylor, 2004</marker>
<rawString>Sarah M. Taylor. 2004. Information extraction tools: Deciphering human language. IT Professional, 6(6):28 – 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York.</location>
<contexts>
<context position="12071" citStr="Vapnik, 1995" startWordPosition="1909" endWordPosition="1910">ce so that the structure of the data can be more easily and adequately discovered. Specifically, a nonlinear transformation Φ maps data in RN to H of possibly infinite dimensions (Hilbert space). The key idea is the kernel trick – without explicitly specifying Φ and H, the inner product in H can be computed by evaluating a kernel function K in the data space, i.e. &lt; Φ(ri), Φ(rj) &gt;= K(ri, rj) (one of the most frequently used kernel functions is the Gaussian RBF kernel: K(rj, rk) = exp(−AIIrj − rkII2)). This technique has been successfully applied to SVMs to classify nonlinearly separable data (Vapnik, 1995). Kernelization preserves the simplicity in the formalism of the underlying clustering algorithm, meanwhile it yields highly nonlinear boundaries so that spherical clustering algorithms can apply (e.g. (Zhang and Chen, 2003) developed a kernelized object clustering algorithm based on FCM). Let wi denote the objectified virtual cluster vi, i.e. wi = Θ(vi). Using the kernel trick, the squared distance between Φ(rj) and Φ(wi) in the feature space H can be computed as: IIΦ(rj) − Φ(wi)II2 (2) H = &lt; Φ(rj) − Φ(wi),Φ(rj) − Φ(wi) &gt; = &lt; Φ(rj), Φ(rj) &gt; −2 &lt; Φ(rj), Φ(wi) &gt; + &lt; Φ(wi), Φ(wi) &gt; = 2 − 2K(rj, </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Binggong Ding</author>
</authors>
<title>Person resolution in person search results: WebHawk.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management (CIKM),</booktitle>
<pages>163--170</pages>
<contexts>
<context position="30498" citStr="Wan et al., 2005" startWordPosition="4968" endWordPosition="4971">e is relatively stable across a broad range of 0 settings. 4 Related Work The original work in (Bagga and Baldwin, 1998) proposed a CDC system by first performing WDC and then disambiguating based on the summary sentences of the chains. This is similar to ours in that mentions rather than documents are clustered, leveraging the advances in state-of-the-art WDC methods developed in NLP, e.g. (Ng and Cardie, 2001; Yang et al., 2008). On the other hand, our work goes beyond the simple bag-of-word features and vector space model in (Bagga and Baldwin, 1998; Gooi and Allan, 2004) with IE results. (Wan et al., 2005) describes a person resolution system WebHawk that clusters web pages using some extracted personal information including person name, title, organization, email and phone number, besides lexical features. (Mann and Yarowsky, 2003) extracts biographical information, which is relatively scarce in web data, for disambiguation. With the support of state-of-the-art information extraction tools, the profiles of entities in this work covers a broader range of relational information. (Niu et al., 2004) also leveraged IE support, but their approach was evaluated on a small artificial corpus. Also, the</context>
</contexts>
<marker>Wan, Gao, Li, Ding, 2005</marker>
<rawString>Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding. 2005. Person resolution in person search results: WebHawk. In Proceedings of the 14th ACM international conference on Information and knowledge management (CIKM), pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuanli Lisa Xie</author>
<author>Gerardo Beni</author>
</authors>
<title>A validity measure for fuzzy clustering.</title>
<date>1991</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>13</volume>
<issue>8</issue>
<pages>847</pages>
<contexts>
<context position="14843" citStr="Xie and Beni, 1991" startWordPosition="2415" endWordPosition="2418">ters C; threshold B initialize D0 t +— 0 repeat t +— t + 1 // 1– Update membership matrix Ut: uij = PC 2 −M−1 h=1 (djh) 1 // 2– Update kernel distance matrix Dt: d2ji = 2 − 2 · N P uik M k=1 until (t &gt; maxIter) or (t &gt; 1 and IUt − Ut−1I &lt; c) Pθ +— Generate soft partition(Ut,B) Output: Fuzzy partition Pθ 2.2.3 Cluster Validation In the CDC setting, the number of true underlying identities may vary depending on the entities’ level of ambiguity (e.g. name frequency). Selecting the optimal number of clusters is in general a hard research question in clustering1. We adopt the Xie-Beni Index (XBI) (Xie and Beni, 1991) as in ARCA, which is one of the most popular cluster validities for fuzzy clustering algorithms. XieBeni Index (XBI) measures the goodness of clustering using the ratio of the intra-cluster variation and the inter-cluster separation. We measure the kernelized XBI (KXBI) in the feature space as, where the nominator is readily computed using D and the inter-cluster separation in the denominator can be evaluated using the similar kernel trick above (details omitted). Note that KXBI is only defined for C &gt; 1. Thus we pick the C that corresponds to the first minimum of KXBI, and then compare its o</context>
</contexts>
<marker>Xie, Beni, 1991</marker>
<rawString>Xuanli Lisa Xie and Gerardo Beni. 1991. A validity measure for fuzzy clustering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(8):841 – 847.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Jun Lang</author>
<author>Chew L Tan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>An entitymention model for coreference resolution with inductive logic programming.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>843--851</pages>
<contexts>
<context position="30315" citStr="Yang et al., 2008" startWordPosition="4936" endWordPosition="4939">r hand, inverse purity decreases meaning more coreferent entities are not linked due to the stricter threshold. Overall, the changes in the two metrics offset each other and the F score is relatively stable across a broad range of 0 settings. 4 Related Work The original work in (Bagga and Baldwin, 1998) proposed a CDC system by first performing WDC and then disambiguating based on the summary sentences of the chains. This is similar to ours in that mentions rather than documents are clustered, leveraging the advances in state-of-the-art WDC methods developed in NLP, e.g. (Ng and Cardie, 2001; Yang et al., 2008). On the other hand, our work goes beyond the simple bag-of-word features and vector space model in (Bagga and Baldwin, 1998; Gooi and Allan, 2004) with IE results. (Wan et al., 2005) describes a person resolution system WebHawk that clusters web pages using some extracted personal information including person name, title, organization, email and phone number, besides lexical features. (Mann and Yarowsky, 2003) extracts biographical information, which is relatively scarce in web data, for disambiguation. With the support of state-of-the-art information extraction tools, the profiles of entitie</context>
</contexts>
<marker>Yang, Su, Lang, Tan, Liu, Li, 2008</marker>
<rawString>Xiaofeng Yang, Jian Su, Jun Lang, Chew L. Tan, Ting Liu, and Sheng Li. 2008. An entitymention model for coreference resolution with inductive logic programming. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 843–851.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dao-Qiang Zhang</author>
<author>Song-Can Chen</author>
</authors>
<title>Clustering incomplete data using kernel-based fuzzy c-means algorithm.</title>
<date>2003</date>
<journal>Neural Processing Letters,</journal>
<volume>18</volume>
<issue>3</issue>
<pages>162</pages>
<contexts>
<context position="12295" citStr="Zhang and Chen, 2003" startWordPosition="1939" endWordPosition="1942">kernel trick – without explicitly specifying Φ and H, the inner product in H can be computed by evaluating a kernel function K in the data space, i.e. &lt; Φ(ri), Φ(rj) &gt;= K(ri, rj) (one of the most frequently used kernel functions is the Gaussian RBF kernel: K(rj, rk) = exp(−AIIrj − rkII2)). This technique has been successfully applied to SVMs to classify nonlinearly separable data (Vapnik, 1995). Kernelization preserves the simplicity in the formalism of the underlying clustering algorithm, meanwhile it yields highly nonlinear boundaries so that spherical clustering algorithms can apply (e.g. (Zhang and Chen, 2003) developed a kernelized object clustering algorithm based on FCM). Let wi denote the objectified virtual cluster vi, i.e. wi = Θ(vi). Using the kernel trick, the squared distance between Φ(rj) and Φ(wi) in the feature space H can be computed as: IIΦ(rj) − Φ(wi)II2 (2) H = &lt; Φ(rj) − Φ(wi),Φ(rj) − Φ(wi) &gt; = &lt; Φ(rj), Φ(rj) &gt; −2 &lt; Φ(rj), Φ(wi) &gt; + &lt; Φ(wi), Φ(wi) &gt; = 2 − 2K(rj, wi) (3) assuming K(r, r) = 1. The KARC algorithm defines the generic distance d as d2(ej, vi) def = IIΦ(rj) − Φ(wi)II2H = IIΦ(Θ(ej)) − Φ(Θ(vi))II2H (we also use d2ji as a notational shorthand). Using Lagrange Multiplier as i</context>
</contexts>
<marker>Zhang, Chen, 2003</marker>
<rawString>Dao-Qiang Zhang and Song-Can Chen. 2003. Clustering incomplete data using kernel-based fuzzy c-means algorithm. Neural Processing Letters, 18(3):155 – 162.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>