<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025886">
<title confidence="0.802992">
Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm
</title>
<note confidence="0.889752333333333">
Jingjing Liu, Stephanie Seneff
MIT Computer Science &amp; Artificial Intelligence Laboratory
32 Vassar Street, Cambridge, MA 02139
</note>
<email confidence="0.993791">
{jingl, seneff}@csail.mit.edu
</email>
<sectionHeader confidence="0.994634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890086956522">
This paper presents a parse-and-paraphrase pa-
radigm to assess the degrees of sentiment for
product reviews. Sentiment identification has
been well studied; however, most previous
work provides binary polarities only (positive
and negative), and the polarity of sentiment is
simply reversed when a negation is detected.
The extraction of lexical features such as uni-
gram/bigram also complicates the sentiment
classification task, as linguistic structure such
as implicit long-distance dependency is often
disregarded. In this paper, we propose an ap-
proach to extracting adverb-adjective-noun
phrases based on clause structure obtained by
parsing sentences into a hierarchical represen-
tation. We also propose a robust general solu-
tion for modeling the contribution of adver-
bials and negation to the score for degree of
sentiment. In an application involving extract-
ing aspect-based pros and cons from restaurant
reviews, we obtained a 45% relative improve-
ment in recall through the use of parsing me-
thods, while also improving precision.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999902962962963">
Online product reviews have provided an exten-
sive collection of free-style texts as well as prod-
uct ratings prepared by general users, which in
return provide grassroots contributions to users
interested in a particular product or service as
assistance. Yet, valuable as they are, free-style
reviews contain much noisy data and are tedious
to read through in order to reach an overall con-
clusion. Thus, we conducted this study to auto-
matically process and evaluate product reviews
in order to generate both numerical evaluation
and textual summaries of users’ opinions, with
the ultimate goal of adding value to real systems
such as a restaurant-guide dialogue system.
Sentiment summarization has been well stu-
died in the past decade (Turney, 2002; Pang et al.,
2002; Dave et al., 2003; Hu and Liu, 2004a,
2004b; Carenini et al., 2006; Liu et al., 2007).
The polarity of users’ sentiments in each seg-
ment of review texts is extracted, and the polari-
ties of individual sentiments are aggregated
among all the sentences/segments of texts to give
a numerical scaling on sentiment orientation.
Most of the work done for sentiment analysis
so far has employed shallow parsing features
such as part-of-speech tagging. Frequent adjec-
tives and nouns/noun phrases are extracted as
opinion words and representative product fea-
tures. However, the linguistic structure of the
sentence is usually not taken into consideration.
High level linguistic features, if well utilized and
accurately extracted, can provide much insight
into the semantic meaning of user opinions and
contribute to the task of sentiment identification.
Furthermore, in addition to adjectives and
nouns, adverbials and negation also play an im-
portant role in determining the degree of the
orientation level. For example, “very good” and
“good” certainly express different degrees of
positive sentiment. Also, in previous studies,
when negative expressions are identified, the
polarity of sentiment in the associated segment
of text is simply reversed. However, semantic
expressions are quite different from the absolute
opposite values in mathematics. For example,
“not bad” does not express the opposite meaning
of “bad”, which would be highly positive. Simp-
ly reversing the polarity of sentiment on the ap-
pearance of negations may result in inaccurate
interpretation of sentiment expressions. Thus, a
system which attempts to quantify sentiment
while ignoring adverbials is missing a significant
component of the sentiment score, especially if
the adverbial is a negative word.
</bodyText>
<page confidence="0.966895">
161
</page>
<note confidence="0.996626">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161–169,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999830770833333">
Another challenging aspect of negation is
proper scoping of the negative reference over the
right constituent, which we argue, can be han-
dled quite well with careful linguistic analysis.
Take the sentence “I don’t think the place is very
clean” as example. A linguistic approach asso-
ciating long-distance elements with semantic
relations can identify that the negation “not”
scopes over the complement clause, thus extract-
ing “not very clean” instead of “very clean”.
Our goal in modeling adverbials is to investi-
gate whether a simple linear correction model
can capture the polarity contribution of all ad-
verbials. Furthermore, is it also appropriate to
adjust for multiple adverbs, including negation,
via a linear additive model? I.e., can “not very
good” be modeled as not(very(good))? The fact
that “not very good” seems to be less negative
than “not good” suggests that such an algorithm
might work well. From these derivations we have
developed a model which treats negations in the
exact same way as modifying adverbs, via an
accumulative linear offset model. This yields a
very generic and straightforward solution to
modeling the strength of sentiment expression.
In this paper we utilize a parse-and-paraphrase
paradigm to identify semantically related phrases
in review texts, taking quantifiers (e.g., modify-
ing adverbs) and qualifiers (e.g., negations) into
special consideration. The approach makes use
of a lexicalized probabilistic syntactic grammar
to identify and extract sets of adverb-adjective-
noun phrases that match review-related patterns.
Such patterns are constructed based on well-
formed linguistic structure; thus, relevant phrases
can be extracted reliably.
We also propose a cumulative linear offset
model to calculate the degree of sentiment for
joint adjectives and quantifiers/qualifiers. The
proposed sentiment prediction model takes mod-
ifying adverbs and negations as universal scales
on strength of sentiment, and conducts cumula-
tive calculation on the degree of sentiment for
the associated adjective. With this model, we can
provide not only qualitative textual summariza-
tion such as “good food” and “bad service”, but
also a numerical scoring of sentiment, i.e., “how
good the food is” and “how bad the service is.”
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999965708333333">
There have been many studies on sentiment
classification and opinion summarization (Pang
and Lee, 2004, 2005; Gamon et al., 2005; Popes-
cu and Etzioni, 2005; Liu et al., 2005; Zhuang et
al., 2006; Kim and Hovy, 2006). Specifically,
aspect rating as an interesting topic has also been
widely studied (Titov and McDonald, 2008a;
Snyder and Barzilay, 2007; Goldberg and Zhu,
2006). Recently, Baccianella et. al. (2009)
conducted a study on multi-facet rating of
product reviews with special emphasis on how to
generate vectorial representations of the text by
means of POS tagging, sentiment analysis, and
feature selection for ordinal regression learning.
Titov and McDonald (2008b) proposed a joint
model of text and aspect ratings which utilizes a
modified LDA topic model to build topics that
are representative of ratable aspects, and builds a
set of sentiment predictors. Branavan et al. (2008)
proposed a method for leveraging unstructured
annotations in product reviews to infer semantic
document properties, by clustering user
annotations into semantic properties and tying
the induced clusters to hidden topics in the text.
</bodyText>
<sectionHeader confidence="0.9906" genericHeader="method">
3 System Overview
</sectionHeader>
<bodyText confidence="0.999684222222222">
Our review summarization task is to extract sets
of descriptor-topic pairs (e.g., “excellent service”)
from a set of reviews (e.g., for a particular res-
taurant), and to cluster the extracted phrases into
representative aspects on a set of dimensions
(e.g., “food”, “service” and “atmosphere”). Dri-
ven by this motivation, we propose a three-stage
system that automatically processes reviews. A
block diagram is given in Figure 1.
</bodyText>
<figureCaption confidence="0.993578">
Figure 1. Framework of review processing.
</figureCaption>
<bodyText confidence="0.999926583333333">
The first stage is sentence-level data filtering.
Review data published by general users is often
in free-style, and a large fraction of the data is
either ill-formed or not relevant to the task. We
classify these as out of domain sentences. To fil-
ter out such noisy data, we collect unigram statis-
tics on all the relevant words in the corpus, and
select high frequency adjectives and nouns. Any
sentence that contains none of the high-
frequency nouns or adjectives is rejected from
further analysis. The remaining in-domain sen-
tences are subjected to the second stage, parse
</bodyText>
<page confidence="0.996751">
162
</page>
<bodyText confidence="0.998682521739131">
analysis and semantic understanding, for topic
extraction.
From the parsable sentences we extract de-
scriptor-topic phrase patterns based on a careful-
ly-designed generation grammar. We then apply
LM (language model) based topic clustering to
group the extracted phrases into representative
aspects. The third stage scores the degree of sen-
timent for adjectives, as well as the strength of
sentiment for modifying adverbs and negations,
which further refine the degree of sentiment of
the associated adjectives. We then run a linear
additive model to assign a combined sentiment
score for each extracted phrase.
The rest of the paper is structured as follows:
In Section 4, we explain the linguistic analysis.
In Section 5, we describe the cumulative model
for assessing the degree of sentiment. Section 6
provides a systematic evaluation, conducted on
real data in the restaurant review domain har-
vested from the Web. Section 7 provides a dis-
cussion analyzing the results. Section 8 summa-
rizes the paper as well as pointing to future work.
</bodyText>
<sectionHeader confidence="0.964623" genericHeader="method">
4 Linguistic Analysis
</sectionHeader>
<subsectionHeader confidence="0.999263">
4.1 Parse-and-Paraphrase
</subsectionHeader>
<bodyText confidence="0.999945035714286">
Our linguistic analysis is based on a parse-and-
paraphrase paradigm. Instead of the flat structure
of a surface string, the parser provides a hierar-
chical representation, which we call a linguistic
frame (Xu et al., 2008). It preserves linguistic
structure by encoding different layers of seman-
tic dependencies. The grammar captures syntac-
tic structure through a set of carefully con-
structed context free grammar rules, and employs
a feature-passing mechanism to enforce long dis-
tance constraints. The grammar is lexicalized,
and uses a statistical model to rank order compet-
ing hypotheses. It knows explicitly about 9,000
words, with all unknown words being interpreted
as nouns. The grammar probability model was
trained automatically on the corpus of review
sentences.
To produce the phrases, a set of generation
rules is carefully constructed to only extract sets
of related adverbs, adjectives and nouns. The
adjective-noun relationships are captured from
the following linguistic patterns: (1) all adjec-
tives attached directly to a noun in a noun phrase,
(2) adjectives embedded in a relative clause
modifying a noun, and (3) adjectives related to
nouns in a subject-predicate relationship in a
clause. These patterns are compatible, i.e., if a
clause contains both a modifying adjective and a
predicate adjective related to the same noun, two
adjective-noun pairs are generated by different
patterns. As in, “The efficient waitress was none-
theless very courteous.” It is a “parse-and-
paraphrase-like” paradigm: the paraphrase tries
to preserve the original words intact, while reor-
dering them and/or duplicating them into mul-
tiple NP units. Since they are based on syntactic
structure, the generation rules can also be applied
in any other domain involving opinion mining.
An example linguistic frame is shown in Fig-
ure 2, which encodes the sentence “The caesar
with salmon or chicken is really quite good.” In
this example, for the adjective “good”, the near-
by noun “chicken” would be associated with it if
only proximity is considered. From the linguistic
frame, however, we can easily associate “caesar”
with “good” by extracting the head of the topic
sub-frame and the head of the predicate sub-
frame, which are encoded in the same layer (root
layer) of the linguistic frame. Also, we can tell
from the predicate sub-frame that there is an ad-
verb “quite” modifying the head word “good”.
The linguistic frame also encodes an adverb “re-
ally” in the upstairs layer. A well-constructed
generation grammar can create customized ad-
verb-adjective-noun phrases such as “quite good
caesar” or “really quite good caesar”.
</bodyText>
<figure confidence="0.984468090909091">
(c cstatement
:topic {q caesar
:quantifier &amp;quot;def&amp;quot;
:pred (p with :topic (q salmon
:pred (p conjunction
:or (q chicken }}}}
:adv &amp;quot;really&amp;quot;
:pred {p adj_complement
:pred (p adjective
:adv &amp;quot;quite&amp;quot;
:pred (p quality :topic &amp;quot;good&amp;quot; }}}}
</figure>
<figureCaption confidence="0.8088285">
Figure 2. Linguistic frame for “The caesar with
salmon or chicken is really quite good.”
</figureCaption>
<bodyText confidence="0.999899166666667">
Interpreting negation in English is not
straightforward, and it is often impossible to do
correctly without a deep linguistic analysis. Xu-
ehui Wu (2005) wrote: “The scope of negation is
a complex linguistic phenomenon. It is easy to
perceive but hard to be defined from a syntactic
point of view. Misunderstanding or ambiguity
may occur when the negative scope is not un-
derstood clearly and correctly.” The majority
rule for negation is that it scopes over the re-
mainder of its containing clause, and this works
well for most cases. For example, Figure 3 shows
</bodyText>
<page confidence="0.994395">
163
</page>
<bodyText confidence="0.7802655">
the linguistic frame for the sentence “Their menu
was a good one that didn’t try to do too much.”
</bodyText>
<figure confidence="0.93244175">
(c cstatement
:topic (q menu :poss &amp;quot;their&amp;quot; } }
:complement (q pronoun :name “one”
:adj_clause (c cstatement
:conjn &amp;quot;that&amp;quot;
:negate &amp;quot;not&amp;quot;
:pred (p try :to_clause (p do
:topic (q object
:adv &amp;quot;too&amp;quot;
:quant &amp;quot;much&amp;quot; }}}}
:pred {p adjective
:pred (p quality :topic &amp;quot;good&amp;quot; }}}
</figure>
<figureCaption confidence="0.9863">
Figure 3. Linguistic frame for “Their menu was a
good one that didn’t try to do too much.”
</figureCaption>
<bodyText confidence="0.999987133333333">
Traditional approaches which do not consider
the linguistic structure would treat the appear-
ance of “not” as a negation and simply reverse
the sentiment of the sentence to negative polarity,
which is wrong as the sentence actually ex-
presses positive opinion for the topic “menu”. In
our approach, the negation “not” is identified as
under the sub-frame of the complement clause,
instead of in the same or higher layer of the ad-
jective sub-frame; thus it is considered as unre-
lated to the adjective “good”. In this way we can
successfully predict the scope of the reference of
the negation over the correct constituent of a sen-
tence and create proper association between ne-
gation and its modified words.
</bodyText>
<subsectionHeader confidence="0.998718">
4.2 LM-based Topic Clustering
</subsectionHeader>
<bodyText confidence="0.999980833333333">
To categorize the extracted phrases into repre-
sentative aspects, we automatically group the
identified topics into a set of clusters based on
LM probabilities. The LM-based algorithm as-
sumes that topics which are semantically related
have high probability of co-occurring with simi-
lar descriptive words. For example, “delicious”
might co-occur frequently with both “pizza” and
“dessert”. By examining the distribution of bi-
gram probability of these topics with correspond-
ing descriptive words, we can group “pizza” and
“dessert” into the same cluster of “food”.
We select a small set of the most common top-
ics, i.e., topics with the highest frequency counts,
and put them into an initial set I. Then, for each
candidate topic tc outside set I, we calculate its
probability given each topic ti within the initial
set I, given by:
</bodyText>
<equation confidence="0.999666">
P(tc |ti) = ∑aeAP(tc|a) · P(a|t)
� ∑ ���,���
���� · ���,���
��� P(ti)
= 1 ∑aeA 1 · P(a, tc) · P(a, ti) (1)
P(ti)P(a)
</equation>
<bodyText confidence="0.99999175">
where A represents the set of all the adjectives in
the corpus. For each candidate topic tc , we
choose the cluster of the initial topic ti with
which it has the highest probability score.
There are also cases where a meaningful ad-
jective occurs in the absence of an associated
topic, e.g., “It is quite expensive.” We call such
cases the “widow-adjective” case. Without hard-
coded ontology matching, it is difficult to identi-
fy “expensive” as a price-related expression. To
discover such cases and associate them with re-
lated topics, we propose a “surrogate topic”
matching approach based on bigram probability.
As aforementioned, the linguistic frame orga-
nizes all adjectives into separate clauses. Thus,
we create a “surrogate topic” category in the lin-
guistic frames for widow-adjective cases, which
makes it easy to detect descriptors that are affi-
liated with uninformative topics like the pronoun
“it”. We then have it generate phrases such as
“expensive surrogate_topic” and use bigram
probability statistics to automatically map each
sufficiently strongly associated adjective to its
most common topic among our major classes,
e.g., mapping “expensive” with its surrogate top-
ic “price”. Therefore, we can generate sets of
additional phrases in which the topic is “halluci-
nated” from the widow-adjective.
</bodyText>
<sectionHeader confidence="0.925504" genericHeader="method">
5 Assessment of Sentiment Strength
</sectionHeader>
<subsectionHeader confidence="0.984089">
5.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.9983552">
Given the sets of adverb-adjective-noun phrases
extracted by linguistic analysis, our goal is to
assign a score for the degree of sentiment to each
phrase and calculate an average rating for each
aspect. An example summary is given in Table 1.
</bodyText>
<tableCaption confidence="0.999724">
Table 1. Example of review summary.
</tableCaption>
<table confidence="0.997063777777778">
Aspect Extracted phrases Rating
Atmosphere very nice ambiance, 4.8
outdoor patio
Food not bad meal, 4.1
quite authentic food
Place not great place, 2.8
very smoky restaurant
Price so high bill, high cost, 2.2
not cheap price
</table>
<bodyText confidence="0.9992126">
To calculate the numerical degree of sentiment,
there are three major problems to solve: 1) how
to associate numerical scores with textual senti-
ment; 2) whether to calculate sentiment scores
for adjectives and adverbs jointly or separately; 3)
</bodyText>
<page confidence="0.993888">
164
</page>
<bodyText confidence="0.999959672727273">
whether to treat negations as special cases or in
the same way as modifying adverbs.
There have been studies on building sentiment
lexicons to define the strength of sentiment of
words. Esuli and Sebastiani (2006) constructed a
lexical resource, SentiWordNet, a WordNet-like
lexicon emphasizing sentiment orientation of
words and providing numerical scores of how
objective, positive and negative these words are.
However, lexicon-based methods can be tedious
and inefficient and may not be accurate due to
the complex cross-relations in dictionaries like
WordNet. Instead, our primary approach to sen-
timent scoring is to make use of collective data
such as user ratings. In product reviews collected
from online forums, the format of a review entry
often consists of three parts: pros/cons, free-style
text and user rating. We assume that user rating
is normally consistent with the tone of the review
text published by the same user. By associating
user ratings with each phrase extracted from re-
view texts, we can easily associate numerical
scores with textual sentiment.
A simple strategy of rating assignment is to
take each extracted adverb-adjective pair as a
composite unit. However, this method is likely to
lead to a large number of rare combinations, thus
suffering from sparse data problems. Therefore,
an interesting question to ask is whether it is
feasible to assign to each adverb a perturbation
score, which adjusts the score of the associated
adjective up or down by a fixed scalar value.
This approach thus hypothesizes that “very ex-
pensive” is as much worse than “expensive” as
“very romantic” is better than “romantic”. This
allows us to pool all instances of a given adverb
regardless of which adjective it is associated with,
in order to compute the absolute value of the per-
turbation score for that adverb. Therefore, we
consider adverbs and adjectives separately when
calculating the sentiment score, treating each
modifying adverb as a universal quantifier which
consistently scales up/down the strength of sen-
timent for the adjectives it modifies.
Furthermore, instead of treating negation as a
special case, the universal model works for all
adverbials. The model hypothesizes that “not bad”
is as much better than “bad” as “not good” is
worse than “good”, i.e., negations push posi-
tive/negative adjectives to the other side of sen-
timent polarity by a universal scale. This again,
allows us to pool all instances of a given nega-
tion and compute the absolute value of the per-
turbation score for that negation, in the same way
as dealing with modifying adverbs.
</bodyText>
<subsectionHeader confidence="0.98615">
5.2 Linear Additive Model
</subsectionHeader>
<bodyText confidence="0.986583181818182">
For each adjective, we average all its ratings giv-
en by:
∑ri n i
where P represents the set of appearances of ad-
jective adj, ri represents the associated user rat-
ing in each appearance of adj, # represents the
number of entities (e.g., restaurants) in the entire
data set, and $!i represents the number of entities
with rating ri. The score is averaged over all the
appearances, weighted by the frequency count of
each category of rating to remove bias towards
any category.
As for adverbs, using a slightly modified ver-
sion of equation (2), we can get a rating table for
all adverb-adjective pairs. For each adverb adv,
we get a list of all its possible combinations with
adjectives. Then, for each adj in the list, we cal-
culate the distance between the rating of adv-adj
and the rating of the adj alone. We then aggre-
gate the distances among all the pairs of adv-adj
and adj in the list, weighted by the frequency
count of each adv-adj pair:
</bodyText>
<equation confidence="0.979415">
·
∑ .E0 c&amp;&apos;(t-a)*,a)+./
Po1(adjt) · (r(ad%, adjt) 2 r(adjt)) (3)
</equation>
<bodyText confidence="0.9999169">
where co3$t(ad%, adjt) represents the count of
the combination ad% 2 adjt, 4 represents the set
of adjectives that co-occur with ad% ,
r(ad%, adjt) represents the sentiment rating of
the combination ad% 2 adjt , and r(adjt)
represents the sentiment rating of the adjective
adjt alone. Po1(adjt) represents the polarity of
adjt, assigned as 1 if adjt is positive, and -1 if
negative.
Specifically, negations are well handled by the
same scoring strategy, treated exactly the same
as modifying adverbs, except that they get such
strong negative scores that the sentiment of the
associated adjectives is pushed to the other side
of the polarity scale.
After obtaining the strength rating for adverbs
and the sentiment rating for adjectives, the next
step is to assign the strength of sentiment to each
phrase (negation-adverb-adjective-noun) ex-
tracted by linguistic analysis, as given by:
</bodyText>
<equation confidence="0.98645075">
Score 5$e6-ad%(adj)/7 = r(adj) 8
Po1(adj) · r(ad%) 8 Po1(adj) · r($e6) (4)
∑ � · !�
��&amp;quot; � �
Score adj) =
N (2)
Score ad%) = ∑ �&amp;&apos;(���)*,�)+,�
���
</equation>
<page confidence="0.986521">
165
</page>
<bodyText confidence="0.99851365">
where r(adj) represents the rating of adjective
adj, r(adv) represents the rating of adverb adv,
and r(neg) represents the rating of negation neg.
Po1(adj) represents the polarity of adj, assigned
as 1 if adj is positive, and -1 if negative. Thus, if
adj is positive, we assign a combined rating
r(adj) + r(adv) to this phrase. If it is negative,
we assign r(adj) — r(adv). Specifically, if it is
a negation case, we further assign a linear offset
r(neg) if adj is positive or —r(neg) if adj is
negative. For example, given the ratings &lt;good:
4.5&gt;, &lt;bad: 1.5&gt;, &lt;very: 0.5&gt; and &lt;not: -3.0&gt;,
we would assign “5.0” to “very good”
(score(very(good))=4.5+0.5), “1.0” to “very bad”
(score(very(bad))=1.5-0.5), and “2.0” to “not
very good” (score(not(very(good)))= 4.5+0.5-
3.0). The corresponding sequence of different
degrees of sentiment is: “very good: 5.0” &gt;
“good: 4.5” &gt; “not very good: 2.0” &gt; “bad: 1.5” &gt;
“very bad: 1.0”.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999369">
In this section we present a systematic evaluation
of the proposed approaches conducted on real
data. We crawled a data collection of 137,569
reviews on 24,043 restaurants in 9 cities in the
U.S. from an online restaurant evaluation web-
site1. Most of the reviews have both pros/cons
and free-style text. For the purpose of evaluation,
we take those reviews containing pros/cons as
the experimental set, which is 72.7% (99,147 re-
views) of the original set.
</bodyText>
<subsectionHeader confidence="0.99764">
6.1 Topic Extraction
</subsectionHeader>
<bodyText confidence="0.999912882352941">
Based on the experimental set, we first filtered
out-of-domain sentences based on frequency
count, leaving a set of 857,466 in-domain sen-
tences (67.5%). This set was then subjected to
parse analysis; 78.6% of them are parsable.
Given the parsing results in the format of lin-
guistic frame, we used a set of language genera-
tion rules to extract relevant adverb-adjective-
noun phrases. We then selected the most frequent
6 topics that represented appropriate dimensions
for the restaurant domain (“place”, “food”, “ser-
vice”, “price”, “atmosphere” and “portion”) as
the initial set, and clustered the extracted topic
mentions into different aspect categories by
creating a set of topic mappings with the LM-
based clustering method. Phrases not belonging
to any category are filtered out.
</bodyText>
<footnote confidence="0.706632">
1 http://www.citysearch.com
</footnote>
<bodyText confidence="0.999968763636364">
To evaluate the performance of the proposed
approach (LING) to topic extraction, we com-
pare it with a baseline method similar to (Hu and
Liu, 2004a, 2004b; Liu et al., 2005). We per-
formed part-of-speech tagging on both parsable
and unparsable sentences, extracted each pair of
noun and adjective that has the smallest proximi-
ty, and filtered out those with low frequency
counts. Adverbs and negation words that are ad-
jacent to the identified adjectives were also ex-
tracted along with the adjective-noun pairs. We
call this the “neighbor baseline” (NB).
The proposed method is unable to make use of
the non-parsable sentences, which make up over
20% of the data. Hence, it seems plausible to
utilize a back-off mechanism for these sentences
via a combined system (COMB) incorporating
NB only for the sentences that fail to parse.
In considering how to construct the “ground
truth” set of pros and cons for particular aspects,
our goal was to minimize error as much as possi-
ble without requiring exorbitant amounts of ma-
nual labeling. We also wanted to assure that the
methods were equally fair to both systems
(LING and NB). To these ends, we decided to
pool together all of the topic mappings and sur-
rogate topic hallucinations obtained automatical-
ly from both systems, and then to manually edit
the resulting list to eliminate any that were
deemed unreasonable. We then applied these
edited mappings in an automatic procedure to the
adjective-noun pairs in the user-provided pros
and cons of all the restaurant reviews. The result-
ing aspect-categorized phrase lists are taken as
the ground truth. Each system then used its own
(unedited) set of mappings in processing the as-
sociated review texts.
We also needed an algorithm to decide on a
particular set of reviews for consideration, again,
with the goal of omitting bias towards either of
the two systems. We decided to retain as the
evaluation set all reviews which obtained at least
one topic extraction from both systems. Thus the
two systems processed exactly the same data
with exactly the same definitions of “ground
truth”. Performance was evaluated on this set of
62,588 reviews in terms of recall (percentage of
topics in the ground truth that are also identified
from the review body) and precision (percentage
of extracted topics that are also in the ground
truth). These measures are computed separately
for each review, and then averaged over all re-
views.
As shown in Table 2, without clustering, the
LING approach gets 4.6% higher recall than the
</bodyText>
<page confidence="0.997221">
166
</page>
<bodyText confidence="0.999666333333333">
NB baseline. And the recall from the COMB ap-
proach is 3.9% higher than that from the LING
approach and 8.5% higher than that from the NB
baseline. With topic clustering, the COMB ap-
proach also gets the highest recall, with a 4.9%
and 17.5% increase from the LING approach and
the NB baseline respectively. The precision is
quite close among the different approaches,
around 60%. Table 2 also shows that the topic
clustering approach increases the recall by 4.8%
for the NB baseline, 12.8% for the LING ap-
proach, and 13.8% for the COMB approach.
</bodyText>
<tableCaption confidence="0.998954666666667">
Table 2. Experimental results of topic extraction by
the NB baseline, the proposed LING approach and
a combined system (COMB).
</tableCaption>
<table confidence="0.999809375">
No Clustering
NB LING COMB
Recall 39.6% 44.2% 48.1%
Precision 60.2% 60.0% 59.8%
With Clustering
NB LING COMB
Recall 44.4% 57.0% 61.9%
Precision 56.8% 61.1% 60.8%
</table>
<subsectionHeader confidence="0.999889">
6.2 Sentiment Scoring
</subsectionHeader>
<bodyText confidence="0.999986517241379">
To score the degree of sentiment for each ex-
tracted phrase, we built a table of sentiment score
(&lt;adjective: score&gt;) for adjectives and a table of
strength score (&lt;adverb: score&gt;) for adverbs.
The pros/cons often contain short and well-
structured phrases, and have better parsing quali-
ty than the long and complex sentences in free-
style texts; pros/cons also have clear sentiment
orientations. Thus, we use pros/cons to score the
sentiment of adjectives, which requires strong
polarity association. To obtain reliable ratings,
we associate the adjectives in the “pros” of re-
view entries which have a user rating 4 or 5, and
associate the adjectives in the “cons” of review
entries with user ratings 1 or 2 (the scale of user
rating is 1 to 5). Reviews with rating 3 are on the
boundary of sentiment, so we associate both
sides with the overall rating. On the other hand,
the frequencies of adverbs in free-style texts are
much higher than those in pros/cons, as
pros/cons mostly contain adjective-noun patterns.
Thus, we use free-style texts instead of pros/cons
to score the strength of adverbs.
Partial results of the sentiment scoring are
shown in Tables 3 and 4. As shown in Table 3,
the polarity of sentiment as well as the degree of
polarity of an adjective can be distinguished by
its score. The higher the sentiment score is, the
more positive the adjective is.
</bodyText>
<tableCaption confidence="0.997819">
Table 3. Sentiment scoring for selected adjectives.
</tableCaption>
<table confidence="0.999494">
Adjective Rating Adjective Rating
Excellent 5.0 Awesome 4.8
Easy 4.1 Great 4.4
Good 3.9 Limited 3.4
Inattentive 2.75 Overpriced 2.3
Rude 1.69 Horrible 1.3
</table>
<bodyText confidence="0.95562425">
Table 4 gives the scores of strength for most
common adverbs. The higher the strength score
is, the more the adverb scales up/down the de-
gree of sentiment of the adjective it modifies.
While “not” gets a strong negative score, some
adverbs such as “a little” (-0.65) and “a bit” (-
0.83) also get negative scores, indicating slightly
less sentiment for the associated adjectives.
</bodyText>
<tableCaption confidence="0.999394">
Table 4. Strength scoring for selected adverbs.
</tableCaption>
<table confidence="0.999590666666667">
Adverb Rating Adverb Rating
Super 0.58 Fairly 0.13
Extremely 0.54 Pretty 0.07
Incredibly 0.49 A little -0.65
Very 0.44 A bit -0.83
Really 0.39 Not -3.10
</table>
<bodyText confidence="0.999706142857143">
To evaluate the performance of sentiment
scoring, we randomly selected a subset of 1,000
adjective-noun phrases and asked two annotators
to independently rate the sentiment of each
phrase on a scale of 1 to 5. We compared the
sentiment scoring between our system and the
annotations in a measurement of mean distance:
</bodyText>
<equation confidence="0.854747">
distance = �
|� |Zp s |rLr - rap |(5)
</equation>
<bodyText confidence="0.999981545454545">
where S represents the set of phrases, p
represents each phrase in the set S, rig, represents
the rating on phrase p from our sentiment scor-
ing system, and rap represents the annotated rat-
ing on phrase p. As shown in Table 5, the ob-
tained mean distance between the scoring from
our approach and that from each annotation set is
0.46 and 0.43 respectively, based on the absolute
rating scale from 1 to 5. This shows that the scor-
ing of sentiment from our system is quite close to
human annotation. The kappa agreement be-
tween the two annotation sets is 0.68, indicating
high consistency between the annotators. The
reliability of these results gives us sufficient con-
fidence to make use of the scores of sentiments
for summarization.
To examine the prediction of sentiment polari-
ty, for each annotation set, we pooled the phrases
with rating 4/5 into “positive”, rating 1/2 into
“negative”, and rating 3 into “neutral”. Then we
rounded up the sentiment scores from our system
to integers and pooled the scores into three polar-
</bodyText>
<page confidence="0.992551">
167
</page>
<bodyText confidence="0.999894">
ity sets (“positive”, “negative” and “neutral”) in
the same way. As shown in Table 5, the obtained
kappa agreement between the result from our
system and that from each annotation set is 0.55
and 0.60 respectively. This shows reasonably
high agreement on the polarity of sentiment be-
tween our system and human evaluation.
</bodyText>
<tableCaption confidence="0.946783">
Table 5. Comparison of sentiment scoring between
the proposed approach and two annotation sets.
</tableCaption>
<table confidence="0.999740333333333">
Annotation 1 Annotation 2
Mean distance 0.46 0.43
Kappa agreement 0.55 0.60
</table>
<tableCaption confidence="0.9209205">
Table 6. Experimental results of topic extraction
based on sentiment polarity matching.
</tableCaption>
<table confidence="0.999770875">
No Clustering
NB LING COMB
Recall 34.5% 38.9% 42.2%
Precision 53.8% 54.0% 53.3%
With Clustering
NB LING COMB
Recall 37.4% 49.7% 54.1%
Precision 48.5% 52.9% 51.4%
</table>
<bodyText confidence="0.999880928571429">
To evaluate the combination of topic extrac-
tion and sentiment identification, we repeated the
topic extraction experiments presented in Table 2,
but this time requiring as well a correct polarity
assignment to obtain a match with the pros/cons
ground truth. As shown in Table 6, the COMB
approach gets the highest recall both with and
without topic clustering, and the recall from the
LING approach is higher than that from the NB
baseline in both cases as well, indicating the su-
periority of the proposed approach. The precision
is stable among the different approaches, consis-
tent with the case without the consideration of
sentiment polarity.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999992431818182">
It is surprising that the parse-and-paraphrase me-
thod performs so well, despite the fact that it uti-
lizes less than 80% of the data (parsable set). In
this section, we will discuss two experiments that
were done to tease apart the contributions of dif-
ferent variables. In both experiments, we com-
pared the change in relative improvement in re-
call between NB and LING, relative to the values
in Table 6, in the with-clustering condition. In
the table, LING obtains a score of 49.7% for re-
call, which is a 33% relative increase from the
score for NB (37.4%). Three distinct factors
could play a role in the improvement: the widow-
adjective topic hallucinations, the topic mapping
for clustering, and the extracted phrases them-
selves. An experiment involving omitting topic
hallucinations from widow adjectives determined
that these account for 12% of the relative in-
crease. To evaluate the contribution of clustering,
we replaced the mapping tables used by both sys-
tems with the edited one used by the ground truth
computation. Thus, both systems made use of the
same mapping table, removing this variable from
consideration. This improved the performance of
both systems (NB and LING), but resulted in a
decrease of LING’s relative improvement by
17%. This implies that LING’s mapping table is
superior. Since both systems use the same senti-
ment scores for adjectives and adverbs, the re-
mainder of the difference (71%) must be due
simply to higher quality extracted phrases.
We suspected that over-generated phrases (the
40% of phrases that find no mappings in the
pros/cons) might not really be a problem. To test
this hypothesis, we selected 100 reviews for their
high density of extracted phrases, and manually
evaluated all the over-generated phrases. We
found that over 80% were well formed, correct,
and informative. Therefore, a lower precision
here does not necessarily mean poor performance,
but instead shows that the pros/cons provided by
users are often incomplete. By extracting sum-
maries from review texts we can recover addi-
tional valuable information.
</bodyText>
<sectionHeader confidence="0.998623" genericHeader="conclusions">
8 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999966043478261">
This paper presents a parse-and-paraphrase ap-
proach to assessing the degree of sentiment for
product reviews. A general purpose context free
grammar is employed to parse review sentences,
and semantic understanding methods are devel-
oped to extract representative negation-adverb-
adjective-noun phrases based on well-defined
semantic rules. A language modeling-based me-
thod is proposed to cluster topics into respective
categories. We also introduced in this paper a
cumulative linear offset model for supporting the
assessment of the strength of sentiment in adjec-
tives and quantifiers/qualifiers (including nega-
tions) on a numerical scale. We demonstrated
that the parse-and-paraphrase method can per-
form substantially better than a neighbor baseline
on topic extraction from reviews even with less
data. The future work focuses in two directions:
(1) building a relational database from the sum-
maries and ratings and using it to enhance users’
experiences in a multimodal spoken dialogue
system; and (2) applying our techniques to other
domains to demonstrate generality.
</bodyText>
<page confidence="0.997689">
168
</page>
<sectionHeader confidence="0.998323" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997955075471698">
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2009. Multi-facet Rating of Product Re-
views. In Proceedings of European Conference on
Information Retrieval.
S.R.K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of the Annual Conference of the As-
sociation for Computational Linguistics.
Giuseppe Carenini, Raymond Ng, and Adam Pauls
2006. Multi-Document Summarization of Evalua-
tive Text. In Proceedings of the Conference of the
European Chapter of the Association for Computa-
tional Linguistics.
Kushal Dave, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: opinion ex-
traction and semantic classification of product re-
views. In Proceedings of the International Confe-
rence on World Wide Web.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A Publicly Available Lexical Re-
source for Opinion Mining. In Proceedings of the
5th Conference on Language Resources and Evalu-
ation.
Michael Gamon, Anthony Aue, Simon Corston-Oliver,
and Eric Ringger . 2005. Pulse: Mining customer
opinions from free text. In Proceedings of the 6th
International Symposium on Intelligent Data Anal-
ysis.
Andrew Goldberg and Xiaojin Zhu. 2006. Seeing
stars when there aren&apos;t many stars: Graph-based
semi-supervised learning for sentiment categoriza-
tion. In HLT-NAACL 2006 Workshop on Text-
graphs: Graph-based Algorithms for Natural Lan-
guage Processing.
Minqing Hu and Bing Liu. 2004a. Mining and sum-
marizing customer reviews. In Proceedings of the
2004 ACM SIGKDD international conference on
Knowledge Discovery and Data mining.
Minqing Hu and Bing Liu. 2004b. Mining Opinion
Features in Customer Reviews. In Proceedings of
Nineteenth National Conference on Artificial Intel-
ligence.
Soo-Min Kim and Eduard Hovy. 2006. Automatic
identification of pro and con reasons in online re-
views. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 483–490.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opi-
nions on the web. In Proceedings of International
Conference on World Wide Web.
Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou
Huang, and Ming Zhou. 2007. Low-Quality Prod-
uct Review Detection in Opinion Summarization.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the Annual Conference of the Association for
Computational Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
Annual Conference of the Association for Compu-
tational Linguistics.
Bo Pang, Lillian Lee, and S. Vaithyanathan. 2002.
Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
A.M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Benjamin Snyder and Regina Barzilay. 2007. Mul-
tiple Aspect Ranking using the Good Grief Algo-
rithm. In Proceedings of the Joint Conference of
the North American Chapter of the Association for
Computational Linguistics and Human Language
Technologies.
Ivan Titov and Ryan McDonald. 2008a. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17h International Conference on
World Wide Web.
Ivan Titov and Ryan McDonald. 2008b. A Joint Mod-
el of Text and Aspect Ratings for Sentiment Sum-
marization. In Proceedings of the Annual Confe-
rence of the Association for Computational Lin-
guistics.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Sentiment orientation applied to unsupervised clas-
sification of reviews. In Proceedings of the Annual
Conference of the Association for Computational
Linguistics.
Xuehui Wu, 2005. On the Scope of Negation in Eng-
lish, Sino-US English Teaching, Vol. 2, No. 9, Sep.
2005. pp. 53-56.
Yushi Xu, Jingjing Liu, Stephanie Seneff. 2008.
Mandarin Language Understanding in Dialogue
Context. In Proceedings of International Sympo-
sium on Chinese Spoken Language Processing.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM international conference
on Information and knowledge management.
</reference>
<page confidence="0.99878">
169
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956005">
<title confidence="0.995614">Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm</title>
<author confidence="0.988534">Jingjing Liu</author>
<author confidence="0.988534">Stephanie Seneff</author>
<affiliation confidence="0.999194">MIT Computer Science &amp; Artificial Intelligence</affiliation>
<address confidence="0.999499">32 Vassar Street, Cambridge, MA 02139</address>
<email confidence="0.999835">jingl@csail.mit.edu</email>
<email confidence="0.999835">seneff@csail.mit.edu</email>
<abstract confidence="0.998799958333334">This paper presents a parse-and-paraphrase paradigm to assess the degrees of sentiment for product reviews. Sentiment identification has been well studied; however, most previous work provides binary polarities only (positive and negative), and the polarity of sentiment is simply reversed when a negation is detected. The extraction of lexical features such as unigram/bigram also complicates the sentiment classification task, as linguistic structure such as implicit long-distance dependency is often disregarded. In this paper, we propose an approach to extracting adverb-adjective-noun phrases based on clause structure obtained by parsing sentences into a hierarchical representation. We also propose a robust general solution for modeling the contribution of adverbials and negation to the score for degree of sentiment. In an application involving extracting aspect-based pros and cons from restaurant reviews, we obtained a 45% relative improvement in recall through the use of parsing methods, while also improving precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Multi-facet Rating of Product Reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of European Conference on Information Retrieval.</booktitle>
<marker>Baccianella, Esuli, Sebastiani, 2009</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2009. Multi-facet Rating of Product Reviews. In Proceedings of European Conference on Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning document-level semantic properties from free-text annotations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7138" citStr="Branavan et al. (2008)" startWordPosition="1081" endWordPosition="1084">lso been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors. Branavan et al. (2008) proposed a method for leveraging unstructured annotations in product reviews to infer semantic document properties, by clustering user annotations into semantic properties and tying the induced clusters to hidden topics in the text. 3 System Overview Our review summarization task is to extract sets of descriptor-topic pairs (e.g., “excellent service”) from a set of reviews (e.g., for a particular restaurant), and to cluster the extracted phrases into representative aspects on a set of dimensions (e.g., “food”, “service” and “atmosphere”). Driven by this motivation, we propose a three-stage sy</context>
</contexts>
<marker>Branavan, Chen, Eisenstein, Barzilay, 2008</marker>
<rawString>S.R.K. Branavan, Harr Chen, Jacob Eisenstein, and Regina Barzilay. 2008. Learning document-level semantic properties from free-text annotations. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Adam Pauls</author>
</authors>
<title>Multi-Document Summarization of Evaluative Text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2112" citStr="Carenini et al., 2006" startWordPosition="315" endWordPosition="318">icular product or service as assistance. Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion. Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users’ opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system. Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007). The polarity of users’ sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation. Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging. Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features. However, the linguistic structure of the sentence is usually not taken into consideration. High level linguisti</context>
</contexts>
<marker>Carenini, Ng, Pauls, 2006</marker>
<rawString>Giuseppe Carenini, Raymond Ng, and Adam Pauls 2006. Multi-Document Summarization of Evaluative Text. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on World Wide Web.</booktitle>
<contexts>
<context position="2063" citStr="Dave et al., 2003" startWordPosition="306" endWordPosition="309">s contributions to users interested in a particular product or service as assistance. Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion. Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users’ opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system. Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007). The polarity of users’ sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation. Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging. Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features. However, the linguistic structure of the sentence is usually n</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of the International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="17498" citStr="Esuli and Sebastiani (2006)" startWordPosition="2754" endWordPosition="2757">ce, 4.8 outdoor patio Food not bad meal, 4.1 quite authentic food Place not great place, 2.8 very smoky restaurant Price so high bill, high cost, 2.2 not cheap price To calculate the numerical degree of sentiment, there are three major problems to solve: 1) how to associate numerical scores with textual sentiment; 2) whether to calculate sentiment scores for adjectives and adverbs jointly or separately; 3) 164 whether to treat negations as special cases or in the same way as modifying adverbs. There have been studies on building sentiment lexicons to define the strength of sentiment of words. Esuli and Sebastiani (2006) constructed a lexical resource, SentiWordNet, a WordNet-like lexicon emphasizing sentiment orientation of words and providing numerical scores of how objective, positive and negative these words are. However, lexicon-based methods can be tedious and inefficient and may not be accurate due to the complex cross-relations in dictionaries like WordNet. Instead, our primary approach to sentiment scoring is to make use of collective data such as user ratings. In product reviews collected from online forums, the format of a review entry often consists of three parts: pros/cons, free-style text and u</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining. In Proceedings of the 5th Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Simon Corston-Oliver</author>
<author>Eric Ringger</author>
</authors>
<title>Pulse: Mining customer opinions from free text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Symposium on Intelligent Data Analysis.</booktitle>
<contexts>
<context position="6370" citStr="Gamon et al., 2005" startWordPosition="960" endWordPosition="963">nt adjectives and quantifiers/qualifiers. The proposed sentiment prediction model takes modifying adverbs and negations as universal scales on strength of sentiment, and conducts cumulative calculation on the degree of sentiment for the associated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect rating</context>
</contexts>
<marker>Gamon, Aue, Corston-Oliver, Ringger, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, Simon Corston-Oliver, and Eric Ringger . 2005. Pulse: Mining customer opinions from free text. In Proceedings of the 6th International Symposium on Intelligent Data Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Goldberg</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Seeing stars when there aren&apos;t many stars: Graph-based semi-supervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In HLT-NAACL 2006 Workshop on Textgraphs: Graph-based Algorithms for Natural Language Processing.</booktitle>
<contexts>
<context position="6618" citStr="Goldberg and Zhu, 2006" startWordPosition="1001" endWordPosition="1004">ated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors. Branavan et al. (2008) proposed a method for leveraging unstructured annotations in product reviews to</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>Andrew Goldberg and Xiaojin Zhu. 2006. Seeing stars when there aren&apos;t many stars: Graph-based semi-supervised learning for sentiment categorization. In HLT-NAACL 2006 Workshop on Textgraphs: Graph-based Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 ACM SIGKDD international conference on Knowledge Discovery and Data mining.</booktitle>
<contexts>
<context position="2081" citStr="Hu and Liu, 2004" startWordPosition="310" endWordPosition="313">users interested in a particular product or service as assistance. Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion. Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users’ opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system. Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007). The polarity of users’ sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation. Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging. Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features. However, the linguistic structure of the sentence is usually not taken into cons</context>
<context position="24315" citStr="Hu and Liu, 2004" startWordPosition="3876" endWordPosition="3879">ct relevant adverb-adjectivenoun phrases. We then selected the most frequent 6 topics that represented appropriate dimensions for the restaurant domain (“place”, “food”, “service”, “price”, “atmosphere” and “portion”) as the initial set, and clustered the extracted topic mentions into different aspect categories by creating a set of topic mappings with the LMbased clustering method. Phrases not belonging to any category are filtered out. 1 http://www.citysearch.com To evaluate the performance of the proposed approach (LING) to topic extraction, we compare it with a baseline method similar to (Hu and Liu, 2004a, 2004b; Liu et al., 2005). We performed part-of-speech tagging on both parsable and unparsable sentences, extracted each pair of noun and adjective that has the smallest proximity, and filtered out those with low frequency counts. Adverbs and negation words that are adjacent to the identified adjectives were also extracted along with the adjective-noun pairs. We call this the “neighbor baseline” (NB). The proposed method is unable to make use of the non-parsable sentences, which make up over 20% of the data. Hence, it seems plausible to utilize a back-off mechanism for these sentences via a </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004a. Mining and summarizing customer reviews. In Proceedings of the 2004 ACM SIGKDD international conference on Knowledge Discovery and Data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining Opinion Features in Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of Nineteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2081" citStr="Hu and Liu, 2004" startWordPosition="310" endWordPosition="313">users interested in a particular product or service as assistance. Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion. Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users’ opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system. Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007). The polarity of users’ sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation. Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging. Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features. However, the linguistic structure of the sentence is usually not taken into cons</context>
<context position="24315" citStr="Hu and Liu, 2004" startWordPosition="3876" endWordPosition="3879">ct relevant adverb-adjectivenoun phrases. We then selected the most frequent 6 topics that represented appropriate dimensions for the restaurant domain (“place”, “food”, “service”, “price”, “atmosphere” and “portion”) as the initial set, and clustered the extracted topic mentions into different aspect categories by creating a set of topic mappings with the LMbased clustering method. Phrases not belonging to any category are filtered out. 1 http://www.citysearch.com To evaluate the performance of the proposed approach (LING) to topic extraction, we compare it with a baseline method similar to (Hu and Liu, 2004a, 2004b; Liu et al., 2005). We performed part-of-speech tagging on both parsable and unparsable sentences, extracted each pair of noun and adjective that has the smallest proximity, and filtered out those with low frequency counts. Adverbs and negation words that are adjacent to the identified adjectives were also extracted along with the adjective-noun pairs. We call this the “neighbor baseline” (NB). The proposed method is unable to make use of the non-parsable sentences, which make up over 20% of the data. Hence, it seems plausible to utilize a back-off mechanism for these sentences via a </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004b. Mining Opinion Features in Customer Reviews. In Proceedings of Nineteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic identification of pro and con reasons in online reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>483--490</pages>
<contexts>
<context position="6457" citStr="Kim and Hovy, 2006" startWordPosition="977" endWordPosition="980">s modifying adverbs and negations as universal scales on strength of sentiment, and conducts cumulative calculation on the degree of sentiment for the associated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of </context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Automatic identification of pro and con reasons in online reviews. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 483–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web. In</title>
<date>2005</date>
<booktitle>Proceedings of International Conference on World Wide Web.</booktitle>
<contexts>
<context position="6415" citStr="Liu et al., 2005" startWordPosition="969" endWordPosition="972">roposed sentiment prediction model takes modifying adverbs and negations as universal scales on strength of sentiment, and conducts cumulative calculation on the degree of sentiment for the associated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model t</context>
<context position="24342" citStr="Liu et al., 2005" startWordPosition="3881" endWordPosition="3884">venoun phrases. We then selected the most frequent 6 topics that represented appropriate dimensions for the restaurant domain (“place”, “food”, “service”, “price”, “atmosphere” and “portion”) as the initial set, and clustered the extracted topic mentions into different aspect categories by creating a set of topic mappings with the LMbased clustering method. Phrases not belonging to any category are filtered out. 1 http://www.citysearch.com To evaluate the performance of the proposed approach (LING) to topic extraction, we compare it with a baseline method similar to (Hu and Liu, 2004a, 2004b; Liu et al., 2005). We performed part-of-speech tagging on both parsable and unparsable sentences, extracted each pair of noun and adjective that has the smallest proximity, and filtered out those with low frequency counts. Adverbs and negation words that are adjacent to the identified adjectives were also extracted along with the adjective-noun pairs. We call this the “neighbor baseline” (NB). The proposed method is unable to make use of the non-parsable sentences, which make up over 20% of the data. Hence, it seems plausible to utilize a back-off mechanism for these sentences via a combined system (COMB) inco</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In Proceedings of International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingjing Liu</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Yalou Huang</author>
<author>Ming Zhou</author>
</authors>
<title>Low-Quality Product Review Detection in Opinion Summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2131" citStr="Liu et al., 2007" startWordPosition="319" endWordPosition="322">ce as assistance. Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion. Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users’ opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system. Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007). The polarity of users’ sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation. Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging. Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features. However, the linguistic structure of the sentence is usually not taken into consideration. High level linguistic features, if well</context>
</contexts>
<marker>Liu, Cao, Lin, Huang, Zhou, 2007</marker>
<rawString>Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou Huang, and Ming Zhou. 2007. Low-Quality Product Review Detection in Opinion Summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6344" citStr="Pang and Lee, 2004" startWordPosition="955" endWordPosition="958">egree of sentiment for joint adjectives and quantifiers/qualifiers. The proposed sentiment prediction model takes modifying adverbs and negations as universal scales on strength of sentiment, and conducts cumulative calculation on the degree of sentiment for the associated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2044" citStr="Pang et al., 2002" startWordPosition="302" endWordPosition="305">n provide grassroots contributions to users interested in a particular product or service as assistance. Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion. Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users’ opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system. Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007). The polarity of users’ sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation. Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging. Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features. However, the linguistic structure of the se</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6397" citStr="Popescu and Etzioni, 2005" startWordPosition="964" endWordPosition="968">antifiers/qualifiers. The proposed sentiment prediction model takes modifying adverbs and negations as universal scales on strength of sentiment, and conducts cumulative calculation on the degree of sentiment for the associated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A.M. Popescu and O. Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Multiple Aspect Ranking using the Good Grief Algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies.</booktitle>
<contexts>
<context position="6593" citStr="Snyder and Barzilay, 2007" startWordPosition="997" endWordPosition="1000">of sentiment for the associated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors. Branavan et al. (2008) proposed a method for leveraging unstructured annotati</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2007. Multiple Aspect Ranking using the Good Grief Algorithm. In Proceedings of the Joint Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17h International Conference on World Wide Web.</booktitle>
<contexts>
<context position="6565" citStr="Titov and McDonald, 2008" startWordPosition="993" endWordPosition="996"> calculation on the degree of sentiment for the associated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors. Branavan et al. (2008) proposed a method for leve</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008a. Modeling online reviews with multi-grain topic models. In Proceedings of the 17h International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A Joint Model of Text and Aspect Ratings for Sentiment Summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6565" citStr="Titov and McDonald, 2008" startWordPosition="993" endWordPosition="996"> calculation on the degree of sentiment for the associated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors. Branavan et al. (2008) proposed a method for leve</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008b. A Joint Model of Text and Aspect Ratings for Sentiment Summarization. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Sentiment orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2025" citStr="Turney, 2002" startWordPosition="300" endWordPosition="301">which in return provide grassroots contributions to users interested in a particular product or service as assistance. Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion. Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users’ opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system. Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007). The polarity of users’ sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation. Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging. Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features. However, the linguistic </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? Sentiment orientation applied to unsupervised classification of reviews. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuehui Wu</author>
</authors>
<title>On the Scope of Negation in</title>
<date>2005</date>
<journal>English, Sino-US English Teaching,</journal>
<volume>2</volume>
<pages>53--56</pages>
<contexts>
<context position="12614" citStr="Wu (2005)" startWordPosition="1948" endWordPosition="1949">onstructed generation grammar can create customized adverb-adjective-noun phrases such as “quite good caesar” or “really quite good caesar”. (c cstatement :topic {q caesar :quantifier &amp;quot;def&amp;quot; :pred (p with :topic (q salmon :pred (p conjunction :or (q chicken }}}} :adv &amp;quot;really&amp;quot; :pred {p adj_complement :pred (p adjective :adv &amp;quot;quite&amp;quot; :pred (p quality :topic &amp;quot;good&amp;quot; }}}} Figure 2. Linguistic frame for “The caesar with salmon or chicken is really quite good.” Interpreting negation in English is not straightforward, and it is often impossible to do correctly without a deep linguistic analysis. Xuehui Wu (2005) wrote: “The scope of negation is a complex linguistic phenomenon. It is easy to perceive but hard to be defined from a syntactic point of view. Misunderstanding or ambiguity may occur when the negative scope is not understood clearly and correctly.” The majority rule for negation is that it scopes over the remainder of its containing clause, and this works well for most cases. For example, Figure 3 shows 163 the linguistic frame for the sentence “Their menu was a good one that didn’t try to do too much.” (c cstatement :topic (q menu :poss &amp;quot;their&amp;quot; } } :complement (q pronoun :name “one” :adj_cl</context>
</contexts>
<marker>Wu, 2005</marker>
<rawString>Xuehui Wu, 2005. On the Scope of Negation in English, Sino-US English Teaching, Vol. 2, No. 9, Sep. 2005. pp. 53-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yushi Xu</author>
<author>Jingjing Liu</author>
<author>Stephanie Seneff</author>
</authors>
<title>Mandarin Language Understanding in Dialogue Context.</title>
<date>2008</date>
<booktitle>In Proceedings of International Symposium on Chinese Spoken Language Processing.</booktitle>
<contexts>
<context position="9746" citStr="Xu et al., 2008" startWordPosition="1493" endWordPosition="1496">guistic analysis. In Section 5, we describe the cumulative model for assessing the degree of sentiment. Section 6 provides a systematic evaluation, conducted on real data in the restaurant review domain harvested from the Web. Section 7 provides a discussion analyzing the results. Section 8 summarizes the paper as well as pointing to future work. 4 Linguistic Analysis 4.1 Parse-and-Paraphrase Our linguistic analysis is based on a parse-andparaphrase paradigm. Instead of the flat structure of a surface string, the parser provides a hierarchical representation, which we call a linguistic frame (Xu et al., 2008). It preserves linguistic structure by encoding different layers of semantic dependencies. The grammar captures syntactic structure through a set of carefully constructed context free grammar rules, and employs a feature-passing mechanism to enforce long distance constraints. The grammar is lexicalized, and uses a statistical model to rank order competing hypotheses. It knows explicitly about 9,000 words, with all unknown words being interpreted as nouns. The grammar probability model was trained automatically on the corpus of review sentences. To produce the phrases, a set of generation rules</context>
</contexts>
<marker>Xu, Liu, Seneff, 2008</marker>
<rawString>Yushi Xu, Jingjing Liu, Stephanie Seneff. 2008. Mandarin Language Understanding in Dialogue Context. In Proceedings of International Symposium on Chinese Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiao-Yan Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM international conference on Information and knowledge management.</booktitle>
<contexts>
<context position="6436" citStr="Zhuang et al., 2006" startWordPosition="973" endWordPosition="976">prediction model takes modifying adverbs and negations as universal scales on strength of sentiment, and conducts cumulative calculation on the degree of sentiment for the associated adjective. With this model, we can provide not only qualitative textual summarization such as “good food” and “bad service”, but also a numerical scoring of sentiment, i.e., “how good the food is” and “how bad the service is.” 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006). Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006). Recently, Baccianella et. al. (2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that a</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the 15th ACM international conference on Information and knowledge management.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>