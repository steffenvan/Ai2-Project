<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<note confidence="0.952081">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 295-302.
</note>
<title confidence="0.999064">
Discriminative Training and Maximum Entropy Models for Statistical
Machine Translation
</title>
<author confidence="0.990164">
Franz Josef Och and Hermann Ney
</author>
<affiliation confidence="0.823835333333333">
Lehrstuhl f¨ur Informatik VI, Computer Science Department
RWTH Aachen - University of Technology
D-52056 Aachen, Germany
</affiliation>
<email confidence="0.997127">
{och,ney}@informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998998">
We present a framework for statistical
machine translation of natural languages
based on direct maximum entropy mod-
els, which contains the widely used sour-
ce-channel approach as a special case. All
knowledge sources are treated as feature
functions, which depend on the source
language sentence, the target language
sentence and possible hidden variables.
This approach allows a baseline machine
translation system to be extended easily by
adding new feature functions. We show
that a baseline statistical machine transla-
tion system is significantly improved us-
ing this approach.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999616">
We are given a source (‘French’) sentence fJ1 =
f1, ... , fj, ... , fJ, which is to be translated into a
target (‘English’) sentence eI1 = e1, ... , ei, ... , eI.
Among all possible target sentences, we will choose
the sentence with the highest probability:1
</bodyText>
<equation confidence="0.927956">
ˆeI1 = argmax {Pr(eI1|fJ1 )} (1)
el 1
</equation>
<bodyText confidence="0.977515">
The argmax operation denotes the search problem,
i.e. the generation of the output sentence in the target
language.
</bodyText>
<footnote confidence="0.97113275">
1The notational convention will be as follows. We use the
symbol Pr(·) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(·).
</footnote>
<subsectionHeader confidence="0.537927">
1.1 Source-Channel Model
</subsectionHeader>
<bodyText confidence="0.9739455">
According to Bayes’ decision rule, we can equiva-
lently to Eq. 1 perform the following maximization:
</bodyText>
<equation confidence="0.9669915">
ˆeI1 = argmax {Pr(eI1) · Pr(fJ1 |eI1)} (2)
el 1
</equation>
<bodyText confidence="0.999715608695652">
This approach is referred to as source-channel ap-
proach to statistical MT. Sometimes, it is also re-
ferred to as the ‘fundamental equation of statisti-
cal MT’ (Brown et al., 1993). Here, Pr(eI1) is
the language model of the target language, whereas
Pr(fJ1 |eI1) is the translation model. Typically, Eq. 2
is favored over the direct translation model of Eq. 1
with the argument that it yields a modular approach.
Instead of modeling one probability distribution,
we obtain two different knowledge sources that are
trained independently.
The overall architecture of the source-channel ap-
proach is summarized in Figure 1. In general, as
shown in this figure, there may be additional trans-
formations to make the translation task simpler for
the algorithm. Typically, training is performed by
applying a maximum likelihood approach. If the
language model Pr(eI1) = pγ(eI1) depends on pa-
rameters γ and the translation model Pr(fJ1 |eI1) =
pθ(fJ1 |eI1) depends on parameters θ, then the opti-
mal parameter values are obtained by maximizing
the likelihood on a parallel training corpus fS1 , eS1
(Brown et al., 1993):
</bodyText>
<equation confidence="0.9304472">
pθ(fs|es) (3)
pγ(es) (4)
θˆ = argmax
θ
S
H
s=1
S
γˆ = argmax H
γ s=1
</equation>
<figure confidence="0.779337105263158">
Source
Language Text
2
Preprocessing
ˆei = argmax
el
1
{Pr(ei) · Pr(fJ1 |ei)}
Pr(ei): Language Model
Pr(fJ1 |ei): Translation Model
2
��
��
Global Search
2
Postprocessing
2
Target
Language Text
</figure>
<figureCaption confidence="0.999995">
Figure 1: Architecture of the translation approach based on source-channel models.
</figureCaption>
<bodyText confidence="0.98897">
We obtain the following decision rule: instead of Eq. 5 (Och et al., 1999):
</bodyText>
<equation confidence="0.866584333333333">
ˆei = argmax {pˆ�(ei) · pˆ�(fJ1 |ei)} (5) ˆei = argmax {pˆ�(ei) · pˆ�(e,|fJ1 )} (6)
el el
1 1
</equation>
<bodyText confidence="0.862263">
State-of-the-art statistical MT systems are based on
this approach. Yet, the use of this decision rule has
various problems:
</bodyText>
<listItem confidence="0.997986533333333">
1. The combination of the language model pˆ�(el)
and the translation model pˆ�(fJ1 |ei) as shown
in Eq. 5 can only be shown to be optimal if the
true probability distributions pˆ�(ei) = Pr(ei)
and pˆ�(fJ1 |ei) = Pr(fJ1 |ei) are used. Yet,
we know that the used models and training
methods provide only poor approximations of
the true probability distributions. Therefore, a
different combination of language model and
translation model might yield better results.
2. There is no straightforward way to extend a
baseline statistical MT model by including ad-
ditional dependencies.
3. Often, we observe that comparable results are
obtained by using the following decision rule
</listItem>
<bodyText confidence="0.999989">
Here, we replaced pˆ�(fJ1 |ei) by pˆ�(ei|fJ1 ).
From a theoretical framework of the source-
channel approach, this approach is hard to jus-
tify. Yet, if both decision rules yield the same
translation quality, we can use that decision
rule which is better suited for efficient search.
</bodyText>
<subsectionHeader confidence="0.8702335">
1.2 Direct Maximum Entropy Translation
Model
</subsectionHeader>
<bodyText confidence="0.9971745">
As alternative to the source-channel approach, we
directly model the posterior probability Pr(ei|fJ1 ).
An especially well-founded framework for doing
this is maximum entropy (Berger et al., 1996). In
this framework, we have a set of M feature func-
tions hm(ei, fJ1 ), m = 1, ... , M. For each feature
function, there exists a model parameter am, m =
1, ... , M. The direct translation probability is given
</bodyText>
<figure confidence="0.854899142857143">
Source
Language Text
2
Preprocessing
A1 · h1(eI1, fJ1 )
A2 · h2(eI1, fJ1 )
. . .
2
Global Search
argmax n M oAmhm(eI1, fJ1 )
eI P1
1
��
��
��
by:
2
Postprocessing
2
Target
Language Text
</figure>
<figureCaption confidence="0.99997">
Figure 2: Architecture of the translation approach based on direct maximum entropy models.
</figureCaption>
<bodyText confidence="0.988356">
the following two feature functions:
</bodyText>
<equation confidence="0.964337666666667">
Pr(eI1|fJ1 ) = pλM1 (eI1|fJ1 ) (7)
exp[PMm=1 Amhm(eI1, fJ1 )]
= Pe,I exp[PMm= 1 Amhm(e&apos;I ,fl )] (8)
</equation>
<bodyText confidence="0.999747">
This approach has been suggested by (Papineni et
al., 1997; Papineni et al., 1998) for a natural lan-
guage understanding task.
We obtain the following decision rule:
Hence, the time-consuming renormalization in Eq. 8
is not needed in search. The overall architecture of
the direct maximum entropy models is summarized
in Figure 2.
Interestingly, this framework contains as special
case the source channel approach (Eq. 5) if we use
</bodyText>
<equation confidence="0.9986635">
h1(eI1, fJ1 ) = log pˆγ(eI1) (9)
h2(eI1,fJ1 ) = log pˆθ(fJ1 |eI1) (10)
</equation>
<bodyText confidence="0.9997624375">
and set A1 = A2 = 1. Optimizing the corresponding
parameters A1 and A2 of the model in Eq. 8 is equiv-
alent to the optimization of model scaling factors,
which is a standard approach in other areas such as
speech recognition or pattern recognition.
The use of an ‘inverted’ translation model in the
unconventional decision rule of Eq. 6 results if we
use the feature function log Pr(eI1|fJ1 ) instead of
log Pr(fJ1 |eI1). In this framework, this feature can
be as good as log Pr(fJ1 |eI1). It has to be empirically
verified, which of the two features yields better re-
sults. We even can use both features log Pr(eI1|fJ1 )
and log Pr(fJ1 |eI1), obtaining a more symmetric
translation model.
As training criterion, we use the maximum class
posterior probability criterion:
</bodyText>
<equation confidence="0.953765277777778">
(S
Xlog pλM1(es|fs) (11)
s=1
ˆeI1 = argmax
eI
1
= argmax
eI
1
n M
X
m=1
oAmhm(eI1, fJ1 )
n o
Pr(eI 1|fJ 1 )
ˆAM1 = argmax
λM
1
</equation>
<bodyText confidence="0.999917333333333">
This corresponds to maximizing the equivocation
or maximizing the likelihood of the direct transla-
tion model. This direct optimization of the poste-
rior probability in Bayes decision rule is referred to
as discriminative training (Ney, 1995) because we
directly take into account the overlap in the proba-
bility distributions. The optimization problem has
one global optimum and the optimization criterion
is convex.
</bodyText>
<subsectionHeader confidence="0.8828685">
1.3 Alignment Models and Maximum
Approximation
</subsectionHeader>
<bodyText confidence="0.9999865">
Typically, the probability Pr(fJ1 |eI1) is decomposed
via additional hidden variables. In statistical align-
ment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is in-
troduced as a hidden variable:
</bodyText>
<sectionHeader confidence="0.862217" genericHeader="method">
2 Alignment Templates
</sectionHeader>
<bodyText confidence="0.999979428571429">
As specific MT method, we use the alignment tem-
plate approach (Och et al., 1999). The key elements
of this approach are the alignment templates, which
are pairs of source and target language phrases to-
gether with an alignment between the words within
the phrases. The advantage of the alignment tem-
plate approach compared to single word-based sta-
tistical translation models is that word context and
local changes in word order are explicitly consid-
ered.
The alignment template model refines the transla-
tion probability Pr(fJ1 |eI1) by introducing two hid-
den variables zK1 and aK1 for the K alignment tem-
plates and the alignment of the alignment templates:
</bodyText>
<equation confidence="0.956119666666667">
�Pr(fJ 1 |eI 1) = Pr(fJ1 , aJ1 |eI1) �Pr(fJ 1 |eI 1) = Pr(aK1 |eI1) ·
aJ zK 1 ,aK 1
1
</equation>
<bodyText confidence="0.98711275">
The alignment mapping is j → i = aj from source
position j to target position i = aj.
Search is performed using the so-called maximum
approximation:
</bodyText>
<equation confidence="0.998854571428571">
ˆeI1 = argmax
eI
1
� �
Pr(eI1) · max Pr(fJ1 ,aJ1 |eI1)
aJ
1
</equation>
<bodyText confidence="0.999656555555555">
Hence, the search space consists of the set of all pos-
sible target language sentences eI1 and all possible
alignments aJ1 .
Generalizing this approach to direct translation
models, we extend the feature functions to in-
clude the dependence on the additional hidden vari-
able. Using M feature functions of the form
hm(eI1, fJ1 , aJ1), m = 1, ... , M, we obtain the fol-
lowing model:
</bodyText>
<equation confidence="0.9960432">
Pr(eI1, aJ1 |fJ1 ) =
( M I J )J)
exp �m=1 amhm(e1, f1 ,a1
Ee/I a�j exp (EM =1 λmhm(el I1, fJ1 , aIJ1))
1 1
</equation>
<bodyText confidence="0.9997902">
Obviously, we can perform the same step for transla-
tion models with an even richer structure of hidden
variables than only the alignment aJ1 . To simplify
the notation, we shall omit in the following the de-
pendence on the hidden variables of the model.
</bodyText>
<equation confidence="0.902457">
Pr(zK1 |aK1 , eI1) · Pr(fJ 1 |zK1 , aK1 , eI1)
</equation>
<bodyText confidence="0.998755307692308">
Hence, we obtain three different probability
distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and
Pr(fJ1 |zK1 ,aK1 ,eI1). Here, we omit a detailed de-
scription of modeling, training and search, as this is
not relevant for the subsequent exposition. For fur-
ther details, see (Och et al., 1999).
To use these three component models in a direct
maximum entropy approach, we define three dif-
ferent feature functions for each component of the
translation model instead of one feature function for
the whole translation model p(fJ1 |eI1). The feature
functions have then not only a dependence on fJ1
and eI1 but also on zK1 , aK1 .
</bodyText>
<sectionHeader confidence="0.997156" genericHeader="method">
3 Feature functions
</sectionHeader>
<bodyText confidence="0.996794333333333">
So far, we use the logarithm of the components of
a translation model as feature functions. This is a
very convenient approach to improve the quality of
a baseline system. Yet, we are not limited to train
only model scaling factors, but we have many possi-
bilities:
</bodyText>
<listItem confidence="0.743941">
• We could add a sentence length feature:
h(fJ1 , eI1) = I
</listItem>
<bodyText confidence="0.9919785">
This corresponds to a word penalty for each
produced target word.
</bodyText>
<figure confidence="0.605212142857143">
{ �Pr(eI 1) · Pr(fJ1 , aJ1 |eI1) }
aJ
1
≈ argmax
eI
1
=
</figure>
<listItem confidence="0.8800155">
• We could use additional language models by
using features of the following form:
</listItem>
<equation confidence="0.483441">
h(fJ1 , eI1) = h(eI1)
</equation>
<listItem confidence="0.8588901">
• We could use a feature that counts how many
entries of a conventional lexicon co-occur in
the given sentence pair. Therefore, the weight
for the provided conventional dictionary can be
learned. The intuition is that the conventional
dictionary is expected to be more reliable than
the automatically trained lexicon and therefore
should get a larger weight.
• We could use lexical features, which fire if a
certain lexical relationship (f, e) occurs:
</listItem>
<equation confidence="0.99570175">
⎛ ⎞Ã XI !
XJ
h(fJ 1 , eI 1) = ⎝ δ(f, fj) ⎠ · δ(e,ei)
j=1 i=1
</equation>
<bodyText confidence="0.9919805">
• We could use grammatical features that relate
certain grammatical dependencies of source
and target language. For example, using a func-
tion k(·) that counts how many verb groups ex-
ist in the source or the target sentence, we can
define the following feature, which is 1 if each
of the two sentences contains the same number
of verb groups:
</bodyText>
<equation confidence="0.908654">
h(fJ1 , eI1) = δ(k(fJ1 ), k(eI1)) (12)
</equation>
<bodyText confidence="0.999975">
In the same way, we can introduce semantic
features or pragmatic features such as the di-
alogue act classification.
We can use numerous additional features that deal
with specific problems of the baseline statistical MT
system. In this paper, we shall use the first three of
these features. As additional language model, we
use a class-based five-gram language model. This
feature and the word penalty feature allow a straight-
forward integration into the used dynamic program-
ming search algorithm (Och et al., 1999). As this is
not possible for the conventional dictionary feature,
we use n-best rescoring for this feature.
</bodyText>
<sectionHeader confidence="0.993565" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.998693307692308">
To train the model parameters λM1 of the direct trans-
lation model according to Eq. 11, we use the GIS
(Generalized Iterative Scaling) algorithm (Darroch
and Ratcliff, 1972). It should be noted that, as
was already shown by (Darroch and Ratcliff, 1972),
by applying suitable transformations, the GIS algo-
rithm is able to handle any type of real-valued fea-
tures. To apply this algorithm, we have to solve var-
ious practical problems.
The renormalization needed in Eq. 8 requires a
sum over a large number of possible sentences,
for which we do not know an efficient algorithm.
Hence, we approximate this sum by sampling the
space of all possible sentences by a large set of
highly probable sentences. The set of considered
sentences is computed by an appropriately extended
version of the used search algorithm (Och et al.,
1999) computing an approximate n-best list of trans-
lations.
Unlike automatic speech recognition, we do not
have one reference sentence, but there exists a num-
ber of reference sentences. Yet, the criterion as it
is described in Eq. 11 allows for only one reference
translation. Hence, we change the criterion to al-
low Rs reference translations es,1, ... , es,Rs for the
sentence es:
</bodyText>
<equation confidence="0.990227666666667">
ˆλM1 = argmax ( XS )
λM XRs
1
log pλM1 (es,r|fs)
Rs
s=1 r=1
</equation>
<bodyText confidence="0.999970727272727">
We use this optimization criterion instead of the op-
timization criterion shown in Eq. 11.
In addition, we might have the problem that no
single of the reference translations is part of the n-
best list because the search algorithm performs prun-
ing, which in principle limits the possible transla-
tions that can be produced given a certain input sen-
tence. To solve this problem, we define for max-
imum entropy training each sentence as reference
translation that has the minimal number of word er-
rors with respect to any of the reference translations.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.981527888888889">
We present results on the VERBMOBIL task, which
is a speech translation task in the domain of appoint-
ment scheduling, travel planning, and hotel reser-
vation (Wahlster, 1993). Table 1 shows the cor-
pus statistics of this task. We use a training cor-
pus, which is used to train the alignment template
model and the language models, a development cor-
pus, which is used to estimate the model scaling fac-
tors, and a test corpus.
</bodyText>
<tableCaption confidence="0.960961666666667">
Table 1: Characteristics of training corpus (Train),
manual lexicon (Lex), development corpus (Dev),
test corpus (Test).
</tableCaption>
<table confidence="0.997877230769231">
German English
Train Sentences 58 073
Words 519 523 549 921
Singletons 3 453 1698
Vocabulary 7 939 4 672
Lex Entries 12 779
Ext. Vocab. 11501 6 867
Dev Sentences 276
Words 3 159 3 438
PP (trigr. LM) - 28.1
Test Sentences 251
Words 2 628 2 871
PP (trigr. LM) - 30.5
</table>
<bodyText confidence="0.992798571428571">
So far, in machine translation research does not
exist one generally accepted criterion for the evalu-
ation of the experimental results. Therefore, we use
a large variety of different criteria and show that the
obtained results improve on most or all of these cri-
teria. In all experiments, we use the following six
error criteria:
</bodyText>
<listItem confidence="0.923634571428571">
• SER (sentence error rate): The SER is com-
puted as the number of times that the generated
sentence corresponds exactly to one of the ref-
erence translations used for the maximum en-
tropy training.
• WER (word error rate): The WER is computed
as the minimum number of substitution, inser-
tion and deletion operations that have to be per-
formed to convert the generated sentence into
the target sentence.
• PER (position-independent WER): A short-
coming of the WER is the fact that it requires
a perfect word order. The word order of an
acceptable sentence can be different from that
</listItem>
<bodyText confidence="0.999599666666667">
of the target sentence, so that the WER mea-
sure alone could be misleading. To overcome
this problem, we introduce as additional mea-
sure the position-independent word error rate
(PER). This measure compares the words in the
two sentences ignoring the word order.
</bodyText>
<listItem confidence="0.9710935625">
• mWER (multi-reference word error rate): For
each test sentence, there is not only used a sin-
gle reference translation, as for the WER, but
a whole set of reference translations. For each
translation hypothesis, the edit distance to the
most similar sentence is calculated (NieBen et
al., 2000).
• BLEU score: This score measures the precision
of unigrams, bigrams, trigrams and fourgrams
with respect to a whole set of reference trans-
lations with a penalty for too short sentences
(Papineni et al., 2001). Unlike all other eval-
uation criteria used here, BLEU measures ac-
curacy, i.e. the opposite of error rate. Hence,
large BLEU scores are better.
• SSER (subjective sentence error rate): For a
</listItem>
<bodyText confidence="0.939994565217391">
more detailed analysis, subjective judgments
by test persons are necessary. Each trans-
lated sentence was judged by a human exam-
iner according to an error scale from 0.0 to 1.0
(NieBen et al., 2000).
• IER (information item error rate): The test sen-
tences are segmented into information items.
For each of them, if the intended information
is conveyed and there are no syntactic errors,
the sentence is counted as correct (NieBen et
al., 2000).
In the following, we present the results of this ap-
proach. Table 2 shows the results if we use a direct
translation model (Eq. 6).
As baseline features, we use a normal word tri-
gram language model and the three component mod-
els of the alignment templates. The first row shows
the results using only the four baseline features with
λ1 = · · · = λ4 = 1. The second row shows the
result if we train the model scaling factors. We see a
systematic improvement on all error rates. The fol-
lowing three rows show the results if we add the
word penalty, an additional class-based five-gram
</bodyText>
<tableCaption confidence="0.981795">
Table 2: Effect of maximum entropy training for alignment template approach (WP: word penalty feature,
CLM: class-based language model (five-gram), MX: conventional dictionary).
</tableCaption>
<table confidence="0.998166857142857">
objective criteria [%] subjective criteria [%]
SER WER PER mWER BLEU SSER IER
Baseline(Am = 1) 86.9 42.8 33.0 37.7 43.9 35.9 39.0
ME 81.7 40.2 28.7 34.6 49.7 32.5 34.8
ME+WP 80.5 38.6 26.9 32.4 54.1 29.9 32.2
ME+WP+CLM 78.1 38.3 26.9 32.1 55.0 29.1 30.9
ME+WP+CLM+MX 77.8 38.4 26.8 31.9 55.2 28.8 30.9
</table>
<figure confidence="0.8503635">
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
number of iterations
</figure>
<figureCaption confidence="0.999459">
Figure 3: Test error rate over the iterations of the
</figureCaption>
<bodyText confidence="0.999687875">
GIS algorithm for maximum entropy training of
alignment templates.
language model and the conventional dictionary fea-
tures. We observe improved error rates for using the
word penalty and the class-based language model as
additional features.
Figure 3 show how the sentence error rate (SER)
on the test corpus improves during the iterations of
the GIS algorithm. We see that the sentence error
rates converges after about 4000 iterations. We do
not observe significant overfitting.
Table 3 shows the resulting normalized model
scaling factors. Multiplying each model scaling fac-
tor by a constant positive value does not affect the
decision rule. We see that adding new features also
has an effect on the other model scaling factors.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.577413">
The use of direct maximum entropy translation mod-
els for statistical machine translation has been sug-
</bodyText>
<tableCaption confidence="0.85063">
Table 3: Resulting model scaling factors of maxi-
mum entropy training for alignment templates; A1:
trigram language model; A2: alignment template
model, A3: lexicon model, A4: alignment model
(normalized such that E4m=1 Am = 4).
</tableCaption>
<table confidence="0.994913875">
ME +WP +CLM +MX
A1 0.86 0.98 0.75 0.77
A2 2.33 2.05 2.24 2.24
A3 0.58 0.72 0.79 0.75
A4 0.22 0.25 0.23 0.24
WP 2.6 3.03 2.78
CLM 0.33 0.34
MX 2.92
</table>
<bodyText confidence="0.999381411764706">
gested by (Papineni et al., 1997; Papineni et al.,
1998). They train models for natural language un-
derstanding rather than natural language translation.
In contrast to their approach, we include a depen-
dence on the hidden variable of the translation model
in the direct translation model. Therefore, we are
able to use statistical alignment models, which have
been shown to be a very powerful component for
statistical machine translation systems.
In speech recognition, training the parameters of
the acoustic model by optimizing the (average) mu-
tual information and conditional entropy as they are
defined in information theory is a standard approach
(Bahl et al., 1986; Ney, 1995). Combining various
probabilistic models for speech and language mod-
eling has been suggested in (Beyerlein, 1997; Peters
and Klakow, 1999).
</bodyText>
<sectionHeader confidence="0.999228" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9995685">
We have presented a framework for statistical MT
for natural languages, which is more general than the
</bodyText>
<figure confidence="0.998125615384616">
0.88
0.86
0.84
0.82
0.78
0.76
0.74
0.9
0.8
ME
ME+WP
ME+WP+CLM
ME+WP+CLM+MX
</figure>
<bodyText confidence="0.999294142857143">
widely used source-channel approach. It allows a
baseline MT system to be extended easily by adding
new feature functions. We have shown that a base-
line statistical MT system can be significantly im-
proved using this framework.
There are two possible interpretations for a statis-
tical MT system structured according to the source-
channel approach, hence including a model for
Pr(ei) and a model for Pr(fi Iei). We can inter-
pret it as an approximation to the Bayes decision rule
in Eq. 2 or as an instance of a direct maximum en-
tropy model with feature functions log Pr(ei) and
log Pr(fi |ei). As soon as we want to use model
scaling factors, we can only do this in a theoretically
justified way using the second interpretation. Yet,
the main advantage comes from the large number of
additional possibilities that we obtain by using the
second interpretation.
An important open problem of this approach is
the handling of complex features in search. An in-
teresting question is to come up with features that
allow an efficient handling using conventional dy-
namic programming search algorithms.
In addition, it might be promising to optimize the
parameters directly with respect to the error rate of
the MT system as is suggested in the field of pattern
and speech recognition (Juang et al., 1995; Schl¨uter
and Ney, 2001).
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998709375">
L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mer-
cer. 1986. Maximum mutual information estimation
of hidden markov model parameters. In Proc. Int.
Conf. on Acoustics, Speech, and Signal Processing,
pages 49–52, Tokyo, Japan, April.
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39–72, March.
P. Beyerlein. 1997. Discriminative model combina-
tion. In Proc. of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 238–
245, Santa Barbara, CA, December.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470–1480.
B. H. Juang, W. Chou, and C. H. Lee. 1995. Statisti-
cal and discriminative methods for speech recognition.
In A. J. R. Ayuso and J. M. L. Soler, editors, Speech
Recognition and Coding - New Advances and Trends.
Springer Verlag, Berlin, Germany.
H. Ney. 1995. On the probabilistic-interpretation of
neural-network classifiers and discriminative training
criteria. IEEE Trans. on Pattern Analysis and Machine
Intelligence, 17(2):107–119, February.
S. Nießen, F. J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In Proc. of the Second Int.
Conf. on Language Resources and Evaluation (LREC),
pages 39–45, Athens, Greece, May.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf. on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 20–28, University of Maryland, Col-
lege Park, MD, June.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In European
Conf. on Speech Communication and Technology,
pages 1435–1438, Rhodes, Greece, September.
K. A. Papineni, S. Roukos, and R. T. Ward. 1998. Max-
imum likelihood and discriminative training of direct
translation models. In Proc. Int. Conf. on Acoustics,
Speech, and Signal Processing, pages 189–192, Seat-
tle, WA, May.
K. A. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, Yorktown Heights, NY, September.
J. Peters and D. Klakow. 1999. Compact maximum en-
tropy language models. In Proc. of the IEEE Workshop
on Automatic Speech Recognition and Understanding,
Keystone, CO, December.
R. Schl¨uter and H. Ney. 2001. Model-based MCE bound
to the true Bayes’ error. IEEE Signal Processing Let-
ters, 8(5):131–133, May.
W. Wahlster. 1993. Verbmobil: Translation of face-to-
face dialogs. In Proc. of MT Summit IV, pages 127–
135, Kobe, Japan, July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933975">
<note confidence="0.997663">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 295-302.</note>
<title confidence="0.9888135">Discriminative Training and Maximum Entropy Models for Statistical Machine Translation</title>
<author confidence="0.998777">Josef Och Ney</author>
<affiliation confidence="0.9947325">Lehrstuhl f¨ur Informatik VI, Computer Science Department RWTH Aachen - University of Technology</affiliation>
<address confidence="0.999787">D-52056 Aachen, Germany</address>
<abstract confidence="0.99811825">We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>P F Brown</author>
<author>P V de Souza</author>
<author>R L Mercer</author>
</authors>
<title>Maximum mutual information estimation of hidden markov model parameters. In</title>
<date>1986</date>
<booktitle>Proc. Int. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>49--52</pages>
<location>Tokyo, Japan,</location>
<marker>Bahl, Brown, de Souza, Mercer, 1986</marker>
<rawString>L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer. 1986. Maximum mutual information estimation of hidden markov model parameters. In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing, pages 49–52, Tokyo, Japan, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="4785" citStr="Berger et al., 1996" startWordPosition="758" endWordPosition="761">s. 3. Often, we observe that comparable results are obtained by using the following decision rule Here, we replaced pˆ�(fJ1 |ei) by pˆ�(ei|fJ1 ). From a theoretical framework of the sourcechannel approach, this approach is hard to justify. Yet, if both decision rules yield the same translation quality, we can use that decision rule which is better suited for efficient search. 1.2 Direct Maximum Entropy Translation Model As alternative to the source-channel approach, we directly model the posterior probability Pr(ei|fJ1 ). An especially well-founded framework for doing this is maximum entropy (Berger et al., 1996). In this framework, we have a set of M feature functions hm(ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given Source Language Text 2 Preprocessing A1 · h1(eI1, fJ1 ) A2 · h2(eI1, fJ1 ) . . . 2 Global Search argmax n M oAmhm(eI1, fJ1 ) eI P1 1 �� �� �� by: 2 Postprocessing 2 Target Language Text Figure 2: Architecture of the translation approach based on direct maximum entropy models. the following two feature functions: Pr(eI1|fJ1 ) = pλM1 (eI1|fJ1 ) (7) exp[PMm=1 Amhm(eI1, fJ1 )] = Pe,I exp[PMm</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–72, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Beyerlein</author>
</authors>
<title>Discriminative model combination.</title>
<date>1997</date>
<booktitle>In Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>238--245</pages>
<location>Santa Barbara, CA,</location>
<contexts>
<context position="20089" citStr="Beyerlein, 1997" startWordPosition="3418" endWordPosition="3419">ch, we include a dependence on the hidden variable of the translation model in the direct translation model. Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems. In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995). Combining various probabilistic models for speech and language modeling has been suggested in (Beyerlein, 1997; Peters and Klakow, 1999). 7 Conclusions We have presented a framework for statistical MT for natural languages, which is more general than the 0.88 0.86 0.84 0.82 0.78 0.76 0.74 0.9 0.8 ME ME+WP ME+WP+CLM ME+WP+CLM+MX widely used source-channel approach. It allows a baseline MT system to be extended easily by adding new feature functions. We have shown that a baseline statistical MT system can be significantly improved using this framework. There are two possible interpretations for a statistical MT system structured according to the sourcechannel approach, hence including a model for Pr(ei)</context>
</contexts>
<marker>Beyerlein, 1997</marker>
<rawString>P. Beyerlein. 1997. Discriminative model combination. In Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding, pages 238– 245, Santa Barbara, CA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2018" citStr="Brown et al., 1993" startWordPosition="306" endWordPosition="309">ce in the target language. 1The notational convention will be as follows. We use the symbol Pr(·) to denote general probability distributions with (nearly) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·). 1.1 Source-Channel Model According to Bayes’ decision rule, we can equivalently to Eq. 1 perform the following maximization: ˆeI1 = argmax {Pr(eI1) · Pr(fJ1 |eI1)} (2) el 1 This approach is referred to as source-channel approach to statistical MT. Sometimes, it is also referred to as the ‘fundamental equation of statistical MT’ (Brown et al., 1993). Here, Pr(eI1) is the language model of the target language, whereas Pr(fJ1 |eI1) is the translation model. Typically, Eq. 2 is favored over the direct translation model of Eq. 1 with the argument that it yields a modular approach. Instead of modeling one probability distribution, we obtain two different knowledge sources that are trained independently. The overall architecture of the source-channel approach is summarized in Figure 1. In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm. Typically, training is perf</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="12116" citStr="Darroch and Ratcliff, 1972" startWordPosition="2057" endWordPosition="2060">problems of the baseline statistical MT system. In this paper, we shall use the first three of these features. As additional language model, we use a class-based five-gram language model. This feature and the word penalty feature allow a straightforward integration into the used dynamic programming search algorithm (Och et al., 1999). As this is not possible for the conventional dictionary feature, we use n-best rescoring for this feature. 4 Training To train the model parameters λM1 of the direct translation model according to Eq. 11, we use the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff, 1972). It should be noted that, as was already shown by (Darroch and Ratcliff, 1972), by applying suitable transformations, the GIS algorithm is able to handle any type of real-valued features. To apply this algorithm, we have to solve various practical problems. The renormalization needed in Eq. 8 requires a sum over a large number of possible sentences, for which we do not know an efficient algorithm. Hence, we approximate this sum by sampling the space of all possible sentences by a large set of highly probable sentences. The set of considered sentences is computed by an appropriately extended v</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43:1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Juang</author>
<author>W Chou</author>
<author>C H Lee</author>
</authors>
<title>Statistical and discriminative methods for speech recognition. In</title>
<date>1995</date>
<booktitle>Speech Recognition and Coding - New Advances and Trends.</booktitle>
<editor>A. J. R. Ayuso and J. M. L. Soler, editors,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin, Germany.</location>
<marker>Juang, Chou, Lee, 1995</marker>
<rawString>B. H. Juang, W. Chou, and C. H. Lee. 1995. Statistical and discriminative methods for speech recognition. In A. J. R. Ayuso and J. M. L. Soler, editors, Speech Recognition and Coding - New Advances and Trends. Springer Verlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
</authors>
<title>On the probabilistic-interpretation of neural-network classifiers and discriminative training criteria.</title>
<date>1995</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="7047" citStr="Ney, 1995" startWordPosition="1164" endWordPosition="1165">ich of the two features yields better results. We even can use both features log Pr(eI1|fJ1 ) and log Pr(fJ1 |eI1), obtaining a more symmetric translation model. As training criterion, we use the maximum class posterior probability criterion: (S Xlog pλM1(es|fs) (11) s=1 ˆeI1 = argmax eI 1 = argmax eI 1 n M X m=1 oAmhm(eI1, fJ1 ) n o Pr(eI 1|fJ 1 ) ˆAM1 = argmax λM 1 This corresponds to maximizing the equivocation or maximizing the likelihood of the direct translation model. This direct optimization of the posterior probability in Bayes decision rule is referred to as discriminative training (Ney, 1995) because we directly take into account the overlap in the probability distributions. The optimization problem has one global optimum and the optimization criterion is convex. 1.3 Alignment Models and Maximum Approximation Typically, the probability Pr(fJ1 |eI1) is decomposed via additional hidden variables. In statistical alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidden variable: 2 Alignment Templates As specific MT method, we use the alignment template approach (Och et al., 1999). The key elements of this approach are the alignment templates, which are pairs of</context>
<context position="19977" citStr="Ney, 1995" startWordPosition="3402" endWordPosition="3403">ls for natural language understanding rather than natural language translation. In contrast to their approach, we include a dependence on the hidden variable of the translation model in the direct translation model. Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems. In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995). Combining various probabilistic models for speech and language modeling has been suggested in (Beyerlein, 1997; Peters and Klakow, 1999). 7 Conclusions We have presented a framework for statistical MT for natural languages, which is more general than the 0.88 0.86 0.84 0.82 0.78 0.76 0.74 0.9 0.8 ME ME+WP ME+WP+CLM ME+WP+CLM+MX widely used source-channel approach. It allows a baseline MT system to be extended easily by adding new feature functions. We have shown that a baseline statistical MT system can be significantly improved using this framework. There are two possible interpretations fo</context>
</contexts>
<marker>Ney, 1995</marker>
<rawString>H. Ney. 1995. On the probabilistic-interpretation of neural-network classifiers and discriminative training criteria. IEEE Trans. on Pattern Analysis and Machine Intelligence, 17(2):107–119, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nießen</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In Proc. of the Second Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>39--45</pages>
<location>Athens, Greece,</location>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>S. Nießen, F. J. Och, G. Leusch, and H. Ney. 2000. An evaluation tool for machine translation: Fast evaluation for MT research. In Proc. of the Second Int. Conf. on Language Resources and Evaluation (LREC), pages 39–45, Athens, Greece, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>College Park, MD,</location>
<contexts>
<context position="3368" citStr="Och et al., 1999" startWordPosition="530" endWordPosition="533">model Pr(fJ1 |eI1) = pθ(fJ1 |eI1) depends on parameters θ, then the optimal parameter values are obtained by maximizing the likelihood on a parallel training corpus fS1 , eS1 (Brown et al., 1993): pθ(fs|es) (3) pγ(es) (4) θˆ = argmax θ S H s=1 S γˆ = argmax H γ s=1 Source Language Text 2 Preprocessing ˆei = argmax el 1 {Pr(ei) · Pr(fJ1 |ei)} Pr(ei): Language Model Pr(fJ1 |ei): Translation Model 2 �� �� Global Search 2 Postprocessing 2 Target Language Text Figure 1: Architecture of the translation approach based on source-channel models. We obtain the following decision rule: instead of Eq. 5 (Och et al., 1999): ˆei = argmax {pˆ�(ei) · pˆ�(fJ1 |ei)} (5) ˆei = argmax {pˆ�(ei) · pˆ�(e,|fJ1 )} (6) el el 1 1 State-of-the-art statistical MT systems are based on this approach. Yet, the use of this decision rule has various problems: 1. The combination of the language model pˆ�(el) and the translation model pˆ�(fJ1 |ei) as shown in Eq. 5 can only be shown to be optimal if the true probability distributions pˆ�(ei) = Pr(ei) and pˆ�(fJ1 |ei) = Pr(fJ1 |ei) are used. Yet, we know that the used models and training methods provide only poor approximations of the true probability distributions. Therefore, a diffe</context>
<context position="7564" citStr="Och et al., 1999" startWordPosition="1242" endWordPosition="1245">e posterior probability in Bayes decision rule is referred to as discriminative training (Ney, 1995) because we directly take into account the overlap in the probability distributions. The optimization problem has one global optimum and the optimization criterion is convex. 1.3 Alignment Models and Maximum Approximation Typically, the probability Pr(fJ1 |eI1) is decomposed via additional hidden variables. In statistical alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidden variable: 2 Alignment Templates As specific MT method, we use the alignment template approach (Och et al., 1999). The key elements of this approach are the alignment templates, which are pairs of source and target language phrases together with an alignment between the words within the phrases. The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered. The alignment template model refines the translation probability Pr(fJ1 |eI1) by introducing two hidden variables zK1 and aK1 for the K alignment templates and the alignment of the alignment templates: �Pr(fJ 1 |eI 1) = Pr(fJ1</context>
<context position="9511" citStr="Och et al., 1999" startWordPosition="1600" endWordPosition="1603">hm(el I1, fJ1 , aIJ1)) 1 1 Obviously, we can perform the same step for translation models with an even richer structure of hidden variables than only the alignment aJ1 . To simplify the notation, we shall omit in the following the dependence on the hidden variables of the model. Pr(zK1 |aK1 , eI1) · Pr(fJ 1 |zK1 , aK1 , eI1) Hence, we obtain three different probability distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and Pr(fJ1 |zK1 ,aK1 ,eI1). Here, we omit a detailed description of modeling, training and search, as this is not relevant for the subsequent exposition. For further details, see (Och et al., 1999). To use these three component models in a direct maximum entropy approach, we define three different feature functions for each component of the translation model instead of one feature function for the whole translation model p(fJ1 |eI1). The feature functions have then not only a dependence on fJ1 and eI1 but also on zK1 , aK1 . 3 Feature functions So far, we use the logarithm of the components of a translation model as feature functions. This is a very convenient approach to improve the quality of a baseline system. Yet, we are not limited to train only model scaling factors, but we have m</context>
<context position="11824" citStr="Och et al., 1999" startWordPosition="2010" endWordPosition="2013">h of the two sentences contains the same number of verb groups: h(fJ1 , eI1) = δ(k(fJ1 ), k(eI1)) (12) In the same way, we can introduce semantic features or pragmatic features such as the dialogue act classification. We can use numerous additional features that deal with specific problems of the baseline statistical MT system. In this paper, we shall use the first three of these features. As additional language model, we use a class-based five-gram language model. This feature and the word penalty feature allow a straightforward integration into the used dynamic programming search algorithm (Och et al., 1999). As this is not possible for the conventional dictionary feature, we use n-best rescoring for this feature. 4 Training To train the model parameters λM1 of the direct translation model according to Eq. 11, we use the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff, 1972). It should be noted that, as was already shown by (Darroch and Ratcliff, 1972), by applying suitable transformations, the GIS algorithm is able to handle any type of real-valued features. To apply this algorithm, we have to solve various practical problems. The renormalization needed in Eq. 8 requires a su</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F. J. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28, University of Maryland, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>Feature-based language understanding.</title>
<date>1997</date>
<booktitle>In European Conf. on Speech Communication and Technology,</booktitle>
<pages>1435--1438</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="5467" citStr="Papineni et al., 1997" startWordPosition="888" endWordPosition="891">ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given Source Language Text 2 Preprocessing A1 · h1(eI1, fJ1 ) A2 · h2(eI1, fJ1 ) . . . 2 Global Search argmax n M oAmhm(eI1, fJ1 ) eI P1 1 �� �� �� by: 2 Postprocessing 2 Target Language Text Figure 2: Architecture of the translation approach based on direct maximum entropy models. the following two feature functions: Pr(eI1|fJ1 ) = pλM1 (eI1|fJ1 ) (7) exp[PMm=1 Amhm(eI1, fJ1 )] = Pe,I exp[PMm= 1 Amhm(e&apos;I ,fl )] (8) This approach has been suggested by (Papineni et al., 1997; Papineni et al., 1998) for a natural language understanding task. We obtain the following decision rule: Hence, the time-consuming renormalization in Eq. 8 is not needed in search. The overall architecture of the direct maximum entropy models is summarized in Figure 2. Interestingly, this framework contains as special case the source channel approach (Eq. 5) if we use h1(eI1, fJ1 ) = log pˆγ(eI1) (9) h2(eI1,fJ1 ) = log pˆθ(fJ1 |eI1) (10) and set A1 = A2 = 1. Optimizing the corresponding parameters A1 and A2 of the model in Eq. 8 is equivalent to the optimization of model scaling factors, whi</context>
<context position="19326" citStr="Papineni et al., 1997" startWordPosition="3299" endWordPosition="3302">sion rule. We see that adding new features also has an effect on the other model scaling factors. 6 Related Work The use of direct maximum entropy translation models for statistical machine translation has been sugTable 3: Resulting model scaling factors of maximum entropy training for alignment templates; A1: trigram language model; A2: alignment template model, A3: lexicon model, A4: alignment model (normalized such that E4m=1 Am = 4). ME +WP +CLM +MX A1 0.86 0.98 0.75 0.77 A2 2.33 2.05 2.24 2.24 A3 0.58 0.72 0.79 0.75 A4 0.22 0.25 0.23 0.24 WP 2.6 3.03 2.78 CLM 0.33 0.34 MX 2.92 gested by (Papineni et al., 1997; Papineni et al., 1998). They train models for natural language understanding rather than natural language translation. In contrast to their approach, we include a dependence on the hidden variable of the translation model in the direct translation model. Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems. In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>K. A. Papineni, S. Roukos, and R. T. Ward. 1997. Feature-based language understanding. In European Conf. on Speech Communication and Technology, pages 1435–1438, Rhodes, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>Maximum likelihood and discriminative training of direct translation models.</title>
<date>1998</date>
<booktitle>In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>189--192</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="5491" citStr="Papineni et al., 1998" startWordPosition="892" endWordPosition="895"> M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given Source Language Text 2 Preprocessing A1 · h1(eI1, fJ1 ) A2 · h2(eI1, fJ1 ) . . . 2 Global Search argmax n M oAmhm(eI1, fJ1 ) eI P1 1 �� �� �� by: 2 Postprocessing 2 Target Language Text Figure 2: Architecture of the translation approach based on direct maximum entropy models. the following two feature functions: Pr(eI1|fJ1 ) = pλM1 (eI1|fJ1 ) (7) exp[PMm=1 Amhm(eI1, fJ1 )] = Pe,I exp[PMm= 1 Amhm(e&apos;I ,fl )] (8) This approach has been suggested by (Papineni et al., 1997; Papineni et al., 1998) for a natural language understanding task. We obtain the following decision rule: Hence, the time-consuming renormalization in Eq. 8 is not needed in search. The overall architecture of the direct maximum entropy models is summarized in Figure 2. Interestingly, this framework contains as special case the source channel approach (Eq. 5) if we use h1(eI1, fJ1 ) = log pˆγ(eI1) (9) h2(eI1,fJ1 ) = log pˆθ(fJ1 |eI1) (10) and set A1 = A2 = 1. Optimizing the corresponding parameters A1 and A2 of the model in Eq. 8 is equivalent to the optimization of model scaling factors, which is a standard approac</context>
<context position="19350" citStr="Papineni et al., 1998" startWordPosition="3303" endWordPosition="3306">adding new features also has an effect on the other model scaling factors. 6 Related Work The use of direct maximum entropy translation models for statistical machine translation has been sugTable 3: Resulting model scaling factors of maximum entropy training for alignment templates; A1: trigram language model; A2: alignment template model, A3: lexicon model, A4: alignment model (normalized such that E4m=1 Am = 4). ME +WP +CLM +MX A1 0.86 0.98 0.75 0.77 A2 2.33 2.05 2.24 2.24 A3 0.58 0.72 0.79 0.75 A4 0.22 0.25 0.23 0.24 WP 2.6 3.03 2.78 CLM 0.33 0.34 MX 2.92 gested by (Papineni et al., 1997; Papineni et al., 1998). They train models for natural language understanding rather than natural language translation. In contrast to their approach, we include a dependence on the hidden variable of the translation model in the direct translation model. Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems. In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Ba</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1998</marker>
<rawString>K. A. Papineni, S. Roukos, and R. T. Ward. 1998. Maximum likelihood and discriminative training of direct translation models. In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing, pages 189–192, Seattle, WA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center, Yorktown Heights, NY,</location>
<contexts>
<context position="16248" citStr="Papineni et al., 2001" startWordPosition="2772" endWordPosition="2775">on-independent word error rate (PER). This measure compares the words in the two sentences ignoring the word order. • mWER (multi-reference word error rate): For each test sentence, there is not only used a single reference translation, as for the WER, but a whole set of reference translations. For each translation hypothesis, the edit distance to the most similar sentence is calculated (NieBen et al., 2000). • BLEU score: This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a whole set of reference translations with a penalty for too short sentences (Papineni et al., 2001). Unlike all other evaluation criteria used here, BLEU measures accuracy, i.e. the opposite of error rate. Hence, large BLEU scores are better. • SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 (NieBen et al., 2000). • IER (information item error rate): The test sentences are segmented into information items. For each of them, if the intended information is conveyed and there are no syntactic errors, the sentence is counted</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. A. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peters</author>
<author>D Klakow</author>
</authors>
<title>Compact maximum entropy language models.</title>
<date>1999</date>
<booktitle>In Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<location>Keystone, CO,</location>
<contexts>
<context position="20115" citStr="Peters and Klakow, 1999" startWordPosition="3420" endWordPosition="3423">dependence on the hidden variable of the translation model in the direct translation model. Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems. In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995). Combining various probabilistic models for speech and language modeling has been suggested in (Beyerlein, 1997; Peters and Klakow, 1999). 7 Conclusions We have presented a framework for statistical MT for natural languages, which is more general than the 0.88 0.86 0.84 0.82 0.78 0.76 0.74 0.9 0.8 ME ME+WP ME+WP+CLM ME+WP+CLM+MX widely used source-channel approach. It allows a baseline MT system to be extended easily by adding new feature functions. We have shown that a baseline statistical MT system can be significantly improved using this framework. There are two possible interpretations for a statistical MT system structured according to the sourcechannel approach, hence including a model for Pr(ei) and a model for Pr(fi Iei</context>
</contexts>
<marker>Peters, Klakow, 1999</marker>
<rawString>J. Peters and D. Klakow. 1999. Compact maximum entropy language models. In Proc. of the IEEE Workshop on Automatic Speech Recognition and Understanding, Keystone, CO, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schl¨uter</author>
<author>H Ney</author>
</authors>
<title>Model-based MCE bound to the true Bayes’ error.</title>
<date>2001</date>
<journal>IEEE Signal Processing Letters,</journal>
<volume>8</volume>
<issue>5</issue>
<marker>Schl¨uter, Ney, 2001</marker>
<rawString>R. Schl¨uter and H. Ney. 2001. Model-based MCE bound to the true Bayes’ error. IEEE Signal Processing Letters, 8(5):131–133, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>Verbmobil: Translation of face-toface dialogs.</title>
<date>1993</date>
<booktitle>In Proc. of MT Summit IV,</booktitle>
<pages>127--135</pages>
<location>Kobe, Japan,</location>
<contexts>
<context position="13937" citStr="Wahlster, 1993" startWordPosition="2372" endWordPosition="2373">e the problem that no single of the reference translations is part of the nbest list because the search algorithm performs pruning, which in principle limits the possible translations that can be produced given a certain input sentence. To solve this problem, we define for maximum entropy training each sentence as reference translation that has the minimal number of word errors with respect to any of the reference translations. 5 Results We present results on the VERBMOBIL task, which is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation (Wahlster, 1993). Table 1 shows the corpus statistics of this task. We use a training corpus, which is used to train the alignment template model and the language models, a development corpus, which is used to estimate the model scaling factors, and a test corpus. Table 1: Characteristics of training corpus (Train), manual lexicon (Lex), development corpus (Dev), test corpus (Test). German English Train Sentences 58 073 Words 519 523 549 921 Singletons 3 453 1698 Vocabulary 7 939 4 672 Lex Entries 12 779 Ext. Vocab. 11501 6 867 Dev Sentences 276 Words 3 159 3 438 PP (trigr. LM) - 28.1 Test Sentences 251 Words</context>
</contexts>
<marker>Wahlster, 1993</marker>
<rawString>W. Wahlster. 1993. Verbmobil: Translation of face-toface dialogs. In Proc. of MT Summit IV, pages 127– 135, Kobe, Japan, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>