<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.790435">
American Jouital of Computational Linguistics Microfiche 61
</note>
<sectionHeader confidence="0.980954" genericHeader="abstract">
TOWARD A &amp;quot;NATURAL&amp;quot; LANGUAGE
QUESTION-ANSWERING FACILITY
</sectionHeader>
<author confidence="0.887328">
BILL D. MAXWELL AND FRANCIS D. ItioGLE
</author>
<affiliation confidence="0.985056">
Department of CoMputer Science
University of Kansas
</affiliation>
<note confidence="0.779629333333333">
Lawrence 66045
Maxwell is also associated with the Computation Center
Tuggle is also associated with the School of Business
</note>
<copyright confidence="0.327341">
Copyright (;) 1977
Association fpr Computational Linguistics
</copyright>
<sectionHeader confidence="0.92215" genericHeader="introduction">
SUMMARY
</sectionHeader>
<bodyText confidence="0.987127357142857">
This study describes the structure, implementation and potential
of a simple computer program that understands and answers questions in
a humanoid manner. An emphasis has been placed on the creation of an
extendible memory structure--one capable of supporting conversation
in normal, unrestricted Edglish on a variety of topics. An attempt
has slso been made to find procedures that can easily and accurately
determine the meaning of input text. A parser using a combination of
syntax and semantics has been developed for understanding YES-NO
questions, in particular, DO-type (DO, DID, DOES, etc.) questions. A
third and major emphasis has been on the development of procedures
to allow the program to conversepeasily-and &amp;quot;naturally&amp;quot; with a human.
This general ga41 has been met by developing procedures that generate
answers to DO-questions in a manner similar to the way a person might
answer them,
</bodyText>
<sectionHeader confidence="0.635915" genericHeader="method">
TABLE OF CONTENTS
</sectionHeader>
<table confidence="0.913071333333333">
1. INTRODUCTION 5
1.1 MAIN GOALS 7
1.2 SAMPLE DIALOG 8
1.3 REVIEW 10
1.3.; Memory Models 10
1.3.2 Linguistic Parsers 11
1.3.3 Outtmt Generation 13
2. PROGRAM OVERIVEW 14
2.1 PROGRAM STRUCTURE 14
2.2 THE RUNNING PROGRAM 15
2.3 DATA FILES 17
3d MEMORY STRUCTURE 17
3.1 COMPONENTS 18
3.2 RELATIONS 19
3.3 SUBSTRUCTURES 23
3.4 NODES 25
4. PARSER 31
4.1 PARSING STRATEGY e • 31
4.2 GRAMMAR 33
4.2.1 Acceptable Input Forms • • 33
4.2.2 Semantics and Syntax 35
4.2.3 Pronouns, Ambiguity add Undefined Words 36
4.3 PARSING ALGORITHM 36
4.3.1 Preprocessing of the Input Text 37
4.3.2 Determining Type of Input 37
4.3.3 Paring a DO-Question or Statement 38
5. MEMORY stARCHING 43
5.1 OVERVIEW 43
5.2 THE MEMORY MATCH STRUCTURE . 44
5.2.1 General Structure 44
5.2.2 Actor, Act and Object Comparison Results 44
5.2.3 Word Modification Results 45
5.2.4 Example Structures 46
5.3 MATCHING MEMORY 49
5.3.1 Basic Algorithm 49
5.3.2 Selecting Suitable Actors 49
</table>
<sectionHeader confidence="0.566455" genericHeader="method">
6. PRODUCTION OF OUTPUT 51
6.1 OVERVIEW 51
6.2 THE OUTPUT PRODUCTION LIST 52
</sectionHeader>
<address confidence="0.7099334">
6.3 PRODUCTION METHOD 53
6.3.1 Responding to Input lot Understood 53
6.3.2 Determining Mode of Response
6.3.3 Producing a Norval Answer 54
6.3.4 Making Output Grammatical ... 55
</address>
<note confidence="0.978385583333333">
7. DISCUSSION . • • 56
7.1 RESULTS 56
7.1.1 Objectives Met 56
7.1.2 Does the Program Really Understand? 58
7.2 LIMITATIONS 59
7.2.1 Memory Model , 59
7.2.2 Parser 59
7.2.3 Output Production 60
7.3 IGNORED PROBLEMS •. 61
7.3.1 Intentions and Motivations 61
7.3.2 Norman&apos;s Problems 62
REFERENCES • • • . 64
</note>
<sectionHeader confidence="0.986308" genericHeader="method">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.977445514285715">
There are a number of extant computer programs which interact
intelligently with human interrogators, but all do so in a way we
characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they con-
verse in a way significantly unlike two normal adult humans do. For
example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the
Woods&apos; moonrocks system [20] the problem of being task-constrained.
Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and
doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally,
the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s
SOPHIE [3], seem officiously pedantic. In contrast to those approaches,
we wanted to construct a program able to carry on an intelligent and
&amp;quot;natural&amp;quot; dialogue with a human user.
As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine
two computer programs (CPs) conversing with a human (H) in which each
CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the
other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from
misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;)
H: The ball is red.
UCP.: By &apos;the ball,&apos; I assume you mean the ball the boy owns.
NCP: 0.K.
H: Does the boy own a large, round ball?
UCP: Na [or at best] I don&apos;t know.
NCP: He owns a red one. It probably is round and may be large.
H: The boy lives on Maim Street.
UCP:. By &apos;the boy&apos;, I assume you mean the one who owns the red ball.
NCP: O.K.
-6--
H: Where does the boy play with his ball?
UCP: I don&apos;t kdow.
NCP: I don&apos;t know, but I assume near his residence on Main Street.
H: A house is on Main Street.
UCP: O.K.
NCP: Is it the house of the boy who owns the red ball?
H: It was,expensive to construct.
UCP: By &apos;it,&apos; do you mean
</bodyText>
<listItem confidence="0.717604666666667">
1. the house
2. Main Street?
NCP: O.K.
</listItem>
<bodyText confidence="0.992108483870968">
H: The ball broke a window.
UCP: By &apos;the ball,&apos; I assume you mean the red one the boy owns. By a
window,&apos; I assume it is a part of the house in Main Street.
NCP: Did the boy throw the ball?
H: Was the window expensive to repair?
UCP: I don&apos;t know.
NCP: Since the house was expensive to construct, I assume that its
windows ere expensive to repair.
From this shprt, simple dialogue some of the characteristics which
distinguish &amp;quot;natural&amp;quot; dialogue can be seen: &amp;quot;natural&amp;quot; language under-
standing processes are able to work with partial and overlapping
information, are able to retain ambiguity until disambigation is needed,
are able to perform &amp;quot;short&amp;quot; chains of deductions, are able to engage in
common -sense reasoning, and are able to pose reciprocal questions to
the human to confirm expectations.
Reasons for trying to instill a certain degree of &amp;quot;humanness&amp;quot; to
computer programs should be obvious--to facilitate their acceptance, to
extend their use, and to make them more pleasant to deal with. People
will be much more willing LO work with a computer program if it gives the
appearance of being humanoid itself, whether the kernel part of the
program concerns CAI, MIS, or whatever.
-7-
We have developed a computer program called JIMMY3 which embodies
some of the gbwre set of characteristics of a &amp;quot;natural&amp;quot; language processing
system so as to demonstrate its feasibility, usefulness, and potential
power. The implementation has necessarily been largely ad hoc, but as
Lindsay [6] notes, this is not altogether bad. Newell [7] properly
records that there is a tradeoff between generality and power. Like
Lindsay, we deliberately eschew the genera] in favor of the specific.
In the interests of replicability and extensibility, we also provide a
reasonably complete description of the inards of JIMMY3.
</bodyText>
<subsectionHeader confidence="0.995297">
1.1 MAJOR GOALS
</subsectionHeader>
<bodyText confidence="0.965271923076923">
The focus of this work has been on three problems: a) the develop-
ment Df a memory structure useful for engaging in a dialogue with a person,
b) the development of procedures that can easily and accurately determine
the meaning of input text, and c) the development of procedures for the
generation of output.
As contrasted with more conventional models for language understand-
ing where input is decoded in terms of concepts and then mapped into
memory, this model storcs surface structures more or less intact without
any conversion. This approach simplifies both the input processing and
output generation phases of the system. However, it does necessitate the
use of complex memory matching procedures during the answer-producing
phase, of understanding.
For parsing, several ad hoc rules were developed for applying a
metric to measure &amp;quot;meaningfulness.&amp;quot; Using this procedure, several
different interpretations of the input are examined. The one having the
highest &amp;quot;score,&amp;quot; as determined by the metric, is selected as the proper
meaning.
-8-
Most memory models and understanding systems do not have elaborate
facilities for the creatibn of output that is interesting or stimulating
to. conversation. Output production by this program approaches solutions
to three related problems: a) what information should be used in the
response to a given input, b) how a response can be structured so
certain parts are emphasized and c) how A response can be made tp
appear natural in the sense of being like A similar remark a person
might make.
</bodyText>
<subsectionHeader confidence="0.98683">
1.2 SAMPLE DIALOGUE
</subsectionHeader>
<bodyText confidence="0.996644">
To get a feeling for the types of responses the currently Imple-
mented program tan produce, the following short dialogue is presented.
Note that with only the capability for answering DO-questions
implemeqted, a continuing dialogue is not very easy to obtain.
Three explicit facts existed in memory at the time of this con-
versation. They were:
</bodyText>
<listItem confidence="0.501046666666667">
1) BRANDT MAXWELL OWNS A RED BOOK.
2) BRANDT MAXWELL OWNS A BLUE BOOK.
3) JIMMY3 KNOWS BRANDT MAXWELL.
</listItem>
<bodyText confidence="0.9698208">
(this computer model has been dubbed JIMMY3)
The dialogue is given in upper case with commentary in lower case.
The j: and p: (identifying JIMMY3 and person) were added Here for
clarity.
J: HI. WHO ARE YOU?
</bodyText>
<subsectionHeader confidence="0.652394">
JIMMY3 must find out who it is talking with in orderr
</subsectionHeader>
<bodyText confidence="0.8766825">
to translate the pronoun &amp;quot;I&amp;quot; to facilitate memory
searching.
</bodyText>
<subsectionHeader confidence="0.395715">
P: BitANDT MAXWELL.
</subsectionHeader>
<bodyText confidence="0.978771">
-9-
j: OK.
BRANDT@MAXWELL is recognized as a legitimate name.
The @ in BRANDT@MAXWELL is used to indicate the
combining of two or more words to form a separate
entity. In the above example, the names BRANDT
and MAXWELL are recognized as first and last names,
respectively, that, when appearing together, are
interpreted as the full name, BRANDT@MAXWELL.
p: DO YOU KNOW ANYONE?
J: YES, YOU.
Parsing is accomplished by matching input to
templates consisting of (ACTOR ACT OBJECT)
describing the different meaning senses for main
verbs. This question is parsed using the (PERSON
KNOW PERSON) definition of KNOW. On input, YOU
is translated to JIMMY3 (which has ISA PERSON as
one of its properties).
Memory is matched against the (JIMMY3 KNOW
PERSON) pattern which yields the match (JIMMY3
KNOWS BRANDT@MAXWELL)&apos;. Output procedures then
cdnvert the match into-the given response.
p: DO YOU KNOW BILL MAXWELL?
j: NO.
The search using (JIMMY3 KNOW BILL@MAXWELL) yields
no match. Under other circumstances a failure of
this type would produce an answer of &amp;quot;I DON&apos;T KNOW&amp;quot;.
However, the act KNOW is tagged so that &amp;quot;NO&amp;quot; is the
proper response if a search of memory fails when
the triple has KNOW as its act(main verb).
</bodyText>
<sectionHeader confidence="0.90105" genericHeader="method">
DO YOU THINK?
I DON&apos;T KNOW.
</sectionHeader>
<subsectionHeader confidence="0.554943">
The input was correctly parsed as (PERSON THINK NIL)
but the memory search found no matches. The
</subsectionHeader>
<bodyText confidence="0.938266333333333">
current model has no facilities for introspection
other than the examination of triples in memory
that deal explicitly with JIMMY3.
</bodyText>
<sectionHeader confidence="0.709741" genericHeader="method">
DO YOU KNOW ANYTHING?
I DON&apos;T UNDERSTAND THE USE OF KNOW.
</sectionHeader>
<bodyText confidence="0.91497575">
At the time this dialogue was produced, the only mean-
ing of KNOW contained in memory was (PERSON KNOW PERSON).
Therefore, the reasonable parse of (PERSON KNOW THING)
was not found.
</bodyText>
<figure confidence="0.141629333333333">
-10--
DO I HAVE A BLUE ANIMAL BOOK?
A BLUE BOOK BUT NOT AN ANIMAL BOOK:
</figure>
<tableCaption confidence="0.474409178571429">
Some, but not all, of the modifiers on the input
were matched by a triple in memory as in4icated by
the response. Since not all modifiers matched, the
ansOer is not prefaced with YES.
DOES ANYONE HAVE A RED THING?
YES. YOU OWN A RED BOOK.
Memory Ls searched using the (PERSON HAVE THING)
pattern which best matches (BRANDT@MAXWELL OWNS
BOOK). By comparing modifiers, (BRANDT@MAXWELL
OWNS RED-BOOK) is selected over (BRANDMAXWELL
OWNS BLUE-BOOK).
NI I OWN A GREEN BOOK?
NO. YOU OWN A RED BOOK.
Since GREEN and RED are mutually exclusive instances
of COLOR, the Answer is NO. (BRANDT@MAXWELL OWN
BLUE-BOOK) was also matched bun the triple with
RED BOOK was used to produce the response since it
was found first by the memory search routine.
13: DO I HAVE AN OLD BOOK?
YES. A RED ONE. I DON&apos;T KNOW WHAT OLD MEANS.
OLD is ignored sinov it is an unaefined word. Input
is parsed as if it had been &amp;quot;DO I HAVE AN BOOK?&amp;quot;
p: DOES BILL MAXWELL OWN A BOOK?
j: / DON&apos;T KNOW.
The fact (BILL@MAXWELL OWNS BOOK) is not present
in memory. Neither is any contradiction of that fact.
p: BYE.
j: GOODBYE.
</tableCaption>
<subsectionHeader confidence="0.9622045">
1.3 REVIEW
1.3.1 MEMORY MODELS.
</subsectionHeader>
<bodyText confidence="0.998317">
The research involving memory models can be divided into two basic
approaches. First are models created for the exploration of theories of
human memory and for the testing of linguistio.theories. In these
systems, the other features of a complete language understanding system
assume a background position since the main emphasis is the memory model
itself. Models which fall into this class are the works of Quinlan [11],
Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research
group [10]. The second type of memory model is that developed as part
of a systeth &apos;which has some component other than memory as the major
emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s
Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia
model [5].
The memory model developed by this current research does not
correspond to any existing model. It is not based -on case grammar in
a strict sense, but stores information in a form more closely related
to surface structures utilizing only three main components: actor,
act and object. Although this development was influented to a limited
extent by Anderson and Bower&apos;s model [1], it was shaped in actual design
by the parser dsed by Wilks (pee below) which attempts to find meaning
by searching for triples. Wilks&apos; work was also influential in the
development of the parser.
</bodyText>
<subsectionHeader confidence="0.366287">
1.3.2 LINGUISTIC PARSERS
</subsectionHeader>
<bodyText confidence="0.979848346938776">
In the area of linguistic parsers, there are currently four
different methods being used. They are: a) semantic triples, b) aug-
mented transition networks, c) procedures and d) pattern matching.
These can be described most easily by examining specific examples
of each.
The parser developed by Yorick Wilks [17, 18] is based on identi-
fying triples composed of an actor, an act and an object. Input is
parsed by applying trial templates to the input and identifying
candidates to fill the three slots in a template. The association of
an input word with a template slot is made by consideration of the
-12-
semantic relationship between the input word and the requirements of the
slot, The closer the relationship, the better the match, and therefore,
the better the parse. The choice between several potential parsings
of an input string is determined by a scheme for computing the semantic
density of the parse based on the number and types of matches that are
obtained by filling the slots in the triple. Assuming English is a
redundant language, the parse having the greatest density is the one
which contains the most interconnections and, therefore, is the one to
be accepted as the correct parse.
An important feature of Wilks&apos; system is that syntactic properties
of words take a role secondary to that assumed by the semantic properties.
Thus it is possible to determine the meaning of input which is ill-formed
and grammatically ipcorrect.
Augmented transition networks have been used for agkme time as a
method for parsing. The principal system utilizing this method is the
NASA lunar rocks system developed by Woods [20]. The parser used by th
LNR group [10] is of similar design. While this technique is very
flexible in allowing a large grammar to be consistently modelled, most
implementations of it have been strictly syntactic. Little attempt has
been made to incorporate semantic knowledge into the parsing. Without
semantic knowledge, and using the strict grammatical rules embedded in
the transition networks, this approach is really very brittle. It is not
capable of handling ungrammatical input with much success.
-11-
Closely related to the augmented transition network approach is the
use of procedures to describe a grammar as exemplified by Winograd&apos;s pro-
gram [19]. This system is also predominately syntactic in nature. All
information about the grammar is represented in terms of actual routines
whioh are invoked during parsing.
The input ana.iyzer used in Colby&apos;s Artificial Paranoia model [5]
is essentially a pattern matcher which uses a few tricks to normalize
all input to short strings whiW1 it hopes to recognize. It is not really
either syntactically or semantically based but depends mostly on trans-
formations to reduce input to simple, empirically derived, recognizable
forms. The power of this approach lies in its ability tg accept even
the most ungrammatical input and relate it to sodething which is already
known. Thus, there is a given context in which all input is Interpreted--
the context of what the model knows and wants to continue talking about.
</bodyText>
<subsectionHeader confidence="0.514146">
1.3.3 OUTPUT GENERATION
</subsectionHeader>
<bodyText confidence="0.95758625">
Perhaps the work on output production which has most influenced the
current program is the model by Colby [5]. As described in the previous
section, input is recognized by reduction to simple identifiable patterns
which can be matched to prestored strings in the program&apos;s memory. Also
included in the memory are corresponding sets of strings which are to be
used as output. Fixed responses are selected based on the current state
of the program&apos;s self model; the state of its model of the person and the
previous conversation. This technique gives the appearance of a normal,
humanlike dialogue. This approach is successful because conversation is
always directed-in one particular, very narrow, direction by the way input
is understood. The context is fixed and must not deviate from a single
-14-
</bodyText>
<sectionHeader confidence="0.571951" genericHeader="method">
2. PROGRAM OVERVIEW
</sectionHeader>
<subsectionHeader confidence="0.937294">
2.1 PROGRAM STRUCTURE
</subsectionHeader>
<bodyText confidence="0.964742">
The logical structure for the program consists of essentially four
main sections executed in sequence with each step producing information
required by the following step. This design, with minor variation, is
characteristic of many existing programs for language understanding.
One notable exception has been the SPEECH UNDERSTANDING PROJECT EC
which advocates the use of parallel modules working simultaneously on
the input, passing data freely between routines, until the desired end
result is reached.
The following algorithm describes how JIMMY1 behaves at the mbst
superficial level.
DO until person is through talking:
</bodyText>
<listItem confidence="0.981683333333333">
(1) . Request user input and translate English words into internal
▪ codes (MEMORY node numbers).
(2) . Parse input to create the &amp;quot;best&amp;quot; wrse network(s).
(3) . Match each parse network with structures in memory to produce
▪ the &amp;quot;best&amp;quot; match(es).
(4) . Produce a response based on the memory match(es).
</listItem>
<bodyText confidence="0.977346884615385">
END-DO
To make the processing of text more efficient, English words and
punctuation are translated in step (1) into node pointers. Undefined
words are changed to null pointers.
The parser then creates a parse network from which a network density
is computed. This density is a measure of how well the particular parse
captures the meaning of the input. The parsing procedure is driven by
-15-
heuristics to do a search of the most likely parse networks. In case of
ambiguity, several parse networks can be passed to the taemoryaiatch routine.
If the input was a statement, the information given by the network is
stored in short term membry (SDI). If a question is asked, then the parse
network is matched against event nodes in the memory structure to find
potential answers. A match score id computed for each pattern match
based on how well the three major components (ACTOR, ACT and OBJECT) ant
ail minor components (single modifiers and prepositional phrase modification)
match. Only the tlosest matches are retained for use by the output
gdheratax. Ambiguity in the parse, if it exists, is resolved here by the
selection of the best match regardless of its generating parse network.
The response is generated by procedures operating on the memory
matches. The value of the match score determines generally what the
response content should be. The exact form of the response is determined
the way the components of the input matched or did not Match) the
memory pattern. After a form is decided on, the response is made
grammatical gmmILhen printed. Control is then returned to the input
routine and the billike sequence is ready to be repeated.
</bodyText>
<subsectionHeader confidence="0.976395">
2.2 THE RUNNING PROGRAM
</subsectionHeader>
<bodyText confidence="0.989178173913043">
The environment, the time-sharing system on the Honeywell 66/60
running GCOS at the University of Kansas, in which this program was
developed and is run dictated its form This machine poses several
problems besides the lack of suitable languages, the most serious being
the 25K word limit on the amount of memory an interactive program
can obtain.
-16-
Since there are no interactive string or list processine languages
available on the system, only FORTRAN came close to satisfying the
requirements of a language in -which a natural language system could be
realistically implemented. What was wanted was a high level language
with overlay capabilities, small but powerful I/O packages and the
facility for independently compiled and tested subroutines with utilities
for the maintenance of subroutine libraries.
The program now running consists of over 130 subroutines written in
a total of about 13K lines of FORTRAN code. It runs in approximately
20K (36-bit) words of memory when segmented into 5 overlays. An
unlinked version is approximately 37K words in size. The use of core
by various parts of the program is given in Table 1.
Response time for the program is good considering the amount of
overhead required because of memory constraints. Dialogue like that
given earlier takes 2-3 seconds between the last character of input
until the answer starts to Print.
</bodyText>
<table confidence="0.989751066666667">
TOTALS
I/0 PACKAGE 1.5 1.5
DATA
SCRTCH array 2.0
MEMORY Paging area 1.0
WORDLIST area 1.3
Miscellaneous 1.0 5.3
PROGRAM
Main link + support routines 10.2
Initialization &amp; Setup 1.9
Command processor 3.3
Parsing 5.3
Memory matching 6.2
Output production 3.3 30.2
37.0
</table>
<tableCaption confidence="0.8751725">
Table 1: Storage allocation for unlinked program in
thousands of Honeywell 66/60 words
</tableCaption>
<bodyText confidence="0.646512">
-17-
</bodyText>
<subsectionHeader confidence="0.89757">
2.3 DATA FILES
</subsectionHeader>
<bodyText confidence="0.999589">
Two data files, WORDLIST and MEMORY, are required by the program.
The WORDLIST file contains the text representation for all words,
punctuation and system commands along with the keys for translating that
text into memory node numbers. The MEMORY file is the collection of all
memory nodes.
To aid in the recreation and continual updating of these files, the
data Pnntained in them is present in a text file which is maintained on-
line using the time-sharing text editor. After changes to this primary
file, aremade, a program, separate from JIMMY3, translates the text code
Into the WORDLIST and MEMORY files. A second program can unload the
WORDLIST and MEMORY files back to text when necessary. Currently the
text file contains a vocabulary of 387 entries (t7ords, punctuation and
commands), 22 ACT usages and 7 facts. This information is encoded in
approximately 4000 lines of symbolic node representation. It, when
loaded, creates a WORDLIST of 387 setiarate entries and a MEMORY with
approximately 800 memory nodes.
</bodyText>
<sectionHeader confidence="0.784608" genericHeader="method">
3. MEMORY STRUCTURE
</sectionHeader>
<bodyText confidence="0.989855">
The memory system for JIMMY3 is an aggregation of four components:
WORDLIST, MEMORY, temporary structures and STM. With the exception of the
WORDLIST, each part is a collection of one or more substructures consist-
ing of nodes connected by relations.
-18-
</bodyText>
<subsectionHeader confidence="0.863692">
3.1 COMPONENTS
</subsectionHeader>
<bodyText confidence="0.999209302325582">
WORDLIST. The WORDLIST is an index into the MEMORY component and
consists of representettions for all units of input JIMMY3 is to recognize.
Items not included in the WORDLIST are declared undefined by the input
decoding routines. In addition to the exact text representation, a
pointer to the corresponding node in MEMORY is given.
MEMORY. This is the model&apos;s long term memory--essentially a collec-
tion of interrelated nodes.
A node is the smallest packet of information in MEMORY that can be
referenced by a single pointer (either a WORDLIST pointer or a pointer
from another node.) The information contained in a node may describe or
modify a single word or symbol or may be a collection of relations
connecting several nodes into a more complex structure.
Information stored in nodes describing single words is varied and
includes such items as part of speech, inflectional variations and subset
or superset names. Other types of data, for examp14, events and state-
ments of fact, are formed by nodes that contain pointer structures
relating other nodes in a predetermined fashion.
TEMPORARY STRUCTURES. There are three basic memory structures of a
transient nature that can exist during the processing required to deter-
mine meaning and produce an answer. They are: a) the parse network,
b) the memory match structure, and c) the output production list.
A parse network is a small collection of nodes of the same format,
and connected in the same fashion, as the nodes in MEMORY. Produced by
the parser, it is used to represent the meaning of input text. This net-
-19-
work contains information about the major components of the input, i.e.,
what words or phrases represent the ACTOR, ACT and OBJECT; it also contains
tnformatlon Omit 3-11 modifying words and phrases found in the input.
A memory match structure is produced by matching the parse network
with MEMORY. It contains comparison data relating the corresponding
components of the input (parse hetwork) and a substructure in MEMORY.
The output production list is used during the examination of the
memory match structures to accumulate the discrete components of the
response to be made, i.e., the words and punctuation for the answer to
the questiln. Each element in this list contains a pointer to a text
representation for the word (or punctuation), its function and its
relation to other words in the sentence. When the output list has been
formed, it is passed to a print routine which writes the answer to the
terminal.
STM. STM is the short term memory component for the program. It
is essentially a collection area for parse networks, memory match
structures and output production lists generated for previous inputs
and their responses.
</bodyText>
<subsectionHeader confidence="0.891836">
3.2 RELATIONS
</subsectionHeader>
<bodyText confidence="0.962623902439025">
Among the attributes used to relate the different MEMORY and parse
network nodes to each other are those for describing hierarchies and
chains.
HIERARCHY. Hierarchies are vertical structures formed by relating
nodes to one another using the ISA attribute. Set (superset, subset)
relations are implemented as hierarchies. A typical example would be
-20-
the path through the nodes: BRANDT (ISA) BOY (ISA) PERSON which displays
the relations among the three nodes.
In a hierarchy, each node is connected only to a singie node
immediately before and after it. However, for any given hierarchy, it
is possible that the node q involved are connected to others both in and
out of the hierarchy by connections independent of that particular
hierarchy structure.
Transitivity is a property of hierarchies as implemented by the
program. Therefore, in the example ibove, the information, BRANDT (ISA)
PERSON, is implicit in the hierarchy. It should be pointed out that in
the forward link between BRANDT and BOY, the connection is not one of
set membership but rather one of subset as is the relation between BOY
and PERSON. This is accomplished by the concept of GENERIC nodes even
for specific instances of, say, people. Therefore, the BRANDT in the
hierarchy is a GENERIC node which will, in turn, have under it an
INSTANCE node of BRANDT which is related by set membership. The GENERIC
BRANDT is a set of one element.
CHAIN. The mechanism for constructing horizontal structures is
the chain. As contrasted to the before link-after link structure of
the hierarchy, the chain allows the creation of a linearly ordered set
of nodes each relating to a common &amp;quot;root&amp;quot; node to which it is attached.
This allows the creation of sets of nodes related by a common property.
For example, chains exist in MEMORY tying together all inflectional
variations of a word. Another example of a chain is the list of color
21
nodes: RED, BLUE and GREEN. A singly linked list exists through these
nodes but in addition, each of the three points back to the node for
COLOR--the node which points to the first color in the list.
The more important links currently used to form chains are
described next.
DEFN. The DEFN link is used to tie together all nodes that
represent different definitions for a word. Thus, for every symbol in
the WORDLIST, there is a chain of nodes in MEMORY connected via the
DEFN link.
</bodyText>
<table confidence="0.89332475">
PERSON
lTsni .4 &apos; noxt node
rel,resenuing a
su&apos;oset of BOY.
</table>
<figureCaption confidence="0.644144">
Figure I: Hierarchy for BRANDT (ISA)
BOY (ISA) PERSON
</figureCaption>
<bodyText confidence="0.893806333333333">
INST. A chain emanates from each definition of each word in MEMORY.
These chains are created with the INST link and represent the set of all
instances of the word given by the root node of the chain.
</bodyText>
<figure confidence="0.9969006">
eft.
f.
r.
• (.11S1\Igi •
f.? . ......
•
—:7› To next node
rupresenping a
[.]ISAi_ I &apos; subset of PERSON.
•
</figure>
<bodyText confidence="0.976465804878049">
-22-
ISA. This link is used to generate hierarchies. Actually created
is a chain with the property that every node in the chain is related by
the ISA link to the root node. The root node can be a member of
another ISA chain, thus giving a multilevel filerarchy. Figure 1 shows
part of the hierarchy for BRANDT (ISA) BOY (ISA) PERSON.
ACTOR. This is used to connect a set of ACTION nodes in which the
root node is used as the ACTOR of a triple. For example, there would be
a chain through the triples representing (BRANDT HAVE BOOK), (BRANDT LIKE
MILK) and (BRANDT HAVE CAT).
ACT. Used to form sets of ACTION nodes which contain the root node
as the ACT of a triple.
OBJECT. Similar to the ACTOR and ACT links except it chains through
ACTION nodes which. have the root node as the OBJECT of a triple.
MODIFY. This is used to specify a set of nodes which are modified
by the root node. The type of this single word modification is partially
determined by the part of speech of the root node and that of the modified
node. It is further specified by the hierarchy of the root node and the
node being modified. For example, RED may be used to modify BOOK. In
terms of grammatical function, the parts of speech specify this as an
adjective modifying a noun. However, examination of the hierarchies
discloses that RED (ISA) COLOR which is a property that THINGS (BOOK (ISA)
THING) can have.
POS. This is the part of speech link.
PREP. This link is used in a CONTEXT node to point to the preposition
part of the prepositional phrase represented by the node.
-23-
POBJ. The CONTEXT node also contains the reference to a prepositional
object.
CNTXT. This is the link used to attach a CONTEXT node as a modifier
of another node. The chain created by a CNTXT link represents all nodes
modified by the same CONTEXT node.
INFLEC. Most words in MEMORY belong to an INFLEC chain. This is a
set of inflectional variations of the word. It brings together different
forms which vary in tense, number, person, etc. The INFLEC chain through
&amp;quot;I&amp;quot; would join nodes for I. FIE, MY, and MINE. A similar chain through
OWN would link nodes for OWN, OWNS, OWNED and OWNING.
ANTONYM. All antonyms of a word are chained together using this
link. The words in the chain are not antonyms of each other.
PARTS. This link is used in nodes to express the sub-part, super-
part relationships.
</bodyText>
<subsectionHeader confidence="0.758916">
3.3 SUBSTRUCTURES
</subsectionHeader>
<bodyText confidence="0.99714755">
Using the relations for constructing hierarchies and chains, various
other, more complex, structures can be created. These are structures
formed by the coincidence of several hierarchies or chains passing through
a single node. Of all possible substructures, triples (ACTION nodes),
CONTEXT nodes and EMANTIC MODIFICATION nodes are of greatest importanLe.
TRIPLES. A triple (ACTION node) is a node through which passes three
separate chains, one each for ACTOR, ACT, and OBJECT. These triples are
used to specify events, facts and as semantic definitions for the ACTs
as ranges for acceptable candidates for ACTORs and OBJECTs.) The
chains for ACTOR, ACT and OBJECT originate in nodes which represent those
-24-
major components and continue through this node to where they merge in
different combinations with still other chains to form more triples.
Figure 2 shows the relevant sbructuro of a triple representing &amp;quot;PERSON
HAVE BOOK&amp;quot;.
CONTEXT NODES. A close relative of the triple is the context node.
It too has chains passing through it determining its structure. However,
it has only two chains: those for PREP and POBJ. These specify a pre-
position and its object. Nodes of this sort are used as modifiers of
single nodes, triples and other context nodes.
</bodyText>
<figure confidence="0.752675055555555">
PERSON
.7*
• .
•I.E.l OR EI-3--P-•\:-,
:-SACT . .
,
/&amp;quot;. HAVE.—
. ;
• PlACTORE
next triple usini
Er1013JECf RAO: as an ACf.
r . .
• r
;7.1300K fo next triple u in
: HOOK a an OBJL.C.I.
--1-i•l0L3JECri
In next triple urinn
PERSON a s an ACTOR.
</figure>
<figureCaption confidence="0.94843">
Figure 2: A triple representing (PERSON
HAVE BOOK).
</figureCaption>
<bodyText confidence="0.971856975">
-25-
SEMANTIC MODIFICATION NODES. These nodes are not similar to triples
or context nodes either in design or in function. However, they are one
of the major substructures appearing in MEMORY so will be considered here
briefly. These nodes or collections of nodes are the data strings whith
are used to drive the parser and in that capacityl will be described later.
Essentially they contain ordered lists of semantic categories representing
potential modification patterns for words. These lists are applied by the
parser to determine semantically acceptable word strings in the input that
can modify other word strings. For example, a noun would have a list
describing the types of adjectives that could modify it. or adjectives,
there would be a list of potential adverb types. Each definition of every
word in MEMORY that is to be recognized during parsing of input must have
attached to it a semantic mudification node. In eases where many words
have the Same node, the structure for semantic modification is implemented
as a chain through-all words having the same modification requirements.
3.4 NODES
Nodes in MEMORY are represented as fixed-size blocks of contiguous disk
or core locations and are the smallest units of MEMORY that can be refer-
enced. Each node is composed of links and variable length attributes which
may be data or pointers to other nodes. All the space allocated to a node
does not have to be used. In fact, most nodes use only part of their
allocated space to contain attributes; the remainder is empty (zero). The
current model uses a size of 8 words for its nodes. (This size was not
determined eMpirically as the optimum size but was, instead, selected
because of disk hardware considerations.) The first word of every node in
MEMORY is used for bookkeeping and, therefore, is not available for storing
-26-
attributes. Contained in the first word is the node number itself, an
indicator of the kind of node plus a pointer that gives :the next word of
the node available for use as a Jialk or variable length attribute.
Whenever a single node has more attributes than it can contain, additional
8-word blocks are allocated as extensions of the original node. These
extensions are connected in a chain to the original node by the COM link
and are transparent to all the program except for the most basic MEMORY
manipulation routines.
There are eight different kinds of nodes in memory, each with its
own function. The kinds are TYPE, SIMPLE GENERIC, ACTION GENERIC, CONTEXT
GENERIC, SIMPLE. INSTANCE, ACTION INSTANCE, CONTEXT INSTANCE and SEMANTIC
MODIFICATION. Each hap a different purpose in the representation of in-
formation in MEMORY. Briefly, their purposes are as follows. The TYPE
node is used as the reference point between the WORDLIST and MEMORY. The
three GENERIC nodes are used to specify syntactic and semantic information
associated with words and substructures given by the INSTANCE nodes. The
INSTANCE nodes are used to represent actual instances of words or facts.
SEMANTIC MODIFICATION nodes are used to contain information required by
the parser to help direct its selection of the modification patterns during
parsing of the input. The other distinction made on node types is among
SIMPLE, ACTION and CONTEXT. SIMPLE nodes represent single words, ACTION
nodes are used to represent triples and CONTEXT nodes are used for
prepositional phrases.
TYPE. The TYPE nodes in MEMORY are in a one-to-one correspondence with
the entries in the WORDLIST and serve as the reference nodes for the WORDLIST
pointers to MEMORY. Some attributes that may appear in a TYPE node are DUN
-27-
and POS. First to appear in a TYPE node is an attribute giving the text
representation for the symbol. Any routines, such as the output produc-
tion programs, can get the text representation for printing directly from
the TYPE nodes. This text is repeated here since the symbol given ii the
WORDLIST is in 6 character Chunks linked together -- a form not suitable
for printing. The second attribute always present is the DEFN link used
to chain together all definitions of the symbol. Only words and
punctuation will have non-null chains of definitions. For words, there
is a SIMPLE GENERIC node in the DEFN rbftia for each different word
usage. For punctuation, there is a single SIMPLE GENERIC node chained
to the TYPE node. System commands and set names have a null set oix
usages since information of a more detailed nature for them is not
required.. Figure 3 shows the relationship between TYPE uodes and the
WORDLIST and between TYPE-nodes and SIMPLE GENERIt nodes.
Among tne optional attributes used in a TYPE node is the SYSSET link
used to chain together all TYPE nodes which name items in the set. An
example of the use of TYSSET is for part of speech. In the TYPE node
for the symbol &lt;POS&gt; is the root for the chain through the TYPE nodes for
NOUN, PRONOUN, VERB, etc.
The POS link is present in the TYPE nodes for words that name the
various grammatical properties (singular, nominal, etc.) and parts of
speech (noun, etc.).
SIMPLE GENERIC
*:*E9DEFN[0] : node for a second
usage of BOOK.
</bodyText>
<figureCaption confidence="0.974884">
Figure 3: A segment of MEMORY showing the twor majur
functions of the TYPE node.
</figureCaption>
<figure confidence="0.500999">
1
1 BOOK
1
</figure>
<bodyText confidence="0.99042925">
SIMPLE GENERIC. These nodes are used to represent the different
usages for words, i.e., bio serve mainly as a source for semantic and
syntactic information about a word. A large variety of attributes can
appear in a SIMPLE GENERIC node, such as DEFN, INST, ISA, POS, SYSMOD,
INFLEC, SYNONYM, ANTONYM, ACTOR, ACT, OBJECT, MODIFY, and PARTS. Of
these, only two are required. The DEFN link must be present to tie this
usage of the word with its TYPE node and to continue the chain to the
next usage, if any. Alsok required is the part of peech link, POS.
</bodyText>
<figure confidence="0.980037964285714">
-28-
WORDLIST MEMORY
■•■■•1
1 1
fti
.0basil.
I . $
emb ■•• ■INY two mmir 41.&gt; • MDEFN
i
I t
i ort
f ..4
t —i
1 i 0 1.4
1 d
•
t a •
• 4
I — I 4 t
TYPE node
for BOOK.
•
IF • . „
s&apos;Ibel!dDEFNE9 SIMPLE GENERIC
node for one
• usage of BOOK.
•
-29-
</figure>
<bodyText confidence="0.963213538461539">
Of the other attributes that can appear, two of the more important nnes
are the INST and ISA links. The tNST link is used to create the chain
of specific instances of this word represented by SIMPLE INSTANCE nodes
To create hierarchies within the set of SIMPLE GENERIC nodes, the ISA
link is used. Although each ISA link belongs to a chain, the presence of
two ISA links -- one a root link, the other d Oon lihk -- relates the
current node to the one immediately above it and the ones below it.
The ACTOR, ACT and OBJECT links in a SIMPLE GENERIC node point tc
ACTION GENERIC nodes that use the node as an ACTOR, ACT or OBJECT. The
PREP and POBJ links, if present, point to CONTEXT GENERIC nodes that use
this node as a prepositiop or a prepositional object. The INFLEC, MODIFY,
PARTS, ANTONYM and SYNONYM links point to other SIMPLE GENERIC nodes that.
are related to the current node in the specified manner.
</bodyText>
<sectionHeader confidence="0.918826" genericHeader="method">
ACTION GENERIC. These nodes represent the triples Used to give the
</sectionHeader>
<bodyText confidence="0.997502375">
meanings for ACTS. They usually contain three mandatory links- those for
the ACTOR, ACT and OBJECT chains. However, for some ACTs, the OBJECT is
either not required or is optional. As an example of an ACTION GENERIC
node, consider the ACT &amp;quot;lame. In the curreht MEMORY, it has three triples
attached to it giving the verb senses of (PERSON OWN THING), (PERSON OWN
ANIMAL) and (PERSON OWN SLAVE).
Only two other attributes are allowed in an ACTION GENERIC node.
These are the MODIFY And CNTXT links whith are used to specify single word
and prepositionalphrase modification of the ACTION node. When these two
links appear in a GENERTC node, they refer to the potential types of
modification that may occur.
-30-
CONTEXT GENERIC. Nodes of this kind always contain exactly three
links: PREP, POBJ and CNTXT. Since this node is used to specify potential
types of modification, the PREP and POBJ links are used to point to
particular SIMPLE GENERIC nodes for the preposition and prepositional
object. The CNTXT link is used to tie this CONTEXT node to the ACTION
GENERIC node it modifies.
SIMPLE INSTANCE. A SIMPLE INSTANCE node is present in MEMORY for each
distinct instance of each word that has been used anywhere &amp;quot; an ACTOR, ACT,
OBJECT, modifier, etc., in the. representation of information by ACTION
INSTANCE and CONTEXT INSTANCE nodes, Thbre is only one mandatory link in
the SIMPLE INSTANCE node, the INST link. However, there are usually seveLal
more selected from the set of AEITOR, ACT, OBJECT, PREP, POBJ, and MODIFY
depending on the uses to which this articular instance has been put. In
the case of all links except MODIFY, the link points to the ACTION INSTANCE
or CONTEXT INSTANCE nodes where the current node is used. For MODIFY,
however, it can show where this node modifies another or is modified by
another depending on whether or not this is the root link.
ACTION INSTANCE. All factual information within the svstPm is
represented by ACTION INSTANCE nodes. These nodes are triples that bring
together the relations between actual INSTANCES of ACTORs, ACTs and
OBJECTs plus their modification.
CONTEXT INSTANCE. The modification of ACTION INSTANCE nodes by
prepositional phrases is specified by the use of PREP, POBJ and CNTXT links
in nodes of this kind.
-s]-
SEMANTIC MODIFICATION. The structural information necessary for the
parser to determine correct forms of modification ig given by varitible
length attributes that can occur in this kind of node.
</bodyText>
<sectionHeader confidence="0.975671" genericHeader="method">
4. PARSER
</sectionHeader>
<subsectionHeader confidence="0.800067">
4.1 PARSING STRATEGY
</subsectionHeader>
<bodyText confidence="0.986020714285714">
The data structure used to drive the parser is the triple (ACTION
GENERIC node) which specifies the semantics for the major components of
the parse. By applying the triple as a template to the input, the ACTOR,
ACT and OBJECT can be identified.
As the input is parsed, its meaning is converted into a parse network
and a network &amp;quot;score&amp;quot; is calculated Usually there are several parse
networks constructed from a single input representing different meanings of
that input. The best parse is that one which has the highest score from its
parse network,..
A parse network is created from nodes similar in design and function to
those present in MEMORY. Like the MEMORY structures, it is composed of
INSTANCE nodes of all kinds: SIMPLE, ACTION and CONTEXT (see Figure 4).
These nodes are connected to one another by the same kinds of attributes,
e.g., the ACTION INSTANCE node has links to the ACTOR, ACT and OBJECT
INSTANCE nodes, the SIMPLE INSTANCE nodes contain MODIFY and CNTXT links
to other SIMPLE nodes or CONTEXT nodes, respectively, etc. Because the
normal access paths to INSTANCE nodes using DEFN and MIST links in TYPE and
GENERIC nodes do not exist for temporary nodes, the nodes in a parse net-
work are kept track of by a system of pointers as shown along the right ion
Figure 4. The entire network is referenced by the pointer word in the upper
right hand corner. A11 references in the temporary nodes of the parse net-
</bodyText>
<figure confidence="0.961381547619047">
-32-
MEMORY TEMPORARY STORAGE
: :•:
..:40]ACTOR[
[IJACT[ ]
[4]OBJECT[ 1:
1 .
• •‘• • • N. • • • • •
.
■
...... %
, • 1
:[ 1ACTOR[ 1 :
A-T-7n
]INST[ ] : 1
•
— -
:SI..
:[ ]ACT[ ]
]
.SG
•
:(BRANDT) _ j
•
.SG
:(HAVE)
:SG I t
:(BOOK) _ 1 r
;
1 /
1 .
1 4 :SI
1
I :[ ]OBJECT] 3:
I
..... ...I - ■:EslINSTE ]
,SI
:[ NODIFY[ 1:
S
•411■••■•.i.
4.1.0 -1{01INSTI 1 :
4•1.•
</figure>
<figureCaption confidence="0.999836">
Figure 4: Parse network for &amp;quot;BRANDT HAVE RED BOOK.&amp;quot;
</figureCaption>
<bodyText confidence="0.990854125">
-33-
work to GENERIC nodes are by links which point to nodes that are part of
MEMORY. This relates the input to specific parts of memory as well as
provides the required syntactic and semantic properties of the input
words for reference during other stages of parsing. When complete, the
parse network is in a format identical to similar structures in MEMORY.
This is very important later during the matching of input to MEMORY where
compatibility between the two is necessary.
</bodyText>
<subsectionHeader confidence="0.396749">
4.2 GRAMMAR
4.2.1 ACCEPTABLE INPUT FORMS
</subsectionHeader>
<bodyText confidence="0.5926655">
The parser has been developed to correctly handle restricted forms
of DO-questions and declarative sentences.
</bodyText>
<figure confidence="0.963176666666667">
&lt;DO-QUESTION&gt; ::= &lt;DO&gt; &lt;ACTOR&gt; &lt;ACT&gt; &lt;OBJECT&gt; ?
&lt;STATEMENT&gt; ::= &lt;ACTOR&gt; &lt;ACT&gt; &lt;OBJECT&gt;
&lt;DO&gt; ::= DO or DID or DOES
&lt;ACTOR&gt; ::= ((left modification&gt;) ACTOR
(&lt;right modification&gt;)
&lt;ACT&gt;
&lt;OBJECT&gt;
&lt;right modification&gt;
&lt;left modification&gt;
</figure>
<bodyText confidence="0.951334421052632">
::= (&lt;context phrase&gt;) (&lt;adverb&gt;) ACT
(&lt;adverb&gt;) (&lt;context phrase&gt;)
::= (&lt;1eft modification&gt;) OBJECT
(&lt;right modification&gt;)
::= &lt;prep&gt; (&lt;1eft modification&gt;)
&lt;prepobj&gt;
::= a string of words, usually adjectives, nouns,
adverbs and determiners which_modify the item
to their right.
&lt;prep&gt; ::= one of a set of prepositions specified by an
attribute
-34-
&lt;preobj&gt; ::= a noun from a particular semantic class as
specified by an attribute
&lt;context phrase&gt; ::... essentially the same as &lt;right modification&gt;
but is used to modify verbs
&lt;adverb&gt; ::■ an adverb from a set of particular words
specified by an attribute or an adverb plus
Its &lt;left modification&gt;.
</bodyText>
<tableCaption confidence="0.85999">
Table 2: Grammar for DO-questions and statements.
</tableCaption>
<bodyText confidence="0.999664142857143">
The three components of &lt;ACTOR&gt;, &lt;ACT&gt; and &lt;OBJECT&gt; are identical for the
DO-question and statement. The &lt;DO&gt; component is found only in DO-questions.
The question mark and period are the only terminating punctuation symbols
currently allowed. All of these components are expanded in Table 2 in a
BNF-like format.
Additional restrictions currently imposed upon the input are the
following:
</bodyText>
<listItem confidence="0.991378">
1. No relative clauses are allowed.
2. No compound units (subject, verb, etc.) are allowed.
</listItem>
<bodyText confidence="0.84852615">
Elements in Table 2 enclosed in &lt; &gt; are non-terminal elements of the
grammar. Such elements- enclosed in ( ) are optional.
In the definition of &lt;ACTOR&gt; and &lt;OBJECT&gt;, the left and right modi-
fication is to the immediate left or right of the word being modified. In
the case of &lt;ACT&gt;, however, the &lt;adverb&gt; and &lt;context phrase&gt; modification
can occur anywhere in the sentence. Usually the single word modifiers are
adjacent to the ACT but do not have to be. The &lt;context phrase&gt;s usually
appear at the beginning of the sentence or somewhere after the ACT.
-35-
In the case of the last five definitions in Table 2, i.e.,
&lt;left modification&gt; through &lt;adverb&gt;, there are restrictions on the
nodes which are applicable at that point in the parsing. Some examples
of the types of modification allowed are:
&lt;left modification&gt; of a noun a blue animal book
&lt;left modification&gt; of an adjective a very blue sky
&lt;right modification&gt; of a noun a friend of mine
&lt;context phrase&gt; in June
to the house
&lt;adverb&gt; yesterday
not
</bodyText>
<subsectionHeader confidence="0.642409">
4.2.2 SEMANTICS AND SYNTAX
</subsectionHeader>
<bodyText confidence="0.982565882352941">
Semantics and syntax have been integrated throughout the parsing
procedure so that both work together in the selection of appropriate
words and phrases out of which the parse network is constructed.
When matching an ACTION GENEPIC node to the input, syntax is used first
to identify nouns as potential ACTORs and OBJECTs. Then the semantic
acceptability of each is verified by comparing the word with the semantic
class specified by the triple.
Syntax is checked by a simple matching of the part of speech.
Proper word order within a grammatical subunit is maintained automaticall
by the way the modification requirements are set up. A word is deemed
semantically acceptable if it matches the semantic class listed as a
requirement, or if a word upward in its hierarchy matches the semantic
class. For example, suppose the candidate is BRANDT and the required
semantic class is PERSON. In the hierarchy containing BRANDT we have
BRANDT (ISA) BOY (ISA) PERSON. At that point there is a match on PERSON,
so BRANDT would be semantically acceptable.
-36-
</bodyText>
<subsubsectionHeader confidence="0.545191">
4.2.3 PRONOUNS, AMBIGUITY AND UNDEFINED WORDS
</subsubsectionHeader>
<bodyText confidence="0.964899545454545">
The currently implemented version of the program provides for only
very simple treatment of pronouns. On input, &amp;quot;I&amp;quot; pronouns (I, ME, MY, etc.)
are translated to the name of the person talking. &amp;quot;YOU&amp;quot; pronouns are
translated to JIMMY3. Similarly, on output, the person&apos;s name and JIMMY3
are translated back to &amp;quot;YOU&amp;quot; and &amp;quot;I&amp;quot;, respectively.
Ambiguity is not a problem in this model. If two parse networks
have identical scores, ambiguity is resolved by the memory matching scheme.
All networks are matched against memory in the attempt to locate an answer.
If several matches come out equally likely, they can all be reported.
This simple-minded approach works well by relating the input to what the
program knows.
</bodyText>
<subsectionHeader confidence="0.754422">
Throughout the program, undefined words in the input are ignored.
</subsectionHeader>
<bodyText confidence="0.9905005">
However, their text representations are saved so they can be printed
later to help explain, say, why the program was unable to interpret the
input. As illustrated by one line of the sample conversation in
section 1, the response to &amp;quot;DO I HAVE AN OLD BOOK?&amp;quot; was &amp;quot;YES. A RED ONE.
I DON&apos;T KNOW WHAT OLD MEANS.&amp;quot; The input was Interpreted as if it had
been &amp;quot;DO I HAVE AN BOOK?&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.997772">
4.3 PARSING ALGORITHM
</subsectionHeader>
<bodyText confidence="0.948752571428572">
There are two operations performed on the input text before it is
actually parsed: a) preprocessing of the text in an effort to
&amp;quot;standardize&amp;quot; it and b) the determination of the type of input received
so the proper parsing technique can be selected.
-37-
4.3.1 PREPROCtSSING OF THE INPUT TEXT
The first operation performed on the input is the translation of
each defined component to its TYPE node pointer to MEMORY. Undefined
input items are converted to null pointers. After the translation to
TYPE node pointers, the input is checked for standard greetings or
cliches that usually elicit a standard response. Examples are &amp;quot;HELLO.
HOW ARE YOU?&amp;quot;, &amp;quot;HI&amp;quot;, &amp;quot;GOODBYE&amp;quot;, etc.
The second type of preprocessing is the attempted reductioh of
words and phrases to simpler forms. This approach can be used for the
reduction of slang, misspelled words, idioms, names, etc. Stored in the
TYPE nodes for input items that can be reduced are context strings in
which an item can occur plus its replacement form.
4.3.2 DETERMINING TYPE OF INPUT
This part of the parsing algorithm is where the kind of input, i.e.,
DO-question, IS-question, Wh-question, declarative statement, etc., is
determined.
</bodyText>
<subsectionHeader confidence="0.676955">
Questions can be detected by the presence of the question mark at
</subsectionHeader>
<bodyText confidence="0.991419">
their end. Discrimination of questions into classes of YES-NO, or Wh-
is determined almost completely by the first word of the question. The
only questions not correctly classified by this approach are those with
inverted order, e.g., &amp;quot;HE ASKED A QUESTION, DIDN&apos;T HE?&amp;quot; and hypothetical
questions, e.g., &amp;quot;IF I ASK A QUESTION. WHAT WILL YOUR ANSWER BE?&amp;quot;.
Any input that ends with a period and does not begin with a verb is
classified as a statement.
Text that does not end with either a question mark or a period is re-
jected with a request that the person supply punctuation with his input.
-38-
</bodyText>
<subsubsectionHeader confidence="0.355194">
4.3.3 PARSING A DO-QUESTION OR STATEMENT
</subsubsectionHeader>
<bodyText confidence="0.999677333333333">
The DO-question and the statement are parsed in identical fashion
after the DO-word which begins the question is stripped off and the final
punctuation is thrown away.
An exhaustive approach to parsing has been selected for implementation
rather that one designed to use prediction coupled with backup facilities.
This decision was made partly on technical grounds--the inherent difficulty
in implementing such a parser in FORTRAN. A more important consideration,
though, was the attractiveness of working with the input as a whole using
the matching of templates (GENERIC ACTION nodes) rather than parsing in a
serial fashion whereby components are recognized in some left-to-right
decoding process.
The algorithm in Figure 5 describes how the parser works.
</bodyText>
<listItem confidence="0.984626">
(I) DO for all possible combinations of usages of the input words:
(2) . DO for all combinations of reasonable referents for all pronouns:
• •
(3) . . DO for each verb in the input:
• • •
(4) . . . DO for all GENERIC ACTION nodes for that verb:
• • • .
(5) . . . . Find suitable actor and object candidates.
• • • •
(6) . . . . DO for all combinations of actors and objects;
(7) Create a parse network skeleton.
(8) Add modification to the parse network.
(8) Retain network if better than previous one.
• • . . END-DO
. END-DO
• • •
. . END-DO
• •
. END-DO
</listItem>
<bodyText confidence="0.200294">
END-DO
</bodyText>
<figureCaption confidence="0.998921">
Figure 5: The parsing algorithm
</figureCaption>
<bodyText confidence="0.740720894736842">
Essentially, the algorithm is set up to produce all possible parse
networks using the known meanings of the words.
Steps (7), (8), and 10) are the heart of the parsing scheme. For
each triple provided by step (6), a skeleton consisting of an ACTION
INSTANCE node and three SIMPLE INSTANCE nodes is constructed as a temporary;
data structure. The other words in the input are then attached to it
according to the following modification scheme.
1. Apply left modification to the ACTOR, i.e., form a noun
group that consists of the ACTOR plus all its modification
that lies to its immediate left. This includes all adjectives
and determiners.
2. Apply left modification to the OBJECT. This process is
identical to that used to get left modification of the ACTOR.
3. Find CONTEXT modification of the ACT. Prepositional phrases
which modify the main verb are located. As prepositional objects
are found, they have their left modification attached by a process
identical to that used in steps 1 and 2 for the ACTOR and the
OBJECT. Note that no right modification is attempted for objects
of prepositions.
</bodyText>
<listItem confidence="0.590361">
4. Find right modification (prepositional phrase) for the ACTOR.
5. Find right modification for the OBJECT.
6. Locate single word (adverb) modification of the ACT. This
</listItem>
<bodyText confidence="0.918224">
process works from right to left through all remaining unattached
words of the input.
As each modification is identified, nodes are attached to the grow
ing parse network. Simple modification nodes are attached using the
MODIFY attribute; phrase modification is constructed using a CONTEXT
substructure and attached with a CNTXT attribute.
Upon completing the parsing, the score for the newly constructed
parse network is compared with the score for the previous best network.
The higher scoring one is retained to the next iteration.
-40-
The scheme developed for scoring a parse network is as follows:
</bodyText>
<reference confidence="0.9712296">
1. +5 is added for both the actor and the object when they are
identified. A network with a null object would get only +3 for
its actor.
2. Add +1 for each single word modifier.
3. Add +1 for each prepositional
the preposition plus its object.
additional points.
4. After the network is created,
input, including undefined words,
of the network.
</reference>
<bodyText confidence="0.795544842105263">
phrase. Note that this is just
Modification of the object scores
subtract +1 for each word of the
that was not used in the creation
Upon termination of the algorithm, there will be a &amp;quot;best&amp;quot; parse net-
work which represents the meaning of the input. In case several networks
had the same &amp;quot;best&amp;quot; score, then disambiguation of meaning is deferred, to
be resolved according to the memory matching process described in section 5.
To detect and control the parsing of &amp;quot;garbage&amp;quot; input, there is a
threshold value for the parse network score that must be exceeded before
the parse will be accepted. The current threshold value is zero. A parsing
that does not exceed this value is rejected and leads to the response of
&amp;quot;I DON&apos;T UNDERSTAND THAT.&amp;quot;
To illustrate the way the parsing algorithm works, consider the
question DOES BRANDT OWN A RED ANIMAL BOOK?
After the DO-question form of the input is recognized, the DOES and the
question mark are discarded leaving
BRANDT OWN A RED ANIMAL BOOK
to be processed. In MEMORY, these words have the following usages:
</bodyText>
<sectionHeader confidence="0.5773705" genericHeader="method">
BRANDT
OWN
A
RED
ANIMAL
BOOK
</sectionHeader>
<reference confidence="0.926355625">
- pos=noun; ISA BOY.
- pos=verb; GENERIC ACTION nodes are (PERSON OWN THING) and
(PERSON OWN ANIMAL).
- pos=article.
- pos=adiective; ISA COLOR.
- pos=neun.
- i&apos;os=noun; ISA THING.
-41-
</reference>
<bodyText confidence="0.896334137931035">
In this example, each word has only one usage so, in terms of the
algorithm in Figure 3, the top level loop (1) will be iterated once. At
step (2), there are no pronoun referents to resolve. For the verb &amp;quot;OWN&amp;quot;,
there are two triples that must be matched to the input.
Using (PERSON OWN THING), in step (5) we compile a list containing
BRANDT as its single entry to be used as an ACTOR candidate (BRANDT (ISA)
PERSON). Similarly, the set of OBJECT candidates contains BOOK since
BOOK (ISA) THING. Now, in step (6), the only possible combination of
ACTOR and OBJECT, (BRANDT OWN BOOK) is formed and passed to step (7)
where the skeleton of this network is formed.
In step (8) the procedure for adding modification is applied.
There is no left modification possible for BRANDT. However, for BOOK,
there are three words, ANIMAL, RED and A, to its left that have not been
used in the parse so far. All are found to wodify BOOK.
The score for this parse network is 9, 3 each for a semantically
acceptable ACTOR and OBJECT plus one each for A, RED, and ANIMAL. There
are no unused or undefined words in the input so nothing is subtracted.
Now, consider what happens when a second meaning of OWN, (PERSON
OWN ANIMAL), is used in steps (5) through (8). Again, the set of
suitable ACTORs will be the singleton, BRANDT.
The set of suitable objects contains only ANIMAL. Therefore, the
triple, (PERSON OWN ANIMAL), matches (BRANDT OWN ANIMAL) and the node
for ANIMAL allows A and RED as modifiers. However, there is no way to
attach BOOK to the parse network. This second parsing gets a score of
7 (ACTOR = +3, OBJECT = +3, A= +1, RED = +1, and BOOK = -1).
-42-
As a second exaMple, consider the string:
DID BRANDT WILL THE PROPERTY TO YOU?
with
</bodyText>
<reference confidence="0.494334625">
BRANDT - pos=noun; ISA PERSON.
WILL pos=noun; ISA PERSON.
WILL - pos=verb; GA node is (PERSON WILL THING).
(TO PERSON) is optional context modification.
THE - pos=article.
PROPERTY - pos=noun; ISA THING.
TO - pos=preposition.
JIMMY3 pos=noun; ISA PERSON.
</reference>
<bodyText confidence="0.996561538461539">
One possible set of definitions (usages) of the words with WILL as a
noun will not include any verbs. Therefore, that combination is rejected
immediately in step (3) of the parsing algorithm. The other possible set
contains WILL as a verb.
Once that set of usages is decided on, the parsing is straight-forward
in the manner similar to that used in the previous example. The major
difference in this input is the existence of the phrase &amp;quot;TO JIMMY3 (as
transformed from &amp;quot;TO YOU&amp;quot; in step (2)). Context modification for the ACT
is searched for nefore right modification of the OBJECT so &amp;quot;TO JIMEY3&amp;quot; is
properly attached to WILL in the parse. However, the node for PROPERTY
did not contain any patterns for right modification so that phrase could
not have been attached to the OBJECT anyway. The complete parse for this
input has a score of 8.
</bodyText>
<figure confidence="0.990373272727273">
-43-
5. MEMORY SEARCHING
5.1 OVERVIEW
••••••:
&apos;77----..,4i • 1 • :_____,.---); &amp;quot; ,
f:4,.....p. . -
•
&apos;
7.:
•
8:
</figure>
<figureCaption confidence="0.9917975">
Figure 6. General fort of a memory
match struGture.
</figureCaption>
<bodyText confidence="0.956873625">
The strategy used in MEMORY searching is similar in one respect to
the parsing procedure. Namely, during the search process a structure is
constructed and a score is calculated to measure the degree of similarity
between the question and the candidate answer. However, unlike the
exhaustive process used in obtaining a parse, the memory searching proceduri
is rather selective and does not examine all, or even a large part, of
MEMORY.
-44-
</bodyText>
<subsectionHeader confidence="0.864213">
5.2 THE MEMORY MATCH STRUCTURE
5.2.1 GENERAL STRUCTURE
</subsectionHeader>
<bodyText confidence="0.998649333333333">
For each match attempted between a parse network and an ACTION INSTANCE
node in MEMORY, a complete, new memory match s.tructure is generated. This
structure has a static component of 8 registers plus anywhere from zero to
four variable length, linked lists attached to it at various places --
(see Figure 6). These linked lists are used for collecting information
about word modification.
</bodyText>
<subsubsectionHeader confidence="0.842293">
5.2.2 ACTOR, ACT AND OBJECT COMPARISON RESULTS
</subsubsectionHeader>
<bodyText confidence="0.995233625">
Three of the registers, corresponding to the ACTOR, ACT and OBJECT
comparison results, form the heart of the memory match structure. In each
register is recorded the exact kind of match between input and MEMORY.
Three values are contained for the ACT comparison results: a pointer
to the SIMPLE GENERIC node id MEMORY for the ACT Riven in the parse network,
a pointer to the list which has the comparison between the modifiers of
input and of the MEMORY structure, and an indicator
Five possible types of matches can occur for ACTs.
</bodyText>
<reference confidence="0.31071625">
1. (+0) No match.
2. (+5) Exact match.
3. (+4) The input and MEMORY ACTs are synonyms.
4. (+4) The input and MEMORY ACTs are antonyms.
</reference>
<footnote confidence="0.394661">
9f the type of match.
</footnote>
<sectionHeader confidence="0.398675" genericHeader="method">
5. (+0) The ACT was missing from either the input or the MEMORY
node.
</sectionHeader>
<bodyText confidence="0.969152818181818">
The registers for ACTOR and OBJECT are identical. Like the ACT, they
contain three items of information: a pointer to the SIMPLE GENERIC node
for the ACTOR in the parse network, a pointer to the modifier list and the
match type indicator. The match type indicator for the ACTOR is divided
-45-
into three subunits. First, the number, singular or plural, of the ACTOR
is given. Second, the type of reference to the input ACTOR is recorded
as either a specific instance (i.e., it referred to a particular instance
of the ACTOR) or as an indefinite reference (referred to a class of
ACTORs rather than a specific one). Finally, there is the result of the
match between input and MEMORY.
</bodyText>
<reference confidence="0.956533235294118">
1. (+0) No match.
2. (+5) Exact match.
3. (+4) The input ACTOR is a member of the set named by the
MEMORY ACTOR, i.e., input ACTOR (ISA) MEMORY ACTOR.
4. (+4) The input ACTOR is the name of a set for which the
MEMORY ACTOR is a member, i.e., MEMORY ACTOR (ISA) input ACTOR.
5. (+4) Input ACTOR matched a synonym of the MEMORY ACTOR.
6. (4-4) Input ACTOR matched an ACTOR of another node in MEMORY of
the form &amp;quot;ACTOR1 BE ACTOR2&amp;quot; where ACTOR1 matched, the input ACTOR
and ACTOR2 matched Che MEMORY ACTOR, or vice versa, i.e., input
ACTOR BE x and x BE MEMORY ACTOR.
7. (+2) Did not find an instance of the input ACTOR in MEMORY
but did find an instance of a member of the set the input ACTOR
would belong to if it had been in memory, i.e., the two relations
of input ACTOR (ISA) x and MEMORY ACTOR (ISA) x both hold for some
suitable cdiegory x.
8. (+0) ACTOR missing from either input or MEMORY.
</reference>
<sectionHeader confidence="0.808961" genericHeader="method">
5.2.3 WORD MODIFICATION RESULTS
Registers of the match structure corresponding to the ACTION, ACTOR,
ACT and OBJECT nodes can all have modifier lists attached.
</sectionHeader>
<bodyText confidence="0.686968777777778">
The word modification list is singly linked; each item on the list
is given by two registers and represents a single modification, either
single word or phrase. All words or phrases that modify, say, the ACTOR,
will be on the same list. However, if any of those words is modified by a
-46-
word or phrase, then it will have a list attached containing all its modi-
fiers. Thus, for any component of the input, word modification is really a
tree of sublista whose structure is determined by the input word relations.
Six results of the match of modifiers are possible.
</bodyText>
<reference confidence="0.9466566">
1. (+0) Was not compared because previous level modification did
not match.
2. (+2) Exact match.
3. (+2) Exact match if inflections are ignored, e.g., singular
matching plural.
4. (+1) One of the two modification words is a member of the set
named by the other, i.e., there is an ISA chain leading from one to
the other.
5. (+1) The two words are both from a set of mutually exclusive
elements, e.g., matching RED with BLUE.
6. (+0) No match because the modifier appeared in only one of
the two (input and MEMORY) planes.
5.2.4 EXAMPLE STRUCTURES
Consider the question:
(1) DOES A PERSON HAVE A RED BOOK?
</reference>
<bodyText confidence="0.983583142857143">
This will be parsed as (PERSON HAVE BOOK) with RED modifying BOOK. The
two indefinite articles will also be part of the parse network but are not
matched since articles are not included in any memory structures. The
mantling of this parse network with a MEMORY structure for
(2) BRANDT OWNS A RED BOOK
would yield a memory match structure with the following properties.
Mitch score = 15; Maximum possible score = 17
</bodyText>
<figure confidence="0.861837444444444">
-47- (+4 points)
MEMORY ACTOR = BRANDT (PERSON).
Input mode = singular, indefinite.
MEMORY mode = singular.
Match type = MEMORY (ISA) input.
MEMORY ACT = OWNS (HAVE).
Match type = synonym (+4 points)
MEMORY OBJECT = BOOK (BOOK) (+5 points)
Input mode = singular, indefinite. (+2 points)
MEMORY mode m singular.
Match type = exact match.
Modifiers:
RED - Location = input and MEMO
Match type = exact match.
If (1) above is matched against the MEMORY structure representing
(3) BRANDT HAS A BLUE ANIMAL BOOK
The following memory match structure will result.
Match score = 15; Maximum possible score = 17
</figure>
<sectionHeader confidence="0.958297875" genericHeader="method">
MEMORY ACTOR - BRANDT (PERSON)
Input mode = singular, indefinite.
MEMORY mode = singular.
Match type = MEMORY (ISA) input. (+4 points)
MEMORY ACT = HAVE (HAS)
Match type = exact match (inflections are (+5 points)
ignored)
MEMORY OBJECT = BOOK (BOOK)
Input mode = singular, indefinite.
MEMORY node = singular.
Match type = exact match. (+5 points)
Modifiers:
RED - Location = input And MEMORY. (+1 point)
(Note: RED matches BLUE as mutually exclusive elements
from the same set.)
ANIMAL - Location = MEMORY only. (+0 points)
</sectionHeader>
<bodyText confidence="0.8745242">
This second MEMORY node matches the input as well (+15 score) as
the first because of the slightly better ACT match even though the OBJECT
is closer in the first case. It serves to indicate some of the problems
that are encountered by the procedure during matching.
-48--
</bodyText>
<listItem confidence="0.962990611111111">
(1) DO for all parse networks:
(2) . Compute maximum possible match score for this network.
(3) . Don&apos;t search for this network if maximum is not good enough.
(4) . Cet list of synonyms and antonyms for parse network ACT.
(5) . DO until a reasonable match has been obtained or until no more
▪ ACTORs can be found:
(6) . . Select an ACTOR to match on.
(7) . . DO for all INSTANCES of that ACTOR:
(8) . . DO for all ACTION INSTANCES which have that ACTOR:
• • • •
(9) • . Create the skeleton for a memory match structure.
(10) . • . Compare modifiers of the ACTION INSTANCE nodes.
(11) . . Compare ACTORs and their modifiers.
(12) . . Compare ACTs and their modifiers.
(13) . ▪ . Compare OBJECTS and their modifiers.
(14) . . If no good OBJECTS, then try alternates.
(15) • ▪ . Accumulate the total match score for the structure.
(16) . ▪ . Add structure to the list for that ACTOR INSTANCE.
</listItem>
<figure confidence="0.574742">
. . .
• . END-DO
. END-DO
(17) . Save best match structures for that ACTOR.
. E▪ ND-DO
(18) . Further prune the set of best match structures
END-DO
</figure>
<figureCaption confidence="0.933158">
Figure 7: The memory search algorithm.
</figureCaption>
<bodyText confidence="0.439355">
-49-
</bodyText>
<sectionHeader confidence="0.68869" genericHeader="method">
5.3 MATCHING MEMORY
5.3.1 BASIC ALGORITHM
</sectionHeader>
<bodyText confidence="0.99835605">
The algorithm used to search memory (see Figure 7) examines a limited
subset of all structures in MEMORY while trying to match the input. The
search is restricted to ACTION INSTANCE nodes in MEMORY that have either
the same or a closely related ACTOR. The object of the algorithm is to
obtain a small set of the best matches of the parse network with an
ACTION INSTANCE node in MEMORY.
To handle ambiguous input, i.e., multiple parse networks, the matching
procedure must be repeated for each network passed on by the parser (see
step (1)). Unpromising networks are eliminated in steps (2) and (3).
Synonym and antonym lists are compiled in step (4).
The termination criteria for MEMORY searching is the discovery of a
suitable match or the exhaustion of the set of suitable ACTORs used to
direct the search. The adequacy of the match between input and MEMORY is
the memory match score--the accumulation of many component scores which
measure the sivilarity of corresponding parts of two structures. The
termi.nation criterion for a suitable match is based on the value of this
score relative to the network&apos;s maximum possible score determined in step
(2). The threshold fat a &amp;quot;suitable&amp;quot; match is currently set at 70 percent
of the maximum mssible score. The number 70 is not perfect in any sense
but was selected in a trial-and-error fashion.
</bodyText>
<sectionHeader confidence="0.511221" genericHeader="method">
5.3.2 SELECTING SUITABLE ACTORS
</sectionHeader>
<bodyText confidence="0.809832">
The selection of actors to Control the range of the search procedure
is designed to provide a reasonable set of nodes closely related to the
input ACTOR. This selection procedure is used only when no ACTION INSTANCE
nodes with the input ACTOR produce sufficiently good matches.
-50-
There are five alternate Methods, described below, for getting new
ACTOR candidates. Not all of these five are always used, however. The
ones to use and the order in which they are to be applied is determined by
the mode and number of the input ACTOR.
</bodyText>
<reference confidence="0.994341444444444">
1. Search memory of ACTION INSTANCE nodes of the form
&amp;quot;ACTOR1 BE ACTOR2&amp;quot; where either ACTOR1 or ACTOR2 matches the
input ACTOR exactly. Collect the unmatched members of all
these nodes for use as new ACTOR candidates. For example,
suppose the ACTOR, BRANDT, was not successful at generating a
good match. Search for INSTANCES of (X BE BRANDT) and
(BRANDT BE X) where X is in the same general hierarchy as
BRANDT, e.g., (BRANDT IS SECOND-RADER). Now SECOND-GRADER
can be used as a source for more ACTION INSTANCE nodes to search.
2. Use all nodes above the ACTOR in its hierarchy.
3. Use all nodes below the ACTOR in its hierarchy.
4. Use all synonyms of the input ACTOR.
5. Find all INSTANCE nodes that are in the same set as the
input ACTOR, i.e., search the other nodes in the ISA chain
rooted in the node immediately above the input ACTOR in its
hierarchy. For example, for an input ACTOR, BRANDT, we have
BRANDT (ISA) BOY. Search the chain which gives all subSets
of BOY, but excludes BRANDT.
</reference>
<bodyText confidence="0.8943332">
For specific mode input ACTORs, the above procedures are executed in
the order: 1, 2, 4, 5. Procedure 3 is not used for specific input
ACTORs since its purpoce is to find specific instances for general references.
For indefinite input ACTORs, the procedures are executed in the order: 1, 2,
3, 4. ProCedure 5 is not used since it would lead to too diversified a set
of potential candidates. For instance, consider PERSON as the input ACTOR.
If we had PERSON (ISA) THING, and ANIMAL (ISA) THING, ACTORs could be
selected that are only vaguely related to the input.
This set of procedures can be executed twice. The first time, the input
ACTOR is used es it appeared in the input. The second time, its number is
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.176239">
<title confidence="0.91975">Jouital of Computational Linguistics 61 TOWARD A &amp;quot;NATURAL&amp;quot; QUESTION-ANSWERING FACILITY</title>
<author confidence="0.970591">D FRANCIS D</author>
<affiliation confidence="0.9944805">Department of CoMputer University of</affiliation>
<note confidence="0.8787956">Lawrence 66045 Maxwell is also associated with the Computation Tuggle is also associated with the School of Business Copyright (;) 1977 Association fpr Computational Linguistics</note>
<abstract confidence="0.9509544">SUMMARY This study describes the structure, implementation and potential of a simple computer program that understands and answers questions in a humanoid manner. An emphasis has been placed on the creation of an memory structure--one capable of conversation unrestricted Edglish on a variety of topics. An attempt has slso been made to find procedures that can easily and accurately determine the meaning of input text. A parser using a combination of syntax and semantics has been developed for understanding YES-NO questions, in particular, DO-type (DO, DID, DOES, etc.) questions. A third and major emphasis has been on the development of procedures allow the program to conversepeasily-and &amp;quot;naturally&amp;quot; with human. This general ga41 has been met by developing procedures that generate answers to DO-questions in a manner similar to the way a person might answer them,</abstract>
<intro confidence="0.7764">TABLE OF CONTENTS</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>5 is added for both the actor and the object when they are identified. A network with a null object would get only +3 for its actor.</title>
<contexts>
<context position="12157" citStr="[1]" startWordPosition="2047" endWordPosition="2047">W. The fact (BILL@MAXWELL OWNS BOOK) is not present in memory. Neither is any contradiction of that fact. p: BYE. j: GOODBYE. 1.3 REVIEW 1.3.1 MEMORY MODELS. The research involving memory models can be divided into two basic approaches. First are models created for the exploration of theories of human memory and for the testing of linguistio.theories. In these systems, the other features of a complete language understanding system assume a background position since the main emphasis is the memory model itself. Models which fall into this class are the works of Quinlan [11], Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research group [10]. The second type of memory model is that developed as part of a systeth &apos;which has some component other than memory as the major emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia model [5]. The memory model developed by this current research does not correspond to any existing model. It is not based -on case grammar in a strict sense, but stores information in a form more closely related to surface structures utilizing only three main components: ac</context>
</contexts>
<marker>1.</marker>
<rawString>+5 is added for both the actor and the object when they are identified. A network with a null object would get only +3 for its actor.</rawString>
</citation>
<citation valid="false">
<title>Add +1 for each single word modifier.</title>
<contexts>
<context position="17782" citStr="(2)" startWordPosition="2951" endWordPosition="2951">ing information required by the following step. This design, with minor variation, is characteristic of many existing programs for language understanding. One notable exception has been the SPEECH UNDERSTANDING PROJECT EC which advocates the use of parallel modules working simultaneously on the input, passing data freely between routines, until the desired end result is reached. The following algorithm describes how JIMMY1 behaves at the mbst superficial level. DO until person is through talking: (1) . Request user input and translate English words into internal ▪ codes (MEMORY node numbers). (2) . Parse input to create the &amp;quot;best&amp;quot; wrse network(s). (3) . Match each parse network with structures in memory to produce ▪ the &amp;quot;best&amp;quot; match(es). (4) . Produce a response based on the memory match(es). END-DO To make the processing of text more efficient, English words and punctuation are translated in step (1) into node pointers. Undefined words are changed to null pointers. The parser then creates a parse network from which a network density is computed. This density is a measure of how well the particular parse captures the meaning of the input. The parsing procedure is driven by -15- heuris</context>
<context position="51551" citStr="(2)" startWordPosition="8693" endWordPosition="8693">plementation rather that one designed to use prediction coupled with backup facilities. This decision was made partly on technical grounds--the inherent difficulty in implementing such a parser in FORTRAN. A more important consideration, though, was the attractiveness of working with the input as a whole using the matching of templates (GENERIC ACTION nodes) rather than parsing in a serial fashion whereby components are recognized in some left-to-right decoding process. The algorithm in Figure 5 describes how the parser works. (I) DO for all possible combinations of usages of the input words: (2) . DO for all combinations of reasonable referents for all pronouns: • • (3) . . DO for each verb in the input: • • • (4) . . . DO for all GENERIC ACTION nodes for that verb: • • • . (5) . . . . Find suitable actor and object candidates. • • • • (6) . . . . DO for all combinations of actors and objects; (7) Create a parse network skeleton. (8) Add modification to the parse network. (8) Retain network if better than previous one. • • . . END-DO . END-DO • • • . . END-DO • • . END-DO END-DO Figure 5: The parsing algorithm Essentially, the algorithm is set up to produce all possible parse network</context>
</contexts>
<marker>2.</marker>
<rawString>Add +1 for each single word modifier.</rawString>
</citation>
<citation valid="false">
<title>Add +1 for each prepositional the preposition plus its object. additional points.</title>
<contexts>
<context position="3471" citStr="[3]" startWordPosition="556" endWordPosition="556">t computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean the ball the boy owns. NCP: 0.K. </context>
</contexts>
<marker>3.</marker>
<rawString>Add +1 for each prepositional the preposition plus its object. additional points.</rawString>
</citation>
<citation valid="false">
<authors>
<author>ISA THING -41-BRANDT i&apos;osnoun</author>
</authors>
<title>After the network is created, input, including undefined words, of the network. - pos=noun; ISA BOY. - pos=verb;</title>
<journal>GENERIC ACTION nodes are (PERSON OWN THING)</journal>
<publisher>ISA PERSON.</publisher>
<contexts>
<context position="3437" citStr="[4]" startWordPosition="550" endWordPosition="550">UCTION There are a number of extant computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean</context>
<context position="43144" citStr="[4]" startWordPosition="7288" endWordPosition="7288">e ACTOR, ACT and OBJECT INSTANCE nodes, the SIMPLE INSTANCE nodes contain MODIFY and CNTXT links to other SIMPLE nodes or CONTEXT nodes, respectively, etc. Because the normal access paths to INSTANCE nodes using DEFN and MIST links in TYPE and GENERIC nodes do not exist for temporary nodes, the nodes in a parse network are kept track of by a system of pointers as shown along the right ion Figure 4. The entire network is referenced by the pointer word in the upper right hand corner. A11 references in the temporary nodes of the parse net-32- MEMORY TEMPORARY STORAGE : :•: ..:40]ACTOR[ [IJACT[ ] [4]OBJECT[ 1: 1 . • •‘• • • N. • • • • • . ■ ...... % , • 1 :[ 1ACTOR[ 1 : A-T-7n ]INST[ ] : 1 • — - :SI.. :[ ]ACT[ ] ] .SG • :(BRANDT) _ j • .SG :(HAVE) :SG I t :(BOOK) _ 1 r ; 1 / 1 . 1 4 :SI 1 I :[ ]OBJECT] 3: I ..... ...I - ■:EslINSTE ] ,SI :[ NODIFY[ 1: S •411■••■•.i. 4.1.0 -1{01INSTI 1 : 4•1.• Figure 4: Parse network for &amp;quot;BRANDT HAVE RED BOOK.&amp;quot; -33- work to GENERIC nodes are by links which point to nodes that are part of MEMORY. This relates the input to specific parts of memory as well as provides the required syntactic and semantic properties of the input words for reference during other </context>
</contexts>
<marker>4.</marker>
<rawString>After the network is created, input, including undefined words, of the network. - pos=noun; ISA BOY. - pos=verb; GENERIC ACTION nodes are (PERSON OWN THING) and (PERSON OWN ANIMAL). - pos=article. - pos=adiective; ISA COLOR. - pos=neun. - i&apos;os=noun; ISA THING. -41-BRANDT - pos=noun; ISA PERSON. WILL pos=noun; ISA PERSON. WILL - pos=verb; GA node is (PERSON WILL THING). (TO PERSON) is optional context modification. THE - pos=article. PROPERTY - pos=noun; ISA THING. TO - pos=preposition. JIMMY3 pos=noun; ISA PERSON.</rawString>
</citation>
<citation valid="false">
<note>(+0) No match.</note>
<contexts>
<context position="12157" citStr="[1]" startWordPosition="2047" endWordPosition="2047">W. The fact (BILL@MAXWELL OWNS BOOK) is not present in memory. Neither is any contradiction of that fact. p: BYE. j: GOODBYE. 1.3 REVIEW 1.3.1 MEMORY MODELS. The research involving memory models can be divided into two basic approaches. First are models created for the exploration of theories of human memory and for the testing of linguistio.theories. In these systems, the other features of a complete language understanding system assume a background position since the main emphasis is the memory model itself. Models which fall into this class are the works of Quinlan [11], Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research group [10]. The second type of memory model is that developed as part of a systeth &apos;which has some component other than memory as the major emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia model [5]. The memory model developed by this current research does not correspond to any existing model. It is not based -on case grammar in a strict sense, but stores information in a form more closely related to surface structures utilizing only three main components: ac</context>
</contexts>
<marker>1.</marker>
<rawString>(+0) No match.</rawString>
</citation>
<citation valid="false">
<note>(+5) Exact match.</note>
<contexts>
<context position="17782" citStr="(2)" startWordPosition="2951" endWordPosition="2951">ing information required by the following step. This design, with minor variation, is characteristic of many existing programs for language understanding. One notable exception has been the SPEECH UNDERSTANDING PROJECT EC which advocates the use of parallel modules working simultaneously on the input, passing data freely between routines, until the desired end result is reached. The following algorithm describes how JIMMY1 behaves at the mbst superficial level. DO until person is through talking: (1) . Request user input and translate English words into internal ▪ codes (MEMORY node numbers). (2) . Parse input to create the &amp;quot;best&amp;quot; wrse network(s). (3) . Match each parse network with structures in memory to produce ▪ the &amp;quot;best&amp;quot; match(es). (4) . Produce a response based on the memory match(es). END-DO To make the processing of text more efficient, English words and punctuation are translated in step (1) into node pointers. Undefined words are changed to null pointers. The parser then creates a parse network from which a network density is computed. This density is a measure of how well the particular parse captures the meaning of the input. The parsing procedure is driven by -15- heuris</context>
<context position="51551" citStr="(2)" startWordPosition="8693" endWordPosition="8693">plementation rather that one designed to use prediction coupled with backup facilities. This decision was made partly on technical grounds--the inherent difficulty in implementing such a parser in FORTRAN. A more important consideration, though, was the attractiveness of working with the input as a whole using the matching of templates (GENERIC ACTION nodes) rather than parsing in a serial fashion whereby components are recognized in some left-to-right decoding process. The algorithm in Figure 5 describes how the parser works. (I) DO for all possible combinations of usages of the input words: (2) . DO for all combinations of reasonable referents for all pronouns: • • (3) . . DO for each verb in the input: • • • (4) . . . DO for all GENERIC ACTION nodes for that verb: • • • . (5) . . . . Find suitable actor and object candidates. • • • • (6) . . . . DO for all combinations of actors and objects; (7) Create a parse network skeleton. (8) Add modification to the parse network. (8) Retain network if better than previous one. • • . . END-DO . END-DO • • • . . END-DO • • . END-DO END-DO Figure 5: The parsing algorithm Essentially, the algorithm is set up to produce all possible parse network</context>
</contexts>
<marker>2.</marker>
<rawString>(+5) Exact match.</rawString>
</citation>
<citation valid="false">
<title>(+4) The input and MEMORY ACTs are synonyms.</title>
<contexts>
<context position="3471" citStr="[3]" startWordPosition="556" endWordPosition="556">t computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean the ball the boy owns. NCP: 0.K. </context>
</contexts>
<marker>3.</marker>
<rawString>(+4) The input and MEMORY ACTs are synonyms.</rawString>
</citation>
<citation valid="false">
<title>(+4) The input and MEMORY ACTs are antonyms.</title>
<contexts>
<context position="3437" citStr="[4]" startWordPosition="550" endWordPosition="550">UCTION There are a number of extant computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean</context>
<context position="43144" citStr="[4]" startWordPosition="7288" endWordPosition="7288">e ACTOR, ACT and OBJECT INSTANCE nodes, the SIMPLE INSTANCE nodes contain MODIFY and CNTXT links to other SIMPLE nodes or CONTEXT nodes, respectively, etc. Because the normal access paths to INSTANCE nodes using DEFN and MIST links in TYPE and GENERIC nodes do not exist for temporary nodes, the nodes in a parse network are kept track of by a system of pointers as shown along the right ion Figure 4. The entire network is referenced by the pointer word in the upper right hand corner. A11 references in the temporary nodes of the parse net-32- MEMORY TEMPORARY STORAGE : :•: ..:40]ACTOR[ [IJACT[ ] [4]OBJECT[ 1: 1 . • •‘• • • N. • • • • • . ■ ...... % , • 1 :[ 1ACTOR[ 1 : A-T-7n ]INST[ ] : 1 • — - :SI.. :[ ]ACT[ ] ] .SG • :(BRANDT) _ j • .SG :(HAVE) :SG I t :(BOOK) _ 1 r ; 1 / 1 . 1 4 :SI 1 I :[ ]OBJECT] 3: I ..... ...I - ■:EslINSTE ] ,SI :[ NODIFY[ 1: S •411■••■•.i. 4.1.0 -1{01INSTI 1 : 4•1.• Figure 4: Parse network for &amp;quot;BRANDT HAVE RED BOOK.&amp;quot; -33- work to GENERIC nodes are by links which point to nodes that are part of MEMORY. This relates the input to specific parts of memory as well as provides the required syntactic and semantic properties of the input words for reference during other </context>
</contexts>
<marker>4.</marker>
<rawString>(+4) The input and MEMORY ACTs are antonyms.</rawString>
</citation>
<citation valid="false">
<note>(+0) No match.</note>
<contexts>
<context position="12157" citStr="[1]" startWordPosition="2047" endWordPosition="2047">W. The fact (BILL@MAXWELL OWNS BOOK) is not present in memory. Neither is any contradiction of that fact. p: BYE. j: GOODBYE. 1.3 REVIEW 1.3.1 MEMORY MODELS. The research involving memory models can be divided into two basic approaches. First are models created for the exploration of theories of human memory and for the testing of linguistio.theories. In these systems, the other features of a complete language understanding system assume a background position since the main emphasis is the memory model itself. Models which fall into this class are the works of Quinlan [11], Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research group [10]. The second type of memory model is that developed as part of a systeth &apos;which has some component other than memory as the major emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia model [5]. The memory model developed by this current research does not correspond to any existing model. It is not based -on case grammar in a strict sense, but stores information in a form more closely related to surface structures utilizing only three main components: ac</context>
</contexts>
<marker>1.</marker>
<rawString>(+0) No match.</rawString>
</citation>
<citation valid="false">
<note>(+5) Exact match.</note>
<contexts>
<context position="17782" citStr="(2)" startWordPosition="2951" endWordPosition="2951">ing information required by the following step. This design, with minor variation, is characteristic of many existing programs for language understanding. One notable exception has been the SPEECH UNDERSTANDING PROJECT EC which advocates the use of parallel modules working simultaneously on the input, passing data freely between routines, until the desired end result is reached. The following algorithm describes how JIMMY1 behaves at the mbst superficial level. DO until person is through talking: (1) . Request user input and translate English words into internal ▪ codes (MEMORY node numbers). (2) . Parse input to create the &amp;quot;best&amp;quot; wrse network(s). (3) . Match each parse network with structures in memory to produce ▪ the &amp;quot;best&amp;quot; match(es). (4) . Produce a response based on the memory match(es). END-DO To make the processing of text more efficient, English words and punctuation are translated in step (1) into node pointers. Undefined words are changed to null pointers. The parser then creates a parse network from which a network density is computed. This density is a measure of how well the particular parse captures the meaning of the input. The parsing procedure is driven by -15- heuris</context>
<context position="51551" citStr="(2)" startWordPosition="8693" endWordPosition="8693">plementation rather that one designed to use prediction coupled with backup facilities. This decision was made partly on technical grounds--the inherent difficulty in implementing such a parser in FORTRAN. A more important consideration, though, was the attractiveness of working with the input as a whole using the matching of templates (GENERIC ACTION nodes) rather than parsing in a serial fashion whereby components are recognized in some left-to-right decoding process. The algorithm in Figure 5 describes how the parser works. (I) DO for all possible combinations of usages of the input words: (2) . DO for all combinations of reasonable referents for all pronouns: • • (3) . . DO for each verb in the input: • • • (4) . . . DO for all GENERIC ACTION nodes for that verb: • • • . (5) . . . . Find suitable actor and object candidates. • • • • (6) . . . . DO for all combinations of actors and objects; (7) Create a parse network skeleton. (8) Add modification to the parse network. (8) Retain network if better than previous one. • • . . END-DO . END-DO • • • . . END-DO • • . END-DO END-DO Figure 5: The parsing algorithm Essentially, the algorithm is set up to produce all possible parse network</context>
</contexts>
<marker>2.</marker>
<rawString>(+5) Exact match.</rawString>
</citation>
<citation valid="false">
<title>(+4) The input ACTOR is a member of the set named by</title>
<journal>the MEMORY ACTOR, i.e., input ACTOR (ISA) MEMORY ACTOR.</journal>
<contexts>
<context position="3471" citStr="[3]" startWordPosition="556" endWordPosition="556">t computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean the ball the boy owns. NCP: 0.K. </context>
</contexts>
<marker>3.</marker>
<rawString>(+4) The input ACTOR is a member of the set named by the MEMORY ACTOR, i.e., input ACTOR (ISA) MEMORY ACTOR.</rawString>
</citation>
<citation valid="false">
<title>(+4) The input ACTOR is the name of a set for which the MEMORY ACTOR is a member, i.e., MEMORY ACTOR (ISA) input ACTOR.</title>
<contexts>
<context position="3437" citStr="[4]" startWordPosition="550" endWordPosition="550">UCTION There are a number of extant computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean</context>
<context position="43144" citStr="[4]" startWordPosition="7288" endWordPosition="7288">e ACTOR, ACT and OBJECT INSTANCE nodes, the SIMPLE INSTANCE nodes contain MODIFY and CNTXT links to other SIMPLE nodes or CONTEXT nodes, respectively, etc. Because the normal access paths to INSTANCE nodes using DEFN and MIST links in TYPE and GENERIC nodes do not exist for temporary nodes, the nodes in a parse network are kept track of by a system of pointers as shown along the right ion Figure 4. The entire network is referenced by the pointer word in the upper right hand corner. A11 references in the temporary nodes of the parse net-32- MEMORY TEMPORARY STORAGE : :•: ..:40]ACTOR[ [IJACT[ ] [4]OBJECT[ 1: 1 . • •‘• • • N. • • • • • . ■ ...... % , • 1 :[ 1ACTOR[ 1 : A-T-7n ]INST[ ] : 1 • — - :SI.. :[ ]ACT[ ] ] .SG • :(BRANDT) _ j • .SG :(HAVE) :SG I t :(BOOK) _ 1 r ; 1 / 1 . 1 4 :SI 1 I :[ ]OBJECT] 3: I ..... ...I - ■:EslINSTE ] ,SI :[ NODIFY[ 1: S •411■••■•.i. 4.1.0 -1{01INSTI 1 : 4•1.• Figure 4: Parse network for &amp;quot;BRANDT HAVE RED BOOK.&amp;quot; -33- work to GENERIC nodes are by links which point to nodes that are part of MEMORY. This relates the input to specific parts of memory as well as provides the required syntactic and semantic properties of the input words for reference during other </context>
</contexts>
<marker>4.</marker>
<rawString>(+4) The input ACTOR is the name of a set for which the MEMORY ACTOR is a member, i.e., MEMORY ACTOR (ISA) input ACTOR.</rawString>
</citation>
<citation valid="false">
<title>(+4) Input ACTOR matched a synonym of the MEMORY ACTOR.</title>
<contexts>
<context position="3344" citStr="[5]" startWordPosition="536" endWordPosition="536">.3.1 Intentions and Motivations 61 7.3.2 Norman&apos;s Problems 62 REFERENCES • • • . 64 1. INTRODUCTION There are a number of extant computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those res</context>
<context position="12492" citStr="[5]" startWordPosition="2100" endWordPosition="2100">nguistio.theories. In these systems, the other features of a complete language understanding system assume a background position since the main emphasis is the memory model itself. Models which fall into this class are the works of Quinlan [11], Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research group [10]. The second type of memory model is that developed as part of a systeth &apos;which has some component other than memory as the major emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia model [5]. The memory model developed by this current research does not correspond to any existing model. It is not based -on case grammar in a strict sense, but stores information in a form more closely related to surface structures utilizing only three main components: actor, act and object. Although this development was influented to a limited extent by Anderson and Bower&apos;s model [1], it was shaped in actual design by the parser dsed by Wilks (pee below) which attempts to find meaning by searching for triples. Wilks&apos; work was also influential in the development of the parser. 1.3.2 LINGUISTIC PARSER</context>
<context position="15615" citStr="[5]" startWordPosition="2608" endWordPosition="2608">ing. Without semantic knowledge, and using the strict grammatical rules embedded in the transition networks, this approach is really very brittle. It is not capable of handling ungrammatical input with much success. -11- Closely related to the augmented transition network approach is the use of procedures to describe a grammar as exemplified by Winograd&apos;s program [19]. This system is also predominately syntactic in nature. All information about the grammar is represented in terms of actual routines whioh are invoked during parsing. The input ana.iyzer used in Colby&apos;s Artificial Paranoia model [5] is essentially a pattern matcher which uses a few tricks to normalize all input to short strings whiW1 it hopes to recognize. It is not really either syntactically or semantically based but depends mostly on transformations to reduce input to simple, empirically derived, recognizable forms. The power of this approach lies in its ability tg accept even the most ungrammatical input and relate it to sodething which is already known. Thus, there is a given context in which all input is Interpreted-- the context of what the model knows and wants to continue talking about. 1.3.3 OUTPUT GENERATION P</context>
</contexts>
<marker>5.</marker>
<rawString>(+4) Input ACTOR matched a synonym of the MEMORY ACTOR.</rawString>
</citation>
<citation valid="false">
<title>(4-4) Input ACTOR matched an ACTOR of another node</title>
<booktitle>in MEMORY of the form &amp;quot;ACTOR1 BE ACTOR2&amp;quot; where ACTOR1 matched, the input ACTOR and ACTOR2 matched Che MEMORY ACTOR, or vice versa, i.e., input ACTOR BE x and x BE MEMORY ACTOR.</booktitle>
<contexts>
<context position="6134" citStr="[6]" startWordPosition="1033" endWordPosition="1033">hould be obvious--to facilitate their acceptance, to extend their use, and to make them more pleasant to deal with. People will be much more willing LO work with a computer program if it gives the appearance of being humanoid itself, whether the kernel part of the program concerns CAI, MIS, or whatever. -7- We have developed a computer program called JIMMY3 which embodies some of the gbwre set of characteristics of a &amp;quot;natural&amp;quot; language processing system so as to demonstrate its feasibility, usefulness, and potential power. The implementation has necessarily been largely ad hoc, but as Lindsay [6] notes, this is not altogether bad. Newell [7] properly records that there is a tradeoff between generality and power. Like Lindsay, we deliberately eschew the genera] in favor of the specific. In the interests of replicability and extensibility, we also provide a reasonably complete description of the inards of JIMMY3. 1.1 MAJOR GOALS The focus of this work has been on three problems: a) the development Df a memory structure useful for engaging in a dialogue with a person, b) the development of procedures that can easily and accurately determine the meaning of input text, and c) the developme</context>
</contexts>
<marker>6.</marker>
<rawString>(4-4) Input ACTOR matched an ACTOR of another node in MEMORY of the form &amp;quot;ACTOR1 BE ACTOR2&amp;quot; where ACTOR1 matched, the input ACTOR and ACTOR2 matched Che MEMORY ACTOR, or vice versa, i.e., input ACTOR BE x and x BE MEMORY ACTOR.</rawString>
</citation>
<citation valid="false">
<title>(+2) Did not find an instance of the input ACTOR in MEMORY but did find an instance of a member of the set the input ACTOR would belong to if it had been in memory, i.e., the two relations of input ACTOR (ISA) x and MEMORY ACTOR (ISA) x both hold for some suitable cdiegory x.</title>
<contexts>
<context position="6180" citStr="[7]" startWordPosition="1041" endWordPosition="1041">ce, to extend their use, and to make them more pleasant to deal with. People will be much more willing LO work with a computer program if it gives the appearance of being humanoid itself, whether the kernel part of the program concerns CAI, MIS, or whatever. -7- We have developed a computer program called JIMMY3 which embodies some of the gbwre set of characteristics of a &amp;quot;natural&amp;quot; language processing system so as to demonstrate its feasibility, usefulness, and potential power. The implementation has necessarily been largely ad hoc, but as Lindsay [6] notes, this is not altogether bad. Newell [7] properly records that there is a tradeoff between generality and power. Like Lindsay, we deliberately eschew the genera] in favor of the specific. In the interests of replicability and extensibility, we also provide a reasonably complete description of the inards of JIMMY3. 1.1 MAJOR GOALS The focus of this work has been on three problems: a) the development Df a memory structure useful for engaging in a dialogue with a person, b) the development of procedures that can easily and accurately determine the meaning of input text, and c) the development of procedures for the generation of output.</context>
</contexts>
<marker>7.</marker>
<rawString>(+2) Did not find an instance of the input ACTOR in MEMORY but did find an instance of a member of the set the input ACTOR would belong to if it had been in memory, i.e., the two relations of input ACTOR (ISA) x and MEMORY ACTOR (ISA) x both hold for some suitable cdiegory x.</rawString>
</citation>
<citation valid="false">
<title>(+0) ACTOR missing from either input or MEMORY.</title>
<contexts>
<context position="51896" citStr="(8)" startWordPosition="8771" endWordPosition="8771">C ACTION nodes) rather than parsing in a serial fashion whereby components are recognized in some left-to-right decoding process. The algorithm in Figure 5 describes how the parser works. (I) DO for all possible combinations of usages of the input words: (2) . DO for all combinations of reasonable referents for all pronouns: • • (3) . . DO for each verb in the input: • • • (4) . . . DO for all GENERIC ACTION nodes for that verb: • • • . (5) . . . . Find suitable actor and object candidates. • • • • (6) . . . . DO for all combinations of actors and objects; (7) Create a parse network skeleton. (8) Add modification to the parse network. (8) Retain network if better than previous one. • • . . END-DO . END-DO • • • . . END-DO • • . END-DO END-DO Figure 5: The parsing algorithm Essentially, the algorithm is set up to produce all possible parse networks using the known meanings of the words. Steps (7), (8), and 10) are the heart of the parsing scheme. For each triple provided by step (6), a skeleton consisting of an ACTION INSTANCE node and three SIMPLE INSTANCE nodes is constructed as a temporary; data structure. The other words in the input are then attached to it according to the followi</context>
</contexts>
<marker>8.</marker>
<rawString>(+0) ACTOR missing from either input or MEMORY.</rawString>
</citation>
<citation valid="false">
<title>(+0) Was not compared because previous level modification did not match.</title>
<contexts>
<context position="12157" citStr="[1]" startWordPosition="2047" endWordPosition="2047">W. The fact (BILL@MAXWELL OWNS BOOK) is not present in memory. Neither is any contradiction of that fact. p: BYE. j: GOODBYE. 1.3 REVIEW 1.3.1 MEMORY MODELS. The research involving memory models can be divided into two basic approaches. First are models created for the exploration of theories of human memory and for the testing of linguistio.theories. In these systems, the other features of a complete language understanding system assume a background position since the main emphasis is the memory model itself. Models which fall into this class are the works of Quinlan [11], Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research group [10]. The second type of memory model is that developed as part of a systeth &apos;which has some component other than memory as the major emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia model [5]. The memory model developed by this current research does not correspond to any existing model. It is not based -on case grammar in a strict sense, but stores information in a form more closely related to surface structures utilizing only three main components: ac</context>
</contexts>
<marker>1.</marker>
<rawString>(+0) Was not compared because previous level modification did not match.</rawString>
</citation>
<citation valid="false">
<note>(+2) Exact match.</note>
<contexts>
<context position="17782" citStr="(2)" startWordPosition="2951" endWordPosition="2951">ing information required by the following step. This design, with minor variation, is characteristic of many existing programs for language understanding. One notable exception has been the SPEECH UNDERSTANDING PROJECT EC which advocates the use of parallel modules working simultaneously on the input, passing data freely between routines, until the desired end result is reached. The following algorithm describes how JIMMY1 behaves at the mbst superficial level. DO until person is through talking: (1) . Request user input and translate English words into internal ▪ codes (MEMORY node numbers). (2) . Parse input to create the &amp;quot;best&amp;quot; wrse network(s). (3) . Match each parse network with structures in memory to produce ▪ the &amp;quot;best&amp;quot; match(es). (4) . Produce a response based on the memory match(es). END-DO To make the processing of text more efficient, English words and punctuation are translated in step (1) into node pointers. Undefined words are changed to null pointers. The parser then creates a parse network from which a network density is computed. This density is a measure of how well the particular parse captures the meaning of the input. The parsing procedure is driven by -15- heuris</context>
<context position="51551" citStr="(2)" startWordPosition="8693" endWordPosition="8693">plementation rather that one designed to use prediction coupled with backup facilities. This decision was made partly on technical grounds--the inherent difficulty in implementing such a parser in FORTRAN. A more important consideration, though, was the attractiveness of working with the input as a whole using the matching of templates (GENERIC ACTION nodes) rather than parsing in a serial fashion whereby components are recognized in some left-to-right decoding process. The algorithm in Figure 5 describes how the parser works. (I) DO for all possible combinations of usages of the input words: (2) . DO for all combinations of reasonable referents for all pronouns: • • (3) . . DO for each verb in the input: • • • (4) . . . DO for all GENERIC ACTION nodes for that verb: • • • . (5) . . . . Find suitable actor and object candidates. • • • • (6) . . . . DO for all combinations of actors and objects; (7) Create a parse network skeleton. (8) Add modification to the parse network. (8) Retain network if better than previous one. • • . . END-DO . END-DO • • • . . END-DO • • . END-DO END-DO Figure 5: The parsing algorithm Essentially, the algorithm is set up to produce all possible parse network</context>
</contexts>
<marker>2.</marker>
<rawString>(+2) Exact match.</rawString>
</citation>
<citation valid="false">
<title>(+2) Exact match if inflections are ignored, e.g., singular matching plural.</title>
<contexts>
<context position="3471" citStr="[3]" startWordPosition="556" endWordPosition="556">t computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean the ball the boy owns. NCP: 0.K. </context>
</contexts>
<marker>3.</marker>
<rawString>(+2) Exact match if inflections are ignored, e.g., singular matching plural.</rawString>
</citation>
<citation valid="false">
<title>(+1) One of the two modification words is a member of the set named by the other, i.e., there is an ISA chain leading from one to the other.</title>
<contexts>
<context position="3437" citStr="[4]" startWordPosition="550" endWordPosition="550">UCTION There are a number of extant computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean</context>
<context position="43144" citStr="[4]" startWordPosition="7288" endWordPosition="7288">e ACTOR, ACT and OBJECT INSTANCE nodes, the SIMPLE INSTANCE nodes contain MODIFY and CNTXT links to other SIMPLE nodes or CONTEXT nodes, respectively, etc. Because the normal access paths to INSTANCE nodes using DEFN and MIST links in TYPE and GENERIC nodes do not exist for temporary nodes, the nodes in a parse network are kept track of by a system of pointers as shown along the right ion Figure 4. The entire network is referenced by the pointer word in the upper right hand corner. A11 references in the temporary nodes of the parse net-32- MEMORY TEMPORARY STORAGE : :•: ..:40]ACTOR[ [IJACT[ ] [4]OBJECT[ 1: 1 . • •‘• • • N. • • • • • . ■ ...... % , • 1 :[ 1ACTOR[ 1 : A-T-7n ]INST[ ] : 1 • — - :SI.. :[ ]ACT[ ] ] .SG • :(BRANDT) _ j • .SG :(HAVE) :SG I t :(BOOK) _ 1 r ; 1 / 1 . 1 4 :SI 1 I :[ ]OBJECT] 3: I ..... ...I - ■:EslINSTE ] ,SI :[ NODIFY[ 1: S •411■••■•.i. 4.1.0 -1{01INSTI 1 : 4•1.• Figure 4: Parse network for &amp;quot;BRANDT HAVE RED BOOK.&amp;quot; -33- work to GENERIC nodes are by links which point to nodes that are part of MEMORY. This relates the input to specific parts of memory as well as provides the required syntactic and semantic properties of the input words for reference during other </context>
</contexts>
<marker>4.</marker>
<rawString>(+1) One of the two modification words is a member of the set named by the other, i.e., there is an ISA chain leading from one to the other.</rawString>
</citation>
<citation valid="false">
<title>(+1) The two words are both from a set of mutually exclusive elements, e.g., matching RED with BLUE.</title>
<contexts>
<context position="3344" citStr="[5]" startWordPosition="536" endWordPosition="536">.3.1 Intentions and Motivations 61 7.3.2 Norman&apos;s Problems 62 REFERENCES • • • . 64 1. INTRODUCTION There are a number of extant computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those res</context>
<context position="12492" citStr="[5]" startWordPosition="2100" endWordPosition="2100">nguistio.theories. In these systems, the other features of a complete language understanding system assume a background position since the main emphasis is the memory model itself. Models which fall into this class are the works of Quinlan [11], Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research group [10]. The second type of memory model is that developed as part of a systeth &apos;which has some component other than memory as the major emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia model [5]. The memory model developed by this current research does not correspond to any existing model. It is not based -on case grammar in a strict sense, but stores information in a form more closely related to surface structures utilizing only three main components: actor, act and object. Although this development was influented to a limited extent by Anderson and Bower&apos;s model [1], it was shaped in actual design by the parser dsed by Wilks (pee below) which attempts to find meaning by searching for triples. Wilks&apos; work was also influential in the development of the parser. 1.3.2 LINGUISTIC PARSER</context>
<context position="15615" citStr="[5]" startWordPosition="2608" endWordPosition="2608">ing. Without semantic knowledge, and using the strict grammatical rules embedded in the transition networks, this approach is really very brittle. It is not capable of handling ungrammatical input with much success. -11- Closely related to the augmented transition network approach is the use of procedures to describe a grammar as exemplified by Winograd&apos;s program [19]. This system is also predominately syntactic in nature. All information about the grammar is represented in terms of actual routines whioh are invoked during parsing. The input ana.iyzer used in Colby&apos;s Artificial Paranoia model [5] is essentially a pattern matcher which uses a few tricks to normalize all input to short strings whiW1 it hopes to recognize. It is not really either syntactically or semantically based but depends mostly on transformations to reduce input to simple, empirically derived, recognizable forms. The power of this approach lies in its ability tg accept even the most ungrammatical input and relate it to sodething which is already known. Thus, there is a given context in which all input is Interpreted-- the context of what the model knows and wants to continue talking about. 1.3.3 OUTPUT GENERATION P</context>
</contexts>
<marker>5.</marker>
<rawString>(+1) The two words are both from a set of mutually exclusive elements, e.g., matching RED with BLUE.</rawString>
</citation>
<citation valid="false">
<title>(+0) No match because the modifier appeared in only one of the two (input and MEMORY) planes.</title>
<contexts>
<context position="6134" citStr="[6]" startWordPosition="1033" endWordPosition="1033">hould be obvious--to facilitate their acceptance, to extend their use, and to make them more pleasant to deal with. People will be much more willing LO work with a computer program if it gives the appearance of being humanoid itself, whether the kernel part of the program concerns CAI, MIS, or whatever. -7- We have developed a computer program called JIMMY3 which embodies some of the gbwre set of characteristics of a &amp;quot;natural&amp;quot; language processing system so as to demonstrate its feasibility, usefulness, and potential power. The implementation has necessarily been largely ad hoc, but as Lindsay [6] notes, this is not altogether bad. Newell [7] properly records that there is a tradeoff between generality and power. Like Lindsay, we deliberately eschew the genera] in favor of the specific. In the interests of replicability and extensibility, we also provide a reasonably complete description of the inards of JIMMY3. 1.1 MAJOR GOALS The focus of this work has been on three problems: a) the development Df a memory structure useful for engaging in a dialogue with a person, b) the development of procedures that can easily and accurately determine the meaning of input text, and c) the developme</context>
</contexts>
<marker>6.</marker>
<rawString>(+0) No match because the modifier appeared in only one of the two (input and MEMORY) planes.</rawString>
</citation>
<citation valid="false">
<booktitle>2.4 EXAMPLE STRUCTURES Consider the question: (1) DOES A PERSON HAVE A</booktitle>
<publisher>RED BOOK?</publisher>
<contexts>
<context position="3344" citStr="[5]" startWordPosition="536" endWordPosition="536">.3.1 Intentions and Motivations 61 7.3.2 Norman&apos;s Problems 62 REFERENCES • • • . 64 1. INTRODUCTION There are a number of extant computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those res</context>
<context position="12492" citStr="[5]" startWordPosition="2100" endWordPosition="2100">nguistio.theories. In these systems, the other features of a complete language understanding system assume a background position since the main emphasis is the memory model itself. Models which fall into this class are the works of Quinlan [11], Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research group [10]. The second type of memory model is that developed as part of a systeth &apos;which has some component other than memory as the major emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia model [5]. The memory model developed by this current research does not correspond to any existing model. It is not based -on case grammar in a strict sense, but stores information in a form more closely related to surface structures utilizing only three main components: actor, act and object. Although this development was influented to a limited extent by Anderson and Bower&apos;s model [1], it was shaped in actual design by the parser dsed by Wilks (pee below) which attempts to find meaning by searching for triples. Wilks&apos; work was also influential in the development of the parser. 1.3.2 LINGUISTIC PARSER</context>
<context position="15615" citStr="[5]" startWordPosition="2608" endWordPosition="2608">ing. Without semantic knowledge, and using the strict grammatical rules embedded in the transition networks, this approach is really very brittle. It is not capable of handling ungrammatical input with much success. -11- Closely related to the augmented transition network approach is the use of procedures to describe a grammar as exemplified by Winograd&apos;s program [19]. This system is also predominately syntactic in nature. All information about the grammar is represented in terms of actual routines whioh are invoked during parsing. The input ana.iyzer used in Colby&apos;s Artificial Paranoia model [5] is essentially a pattern matcher which uses a few tricks to normalize all input to short strings whiW1 it hopes to recognize. It is not really either syntactically or semantically based but depends mostly on transformations to reduce input to simple, empirically derived, recognizable forms. The power of this approach lies in its ability tg accept even the most ungrammatical input and relate it to sodething which is already known. Thus, there is a given context in which all input is Interpreted-- the context of what the model knows and wants to continue talking about. 1.3.3 OUTPUT GENERATION P</context>
</contexts>
<marker>5.</marker>
<rawString>2.4 EXAMPLE STRUCTURES Consider the question: (1) DOES A PERSON HAVE A RED BOOK?</rawString>
</citation>
<citation valid="false">
<title>Search memory of ACTION INSTANCE nodes of the form &amp;quot;ACTOR1 BE ACTOR2&amp;quot; where either ACTOR1 or ACTOR2 matches the input ACTOR exactly. Collect the unmatched members of all these nodes for use as new ACTOR candidates. For example, suppose the ACTOR, BRANDT, was not successful at generating a good match.</title>
<journal>Search for INSTANCES of (X BE BRANDT) and (BRANDT BE X) where X</journal>
<contexts>
<context position="12157" citStr="[1]" startWordPosition="2047" endWordPosition="2047">W. The fact (BILL@MAXWELL OWNS BOOK) is not present in memory. Neither is any contradiction of that fact. p: BYE. j: GOODBYE. 1.3 REVIEW 1.3.1 MEMORY MODELS. The research involving memory models can be divided into two basic approaches. First are models created for the exploration of theories of human memory and for the testing of linguistio.theories. In these systems, the other features of a complete language understanding system assume a background position since the main emphasis is the memory model itself. Models which fall into this class are the works of Quinlan [11], Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research group [10]. The second type of memory model is that developed as part of a systeth &apos;which has some component other than memory as the major emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia model [5]. The memory model developed by this current research does not correspond to any existing model. It is not based -on case grammar in a strict sense, but stores information in a form more closely related to surface structures utilizing only three main components: ac</context>
</contexts>
<marker>1.</marker>
<rawString>Search memory of ACTION INSTANCE nodes of the form &amp;quot;ACTOR1 BE ACTOR2&amp;quot; where either ACTOR1 or ACTOR2 matches the input ACTOR exactly. Collect the unmatched members of all these nodes for use as new ACTOR candidates. For example, suppose the ACTOR, BRANDT, was not successful at generating a good match. Search for INSTANCES of (X BE BRANDT) and (BRANDT BE X) where X is in the same general hierarchy as BRANDT, e.g., (BRANDT IS SECOND-RADER). Now SECOND-GRADER can be used as a source for more ACTION INSTANCE nodes to search.</rawString>
</citation>
<citation valid="false">
<title>Use all nodes above the ACTOR in its hierarchy.</title>
<contexts>
<context position="17782" citStr="(2)" startWordPosition="2951" endWordPosition="2951">ing information required by the following step. This design, with minor variation, is characteristic of many existing programs for language understanding. One notable exception has been the SPEECH UNDERSTANDING PROJECT EC which advocates the use of parallel modules working simultaneously on the input, passing data freely between routines, until the desired end result is reached. The following algorithm describes how JIMMY1 behaves at the mbst superficial level. DO until person is through talking: (1) . Request user input and translate English words into internal ▪ codes (MEMORY node numbers). (2) . Parse input to create the &amp;quot;best&amp;quot; wrse network(s). (3) . Match each parse network with structures in memory to produce ▪ the &amp;quot;best&amp;quot; match(es). (4) . Produce a response based on the memory match(es). END-DO To make the processing of text more efficient, English words and punctuation are translated in step (1) into node pointers. Undefined words are changed to null pointers. The parser then creates a parse network from which a network density is computed. This density is a measure of how well the particular parse captures the meaning of the input. The parsing procedure is driven by -15- heuris</context>
<context position="51551" citStr="(2)" startWordPosition="8693" endWordPosition="8693">plementation rather that one designed to use prediction coupled with backup facilities. This decision was made partly on technical grounds--the inherent difficulty in implementing such a parser in FORTRAN. A more important consideration, though, was the attractiveness of working with the input as a whole using the matching of templates (GENERIC ACTION nodes) rather than parsing in a serial fashion whereby components are recognized in some left-to-right decoding process. The algorithm in Figure 5 describes how the parser works. (I) DO for all possible combinations of usages of the input words: (2) . DO for all combinations of reasonable referents for all pronouns: • • (3) . . DO for each verb in the input: • • • (4) . . . DO for all GENERIC ACTION nodes for that verb: • • • . (5) . . . . Find suitable actor and object candidates. • • • • (6) . . . . DO for all combinations of actors and objects; (7) Create a parse network skeleton. (8) Add modification to the parse network. (8) Retain network if better than previous one. • • . . END-DO . END-DO • • • . . END-DO • • . END-DO END-DO Figure 5: The parsing algorithm Essentially, the algorithm is set up to produce all possible parse network</context>
</contexts>
<marker>2.</marker>
<rawString>Use all nodes above the ACTOR in its hierarchy.</rawString>
</citation>
<citation valid="false">
<title>Use all nodes below the ACTOR in its hierarchy.</title>
<contexts>
<context position="3471" citStr="[3]" startWordPosition="556" endWordPosition="556">t computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean the ball the boy owns. NCP: 0.K. </context>
</contexts>
<marker>3.</marker>
<rawString>Use all nodes below the ACTOR in its hierarchy.</rawString>
</citation>
<citation valid="false">
<title>Use all synonyms of the input ACTOR.</title>
<contexts>
<context position="3437" citStr="[4]" startWordPosition="550" endWordPosition="550">UCTION There are a number of extant computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those responses we have labelled as &amp;quot;UCP.&amp;quot;) H: The ball is red. UCP.: By &apos;the ball,&apos; I assume you mean</context>
<context position="43144" citStr="[4]" startWordPosition="7288" endWordPosition="7288">e ACTOR, ACT and OBJECT INSTANCE nodes, the SIMPLE INSTANCE nodes contain MODIFY and CNTXT links to other SIMPLE nodes or CONTEXT nodes, respectively, etc. Because the normal access paths to INSTANCE nodes using DEFN and MIST links in TYPE and GENERIC nodes do not exist for temporary nodes, the nodes in a parse network are kept track of by a system of pointers as shown along the right ion Figure 4. The entire network is referenced by the pointer word in the upper right hand corner. A11 references in the temporary nodes of the parse net-32- MEMORY TEMPORARY STORAGE : :•: ..:40]ACTOR[ [IJACT[ ] [4]OBJECT[ 1: 1 . • •‘• • • N. • • • • • . ■ ...... % , • 1 :[ 1ACTOR[ 1 : A-T-7n ]INST[ ] : 1 • — - :SI.. :[ ]ACT[ ] ] .SG • :(BRANDT) _ j • .SG :(HAVE) :SG I t :(BOOK) _ 1 r ; 1 / 1 . 1 4 :SI 1 I :[ ]OBJECT] 3: I ..... ...I - ■:EslINSTE ] ,SI :[ NODIFY[ 1: S •411■••■•.i. 4.1.0 -1{01INSTI 1 : 4•1.• Figure 4: Parse network for &amp;quot;BRANDT HAVE RED BOOK.&amp;quot; -33- work to GENERIC nodes are by links which point to nodes that are part of MEMORY. This relates the input to specific parts of memory as well as provides the required syntactic and semantic properties of the input words for reference during other </context>
</contexts>
<marker>4.</marker>
<rawString>Use all synonyms of the input ACTOR.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Find all</author>
</authors>
<title>INSTANCE nodes that are in the same set as the input ACTOR, i.e., search the other nodes in the ISA chain rooted in the node immediately above the input ACTOR in its hierarchy. For example, for an input ACTOR, BRANDT, we have</title>
<contexts>
<context position="3344" citStr="[5]" startWordPosition="536" endWordPosition="536">.3.1 Intentions and Motivations 61 7.3.2 Norman&apos;s Problems 62 REFERENCES • • • . 64 1. INTRODUCTION There are a number of extant computer programs which interact intelligently with human interrogators, but all do so in a way we characterize as &amp;quot;unnatural&amp;quot;. By &amp;quot;unnatural,&amp;quot; we mean that they converse in a way significantly unlike two normal adult humans do. For example, Winograd&apos;s SHRDLU [19] seems child-like, and it shares with the Woods&apos; moonrocks system [20] the problem of being task-constrained. Weizenbaum&apos;s ELIZA [16] gives the appearance of being vague, evasive and doubting. Colby&apos;s PARRY [5], of course, is variously paranoid. Finally, the CAI systems, such as Carbonell&apos;s SCHOLAR [4] and Brown and Burton&apos;s SOPHIE [3], seem officiously pedantic. In contrast to those approaches, we wanted to construct a program able to carry on an intelligent and &amp;quot;natural&amp;quot; dialogue with a human user. As an example of a &amp;quot;natural&amp;quot; and an &amp;quot;unnatural&amp;quot; dialogue, imagine two computer programs (CPs) conversing with a human (H) in which each CP already knows &amp;quot;THE TOY am A BALL.&amp;quot; One CP is &amp;quot;natural&amp;quot; (NCP); the other is &amp;quot;unnatural&amp;quot; (UCP). (One may find fragments of responses from misting programs in those res</context>
<context position="12492" citStr="[5]" startWordPosition="2100" endWordPosition="2100">nguistio.theories. In these systems, the other features of a complete language understanding system assume a background position since the main emphasis is the memory model itself. Models which fall into this class are the works of Quinlan [11], Anderson and Bower [1], and Norman, Rumelhart and the &apos;LNR research group [10]. The second type of memory model is that developed as part of a systeth &apos;which has some component other than memory as the major emphasis. These models include Winograd&apos;s blocks world [19], Schank&apos;s Conceptual Dependency System [13, 14] and Colby&apos;s Artificial Paranoia model [5]. The memory model developed by this current research does not correspond to any existing model. It is not based -on case grammar in a strict sense, but stores information in a form more closely related to surface structures utilizing only three main components: actor, act and object. Although this development was influented to a limited extent by Anderson and Bower&apos;s model [1], it was shaped in actual design by the parser dsed by Wilks (pee below) which attempts to find meaning by searching for triples. Wilks&apos; work was also influential in the development of the parser. 1.3.2 LINGUISTIC PARSER</context>
<context position="15615" citStr="[5]" startWordPosition="2608" endWordPosition="2608">ing. Without semantic knowledge, and using the strict grammatical rules embedded in the transition networks, this approach is really very brittle. It is not capable of handling ungrammatical input with much success. -11- Closely related to the augmented transition network approach is the use of procedures to describe a grammar as exemplified by Winograd&apos;s program [19]. This system is also predominately syntactic in nature. All information about the grammar is represented in terms of actual routines whioh are invoked during parsing. The input ana.iyzer used in Colby&apos;s Artificial Paranoia model [5] is essentially a pattern matcher which uses a few tricks to normalize all input to short strings whiW1 it hopes to recognize. It is not really either syntactically or semantically based but depends mostly on transformations to reduce input to simple, empirically derived, recognizable forms. The power of this approach lies in its ability tg accept even the most ungrammatical input and relate it to sodething which is already known. Thus, there is a given context in which all input is Interpreted-- the context of what the model knows and wants to continue talking about. 1.3.3 OUTPUT GENERATION P</context>
</contexts>
<marker>5.</marker>
<rawString>Find all INSTANCE nodes that are in the same set as the input ACTOR, i.e., search the other nodes in the ISA chain rooted in the node immediately above the input ACTOR in its hierarchy. For example, for an input ACTOR, BRANDT, we have BRANDT (ISA) BOY. Search the chain which gives all subSets of BOY, but excludes BRANDT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>