<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998181">
Nonlinear Evidence Fusion and Propagation
for Hyponymy Relation Mining
</title>
<author confidence="0.999">
Fan Zhang2* Shuming Shi1 Jing Liu2 Shuqi Sun3* Chin-Yew Lin1
</author>
<affiliation confidence="0.999008333333333">
1Microsoft Research Asia
2Nankai University, China
3Harbin Institute of Technology, China
</affiliation>
<email confidence="0.994852">
{shumings, cyl}@microsoft.com
</email>
<sectionHeader confidence="0.998547" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999826">
This paper focuses on mining the hypon-
ymy (or is-a) relation from large-scale,
open-domain web documents. A nonlinear
probabilistic model is exploited to model
the correlation between sentences in the
aggregation of pattern matching results.
Based on the model, we design a set of ev-
idence combination and propagation algo-
rithms. These significantly improve the
result quality of existing approaches. Ex-
perimental results conducted on 500 mil-
lion web pages and hypernym labels for
300 terms show over 20% performance
improvement in terms of P@5, MAP and
R-Precision.
</bodyText>
<sectionHeader confidence="0.982413" genericHeader="keywords">
1 Introduction1
</sectionHeader>
<bodyText confidence="0.998888857142857">
An important task in text mining is the automatic
extraction of entities and their lexical relations; this
has wide applications in natural language pro-
cessing and web search. This paper focuses on
mining the hyponymy (or is-a) relation from large-
scale, open-domain web documents. From the
viewpoint of entity classification, the problem is to
automatically assign fine-grained class labels to
terms.
There have been a number of approaches
(Hearst 1992; Pantel &amp; Ravichandran 2004; Snow
et al., 2005; Durme &amp; Pasca, 2008; Talukdar et al.,
2008) to address the problem. These methods typi-
cally exploited manually-designed or automatical-
</bodyText>
<footnote confidence="0.5575235">
* This work was performed when Fan Zhang and Shuqi Sun
were interns at Microsoft Research Asia
</footnote>
<bodyText confidence="0.995435756756757">
ly-learned patterns (e.g., “NP such as NP”, “NP
like NP”, “NP is a NP”). Although some degree of
success has been achieved with these efforts, the
results are still far from perfect, in terms of both
recall and precision. As will be demonstrated in
this paper, even by processing a large corpus of
500 million web pages with the most popular pat-
terns, we are not able to extract correct labels for
many (especially rare) entities. Even for popular
terms, incorrect results often appear in their label
lists.
The basic philosophy in existing hyponymy ex-
traction approaches (and also many other text-
mining methods) is counting: count the number of
supporting sentences. Here a supporting sentence
of a term-label pair is a sentence from which the
pair can be extracted via an extraction pattern. We
demonstrate that the specific way of counting has a
great impact on result quality, and that the state-of-
the-art counting methods are not optimal. Specifi-
cally, we examine the problem from the viewpoint
of probabilistic evidence combination and find that
the probabilistic assumption behind simple count-
ing is the statistical independence between the ob-
servations of supporting sentences. By assuming a
positive correlation between supporting sentence
observations and adopting properly designed non-
linear combination functions, the results precision
can be improved.
It is hard to extract correct labels for rare terms
from a web corpus due to the data sparseness prob-
lem. To address this issue, we propose an evidence
propagation algorithm motivated by the observa-
tion that similar terms tend to share common hy-
pernyms. For example, if we already know that 1)
Helsinki and Tampere are cities, and 2) Porvoo is
similar to Helsinki and Tampere, then Porvoo is
</bodyText>
<page confidence="0.969448">
1159
</page>
<note confidence="0.979396">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1159–1168,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99801251948052">
very likely also a city. This intuition, however, The probabilistic evidence combination model
does not mean that the labels of a term can always that we exploit here was first proposed in (Shi et
be transferred to its similar terms. For example, al., 2009), for combining the page in-link evidence
Mount Vesuvius and Kilimanjaro are volcanoes in building a nonlinear static-rank computation
and Lhotse is similar to them, but Lhotse is not a algorithm. We applied it to the hyponymy extrac-
volcano. Therefore we should be very conservative tion problem because the model takes the depend-
and careful in hypernym propagation. In our prop- ency between supporting sentences into
agation algorithm, we first construct some pseudo consideration and the resultant evidence fusion
supporting sentences for a term from the support- formulas are quite simple. In (Snow et al., 2006), a
ing sentences of its similar terms. Then we calcu- probabilistic model was adopted to combine evi-
late label scores for terms by performing nonlinear dence from heterogeneous relationships to jointly
evidence combination based on the (pseudo and optimize the relationships. The independence of
real) supporting sentences. Such a nonlinear prop- evidence was assumed in their model. In compari-
agation algorithm is demonstrated to perform bet- son, we show that better results will be obtained if
ter than linear propagation. the evidence correlation is modeled appropriately.
Experimental results on a publicly available col- Our evidence propagation is basically about us-
lection of 500 million web pages with hypernym ing term similarity information to help instance
labels annotated for 300 terms show that our non- labeling. There have been several approaches
linear evidence fusion and propagation significant- which improve hyponymy extraction with instance
ly improve the precision and coverage of the clusters built by distributional similarity. In (Pantel
extracted hyponymy data. This is one of the tech- &amp; Ravichandran, 2004), labels were assigned to
nologies adopted in our semantic search and min- the committee (i.e., representative members) of a
ing system NeedleSeek2. semantic class and used as the hypernyms of the
In the next section, we discuss major related ef- whole class. Labels generated by their approach
forts and how they differ from our work. Section 3 tend to be rather coarse-grained, excluding the pos-
is a brief description of the baseline approach. The sibility of a term having its private labels (consid-
probabilistic evidence combination model that we ering the case that one meaning of a term is not
exploited is introduced in Section 4. Our main ap- covered by the input semantic classes). In contrast
proach is illustrated in Section 5. Section 6 shows to their method, our label scoring and ranking ap-
our experimental settings and results. Finally, Sec- proach is applied to every single term rather than a
tion 7 concludes this paper. semantic class. In addition, we also compute label
scores in a nonlinear way, which improves results
quality. In Snow et al. (2005), a supervised ap-
proach was proposed to improve hypernym classi-
fication using coordinate terms. In comparison, our
approach is unsupervised. Durme &amp; Pasca (2008)
cleaned the set of instance-label pairs with a
TF*IDF like method, by exploiting clusters of se-
mantically related phrases. The core idea is to keep
a term-label pair (T, L) only if the number of terms
having the label L in the term T’s cluster is above a
threshold and if L is not the label of too many clus-
ters (otherwise the pair will be discarded). In con-
trast, we are able to add new (high-quality) labels
for a term with our evidence propagation method.
On the other hand, low quality labels get smaller
score gains via propagation and are ranked lower.
Label propagation is performed in (Talukdar et
al., 2008; Talukdar &amp; Pereira, 2010) based on mul-
tiple instance-label graphs. Term similarity infor-
mation was not used in their approach.
2 Related Work
Existing efforts for hyponymy relation extraction
have been conducted upon various types of data
sources, including plain-text corpora (Hearst 1992;
Pantel &amp; Ravichandran, 2004; Snow et al., 2005;
Snow et al., 2006; Banko, et al., 2007; Durme &amp;
Pasca, 2008; Talukdar et al., 2008), semi-
structured web pages (Cafarella et al., 2008; Shin-
zato &amp; Torisawa, 2004), web search results (Geraci
et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen,
2009), and query logs (Pasca 2010). Our target for
optimization in this paper is the approaches that
use lexico-syntactic patterns to extract hyponymy
relations from plain-text corpora. Our future work
will study the application of the proposed algo-
rithms on other types of approaches.
2 http://research.microsoft.com/en-us/projects/needleseek/ or
http://needleseek.msra.cn/
1160
Most existing work tends to utilize small-scale
or private corpora, whereas the corpus that we used
is publicly available and much larger than most of
the existing work. We published our term sets (re-
fer to Section 6.1) and their corresponding user
judgments so researchers working on similar topics
can reproduce our results.
</bodyText>
<table confidence="0.965375">
Type Pattern
Hearst-I NPL {,) (such as) {NP,)* {and|or) NP
Hearst-II NPL {,) (include(s)  |including) {NP,)*
{and|or) NP
Hearst-III NPL {,) (e.g.|e.g) {NP,)* {and|or) NP
IsA-I NP (is|are|was|were|being) (a|an) NPL
IsA-II NP (is|are|was|were|being) {the, those) NPL
IsA-III NP (is|are|was|were|being) {another, any) NPL
</table>
<tableCaption confidence="0.9557075">
Table 1. Patterns adopted in this paper (NP: named
phrase representing an entity; NPL: label)
</tableCaption>
<sectionHeader confidence="0.997211" genericHeader="introduction">
3 Preliminaries
</sectionHeader>
<bodyText confidence="0.999982">
The problem addressed in this paper is corpus-
based is-a relation mining: extracting hypernyms
(as labels) for entities from a large-scale, open-
domain document corpus. The desired output is a
mapping from terms to their corresponding hyper-
nyms, which can naturally be represented as a
weighted bipartite graph (term-label graph). Typi-
cally we are only interested in top labels of a term
in the graph.
Following existing efforts, we adopt pattern-
matching as a basic way of extracting hyper-
nymy/hyponymy relations. Two types of patterns
(refer to Table 1) are employed, including the pop-
ular “Hearst patterns” (Hearst, 1992) and the IsA
patterns which are exploited less frequently in ex-
isting hyponym mining efforts. One or more term-
label pairs can be extracted if a pattern matches a
sentence. In the baseline approach, the weight of
an edge T4L (from term T to hypernym label L) in
the term-label graph is computed as,
</bodyText>
<equation confidence="0.999044">
w(T4L) = m - IDF(L) = m - log l+ l+n (L) (3.1)
</equation>
<bodyText confidence="0.999894071428572">
where m is the number of times the pair (T, L) is
extracted from the corpus, DF(L) is the number of
in-links of L in the graph, N is total number of
terms in the graph, and IDF means the “inverse
document frequency”.
A term can only keep its top-k neighbors (ac-
cording to the edge weight) in the graph as its final
labels.
Our pattern matching algorithm implemented in
this paper uses part-of-speech (POS) tagging in-
formation, without adopting a parser or a chunker.
The noun phrase boundaries (for terms and labels)
are determined by a manually designed POS tag
list.
</bodyText>
<sectionHeader confidence="0.980886" genericHeader="method">
4 Probabilistic Label-Scoring Model
</sectionHeader>
<bodyText confidence="0.941727454545455">
Here we model the hyponymy extraction problem
from the probability theory point of view, aiming
at estimating the score of a term-label pair (i.e., the
score of a label w.r.t. a term) with probabilistic
evidence combination. The model was studied in
(Shi et al., 2009) to combine the page in-link evi-
dence in building a nonlinear static-rank computa-
tion algorithm.
We represent the score of a term-label pair by
the probability of the label being a correct hyper-
nym of the term, and define the following events,
AT,L: Label L is a hypernym of term T (the ab-
breviated form A is used in this paper unless it is
ambiguous).
Ei: The observation that (T, L) is extracted from
a sentence Si via pattern matching (i.e., Si is a sup-
porting sentence of the pair).
Assuming that we already know m supporting
sentences (S1~Sm), our problem is to compute
P(A|E1,E2,..,Em), the posterior probability that L is
a hypernym of term T, given evidence E1~Em.
Formally, we need to find a function f to satisfy,
</bodyText>
<equation confidence="0.985305">
P(A|E1,...,Em) = f(P(A), P(A|E1)..., P(A|Em) ) (4.1)
</equation>
<bodyText confidence="0.9999025">
For simplicity, we first consider the case of
m=2. The case of m&gt;2 is quite similar.
We start from the simple case of independent
supporting sentences. That is,
</bodyText>
<equation confidence="0.966057">
P(El, E2) = P(El) - P( E2) (4.2)
P (El, E2 IA) = P (El IA) - P ( E2 IA) (4.3)
By applying Bayes rule, we get,
P( El, E2 IA) - P(A)
P(AIEl,E2) = P( El,E2)
( E2 I A) - P (A) 1
P(E2) - A)
P(
(AI El) - P(AI E2)
P (A)
Then define
P(AIE)
(AIE) = g( A) = log (P(AIE)) — lo9(P(A))
P
(El IA) - P (A)
P (El)
(4.4)
</equation>
<page confidence="0.856874">
1161
</page>
<bodyText confidence="0.999543714285714">
Here G(A|E) represents the log-probability-gain
of A given E, with the meaning of the gain in the
log-probability value of A after the evidence E is
observed (or known). It is a measure of the impact
of evidence E to the probability of event A. With
the definition of G(A|E), Formula 4.4 can be trans-
formed to,
</bodyText>
<equation confidence="0.578274">
( ) ( ) ( ) (4.5)
</equation>
<bodyText confidence="0.999620769230769">
Therefore, if E1 and E2 are independent, the log-
probability-gain of A given both pieces of evidence
will exactly be the sum of the gains of A given eve-
ry single piece of evidence respectively. It is easy
to prove (by following a similar procedure) that the
above Formula holds for the case of m&gt;2, as long
as the pieces of evidence are mutually independent.
Therefore for a term-label pair with m mutually
independent supporting sentences, if we set every
gain G(A|Ei) to be a constant value g, the posterior
gain score of the pair will be ∑ . If the
value g is the IDF of label L, the posterior gain will
be,
</bodyText>
<equation confidence="0.928353">
G(AT,L|E1...,Em) ∑ ( )
( ) (4.6)
</equation>
<bodyText confidence="0.998776666666667">
This is exactly the Formula 3.1. By this way, we
provide a probabilistic explanation of scoring the
candidate labels for a term via simple counting.
</bodyText>
<table confidence="0.999085428571429">
Hearst-I IsA-I E1: Hearst-I
E2: IsA-I
( ) 66.87 17.30 24.38
RA: ( ) ( )
R: ( ) 5997 1711 802.7
( ) ( )
RA/R 0.011 0.010 0.030
</table>
<tableCaption confidence="0.97226">
Table 2. Evidence dependency estimation for intra-
pattern and inter-pattern supporting sentences
</tableCaption>
<bodyText confidence="0.997943588235294">
In the above analysis, we assume the statistical
independence of the supporting sentence observa-
tions, which may not hold in reality. Intuitively, if
we already know one supporting sentence S1 for a
term-label pair (T, L), then we have more chance to
find another supporting sentence than if we do not
know S1. The reason is that, before we find S1, we
have to estimate the probability with the chance of
discovering a supporting sentence for a random
term-label pair. The probability is quite low be-
cause most term-label pairs do not have hyponymy
relations. Once we have observed S1, however, the
chance of (T, L) having a hyponymy relation in-
creases. Therefore the chance of observing another
supporting sentence becomes larger than before.
Table 2 shows the rough estimation of
( ) (denoted as RA), ( ) (denoted
</bodyText>
<equation confidence="0.586888">
( ) ( ) ( ) ( )
</equation>
<bodyText confidence="0.999658">
as R), and their ratios. The statistics are obtained
by performing maximal likelihood estimation
(MLE) upon our corpus and a random selection of
term-label pairs from our term sets (see Section
6.1) together with their top labels3. The data veri-
fies our analysis about the correlation between E1
and E2 (note that R=1 means independent). In addi-
tion, it can be seen that the conditional independ-
ence assumption of Formula 4.3 does not hold
(because RA&gt;1). It is hence necessary to consider
the correlation between supporting sentences in the
model. The estimation of Table 2 also indicates
that,
</bodyText>
<equation confidence="0.649263">
( ) ( )
</equation>
<bodyText confidence="0.9991205">
By following a similar procedure as above, with
Formulas 4.2 and 4.3 replaced by 4.7, we have,
</bodyText>
<equation confidence="0.61638">
( ) ( ) ( ) (4.8)
</equation>
<bodyText confidence="0.948573">
This formula indicates that when the supporting
sentences are positively correlated, the posterior
score of label L w.r.t. term T (given both the sen-
tences) is smaller than the sum of the gains caused
by one sentence only. In the extreme case that sen-
tence S2 fully depends on E1 (i.e. P(E2|E1)=1), it is
easy to prove that
( ) ( )
It is reasonable, since event E2 does not bring in
more information than E1.
Formula 4.8 cannot be used directly for compu-
ting the posterior gain. What we really need is a
function h satisfying
( ) ( ( ) ( )) (4.9)
and
( ) ∑ (4.10)
Shi et al. (2009) discussed other constraints to h
and suggested the following nonlinear functions,
</bodyText>
<equation confidence="0.516504727272727">
) (4.11)
( ) ( ∑ ( )
3 RA is estimated from the labels judged as “Good”; whereas
the estimation of R is from all judged labels.
( )
( ) ( )
( )
(4.7)
1162
( ) √∑
(p&gt;1) (4.12)
</equation>
<bodyText confidence="0.999336666666667">
In the next section, we use the above two h func-
tions as basic building blocks to compute label
scores for terms.
</bodyText>
<sectionHeader confidence="0.988515" genericHeader="method">
5 Our Approach
</sectionHeader>
<bodyText confidence="0.99975">
Multiple types of patterns (Table 1) can be adopted
to extract term-label pairs. For two supporting sen-
tences the correlation between them may depend
on whether they correspond to the same pattern. In
Section 5.1, our nonlinear evidence fusion formu-
las are constructed by making specific assumptions
about the correlation between intra-pattern sup-
porting sentences and inter-pattern ones.
Then in Section 5.2, we introduce our evidence
propagation technique in which the evidence of a
(T, L) pair is propagated to the terms similar to T.
</bodyText>
<subsectionHeader confidence="0.917956">
5.1 Nonlinear evidence fusion
</subsectionHeader>
<bodyText confidence="0.975040136363636">
For a term-label pair (T, L), assuming K patterns
are used for hyponymy extraction and the support-
ing sentences discovered with pattern i are,
(5.1)
where mi is the number of supporting sentences
corresponding to pattern i. Also assume the gain
score of Si,j is xi,j, i.e., xi,j=G(A|Si,j).
Generally speaking, supporting sentences corre-
sponding to the same pattern typically have a high-
er correlation than the sentences corresponding to
different patterns. This can be verified by the data
in Table-2. By ignoring the inter-pattern correla-
tions, we make the following simplified assump-
tion:
Assumption: Supporting sentences correspond-
ing to the same pattern are correlated, while those
of different patterns are independent.
According to this assumption, our label-scoring
function is,
( ) ∑ ( ) (5.2)
In the simple case that ( ), if the h
function of Formula 4.12 is adopted, then,
</bodyText>
<equation confidence="0.793241">
( ) (∑ √
) ( ) (5.3)
</equation>
<bodyText confidence="0.999926818181818">
We use an example to illustrate the above for-
mula.
Example: For term T and label L1, assume the
numbers of the supporting sentences corresponding
to the six pattern types in Table 1 are (4, 4, 4, 4, 4,
4), which means the number of supporting sen-
tences discovered by each pattern type is 4. Also
assume the supporting-sentence-count vector of
label L2 is (25, 0, 0, 0, 0, 0). If we use Formula 5.3
to compute the scores of L1 and L2, we can have
the following (ignoring IDF for simplicity),
</bodyText>
<equation confidence="0.992605">
Score(L1) √ ; Score(L2) √
</equation>
<bodyText confidence="0.99989725">
One the other hand, if we simply count the total
number of supporting sentences, the score of L2
will be larger.
The rationale implied in the formula is: For a
given term T, the labels supported by multiple
types of patterns tend to be more reliable than
those supported by a single pattern type, if they
have the same number of supporting sentences.
</bodyText>
<subsectionHeader confidence="0.997107">
5.2 Evidence propagation
</subsectionHeader>
<bodyText confidence="0.906617148148148">
According to the evidence fusion algorithm de-
scribed above, in order to extract term labels relia-
bly, it is desirable to have many supporting
sentences of different types. This is a big challenge
for rare terms, due to their low frequency in sen-
tences (and even lower frequency in supporting
sentences because not all occurrences can be cov-
ered by patterns). With evidence propagation, we
aim at discovering more supporting sentences for
terms (especially rare terms). Evidence propaga-
tion is motivated by the following two observa-
tions:
(I) Similar entities or coordinate terms tend to
share some common hypernyms.
(II) Large term similarity graphs are able to be
built efficiently with state-of-the-art techniques
(Agirre et al., 2009; Pantel et al., 2009; Shi et al.,
2010). With the graphs, we can obtain the similari-
ty between two terms without their hypernyms be-
ing available.
The first observation motivates us to “borrow”
the supporting sentences from other terms as auxil-
iary evidence of the term. The second observation
means that new information is brought with the
state-of-the-art term similarity graphs (in addition
to the term-label information discovered with the
patterns of Table 1).
</bodyText>
<page confidence="0.803434">
i=�
1163
</page>
<bodyText confidence="0.9999042">
Our evidence propagation algorithm contains
two phases. In phase I, some pseudo supporting
sentences are constructed for a term from the sup-
porting sentences of its neighbors in the similarity
graph. Then we calculate the label scores for terms
based on their (pseudo and real) supporting sen-
tences.
Phase I: For every supporting sentence S and
every similar term T1 of the term T, add a pseudo
supporting sentence S1 for T1, with the gain score,
</bodyText>
<equation confidence="0.992389">
G(AT,.L1IS1) = y - Sim(T,Ti) - G(ATLIS) (5.5)
</equation>
<bodyText confidence="0.999966463414634">
where y G [0,1] is the propagation factor, and
Sim(-,-) is the term similarity function taking val-
ues in [0, 1]. The formula reasonably assumes that
the gain score of the pseudo supporting sentence
depends on the gain score of the original real sup-
porting sentence, the similarity between the two
terms, and the propagation factor.
Phase II: The nonlinear evidence combination
formulas in the previous subsection are adopted to
combine the evidence of pseudo supporting sen-
tences.
Term similarity graphs can be obtained by dis-
tributional similarity or patterns (Agirre et al.,
2009; Pantel et al., 2009; Shi et al., 2010). We call
the first type of graph DS and the second type PB.
DS approaches are based on the distributional hy-
pothesis (Harris, 1985), which says that terms ap-
pearing in analogous contexts tend to be similar. In
a DS approach, a term is represented by a feature
vector, with each feature corresponding to a con-
text in which the term appears. The similarity be-
tween two terms is computed as the similarity
between their corresponding feature vectors. In PB
approaches, a list of carefully-designed (or auto-
matically learned) patterns is exploited and applied
to a text collection, with the hypothesis that the
terms extracted by applying each of the patterns to
a specific piece of text tend to be similar. Two cat-
egories of patterns have been studied in the litera-
ture (Heast 1992; Pasca 2004; Kozareva et al.,
2008; Zhang et al., 2009): sentence lexical patterns,
and HTML tag patterns. An example of sentence
lexical patterns is “T {, T}*{,} (and|or) T”. HTML
tag patterns include HTML tables, drop-down lists,
and other tag repeat patterns. In this paper, we
generate the DS and PB graphs by adopting the
best-performed methods studied in (Shi et al.,
2010). We will compare, by experiments, the prop-
agation performance of utilizing the two categories
of graphs, and also investigate the performance of
utilizing both graphs for evidence propagation.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998831">
6.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999719763157895">
Corpus We adopt a publicly available dataset in
our experiments: ClueWeb094. This is a very large
dataset collected by Carnegie Mellon University in
early 2009 and has been used by several tracks of
the Text Retrieval Conference (TREC)5. The whole
dataset consists of 1.04 billion web pages in ten
languages while only those in English, about 500
million pages, are used in our experiments. The
reason for selecting such a dataset is twofold: First,
it is a corpus large enough for conducting web-
scale experiments and getting meaningful results.
Second, since it is publicly available, it is possible
for other researchers to reproduce the experiments
in this paper.
Term sets Approaches are evaluated by using
two sets of selected terms: Wiki200, and Ext100.
For every term in the term sets, each approach
generates a list of hypernym labels, which are
manually judged by human annotators. Wiki200 is
constructed by first randomly selecting 400 Wik-
ipedia6 titles as our candidate terms, with the prob-
ability of a title T being selected to be log(1 +
F(T)), where F(T) is the frequency of T in our data
corpus. The reason of adopting such a probability
formula is to balance popular terms and rare ones
in our term set. Then 200 terms are manually se-
lected from the 400 candidate terms, with the prin-
ciple of maximizing the diversity of terms in terms
of length (i.e., number of words) and type (person,
location, organization, software, movie, song, ani-
mal, plant, etc.). Wiki200 is further divided into
two subsets: Wiki100H and Wiki100L, containing
respectively the 100 high-frequency and low-
frequency terms. Ext100 is built by first selecting
200 non-Wikipedia-title terms at random from the
term-label graph generated by the baseline ap-
proach (Formula 3.1), then manually selecting 100
terms.
</bodyText>
<tableCaption confidence="0.848092">
Some sample terms in the term sets are listed in
Table 3.
</tableCaption>
<footnote confidence="0.998980333333333">
4 http://boston.lti.cs.cmu.edu/Data/clueweb09/
5 http://trec.nist.gov/
6 http://www.wikipedia.org/
</footnote>
<page confidence="0.973826">
1164
</page>
<table confidence="0.996399769230769">
Term Sample Terms
Set
Wiki200 Canon EOS 400D, Disease management, El Sal-
vador, Excellus Blue Cross Blue Shield, F33,
Glasstron, Indium, Khandala, Kung Fu, Lake
Greenwood, Le Gris, Liriope, Lionel Barrymore,
Milk, Mount Alto, Northern Wei, Pink Lady,
Shawshank, The Dog Island, White flight, World
War II...
A2B, Antique gold, GPTEngine, Jinjiang Inn,
Ext100 Moyea SWF to Apple TV Converter, Nanny ser-
vice, Outdoor living, Plasmid DNA, Popon, Spam
detection, Taylor Ho Bynum, Villa Michelle...
</table>
<tableCaption confidence="0.999346">
Table 3. Sample terms in our term sets
</tableCaption>
<bodyText confidence="0.988917">
Annotation For each term in the term set, the
top-5 results (i.e., hypernym labels) of various
methods are mixed and judged by human annota-
tors. Each annotator assigns each result item a
judgment of “Good”, “Fair” or “Bad”. The annota-
tors do not know the method by which a result item
is generated. Six annotators participated in the la-
beling with a rough speed of 15 minutes per term.
We also encourage the annotators to add new good
results which are not discovered by any method.
The term sets and their corresponding user anno-
tations are available for download at the following
links (dataset ID=data.queryset.semcat01):
http://research.microsoft.com/en-us/projects/needleseek/
http://needleseek.msra.cn/datasets/
Evaluation We adopt the following metrics to
evaluate the hypernym list of a term generated by
each method. The evaluation score on a term set is
the average over all the terms.
Precision@k: The percentage of relevant (good
or fair) labels in the top-k results (labels judged as
“Fair” are counted as 0.5)
Recall@k: The ratio of relevant labels in the top-
k results to the total number of relevant labels
R-Precision: Precision@R where R is the total
number of labels judged as “Good”
Mean average precision (MAP): The average of
precision values at the positions of all good or fair
results
Before annotation and evaluation, the hypernym
list generated by each method for each term is pre-
processed to remove duplicate items. Two hyper-
nyms are called duplicate items if they share the
same head word (e.g., “military conflict” and “con-
flict”). For duplicate hypernyms, only the first (i.e.,
the highest ranked one) in the list is kept. The goal
with such a preprocessing step is to partially con-
sider results diversity in evaluation and to make a
more meaningful comparison among different
methods. Consider two hypernym lists for “sub-
way”:
</bodyText>
<table confidence="0.506603">
List-1: restaurant; chain restaurant; worldwide chain
restaurant; franchise; restaurant franchise...
List-2: restaurant; franchise; transportation; company;
fast food...
</table>
<bodyText confidence="0.998223">
There are more detailed hypernyms in the first
list about “subway” as a restaurant or a franchise;
while the second list covers a broader range of
meanings for the term. It is hard to say which is
better (without considering the upper-layer appli-
cations). With this preprocessing step, we keep our
focus on short hypernyms rather than detailed ones.
</bodyText>
<table confidence="0.999811727272727">
Term Set Method MAP R-Prec P@1 P@5
Wiki200 Linear 0.357 0.376 0.783 0.547
Log 0.371 0.384 0.803 0.561
3.92% 2.13% 2.55% 2.56%
PNorm 0.372 0.384 0.800 0.562
4.20% 2.13% 2.17% 2.74%
Wiki100H Linear 0.363 0.382 0.805 0.627
Log 0.393 0.402 0.845 0.660
8.26% 5.24% 4.97% 5.26%
PNorm 0.395 0.403 0.840 0.662
8.82% 5.50% 4.35% 5.28%
</table>
<tableCaption confidence="0.990329333333333">
Table 4. Performance comparison among various
evidence fusion methods (Term sets: Wiki200 and
Wiki100H; p=2 for PNorm)
</tableCaption>
<subsectionHeader confidence="0.999081">
6.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999990222222222">
We first compare the evaluation results of different
evidence fusion methods mentioned in Section 4.1.
In Table 4, Linear means that Formula 3.1 is used
to calculate label scores, whereas Log and PNorm
represent our nonlinear approach with Formulas
4.11 and 4.12 being utilized. The performance im-
provement numbers shown in the table are based
on the linear version; and the upward pointing ar-
rows indicate relative percentage improvement
over the baseline. From the table, we can see that
the nonlinear methods outperform the linear ones
on the Wiki200 term set. It is interesting to note
that the performance improvement is more signifi-
cant on Wiki100H, the set of high frequency terms.
By examining the labels and supporting sentences
for the terms in each term set, we find that for
many low-frequency terms (in Wiki100L), there
are only a few supporting sentences (corresponding
</bodyText>
<page confidence="0.982618">
1165
</page>
<bodyText confidence="0.999813925925926">
to one or two patterns). So the scores computed by
various fusion algorithms tend to be similar. In
contrast, more supporting sentences can be discov-
ered for high-frequency terms. Much information
is contained in the sentences about the hypernyms
of the high-frequency terms, but the linear function
of Formula 3.1 fails to make effective use of it.
The two nonlinear methods achieve better perfor-
mance by appropriately modeling the dependency
between supporting sentences and computing the
log-probability gain in a better way.
The comparison of the linear and nonlinear
methods on the Ext100 term set is shown in Table
5. Please note that the terms in Ext100 do not ap-
pear in Wikipedia titles. Thanks to the scale of the
data corpus we are using, even the baseline ap-
proach achieves reasonably good performance.
Please note that the terms (refer to Table 3) we are
using are “harder” than those adopted for evalua-
tion in many existing papers. Again, the results
quality is improved with the nonlinear methods,
although the performance improvement is not big
due to the reason that most terms in Ext100 are
rare. Please note that the recall (R@1, R@5) in this
paper is pseudo-recall, i.e., we treat the number of
known relevant (Good or Fair) results as the total
number of relevant ones.
</bodyText>
<table confidence="0.999435333333333">
Method MAP R-Prec P@1 P@5 R@1 R@5
Linear 0.384 0.429 0.665 0.472 0.116 0.385
0.395 0.429 0.715 0.472 0.125 0.385
Log T2.86% T0% T7.52% T0% T7.76% T0%
0.390 0.429 0.700 0.472 0.120 0.385
PNorm T1.56% T0% T5.26% T0% T3.45% T0%
</table>
<tableCaption confidence="0.925799333333333">
Table 5. Performance comparison among various
evidence fusion methods (Term set: Ext100; p=2
for PNorm)
</tableCaption>
<bodyText confidence="0.999582272727273">
The parameter p in the PNorm method is related
to the degree of correlations among supporting
sentences. The linear method of Formula 3.1 corre-
sponds to the special case of p=1; while p=oo rep-
resents the case that other supporting sentences are
fully correlated to the supporting sentence with the
maximal log-probability gain. Figure 1 shows that,
for most of the term sets, the best performance is
obtained for p G[2.0, 4.0]. The reason may be that
the sentence correlations are better estimated with
p values in this range.
</bodyText>
<figureCaption confidence="0.8997045">
Figure 1. Performance curves of PNorm with dif-
ferent parameter values (Measure: MAP)
</figureCaption>
<bodyText confidence="0.998166933333333">
The experimental results of evidence propaga-
tion are shown in Table 6. The methods for com-
parison are,
Base: The linear function without propagation.
NL: Nonlinear evidence fusion (PNorm with
p=2) without propagation.
LP: Linear propagation, i.e., the linear function
is used to combine the evidence of pseudo support-
ing sentences.
NLP: Nonlinear propagation where PNorm
(p=2) is used to combine the pseudo supporting
sentences.
NL+NLP: The nonlinear method is used to
combine both supporting sentences and pseudo
supporting sentences.
</bodyText>
<table confidence="0.9998626">
Method MAP R-Prec P@1 P@5 R@5
Base 0.357 0.376 0.783 0.547 0.317
0.372 0.384 0.800 0.562 0.325
NL T4.20% T2.13% T2.17% T2.74% T2.52%
0.357 0.376 0.783 0.547 0.317
LP T0% T0% T0% T0% T0%
0.396 0.418 0.785 0.605 0.357
NLP T10.9% T11.2% T0.26% T10.6% T12.6%
0.447 0.461 0.840 0.667 0.404
NL+NLP T25.2% T22.6% T7.28% T21.9% T27.4%
</table>
<tableCaption confidence="0.781561">
Table 6. Evidence propagation results (Term set:
Wiki200; Similarity graph: PB; Nonlinear formula:
PNorm)
</tableCaption>
<bodyText confidence="0.999810333333333">
In this paper, we generate the DS (distributional
similarity) and PB (pattern-based) graphs by adopt-
ing the best-performed methods studied in (Shi et
al., 2010). The performance improvement numbers
(indicated by the upward pointing arrows) shown
in tables 6~9 are relative percentage improvement
</bodyText>
<page confidence="0.977002">
1166
</page>
<bodyText confidence="0.999912125">
over the base approach (i.e., linear function with-
out propagation). The values of parameter y are set
to maximize the MAP values.
Several observations can be made from Table 6.
First, no performance improvement can be ob-
tained with the linear propagation method (LP),
while the nonlinear propagation algorithm (NLP)
works quite well in improving both precision and
recall. The results demonstrate the high correlation
between pseudo supporting sentences and the great
potential of using term similarity to improve hy-
pernymy extraction. The second observation is that
the NL+NLP approach achieves a much larger per-
formance improvement than NL and NLP. Similar
results (omitted due to space limitation) can be
observed on the Ext100 term set.
</bodyText>
<table confidence="0.99980725">
Method MAP R-Prec P@1 P@5 R@5
Base 0.357 0.376 0.783 0.547 0.317
NL+NLP 0.415 0.439 0.830 0.633 0.379
(PB) T16.2% T16.8% T6.00% T15.7% T19.6%
NL+NLP 0.456 0.469 0.843 0.673 0.406
(DS) T27.7% T24.7% T7.66% T23.0% T28.1%
NL+NLP 0.473 0.487 0.860 0.700 0.434
(PB+DS) T32.5% T29.5% T9.83% T28.0% T36.9%
</table>
<tableCaption confidence="0.956405333333333">
Table 7. Combination of PB and DS graphs for
evidence propagation (Term set: Wiki200; Nonlin-
ear formula: Log)
</tableCaption>
<table confidence="0.999959125">
Method MAP R-Prec P@1 P@5 R@5
Base 0.351 0.370 0.760 0.467 0.317
NL+NLP 0.411 0.448 0.770 0.564 0.401
(PB) T17.1% T21.1% T1.32% T20.8% T26.5%
NL+NLP 0.469 0.490 0.815 0.622 0.438
(DS) T33.6% T32.4% T7.24% T33.2% T38.2%
NL+NLP 0.491 0.513 0.860 0.654 0.479
(PB+DS) T39.9% T38.6% T13.2% T40.0% T51.1%
</table>
<tableCaption confidence="0.996036">
Table 8. Combination of PB and DS graphs for
evidence propagation (Term set: Wiki100L)
</tableCaption>
<bodyText confidence="0.9999266">
Now let us study whether it is possible to com-
bine the PB and DS graphs to obtain better results.
As shown in Tables 7, 8, and 9 (for term sets
Wiki200, Wiki100L, and Ext100 respectively, us-
ing the Log formula for fusion and propagation),
utilizing both graphs really yields additional per-
formance gains. We explain this by the fact that the
information in the two term similarity graphs tends
to be complimentary. The performance improve-
ment over Wiki100L is especially remarkable. This
is reasonable because rare terms do not have ade-
quate information in their supporting sentences due
to data sparseness. As a result, they benefit the
most from the pseudo supporting sentences propa-
gated with the similarity graphs.
</bodyText>
<table confidence="0.999868875">
Method MAP R-Prec P@1 P@5 R@5
Base 0.384 0.429 0.665 0.472 0.385
NL+NLP 0.454 0.479 0.745 0.550 0.456
(PB) T18.3% T11.7% T12.0% T16.5% T18.4%
NL+NLP 0.404 0.441 0.720 0.486 0.402
(DS) T5.18% T2.66% T8.27% T2.97% T4.37%
NL+NLP(P 0.483 0.518 0.760 0.586 0.492
B+DS) T26.0% T20.6% T14.3% T24.2% T27.6%
</table>
<tableCaption confidence="0.9919715">
Table 9. Combination of PB and DS graphs for
evidence propagation (Term set: Ext100)
</tableCaption>
<sectionHeader confidence="0.998371" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999981923076923">
We demonstrated that the way of aggregating sup-
porting sentences has considerable impact on re-
sults quality of the hyponym extraction task using
lexico-syntactic patterns, and the widely-used
counting method is not optimal. We applied a se-
ries of nonlinear evidence fusion formulas to the
problem and saw noticeable performance im-
provement. The data quality is improved further
with the combination of nonlinear evidence fusion
and evidence propagation. We also introduced a
new evaluation corpus with annotated hypernym
labels for 300 terms, which were shared with the
research community.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998792">
We would like to thank Matt Callcut for reading
through the paper. Thanks to the annotators for
their efforts in judging the hypernym labels.
Thanks to Yueguo Chen, Siyu Lei, and the anony-
mous reviewers for their helpful comments and
suggestions. The first author is partially supported
by the NSF of China (60903028,61070014), and
Key Projects in the Tianjin Science and Technolo-
gy Pillar Program.
</bodyText>
<page confidence="0.993056">
1167
</page>
<sectionHeader confidence="0.998343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999968206896552">
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas-
ca, and A. Soroa. 2009. A Study on Similarity and
Relatedness Using Distributional and WordNet-based
Approaches. In Proc. of NAACL-HLT’2009.
M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open Information Extraction
from the Web. In Proc. of IJCAI’2007.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y.
Zhang. 2008. WebTables: Exploring the Power of
Tables on the Web. In Proceedings of the 34th Con-
ference on Very Large Data Bases (VLDB’2008),
pages 538–549, Auckland, New Zealand.
B. Van Durme and M. Pasca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of
labeled instances for open-domain information ex-
traction. Twenty-Third AAAI Conference on Artifi-
cial Intelligence.
F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani.
2006. Cluster Generation and Cluster Labelling for
Web Snippets: A Fast and Accurate Hierarchical So-
lution. In Proceedings of the 13th Conference on
String Processing and Information Retrieval
(SPIRE’2006), pages 25–36, Glasgow, Scotland.
Z. S. Harris. 1985. Distributional Structure. The Philos-
ophy of Linguistics. New York: Oxford University
Press.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Fourteenth International
Conference on Computational Linguistics, Nantes,
France.
Z. Kozareva, E. Riloff, E.H. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proc. of ACL&apos;2008.
P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu and
V. Vyas. 2009. Web-Scale Distributional Similarity
and Entity Set Expansion. EMNLP’2009. Singapore.
P. Pantel and D. Ravichandran. 2004. Automatically
Labeling Semantic Classes. In Proc. of the 2004 Hu-
man Language Technology Conference (HLT-
NAACL’2004), 321–328.
M. Pasca. 2004. Acquisition of Categorized Named
Entities for Web Search. In Proc. of CIKM’2004.
M. Pasca. 2010. The Role of Queries in Ranking La-
beled Instances Extracted from Text. In Proc. of
COLING’2010, Beijing, China.
S. Shi, B. Lu, Y. Ma, and J.-R. Wen. 2009. Nonlinear
Static-Rank Computation. In Proc. of CIKM’2009,
Kong Kong.
S. Shi, H. Zhang, X. Yuan, J.-R. Wen. 2010. Corpus-
based Semantic Class Mining: Distributional vs. Pat-
tern-Based Approaches. In Proc. of COLING’2010,
Beijing, China.
K. Shinzato and K. Torisawa. 2004. Acquiring Hypon-
ymy Relations from Web Documents. In Proc. of the
2004 Human Language Technology Conference
(HLT-NAACL’2004).
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning
Syntactic Patterns for Automatic Hypernym Discov-
ery. In Proceedings of the 19th Conference on Neural
Information Processing Systems.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
Taxonomy Induction from Heterogenous Evidence.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL-06), 801–808.
P. P. Talukdar and F. Pereira. 2010. Experiments in
Graph-based Semi-Supervised Learning Methods for
Class-Instance Acquisition. In 48th Annual Meeting
of the Association for Computational Linguistics
(ACL’2010).
P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran,
R. Bhagat, and F. Pereira. 2008. Weakly-Supervised
Acquisition of Labeled Class Instances using Graph
Random Walks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP’2008), pages 581–589.
R.C. Wang. W.W. Cohen. Automatic Set Instance Ex-
traction using the Web. In Proc. of the 47th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP’2009), pages 441–449, Sin-
gapore.
H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. 2009. Em-
ploying Topic Models for Pattern-based Semantic
Class Discovery. In Proc. of the 47th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-IJCNLP’2009), pages 441–449, Singapore.
</reference>
<page confidence="0.995246">
1168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9988575">Nonlinear Evidence Fusion and for Hyponymy Relation Mining</title>
<author confidence="0.990978">Shuming Jing Shuqi Chin-Yew</author>
<affiliation confidence="0.981491666666667">Research University, Institute of Technology, China</affiliation>
<email confidence="0.998366">shumings@microsoft.com</email>
<email confidence="0.998366">cyl@microsoft.com</email>
<abstract confidence="0.977308885462556">This paper focuses on mining the hyponymy (or is-a) relation from large-scale, open-domain web documents. A nonlinear probabilistic model is exploited to model the correlation between sentences in the aggregation of pattern matching results. Based on the model, we design a set of evidence combination and propagation algorithms. These significantly improve the result quality of existing approaches. Experimental results conducted on 500 million web pages and hypernym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision. An important task in text mining is the automatic extraction of entities and their lexical relations; this has wide applications in natural language processing and web search. This paper focuses on mining the hyponymy (or is-a) relation from largescale, open-domain web documents. From the viewpoint of entity classification, the problem is to assign labels to terms. There have been a number of approaches (Hearst 1992; Pantel &amp; Ravichandran 2004; Snow et al., 2005; Durme &amp; Pasca, 2008; Talukdar et al., 2008) to address the problem. These methods typiexploited manually-designed or automaticalwork was performed when Fan Zhang and Shuqi Sun were interns at Microsoft Research Asia patterns such as is a Although some degree of success has been achieved with these efforts, the results are still far from perfect, in terms of both recall and precision. As will be demonstrated in this paper, even by processing a large corpus of 500 million web pages with the most popular patterns, we are not able to extract correct labels for many (especially rare) entities. Even for popular terms, incorrect results often appear in their label lists. The basic philosophy in existing hyponymy extraction approaches (and also many other textmethods) is count the number of sentences. Here a sentence of a term-label pair is a sentence from which the pair can be extracted via an extraction pattern. We demonstrate that the specific way of counting has a great impact on result quality, and that the state-ofthe-art counting methods are not optimal. Specifically, we examine the problem from the viewpoint of probabilistic evidence combination and find that the probabilistic assumption behind simple countis the statistical the observations of supporting sentences. By assuming a correlation supporting sentence observations and adopting properly designed nonlinear combination functions, the results precision can be improved. It is hard to extract correct labels for rare terms from a web corpus due to the data sparseness problem. To address this issue, we propose an evidence propagation algorithm motivated by the observation that similar terms tend to share common hypernyms. For example, if we already know that 1) Helsinki and Tampere are cities, and 2) Porvoo is similar to Helsinki and Tampere, then Porvoo is 1159 of the 49th Annual Meeting of the Association for Computational pages 1159–1168, Oregon, June 19-24, 2011. Association for Computational Linguistics very likely also a city. This intuition, however, does not mean that the labels of a term can always be transferred to its similar terms. For example, Mount Vesuvius and Kilimanjaro are volcanoes and Lhotse is similar to them, but Lhotse is not a volcano. Therefore we should be very conservative and careful in hypernym propagation. In our propalgorithm, we first construct some sentences a term from the support-ing sentences of its similar terms. Then we calculabel scores for terms by performing evidence combination based on the (pseudo and real) supporting sentences. Such a nonlinear prop-agation algorithm is demonstrated to perform bet-ter than linear propagation. The probabilistic evidence combination model that we exploit here was first proposed in (Shi et al., 2009), for combining the page in-link evidence in building a nonlinear static-rank computation algorithm. We applied it to the hyponymy extrac-tion problem because the model takes the depend-ency between supporting sentences into consideration and the resultant evidence fusion formulas are quite simple. In (Snow et al., 2006), a probabilistic model was adopted to combine evi-dence from heterogeneous relationships to jointly optimize the relationships. The independence of evidence was assumed in their model. In compari-son, we show that better results will be obtained if the evidence correlation is modeled appropriately. Experimental results on a publicly available col-lection of 500 million web pages with hypernym labels annotated for 300 terms show that our non-linear evidence fusion and propagation significant-ly improve the precision and coverage of the extracted hyponymy data. This is one of the tech-nologies adopted in our semantic search and minsystem Our evidence propagation is basically about us-ing term similarity information to help instance labeling. There have been several approaches which improve hyponymy extraction with instance clusters built by distributional similarity. In (Pantel &amp; Ravichandran, 2004), labels were assigned to the committee (i.e., representative members) of a semantic class and used as the hypernyms of the whole class. Labels generated by their approach tend to be rather coarse-grained, excluding the pos-sibility of a term having its private labels (consid-ering the case that one meaning of a term is not covered by the input semantic classes). In contrast to their method, our label scoring and ranking ap-proach is applied to every single term rather than a semantic class. In addition, we also compute label scores in a nonlinear way, which improves results quality. In Snow et al. (2005), a supervised ap-proach was proposed to improve hypernym classi-fication using coordinate terms. In comparison, our approach is unsupervised. Durme &amp; Pasca (2008) cleaned the set of instance-label pairs with a TF*IDF like method, by exploiting clusters of se-mantically related phrases. The core idea is to keep term-label pair only if the number of terms the label the term cluster is above a and if not the label of too many clus-ters (otherwise the pair will be discarded). In con-trast, we are able to add new (high-quality) labels for a term with our evidence propagation method. On the other hand, low quality labels get smaller score gains via propagation and are ranked lower. In the next section, we discuss major related ef-forts and how they differ from our work. Section 3 is a brief description of the baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main ap-proach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Sec-tion 7 concludes this paper. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on mul-tiple instance-label graphs. Term similarity infor-mation was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semi-structured web pages (Cafarella et al., 2008; Shin-zato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algo-rithms on other types of approaches. 2http://research.microsoft.com/en-us/projects/needleseek/or http://needleseek.msra.cn/ 1160 Most existing work tends to utilize small-scale or private corpora, whereas the corpus that we used is publicly available and much larger than most of the existing work. We published our term sets (refer to Section 6.1) and their corresponding user judgments so researchers working on similar topics can reproduce our results. Type Pattern Hearst-I (such as) {and|or) NP Hearst-II (include(s)  |including) {and|or) NP Hearst-III (e.g.|e.g) {and|or) NP IsA-I (is|are|was|were|being) (a|an) IsA-II (is|are|was|were|being) {the, those) IsA-III (is|are|was|were|being) {another, any) Table 1. Patterns adopted in this paper (NP: named representing an entity; label) 3 Preliminaries The problem addressed in this paper is corpusmining: extracting hypernyms (as labels) for entities from a large-scale, opendomain document corpus. The desired output is a mapping from terms to their corresponding hypernyms, which can naturally be represented as a weighted bipartite graph (term-label graph). Typically we are only interested in top labels of a term in the graph. Following existing efforts, we adopt patternmatching as a basic way of extracting hypernymy/hyponymy relations. Two types of patterns (refer to Table 1) are employed, including the pop- 1992) and the IsA patterns which are exploited less frequently in existing hyponym mining efforts. One or more termlabel pairs can be extracted if a pattern matches a sentence. In the baseline approach, the weight of edge term hypernym label in the term-label graph is computed as, m - IDF(L) = m log l+n (L) the number of times the pair is from the corpus, is the number of of the graph, total number of in the graph, and IDF means the term can only keep its (according to the edge weight) in the graph as its final labels. Our pattern matching algorithm implemented in this paper uses part-of-speech (POS) tagging information, without adopting a parser or a chunker. The noun phrase boundaries (for terms and labels) are determined by a manually designed POS tag list. 4 Probabilistic Label-Scoring Model Here we model the hyponymy extraction problem from the probability theory point of view, aiming at estimating the score of a term-label pair (i.e., the score of a label w.r.t. a term) with probabilistic evidence combination. The model was studied in (Shi et al., 2009) to combine the page in-link evidence in building a nonlinear static-rank computation algorithm. We represent the score of a term-label pair by the probability of the label being a correct hypernym of the term, and define the following events, Label a hypernym of term abform used in this paper unless it is ambiguous). The observation that is extracted from sentence pattern matching (i.e., a supporting sentence of the pair). that we already know our problem is to compute the that hypernym of term given evidence we need to find a function satisfy, = ) (4.1) For simplicity, we first consider the case of The case of is quite similar. We start from the simple case of independent supporting sentences. That is, = - P( IA) = P IA) - P ( IA) By applying Bayes rule, we get, - P(A) = P( A) - P (A) 1 P( - P(AI P (A) Then define P(AIE) (AIE) = g( A) = log (P(AIE)) — lo9(P(A)) P IA) - P (A) (4.4) 1161 represents the with the meaning of the the value of the evidence observed (or known). It is a measure of the impact evidence the probability of event With definition of Formula 4.4 can be transformed to, ) ( ) ( ) if independent, the logof both pieces of evidence exactly be the sum of the gains of every single piece of evidence respectively. It is easy to prove (by following a similar procedure) that the Formula holds for the case of as long as the pieces of evidence are mutually independent. for a term-label pair with independent supporting sentences, if we set every to be a constant value the posterior score of the pair will be If the IDF of label the posterior gain will be, ( ) ) This is exactly the Formula 3.1. By this way, we provide a probabilistic explanation of scoring the candidate labels for a term via simple counting. Hearst-I IsA-I E1: Hearst-I E2: IsA-I ( ) 66.87 17.30 24.38 ( ) ( ) ( ) 5997 1711 802.7 ( ) ( ) 0.011 0.010 0.030 Table 2. Evidence dependency estimation for intrapattern and inter-pattern supporting sentences In the above analysis, we assume the statistical independence of the supporting sentence observations, which may not hold in reality. Intuitively, if already know one supporting sentence a pair then we have more chance to find another supporting sentence than if we do not The reason is that, before we find we have to estimate the probability with the chance of a supporting sentence for a term-label pair. The probability is quite low because most term-label pairs do not have hyponymy Once we have observed however, the of having a hyponymy relation increases. Therefore the chance of observing another supporting sentence becomes larger than before. Table 2 shows the rough estimation of ) (denoted as ( ) (denoted ( ) ( ) ( ) ( ) and their ratios. The statistics are obtained by performing maximal likelihood estimation (MLE) upon our corpus and a random selection of term-label pairs from our term sets (see Section together with their top The data veriour analysis about the correlation between that means independent). In addition, it can be seen that the conditional independence assumption of Formula 4.3 does not hold It is hence necessary to consider the correlation between supporting sentences in the model. The estimation of Table 2 also indicates that, ( ) ( ) By following a similar procedure as above, with Formulas 4.2 and 4.3 replaced by 4.7, we have, ) ( ) ( ) This formula indicates that when the supporting sentences are positively correlated, the posterior of label term both the sentences) is smaller than the sum of the gains caused by one sentence only. In the extreme case that sendepends on it is easy to prove that ( ) ( ) is reasonable, since event not bring in information than Formula 4.8 cannot be used directly for computing the posterior gain. What we really need is a ) ( ( ) ( )) and ) ∑ et al. (2009) discussed other constraints to and suggested the following nonlinear functions, ( ) ( ∑ ( ) estimated from the labels judged as “Good”; whereas estimation of from all judged labels. ( ) ( ) ( ) ( ) (4.7) 1162 ( ) √∑ (4.12) the next section, we use the above two functions as basic building blocks to compute label scores for terms. 5 Our Approach Multiple types of patterns (Table 1) can be adopted to extract term-label pairs. For two supporting sentences the correlation between them may depend on whether they correspond to the same pattern. In Section 5.1, our nonlinear evidence fusion formulas are constructed by making specific assumptions about the correlation between intra-pattern supporting sentences and inter-pattern ones. Then in Section 5.2, we introduce our evidence propagation technique in which the evidence of a L) pair is propagated to the terms similar to evidence fusion a term-label pair assuming are used for hyponymy extraction and the supportsentences discovered with pattern (5.1) the number of supporting sentences to pattern Also assume the gain of Generally speaking, supporting sentences corresponding to the same pattern typically have a higher correlation than the sentences corresponding to different patterns. This can be verified by the data in Table-2. By ignoring the inter-pattern correlations, we make the following simplified assumption: Supporting sentences corresponding to the same pattern are correlated, while those of different patterns are independent. According to this assumption, our label-scoring function is, ( ) ∑ ( ) In the simple case that if the function of Formula 4.12 is adopted, then, ( ) (∑ √ ) ( ) We use an example to illustrate the above formula. For term label assume the numbers of the supporting sentences corresponding to the six pattern types in Table 1 are (4, 4, 4, 4, 4, 4), which means the number of supporting sentences discovered by each pattern type is 4. Also assume the supporting-sentence-count vector of (25, 0, 0, 0, 0, 0). If we use Formula 5.3 compute the scores of we can have the following (ignoring IDF for simplicity), One the other hand, if we simply count the total of supporting sentences, the score of will be larger. The rationale implied in the formula is: For a term the labels supported by multiple types of patterns tend to be more reliable than those supported by a single pattern type, if they have the same number of supporting sentences. propagation According to the evidence fusion algorithm described above, in order to extract term labels reliably, it is desirable to have many supporting sentences of different types. This is a big challenge for rare terms, due to their low frequency in sentences (and even lower frequency in supporting sentences because not all occurrences can be covered by patterns). With evidence propagation, we aim at discovering more supporting sentences for terms (especially rare terms). Evidence propagation is motivated by the following two observations: (I) Similar entities or coordinate terms tend to share some common hypernyms. (II) Large term similarity graphs are able to be built efficiently with state-of-the-art techniques (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). With the graphs, we can obtain the similarity between two terms without their hypernyms being available. first observation motivates us to the supporting sentences from other terms as auxiliary evidence of the term. The second observation that is brought with the state-of-the-art term similarity graphs (in addition to the term-label information discovered with the patterns of Table 1). i=� 1163 Our evidence propagation algorithm contains phases. In phase I, some supporting constructed for a term from the supporting sentences of its neighbors in the similarity graph. Then we calculate the label scores for terms based on their (pseudo and real) supporting sentences. For every supporting sentence similar term the term add a pseudo sentence with the gain score, = y - - G [0,1] the propagation factor, and the term similarity function taking values in [0, 1]. The formula reasonably assumes that the gain score of the pseudo supporting sentence depends on the gain score of the original real supporting sentence, the similarity between the two terms, and the propagation factor. The nonlinear evidence combination formulas in the previous subsection are adopted to combine the evidence of pseudo supporting sentences. Term similarity graphs can be obtained by distributional similarity or patterns (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). We call first type of graph the second type DS approaches are based on the distributional hypothesis (Harris, 1985), which says that terms appearing in analogous contexts tend to be similar. In a DS approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to a text collection, with the hypothesis that the terms extracted by applying each of the patterns to a specific piece of text tend to be similar. Two categories of patterns have been studied in the literature (Heast 1992; Pasca 2004; Kozareva et al., 2008; Zhang et al., 2009): sentence lexical patterns, and HTML tag patterns. An example of sentence patterns is {, T}*{,} (and|or) HTML tag patterns include HTML tables, drop-down lists, and other tag repeat patterns. In this paper, we generate the DS and PB graphs by adopting the best-performed methods studied in (Shi et al., 2010). We will compare, by experiments, the propagation performance of utilizing the two categories of graphs, and also investigate the performance of utilizing both graphs for evidence propagation. 6 Experiments setup adopt a publicly available dataset in experiments: This is a very large dataset collected by Carnegie Mellon University in early 2009 and has been used by several tracks of Text Retrieval Conference The whole dataset consists of 1.04 billion web pages in ten languages while only those in English, about 500 million pages, are used in our experiments. The reason for selecting such a dataset is twofold: First, it is a corpus large enough for conducting webscale experiments and getting meaningful results. Second, since it is publicly available, it is possible for other researchers to reproduce the experiments in this paper. sets are evaluated by using sets of selected terms: and For every term in the term sets, each approach generates a list of hypernym labels, which are manually judged by human annotators. Wiki200 is constructed by first randomly selecting 400 Wiktitles as our candidate terms, with the probof a title selected to be + where is the frequency of our data corpus. The reason of adopting such a probability formula is to balance popular terms and rare ones in our term set. Then 200 terms are manually selected from the 400 candidate terms, with the principle of maximizing the diversity of terms in terms of length (i.e., number of words) and type (person, location, organization, software, movie, song, animal, plant, etc.). Wiki200 is further divided into subsets: containing respectively the 100 high-frequency and lowfrequency terms. Ext100 is built by first selecting terms at random from the term-label graph generated by the baseline approach (Formula 3.1), then manually selecting 100 terms. Some sample terms in the term sets are listed in Table 3.</abstract>
<note confidence="0.835221666666667">4http://boston.lti.cs.cmu.edu/Data/clueweb09/ 5http://trec.nist.gov/ 6http://www.wikipedia.org/</note>
<date confidence="0.769864">1164</date>
<title confidence="0.673114">Term Set Sample Terms</title>
<address confidence="0.792632">Wiki200 Canon EOS 400D, Disease management, El Sal-vador, Excellus Blue Cross Blue Shield, F33, Glasstron, Indium, Khandala, Kung Fu, Lake Greenwood, Le Gris, Liriope, Lionel Barrymore, Milk, Mount Alto, Northern Wei, Pink Lady, Shawshank, The Dog Island, White flight, World A2B, Antique gold, GPTEngine, Jinjiang Inn, Ext100 Moyea SWF to Apple TV Converter, Nanny ser-vice, Outdoor living, Plasmid DNA, Popon, Spam Taylor Ho Bynum, Villa</address>
<abstract confidence="0.990320714285714">Table 3. Sample terms in our term sets each term in the term set, the top-5 results (i.e., hypernym labels) of various methods are mixed and judged by human annotators. Each annotator assigns each result item a “Good”, “Fair” or “Bad”. annotators do not know the method by which a result item is generated. Six annotators participated in the labeling with a rough speed of 15 minutes per term. We also encourage the annotators to add new good results which are not discovered by any method. The term sets and their corresponding user annotations are available for download at the following links (dataset ID=data.queryset.semcat01):</abstract>
<web confidence="0.9956135">http://research.microsoft.com/en-us/projects/needleseek/ http://needleseek.msra.cn/datasets/</web>
<abstract confidence="0.966282111940298">adopt the following metrics to evaluate the hypernym list of a term generated by each method. The evaluation score on a term set is the average over all the terms. The percentage of relevant (good fair) labels in the (labels judged as “Fair” are counted as 0.5) The ratio of relevant labels in the topto the total number of relevant labels Precision@R where the total number of labels judged as “Good” average precision The average of precision values at the positions of all good or fair results Before annotation and evaluation, the hypernym list generated by each method for each term is preto remove Two hyperare called if they share the head word (e.g., For duplicate hypernyms, only the first (i.e., the highest ranked one) in the list is kept. The goal with such a preprocessing step is to partially consider results diversity in evaluation and to make a more meaningful comparison among different Consider two hypernym lists for List-1: restaurant; chain restaurant; worldwide chain franchise; restaurant List-2: restaurant; franchise; transportation; company; fast food... There are more detailed hypernyms in the first about a restaurant or a franchise; while the second list covers a broader range of meanings for the term. It is hard to say which is better (without considering the upper-layer applications). With this preprocessing step, we keep our focus on short hypernyms rather than detailed ones. Term Set Method MAP R-Prec P@1 P@5 Wiki200 Linear 0.357 0.376 0.783 0.547 Log 0.371 0.384 0.803 0.561 3.92% 2.13% 2.55% 2.56% PNorm 0.372 0.384 0.800 0.562 4.20% 2.13% 2.17% 2.74% Wiki100H Linear 0.363 0.382 0.805 0.627 Log 0.393 0.402 0.845 0.660 8.26% 5.24% 4.97% 5.26% PNorm 0.395 0.403 0.840 0.662 8.82% 5.50% 4.35% 5.28% Table 4. Performance comparison among various evidence fusion methods (Term sets: Wiki200 and for PNorm) results We first compare the evaluation results of different evidence fusion methods mentioned in Section 4.1. Table 4, that Formula 3.1 is used calculate label scores, whereas represent our nonlinear approach with Formulas 4.11 and 4.12 being utilized. The performance improvement numbers shown in the table are based on the linear version; and the upward pointing arrows indicate relative percentage improvement over the baseline. From the table, we can see that the nonlinear methods outperform the linear ones on the Wiki200 term set. It is interesting to note that the performance improvement is more significant on Wiki100H, the set of high frequency terms. By examining the labels and supporting sentences for the terms in each term set, we find that for many low-frequency terms (in Wiki100L), there are only a few supporting sentences (corresponding 1165 to one or two patterns). So the scores computed by various fusion algorithms tend to be similar. In contrast, more supporting sentences can be discovered for high-frequency terms. Much information is contained in the sentences about the hypernyms of the high-frequency terms, but the linear function of Formula 3.1 fails to make effective use of it. The two nonlinear methods achieve better performance by appropriately modeling the dependency between supporting sentences and computing the log-probability gain in a better way. The comparison of the linear and nonlinear methods on the Ext100 term set is shown in Table 5. Please note that the terms in Ext100 do not appear in Wikipedia titles. Thanks to the scale of the data corpus we are using, even the baseline approach achieves reasonably good performance. Please note that the terms (refer to Table 3) we are are than those adopted for tion in many existing papers. Again, the results quality is improved with the nonlinear methods, although the performance improvement is not big due to the reason that most terms in Ext100 are rare. Please note that the recall (R@1, R@5) in this paper is pseudo-recall, i.e., we treat the number of (Good or Fair) results as the total number of relevant ones. Method MAP R-Prec P@1 P@5 R@1 R@5 Linear 0.384 0.429 0.665 0.472 0.116 0.385 0.395 0.429 0.715 0.472 0.125 0.385 Log 0.390 0.429 0.700 0.472 0.120 0.385 PNorm Table 5. Performance comparison among various fusion methods (Term set: Ext100; for PNorm) parameter the PNorm method is related to the degree of correlations among supporting sentences. The linear method of Formula 3.1 correto the special case of while represents the case that other supporting sentences are fully correlated to the supporting sentence with the maximal log-probability gain. Figure 1 shows that, for most of the term sets, the best performance is for 4.0]. The reason may be that the sentence correlations are better estimated with in this range. 1. Performance curves of PNorm with ferent parameter values (Measure: MAP) The experimental results of evidence propagation are shown in Table 6. The methods for comparison are, The linear function without propagation. Nonlinear evidence fusion (PNorm with without propagation. Linear propagation, i.e., the linear function is used to combine the evidence of pseudo supporting sentences. Nonlinear propagation where PNorm is used to combine the pseudo supporting sentences. The nonlinear method is used to combine both supporting sentences and pseudo supporting sentences.</abstract>
<note confidence="0.6439325">Method MAP R-Prec P@1 P@5 R@5 Base 0.357 0.376 0.783 0.547 0.317</note>
<phone confidence="0.724307">0.372 0.384 0.800 0.562 0.325</phone>
<email confidence="0.803755">NL</email>
<phone confidence="0.752824">0.357 0.376 0.783 0.547 0.317</phone>
<email confidence="0.573194">LP</email>
<phone confidence="0.54605">0.396 0.418 0.785 0.605 0.357</phone>
<email confidence="0.510425">NLP</email>
<phone confidence="0.49248">0.447 0.461 0.840 0.667 0.404</phone>
<abstract confidence="0.959404888888889">NL+NLP Table 6. Evidence propagation results (Term Wiki200; Similarity graph: PB; Nonlinear formula: PNorm) In this paper, we generate the DS (distributional similarity) and PB (pattern-based) graphs by adopting the best-performed methods studied in (Shi et al., 2010). The performance improvement numbers (indicated by the upward pointing arrows) shown in tables 6~9 are relative percentage improvement 1166 the (i.e., linear function withpropagation). The values of parameter set to maximize the MAP values. Several observations can be made from Table 6. First, no performance improvement can be obtained with the linear propagation method (LP), while the nonlinear propagation algorithm (NLP) works quite well in improving both precision and recall. The results demonstrate the high correlation between pseudo supporting sentences and the great potential of using term similarity to improve hypernymy extraction. The second observation is that the NL+NLP approach achieves a much larger performance improvement than NL and NLP. Similar results (omitted due to space limitation) can be observed on the Ext100 term set.</abstract>
<note confidence="0.775337111111111">Method MAP R-Prec P@1 P@5 R@5 Base 0.357 0.376 0.783 0.547 0.317 NL+NLP 0.415 0.439 0.830 0.633 0.379 (PB) NL+NLP 0.456 0.469 0.843 0.673 0.406 (DS) NL+NLP 0.473 0.487 0.860 0.700 0.434 (PB+DS) Table 7. Combination of PB and DS graphs propagation (Term set: Wiki200; ear formula: Log) Method MAP R-Prec P@1 P@5 R@5 Base 0.351 0.370 0.760 0.467 0.317 NL+NLP 0.411 0.448 0.770 0.564 0.401 (PB) NL+NLP 0.469 0.490 0.815 0.622 0.438 (DS) NL+NLP 0.491 0.513 0.860 0.654 0.479</note>
<abstract confidence="0.85887042">(PB+DS) Table 8. Combination of PB and DS graphs evidence propagation (Term set: Wiki100L) Now let us study whether it is possible to combine the PB and DS graphs to obtain better results. As shown in Tables 7, 8, and 9 (for term sets Wiki200, Wiki100L, and Ext100 respectively, usthe for fusion and propagation), utilizing both graphs really yields additional performance gains. We explain this by the fact that the information in the two term similarity graphs tends to be complimentary. The performance improvement over Wiki100L is especially remarkable. This is reasonable because rare terms do not have adequate information in their supporting sentences due to data sparseness. As a result, they benefit the most from the pseudo supporting sentences propagated with the similarity graphs. Method MAP R-Prec P@1 P@5 R@5 Base 0.384 0.429 0.665 0.472 0.385 NL+NLP 0.454 0.479 0.745 0.550 0.456 (PB) NL+NLP 0.404 0.441 0.720 0.486 0.402 (DS) NL+NLP(P 0.483 0.518 0.760 0.586 0.492 B+DS) Table 9. Combination of PB and DS graphs evidence propagation (Term set: Ext100) 7 Conclusion We demonstrated that the way of aggregating supporting sentences has considerable impact on results quality of the hyponym extraction task using lexico-syntactic patterns, and the widely-used counting method is not optimal. We applied a series of nonlinear evidence fusion formulas to the problem and saw noticeable performance improvement. The data quality is improved further with the combination of nonlinear evidence fusion and evidence propagation. We also introduced a new evaluation corpus with annotated hypernym labels for 300 terms, which were shared with the research community. Acknowledgments We would like to thank Matt Callcut for reading through the paper. Thanks to the annotators for their efforts in judging the hypernym labels. Thanks to Yueguo Chen, Siyu Lei, and the anonymous reviewers for their helpful comments and suggestions. The first author is partially supported by the NSF of China (60903028,61070014), and</abstract>
<keyword confidence="0.6478985">Key Projects in the Tianjin Science and Technology Pillar Program.</keyword>
<date confidence="0.5879">1167</date>
<note confidence="0.5796804">References E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pasca, and A. Soroa. 2009. A Study on Similarity and Relatedness Using Distributional and WordNet-based In Proc. of</note>
<author confidence="0.94802">M Banko</author>
<author confidence="0.94802">M J Cafarella</author>
<author confidence="0.94802">S Soderland</author>
<author confidence="0.94802">M Broadhead</author>
<abstract confidence="0.50676875">and O. Etzioni. 2007. Open Information Extraction the Web. In Proc. of M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang. 2008. WebTables: Exploring the Power of Tables on the Web. In Proceedings of the 34th Conon Very Large Data Bases Auckland, New Zealand. B. Van Durme and M. Pasca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. Twenty-Third AAAI Conference on Artificial Intelligence.</abstract>
<note confidence="0.846259666666667">F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani. 2006. Cluster Generation and Cluster Labelling for Web Snippets: A Fast and Accurate Hierarchical Solution. In Proceedings of the 13th Conference on String Processing and Information Retrieval pages Glasgow, Scotland.</note>
<author confidence="0.556118">Distributional Structure The Philos-</author>
<affiliation confidence="0.74167">ophy of Linguistics. New York: Oxford University</affiliation>
<note confidence="0.819561375">Press. M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Fourteenth International Conference on Computational Linguistics, Nantes, France. Z. Kozareva, E. Riloff, E.H. Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proc. of ACL&apos;2008. P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu and V. Vyas. 2009. Web-Scale Distributional Similarity Entity Set Expansion. P. Pantel and D. Ravichandran. 2004. Automatically Labeling Semantic Classes. In Proc. of the 2004 Human Language Technology Conference (HLT- M. Pasca. 2004. Acquisition of Categorized Named for Web Search. In Proc. of</note>
<author confidence="0.759457">The Role of Queries in Ranking La-</author>
<affiliation confidence="0.779663">beled Instances Extracted from Text. In Proc. of</affiliation>
<address confidence="0.810597">Beijing, China.</address>
<note confidence="0.886924777777778">S. Shi, B. Lu, Y. Ma, and J.-R. Wen. 2009. Nonlinear Computation. In Proc. of Kong Kong. S. Shi, H. Zhang, X. Yuan, J.-R. Wen. 2010. Corpusbased Semantic Class Mining: Distributional vs. Pat- Approaches. In Proc. of Beijing, China. K. Shinzato and K. Torisawa. 2004. Acquiring Hyponymy Relations from Web Documents. In Proc. of the 2004 Human Language Technology Conference R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning Syntactic Patterns for Automatic Hypernym Discovery. In Proceedings of the 19th Conference on Neural Information Processing Systems. R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic Taxonomy Induction from Heterogenous Evidence. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meet-</note>
<title confidence="0.661406">ing of the Association for Computational Linguistics</title>
<author confidence="0.576229">Experiments in</author>
<title confidence="0.747118666666667">Graph-based Semi-Supervised Learning Methods for Class-Instance Acquisition. In 48th Annual Meeting of the Association for Computational Linguistics</title>
<author confidence="0.8105055">Weakly-Supervised</author>
<abstract confidence="0.273223928571429">Acquisition of Labeled Class Instances using Graph Random Walks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language pages R.C. Wang. W.W. Cohen. Automatic Set Instance Extraction using the Web. In Proc. of the 47th Annual Meeting of the Association for Computational Linpages Singapore. H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. 2009. Employing Topic Models for Pattern-based Semantic Class Discovery. In Proc. of the 47th Annual Meeting of the Association for Computational Linguistics pages Singapore.</abstract>
<intro confidence="0.214533">1168</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pasca</author>
<author>A Soroa</author>
</authors>
<title>A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT’2009.</booktitle>
<contexts>
<context position="19148" citStr="Agirre et al., 2009" startWordPosition="3220" endWordPosition="3223"> to have many supporting sentences of different types. This is a big challenge for rare terms, due to their low frequency in sentences (and even lower frequency in supporting sentences because not all occurrences can be covered by patterns). With evidence propagation, we aim at discovering more supporting sentences for terms (especially rare terms). Evidence propagation is motivated by the following two observations: (I) Similar entities or coordinate terms tend to share some common hypernyms. (II) Large term similarity graphs are able to be built efficiently with state-of-the-art techniques (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). With the graphs, we can obtain the similarity between two terms without their hypernyms being available. The first observation motivates us to “borrow” the supporting sentences from other terms as auxiliary evidence of the term. The second observation means that new information is brought with the state-of-the-art term similarity graphs (in addition to the term-label information discovered with the patterns of Table 1). i=� 1163 Our evidence propagation algorithm contains two phases. In phase I, some pseudo supporting sentences are constructed for a te</context>
<context position="20697" citStr="Agirre et al., 2009" startWordPosition="3474" endWordPosition="3477">IS1) = y - Sim(T,Ti) - G(ATLIS) (5.5) where y G [0,1] is the propagation factor, and Sim(-,-) is the term similarity function taking values in [0, 1]. The formula reasonably assumes that the gain score of the pseudo supporting sentence depends on the gain score of the original real supporting sentence, the similarity between the two terms, and the propagation factor. Phase II: The nonlinear evidence combination formulas in the previous subsection are adopted to combine the evidence of pseudo supporting sentences. Term similarity graphs can be obtained by distributional similarity or patterns (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). We call the first type of graph DS and the second type PB. DS approaches are based on the distributional hypothesis (Harris, 1985), which says that terms appearing in analogous contexts tend to be similar. In a DS approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to </context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pasca, and A. Soroa. 2009. A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches. In Proc. of NAACL-HLT’2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M J Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open Information Extraction from the Web. In</title>
<date>2007</date>
<booktitle>Proc. of IJCAI’2007.</booktitle>
<contexts>
<context position="7763" citStr="Banko, et al., 2007" startWordPosition="1227" endWordPosition="1230">e to add new (high-quality) labels for a term with our evidence propagation method. On the other hand, low quality labels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 Most existing wor</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open Information Extraction from the Web. In Proc. of IJCAI’2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cafarella</author>
<author>A Halevy</author>
<author>D Wang</author>
<author>E Wu</author>
<author>Y Zhang</author>
</authors>
<title>WebTables: Exploring the Power of Tables on the Web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 34th Conference on Very Large Data Bases (VLDB’2008),</booktitle>
<pages>538--549</pages>
<location>Auckland, New Zealand.</location>
<contexts>
<context position="7858" citStr="Cafarella et al., 2008" startWordPosition="1243" endWordPosition="1246">ther hand, low quality labels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 Most existing work tends to utilize small-scale or private corpora, whereas the corpus that we used is publicly </context>
</contexts>
<marker>Cafarella, Halevy, Wang, Wu, Zhang, 2008</marker>
<rawString>M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang. 2008. WebTables: Exploring the Power of Tables on the Web. In Proceedings of the 34th Conference on Very Large Data Bases (VLDB’2008), pages 538–549, Auckland, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Van Durme</author>
<author>M Pasca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>Twenty-Third AAAI Conference on Artificial Intelligence.</booktitle>
<marker>Van Durme, Pasca, 2008</marker>
<rawString>B. Van Durme and M. Pasca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. Twenty-Third AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Geraci</author>
<author>M Pellegrini</author>
<author>M Maggini</author>
<author>F Sebastiani</author>
</authors>
<title>Cluster Generation and Cluster Labelling for Web Snippets: A Fast and Accurate Hierarchical Solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 13th Conference on String Processing and Information Retrieval (SPIRE’2006),</booktitle>
<pages>25--36</pages>
<location>Glasgow, Scotland.</location>
<contexts>
<context position="7927" citStr="Geraci et al., 2006" startWordPosition="1255" endWordPosition="1258">d are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 Most existing work tends to utilize small-scale or private corpora, whereas the corpus that we used is publicly available and much larger than most of the existing work. We publishe</context>
</contexts>
<marker>Geraci, Pellegrini, Maggini, Sebastiani, 2006</marker>
<rawString>F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani. 2006. Cluster Generation and Cluster Labelling for Web Snippets: A Fast and Accurate Hierarchical Solution. In Proceedings of the 13th Conference on String Processing and Information Retrieval (SPIRE’2006), pages 25–36, Glasgow, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>Distributional Structure. The Philosophy of Linguistics.</title>
<date>1985</date>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="20869" citStr="Harris, 1985" startWordPosition="3508" endWordPosition="3509">mes that the gain score of the pseudo supporting sentence depends on the gain score of the original real supporting sentence, the similarity between the two terms, and the propagation factor. Phase II: The nonlinear evidence combination formulas in the previous subsection are adopted to combine the evidence of pseudo supporting sentences. Term similarity graphs can be obtained by distributional similarity or patterns (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). We call the first type of graph DS and the second type PB. DS approaches are based on the distributional hypothesis (Harris, 1985), which says that terms appearing in analogous contexts tend to be similar. In a DS approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to a text collection, with the hypothesis that the terms extracted by applying each of the patterns to a specific piece of text tend to be similar. Two categories of patterns </context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Z. S. Harris. 1985. Distributional Structure. The Philosophy of Linguistics. New York: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Fourteenth International Conference on Computational Linguistics,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="1296" citStr="Hearst 1992" startWordPosition="191" endWordPosition="192">nducted on 500 million web pages and hypernym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision. 1 Introduction1 An important task in text mining is the automatic extraction of entities and their lexical relations; this has wide applications in natural language processing and web search. This paper focuses on mining the hyponymy (or is-a) relation from largescale, open-domain web documents. From the viewpoint of entity classification, the problem is to automatically assign fine-grained class labels to terms. There have been a number of approaches (Hearst 1992; Pantel &amp; Ravichandran 2004; Snow et al., 2005; Durme &amp; Pasca, 2008; Talukdar et al., 2008) to address the problem. These methods typically exploited manually-designed or automatical* This work was performed when Fan Zhang and Shuqi Sun were interns at Microsoft Research Asia ly-learned patterns (e.g., “NP such as NP”, “NP like NP”, “NP is a NP”). Although some degree of success has been achieved with these efforts, the results are still far from perfect, in terms of both recall and precision. As will be demonstrated in this paper, even by processing a large corpus of 500 million web pages wi</context>
<context position="7675" citStr="Hearst 1992" startWordPosition="1213" endWordPosition="1214">oo many clusters (otherwise the pair will be discarded). In contrast, we are able to add new (high-quality) labels for a term with our evidence propagation method. On the other hand, low quality labels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.micro</context>
<context position="9722" citStr="Hearst, 1992" startWordPosition="1524" endWordPosition="1525">lem addressed in this paper is corpusbased is-a relation mining: extracting hypernyms (as labels) for entities from a large-scale, opendomain document corpus. The desired output is a mapping from terms to their corresponding hypernyms, which can naturally be represented as a weighted bipartite graph (term-label graph). Typically we are only interested in top labels of a term in the graph. Following existing efforts, we adopt patternmatching as a basic way of extracting hypernymy/hyponymy relations. Two types of patterns (refer to Table 1) are employed, including the popular “Hearst patterns” (Hearst, 1992) and the IsA patterns which are exploited less frequently in existing hyponym mining efforts. One or more termlabel pairs can be extracted if a pattern matches a sentence. In the baseline approach, the weight of an edge T4L (from term T to hypernym label L) in the term-label graph is computed as, w(T4L) = m - IDF(L) = m - log l+ l+n (L) (3.1) where m is the number of times the pair (T, L) is extracted from the corpus, DF(L) is the number of in-links of L in the graph, N is total number of terms in the graph, and IDF means the “inverse document frequency”. A term can only keep its top-k neighbo</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Fourteenth International Conference on Computational Linguistics, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E H Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.</title>
<date>2008</date>
<booktitle>In Proc. of ACL&apos;2008.</booktitle>
<contexts>
<context position="7950" citStr="Kozareva et al., 2008" startWordPosition="1259" endWordPosition="1262">abel propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 Most existing work tends to utilize small-scale or private corpora, whereas the corpus that we used is publicly available and much larger than most of the existing work. We published our term sets (refer </context>
<context position="21551" citStr="Kozareva et al., 2008" startWordPosition="3623" endWordPosition="3626">to be similar. In a DS approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to a text collection, with the hypothesis that the terms extracted by applying each of the patterns to a specific piece of text tend to be similar. Two categories of patterns have been studied in the literature (Heast 1992; Pasca 2004; Kozareva et al., 2008; Zhang et al., 2009): sentence lexical patterns, and HTML tag patterns. An example of sentence lexical patterns is “T {, T}*{,} (and|or) T”. HTML tag patterns include HTML tables, drop-down lists, and other tag repeat patterns. In this paper, we generate the DS and PB graphs by adopting the best-performed methods studied in (Shi et al., 2010). We will compare, by experiments, the propagation performance of utilizing the two categories of graphs, and also investigate the performance of utilizing both graphs for evidence propagation. 6 Experiments 6.1 Experimental setup Corpus We adopt a public</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, E.H. Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proc. of ACL&apos;2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>E Crestan</author>
<author>A Borkovsky</author>
<author>A-M Popescu</author>
<author>V Vyas</author>
</authors>
<date>2009</date>
<booktitle>Web-Scale Distributional Similarity and Entity Set Expansion. EMNLP’2009.</booktitle>
<contexts>
<context position="19169" citStr="Pantel et al., 2009" startWordPosition="3224" endWordPosition="3227">ing sentences of different types. This is a big challenge for rare terms, due to their low frequency in sentences (and even lower frequency in supporting sentences because not all occurrences can be covered by patterns). With evidence propagation, we aim at discovering more supporting sentences for terms (especially rare terms). Evidence propagation is motivated by the following two observations: (I) Similar entities or coordinate terms tend to share some common hypernyms. (II) Large term similarity graphs are able to be built efficiently with state-of-the-art techniques (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). With the graphs, we can obtain the similarity between two terms without their hypernyms being available. The first observation motivates us to “borrow” the supporting sentences from other terms as auxiliary evidence of the term. The second observation means that new information is brought with the state-of-the-art term similarity graphs (in addition to the term-label information discovered with the patterns of Table 1). i=� 1163 Our evidence propagation algorithm contains two phases. In phase I, some pseudo supporting sentences are constructed for a term from the supportin</context>
<context position="20718" citStr="Pantel et al., 2009" startWordPosition="3478" endWordPosition="3481">- G(ATLIS) (5.5) where y G [0,1] is the propagation factor, and Sim(-,-) is the term similarity function taking values in [0, 1]. The formula reasonably assumes that the gain score of the pseudo supporting sentence depends on the gain score of the original real supporting sentence, the similarity between the two terms, and the propagation factor. Phase II: The nonlinear evidence combination formulas in the previous subsection are adopted to combine the evidence of pseudo supporting sentences. Term similarity graphs can be obtained by distributional similarity or patterns (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). We call the first type of graph DS and the second type PB. DS approaches are based on the distributional hypothesis (Harris, 1985), which says that terms appearing in analogous contexts tend to be similar. In a DS approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to a text collection, wi</context>
</contexts>
<marker>Pantel, Crestan, Borkovsky, Popescu, Vyas, 2009</marker>
<rawString>P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu and V. Vyas. 2009. Web-Scale Distributional Similarity and Entity Set Expansion. EMNLP’2009. Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
</authors>
<title>Automatically Labeling Semantic Classes.</title>
<date>2004</date>
<booktitle>In Proc. of the 2004 Human Language Technology Conference (HLTNAACL’2004),</booktitle>
<pages>321--328</pages>
<contexts>
<context position="1324" citStr="Pantel &amp; Ravichandran 2004" startWordPosition="193" endWordPosition="196">0 million web pages and hypernym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision. 1 Introduction1 An important task in text mining is the automatic extraction of entities and their lexical relations; this has wide applications in natural language processing and web search. This paper focuses on mining the hyponymy (or is-a) relation from largescale, open-domain web documents. From the viewpoint of entity classification, the problem is to automatically assign fine-grained class labels to terms. There have been a number of approaches (Hearst 1992; Pantel &amp; Ravichandran 2004; Snow et al., 2005; Durme &amp; Pasca, 2008; Talukdar et al., 2008) to address the problem. These methods typically exploited manually-designed or automatical* This work was performed when Fan Zhang and Shuqi Sun were interns at Microsoft Research Asia ly-learned patterns (e.g., “NP such as NP”, “NP like NP”, “NP is a NP”). Although some degree of success has been achieved with these efforts, the results are still far from perfect, in terms of both recall and precision. As will be demonstrated in this paper, even by processing a large corpus of 500 million web pages with the most popular patterns</context>
<context position="7704" citStr="Pantel &amp; Ravichandran, 2004" startWordPosition="1215" endWordPosition="1218">ers (otherwise the pair will be discarded). In contrast, we are able to add new (high-quality) labels for a term with our evidence propagation method. On the other hand, low quality labels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needl</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>P. Pantel and D. Ravichandran. 2004. Automatically Labeling Semantic Classes. In Proc. of the 2004 Human Language Technology Conference (HLTNAACL’2004), 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
</authors>
<title>Acquisition of Categorized Named Entities for Web Search.</title>
<date>2004</date>
<booktitle>In Proc. of CIKM’2004.</booktitle>
<contexts>
<context position="21528" citStr="Pasca 2004" startWordPosition="3621" endWordPosition="3622">ntexts tend to be similar. In a DS approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to a text collection, with the hypothesis that the terms extracted by applying each of the patterns to a specific piece of text tend to be similar. Two categories of patterns have been studied in the literature (Heast 1992; Pasca 2004; Kozareva et al., 2008; Zhang et al., 2009): sentence lexical patterns, and HTML tag patterns. An example of sentence lexical patterns is “T {, T}*{,} (and|or) T”. HTML tag patterns include HTML tables, drop-down lists, and other tag repeat patterns. In this paper, we generate the DS and PB graphs by adopting the best-performed methods studied in (Shi et al., 2010). We will compare, by experiments, the propagation performance of utilizing the two categories of graphs, and also investigate the performance of utilizing both graphs for evidence propagation. 6 Experiments 6.1 Experimental setup C</context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>M. Pasca. 2004. Acquisition of Categorized Named Entities for Web Search. In Proc. of CIKM’2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
</authors>
<title>The Role of Queries in Ranking Labeled Instances Extracted from Text.</title>
<date>2010</date>
<booktitle>In Proc. of COLING’2010,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="8000" citStr="Pasca 2010" startWordPosition="1270" endWordPosition="1271">ukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 Most existing work tends to utilize small-scale or private corpora, whereas the corpus that we used is publicly available and much larger than most of the existing work. We published our term sets (refer to Section 6.1) and their corresponding user judgm</context>
</contexts>
<marker>Pasca, 2010</marker>
<rawString>M. Pasca. 2010. The Role of Queries in Ranking Labeled Instances Extracted from Text. In Proc. of COLING’2010, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shi</author>
<author>B Lu</author>
<author>Y Ma</author>
<author>J-R Wen</author>
</authors>
<title>Nonlinear Static-Rank Computation.</title>
<date>2009</date>
<booktitle>In Proc. of CIKM’2009,</booktitle>
<location>Kong Kong.</location>
<contexts>
<context position="10940" citStr="Shi et al., 2009" startWordPosition="1740" endWordPosition="1743">s (according to the edge weight) in the graph as its final labels. Our pattern matching algorithm implemented in this paper uses part-of-speech (POS) tagging information, without adopting a parser or a chunker. The noun phrase boundaries (for terms and labels) are determined by a manually designed POS tag list. 4 Probabilistic Label-Scoring Model Here we model the hyponymy extraction problem from the probability theory point of view, aiming at estimating the score of a term-label pair (i.e., the score of a label w.r.t. a term) with probabilistic evidence combination. The model was studied in (Shi et al., 2009) to combine the page in-link evidence in building a nonlinear static-rank computation algorithm. We represent the score of a term-label pair by the probability of the label being a correct hypernym of the term, and define the following events, AT,L: Label L is a hypernym of term T (the abbreviated form A is used in this paper unless it is ambiguous). Ei: The observation that (T, L) is extracted from a sentence Si via pattern matching (i.e., Si is a supporting sentence of the pair). Assuming that we already know m supporting sentences (S1~Sm), our problem is to compute P(A|E1,E2,..,Em), the pos</context>
<context position="15660" citStr="Shi et al. (2009)" startWordPosition="2622" endWordPosition="2625"> by 4.7, we have, ( ) ( ) ( ) (4.8) This formula indicates that when the supporting sentences are positively correlated, the posterior score of label L w.r.t. term T (given both the sentences) is smaller than the sum of the gains caused by one sentence only. In the extreme case that sentence S2 fully depends on E1 (i.e. P(E2|E1)=1), it is easy to prove that ( ) ( ) It is reasonable, since event E2 does not bring in more information than E1. Formula 4.8 cannot be used directly for computing the posterior gain. What we really need is a function h satisfying ( ) ( ( ) ( )) (4.9) and ( ) ∑ (4.10) Shi et al. (2009) discussed other constraints to h and suggested the following nonlinear functions, ) (4.11) ( ) ( ∑ ( ) 3 RA is estimated from the labels judged as “Good”; whereas the estimation of R is from all judged labels. ( ) ( ) ( ) ( ) (4.7) 1162 ( ) √∑ (p&gt;1) (4.12) In the next section, we use the above two h functions as basic building blocks to compute label scores for terms. 5 Our Approach Multiple types of patterns (Table 1) can be adopted to extract term-label pairs. For two supporting sentences the correlation between them may depend on whether they correspond to the same pattern. In Section 5.1,</context>
</contexts>
<marker>Shi, Lu, Ma, Wen, 2009</marker>
<rawString>S. Shi, B. Lu, Y. Ma, and J.-R. Wen. 2009. Nonlinear Static-Rank Computation. In Proc. of CIKM’2009, Kong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shi</author>
<author>H Zhang</author>
<author>X Yuan</author>
<author>J-R Wen</author>
</authors>
<title>Corpusbased Semantic Class Mining: Distributional vs. Pattern-Based Approaches.</title>
<date>2010</date>
<booktitle>In Proc. of COLING’2010,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="19188" citStr="Shi et al., 2010" startWordPosition="3228" endWordPosition="3231">erent types. This is a big challenge for rare terms, due to their low frequency in sentences (and even lower frequency in supporting sentences because not all occurrences can be covered by patterns). With evidence propagation, we aim at discovering more supporting sentences for terms (especially rare terms). Evidence propagation is motivated by the following two observations: (I) Similar entities or coordinate terms tend to share some common hypernyms. (II) Large term similarity graphs are able to be built efficiently with state-of-the-art techniques (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). With the graphs, we can obtain the similarity between two terms without their hypernyms being available. The first observation motivates us to “borrow” the supporting sentences from other terms as auxiliary evidence of the term. The second observation means that new information is brought with the state-of-the-art term similarity graphs (in addition to the term-label information discovered with the patterns of Table 1). i=� 1163 Our evidence propagation algorithm contains two phases. In phase I, some pseudo supporting sentences are constructed for a term from the supporting sentences of its </context>
<context position="20737" citStr="Shi et al., 2010" startWordPosition="3482" endWordPosition="3485">e y G [0,1] is the propagation factor, and Sim(-,-) is the term similarity function taking values in [0, 1]. The formula reasonably assumes that the gain score of the pseudo supporting sentence depends on the gain score of the original real supporting sentence, the similarity between the two terms, and the propagation factor. Phase II: The nonlinear evidence combination formulas in the previous subsection are adopted to combine the evidence of pseudo supporting sentences. Term similarity graphs can be obtained by distributional similarity or patterns (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). We call the first type of graph DS and the second type PB. DS approaches are based on the distributional hypothesis (Harris, 1985), which says that terms appearing in analogous contexts tend to be similar. In a DS approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to a text collection, with the hypothesis t</context>
<context position="31716" citStr="Shi et al., 2010" startWordPosition="5253" endWordPosition="5256">o supporting sentences. Method MAP R-Prec P@1 P@5 R@5 Base 0.357 0.376 0.783 0.547 0.317 0.372 0.384 0.800 0.562 0.325 NL T4.20% T2.13% T2.17% T2.74% T2.52% 0.357 0.376 0.783 0.547 0.317 LP T0% T0% T0% T0% T0% 0.396 0.418 0.785 0.605 0.357 NLP T10.9% T11.2% T0.26% T10.6% T12.6% 0.447 0.461 0.840 0.667 0.404 NL+NLP T25.2% T22.6% T7.28% T21.9% T27.4% Table 6. Evidence propagation results (Term set: Wiki200; Similarity graph: PB; Nonlinear formula: PNorm) In this paper, we generate the DS (distributional similarity) and PB (pattern-based) graphs by adopting the best-performed methods studied in (Shi et al., 2010). The performance improvement numbers (indicated by the upward pointing arrows) shown in tables 6~9 are relative percentage improvement 1166 over the base approach (i.e., linear function without propagation). The values of parameter y are set to maximize the MAP values. Several observations can be made from Table 6. First, no performance improvement can be obtained with the linear propagation method (LP), while the nonlinear propagation algorithm (NLP) works quite well in improving both precision and recall. The results demonstrate the high correlation between pseudo supporting sentences and t</context>
</contexts>
<marker>Shi, Zhang, Yuan, Wen, 2010</marker>
<rawString>S. Shi, H. Zhang, X. Yuan, J.-R. Wen. 2010. Corpusbased Semantic Class Mining: Distributional vs. Pattern-Based Approaches. In Proc. of COLING’2010, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shinzato</author>
<author>K Torisawa</author>
</authors>
<title>Acquiring Hyponymy Relations from Web Documents.</title>
<date>2004</date>
<booktitle>In Proc. of the 2004 Human Language Technology Conference (HLT-NAACL’2004).</booktitle>
<contexts>
<context position="7886" citStr="Shinzato &amp; Torisawa, 2004" startWordPosition="1247" endWordPosition="1251">abels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 Most existing work tends to utilize small-scale or private corpora, whereas the corpus that we used is publicly available and much larger th</context>
</contexts>
<marker>Shinzato, Torisawa, 2004</marker>
<rawString>K. Shinzato and K. Torisawa. 2004. Acquiring Hyponymy Relations from Web Documents. In Proc. of the 2004 Human Language Technology Conference (HLT-NAACL’2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Learning Syntactic Patterns for Automatic Hypernym Discovery.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th Conference on Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1343" citStr="Snow et al., 2005" startWordPosition="197" endWordPosition="200">rnym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision. 1 Introduction1 An important task in text mining is the automatic extraction of entities and their lexical relations; this has wide applications in natural language processing and web search. This paper focuses on mining the hyponymy (or is-a) relation from largescale, open-domain web documents. From the viewpoint of entity classification, the problem is to automatically assign fine-grained class labels to terms. There have been a number of approaches (Hearst 1992; Pantel &amp; Ravichandran 2004; Snow et al., 2005; Durme &amp; Pasca, 2008; Talukdar et al., 2008) to address the problem. These methods typically exploited manually-designed or automatical* This work was performed when Fan Zhang and Shuqi Sun were interns at Microsoft Research Asia ly-learned patterns (e.g., “NP such as NP”, “NP like NP”, “NP is a NP”). Although some degree of success has been achieved with these efforts, the results are still far from perfect, in terms of both recall and precision. As will be demonstrated in this paper, even by processing a large corpus of 500 million web pages with the most popular patterns, we are not able t</context>
<context position="6607" citStr="Snow et al. (2005)" startWordPosition="1032" endWordPosition="1035">bility of a term having its private labels (considprobabilistic evidence combination model that we ering the case that one meaning of a term is not exploited is introduced in Section 4. Our main ap- covered by the input semantic classes). In contrast proach is illustrated in Section 5. Section 6 shows to their method, our label scoring and ranking apour experimental settings and results. Finally, Sec- proach is applied to every single term rather than a tion 7 concludes this paper. semantic class. In addition, we also compute label scores in a nonlinear way, which improves results quality. In Snow et al. (2005), a supervised approach was proposed to improve hypernym classification using coordinate terms. In comparison, our approach is unsupervised. Durme &amp; Pasca (2008) cleaned the set of instance-label pairs with a TF*IDF like method, by exploiting clusters of semantically related phrases. The core idea is to keep a term-label pair (T, L) only if the number of terms having the label L in the term T’s cluster is above a threshold and if L is not the label of too many clusters (otherwise the pair will be discarded). In contrast, we are able to add new (high-quality) labels for a term with our evidence</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning Syntactic Patterns for Automatic Hypernym Discovery. In Proceedings of the 19th Conference on Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Semantic Taxonomy Induction from Heterogenous Evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-06),</booktitle>
<pages>801--808</pages>
<contexts>
<context position="4400" citStr="Snow et al., 2006" startWordPosition="684" endWordPosition="687">9), for combining the page in-link evidence Mount Vesuvius and Kilimanjaro are volcanoes in building a nonlinear static-rank computation and Lhotse is similar to them, but Lhotse is not a algorithm. We applied it to the hyponymy extracvolcano. Therefore we should be very conservative tion problem because the model takes the dependand careful in hypernym propagation. In our prop- ency between supporting sentences into agation algorithm, we first construct some pseudo consideration and the resultant evidence fusion supporting sentences for a term from the support- formulas are quite simple. In (Snow et al., 2006), a ing sentences of its similar terms. Then we calcu- probabilistic model was adopted to combine evilate label scores for terms by performing nonlinear dence from heterogeneous relationships to jointly evidence combination based on the (pseudo and optimize the relationships. The independence of real) supporting sentences. Such a nonlinear prop- evidence was assumed in their model. In compariagation algorithm is demonstrated to perform bet- son, we show that better results will be obtained if ter than linear propagation. the evidence correlation is modeled appropriately. Experimental results o</context>
<context position="7742" citStr="Snow et al., 2006" startWordPosition="1223" endWordPosition="1226">ontrast, we are able to add new (high-quality) labels for a term with our evidence propagation method. On the other hand, low quality labels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic Taxonomy Induction from Heterogenous Evidence. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-06), 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P Talukdar</author>
<author>F Pereira</author>
</authors>
<title>Experiments in Graph-based Semi-Supervised Learning Methods for Class-Instance Acquisition.</title>
<date>2010</date>
<booktitle>In 48th Annual Meeting of the Association for Computational Linguistics (ACL’2010).</booktitle>
<contexts>
<context position="7411" citStr="Talukdar &amp; Pereira, 2010" startWordPosition="1172" endWordPosition="1175">f instance-label pairs with a TF*IDF like method, by exploiting clusters of semantically related phrases. The core idea is to keep a term-label pair (T, L) only if the number of terms having the label L in the term T’s cluster is above a threshold and if L is not the label of too many clusters (otherwise the pair will be discarded). In contrast, we are able to add new (high-quality) labels for a term with our evidence propagation method. On the other hand, low quality labels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query logs (Pasca 2010). Our targe</context>
</contexts>
<marker>Talukdar, Pereira, 2010</marker>
<rawString>P. P. Talukdar and F. Pereira. 2010. Experiments in Graph-based Semi-Supervised Learning Methods for Class-Instance Acquisition. In 48th Annual Meeting of the Association for Computational Linguistics (ACL’2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P Talukdar</author>
<author>J Reisinger</author>
<author>M Pasca</author>
<author>D Ravichandran</author>
<author>R Bhagat</author>
<author>F Pereira</author>
</authors>
<title>Weakly-Supervised Acquisition of Labeled Class Instances using Graph Random Walks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP’2008),</booktitle>
<pages>581--589</pages>
<contexts>
<context position="1388" citStr="Talukdar et al., 2008" startWordPosition="205" endWordPosition="208">performance improvement in terms of P@5, MAP and R-Precision. 1 Introduction1 An important task in text mining is the automatic extraction of entities and their lexical relations; this has wide applications in natural language processing and web search. This paper focuses on mining the hyponymy (or is-a) relation from largescale, open-domain web documents. From the viewpoint of entity classification, the problem is to automatically assign fine-grained class labels to terms. There have been a number of approaches (Hearst 1992; Pantel &amp; Ravichandran 2004; Snow et al., 2005; Durme &amp; Pasca, 2008; Talukdar et al., 2008) to address the problem. These methods typically exploited manually-designed or automatical* This work was performed when Fan Zhang and Shuqi Sun were interns at Microsoft Research Asia ly-learned patterns (e.g., “NP such as NP”, “NP like NP”, “NP is a NP”). Although some degree of success has been achieved with these efforts, the results are still far from perfect, in terms of both recall and precision. As will be demonstrated in this paper, even by processing a large corpus of 500 million web pages with the most popular patterns, we are not able to extract correct labels for many (especially</context>
<context position="7384" citStr="Talukdar et al., 2008" startWordPosition="1168" endWordPosition="1171">2008) cleaned the set of instance-label pairs with a TF*IDF like method, by exploiting clusters of semantically related phrases. The core idea is to keep a term-label pair (T, L) only if the number of terms having the label L in the term T’s cluster is above a threshold and if L is not the label of too many clusters (otherwise the pair will be discarded). In contrast, we are able to add new (high-quality) labels for a term with our evidence propagation method. On the other hand, low quality labels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar &amp; Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel &amp; Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme &amp; Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato &amp; Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang &amp; Cohen, 2009), and query l</context>
</contexts>
<marker>Talukdar, Reisinger, Pasca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran, R. Bhagat, and F. Pereira. 2008. Weakly-Supervised Acquisition of Labeled Class Instances using Graph Random Walks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP’2008), pages 581–589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
</authors>
<title>Automatic Set Instance Extraction using the Web. In</title>
<date></date>
<booktitle>Proc. of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP’2009),</booktitle>
<pages>441--449</pages>
<marker>Cohen, </marker>
<rawString>R.C. Wang. W.W. Cohen. Automatic Set Instance Extraction using the Web. In Proc. of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP’2009), pages 441–449, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>M Zhu</author>
<author>S Shi</author>
<author>J-R Wen</author>
</authors>
<title>Employing Topic Models for Pattern-based Semantic Class Discovery.</title>
<date>2009</date>
<booktitle>In Proc. of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP’2009),</booktitle>
<pages>441--449</pages>
<contexts>
<context position="21572" citStr="Zhang et al., 2009" startWordPosition="3627" endWordPosition="3630">approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to a text collection, with the hypothesis that the terms extracted by applying each of the patterns to a specific piece of text tend to be similar. Two categories of patterns have been studied in the literature (Heast 1992; Pasca 2004; Kozareva et al., 2008; Zhang et al., 2009): sentence lexical patterns, and HTML tag patterns. An example of sentence lexical patterns is “T {, T}*{,} (and|or) T”. HTML tag patterns include HTML tables, drop-down lists, and other tag repeat patterns. In this paper, we generate the DS and PB graphs by adopting the best-performed methods studied in (Shi et al., 2010). We will compare, by experiments, the propagation performance of utilizing the two categories of graphs, and also investigate the performance of utilizing both graphs for evidence propagation. 6 Experiments 6.1 Experimental setup Corpus We adopt a publicly available dataset </context>
</contexts>
<marker>Zhang, Zhu, Shi, Wen, 2009</marker>
<rawString>H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. 2009. Employing Topic Models for Pattern-based Semantic Class Discovery. In Proc. of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP’2009), pages 441–449, Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>