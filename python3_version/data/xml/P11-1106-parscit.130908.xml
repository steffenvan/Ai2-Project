<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9963495">
Integrating surprisal and uncertain-input models in online sentence
comprehension: formal techniques and empirical results
</title>
<author confidence="0.997696">
Roger Levy
</author>
<affiliation confidence="0.998611">
Department of Linguistics
University of California at San Diego
</affiliation>
<address confidence="0.960644">
9500 Gilman Drive # 0108
La Jolla, CA 92093-0108
</address>
<email confidence="0.999248">
rlevy@ucsd.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920166666667">
A system making optimal use of available in-
formation in incremental language compre-
hension might be expected to use linguistic
knowledge together with current input to re-
vise beliefs about previous input. Under some
circumstances, such an error-correction capa-
bility might induce comprehenders to adopt
grammatical analyses that are inconsistent
with the true input. Here we present a for-
mal model of how such input-unfaithful gar-
den paths may be adopted and the difficulty
incurred by their subsequent disconfirmation,
combining a rational noisy-channel model of
syntactic comprehension under uncertain in-
put with the surprisal theory of incremental
processing difficulty. We also present a behav-
ioral experiment confirming the key empirical
predictions of the theory.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999895625">
In most formal theories of human sentence compre-
hension, input recognition and syntactic analysis are
taken to be distinct processes, with the only feed-
back from syntax to recognition being prospective
prediction of likely upcoming input (Jurafsky, 1996;
Narayanan and Jurafsky, 1998, 2002; Hale, 2001,
2006; Levy, 2008a). Yet a system making optimal
use of all available information might be expected
to perform fully joint inference on sentence identity
and structure given perceptual input, using linguistic
knowledge both prospectively and retrospectively in
drawing inferences as to how raw input should be
segmented and recognized as a sequence of linguis-
tic tokens, and about the degree to which each input
token should be trusted during grammatical analysis.
Formal models of such joint inference over uncer-
tain input have been proposed (Levy, 2008b), and
corroborative empirical evidence exists that strong
coherence of current input with a perceptual neigh-
bor of previous input may induce confusion in com-
prehenders as to the identity of that previous input
(Connine et al., 1991; Levy et al., 2009).
In this paper we explore a more dramatic predic-
tion of such an uncertain-input theory: that, when
faced with sufficiently biasing input, comprehen-
ders might under some circumstances adopt a gram-
matical analysis inconsistent with the true raw in-
put comprising a sentence they are presented with,
but consistent with a slightly perturbed version of
the input that has higher prior probability. If this is
the case, then subsequent input strongly disconfirm-
ing this “hallucinated” garden-path analysis might
be expected to induce the same effects as seen in
classic cases of garden-path disambiguation tradi-
tionally studied in the psycholinguistic literature.
We explore this prediction by extending the ratio-
nal uncertain-input model of Levy (2008b), integrat-
ing it with SURPRISAL THEORY (Hale, 2001; Levy,
2008a), which successfully accounts for and quan-
tifies traditional garden-path disambiguation effects;
and by testing predictions of the extended model in a
self-paced reading study. Section 2 reviews surprisal
theory and how it accounts for traditional garden-
path effects. Section 3 provides background infor-
mation on garden-path effects relevant to the current
study, describes how we might hope to reveal com-
prehenders’ use of grammatical knowledge to revise
beliefs about the identity of previous linguistic sur-
</bodyText>
<page confidence="0.947499">
1055
</page>
<note confidence="0.9791105">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1055–1065,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999515083333333">
face input and adopt grammatical analyses incon-
sistent with true input through a controlled experi-
ment, and informally outlines how such belief revi-
sions might arise as a side effect in a general the-
ory of rational comprehension under uncertain in-
put. Section 4 defines and estimates parameters for a
model instantiating the general theory, and describes
the predictions of the model for the experiment de-
scribed in Section 3 (along with the inference proce-
dures required to determine those predictions). Sec-
tion 5 reports the results of the experiment. Section 6
concludes.
</bodyText>
<sectionHeader confidence="0.9904295" genericHeader="method">
2 Garden-path disambiguation under
surprisal
</sectionHeader>
<bodyText confidence="0.999358888888889">
The SURPRISAL THEORY of incremental sentence-
processing difficulty (Hale, 2001; Levy, 2008a)
posits that the cognitive effort required to process a
given word wi of a sentence in its context is given by
the simple information-theoretic measure of the log
of the inverse of the word’s conditional probability
(also called its “surprisal” or “Shannon information
content”) in its intra-sentential context w1,...,i−1 and
extra-sentential context Ctxt:
</bodyText>
<equation confidence="0.979283">
1
Effort(wi) ∝ log
P(wi|w1...i−1, Ctxt)
</equation>
<bodyText confidence="0.9811789">
(In the rest of this paper, we consider isolated-
sentence comprehension and ignore Ctxt.) The the-
ory derives empirical support not only from con-
trolled experiments manipulating grammatical con-
text but also from broad-coverage studies of read-
ing times for naturalistic text (Demberg and Keller,
2008; Boston et al., 2008; Frank, 2009; Roark et al.,
2009), including demonstration that the shape of the
relationship between word probability and reading
time is indeed log-linear (Smith and Levy, 2008).
Surprisal has had considerable success in ac-
counting for one of the best-known phenomena in
psycholinguistics, the GARDEN-PATH SENTENCE
(Frazier, 1979), in which a local ambiguity biases
the comprehender’s incremental syntactic interpre-
tation so strongly that upon encountering disam-
biguating input the correct interpretation can only
be recovered with great effort, if at all. The most
famous example is (1) below (Bever, 1970):
(1) The horse raced past the barn fell.
where the context before the final word is strongly
biased toward an interpretation where raced is the
main verb of the sentence (MV; Figure 1a), the in-
tended interpretation, where raced begins a reduced
relative clause (RR; Figure 1b) and fell is the main
verb, is extremely difficult to recover. Letting Tj
range over the possible incremental syntactic analy-
ses of words w1...6 preceding fell, under surprisal the
conditional probability of the disambiguating con-
tinuation fell can be approximated as
</bodyText>
<equation confidence="0.993576">
�P(fell|w1...6) _ P(fell|Tj, w1...6)P(Tj|w1...6)
j
(I)
</equation>
<bodyText confidence="0.99990655">
For all possible predisambiguation analyses Tj,
either the analysis is disfavored by the context
(P(Tj|w1...6) is low) or the analysis makes the
disambiguating word unlikely (P(fell|Tj,w1...6) is
low). Since every summand in the marginalization
of Equation (I) has a very small term in it, the total
marginal probability is thus small and the surprisal
is high. Hale (2001) demonstrated that surprisal thus
predicts strong garden-pathing effects in the classic
sentence The horse raced past the barn fell on ba-
sis of the overall rarity of reduced relative clauses
alone. More generally, Jurafsky (1996) used a com-
bination of syntactic probabilities (reduced RCs are
rare) and argument-structure probabilities (raced is
usually intransitive) to estimate the probability ratio
of the two analyses of pre-disambiguation context
in Figure 1 as roughly 82:1, putting a lower bound
on the additional surprisal incurred at fell for the
reduced-RC variant over the unreduced variant (The
horse that was raced past the barn fell) of 6.4 bits.1
</bodyText>
<sectionHeader confidence="0.951475" genericHeader="method">
3 Garden-pathing and input uncertainty
</sectionHeader>
<bodyText confidence="0.997803428571429">
We now move on to cases where garden-pathing can
apparently be blocked by only small changes to the
surface input, which we will take as a starting point
for developing an integrated theory of uncertain-
input inference and surprisal. The backdrop is what
is known in the psycholinguistic literature as the
NP/Z ambiguity, exemplified in (2) below:
</bodyText>
<footnote confidence="0.84278">
1We say that this is a “lower bound” because incorporat-
ing even finer-grained information—such as the fact that horse
is a canonical subject for intransitive raced—into the estimate
would almost certainly push the probability ratio even farther in
favor of the main-clause analysis.
</footnote>
<page confidence="0.995036">
1056
</page>
<figure confidence="0.957538">
(b) RR interpretation
</figure>
<figureCaption confidence="0.998912">
Figure 1: Classic garden pathing
</figureCaption>
<listItem confidence="0.408528">
(2) While Mary was mending the socks fell off her lap.
</listItem>
<bodyText confidence="0.9952572">
In incremental comprehension, the phrase the socks
is ambiguous between being the NP object of the
preceding subordinate-clause verb mending versus
being the subject of the main clause (in which
case mending has a Zero object); in sentences like
</bodyText>
<listItem confidence="0.996141368421053">
(2) the initial bias is toward the NP interpreta-
tion. The main-clause verb fell disambiguates, rul-
ing out the initially favored NP analysis. It has
been known since Frazier and Rayner (1982) that
this effect of garden-path disambiguation can be
measured in reading times on the main-clause verb
(see also Mitchell, 1987; Ferreira and Henderson,
1993; Adams et al., 1998; Sturt et al., 1999; Hill
and Murray, 2000; Christianson et al., 2001; van
Gompel and Pickering, 2001; Tabor and Hutchins,
2004; Staub, 2007). Small changes to the context
can have huge effects on comprehenders’ initial in-
terpretations, however. It is unusual for sentence-
initial subordinate clauses not to end with a comma
or some other type of punctuation (searches in the
parsed Brown corpus put the rate at about 18%); em-
pirically it has consistently been found that a comma
eliminates the garden-path effect in NP/Z sentences:
(3) While Mary was mending, the socks fell off her lap.
</listItem>
<bodyText confidence="0.9933891">
Understanding sentences like (3) is intuitively much
easier, and reading times at the disambiguating verb
are reliably lower when compared with (2). Fodor
(2002) summarized the power of this effect suc-
cinctly:
[w]ith a comma after mending, there
would be no syntactic garden path left to
be studied. (Fodor, 2002)
In a surprisal model with clean, veridical input,
Fodor’s conclusion is exactly what is predicted: sep-
arating a verb from its direct object with a comma
effectively never happens in edited, published writ-
ten English, so the conditional probability of the
NP analysis should be close to zero.2 When uncer-
tainty about surface input is introduced, however—
due to visual noise, imperfect memory representa-
tions, and/or beliefs about possible speaker error—
analyses come into play in which some parts of the
true string are treated as if they were absent. In
particular, because the two sentences are perceptual
neighbors, the pre-disambiguation garden-path anal-
ysis of (2) may be entertained in (3).
We can get a tighter handle on the effect of input
uncertainty by extending Levy (2008b)’s analysis of
the expected beliefs of a comprehender about the se-
quence of words constituting an input sentence to
joint inference over both sentence identity and sen-
tence structure. For a true sentence w∗ which yields
perceptual input I, joint inference on sentence iden-
tity w and structure T marginalizing over I yields:
</bodyText>
<equation confidence="0.9988805">
PC(T,w|w*) = J PC (T, w  |I, w*) PT (I  |w*) dI
I
</equation>
<bodyText confidence="0.999884625">
where PT(I|w∗) is the true model of noise (percep-
tual inputs derived from the true sentence) and PC(·)
terms reflect the comprehender’s linguistic knowl-
edge and beliefs about the noise processes interven-
ing between intended sentences and perceptual in-
put. w∗ and w must be conditionally independent
given I since w∗ is not observed by the comprehen-
der, giving us (through Bayes’ Rule):
</bodyText>
<equation confidence="0.9632445">
PT(I|w*) dI
PC(I)
</equation>
<bodyText confidence="0.999906">
For present purposes we constrain the comprehen-
der’s model of noise so that T and I are condition-
ally independent given w, an assumption that can be
relaxed in future work.3 This allows us the further
</bodyText>
<construct confidence="0.7344269">
2A handful of VP -&gt; V , NP ... rules can be found
in the Penn Treebank, but they all involve appositives (It [VP
ran, this apocalyptic beast ... ]), vocatives (You should [VP un-
derstand, Jack, ... ]), cognate objects (She [VP smiled, a smile
without humor]), or indirect speech (I [VP thought, you nasty
brute... ]); none involve true direct objects of the type in (3).
3This assumption is effectively saying that noise processes
are syntax-insensitive, which is clearly sensible for environmen-
tal noise but would need to be relaxed for some types of speaker
error.
</construct>
<figure confidence="0.994345111111111">
S S
NP VP NP VP
PP
...
DT NN
The horse
VBD
raced
DT NN
The horse
RRC
S
NP
...
IN
past
(a) MV interpretation
DT NN
the barn
VP
PP
NP
past DT NN
the barn
VBN
raced
IN
</figure>
<equation confidence="0.742164666666667">
P(T, w|w*) = ��
PC(I|T, w)PC(T, w)
1057
simplification to
(ii)
z } |{
PC(I|w)PT(I|w*) dI
PC(I)
(II)
</equation>
<bodyText confidence="0.999205625">
That is, a comprehender’s average inferences about
sentence identity and structure involve a tradeoff
between (i) the prior probability of a grammati-
cal derivation given a speaker’s linguistic knowl-
edge and (ii) the fidelity of the derivation’s yield to
the true sentence, as measured by a combination of
true noise processes and the comprehender’s beliefs
about those processes.
</bodyText>
<subsectionHeader confidence="0.953194">
3.1 Inducing hallucinated garden paths
</subsectionHeader>
<bodyText confidence="0.970257254901961">
through manipulating prior grammatical
probabilities
Returning to our discussion of the NP/Z ambigu-
ity, the relative ease of comprehending (3) entails
an interpretation in the uncertain-input model that
the cost of infidelity to surface input is sufficient to
prevent comprehenders from deriving strong belief
in a hallucinated garden-path analysis of (3) pre-
disambiguation in which the comma is ignored. At
the same time, the uncertain-input theory predicts
that if we manipulate the balance of prior grammat-
ical probabilities PC(T, w) strongly enough (term
(i) in Equation (II)), it may shift the comprehender’s
beliefs toward a garden-path interpretation. This ob-
servation sets the stage for our experimental manip-
ulation, illustrated below:
(4) As the soldiers marched, toward the tank lurched an
injured enemy combatant.
Example (4) is qualitatively similar to (3), but with
two crucial differences. First, there has been LOCA-
TIVE INVERSION (Bolinger, 1971; Bresnan, 1994)
in the main clause: a locative PP has been fronted
before the verb, and the subject NP is realized
postverbally. Locative inversion is a low-frequency
construction, hence it is crucially disfavored by
the comprehender’s prior over possible grammatical
structures. Second, the subordinate-clause verb is
no longer transitive, as in (3); instead it is intran-
sitive but could itself take the main-clause fronted
PP as a dependent. Taken together, these prop-
erties should shift comprehenders’ posterior infer-
ences given prior grammatical knowledge and pre-
disambiguation input more sharply than in (3) to-
ward the input-unfaithful interpretation in which the
immediately preverbal main-clause constituent (to-
ward the tank in (4)) is interpreted as a dependent of
the subordinate-clause verb, as if the comma were
absent.
If comprehenders do indeed seriously entertain
such interpretations, then we should be able to
find the empirical hallmarks (e.g., elevated reading
times) of garden-path disambiguation at the main-
clause verb lurched, which is incompatible with the
“hallucinated” garden-path interpretation. Empiri-
cally, however, it is important to disentangle these
empirical hallmarks of garden-path disambiguation
from more general disruption that may be induced
by encountering locative inversion itself. We ad-
dress this issue by introducing a control condition
in which a postverbal PP is placed within the subor-
dinate clause:
</bodyText>
<listItem confidence="0.795119">
(5) As the soldiers marched into the bunker, toward the
</listItem>
<bodyText confidence="0.833034727272727">
tank lurched an injured enemy combatant. [+PP]
Crucially, this PP fills a similar thematic role
for the subordinate-clause verb marched as the
main-clause fronted PP would, reducing the ex-
tent to which the comprehender’s prior favors the
input-unfaithful interpretation (that is, the prior ra-
tio
P(marched into the bunker toward the tank|VP) for (5) is
P(marched into the bunker|VP)
much lower than the corresponding prior ratio
P(marchedmtoward the tank|VP) for (4)), while leaving
Parched
locative( inversion present. Finally, to ensure that
sentence length itself does not create a confound
driving any observed processing-time difference, we
cross presence/absence of the subordinate-clause PP
with inversion in the main clause:
a. As the soldiers marched, the tank lurched toward
an injured enemy combatant. [Uninverted,−PP]
b. As the soldiers marched into the bunker, the
tank lurched toward an injured enemy combatant.
[Uninverted,+PP]
</bodyText>
<sectionHeader confidence="0.929007" genericHeader="method">
4 Model instantiation and predictions
</sectionHeader>
<bodyText confidence="0.997809">
To determine the predictions of our uncertain-
input/surprisal model for the above sentence types,
we extracted a small grammar from the parsed
</bodyText>
<table confidence="0.986554961538462">
P(T, w|w*) _ (i)
z } |{
PC(T, w)
Z�
1058
TOP → S . 1.000000
S → INVERTED NP 0.003257
S → SBAR S 0.012289
S → SBAR , S 0.041753
S → NP VP 0.942701
INVERTED → PP VBD 1.000000
SBAR → INSBAR S 1.000000
VP → VBD RB 0.002149
VP → VBD PP 0.202024
VP → VBD NP 0.393660
VP → VBD PP PP 0.028029
VP → VBD RP 0.005731
VP → VBD 0.222441
VP → VBD JJ 0.145966
PP → IN NP 1.000000
NP → DT NN 0.274566
NP → NNS 0.047505
NP → NNP 0.101198
NP → DT NNS 0.045082
NP → PRP 0.412192
NP → NN 0.119456
</table>
<tableCaption confidence="0.999283">
Table 1: A small PCFG (lexical rewrite rules omit-
</tableCaption>
<bodyText confidence="0.870826545454546">
ted) covering the constructions used in (4)–(6), with
probabilities estimated from the parsed Brown cor-
pus.
Brown corpus (Kuˇcera and Francis, 1967; Marcus
et al., 1994), covering sentence-initial subordinate
clause and locative-inversion constructions.4,5 The
non-terminal rewrite rules are shown in Table 1,
along with their probabilities; of terminal rewrite
rules for all words which either appear in the sen-
tences to be parsed or appeared at least five times in
the corpus, with probabilities estimated by relative
frequency.
As we describe in the following two sections, un-
4Rule counts were obtained using tgrep2/Tregex pat-
terns (Rohde, 2005; Levy and Andrew, 2006); the probabilities
given are relative frequency estimates. The patterns used can be
found at http://idiom.ucsd.edu/˜rlevy/papers/
acl2011/tregex_patterns.txt.
5Similar to the case noted in Footnote 2, a small number of
VP -&gt; V , PP ... rules can be found in the parsed Brown
corpus. However, the PPs involved are overwhelmingly (i) set
expressions, such as for example, in essence, and of course, or
(ii) manner or temporal adjuncts. The handful of true loca-
tive PPs (5 in total) are all parentheticals intervening between
the verb and a complement strongly selected by the verb (e.g.,
[VP means, in my country, homosexual]); none fulfill one of the
verb’s thematic requirements.
certain input is represented as a weighted finite-state
automaton (WFSA), allowing us to represent the in-
cremental inferences of the comprehender through
intersection of the input WFSA with the PCFG
above (Bar-Hillel et al., 1964; Nederhof and Satta,
2003, 2008).
</bodyText>
<subsectionHeader confidence="0.980167">
4.1 Uncertain-input representations
</subsectionHeader>
<bodyText confidence="0.99992632">
Levy (2008a) introduced the LEVENSHTEIN-
DISTANCE KERNEL as a model of the average effect
of noise in uncertain-input probabilistic sentence
comprehension; this corresponds to term (ii) in
our Equation (II). This kernel had a single noise
parameter governing scaling of the cost of consid-
ering word substitutions, insertions, and deletions
are considered, with the cost of a word substitution
falling off exponentially with Levenshtein distance
between the true word and the substituted word,
and the cost of word insertion or deletion falling off
exponentially with word length. The distribution
over the infinite set of strings w can be encoded
in a weighted finite-state automaton, facilitating
efficient inference.
We use the Levenshtein-distance kernel here to
capture the effects of perceptual noise, but make two
modifications necessary for incremental inference
and for the correct computation of surprisal values
for new input: the distribution over already-seen in-
put must be proper, and possible future inputs must
be costless. The resulting weighted finite-state rep-
resentation of noisy input for a true sentence prefix
w* = w1...i is a j + 1-state automaton with arcs as
follows:
</bodyText>
<listItem confidence="0.985736">
• For each i E 1, ... , j:
– A substitution arc from i −1 to i with cost
</listItem>
<bodyText confidence="0.948321833333333">
proportional to exp[−LD(w′, wi) &apos;y] for
each word w′ in the lexicon, where &apos;y &gt; 0
is a noise parameter and LD(w′, wi) is the
Levenshtein distance between w′ and wi
(when w′ = wi there is no change to the
word);
</bodyText>
<listItem confidence="0.953461571428571">
– A deletion arc from i−1 to i labeled c with
cost proportional to exp[−len(wi)/&apos;y];
– An insertion loop arc from i − 1
to i − 1 with cost proportional to
exp[−len(w′)/&apos;y] for every word w′ in the
lexicon;
• A loop arc from j to j for each word w′ in
</listItem>
<page confidence="0.998003">
1059
</page>
<figureCaption confidence="0.9440365">
Figure 2: Noisy WFSA for partial input it hit...
with lexicon {it,hit,him}, noise parameter γ=1
</figureCaption>
<bodyText confidence="0.7949005">
the lexicon, with zero cost (value 1 in the real
semiring);
</bodyText>
<listItem confidence="0.9908885">
• State j is a zero-cost final state; no other states
are final.
</listItem>
<bodyText confidence="0.999950454545455">
The addition of loop arcs at state n allows mod-
eling of incremental comprehension through the au-
tomaton/grammar intersection (see also Hale, 2006);
and the fact that these arcs are costless ensures that
the partition function of the intersection reflects only
the grammatical prior plus the costs of input already
seen. In order to ensure that the distribution over
already-seen input is proper, we normalize the costs
on outgoing arcs from all states but j.6 Figure 2
gives an example of a simple WFSA representation
for a short partial input with a small lexicon.
</bodyText>
<subsectionHeader confidence="0.529436">
4.2 Inference
</subsectionHeader>
<bodyText confidence="0.978154">
Computing the surprisal incurred by the disam-
biguating element given an uncertain-input repre-
sentation of the sentence involves a standard appli-
cation of the definition of conditional probability
(Hale, 2001):
</bodyText>
<equation confidence="0.9995535">
1 P(I1...i−1)
log P(I1...i|I1...i−1) = log P(I1...i) (III)
</equation>
<bodyText confidence="0.9998618">
Since our uncertain inputs I1...k are encoded by a
WFSA, the probability P(I1...k) is equal to the par-
tition function of the intersection of this WFSA with
the PCFG given in Table 1.7 PCFGs are a special
class of weighted context-free grammars (WCFGs),
</bodyText>
<footnote confidence="0.862140625">
6If a state’s total unnormalized cost of insertion arcs is α and
that of deletion and insertion arcs is ,3, its normalizing constant
is �
1��. Note that we must have α &lt; 1, placing a constraint on
the value that -y can take (above which the normalizing constant
diverges).
7Using the WFSA representation of average noise effects
here actually involves one simplifying assumption, that the av-
</footnote>
<bodyText confidence="0.994465717391304">
which are closed under intersection with WFSAs; a
constructive procedure exists for finding the inter-
section (Bar-Hillel et al., 1964; Nederhof and Satta,
2003). Hence we are left with finding the partition
function of a WCFG, which cannot be computed ex-
actly, but a number of approximation methods are
known (Stolcke, 1995; Smith and Johnson, 2007;
Nederhof and Satta, 2008). In practice, the com-
putation required to compute the partition function
under any of these methods increases with the size
of the WCFG resulting from the intersection, which
for a binarized PCFG with R rules and an n-state
WFSA is Rn2. To increase efficiency we imple-
mented what is to our knowledge a novel method
for finding the minimal grammar including all rules
that will have non-zero probability in the intersec-
tion. We first parse the WFSA bottom-up with
the item-based method of Goodman (1999) in the
Boolean semiring, storing partial results in a chart.
After completion of this bottom-up parse, every rule
that will have non-zero probability in the intersec-
tion PCFG will be identifiable with a set of entries
in the chart, but not all entries in this chart will
have non-zero probability, since some are not con-
nected to the root. Hence we perform a second, top-
down Boolean-semiring parsing pass on the bottom-
up chart, throwing out entries that cannot be derived
from the root. We can then include in the intersec-
tion grammar only those rules from the classic con-
struction that can be identified with a set of surviv-
ing entries in the final parse chart.8 The partition
functions for each category in this intersection gram-
mar can then be computed; we used a fixed-point
method preceded by a topological sort on the gram-
mar’s ruleset, as described by Nederhof and Satta
(2008). To obtain the surprisal of the input deriv-
ing from a word wi in its context, we can thus com-
erage surprisal of Ii, or EP,[log PC (Ii |I1
... i−1)] is well ap-
proximated by the log of the ratio of the expected probabilities
of the noisy inputs I1...i_1 and I1...i, since as discussed in Sec-
tion 3 the quantities P(I1...i_1) and P(I1...i) are expectations
under the true noise distribution. This simplifying assumption
has the advantage of bypassing commitment to a specific repre-
sentation of perceptual input and should be justifiable for rea-
sonable noise functions, but the issue is worth further scrutiny.
</bodyText>
<footnote confidence="0.94048925">
8Note that a standard top-down algorithm such as Earley
parsing cannot be used to avoid the need for both bottom-up
and top-down passes, since the presence of loops in the WFSA
breaks the ability to operate strictly left-to-right.
</footnote>
<figure confidence="0.993293684210526">
him/0.050 him/0.050 him/1.000
hit/0.050
it/0.135
0 1 2
him/0.063
hit/0.172
it/0.467
ǫ/0.063
hit/0.050
it/0.135
him/0.158
hit/0.428
it/0.158
ǫ/0.021
hit/1.000
it/1.000
1060
0.10 0.15 0.20 0.25
Noise level γ (high=noisy)
</figure>
<figureCaption confidence="0.999131">
Figure 3: Model predictions for (4)–(6)
</figureCaption>
<table confidence="0.966183333333333">
Inverted Uninverted
-PP 0.76 0.93
+PP 0.85 0.92
</table>
<tableCaption confidence="0.998435">
Table 2: Question-answering accuracy
</tableCaption>
<bodyText confidence="0.9997725">
number of sentences in each condition and saw each
item only once. Experimental items were pseudo-
randomly interspersed with 62 filler sentences; no
two experimental items were ever adjacent. Punctu-
ation was presented with the word to its left, so that
for (4) the four and fifth button presses would yield
</bodyText>
<figure confidence="0.5545855">
Surprisal at main−clause verb
8.5 9.0 9.5 10.0 10.5 11.0
Inverted, +PP
Uninverted, +PP
Inverted, −PP
Uninverted, −PP
marched,
and
</figure>
<bodyText confidence="0.9934205">
pute the partition functions for noisy inputs I1...i−1
and I1...i corresponding to words w1...i−1 and words
w1...i respectively, and take the log of their ratio as
in Equation (III).
</bodyText>
<subsectionHeader confidence="0.998172">
4.3 Predictions
</subsectionHeader>
<bodyText confidence="0.9999799375">
The noise level γ is a free parameter in this model, so
we plot model predictions—the expected surprisal
of input from the main-clause verb for each vari-
ant of the target sentence in (4)–(6)—over a wide
range of its possible values (Figure 3). The far left of
the graph asymptotes toward the predictions of clean
surprisal, or noise-free input. With little to no input
uncertainty, the presence of the comma rules out the
garden-path analysis of the fronted PP toward the
tank, and the surprisal at the main-clause verb is the
same across condition (here reflecting only the un-
certainty of verb identity for this small grammar).
As input uncertainty increases, however, surprisal
in the [Inverted, −PP] condition increases, reflect-
ing the stronger belief given preceding context in an
input-unfaithful interpretation.
</bodyText>
<sectionHeader confidence="0.97824" genericHeader="method">
5 Empirical results
</sectionHeader>
<bodyText confidence="0.98726056097561">
To test these predictions we conducted a word-by-
word self-paced reading study, in which partici-
pants read by pressing a button to reveal each suc-
cessive word in a sentence; times between but-
ton presses are recorded and analyzed as an in-
dex of incremental processing difficulty (Mitchell,
1984). Forty monolingual native-English speaker
participants read twenty-four sentence quadruplets
(“items”) on the pattern of (4)–(6), with a Latin-
square design so that each participant saw an equal
toward
respectively (right-truncated here for reasons of
space). Every sentence was followed by a yes/no
comprehension question (e.g., Did the tank lurch to-
ward an injured enemy combatant?); participants re-
ceived feedback whenever they answered a question
incorrectly.
Reading-time results are shown in Figure 4. As
can be seen, the model’s predictions are matched
at the main-clause verb: reading times are highest
in the [Inverted, −PP] condition, and there is an
interaction between main-clause inversion and pres-
ence of a subordinate-clause PP such that presence
of the latter reduces reading times more for inverted
than for uninverted main clauses. This interaction
is significant in both by-participants and by-items
ANOVAs (both p &lt; 0.05) and in a linear mixed-
effects analysis with participants- and item-specific
random interactions (t &gt; 2; see Baayen et al., 2008).
The same pattern persists and remains significant
through to the end of the sentence, indicating con-
siderable processing disruption, and is also observed
in question-answering accuracies for experimental
sentences, which are superadditively lowest in the
[Inverted, −PP] condition (Table 2).
The inflated reading times for the [Inverted,
−PP] condition beginning at the main-clause
verb confirm the predictions of the uncertain-
input/surprisal theory. Crucially, the input that
would on our theory induce the comprehender to
question the comma (the fronted main-clause PP)
</bodyText>
<page confidence="0.829293">
1061
</page>
<bodyText confidence="0.872531090909091">
As the soldiers marched(,) into the toward the tank lurched toward an enemy combatant.
bunker,
Figure 4: Average reading times for each part of the
sentence, broken down by experimental condition
is not seen until after the comma is no longer visi-
ble (and presumably has been integrated into beliefs
about syntactic analysis on veridical-input theories).
This empirical result is hence difficult to accommo-
date in accounts which do not share our theory’s cru-
cial property that comprehenders can revise their be-
lief in previous input on the basis of current input.
</bodyText>
<sectionHeader confidence="0.996171" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999954541666667">
Language is redundant: the content of one part of a
sentence carries predictive value both for what will
precede and what will follow it. For this reason, and
because the path from a speaker’s intended utterance
to a comprehender’s perceived input is noisy and
error-prone, a comprehension system making opti-
mal use of available information would use current
input not only for forward prediction but also to as-
sess the veracity of previously encountered input.
Here we have developed a theory of how such an
adaptive error-correcting capacity is a consequence
of noisy-channel inference, with a comprehender’s
beliefs regarding sentence form and structure at any
moment in incremental comprehension reflecting a
balance between fidelity to perceptual input and a
preference for structures with higher prior proba-
bility. As a consequence of this theory, certain
types of sentence contexts will cause the drive to-
ward higher prior-probability analyses to overcome
the drive to maintain fidelity to input, undermin-
ing the comprehender’s belief in an earlier part of
the input actually perceived in favor of an analy-
sis unfaithful to part of the true input. If subse-
quent input strongly disconfirms this incorrect in-
terpretation, we should see behavioral signatures of
classic garden-path disambiguation. Within the the-
ory, the size of this “hallucinated” garden-path ef-
fect is indexed by the surprisal value under uncer-
tain input, marginalizing over the actual sentence
observed. Based on a model implementing the-
ory we designed a controlled psycholinguistic ex-
periment making specific predictions regarding the
role of fine-grained grammatical context in modu-
lating comprehenders’ strength of belief in a highly
specific bit of linguistic input—a comma marking
the end of a sentence-initial subordinate clause—
and tested those predictions in a self-paced read-
ing experiment. As predicted by the theory, read-
ing times at the word disambiguating the “halluci-
nated” garden-path were inflated relative to control
conditions. These results contribute to the theory of
uncertain-input effects in online sentence process-
ing by suggesting that comprehenders may be in-
duced not only to entertain but to adopt relatively
strong beliefs in grammatical analyses that require
modification of the surface input itself. Our results
also bring a new degree of nuance to surprisal the-
ory, demonstrating that perceptual neighbors of true
preceding input may need to be taken into account
in order to estimate how surprising a comprehender
will find subsequent input to be.
Beyond the domain of psycholinguistics, the
methods employed here might also be usefully ap-
plied to practical problems such as parsing of de-
graded or fragmentary sentence input, allowing joint
constraint derived from grammar and available input
to fill in gaps (Lang, 1988). Of course, practical ap-
plications of this sort would raise challenges of their
own, such as extending the grammar to broader cov-
erage, which is delicate here since the surface in-
put places a weaker check on overgeneration from
the grammar than in traditional probabilistic pars-
ing. Larger grammars also impose a technical bur-
den since parsing uncertain input is in practice more
computationally intensive than parsing clean input,
raising the question of what approximate-inference
algorithms might be well-suited to processing un-
certain input with grammatical knowledge. Answers
to this question might in turn be of interest for sen-
tence processing, since the exhaustive-parsing ideal-
ization employed here is not psychologically plausi-
ble. It seems likely that human comprehension in-
</bodyText>
<figure confidence="0.742737166666667">
Reading time (ms)
400 500 600 700
Inverted, +PP
Uninverted, +PP
Inverted, −PP
Uninverted, −PP
</figure>
<page confidence="0.981487">
1062
</page>
<bodyText confidence="0.999948428571429">
volves approximate inference with severely limited
memory that is nonetheless highly optimized to re-
cover something close to the intended meaning of
an utterance, even when the recovered meaning is
not completely faithful to the input itself. Arriving at
models that closely approximate this capacity would
be of both theoretical and practical value.
</bodyText>
<sectionHeader confidence="0.998096" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994473222222222">
Parts of this work have benefited from presentation
at the 2009 Annual Meeting of the Linguistic Soci-
ety of America and the 2009 CUNY Sentence Pro-
cessing Conference. I am grateful to Natalie Katz
and Henry Lu for assistance in preparing materials
and collecting data for the self-paced reading exper-
iment described here. This work was supported by a
UCSD Academic Senate grant, NSF CAREER grant
0953870, and NIH grant 1R01HD065829-01.
</bodyText>
<sectionHeader confidence="0.977319" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.96653270967742">
Adams, B. C., Clifton, Jr., C., and Mitchell, D. C.
(1998). Lexical guidance in sentence processing?
Psychonomic Bulletin &amp; Review, 5(2):265–270.
Baayen, R. H., Davidson, D. J., and Bates, D. M.
(2008). Mixed-effects modeling with crossed ran-
dom effects for subjects and items. Journal of
Memory and Language, 59(4):390–412.
Bar-Hillel, Y., Perles, M., and Shamir, E. (1964).
On formal properties of simple phrase structure
grammars. In Language and Information: Se-
lected Essays on their Theory and Application.
Addison-Wesley.
Bever, T. (1970). The cognitive basis for linguistic
structures. In Hayes, J., editor, Cognition and the
Development of Language, pages 279–362. John
Wiley &amp; Sons.
Bolinger, D. (1971). A further note on the nominal
in the progressive. Linguistic Inquiry, 2(4):584–
586.
Boston, M. F., Hale, J. T., Kliegl, R., Patil, U., and
Vasishth, S. (2008). Parsing costs as predictors of
reading difficulty: An evaluation using the Pots-
dam sentence corpus. Journal of Eye Movement
Research, 2(1):1–12.
Bresnan, J. (1994). Locative inversion and the
architecture of universal grammar. Language,
70(1):72–131.
Christianson, K., Hollingworth, A., Halliwell, J. F.,
and Ferreira, F. (2001). Thematic roles assigned
along the garden path linger. Cognitive Psychol-
ogy, 42:368–407.
</bodyText>
<reference confidence="0.9527516875">
Connine, C. M., Blasko, D. G., and Hall, M. (1991).
Effects of subsequent sentence context in audi-
tory word recognition: Temporal and linguistic
constraints. Journal of Memory and Language,
30(2):234–250.
Demberg, V. and Keller, F. (2008). Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition,
109(2):193–210.
Ferreira, F. and Henderson, J. M. (1993). Reading
processes during syntactic analysis and reanaly-
sis. Canadian Journal of Experimental Psychol-
ogy, 16:555–568.
Fodor, J. D. (2002). Psycholinguistics cannot escape
prosody. In Proceedings of the Speech Prosody
Conference.
Frank, S. L. (2009). Surprisal-based comparison be-
tween a symbolic and a connectionist model of
sentence processing. In Proceedings of the 31st
Annual Conference of the Cognitive Science Soci-
ety, pages 1139–1144.
Frazier, L. (1979). On Comprehending Sentences:
Syntactic Parsing Strategies. PhD thesis, Univer-
sity of Massachusetts.
Frazier, L. and Rayner, K. (1982). Making and
correcting errors during sentence comprehension:
Eye movements in the analysis of structurally
ambiguous sentences. Cognitive Psychology,
14:178–210.
Goodman, J. (1999). Semiring parsing. Computa-
tional Linguistics, 25(4):573–605.
Hale, J. (2001). A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
Second Meeting of the North American Chapter
of the Association for Computational Linguistics,
pages 159–166.
Hale, J. (2006). Uncertainty about the rest of the
sentence. Cognitive Science, 30(4):609–642.
1063
Hill, R. L. and Murray, W. S. (2000). Commas and
spaces: Effects of punctuation on eye movements
and sentence parsing. In Kennedy, A., Radach,
R., Heller, D., and Pynte, J., editors, Reading as a
Perceptual Process. Elsevier.
Jurafsky, D. (1996). A probabilistic model of lexical
and syntactic access and disambiguation. Cogni-
tive Science, 20(2):137–194.
Kuˇcera, H. and Francis, W. N. (1967). Computa-
tional Analysis ofPresent-day American English.
Providence, RI: Brown University Press.
Lang, B. (1988). Parsing incomplete sentences. In
Proceedings of COLING.
Levy, R. (2008a). Expectation-based syntactic com-
prehension. Cognition, 106:1126–1177.
Levy, R. (2008b). A noisy-channel model of ratio-
nal human sentence comprehension under uncer-
tain input. In Proceedings of the 13th Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 234–243.
Levy, R. and Andrew, G. (2006). Tregex and Tsur-
geon: tools for querying and manipulating tree
data structures. In Proceedings of the 2006 con-
ference on Language Resources and Evaluation.
Levy, R., Bicknell, K., Slattery, T., and Rayner,
K. (2009). Eye movement evidence that read-
ers maintain and act on uncertainty about past
linguistic input. Proceedings of the National
Academy of Sciences, 106(50):21086–21090.
Marcus, M. P., Santorini, B., and Marcinkiewicz,
M. A. (1994). Building a large annotated corpus
of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Mitchell, D. C. (1984). An evaluation of subject-
paced reading tasks and other methods for investi-
gating immediate processes in reading. In Kieras,
D. and Just, M. A., editors, New methods in read-
ing comprehension. Hillsdale, NJ: Earlbaum.
Mitchell, D. C. (1987). Lexical guidance in hu-
man parsing: Locus and processing characteris-
tics. In Coltheart, M., editor, Attention and Per-
formance XII: The psychology of reading. Lon-
don: Erlbaum.
Narayanan, S. and Jurafsky, D. (1998). Bayesian
models of human sentence processing. In Pro-
ceedings of the Twelfth Annual Meeting of the
Cognitive Science Society.
Narayanan, S. and Jurafsky, D. (2002). A Bayesian
model predicts human parse preference and read-
ing time in sentence processing. In Advances
in Neural Information Processing Systems, vol-
ume 14, pages 59–65.
Nederhof, M.-J. and Satta, G. (2003). Probabilis-
tic parsing as intersection. In Proceedings of the
International Workshop on Parsing Technologies.
Nederhof, M.-J. and Satta, G. (2008). Computing
partition functions of PCFGs. Research on Logic
and Computation, 6:139–162.
Roark, B., Bachrach, A., Cardenas, C., and Pal-
lier, C. (2009). Deriving lexical and syntactic
expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In
Proceedings ofEMNLP.
Rohde, D. (2005). TGrep2 User Manual, version
1.15 edition.
Smith, N. A. and Johnson, M. (2007). Weighted
and probabilistic context-free grammars are
equally expressive. Computational Linguistics,
33(4):477–491.
Smith, N. J. and Levy, R. (2008). Optimal process-
ing times in reading: a formal model and empiri-
cal investigation. In Proceedings of the 30th An-
nual Meeting of the Cognitive Science Society.
Staub, A. (2007). The parser doesn’t ignore intransi-
tivity, after all. Journal of Experimental Psychol-
ogy: Learning, Memory, &amp; Cognition, 33(3):550–
569.
Stolcke, A. (1995). An efficient probabilistic
context-free parsing algorithm that computes pre-
fix probabilities. Computational Linguistics,
21(2):165–201.
Sturt, P., Pickering, M. J., and Crocker, M. W.
(1999). Structural change and reanalysis difficulty
in language comprehension. Journal of Memory
and Language, 40:136–150.
Tabor, W. and Hutchins, S. (2004). Evidence for
self-organized sentence processing: Digging in
effects. Journal of Experimental Psychology:
Learning, Memory, &amp; Cognition, 30(2):431–450.
</reference>
<page confidence="0.59233">
1064
</page>
<reference confidence="0.983055">
van Gompel, R. P. G. and Pickering, M. J. (2001).
Lexical guidance in sentence processing: A note
on Adams, Clifton, and Mitchell (1998). Psycho-
nomic Bulletin &amp; Review, 8(4):851–857.
</reference>
<page confidence="0.974344">
1065
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.492611">
<title confidence="0.990238">Integrating surprisal and uncertain-input models in online comprehension: formal techniques and empirical results</title>
<author confidence="0.998049">Roger</author>
<affiliation confidence="0.9991315">Department of University of California at San</affiliation>
<address confidence="0.7517935">9500 Gilman Drive # La Jolla, CA</address>
<email confidence="0.999154">rlevy@ucsd.edu</email>
<abstract confidence="0.999690368421052">A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C M Connine</author>
<author>D G Blasko</author>
<author>M Hall</author>
</authors>
<title>Effects of subsequent sentence context in auditory word recognition: Temporal and linguistic constraints.</title>
<date>1991</date>
<journal>Journal of Memory and Language,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="2145" citStr="Connine et al., 1991" startWordPosition="316" endWordPosition="319"> given perceptual input, using linguistic knowledge both prospectively and retrospectively in drawing inferences as to how raw input should be segmented and recognized as a sequence of linguistic tokens, and about the degree to which each input token should be trusted during grammatical analysis. Formal models of such joint inference over uncertain input have been proposed (Levy, 2008b), and corroborative empirical evidence exists that strong coherence of current input with a perceptual neighbor of previous input may induce confusion in comprehenders as to the identity of that previous input (Connine et al., 1991; Levy et al., 2009). In this paper we explore a more dramatic prediction of such an uncertain-input theory: that, when faced with sufficiently biasing input, comprehenders might under some circumstances adopt a grammatical analysis inconsistent with the true raw input comprising a sentence they are presented with, but consistent with a slightly perturbed version of the input that has higher prior probability. If this is the case, then subsequent input strongly disconfirming this “hallucinated” garden-path analysis might be expected to induce the same effects as seen in classic cases of garden</context>
</contexts>
<marker>Connine, Blasko, Hall, 1991</marker>
<rawString>Connine, C. M., Blasko, D. G., and Hall, M. (1991). Effects of subsequent sentence context in auditory word recognition: Temporal and linguistic constraints. Journal of Memory and Language, 30(2):234–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Data from eye-tracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>109</volume>
<issue>2</issue>
<contexts>
<context position="5088" citStr="Demberg and Keller, 2008" startWordPosition="759" endWordPosition="762">of a sentence in its context is given by the simple information-theoretic measure of the log of the inverse of the word’s conditional probability (also called its “surprisal” or “Shannon information content”) in its intra-sentential context w1,...,i−1 and extra-sentential context Ctxt: 1 Effort(wi) ∝ log P(wi|w1...i−1, Ctxt) (In the rest of this paper, we consider isolatedsentence comprehension and ignore Ctxt.) The theory derives empirical support not only from controlled experiments manipulating grammatical context but also from broad-coverage studies of reading times for naturalistic text (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009), including demonstration that the shape of the relationship between word probability and reading time is indeed log-linear (Smith and Levy, 2008). Surprisal has had considerable success in accounting for one of the best-known phenomena in psycholinguistics, the GARDEN-PATH SENTENCE (Frazier, 1979), in which a local ambiguity biases the comprehender’s incremental syntactic interpretation so strongly that upon encountering disambiguating input the correct interpretation can only be recovered with great effort, if at all. The most famous exa</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Demberg, V. and Keller, F. (2008). Data from eye-tracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ferreira</author>
<author>J M Henderson</author>
</authors>
<title>Reading processes during syntactic analysis and reanalysis.</title>
<date>1993</date>
<journal>Canadian Journal of Experimental Psychology,</journal>
<pages>16--555</pages>
<contexts>
<context position="8733" citStr="Ferreira and Henderson, 1993" startWordPosition="1327" endWordPosition="1330">ding the socks fell off her lap. In incremental comprehension, the phrase the socks is ambiguous between being the NP object of the preceding subordinate-clause verb mending versus being the subject of the main clause (in which case mending has a Zero object); in sentences like (2) the initial bias is toward the NP interpretation. The main-clause verb fell disambiguates, ruling out the initially favored NP analysis. It has been known since Frazier and Rayner (1982) that this effect of garden-path disambiguation can be measured in reading times on the main-clause verb (see also Mitchell, 1987; Ferreira and Henderson, 1993; Adams et al., 1998; Sturt et al., 1999; Hill and Murray, 2000; Christianson et al., 2001; van Gompel and Pickering, 2001; Tabor and Hutchins, 2004; Staub, 2007). Small changes to the context can have huge effects on comprehenders’ initial interpretations, however. It is unusual for sentenceinitial subordinate clauses not to end with a comma or some other type of punctuation (searches in the parsed Brown corpus put the rate at about 18%); empirically it has consistently been found that a comma eliminates the garden-path effect in NP/Z sentences: (3) While Mary was mending, the socks fell off </context>
</contexts>
<marker>Ferreira, Henderson, 1993</marker>
<rawString>Ferreira, F. and Henderson, J. M. (1993). Reading processes during syntactic analysis and reanalysis. Canadian Journal of Experimental Psychology, 16:555–568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
</authors>
<title>Psycholinguistics cannot escape prosody.</title>
<date>2002</date>
<booktitle>In Proceedings of the Speech Prosody Conference.</booktitle>
<contexts>
<context position="9503" citStr="Fodor (2002)" startWordPosition="1455" endWordPosition="1456">007). Small changes to the context can have huge effects on comprehenders’ initial interpretations, however. It is unusual for sentenceinitial subordinate clauses not to end with a comma or some other type of punctuation (searches in the parsed Brown corpus put the rate at about 18%); empirically it has consistently been found that a comma eliminates the garden-path effect in NP/Z sentences: (3) While Mary was mending, the socks fell off her lap. Understanding sentences like (3) is intuitively much easier, and reading times at the disambiguating verb are reliably lower when compared with (2). Fodor (2002) summarized the power of this effect succinctly: [w]ith a comma after mending, there would be no syntactic garden path left to be studied. (Fodor, 2002) In a surprisal model with clean, veridical input, Fodor’s conclusion is exactly what is predicted: separating a verb from its direct object with a comma effectively never happens in edited, published written English, so the conditional probability of the NP analysis should be close to zero.2 When uncertainty about surface input is introduced, however— due to visual noise, imperfect memory representations, and/or beliefs about possible speaker </context>
</contexts>
<marker>Fodor, 2002</marker>
<rawString>Fodor, J. D. (2002). Psycholinguistics cannot escape prosody. In Proceedings of the Speech Prosody Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Frank</author>
</authors>
<title>Surprisal-based comparison between a symbolic and a connectionist model of sentence processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1139--1144</pages>
<contexts>
<context position="5122" citStr="Frank, 2009" startWordPosition="767" endWordPosition="768">mple information-theoretic measure of the log of the inverse of the word’s conditional probability (also called its “surprisal” or “Shannon information content”) in its intra-sentential context w1,...,i−1 and extra-sentential context Ctxt: 1 Effort(wi) ∝ log P(wi|w1...i−1, Ctxt) (In the rest of this paper, we consider isolatedsentence comprehension and ignore Ctxt.) The theory derives empirical support not only from controlled experiments manipulating grammatical context but also from broad-coverage studies of reading times for naturalistic text (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009), including demonstration that the shape of the relationship between word probability and reading time is indeed log-linear (Smith and Levy, 2008). Surprisal has had considerable success in accounting for one of the best-known phenomena in psycholinguistics, the GARDEN-PATH SENTENCE (Frazier, 1979), in which a local ambiguity biases the comprehender’s incremental syntactic interpretation so strongly that upon encountering disambiguating input the correct interpretation can only be recovered with great effort, if at all. The most famous example is (1) below (Bever, 1970): (</context>
</contexts>
<marker>Frank, 2009</marker>
<rawString>Frank, S. L. (2009). Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. In Proceedings of the 31st Annual Conference of the Cognitive Science Society, pages 1139–1144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
</authors>
<title>On Comprehending Sentences: Syntactic Parsing Strategies.</title>
<date>1979</date>
<tech>PhD thesis,</tech>
<institution>University of Massachusetts.</institution>
<contexts>
<context position="5442" citStr="Frazier, 1979" startWordPosition="813" endWordPosition="814">er isolatedsentence comprehension and ignore Ctxt.) The theory derives empirical support not only from controlled experiments manipulating grammatical context but also from broad-coverage studies of reading times for naturalistic text (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009), including demonstration that the shape of the relationship between word probability and reading time is indeed log-linear (Smith and Levy, 2008). Surprisal has had considerable success in accounting for one of the best-known phenomena in psycholinguistics, the GARDEN-PATH SENTENCE (Frazier, 1979), in which a local ambiguity biases the comprehender’s incremental syntactic interpretation so strongly that upon encountering disambiguating input the correct interpretation can only be recovered with great effort, if at all. The most famous example is (1) below (Bever, 1970): (1) The horse raced past the barn fell. where the context before the final word is strongly biased toward an interpretation where raced is the main verb of the sentence (MV; Figure 1a), the intended interpretation, where raced begins a reduced relative clause (RR; Figure 1b) and fell is the main verb, is extremely diffi</context>
</contexts>
<marker>Frazier, 1979</marker>
<rawString>Frazier, L. (1979). On Comprehending Sentences: Syntactic Parsing Strategies. PhD thesis, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
<author>K Rayner</author>
</authors>
<title>Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology,</title>
<date>1982</date>
<pages>14--178</pages>
<contexts>
<context position="8574" citStr="Frazier and Rayner (1982)" startWordPosition="1303" endWordPosition="1306"> the probability ratio even farther in favor of the main-clause analysis. 1056 (b) RR interpretation Figure 1: Classic garden pathing (2) While Mary was mending the socks fell off her lap. In incremental comprehension, the phrase the socks is ambiguous between being the NP object of the preceding subordinate-clause verb mending versus being the subject of the main clause (in which case mending has a Zero object); in sentences like (2) the initial bias is toward the NP interpretation. The main-clause verb fell disambiguates, ruling out the initially favored NP analysis. It has been known since Frazier and Rayner (1982) that this effect of garden-path disambiguation can be measured in reading times on the main-clause verb (see also Mitchell, 1987; Ferreira and Henderson, 1993; Adams et al., 1998; Sturt et al., 1999; Hill and Murray, 2000; Christianson et al., 2001; van Gompel and Pickering, 2001; Tabor and Hutchins, 2004; Staub, 2007). Small changes to the context can have huge effects on comprehenders’ initial interpretations, however. It is unusual for sentenceinitial subordinate clauses not to end with a comma or some other type of punctuation (searches in the parsed Brown corpus put the rate at about 18%</context>
</contexts>
<marker>Frazier, Rayner, 1982</marker>
<rawString>Frazier, L. and Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14:178–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="22757" citStr="Goodman (1999)" startWordPosition="3628" endWordPosition="3629"> a number of approximation methods are known (Stolcke, 1995; Smith and Johnson, 2007; Nederhof and Satta, 2008). In practice, the computation required to compute the partition function under any of these methods increases with the size of the WCFG resulting from the intersection, which for a binarized PCFG with R rules and an n-state WFSA is Rn2. To increase efficiency we implemented what is to our knowledge a novel method for finding the minimal grammar including all rules that will have non-zero probability in the intersection. We first parse the WFSA bottom-up with the item-based method of Goodman (1999) in the Boolean semiring, storing partial results in a chart. After completion of this bottom-up parse, every rule that will have non-zero probability in the intersection PCFG will be identifiable with a set of entries in the chart, but not all entries in this chart will have non-zero probability, since some are not connected to the root. Hence we perform a second, topdown Boolean-semiring parsing pass on the bottomup chart, throwing out entries that cannot be derived from the root. We can then include in the intersection grammar only those rules from the classic construction that can be ident</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Goodman, J. (1999). Semiring parsing. Computational Linguistics, 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="1357" citStr="Hale, 2001" startWordPosition="195" endWordPosition="196">curred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory. 1 Introduction In most formal theories of human sentence comprehension, input recognition and syntactic analysis are taken to be distinct processes, with the only feedback from syntax to recognition being prospective prediction of likely upcoming input (Jurafsky, 1996; Narayanan and Jurafsky, 1998, 2002; Hale, 2001, 2006; Levy, 2008a). Yet a system making optimal use of all available information might be expected to perform fully joint inference on sentence identity and structure given perceptual input, using linguistic knowledge both prospectively and retrospectively in drawing inferences as to how raw input should be segmented and recognized as a sequence of linguistic tokens, and about the degree to which each input token should be trusted during grammatical analysis. Formal models of such joint inference over uncertain input have been proposed (Levy, 2008b), and corroborative empirical evidence exis</context>
<context position="2964" citStr="Hale, 2001" startWordPosition="444" endWordPosition="445"> a grammatical analysis inconsistent with the true raw input comprising a sentence they are presented with, but consistent with a slightly perturbed version of the input that has higher prior probability. If this is the case, then subsequent input strongly disconfirming this “hallucinated” garden-path analysis might be expected to induce the same effects as seen in classic cases of garden-path disambiguation traditionally studied in the psycholinguistic literature. We explore this prediction by extending the rational uncertain-input model of Levy (2008b), integrating it with SURPRISAL THEORY (Hale, 2001; Levy, 2008a), which successfully accounts for and quantifies traditional garden-path disambiguation effects; and by testing predictions of the extended model in a self-paced reading study. Section 2 reviews surprisal theory and how it accounts for traditional gardenpath effects. Section 3 provides background information on garden-path effects relevant to the current study, describes how we might hope to reveal comprehenders’ use of grammatical knowledge to revise beliefs about the identity of previous linguistic sur1055 Proceedings of the 49th Annual Meeting of the Association for Computatio</context>
<context position="4380" citStr="Hale, 2001" startWordPosition="654" endWordPosition="655">lled experiment, and informally outlines how such belief revisions might arise as a side effect in a general theory of rational comprehension under uncertain input. Section 4 defines and estimates parameters for a model instantiating the general theory, and describes the predictions of the model for the experiment described in Section 3 (along with the inference procedures required to determine those predictions). Section 5 reports the results of the experiment. Section 6 concludes. 2 Garden-path disambiguation under surprisal The SURPRISAL THEORY of incremental sentenceprocessing difficulty (Hale, 2001; Levy, 2008a) posits that the cognitive effort required to process a given word wi of a sentence in its context is given by the simple information-theoretic measure of the log of the inverse of the word’s conditional probability (also called its “surprisal” or “Shannon information content”) in its intra-sentential context w1,...,i−1 and extra-sentential context Ctxt: 1 Effort(wi) ∝ log P(wi|w1...i−1, Ctxt) (In the rest of this paper, we consider isolatedsentence comprehension and ignore Ctxt.) The theory derives empirical support not only from controlled experiments manipulating grammatical c</context>
<context position="6692" citStr="Hale (2001)" startWordPosition="1007" endWordPosition="1008">the possible incremental syntactic analyses of words w1...6 preceding fell, under surprisal the conditional probability of the disambiguating continuation fell can be approximated as �P(fell|w1...6) _ P(fell|Tj, w1...6)P(Tj|w1...6) j (I) For all possible predisambiguation analyses Tj, either the analysis is disfavored by the context (P(Tj|w1...6) is low) or the analysis makes the disambiguating word unlikely (P(fell|Tj,w1...6) is low). Since every summand in the marginalization of Equation (I) has a very small term in it, the total marginal probability is thus small and the surprisal is high. Hale (2001) demonstrated that surprisal thus predicts strong garden-pathing effects in the classic sentence The horse raced past the barn fell on basis of the overall rarity of reduced relative clauses alone. More generally, Jurafsky (1996) used a combination of syntactic probabilities (reduced RCs are rare) and argument-structure probabilities (raced is usually intransitive) to estimate the probability ratio of the two analyses of pre-disambiguation context in Figure 1 as roughly 82:1, putting a lower bound on the additional surprisal incurred at fell for the reduced-RC variant over the unreduced varian</context>
<context position="21174" citStr="Hale, 2001" startWordPosition="3365" endWordPosition="3366">are costless ensures that the partition function of the intersection reflects only the grammatical prior plus the costs of input already seen. In order to ensure that the distribution over already-seen input is proper, we normalize the costs on outgoing arcs from all states but j.6 Figure 2 gives an example of a simple WFSA representation for a short partial input with a small lexicon. 4.2 Inference Computing the surprisal incurred by the disambiguating element given an uncertain-input representation of the sentence involves a standard application of the definition of conditional probability (Hale, 2001): 1 P(I1...i−1) log P(I1...i|I1...i−1) = log P(I1...i) (III) Since our uncertain inputs I1...k are encoded by a WFSA, the probability P(I1...k) is equal to the partition function of the intersection of this WFSA with the PCFG given in Table 1.7 PCFGs are a special class of weighted context-free grammars (WCFGs), 6If a state’s total unnormalized cost of insertion arcs is α and that of deletion and insertion arcs is ,3, its normalizing constant is � 1��. Note that we must have α &lt; 1, placing a constraint on the value that -y can take (above which the normalizing constant diverges). 7Using the WF</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>Uncertainty about the rest of the sentence.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>1063</pages>
<contexts>
<context position="20532" citStr="Hale, 2006" startWordPosition="3262" endWordPosition="3263"> arc from i−1 to i labeled c with cost proportional to exp[−len(wi)/&apos;y]; – An insertion loop arc from i − 1 to i − 1 with cost proportional to exp[−len(w′)/&apos;y] for every word w′ in the lexicon; • A loop arc from j to j for each word w′ in 1059 Figure 2: Noisy WFSA for partial input it hit... with lexicon {it,hit,him}, noise parameter γ=1 the lexicon, with zero cost (value 1 in the real semiring); • State j is a zero-cost final state; no other states are final. The addition of loop arcs at state n allows modeling of incremental comprehension through the automaton/grammar intersection (see also Hale, 2006); and the fact that these arcs are costless ensures that the partition function of the intersection reflects only the grammatical prior plus the costs of input already seen. In order to ensure that the distribution over already-seen input is proper, we normalize the costs on outgoing arcs from all states but j.6 Figure 2 gives an example of a simple WFSA representation for a short partial input with a small lexicon. 4.2 Inference Computing the surprisal incurred by the disambiguating element given an uncertain-input representation of the sentence involves a standard application of the definiti</context>
</contexts>
<marker>Hale, 2006</marker>
<rawString>Hale, J. (2006). Uncertainty about the rest of the sentence. Cognitive Science, 30(4):609–642. 1063</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Hill</author>
<author>W S Murray</author>
</authors>
<title>Commas and spaces: Effects of punctuation on eye movements and sentence parsing.</title>
<date>2000</date>
<editor>In Kennedy, A., Radach, R., Heller, D., and Pynte, J., editors,</editor>
<publisher>Elsevier.</publisher>
<contexts>
<context position="8796" citStr="Hill and Murray, 2000" startWordPosition="1339" endWordPosition="1342">ase the socks is ambiguous between being the NP object of the preceding subordinate-clause verb mending versus being the subject of the main clause (in which case mending has a Zero object); in sentences like (2) the initial bias is toward the NP interpretation. The main-clause verb fell disambiguates, ruling out the initially favored NP analysis. It has been known since Frazier and Rayner (1982) that this effect of garden-path disambiguation can be measured in reading times on the main-clause verb (see also Mitchell, 1987; Ferreira and Henderson, 1993; Adams et al., 1998; Sturt et al., 1999; Hill and Murray, 2000; Christianson et al., 2001; van Gompel and Pickering, 2001; Tabor and Hutchins, 2004; Staub, 2007). Small changes to the context can have huge effects on comprehenders’ initial interpretations, however. It is unusual for sentenceinitial subordinate clauses not to end with a comma or some other type of punctuation (searches in the parsed Brown corpus put the rate at about 18%); empirically it has consistently been found that a comma eliminates the garden-path effect in NP/Z sentences: (3) While Mary was mending, the socks fell off her lap. Understanding sentences like (3) is intuitively much e</context>
</contexts>
<marker>Hill, Murray, 2000</marker>
<rawString>Hill, R. L. and Murray, W. S. (2000). Commas and spaces: Effects of punctuation on eye movements and sentence parsing. In Kennedy, A., Radach, R., Heller, D., and Pynte, J., editors, Reading as a Perceptual Process. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1309" citStr="Jurafsky, 1996" startWordPosition="188" endWordPosition="189">ul garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory. 1 Introduction In most formal theories of human sentence comprehension, input recognition and syntactic analysis are taken to be distinct processes, with the only feedback from syntax to recognition being prospective prediction of likely upcoming input (Jurafsky, 1996; Narayanan and Jurafsky, 1998, 2002; Hale, 2001, 2006; Levy, 2008a). Yet a system making optimal use of all available information might be expected to perform fully joint inference on sentence identity and structure given perceptual input, using linguistic knowledge both prospectively and retrospectively in drawing inferences as to how raw input should be segmented and recognized as a sequence of linguistic tokens, and about the degree to which each input token should be trusted during grammatical analysis. Formal models of such joint inference over uncertain input have been proposed (Levy, 2</context>
<context position="6921" citStr="Jurafsky (1996)" startWordPosition="1042" endWordPosition="1043">1...6) j (I) For all possible predisambiguation analyses Tj, either the analysis is disfavored by the context (P(Tj|w1...6) is low) or the analysis makes the disambiguating word unlikely (P(fell|Tj,w1...6) is low). Since every summand in the marginalization of Equation (I) has a very small term in it, the total marginal probability is thus small and the surprisal is high. Hale (2001) demonstrated that surprisal thus predicts strong garden-pathing effects in the classic sentence The horse raced past the barn fell on basis of the overall rarity of reduced relative clauses alone. More generally, Jurafsky (1996) used a combination of syntactic probabilities (reduced RCs are rare) and argument-structure probabilities (raced is usually intransitive) to estimate the probability ratio of the two analyses of pre-disambiguation context in Figure 1 as roughly 82:1, putting a lower bound on the additional surprisal incurred at fell for the reduced-RC variant over the unreduced variant (The horse that was raced past the barn fell) of 6.4 bits.1 3 Garden-pathing and input uncertainty We now move on to cases where garden-pathing can apparently be blocked by only small changes to the surface input, which we will</context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>Jurafsky, D. (1996). A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20(2):137–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kuˇcera</author>
<author>W N Francis</author>
</authors>
<title>Computational Analysis ofPresent-day American English.</title>
<date>1967</date>
<publisher>Brown University Press.</publisher>
<location>Providence, RI:</location>
<marker>Kuˇcera, Francis, 1967</marker>
<rawString>Kuˇcera, H. and Francis, W. N. (1967). Computational Analysis ofPresent-day American English. Providence, RI: Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Parsing incomplete sentences.</title>
<date>1988</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="31644" citStr="Lang, 1988" startWordPosition="5031" endWordPosition="5032"> beliefs in grammatical analyses that require modification of the surface input itself. Our results also bring a new degree of nuance to surprisal theory, demonstrating that perceptual neighbors of true preceding input may need to be taken into account in order to estimate how surprising a comprehender will find subsequent input to be. Beyond the domain of psycholinguistics, the methods employed here might also be usefully applied to practical problems such as parsing of degraded or fragmentary sentence input, allowing joint constraint derived from grammar and available input to fill in gaps (Lang, 1988). Of course, practical applications of this sort would raise challenges of their own, such as extending the grammar to broader coverage, which is delicate here since the surface input places a weaker check on overgeneration from the grammar than in traditional probabilistic parsing. Larger grammars also impose a technical burden since parsing uncertain input is in practice more computationally intensive than parsing clean input, raising the question of what approximate-inference algorithms might be well-suited to processing uncertain input with grammatical knowledge. Answers to this question m</context>
</contexts>
<marker>Lang, 1988</marker>
<rawString>Lang, B. (1988). Parsing incomplete sentences. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>106--1126</pages>
<contexts>
<context position="1375" citStr="Levy, 2008" startWordPosition="198" endWordPosition="199">bsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory. 1 Introduction In most formal theories of human sentence comprehension, input recognition and syntactic analysis are taken to be distinct processes, with the only feedback from syntax to recognition being prospective prediction of likely upcoming input (Jurafsky, 1996; Narayanan and Jurafsky, 1998, 2002; Hale, 2001, 2006; Levy, 2008a). Yet a system making optimal use of all available information might be expected to perform fully joint inference on sentence identity and structure given perceptual input, using linguistic knowledge both prospectively and retrospectively in drawing inferences as to how raw input should be segmented and recognized as a sequence of linguistic tokens, and about the degree to which each input token should be trusted during grammatical analysis. Formal models of such joint inference over uncertain input have been proposed (Levy, 2008b), and corroborative empirical evidence exists that strong coh</context>
<context position="2912" citStr="Levy (2008" startWordPosition="436" endWordPosition="437">, comprehenders might under some circumstances adopt a grammatical analysis inconsistent with the true raw input comprising a sentence they are presented with, but consistent with a slightly perturbed version of the input that has higher prior probability. If this is the case, then subsequent input strongly disconfirming this “hallucinated” garden-path analysis might be expected to induce the same effects as seen in classic cases of garden-path disambiguation traditionally studied in the psycholinguistic literature. We explore this prediction by extending the rational uncertain-input model of Levy (2008b), integrating it with SURPRISAL THEORY (Hale, 2001; Levy, 2008a), which successfully accounts for and quantifies traditional garden-path disambiguation effects; and by testing predictions of the extended model in a self-paced reading study. Section 2 reviews surprisal theory and how it accounts for traditional gardenpath effects. Section 3 provides background information on garden-path effects relevant to the current study, describes how we might hope to reveal comprehenders’ use of grammatical knowledge to revise beliefs about the identity of previous linguistic sur1055 Proceedings of the 4</context>
<context position="4392" citStr="Levy, 2008" startWordPosition="656" endWordPosition="657">ent, and informally outlines how such belief revisions might arise as a side effect in a general theory of rational comprehension under uncertain input. Section 4 defines and estimates parameters for a model instantiating the general theory, and describes the predictions of the model for the experiment described in Section 3 (along with the inference procedures required to determine those predictions). Section 5 reports the results of the experiment. Section 6 concludes. 2 Garden-path disambiguation under surprisal The SURPRISAL THEORY of incremental sentenceprocessing difficulty (Hale, 2001; Levy, 2008a) posits that the cognitive effort required to process a given word wi of a sentence in its context is given by the simple information-theoretic measure of the log of the inverse of the word’s conditional probability (also called its “surprisal” or “Shannon information content”) in its intra-sentential context w1,...,i−1 and extra-sentential context Ctxt: 1 Effort(wi) ∝ log P(wi|w1...i−1, Ctxt) (In the rest of this paper, we consider isolatedsentence comprehension and ignore Ctxt.) The theory derives empirical support not only from controlled experiments manipulating grammatical context but a</context>
<context position="10440" citStr="Levy (2008" startWordPosition="1608" endWordPosition="1609">pens in edited, published written English, so the conditional probability of the NP analysis should be close to zero.2 When uncertainty about surface input is introduced, however— due to visual noise, imperfect memory representations, and/or beliefs about possible speaker error— analyses come into play in which some parts of the true string are treated as if they were absent. In particular, because the two sentences are perceptual neighbors, the pre-disambiguation garden-path analysis of (2) may be entertained in (3). We can get a tighter handle on the effect of input uncertainty by extending Levy (2008b)’s analysis of the expected beliefs of a comprehender about the sequence of words constituting an input sentence to joint inference over both sentence identity and sentence structure. For a true sentence w∗ which yields perceptual input I, joint inference on sentence identity w and structure T marginalizing over I yields: PC(T,w|w*) = J PC (T, w |I, w*) PT (I |w*) dI I where PT(I|w∗) is the true model of noise (perceptual inputs derived from the true sentence) and PC(·) terms reflect the comprehender’s linguistic knowledge and beliefs about the noise processes intervening between intended se</context>
<context position="18444" citStr="Levy (2008" startWordPosition="2904" endWordPosition="2905">, and of course, or (ii) manner or temporal adjuncts. The handful of true locative PPs (5 in total) are all parentheticals intervening between the verb and a complement strongly selected by the verb (e.g., [VP means, in my country, homosexual]); none fulfill one of the verb’s thematic requirements. certain input is represented as a weighted finite-state automaton (WFSA), allowing us to represent the incremental inferences of the comprehender through intersection of the input WFSA with the PCFG above (Bar-Hillel et al., 1964; Nederhof and Satta, 2003, 2008). 4.1 Uncertain-input representations Levy (2008a) introduced the LEVENSHTEINDISTANCE KERNEL as a model of the average effect of noise in uncertain-input probabilistic sentence comprehension; this corresponds to term (ii) in our Equation (II). This kernel had a single noise parameter governing scaling of the cost of considering word substitutions, insertions, and deletions are considered, with the cost of a word substitution falling off exponentially with Levenshtein distance between the true word and the substituted word, and the cost of word insertion or deletion falling off exponentially with word length. The distribution over the infini</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Levy, R. (2008a). Expectation-based syntactic comprehension. Cognition, 106:1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>A noisy-channel model of rational human sentence comprehension under uncertain input.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>234--243</pages>
<contexts>
<context position="1375" citStr="Levy, 2008" startWordPosition="198" endWordPosition="199">bsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory. 1 Introduction In most formal theories of human sentence comprehension, input recognition and syntactic analysis are taken to be distinct processes, with the only feedback from syntax to recognition being prospective prediction of likely upcoming input (Jurafsky, 1996; Narayanan and Jurafsky, 1998, 2002; Hale, 2001, 2006; Levy, 2008a). Yet a system making optimal use of all available information might be expected to perform fully joint inference on sentence identity and structure given perceptual input, using linguistic knowledge both prospectively and retrospectively in drawing inferences as to how raw input should be segmented and recognized as a sequence of linguistic tokens, and about the degree to which each input token should be trusted during grammatical analysis. Formal models of such joint inference over uncertain input have been proposed (Levy, 2008b), and corroborative empirical evidence exists that strong coh</context>
<context position="2912" citStr="Levy (2008" startWordPosition="436" endWordPosition="437">, comprehenders might under some circumstances adopt a grammatical analysis inconsistent with the true raw input comprising a sentence they are presented with, but consistent with a slightly perturbed version of the input that has higher prior probability. If this is the case, then subsequent input strongly disconfirming this “hallucinated” garden-path analysis might be expected to induce the same effects as seen in classic cases of garden-path disambiguation traditionally studied in the psycholinguistic literature. We explore this prediction by extending the rational uncertain-input model of Levy (2008b), integrating it with SURPRISAL THEORY (Hale, 2001; Levy, 2008a), which successfully accounts for and quantifies traditional garden-path disambiguation effects; and by testing predictions of the extended model in a self-paced reading study. Section 2 reviews surprisal theory and how it accounts for traditional gardenpath effects. Section 3 provides background information on garden-path effects relevant to the current study, describes how we might hope to reveal comprehenders’ use of grammatical knowledge to revise beliefs about the identity of previous linguistic sur1055 Proceedings of the 4</context>
<context position="4392" citStr="Levy, 2008" startWordPosition="656" endWordPosition="657">ent, and informally outlines how such belief revisions might arise as a side effect in a general theory of rational comprehension under uncertain input. Section 4 defines and estimates parameters for a model instantiating the general theory, and describes the predictions of the model for the experiment described in Section 3 (along with the inference procedures required to determine those predictions). Section 5 reports the results of the experiment. Section 6 concludes. 2 Garden-path disambiguation under surprisal The SURPRISAL THEORY of incremental sentenceprocessing difficulty (Hale, 2001; Levy, 2008a) posits that the cognitive effort required to process a given word wi of a sentence in its context is given by the simple information-theoretic measure of the log of the inverse of the word’s conditional probability (also called its “surprisal” or “Shannon information content”) in its intra-sentential context w1,...,i−1 and extra-sentential context Ctxt: 1 Effort(wi) ∝ log P(wi|w1...i−1, Ctxt) (In the rest of this paper, we consider isolatedsentence comprehension and ignore Ctxt.) The theory derives empirical support not only from controlled experiments manipulating grammatical context but a</context>
<context position="10440" citStr="Levy (2008" startWordPosition="1608" endWordPosition="1609">pens in edited, published written English, so the conditional probability of the NP analysis should be close to zero.2 When uncertainty about surface input is introduced, however— due to visual noise, imperfect memory representations, and/or beliefs about possible speaker error— analyses come into play in which some parts of the true string are treated as if they were absent. In particular, because the two sentences are perceptual neighbors, the pre-disambiguation garden-path analysis of (2) may be entertained in (3). We can get a tighter handle on the effect of input uncertainty by extending Levy (2008b)’s analysis of the expected beliefs of a comprehender about the sequence of words constituting an input sentence to joint inference over both sentence identity and sentence structure. For a true sentence w∗ which yields perceptual input I, joint inference on sentence identity w and structure T marginalizing over I yields: PC(T,w|w*) = J PC (T, w |I, w*) PT (I |w*) dI I where PT(I|w∗) is the true model of noise (perceptual inputs derived from the true sentence) and PC(·) terms reflect the comprehender’s linguistic knowledge and beliefs about the noise processes intervening between intended se</context>
<context position="18444" citStr="Levy (2008" startWordPosition="2904" endWordPosition="2905">, and of course, or (ii) manner or temporal adjuncts. The handful of true locative PPs (5 in total) are all parentheticals intervening between the verb and a complement strongly selected by the verb (e.g., [VP means, in my country, homosexual]); none fulfill one of the verb’s thematic requirements. certain input is represented as a weighted finite-state automaton (WFSA), allowing us to represent the incremental inferences of the comprehender through intersection of the input WFSA with the PCFG above (Bar-Hillel et al., 1964; Nederhof and Satta, 2003, 2008). 4.1 Uncertain-input representations Levy (2008a) introduced the LEVENSHTEINDISTANCE KERNEL as a model of the average effect of noise in uncertain-input probabilistic sentence comprehension; this corresponds to term (ii) in our Equation (II). This kernel had a single noise parameter governing scaling of the cost of considering word substitutions, insertions, and deletions are considered, with the cost of a word substitution falling off exponentially with Levenshtein distance between the true word and the substituted word, and the cost of word insertion or deletion falling off exponentially with word length. The distribution over the infini</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Levy, R. (2008b). A noisy-channel model of rational human sentence comprehension under uncertain input. In Proceedings of the 13th Conference on Empirical Methods in Natural Language Processing, pages 234–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>G Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="17453" citStr="Levy and Andrew, 2006" startWordPosition="2750" endWordPosition="2753">with probabilities estimated from the parsed Brown corpus. Brown corpus (Kuˇcera and Francis, 1967; Marcus et al., 1994), covering sentence-initial subordinate clause and locative-inversion constructions.4,5 The non-terminal rewrite rules are shown in Table 1, along with their probabilities; of terminal rewrite rules for all words which either appear in the sentences to be parsed or appeared at least five times in the corpus, with probabilities estimated by relative frequency. As we describe in the following two sections, un4Rule counts were obtained using tgrep2/Tregex patterns (Rohde, 2005; Levy and Andrew, 2006); the probabilities given are relative frequency estimates. The patterns used can be found at http://idiom.ucsd.edu/˜rlevy/papers/ acl2011/tregex_patterns.txt. 5Similar to the case noted in Footnote 2, a small number of VP -&gt; V , PP ... rules can be found in the parsed Brown corpus. However, the PPs involved are overwhelmingly (i) set expressions, such as for example, in essence, and of course, or (ii) manner or temporal adjuncts. The handful of true locative PPs (5 in total) are all parentheticals intervening between the verb and a complement strongly selected by the verb (e.g., [VP means, in</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Levy, R. and Andrew, G. (2006). Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of the 2006 conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>K Bicknell</author>
<author>T Slattery</author>
<author>K Rayner</author>
</authors>
<title>Eye movement evidence that readers maintain and act on uncertainty about past linguistic input.</title>
<date>2009</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>106</volume>
<issue>50</issue>
<contexts>
<context position="2165" citStr="Levy et al., 2009" startWordPosition="320" endWordPosition="323">t, using linguistic knowledge both prospectively and retrospectively in drawing inferences as to how raw input should be segmented and recognized as a sequence of linguistic tokens, and about the degree to which each input token should be trusted during grammatical analysis. Formal models of such joint inference over uncertain input have been proposed (Levy, 2008b), and corroborative empirical evidence exists that strong coherence of current input with a perceptual neighbor of previous input may induce confusion in comprehenders as to the identity of that previous input (Connine et al., 1991; Levy et al., 2009). In this paper we explore a more dramatic prediction of such an uncertain-input theory: that, when faced with sufficiently biasing input, comprehenders might under some circumstances adopt a grammatical analysis inconsistent with the true raw input comprising a sentence they are presented with, but consistent with a slightly perturbed version of the input that has higher prior probability. If this is the case, then subsequent input strongly disconfirming this “hallucinated” garden-path analysis might be expected to induce the same effects as seen in classic cases of garden-path disambiguation</context>
</contexts>
<marker>Levy, Bicknell, Slattery, Rayner, 2009</marker>
<rawString>Levy, R., Bicknell, K., Slattery, T., and Rayner, K. (2009). Eye movement evidence that readers maintain and act on uncertainty about past linguistic input. Proceedings of the National Academy of Sciences, 106(50):21086–21090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="16951" citStr="Marcus et al., 1994" startWordPosition="2675" endWordPosition="2678"> 0.003257 S → SBAR S 0.012289 S → SBAR , S 0.041753 S → NP VP 0.942701 INVERTED → PP VBD 1.000000 SBAR → INSBAR S 1.000000 VP → VBD RB 0.002149 VP → VBD PP 0.202024 VP → VBD NP 0.393660 VP → VBD PP PP 0.028029 VP → VBD RP 0.005731 VP → VBD 0.222441 VP → VBD JJ 0.145966 PP → IN NP 1.000000 NP → DT NN 0.274566 NP → NNS 0.047505 NP → NNP 0.101198 NP → DT NNS 0.045082 NP → PRP 0.412192 NP → NN 0.119456 Table 1: A small PCFG (lexical rewrite rules omitted) covering the constructions used in (4)–(6), with probabilities estimated from the parsed Brown corpus. Brown corpus (Kuˇcera and Francis, 1967; Marcus et al., 1994), covering sentence-initial subordinate clause and locative-inversion constructions.4,5 The non-terminal rewrite rules are shown in Table 1, along with their probabilities; of terminal rewrite rules for all words which either appear in the sentences to be parsed or appeared at least five times in the corpus, with probabilities estimated by relative frequency. As we describe in the following two sections, un4Rule counts were obtained using tgrep2/Tregex patterns (Rohde, 2005; Levy and Andrew, 2006); the probabilities given are relative frequency estimates. The patterns used can be found at http</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1994). Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Mitchell</author>
</authors>
<title>An evaluation of subjectpaced reading tasks and other methods for investigating immediate processes in reading.</title>
<date>1984</date>
<editor>In Kieras, D. and Just, M. A., editors,</editor>
<publisher>Earlbaum.</publisher>
<location>Hillsdale, NJ:</location>
<contexts>
<context position="26591" citStr="Mitchell, 1984" startWordPosition="4256" endWordPosition="4257">erb is the same across condition (here reflecting only the uncertainty of verb identity for this small grammar). As input uncertainty increases, however, surprisal in the [Inverted, −PP] condition increases, reflecting the stronger belief given preceding context in an input-unfaithful interpretation. 5 Empirical results To test these predictions we conducted a word-byword self-paced reading study, in which participants read by pressing a button to reveal each successive word in a sentence; times between button presses are recorded and analyzed as an index of incremental processing difficulty (Mitchell, 1984). Forty monolingual native-English speaker participants read twenty-four sentence quadruplets (“items”) on the pattern of (4)–(6), with a Latinsquare design so that each participant saw an equal toward respectively (right-truncated here for reasons of space). Every sentence was followed by a yes/no comprehension question (e.g., Did the tank lurch toward an injured enemy combatant?); participants received feedback whenever they answered a question incorrectly. Reading-time results are shown in Figure 4. As can be seen, the model’s predictions are matched at the main-clause verb: reading times a</context>
</contexts>
<marker>Mitchell, 1984</marker>
<rawString>Mitchell, D. C. (1984). An evaluation of subjectpaced reading tasks and other methods for investigating immediate processes in reading. In Kieras, D. and Just, M. A., editors, New methods in reading comprehension. Hillsdale, NJ: Earlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Mitchell</author>
</authors>
<title>Lexical guidance in human parsing: Locus and processing characteristics.</title>
<date>1987</date>
<booktitle>Attention and Performance XII: The psychology of reading.</booktitle>
<editor>In Coltheart, M., editor,</editor>
<publisher>Erlbaum.</publisher>
<location>London:</location>
<contexts>
<context position="8703" citStr="Mitchell, 1987" startWordPosition="1325" endWordPosition="1326">ile Mary was mending the socks fell off her lap. In incremental comprehension, the phrase the socks is ambiguous between being the NP object of the preceding subordinate-clause verb mending versus being the subject of the main clause (in which case mending has a Zero object); in sentences like (2) the initial bias is toward the NP interpretation. The main-clause verb fell disambiguates, ruling out the initially favored NP analysis. It has been known since Frazier and Rayner (1982) that this effect of garden-path disambiguation can be measured in reading times on the main-clause verb (see also Mitchell, 1987; Ferreira and Henderson, 1993; Adams et al., 1998; Sturt et al., 1999; Hill and Murray, 2000; Christianson et al., 2001; van Gompel and Pickering, 2001; Tabor and Hutchins, 2004; Staub, 2007). Small changes to the context can have huge effects on comprehenders’ initial interpretations, however. It is unusual for sentenceinitial subordinate clauses not to end with a comma or some other type of punctuation (searches in the parsed Brown corpus put the rate at about 18%); empirically it has consistently been found that a comma eliminates the garden-path effect in NP/Z sentences: (3) While Mary wa</context>
</contexts>
<marker>Mitchell, 1987</marker>
<rawString>Mitchell, D. C. (1987). Lexical guidance in human parsing: Locus and processing characteristics. In Coltheart, M., editor, Attention and Performance XII: The psychology of reading. London: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>D Jurafsky</author>
</authors>
<title>Bayesian models of human sentence processing.</title>
<date>1998</date>
<booktitle>In Proceedings of the Twelfth Annual Meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="1339" citStr="Narayanan and Jurafsky, 1998" startWordPosition="190" endWordPosition="193">may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory. 1 Introduction In most formal theories of human sentence comprehension, input recognition and syntactic analysis are taken to be distinct processes, with the only feedback from syntax to recognition being prospective prediction of likely upcoming input (Jurafsky, 1996; Narayanan and Jurafsky, 1998, 2002; Hale, 2001, 2006; Levy, 2008a). Yet a system making optimal use of all available information might be expected to perform fully joint inference on sentence identity and structure given perceptual input, using linguistic knowledge both prospectively and retrospectively in drawing inferences as to how raw input should be segmented and recognized as a sequence of linguistic tokens, and about the degree to which each input token should be trusted during grammatical analysis. Formal models of such joint inference over uncertain input have been proposed (Levy, 2008b), and corroborative empir</context>
</contexts>
<marker>Narayanan, Jurafsky, 1998</marker>
<rawString>Narayanan, S. and Jurafsky, D. (1998). Bayesian models of human sentence processing. In Proceedings of the Twelfth Annual Meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>D Jurafsky</author>
</authors>
<title>A Bayesian model predicts human parse preference and reading time in sentence processing.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>14</volume>
<pages>59--65</pages>
<marker>Narayanan, Jurafsky, 2002</marker>
<rawString>Narayanan, S. and Jurafsky, D. (2002). A Bayesian model predicts human parse preference and reading time in sentence processing. In Advances in Neural Information Processing Systems, volume 14, pages 59–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Probabilistic parsing as intersection.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="18389" citStr="Nederhof and Satta, 2003" startWordPosition="2896" endWordPosition="2899">e overwhelmingly (i) set expressions, such as for example, in essence, and of course, or (ii) manner or temporal adjuncts. The handful of true locative PPs (5 in total) are all parentheticals intervening between the verb and a complement strongly selected by the verb (e.g., [VP means, in my country, homosexual]); none fulfill one of the verb’s thematic requirements. certain input is represented as a weighted finite-state automaton (WFSA), allowing us to represent the incremental inferences of the comprehender through intersection of the input WFSA with the PCFG above (Bar-Hillel et al., 1964; Nederhof and Satta, 2003, 2008). 4.1 Uncertain-input representations Levy (2008a) introduced the LEVENSHTEINDISTANCE KERNEL as a model of the average effect of noise in uncertain-input probabilistic sentence comprehension; this corresponds to term (ii) in our Equation (II). This kernel had a single noise parameter governing scaling of the cost of considering word substitutions, insertions, and deletions are considered, with the cost of a word substitution falling off exponentially with Levenshtein distance between the true word and the substituted word, and the cost of word insertion or deletion falling off exponenti</context>
<context position="22039" citStr="Nederhof and Satta, 2003" startWordPosition="3505" endWordPosition="3508"> 1.7 PCFGs are a special class of weighted context-free grammars (WCFGs), 6If a state’s total unnormalized cost of insertion arcs is α and that of deletion and insertion arcs is ,3, its normalizing constant is � 1��. Note that we must have α &lt; 1, placing a constraint on the value that -y can take (above which the normalizing constant diverges). 7Using the WFSA representation of average noise effects here actually involves one simplifying assumption, that the avwhich are closed under intersection with WFSAs; a constructive procedure exists for finding the intersection (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Hence we are left with finding the partition function of a WCFG, which cannot be computed exactly, but a number of approximation methods are known (Stolcke, 1995; Smith and Johnson, 2007; Nederhof and Satta, 2008). In practice, the computation required to compute the partition function under any of these methods increases with the size of the WCFG resulting from the intersection, which for a binarized PCFG with R rules and an n-state WFSA is Rn2. To increase efficiency we implemented what is to our knowledge a novel method for finding the minimal grammar including all rules that will have no</context>
</contexts>
<marker>Nederhof, Satta, 2003</marker>
<rawString>Nederhof, M.-J. and Satta, G. (2003). Probabilistic parsing as intersection. In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Computing partition functions of PCFGs.</title>
<date>2008</date>
<booktitle>Research on Logic and Computation,</booktitle>
<pages>6--139</pages>
<contexts>
<context position="22254" citStr="Nederhof and Satta, 2008" startWordPosition="3541" endWordPosition="3544">. Note that we must have α &lt; 1, placing a constraint on the value that -y can take (above which the normalizing constant diverges). 7Using the WFSA representation of average noise effects here actually involves one simplifying assumption, that the avwhich are closed under intersection with WFSAs; a constructive procedure exists for finding the intersection (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Hence we are left with finding the partition function of a WCFG, which cannot be computed exactly, but a number of approximation methods are known (Stolcke, 1995; Smith and Johnson, 2007; Nederhof and Satta, 2008). In practice, the computation required to compute the partition function under any of these methods increases with the size of the WCFG resulting from the intersection, which for a binarized PCFG with R rules and an n-state WFSA is Rn2. To increase efficiency we implemented what is to our knowledge a novel method for finding the minimal grammar including all rules that will have non-zero probability in the intersection. We first parse the WFSA bottom-up with the item-based method of Goodman (1999) in the Boolean semiring, storing partial results in a chart. After completion of this bottom-up </context>
<context position="23642" citStr="Nederhof and Satta (2008)" startWordPosition="3780" endWordPosition="3783"> chart will have non-zero probability, since some are not connected to the root. Hence we perform a second, topdown Boolean-semiring parsing pass on the bottomup chart, throwing out entries that cannot be derived from the root. We can then include in the intersection grammar only those rules from the classic construction that can be identified with a set of surviving entries in the final parse chart.8 The partition functions for each category in this intersection grammar can then be computed; we used a fixed-point method preceded by a topological sort on the grammar’s ruleset, as described by Nederhof and Satta (2008). To obtain the surprisal of the input deriving from a word wi in its context, we can thus comerage surprisal of Ii, or EP,[log PC (Ii |I1 ... i−1)] is well approximated by the log of the ratio of the expected probabilities of the noisy inputs I1...i_1 and I1...i, since as discussed in Section 3 the quantities P(I1...i_1) and P(I1...i) are expectations under the true noise distribution. This simplifying assumption has the advantage of bypassing commitment to a specific representation of perceptual input and should be justifiable for reasonable noise functions, but the issue is worth further sc</context>
</contexts>
<marker>Nederhof, Satta, 2008</marker>
<rawString>Nederhof, M.-J. and Satta, G. (2008). Computing partition functions of PCFGs. Research on Logic and Computation, 6:139–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>A Bachrach</author>
<author>C Cardenas</author>
<author>C Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="5143" citStr="Roark et al., 2009" startWordPosition="769" endWordPosition="772">ion-theoretic measure of the log of the inverse of the word’s conditional probability (also called its “surprisal” or “Shannon information content”) in its intra-sentential context w1,...,i−1 and extra-sentential context Ctxt: 1 Effort(wi) ∝ log P(wi|w1...i−1, Ctxt) (In the rest of this paper, we consider isolatedsentence comprehension and ignore Ctxt.) The theory derives empirical support not only from controlled experiments manipulating grammatical context but also from broad-coverage studies of reading times for naturalistic text (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009), including demonstration that the shape of the relationship between word probability and reading time is indeed log-linear (Smith and Levy, 2008). Surprisal has had considerable success in accounting for one of the best-known phenomena in psycholinguistics, the GARDEN-PATH SENTENCE (Frazier, 1979), in which a local ambiguity biases the comprehender’s incremental syntactic interpretation so strongly that upon encountering disambiguating input the correct interpretation can only be recovered with great effort, if at all. The most famous example is (1) below (Bever, 1970): (1) The horse raced pa</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Roark, B., Bachrach, A., Cardenas, C., and Pallier, C. (2009). Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rohde</author>
</authors>
<title>TGrep2 User Manual, version 1.15 edition.</title>
<date>2005</date>
<contexts>
<context position="17429" citStr="Rohde, 2005" startWordPosition="2748" endWordPosition="2749"> in (4)–(6), with probabilities estimated from the parsed Brown corpus. Brown corpus (Kuˇcera and Francis, 1967; Marcus et al., 1994), covering sentence-initial subordinate clause and locative-inversion constructions.4,5 The non-terminal rewrite rules are shown in Table 1, along with their probabilities; of terminal rewrite rules for all words which either appear in the sentences to be parsed or appeared at least five times in the corpus, with probabilities estimated by relative frequency. As we describe in the following two sections, un4Rule counts were obtained using tgrep2/Tregex patterns (Rohde, 2005; Levy and Andrew, 2006); the probabilities given are relative frequency estimates. The patterns used can be found at http://idiom.ucsd.edu/˜rlevy/papers/ acl2011/tregex_patterns.txt. 5Similar to the case noted in Footnote 2, a small number of VP -&gt; V , PP ... rules can be found in the parsed Brown corpus. However, the PPs involved are overwhelmingly (i) set expressions, such as for example, in essence, and of course, or (ii) manner or temporal adjuncts. The handful of true locative PPs (5 in total) are all parentheticals intervening between the verb and a complement strongly selected by the v</context>
</contexts>
<marker>Rohde, 2005</marker>
<rawString>Rohde, D. (2005). TGrep2 User Manual, version 1.15 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>M Johnson</author>
</authors>
<title>Weighted and probabilistic context-free grammars are equally expressive.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="22227" citStr="Smith and Johnson, 2007" startWordPosition="3537" endWordPosition="3540">alizing constant is � 1��. Note that we must have α &lt; 1, placing a constraint on the value that -y can take (above which the normalizing constant diverges). 7Using the WFSA representation of average noise effects here actually involves one simplifying assumption, that the avwhich are closed under intersection with WFSAs; a constructive procedure exists for finding the intersection (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Hence we are left with finding the partition function of a WCFG, which cannot be computed exactly, but a number of approximation methods are known (Stolcke, 1995; Smith and Johnson, 2007; Nederhof and Satta, 2008). In practice, the computation required to compute the partition function under any of these methods increases with the size of the WCFG resulting from the intersection, which for a binarized PCFG with R rules and an n-state WFSA is Rn2. To increase efficiency we implemented what is to our knowledge a novel method for finding the minimal grammar including all rules that will have non-zero probability in the intersection. We first parse the WFSA bottom-up with the item-based method of Goodman (1999) in the Boolean semiring, storing partial results in a chart. After co</context>
</contexts>
<marker>Smith, Johnson, 2007</marker>
<rawString>Smith, N. A. and Johnson, M. (2007). Weighted and probabilistic context-free grammars are equally expressive. Computational Linguistics, 33(4):477–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Smith</author>
<author>R Levy</author>
</authors>
<title>Optimal processing times in reading: a formal model and empirical investigation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="5289" citStr="Smith and Levy, 2008" startWordPosition="790" endWordPosition="793">nt”) in its intra-sentential context w1,...,i−1 and extra-sentential context Ctxt: 1 Effort(wi) ∝ log P(wi|w1...i−1, Ctxt) (In the rest of this paper, we consider isolatedsentence comprehension and ignore Ctxt.) The theory derives empirical support not only from controlled experiments manipulating grammatical context but also from broad-coverage studies of reading times for naturalistic text (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009), including demonstration that the shape of the relationship between word probability and reading time is indeed log-linear (Smith and Levy, 2008). Surprisal has had considerable success in accounting for one of the best-known phenomena in psycholinguistics, the GARDEN-PATH SENTENCE (Frazier, 1979), in which a local ambiguity biases the comprehender’s incremental syntactic interpretation so strongly that upon encountering disambiguating input the correct interpretation can only be recovered with great effort, if at all. The most famous example is (1) below (Bever, 1970): (1) The horse raced past the barn fell. where the context before the final word is strongly biased toward an interpretation where raced is the main verb of the sentence</context>
</contexts>
<marker>Smith, Levy, 2008</marker>
<rawString>Smith, N. J. and Levy, R. (2008). Optimal processing times in reading: a formal model and empirical investigation. In Proceedings of the 30th Annual Meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Staub</author>
</authors>
<title>The parser doesn’t ignore intransitivity, after all.</title>
<date>2007</date>
<journal>Journal of Experimental Psychology: Learning, Memory, &amp; Cognition,</journal>
<volume>33</volume>
<issue>3</issue>
<pages>569</pages>
<contexts>
<context position="8895" citStr="Staub, 2007" startWordPosition="1356" endWordPosition="1357">us being the subject of the main clause (in which case mending has a Zero object); in sentences like (2) the initial bias is toward the NP interpretation. The main-clause verb fell disambiguates, ruling out the initially favored NP analysis. It has been known since Frazier and Rayner (1982) that this effect of garden-path disambiguation can be measured in reading times on the main-clause verb (see also Mitchell, 1987; Ferreira and Henderson, 1993; Adams et al., 1998; Sturt et al., 1999; Hill and Murray, 2000; Christianson et al., 2001; van Gompel and Pickering, 2001; Tabor and Hutchins, 2004; Staub, 2007). Small changes to the context can have huge effects on comprehenders’ initial interpretations, however. It is unusual for sentenceinitial subordinate clauses not to end with a comma or some other type of punctuation (searches in the parsed Brown corpus put the rate at about 18%); empirically it has consistently been found that a comma eliminates the garden-path effect in NP/Z sentences: (3) While Mary was mending, the socks fell off her lap. Understanding sentences like (3) is intuitively much easier, and reading times at the disambiguating verb are reliably lower when compared with (2). Fodo</context>
</contexts>
<marker>Staub, 2007</marker>
<rawString>Staub, A. (2007). The parser doesn’t ignore intransitivity, after all. Journal of Experimental Psychology: Learning, Memory, &amp; Cognition, 33(3):550– 569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="22202" citStr="Stolcke, 1995" startWordPosition="3535" endWordPosition="3536">is ,3, its normalizing constant is � 1��. Note that we must have α &lt; 1, placing a constraint on the value that -y can take (above which the normalizing constant diverges). 7Using the WFSA representation of average noise effects here actually involves one simplifying assumption, that the avwhich are closed under intersection with WFSAs; a constructive procedure exists for finding the intersection (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Hence we are left with finding the partition function of a WCFG, which cannot be computed exactly, but a number of approximation methods are known (Stolcke, 1995; Smith and Johnson, 2007; Nederhof and Satta, 2008). In practice, the computation required to compute the partition function under any of these methods increases with the size of the WCFG resulting from the intersection, which for a binarized PCFG with R rules and an n-state WFSA is Rn2. To increase efficiency we implemented what is to our knowledge a novel method for finding the minimal grammar including all rules that will have non-zero probability in the intersection. We first parse the WFSA bottom-up with the item-based method of Goodman (1999) in the Boolean semiring, storing partial res</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Stolcke, A. (1995). An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sturt</author>
<author>M J Pickering</author>
<author>M W Crocker</author>
</authors>
<title>Structural change and reanalysis difficulty in language comprehension.</title>
<date>1999</date>
<journal>Journal of Memory and Language,</journal>
<pages>40--136</pages>
<contexts>
<context position="8773" citStr="Sturt et al., 1999" startWordPosition="1335" endWordPosition="1338">mprehension, the phrase the socks is ambiguous between being the NP object of the preceding subordinate-clause verb mending versus being the subject of the main clause (in which case mending has a Zero object); in sentences like (2) the initial bias is toward the NP interpretation. The main-clause verb fell disambiguates, ruling out the initially favored NP analysis. It has been known since Frazier and Rayner (1982) that this effect of garden-path disambiguation can be measured in reading times on the main-clause verb (see also Mitchell, 1987; Ferreira and Henderson, 1993; Adams et al., 1998; Sturt et al., 1999; Hill and Murray, 2000; Christianson et al., 2001; van Gompel and Pickering, 2001; Tabor and Hutchins, 2004; Staub, 2007). Small changes to the context can have huge effects on comprehenders’ initial interpretations, however. It is unusual for sentenceinitial subordinate clauses not to end with a comma or some other type of punctuation (searches in the parsed Brown corpus put the rate at about 18%); empirically it has consistently been found that a comma eliminates the garden-path effect in NP/Z sentences: (3) While Mary was mending, the socks fell off her lap. Understanding sentences like (3</context>
</contexts>
<marker>Sturt, Pickering, Crocker, 1999</marker>
<rawString>Sturt, P., Pickering, M. J., and Crocker, M. W. (1999). Structural change and reanalysis difficulty in language comprehension. Journal of Memory and Language, 40:136–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Tabor</author>
<author>S Hutchins</author>
</authors>
<title>Evidence for self-organized sentence processing: Digging in effects.</title>
<date>2004</date>
<journal>Journal of Experimental Psychology: Learning, Memory, &amp; Cognition,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="8881" citStr="Tabor and Hutchins, 2004" startWordPosition="1352" endWordPosition="1355">e-clause verb mending versus being the subject of the main clause (in which case mending has a Zero object); in sentences like (2) the initial bias is toward the NP interpretation. The main-clause verb fell disambiguates, ruling out the initially favored NP analysis. It has been known since Frazier and Rayner (1982) that this effect of garden-path disambiguation can be measured in reading times on the main-clause verb (see also Mitchell, 1987; Ferreira and Henderson, 1993; Adams et al., 1998; Sturt et al., 1999; Hill and Murray, 2000; Christianson et al., 2001; van Gompel and Pickering, 2001; Tabor and Hutchins, 2004; Staub, 2007). Small changes to the context can have huge effects on comprehenders’ initial interpretations, however. It is unusual for sentenceinitial subordinate clauses not to end with a comma or some other type of punctuation (searches in the parsed Brown corpus put the rate at about 18%); empirically it has consistently been found that a comma eliminates the garden-path effect in NP/Z sentences: (3) While Mary was mending, the socks fell off her lap. Understanding sentences like (3) is intuitively much easier, and reading times at the disambiguating verb are reliably lower when compared </context>
</contexts>
<marker>Tabor, Hutchins, 2004</marker>
<rawString>Tabor, W. and Hutchins, S. (2004). Evidence for self-organized sentence processing: Digging in effects. Journal of Experimental Psychology: Learning, Memory, &amp; Cognition, 30(2):431–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R P G van Gompel</author>
<author>M J Pickering</author>
</authors>
<title>Lexical guidance in sentence processing: A note on Adams,</title>
<date>2001</date>
<journal>Psychonomic Bulletin &amp; Review,</journal>
<volume>8</volume>
<issue>4</issue>
<location>Clifton, and Mitchell</location>
<marker>van Gompel, Pickering, 2001</marker>
<rawString>van Gompel, R. P. G. and Pickering, M. J. (2001). Lexical guidance in sentence processing: A note on Adams, Clifton, and Mitchell (1998). Psychonomic Bulletin &amp; Review, 8(4):851–857.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>