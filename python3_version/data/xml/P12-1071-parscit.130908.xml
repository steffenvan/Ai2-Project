<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9984735">
Exploiting Multiple Treebanks for Parsing with Quasi-synchronous
Grammars
</title>
<author confidence="0.999514">
Zhenghua Li, Ting Liu∗, Wanxiang Che
</author>
<affiliation confidence="0.997112333333333">
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
</affiliation>
<email confidence="0.980287">
{lzh,tliu,car}@ir.hit.edu.cn
</email>
<note confidence="0.681144666666667">
Abstract Treebanks # of Words Grammar
CTB5 0.51 million Phrase structure
CTB6 0.78 million Phrase structure
1.11 million
0.36 million
about 1 million
CDT Dependency structure
Sinica Phrase structure
TCT Phrase structure
</note>
<tableCaption confidence="0.983846">
Table 1: Several publicly available Chinese treebanks.
</tableCaption>
<bodyText confidence="0.992091736842105">
We present a simple and effective framework
for exploiting multiple monolingual treebanks
with different annotation guidelines for pars-
ing. Several types of transformation patterns
(TP) are designed to capture the systematic an-
notation inconsistencies among different tree-
banks. Based on such TPs, we design quasi-
synchronous grammar features to augment the
baseline parsing models. Our approach can
significantly advance the state-of-the-art pars-
ing accuracy on two widely used target tree-
banks (Penn Chinese Treebank 5.1 and 6.0)
using the Chinese Dependency Treebank as
the source treebank. The improvements are
respectively 1.37% and 1.10% with automatic
part-of-speech tags. Moreover, an indirect
comparison indicates that our approach also
outperforms previous work based on treebank
conversion.
</bodyText>
<sectionHeader confidence="0.999502" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999494">
The scale of available labeled data significantly af-
fects the performance of statistical data-driven mod-
els. As a structural classification problem that is
more challenging than binary classification and se-
quence labeling problems, syntactic parsing is more
prone to suffer from the data sparseness problem.
However, the heavy cost of treebanking typically
limits one single treebank in both scale and genre.
At present, learning from one single treebank seems
inadequate for further boosting parsing accuracy.1
</bodyText>
<footnote confidence="0.8979605">
*Correspondence author: tliu@ir.hit.edu.cn
1Incorporating an increased number of global features, such
as third-order features in graph-based parsers, slightly affects
parsing accuracy (Koo and Collins, 2010; Li et al., 2011).
</footnote>
<bodyText confidence="0.99997376">
Therefore, studies have recently resorted to other re-
sources for the enhancement of parsing models, such
as large-scale unlabeled data (Koo et al., 2008; Chen
et al., 2009; Bansal and Klein, 2011; Zhou et al.,
2011), and bilingual texts or cross-lingual treebanks
(Burkett and Klein, 2008; Huang et al., 2009; Bur-
kett et al., 2010; Chen et al., 2010).
The existence of multiple monolingual treebanks
opens another door for this issue. For example, ta-
ble 1 lists a few publicly available Chinese treebanks
that are motivated by different linguistic theories or
applications. In the current paper, we utilize the
first three treebanks, i.e., the Chinese Penn Tree-
bank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005),
and the Chinese Dependency Treebank (CDT) (Liu
et al., 2006). The Sinica treebank (Chen et al., 2003)
and the Tsinghua Chinese Treebank (TCT) (Qiang,
2004) can be similarly exploited with our proposed
approach, which we leave as future work.
Despite the divergence of annotation philosophy,
these treebanks contain rich human knowledge on
the Chinese syntax, thereby having a great deal of
common ground. Therefore, exploiting multiple
treebanks is very attractive for boosting parsing ac-
curacy. Figure 1 gives an example with different an-
</bodyText>
<page confidence="0.994648">
675
</page>
<note confidence="0.9925355">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 675–684,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998328">
Figure 1: Example with annotations from CTB5 (upper)
and CDT (under).
</figureCaption>
<bodyText confidence="0.999006696969697">
notations from CTB5 and CDT.2 This example illus-
trates that the two treebanks annotate coordination
constructions differently. In CTB5, the last noun is
the head, whereas the first noun is the head in CDT.
One natural idea for multiple treebank exploita-
tion is treebank conversion. First, the annotations
in the source treebank are converted into the style
of the target treebank. Then, both the converted
treebank and the target treebank are combined. Fi-
nally, the combined treebank are used to train a
better parser. However, the inconsistencies among
different treebanks are normally nontrivial, which
makes rule-based conversion infeasible. For exam-
ple, a number of inconsistencies between CTB5 and
CDT are lexicon-sensitive, that is, they adopt dif-
ferent annotations for some particular lexicons (or
word senses). Niu et al. (2009) use sophisticated
strategies to reduce the noises of the converted tree-
bank after automatic treebank conversion.
The present paper proposes a simple and effective
framework for this problem. The proposed frame-
work avoids directly addressing the difficult anno-
tation transformation problem, but focuses on mod-
eling the annotation inconsistencies using transfor-
mation patterns (TP). The TPs are used to compose
quasi-synchronous grammar (QG) features, such
that the knowledge of the source treebank can in-
spire the target parser to build better trees. We con-
duct extensive experiments using CDT as the source
treebank to enhance two target treebanks (CTB5 and
CTB6). Results show that our approach can signifi-
cantly boost state-of-the-art parsing accuracy. More-
over, an indirect comparison indicates that our ap-
</bodyText>
<footnote confidence="0.9988212">
2CTB5 is converted to dependency structures following the
standard practice of dependency parsing (Zhang and Clark,
2008b). Notably, converting a phrase-structure tree into its
dependency-structure counterpart is straightforward and can be
performed by applying heuristic head-finding rules.
</footnote>
<bodyText confidence="0.9605855">
proach also outperforms the treebank conversion ap-
proach of Niu et al. (2009).
</bodyText>
<sectionHeader confidence="0.999778" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999990954545455">
The present work is primarily inspired by Jiang et
al. (2009) and Smith and Eisner (2009). Jiang et al.
(2009) improve the performance of word segmen-
tation and part-of-speech (POS) tagging on CTB5
using another large-scale corpus of different annota-
tion standards (People’s Daily). Their framework is
similar to ours. However, handling syntactic anno-
tation inconsistencies is significantly more challeng-
ing in our case of parsing. Smith and Eisner (2009)
propose effective QG features for parser adaptation
and projection. The first part of their work is closely
connected with our work, but with a few impor-
tant differences. First, they conduct simulated ex-
periments on one treebank by manually creating a
few trivial annotation inconsistencies based on two
heuristic rules. They then focus on better adapting a
parser to a new annotation style with few sentences
of the target style. In contrast, we experiment with
two real large-scale treebanks, and boost the state-
of-the-art parsing accuracy using QG features. Sec-
ond, we explore much richer QG features to fully
exploit the knowledge of the source treebank. These
features are tailored to the dependency parsing prob-
lem. In summary, the present work makes substan-
tial progress in modeling structural annotation in-
consistencies with QG features for parsing.
Previous work on treebank conversion primar-
ily focuses on converting one grammar formalism
of a treebank into another and then conducting a
study on the converted treebank (Collins et al., 1999;
Xia et al., 2008). The work by Niu et al. (2009)
is, to our knowledge, the only study to date that
combines the converted treebank with the existing
target treebank. They automatically convert the
dependency-structure CDT into the phrase-structure
style of CTB5 using a statistical constituency parser
trained on CTB5. Their experiments show that
the combined treebank can significantly improve
the performance of constituency parsers. However,
their method requires several sophisticated strate-
gies, such as corpus weighting and score interpo-
lation, to reduce the influence of conversion errors.
Instead of using the noisy converted treebank as ad-
ditional training data, our approach allows the QG-
</bodyText>
<figure confidence="0.791521166666667">
promote trade and industry
V n c n
ROO
WO
1FC391 91X112 *fl3 T-R4
vv NN cc NN
OBJ
NMOD
NMOD
ROO
LAD
vOB cOO
</figure>
<page confidence="0.997052">
676
</page>
<bodyText confidence="0.995650833333333">
enhanced parsing models to softly learn the system-
atic inconsistencies based on QG features, making
our approach simpler and more robust.
Our approach is also intuitively related to stacked
learning (SL), a machine learning framework that
has recently been applied to dependency parsing
to integrate two main-stream parsing models, i.e.,
graph-based and transition-based models (Nivre and
McDonald, 2008; Martins et al., 2008). However,
the SL framework trains two parsers on the same
treebank and therefore does not need to consider the
problem of annotation inconsistencies.
</bodyText>
<sectionHeader confidence="0.991664" genericHeader="method">
3 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.952254258064516">
Given an input sentence x = w0w,...wn and its POS
tag sequence t = t0t,...tn, the goal of dependency
parsing is to build a dependency tree as depicted in
Figure 1, denoted by d = {(h, m, l) : 0 &lt; h &lt;
n, 0 &lt; m &lt; n, l C G}, where (h, m, l) indicates an
directed arc from the head word (also called father)
wh to the modifier (also called child or dependent)
wm with a dependency label l, and G is the label set.
We omit the label l because we focus on unlabeled
dependency parsing in the present paper. The artifi-
cial node w0, which always points to the root of the
sentence, is used to simplify the formalizations.
In the current research, we adopt the graph-based
parsing models for their state-of-the-art performance
in a variety of languages.3 Graph-based models
view the problem as finding the highest scoring tree
from a directed graph. To guarantee the efficiency of
the decoding algorithms, the score of a dependency
tree is factored into the scores of some small parts
(subtrees).
Scorebs(x, t, d) = wbs &apos; fbs(x, t, d)
E= wpart &apos; fpart(x, t, p)
p⊆d
where p is a scoring part which contains one or more
dependencies of d, and fbs(.) denotes the basic pars-
ing features, as opposed to the QG features. Figure
2 lists the scoring parts used in our work, where g,
h, m, and s, are word indices.
We implement three parsing models of varying
strengths in capturing features to better understand
the effect of the proposed QG features.
</bodyText>
<footnote confidence="0.8482595">
3Our approach can equally be applied to transition-based
parsing models (Yamada and Matsumoto, 2003; Nivre, 2003)
with minor modifications.
dependency sibling grandparent
</footnote>
<figureCaption confidence="0.942988">
Figure 2: Scoring parts used in our graph-based parsing
models.
</figureCaption>
<listItem confidence="0.997724">
• The first-order model (O1) only incorporates
dependency parts (McDonald et al., 2005), and
requires O(n3) parsing time.
• The second-order model using only sibling
parts (O2sib) includes both dependency and
sibling parts (McDonald and Pereira, 2006),
and needs O(n3) parsing time.
• The second-order model (O2) uses all the
scoring parts in Figure 2 (Koo and Collins,
2010). The time complexity of the decoding
algorithm is O(n4).4
</listItem>
<bodyText confidence="0.824871">
For the O2 model, the score function is rewritten as:
</bodyText>
<equation confidence="0.973897166666667">
EScorebs(x, t, d) = wdep &apos; fdep(x, t, h, m)
{(h,m)}⊆d
+ E wsib &apos; fsib(x, t, h, s, m)
{(h,s),(h,m)}⊆d
+ E wgrd &apos; fgrd(x, t, g, h, m)
{(g,h),(h,m)}⊆d
</equation>
<bodyText confidence="0.9999736">
where fdep(.), fsib(.) and fgrd(.) correspond to the
features for the three kinds of scoring parts. We
adopt the standard features following Li et al.
(2011). For the O1 and O2sib models, the above
formula is modified by deactivating the extra parts.
</bodyText>
<sectionHeader confidence="0.993871" genericHeader="method">
4 Dependency Parsing with QG Features
</sectionHeader>
<bodyText confidence="0.88991325">
Smith and Eisner (2006) propose the QG for ma-
chine translation (MT) problems, allowing greater
syntactic divergences between the two languages.
Given a source sentence x′ and its syntactic tree
d′, a QG defines a monolingual grammar that gen-
erates translations of x′, which can be denoted by
p(x, d, a|x′, d′), where x and d refer to a translation
and its parse, and a is a cross-language alignment.
Under a QG, any portion of d can be aligned to any
4We use the coarse-to-fine strategy to prune the search
space, which largely accelerates the decoding procedure (Koo
and Collins, 2010).
</bodyText>
<figure confidence="0.962529791208791">
g
h
M
M
h
s
h
M
677
ψ8,(d &apos;,g,h,m) ⇒
ψ,;b (d&apos;, h, s, m) ⇒
ψd,,(d&apos;, h, m)⇒
g
Target Side Syntactic Structures ofthe Corresponding Source Side
h
h
s
h
In
In
In
Consistent: 55.4%
g
h
In
h
s
28.2%
30.1%
h
In
In
h
s
h
Grand: 11.7%
h
6.7%
6.5%
g
i
i
m
In
In
i
In
i
In
6.2%
6.4%
Sibling. 10.0%
i
h m
h
g
h
s
h
In
h
6.1%
4.9%
Reverse: 8.6%
In
s
g
i
h
In
g
h
5.4%
4.4%
In
i
s
In
Reverse-grand: 1.4%
h
m
i
g
h
h
5.3%
i
4.2%
In
h m
s
</figure>
<figureCaption confidence="0.9885628">
Figure 4: Most frequent transformation patterns (TPs) when using CDT as the source treebank and CTB5 as the
target. A TP comprises two syntactic structures, one in the source side and the other in the target side, and denotes
the process by which the left-side subtree is transformed into the right-side structure. Functions Wde.(.), Wsib(.), and
Wgrd(.) return the specific TP type for a candidate scoring part according to the source tree d′.
Figure 3: Framework of our approach.
</figureCaption>
<bodyText confidence="0.988260555555555">
portion of d′, and the construction of d can be in-
spired by arbitrary substructures of d′. To date, QGs
have been successfully applied to various tasks, such
as word alignment (Smith and Eisner, 2006), ma-
chine translation (Gimpel and Smith, 2011), ques-
tion answering (Wang et al., 2007), and sentence
simplification (Woodsend and Lapata, 2011).
In the present work, we utilize the idea of the QG
for the exploitation of multiple monolingual tree-
banks. The key idea is to let the parse tree of one
style inspire the parsing process of another style.
Different from a MT process, our problem consid-
ers one single sentence (x = x′), and the alignment
a is trivial. Figure 3 shows the framework of our
approach. First, we train a statistical parser on the
source treebank, which is called the source parser.
The source parser is then used to parse the whole tar-
get treebank. At this point, the target treebank con-
tains two sets of annotations, one conforming to the
source style, and the other conforming to the target
style. During both the training and test phases, the
target parser are inspired by the source annotations,
and the score of a target dependency tree becomes
Score(x, t, d′, d) =Scorebs(x, t, d)
+Scoreqg(x, t, d′, d)
The first part corresponds to the baseline model,
whereas the second part is affected by the source tree
d′ and can be rewritten as
Scoreqg(x, t, d′, d) = wqg · fqg(x, t, d′, d)
where fqg(.) denotes the QGfeatures. We expect the
QG features to encourage or penalize certain scor-
ing parts in the target side according to the source
tree d′. Taking Figure 1 as an example, suppose
that the upper structure is the target. The target
parser can raise the score of the candidate depen-
dence “and” ← “industry”, because the depen-
</bodyText>
<figure confidence="0.997891368421053">
Source Treebank
s {(xi, di)}i
Train
Target
Treebank
T-((xj, dj)lj
Parse
Source Parser
Parsers
Out
Parsed
Treebank
Ts={(xj� dj)lj
Target Treebank with
Source Annotations
T&apos;s={(xj, dj s, dj)}j
Train
Target Parser
ParserT
</figure>
<bodyText confidence="0.999908282608695">
dency also appears in the source structure, and ev-
idence in the training data shows that both annota-
tion styles handle conjunctions in the same manner.
Similarly, the parser may add weight to “trade” ←
“industry”, considering that the reverse arc is in
the source structure. Therefore, the QG-enhanced
model must learn the systematic consistencies and
inconsistencies from the training data.
To model such consistency or inconsistency sys-
tematicness, we propose the use of TPs for encoding
the structural correspondence between the source
and target styles. Figure 4 presents the three kinds
of TPs used in our model, which correspond to the
three scoring parts of our parsing models.
Dependency TPs shown in the first row consider
how one dependency in the target side is trans-
formed in the source annotations. We only consider
the five cases shown in the figure. The percentages
in the lower boxes refer to the proportion of the
corresponding pattern, which are counted from the
training data of the target treebank with source anno-
tations T+S. We can see that the noisy source struc-
tures and the gold-standard target structures have
55.4% common dependencies. If the source struc-
ture does not belong to any of the listed five cases,
Odep(d′, h, m) returns “else” (12.9%). We could
consider more complex structures, such as h being
the grand grand father of m, but statistics show that
more complex transformations become very scarce
in the training data.
For the reason that dependency TPs can only
model how one dependency in the target structure is
transformed, we consider more complex transforma-
tions for the other two kinds of scoring parts of the
target parser, i.e., the sibling and grand TPs shown
in the bottom two rows. We only use high-frequency
TPs of a proportion larger than 1.0%, aggregate oth-
ers as “else”, which leaves us with 21 sibling TPs
and 22 grand TPs.
Based on these TPs, we propose the QG fea-
tures for enhancing the baseline parsing models,
which are shown in Table 2. The type of the
TP is conjoined with the related words and POS
tags, such that the QG-enhanced parsing models can
make more elaborate decisions based on the context.
Then, the score contributed by the QG features can
</bodyText>
<equation confidence="0.920426125">
be redefined as
Scoreqg(x, t, d′, d) _
E wqg-dep · fqg-dep(x, t, d′, h, m)
{(h,m)}⊆d
+ � wqg-sib · fqg-sib(x, t, d′, h, s, m)
{(h,s),(h,m)}⊆d
+ � wqg-grd · fqg-grd(x, t, d′, 9, h, m)
{(g,h),(h,m)}⊆d
</equation>
<bodyText confidence="0.999875">
which resembles the baseline model and can be nat-
urally handled by the decoding algorithms.
</bodyText>
<sectionHeader confidence="0.997754" genericHeader="evaluation">
5 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.998797516129032">
We use the CDT as the source treebank (Liu et
al., 2006). CDT consists of 60,000 sentences from
the People’s Daily in 1990s. For the target tree-
bank, we use two widely used versions of Penn Chi-
nese Treebank, i.e., CTB5 and CTB6, which con-
sist of Xinhua newswire, Hong Kong news and ar-
ticles from Sinarama news magazine (Xue et al.,
2005). To facilitate comparison with previous re-
sults, we follow Zhang and Clark (2008b) for data
split and constituency-to-dependency conversion of
CTB5. CTB6 is used as the Chinese data set in the
CoNLL 2009 shared task (Hajiˇc et al., 2009). There-
fore, we adopt the same setting.
CDT and CTB5/6 adopt different POS tag sets,
and converting from one tag set to another is difficult
(Niu et al., 2009).5 To overcome this problem, we
use the People’s Daily corpus (PD),6 a large-scale
corpus annotated with word segmentation and POS
tags, to train a statistical POS tagger. The tagger
produces a universal layer of POS tags for both the
source and target treebanks. Based on the common
tags, the source parser projects the source annota-
tions into the target treebanks. PD comprises ap-
proximately 300 thousand sentences of with approx-
imately 7 million words from the first half of 1998
of People’s Daily.
Table 3 summarizes the data sets used in the
present work. CTB5X is the same with CTB5 but
follows the data split of Niu et al. (2009). We use
CTB5X to compare our approach with their treebank
conversion method (see Table 9).
</bodyText>
<footnote confidence="0.99538825">
5The word segmentation standards of the two treebanks also
slightly differs, which are not considered in this work.
6http://icl.pku.edu.cn/icl_groups/
corpustagging.asp
</footnote>
<page confidence="0.987807">
679
</page>
<equation confidence="0.57501725">
fqg-dep(x, t, d′, h, m) fqg-sib(x, t, d′, h, s, m) fqg-grd(x, t, d′, g, h, m)
⊕dir(h, m) ◦ dist(h, m) ⊕dir(h, m) ⊕dir(h, m) ◦ dir(g, h)
ψdep(d′, h, m) ◦ th ◦ tm ψsib(d′, h, s, m) ◦ th ◦ ts ◦ tm ψgrd(d′, g, h, m) ◦ tg ◦ th ◦ tm
ψdep(d′, h, m) ◦ wh ◦ tm ψsib(d′, h, s, m) ◦ wh ◦ ts ◦ tm ψgrd(d′, g, h, m) ◦ wg ◦ th ◦ tm
ψdep(d′, h, m) ◦ th ◦ wm ψsib(d′, h, s, m) ◦ th ◦ ws ◦ tm ψgrd(d′, g, h, m) ◦ tg ◦ wh ◦ tm
ψdep(d′, h, m) ◦ wh ◦ wm ψsib(d′, h, s, m) ◦ th ◦ ts ◦ wm ψgrd(d′, g, h, m) ◦ tg ◦ th ◦ wm
ψsib(d′, h, s, m) ◦ ts ◦ tm
ψgrd(d′, g, h, m) ◦ tg ◦ tm
</equation>
<tableCaption confidence="0.996058666666667">
Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency
(h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) ◦ dist(h, m) indicates that the features listed in the
corresponding column are also conjoined with dir(h, m) ◦ dist(h, m) to form new features.
</tableCaption>
<table confidence="0.998631333333333">
Corpus Train Dev Test
PD 281,311 5,000 10,000
CDT 55,500 1,500 3,000
CTB5 16,091 803 1,910
CTB5X 18,104 352 348
CTB6 22,277 1,762 2,556
</table>
<tableCaption confidence="0.999932">
Table 3: Data used in this work (in sentence number).
</tableCaption>
<bodyText confidence="0.999956272727273">
We adopt unlabeled attachment score (UAS) as
the primary evaluation metric. We also use Root ac-
curacy (RA) and complete match rate (CM) to give
more insights. All metrics exclude punctuation. We
adopt Dan Bikel’s randomized parsing evaluation
comparator for significance test (Noreen, 1989).7
For all models used in current work (POS tagging
and parsing), we adopt averaged perceptron to train
the feature weights (Collins, 2002). We train each
model for 10 iterations and select the parameters that
perform best on the development set.
</bodyText>
<subsectionHeader confidence="0.942787">
5.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.999972727272727">
This subsection describes how we project the source
annotations into the target treebanks. First, we train
a statistical POS tagger on the training set of PD,
which we name TaggerPD.8 The tagging accuracy
on the test set of PD is 98.30%.
We then use TaggerPD to produce POS tags for
all the treebanks (CDT, CTB5, and CTB6).
Based on the common POS tags, we train a
second-order source parser (O2) on CDT, denoted
by ParserCDT .The UAS on CDT-test is 84.45%.
We then use ParserCDT to parse CTB5 and CTB6.
</bodyText>
<footnote confidence="0.98780625">
7http://www.cis.upenn.edu/[normal-wave˜]
dbikel/software.html
8We adopt the Chinese-oriented POS tagging features pro-
posed in Zhang and Clark (2008a).
</footnote>
<table confidence="0.996167666666667">
Models without QG with QG
O2 86.13 86.44 (+0.31, p = 0.06)
O2sib 85.63 86.17 (+0.54, p = 0.003)
O1 83.16 84.40 (+1.24, p &lt; 10−5)
Li11 86.18 —
Z&amp;N11 86.00 —
</table>
<tableCaption confidence="0.981169">
Table 4: Parsing accuracy (UAS) comparison on CTB5-
</tableCaption>
<figureCaption confidence="0.675739">
test with gold-standard POS tags. Li11 refers to the
second-order graph-based model of Li et al. (2011),
whereas Z&amp;N11 is the feature-rich transition-based
model of Zhang and Nivre (2011).
</figureCaption>
<bodyText confidence="0.9999085">
At this point, both CTB5 and CTB6 contain depen-
dency structures conforming to the style of CDT.
</bodyText>
<subsectionHeader confidence="0.958276">
5.2 CTB5 as the Target Treebank
</subsectionHeader>
<bodyText confidence="0.999966904761905">
Table 4 shows the results when the gold-standard
POS tags of CTB5 are adopted by the parsing mod-
els. We aim to analyze the efficacy of QG features
under the ideal scenario wherein the parsing mod-
els suffer from no error propagation of POS tag-
ging. We determine that our baseline O2 model
achieves comparable accuracy with the state-of-the-
art parsers. We also find that QG features can
boost the parsing accuracy by a large margin when
the baseline parser is weak (O1). The improve-
ment shrinks for stronger baselines (O2sib and O2).
This phenomenon is understandable. When gold-
standard POS tags are available, the baseline fea-
tures are very reliable and the QG features becomes
less helpful for more complex models. The p-values
in parentheses present the statistical significance of
the improvements.
We then turn to the more realistic scenario
wherein the gold-standard POS tags of the target
treebank are unavailable. We train a POS tagger on
the training set of CTB5 to produce the automatic
</bodyText>
<page confidence="0.995405">
680
</page>
<table confidence="0.999078">
Models without QG with QG
O2 79.67 81.04 (+1.37)
O2sib 79.25 80.45 (+1.20)
O1 76.73 79.04 (+2.31)
Li11 joint 80.79 —
Li11 pipeline 79.29 —
</table>
<tableCaption confidence="0.999551666666667">
Table 5: Parsing accuracy (UAS) comparison on CTB5-
test with automatic POS tags. The improvements shown
in parentheses are all statistically significant (p &lt; 10−5).
</tableCaption>
<table confidence="0.999855571428571">
UAS CM RA
79.67 26.81 73.82
79.15 26.34 74.71
81.04 29.63 77.17
80.82 28.80 76.28
80.86 28.48 76.18
80.88 28.90 76.34
</table>
<tableCaption confidence="0.9920925">
Table 6: Feature ablation for Parser-O2 on CTB5-test
with automatic POS tags.
</tableCaption>
<bodyText confidence="0.999931666666667">
POS tags for the development and test sets of CTB5.
The tagging accuracy is 93.88% on the test set. The
automatic POS tags of the training set are produced
using 10-fold cross-validation.9
Table 5 shows the results. We find that QG fea-
tures result in a surprisingly large improvement over
the O1 baseline and can also boost the state-of-
the-art parsing accuracy by a large margin. Li et
al. (2011) show that a joint POS tagging and de-
pendency parsing model can significantly improve
parsing accuracy over a pipeline model. Our QG-
enhanced parser outperforms their best joint model
by 0.25%. Moreover, the QG features can be used to
enhance a joint model and achieve higher accuracy,
which we leave as future work.
</bodyText>
<subsectionHeader confidence="0.998445">
5.3 Analysis Using Parser-O2 with AUTO-POS
</subsectionHeader>
<bodyText confidence="0.989882633333334">
We then try to gain more insights into the effect of
the QG features through detailed analysis. We se-
lect the state-of-the-art O2 parser and focus on the
realistic scenario with automatic POS tags.
Table 6 compares the efficacy of different feature
sets. The first major row analyzes the efficacy of
9We could use the POS tags produced by TaggerPD in Sec-
tion 5.1, which however would make it difficult to compare our
results with previous ones. Moreover, inferior results may be
gained due to the differences between CTB5 and PD in word
segmentation standards and text sources.
the basic features fbs(.) and the QG features fqg(.).
When using the few QG features in Table 2, the ac-
curacy is very close to that when using the basic
features. Moreover, using both features generates
a large improvement. The second major row com-
pares the efficacy of the three kinds of QG features
corresponding to the three types of scoring parts. We
can see that the three feature sets are similarly effec-
tive and yield comparable accuracies. Combining
these features generate an additional improvement
of approximately 0.2%. These results again demon-
strate that all the proposed QG features are effective.
Figure 5 describes how the performance varies
when the scale of CTB5 and CDT changes. In
the left subfigure, the parsers are trained on part
of the CTB5-train, and “16” indicates the use of
all the training instances. Meanwhile, the source
parser ParserCDT is trained on the whole CDT-
train. We can see that QG features render larger
improvement when the target treebank is of smaller
scale, which is quite reasonable. More importantly,
the curves indicate that a QG-enhanced parser
trained on a target treebank of 16,000 sentences
may achieve comparable accuracy with a base-
line parser trained on a treebank that is double
the size (32,000), which is very encouraging.
In the right subfigure, the target treebank is
trained on the whole CTB5-train, whereas the source
parser is trained on part of the CDT-train, and “55.5”
indicates the use of all. The curve clearly demon-
strates that the QG features are more helpful when
the source treebank gets larger, which can be ex-
plained as follows. A larger source treebank can
teach a source parser of higher accuracy; then, the
better source parser can parse the target treebank
more reliably; and finally, the target parser can better
learn the annotation divergences based on QG fea-
tures. These results demonstrate the effectiveness
and stability of our approach.
Table 7 presents the detailed effect of the QG fea-
tures on different dependency patterns. A pattern
“VV → NN” refers to a right-directed dependency
with the head tagged as “VV” and the modifier
tagged as “NN”. whereas “←” means left-directed.
The “w/o QG” column shows the number of the cor-
responding dependency pattern that appears in the
gold-standard trees but misses in the results of the
baseline parser, whereas the signed figures in the
“+QG” column are the changes made by the QG-
</bodyText>
<equation confidence="0.857633142857143">
Setting
fbs(.)
fqg(.)
fbs(.) + fqg(.)
fbs(.) + fqg-dep(.)
fbs(.) + fqg-sib(.)
fbs(.) + fqg-grd(.)
</equation>
<page confidence="0.942273">
681
</page>
<figure confidence="0.981071962962963">
1 2 4 8 16 0 3 6 12 24 55.5
Training Set Size of CTB5 Training Set Size of CDT
with QG
w/o QG
with QG
82
81
80
79
78
77
76
75
74
73
72
71
81.2
81
80.8
80.6
80.4
80.2
80
79.8
79.6
79.4
</figure>
<table confidence="0.966719571428571">
Models without QG with QG
O2 83.23 84.33 (+1.10)
O2sib 82.87 84.11 (+1.37)
O1 80.29 82.76 (+2.47)
Bohnet (2009) 82.68 —
Che et al. (2009) 82.11 —
Gesmundo et al. (2009) 81.70 —
</table>
<tableCaption confidence="0.954329333333333">
Table 8: Parsing accuracy (UAS) comparison on CTB6-
test with automatic POS tags. The improvements shown
in parentheses are all statistically significant (p &lt; 10−5).
</tableCaption>
<figureCaption confidence="0.896182333333333">
Figure 5: Parsing accuracy (UAS) comparison on CTB5-
test when the scale of CDT and CTB5 varies (thousands
in sentence number).
</figureCaption>
<table confidence="0.9998615">
Dependency w/o QG +QG Descriptions
NN &lt;-- NN 858 -78 noun modifier or coordinating nouns
VV → VV 777 -41 object clause or coordinating verbs
VV &lt;-- VV 570 -38 subject clause
VV → NN 509 -79 verb and its object
w0 → VV 357 -57 verb as sentence root
VV &lt;-- NN 328 -32 attributive clause
P &lt;-- VV 278 -37 preposition phrase attachment
VV → DEC 233 -33 attributive clause and auxiliary DE
P → NN 175 -35 preposition and its object
</table>
<tableCaption confidence="0.994734">
Table 7: Detailed effect of QG features on different de-
pendency patterns.
</tableCaption>
<bodyText confidence="0.567414">
enhanced parser. We only list the patterns with an
absolute change larger than 30. We find that the QG
features can significantly help a variety of depen-
dency patterns (i.e., reducing the missing number).
</bodyText>
<subsectionHeader confidence="0.821732">
5.4 CTB6 as the Target Treebank
</subsectionHeader>
<bodyText confidence="0.999967166666667">
We use CTB6 as the target treebank to further verify
the efficacy of our approach. Compared with CTB5,
CTB6 is of larger scale and is converted into de-
pendency structures according to finer-grained head-
finding rules (Hajiˇc et al., 2009). We directly adopt
the same transformation patterns and features tuned
on CTB5. Table 8 shows results. The improvements
are similar to those on CTB5, demonstrating that our
approach is effective and robust. We list the top three
systems of the CoNLL 2009 shared task in Table 8,
showing that our approach also advances the state-
of-the-art parsing accuracy on this data set.10
</bodyText>
<footnote confidence="0.9935825">
10We reproduce their UASs using the data released
by the organizer: http://ufal.mff.cuni.cz/conll2009-st/results/
results.php. The parsing accuracies of the top systems may be
underestimated since the accuracy of the provided POS tags in
CoNLL 2009 is only 92.38% on the test set, while the POS tag-
ger used in our experiments reaches 94.08%.
</footnote>
<tableCaption confidence="0.9475025">
Table 9: Parsing accuracy (UAS) comparison on the test
set of CTB5X. Niu et al. (2009) use the maximum en-
tropy inspired generative parser (GP) of Charniak (2000)
as their constituent parser.
</tableCaption>
<subsectionHeader confidence="0.991865">
5.5 Comparison with Treebank Conversion
</subsectionHeader>
<bodyText confidence="0.999989416666667">
As discussed in Section 2, Niu et al. (2009) automat-
ically convert the dependency-structure CDT to the
phrase-structure annotation style of CTB5X and use
the converted treebank as additional labeled data.
We convert their phrase-structure results on CTB5X-
test into dependency structures using the same head-
finding rules. To compare with their results, we
run our baseline and QG-enhanced O2 parsers on
CTB5X. Table 9 presents the results.11 The indirect
comparison indicates that our approach can achieve
larger improvement than their treebank conversion
based method.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.871184071428571">
The current paper proposes a simple and effective
framework for exploiting multiple large-scale tree-
banks of different annotation styles. We design
rich TPs to model the annotation inconsistencies and
consequently propose QG features based on these
TPs. Extensive experiments show that our approach
can effectively utilize the syntactic knowledge from
another treebank and significantly improve the state-
of-the-art parsing accuracy.
11We thank the authors for sharing their results. Niu et al.
(2009) also use the reranker (RP) of Charniak and Johnson
(2005) as a stronger baseline, but the results are missing. They
find a less improvement on F score with RP than with GP (0.9%
vs. 1.1%). We refer to their Table 5 and 6 for details.
</bodyText>
<figure confidence="0.9817445">
Models
Ours
GP (Niu et al., 2009)
84.16
82.42
86.67 (+2.51)
84.06 (+1.64)
baseline with another treebank
</figure>
<page confidence="0.988104">
682
</page>
<sectionHeader confidence="0.999129" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999393833333333">
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61133012, the National “863” Major Projects via
grant 2011AA01A207, and the National “863”
Leading Technology Research Project via grant
2012AA011102.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999790224489796">
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 693–702, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Bernd Bohnet. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceedings
of the Thirteenth Conference on Computational Iatu-
ral Language Learning (CoILL 2009): Shared Task,
pages 67–72, Boulder, Colorado, June. Association for
Computational Linguistics.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of the 2008 Conference on Empirical Methods in Iat-
ural Language Processing, pages 877–886, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of the Four-
teenth Conference on Computational Iatural Lan-
guage Learning, CoNLL ’10, pages 46–54, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings ofACL-05, pages 173–180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In AILP’00, pages 132–139.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of CoILL 2009: Shared Task, pages 49–
54.
Keh-Jiann Chen, Chi-Ching Luo, Ming-Chung Chang,
Feng-Yi Chen, Chao-Jan Chen, Chu-Ren Huang, and
Zhao-Ming Gao, 2003. Sinica treebank: Design crite-
ria,representational issues and implementation, chap-
ter 13, pages 231–248. Kluwer Academic Publishers.
Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Iatural Language Processing, pages
570–579, Singapore, August. Association for Compu-
tational Linguistics.
Wenliang Chen, Jun’ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 21–29, Uppsala, Sweden, July. Association
for Computational Linguistics.
Micheal Collins, Lance Ramshaw, Jan Hajic, and
Christoph Tillmann. 1999. A statistical parser for
czech. In ACL 1999, pages 505–512.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMILP 2002.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of CoILL 2009: Shared Task,
pages 37–42.
Kevin Gimpel and Noah A. Smith. 2011. Quasi-
synchronous phrase dependency grammars for ma-
chine translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Iatural Language Pro-
cessing, pages 474–485, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Marti, Lluis
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoILL
2009.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Iatural Language Processing,
pages 1222–1231, Singapore, August. Association for
Computational Linguistics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging – a case study. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Iatural Language Processing of
the AFILP, pages 522–530, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1–11, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.989251">
683
</page>
<reference confidence="0.999806085714286">
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings ofACL-08: HLT, pages 595–603, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models
for chinese pos tagging and dependency parsing. In
EMILP 2011, pages 1180–1191.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Building
a dependency treebank for improving Chinese parser.
In Journal of Chinese Language and Computing, vol-
ume 16, pages 207–224.
Andr— F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
EMILP’08, pages 157–166.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings ofEACL 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings ofACL 2005, pages 91–98.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Iatural Language Processing of the
AFILP, pages 46–54, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings ofACL 2008, pages 950–958.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149–160.
Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses: An introduction. John Wiley &amp;
Sons, Inc., New York. Book (ISBN 0471611360 ).
Zhou Qiang. 2004. Annotation scheme for chinese tree-
bank. Journal of Chinese Information Processing,
18(4):1–8.
David Smith and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntac-
tic dependencies. In Proceedings on the Workshop
on Statistical Machine Translation, pages 23–30, New
York City, June. Association for Computational Lin-
guistics.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the 2009 Conference on
Empirical Methods in Iatural Language Processing,
pages 822–831, Singapore, August. Association for
Computational Linguistics.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
2007Joint Conference on Empirical Methods in Iatu-
ral Language Processing and Computational Iatural
Language Learning (EMILP-CoILL), pages 22–32,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of
the 2011 Conference on Empirical Methods in Iatu-
ral Language Processing, pages 409–420, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer,
and Dipti Misra. Sharma. 2008. Towards a multi-
representational treebank. In In Proceedings of the 7th
International Workshop on Treebanks and Linguistic
Theories.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Iatural Lan-
guage Engineering, volume 11, pages 207–238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings ofIWPT 2003, pages 195–206.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings ofACL-08: HLT, pages 888–896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Iat-
ural Language Processing, pages 562–571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188–193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1556–1565, Portland, Oregon, USA,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.99864">
684
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.288616">
<title confidence="0.9952195">Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</title>
<author confidence="0.858931">Ting Wanxiang Li</author>
<affiliation confidence="0.950438">Research Center for Social Computing and Information School of Computer Science and Harbin Institute of Technology,</affiliation>
<address confidence="0.839845">CTB5 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Phrase structure Phrase structure</address>
<abstract confidence="0.971551363636364">CTB6 CDT Sinica TCT Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for pars- Several types of patterns (TP) are designed to capture the systematic annotation inconsistencies among different tree- Based on such TPs, we design quasigrammar to augment the baseline parsing models. Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target treebanks (Penn Chinese Treebank 5.1 and 6.0) using the Chinese Dependency Treebank as the source treebank. The improvements are respectively 1.37% and 1.10% with automatic part-of-speech tags. Moreover, an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>693--702</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2308" citStr="Bansal and Klein, 2011" startWordPosition="320" endWordPosition="323">ver, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 693–702, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Efficient parsing of syntactic and semantic dependency structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Iatural Language Learning (CoILL 2009): Shared Task,</booktitle>
<pages>67--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="27252" citStr="Bohnet (2009)" startWordPosition="4580" endWordPosition="4581">e number of the corresponding dependency pattern that appears in the gold-standard trees but misses in the results of the baseline parser, whereas the signed figures in the “+QG” column are the changes made by the QGSetting fbs(.) fqg(.) fbs(.) + fqg(.) fbs(.) + fqg-dep(.) fbs(.) + fqg-sib(.) fbs(.) + fqg-grd(.) 681 1 2 4 8 16 0 3 6 12 24 55.5 Training Set Size of CTB5 Training Set Size of CDT with QG w/o QG with QG 82 81 80 79 78 77 76 75 74 73 72 71 81.2 81 80.8 80.6 80.4 80.2 80 79.8 79.6 79.4 Models without QG with QG O2 83.23 84.33 (+1.10) O2sib 82.87 84.11 (+1.37) O1 80.29 82.76 (+2.47) Bohnet (2009) 82.68 — Che et al. (2009) 82.11 — Gesmundo et al. (2009) 81.70 — Table 8: Parsing accuracy (UAS) comparison on CTB6- test with automatic POS tags. The improvements shown in parentheses are all statistically significant (p &lt; 10−5). Figure 5: Parsing accuracy (UAS) comparison on CTB5- test when the scale of CDT and CTB5 varies (thousands in sentence number). Dependency w/o QG +QG Descriptions NN &lt;-- NN 858 -78 noun modifier or coordinating nouns VV → VV 777 -41 object clause or coordinating verbs VV &lt;-- VV 570 -38 subject clause VV → NN 509 -79 verb and its object w0 → VV 357 -57 verb as senten</context>
</contexts>
<marker>Bohnet, 2009</marker>
<rawString>Bernd Bohnet. 2009. Efficient parsing of syntactic and semantic dependency structures. In Proceedings of the Thirteenth Conference on Computational Iatural Language Learning (CoILL 2009): Shared Task, pages 67–72, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<pages>877--886</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2401" citStr="Burkett and Klein, 2008" startWordPosition="334" endWordPosition="337">nre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exp</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings of the 2008 Conference on Empirical Methods in Iatural Language Processing, pages 877–886, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Slav Petrov</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Learning better monolingual models with unannotated bilingual text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Iatural Language Learning, CoNLL ’10,</booktitle>
<pages>46--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2443" citStr="Burkett et al., 2010" startWordPosition="342" endWordPosition="346">ebank seems inadequate for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which w</context>
</contexts>
<marker>Burkett, Petrov, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010. Learning better monolingual models with unannotated bilingual text. In Proceedings of the Fourteenth Conference on Computational Iatural Language Learning, CoNLL ’10, pages 46–54, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL-05,</booktitle>
<pages>173--180</pages>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings ofACL-05, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In AILP’00,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="29467" citStr="Charniak (2000)" startWordPosition="4958" endWordPosition="4959">sk in Table 8, showing that our approach also advances the stateof-the-art parsing accuracy on this data set.10 10We reproduce their UASs using the data released by the organizer: http://ufal.mff.cuni.cz/conll2009-st/results/ results.php. The parsing accuracies of the top systems may be underestimated since the accuracy of the provided POS tags in CoNLL 2009 is only 92.38% on the test set, while the POS tagger used in our experiments reaches 94.08%. Table 9: Parsing accuracy (UAS) comparison on the test set of CTB5X. Niu et al. (2009) use the maximum entropy inspired generative parser (GP) of Charniak (2000) as their constituent parser. 5.5 Comparison with Treebank Conversion As discussed in Section 2, Niu et al. (2009) automatically convert the dependency-structure CDT to the phrase-structure annotation style of CTB5X and use the converted treebank as additional labeled data. We convert their phrase-structure results on CTB5Xtest into dependency structures using the same headfinding rules. To compare with their results, we run our baseline and QG-enhanced O2 parsers on CTB5X. Table 9 presents the results.11 The indirect comparison indicates that our approach can achieve larger improvement than t</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In AILP’00, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Yongqiang Li</author>
<author>Yuhang Guo</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
</authors>
<title>Multilingual dependency-based syntactic and semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of CoILL 2009: Shared Task,</booktitle>
<pages>49--54</pages>
<contexts>
<context position="27278" citStr="Che et al. (2009)" startWordPosition="4584" endWordPosition="4587">ponding dependency pattern that appears in the gold-standard trees but misses in the results of the baseline parser, whereas the signed figures in the “+QG” column are the changes made by the QGSetting fbs(.) fqg(.) fbs(.) + fqg(.) fbs(.) + fqg-dep(.) fbs(.) + fqg-sib(.) fbs(.) + fqg-grd(.) 681 1 2 4 8 16 0 3 6 12 24 55.5 Training Set Size of CTB5 Training Set Size of CDT with QG w/o QG with QG 82 81 80 79 78 77 76 75 74 73 72 71 81.2 81 80.8 80.6 80.4 80.2 80 79.8 79.6 79.4 Models without QG with QG O2 83.23 84.33 (+1.10) O2sib 82.87 84.11 (+1.37) O1 80.29 82.76 (+2.47) Bohnet (2009) 82.68 — Che et al. (2009) 82.11 — Gesmundo et al. (2009) 81.70 — Table 8: Parsing accuracy (UAS) comparison on CTB6- test with automatic POS tags. The improvements shown in parentheses are all statistically significant (p &lt; 10−5). Figure 5: Parsing accuracy (UAS) comparison on CTB5- test when the scale of CDT and CTB5 varies (thousands in sentence number). Dependency w/o QG +QG Descriptions NN &lt;-- NN 858 -78 noun modifier or coordinating nouns VV → VV 777 -41 object clause or coordinating verbs VV &lt;-- VV 570 -38 subject clause VV → NN 509 -79 verb and its object w0 → VV 357 -57 verb as sentence root VV &lt;-- NN 328 -32 </context>
</contexts>
<marker>Che, Li, Li, Guo, Qin, Liu, 2009</marker>
<rawString>Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, and Ting Liu. 2009. Multilingual dependency-based syntactic and semantic parsing. In Proceedings of CoILL 2009: Shared Task, pages 49– 54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
</authors>
<title>Chi-Ching Luo, Ming-Chung Chang, Feng-Yi Chen, Chao-Jan Chen, Chu-Ren Huang, and Zhao-Ming Gao,</title>
<date>2003</date>
<booktitle>Sinica treebank: Design criteria,representational issues and implementation, chapter 13,</booktitle>
<pages>231--248</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Chen, 2003</marker>
<rawString>Keh-Jiann Chen, Chi-Ching Luo, Ming-Chung Chang, Feng-Yi Chen, Chao-Jan Chen, Chu-Ren Huang, and Zhao-Ming Gao, 2003. Sinica treebank: Design criteria,representational issues and implementation, chapter 13, pages 231–248. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data.</title>
<date>2009</date>
<contexts>
<context position="2284" citStr="Chen et al., 2009" startWordPosition="316" endWordPosition="319">eness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006</context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<pages>570--579</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker></marker>
<rawString>In Proceedings of the 2009 Conference on Empirical Methods in Iatural Language Processing, pages 570–579, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Bitext dependency parsing with bilingual subtree constraints.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>21--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2463" citStr="Chen et al., 2010" startWordPosition="347" endWordPosition="350"> for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which we leave as future wo</context>
</contexts>
<marker>Chen, Kazama, Torisawa, 2010</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, and Kentaro Torisawa. 2010. Bitext dependency parsing with bilingual subtree constraints. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 21–29, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micheal Collins</author>
<author>Lance Ramshaw</author>
<author>Jan Hajic</author>
<author>Christoph Tillmann</author>
</authors>
<title>A statistical parser for czech. In ACL</title>
<date>1999</date>
<pages>505--512</pages>
<contexts>
<context position="7170" citStr="Collins et al., 1999" startWordPosition="1067" endWordPosition="1070">In contrast, we experiment with two real large-scale treebanks, and boost the stateof-the-art parsing accuracy using QG features. Second, we explore much richer QG features to fully exploit the knowledge of the source treebank. These features are tailored to the dependency parsing problem. In summary, the present work makes substantial progress in modeling structural annotation inconsistencies with QG features for parsing. Previous work on treebank conversion primarily focuses on converting one grammar formalism of a treebank into another and then conducting a study on the converted treebank (Collins et al., 1999; Xia et al., 2008). The work by Niu et al. (2009) is, to our knowledge, the only study to date that combines the converted treebank with the existing target treebank. They automatically convert the dependency-structure CDT into the phrase-structure style of CTB5 using a statistical constituency parser trained on CTB5. Their experiments show that the combined treebank can significantly improve the performance of constituency parsers. However, their method requires several sophisticated strategies, such as corpus weighting and score interpolation, to reduce the influence of conversion errors. I</context>
</contexts>
<marker>Collins, Ramshaw, Hajic, Tillmann, 1999</marker>
<rawString>Micheal Collins, Lance Ramshaw, Jan Hajic, and Christoph Tillmann. 1999. A statistical parser for czech. In ACL 1999, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMILP</booktitle>
<contexts>
<context position="20307" citStr="Collins, 2002" startWordPosition="3398" endWordPosition="3399">s Train Dev Test PD 281,311 5,000 10,000 CDT 55,500 1,500 3,000 CTB5 16,091 803 1,910 CTB5X 18,104 352 348 CTB6 22,277 1,762 2,556 Table 3: Data used in this work (in sentence number). We adopt unlabeled attachment score (UAS) as the primary evaluation metric. We also use Root accuracy (RA) and complete match rate (CM) to give more insights. All metrics exclude punctuation. We adopt Dan Bikel’s randomized parsing evaluation comparator for significance test (Noreen, 1989).7 For all models used in current work (POS tagging and parsing), we adopt averaged perceptron to train the feature weights (Collins, 2002). We train each model for 10 iterations and select the parameters that perform best on the development set. 5.1 Preliminaries This subsection describes how we project the source annotations into the target treebanks. First, we train a statistical POS tagger on the training set of PD, which we name TaggerPD.8 The tagging accuracy on the test set of PD is 98.30%. We then use TaggerPD to produce POS tags for all the treebanks (CDT, CTB5, and CTB6). Based on the common POS tags, we train a second-order source parser (O2) on CDT, denoted by ParserCDT .The UAS on CDT-test is 84.45%. We then use Pars</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMILP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous syntactic-semantic parsing for multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of CoILL 2009: Shared Task,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="27309" citStr="Gesmundo et al. (2009)" startWordPosition="4590" endWordPosition="4593"> that appears in the gold-standard trees but misses in the results of the baseline parser, whereas the signed figures in the “+QG” column are the changes made by the QGSetting fbs(.) fqg(.) fbs(.) + fqg(.) fbs(.) + fqg-dep(.) fbs(.) + fqg-sib(.) fbs(.) + fqg-grd(.) 681 1 2 4 8 16 0 3 6 12 24 55.5 Training Set Size of CTB5 Training Set Size of CDT with QG w/o QG with QG 82 81 80 79 78 77 76 75 74 73 72 71 81.2 81 80.8 80.6 80.4 80.2 80 79.8 79.6 79.4 Models without QG with QG O2 83.23 84.33 (+1.10) O2sib 82.87 84.11 (+1.37) O1 80.29 82.76 (+2.47) Bohnet (2009) 82.68 — Che et al. (2009) 82.11 — Gesmundo et al. (2009) 81.70 — Table 8: Parsing accuracy (UAS) comparison on CTB6- test with automatic POS tags. The improvements shown in parentheses are all statistically significant (p &lt; 10−5). Figure 5: Parsing accuracy (UAS) comparison on CTB5- test when the scale of CDT and CTB5 varies (thousands in sentence number). Dependency w/o QG +QG Descriptions NN &lt;-- NN 858 -78 noun modifier or coordinating nouns VV → VV 777 -41 object clause or coordinating verbs VV &lt;-- VV 570 -38 subject clause VV → NN 509 -79 verb and its object w0 → VV 357 -57 verb as sentence root VV &lt;-- NN 328 -32 attributive clause P &lt;-- VV 278</context>
</contexts>
<marker>Gesmundo, Henderson, Merlo, Titov, 2009</marker>
<rawString>Andrea Gesmundo, James Henderson, Paola Merlo, and Ivan Titov. 2009. A latent variable model of synchronous syntactic-semantic parsing for multiple languages. In Proceedings of CoILL 2009: Shared Task, pages 37–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Quasisynchronous phrase dependency grammars for machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<pages>474--485</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="12914" citStr="Gimpel and Smith, 2011" startWordPosition="2077" endWordPosition="2080">he target. A TP comprises two syntactic structures, one in the source side and the other in the target side, and denotes the process by which the left-side subtree is transformed into the right-side structure. Functions Wde.(.), Wsib(.), and Wgrd(.) return the specific TP type for a candidate scoring part according to the source tree d′. Figure 3: Framework of our approach. portion of d′, and the construction of d can be inspired by arbitrary substructures of d′. To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem considers one single sentence (x = x′), and the alignment a is trivial. Figure 3 shows the framework of our approach. First, we train a statistical parser on the source treebank, which is called the source parser. The source parser is then used to parse th</context>
</contexts>
<marker>Gimpel, Smith, 2011</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2011. Quasisynchronous phrase dependency grammars for machine translation. In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing, pages 474–485, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Marti</author>
<author>Lluis M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of CoILL</booktitle>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Marti, Lluis M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of CoILL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<pages>1222--1231</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2421" citStr="Huang et al., 2009" startWordPosition="338" endWordPosition="341"> from one single treebank seems inadequate for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our prop</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proceedings of the 2009 Conference on Empirical Methods in Iatural Language Processing, pages 1222–1231, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Iatural Language Processing of the AFILP,</booktitle>
<pages>522--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="5725" citStr="Jiang et al. (2009)" startWordPosition="841" endWordPosition="844"> treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. proach also outperforms the treebank conversion approach of Niu et al. (2009). 2 Related Work The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of parsing. Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. The first part of their work is closely connected with our work, but with a few important differences. First, they conduct simulated exper</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Iatural Language Processing of the AFILP, pages 522–530, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2093" citStr="Koo and Collins, 2010" startWordPosition="284" endWordPosition="287">en models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the</context>
<context position="10625" citStr="Koo and Collins, 2010" startWordPosition="1643" endWordPosition="1646">G features. 3Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. dependency sibling grandparent Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4).4 For the O2 model, the score function is rewritten as: EScorebs(x, t, d) = wdep &apos; fdep(x, t, h, m) {(h,m)}⊆d + E wsib &apos; fsib(x, t, h, s, m) {(h,s),(h,m)}⊆d + E wgrd &apos; fgrd(x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep(.), fsib(.) and fgrd(.) correspond to the features for the three kinds of scoring parts. We adopt the standard features following Li et al. (2011). For the O1 and O2sib models, the above formula is modified by deactivating the extra parts. 4 Dependency Parsing with QG Features Smith and Eisner (2006) propose the QG for machine</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>595--603</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2265" citStr="Koo et al., 2008" startWordPosition="312" endWordPosition="315">rom the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL-08: HLT, pages 595–603, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Wenliang Chen</author>
<author>Haizhou Li</author>
</authors>
<title>Joint models for chinese pos tagging and dependency parsing.</title>
<date>2011</date>
<booktitle>In EMILP 2011,</booktitle>
<pages>1180--1191</pages>
<contexts>
<context position="2111" citStr="Li et al., 2011" startWordPosition="288" endWordPosition="291">ral classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we</context>
<context position="11043" citStr="Li et al. (2011)" startWordPosition="1719" endWordPosition="1722">b) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4).4 For the O2 model, the score function is rewritten as: EScorebs(x, t, d) = wdep &apos; fdep(x, t, h, m) {(h,m)}⊆d + E wsib &apos; fsib(x, t, h, s, m) {(h,s),(h,m)}⊆d + E wgrd &apos; fgrd(x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep(.), fsib(.) and fgrd(.) correspond to the features for the three kinds of scoring parts. We adopt the standard features following Li et al. (2011). For the O1 and O2sib models, the above formula is modified by deactivating the extra parts. 4 Dependency Parsing with QG Features Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′, a QG defines a monolingual grammar that generates translations of x′, which can be denoted by p(x, d, a|x′, d′), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4We use the coars</context>
<context position="21399" citStr="Li et al. (2011)" startWordPosition="3579" endWordPosition="3582">gs, we train a second-order source parser (O2) on CDT, denoted by ParserCDT .The UAS on CDT-test is 84.45%. We then use ParserCDT to parse CTB5 and CTB6. 7http://www.cis.upenn.edu/[normal-wave˜] dbikel/software.html 8We adopt the Chinese-oriented POS tagging features proposed in Zhang and Clark (2008a). Models without QG with QG O2 86.13 86.44 (+0.31, p = 0.06) O2sib 85.63 86.17 (+0.54, p = 0.003) O1 83.16 84.40 (+1.24, p &lt; 10−5) Li11 86.18 — Z&amp;N11 86.00 — Table 4: Parsing accuracy (UAS) comparison on CTB5- test with gold-standard POS tags. Li11 refers to the second-order graph-based model of Li et al. (2011), whereas Z&amp;N11 is the feature-rich transition-based model of Zhang and Nivre (2011). At this point, both CTB5 and CTB6 contain dependency structures conforming to the style of CDT. 5.2 CTB5 as the Target Treebank Table 4 shows the results when the gold-standard POS tags of CTB5 are adopted by the parsing models. We aim to analyze the efficacy of QG features under the ideal scenario wherein the parsing models suffer from no error propagation of POS tagging. We determine that our baseline O2 model achieves comparable accuracy with the state-of-theart parsers. We also find that QG features can b</context>
<context position="23509" citStr="Li et al. (2011)" startWordPosition="3935" endWordPosition="3938">significant (p &lt; 10−5). UAS CM RA 79.67 26.81 73.82 79.15 26.34 74.71 81.04 29.63 77.17 80.82 28.80 76.28 80.86 28.48 76.18 80.88 28.90 76.34 Table 6: Feature ablation for Parser-O2 on CTB5-test with automatic POS tags. POS tags for the development and test sets of CTB5. The tagging accuracy is 93.88% on the test set. The automatic POS tags of the training set are produced using 10-fold cross-validation.9 Table 5 shows the results. We find that QG features result in a surprisingly large improvement over the O1 baseline and can also boost the state-ofthe-art parsing accuracy by a large margin. Li et al. (2011) show that a joint POS tagging and dependency parsing model can significantly improve parsing accuracy over a pipeline model. Our QGenhanced parser outperforms their best joint model by 0.25%. Moreover, the QG features can be used to enhance a joint model and achieve higher accuracy, which we leave as future work. 5.3 Analysis Using Parser-O2 with AUTO-POS We then try to gain more insights into the effect of the QG features through detailed analysis. We select the state-of-the-art O2 parser and focus on the realistic scenario with automatic POS tags. Table 6 compares the efficacy of different </context>
</contexts>
<marker>Li, Zhang, Che, Liu, Chen, Li, 2011</marker>
<rawString>Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for chinese pos tagging and dependency parsing. In EMILP 2011, pages 1180–1191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Jinshan Ma</author>
<author>Sheng Li</author>
</authors>
<title>Building a dependency treebank for improving Chinese parser.</title>
<date>2006</date>
<journal>In Journal of Chinese Language and Computing,</journal>
<volume>16</volume>
<pages>207--224</pages>
<contexts>
<context position="2885" citStr="Liu et al., 2006" startWordPosition="415" endWordPosition="418">hen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which we leave as future work. Despite the divergence of annotation philosophy, these treebanks contain rich human knowledge on the Chinese syntax, thereby having a great deal of common ground. Therefore, exploiting multiple treebanks is very attractive for boosting parsing accuracy. Figure 1 gives an example with different an675 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 675–684, Jeju, Republi</context>
<context position="17229" citStr="Liu et al., 2006" startWordPosition="2821" endWordPosition="2824">ich are shown in Table 2. The type of the TP is conjoined with the related words and POS tags, such that the QG-enhanced parsing models can make more elaborate decisions based on the context. Then, the score contributed by the QG features can be redefined as Scoreqg(x, t, d′, d) _ E wqg-dep · fqg-dep(x, t, d′, h, m) {(h,m)}⊆d + � wqg-sib · fqg-sib(x, t, d′, h, s, m) {(h,s),(h,m)}⊆d + � wqg-grd · fqg-grd(x, t, d′, 9, h, m) {(g,h),(h,m)}⊆d which resembles the baseline model and can be naturally handled by the decoding algorithms. 5 Experiments and Analysis We use the CDT as the source treebank (Liu et al., 2006). CDT consists of 60,000 sentences from the People’s Daily in 1990s. For the target treebank, we use two widely used versions of Penn Chinese Treebank, i.e., CTB5 and CTB6, which consist of Xinhua newswire, Hong Kong news and articles from Sinarama news magazine (Xue et al., 2005). To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. CTB6 is used as the Chinese data set in the CoNLL 2009 shared task (Hajiˇc et al., 2009). Therefore, we adopt the same setting. CDT and CTB5/6 adopt different POS tag se</context>
</contexts>
<marker>Liu, Ma, Li, 2006</marker>
<rawString>Ting Liu, Jinshan Ma, and Sheng Li. 2006. Building a dependency treebank for improving Chinese parser. In Journal of Chinese Language and Computing, volume 16, pages 207–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr— F T Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In EMILP’08,</booktitle>
<pages>157--166</pages>
<contexts>
<context position="8408" citStr="Martins et al., 2008" startWordPosition="1260" endWordPosition="1263">he noisy converted treebank as additional training data, our approach allows the QGpromote trade and industry V n c n ROO WO 1FC391 91X112 *fl3 T-R4 vv NN cc NN OBJ NMOD NMOD ROO LAD vOB cOO 676 enhanced parsing models to softly learn the systematic inconsistencies based on QG features, making our approach simpler and more robust. Our approach is also intuitively related to stacked learning (SL), a machine learning framework that has recently been applied to dependency parsing to integrate two main-stream parsing models, i.e., graph-based and transition-based models (Nivre and McDonald, 2008; Martins et al., 2008). However, the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies. 3 Dependency Parsing Given an input sentence x = w0w,...wn and its POS tag sequence t = t0t,...tn, the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m, l) : 0 &lt; h &lt; n, 0 &lt; m &lt; n, l C G}, where (h, m, l) indicates an directed arc from the head word (also called father) wh to the modifier (also called child or dependent) wm with a dependency label l, and G is the label set. We omit the label </context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr— F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In EMILP’08, pages 157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings ofEACL</booktitle>
<contexts>
<context position="10501" citStr="McDonald and Pereira, 2006" startWordPosition="1621" endWordPosition="1624">s. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. dependency sibling grandparent Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4).4 For the O2 model, the score function is rewritten as: EScorebs(x, t, d) = wdep &apos; fdep(x, t, h, m) {(h,m)}⊆d + E wsib &apos; fsib(x, t, h, s, m) {(h,s),(h,m)}⊆d + E wgrd &apos; fgrd(x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep(.), fsib(.) and fgrd(.) correspond to the features for the three kinds of scoring parts. We adopt the standard features following Li et al. (2011). For the O1 and O2sib models, the above formula is modifi</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings ofEACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>91--98</pages>
<contexts>
<context position="10337" citStr="McDonald et al., 2005" startWordPosition="1597" endWordPosition="1600">(.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. dependency sibling grandparent Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4).4 For the O2 model, the score function is rewritten as: EScorebs(x, t, d) = wdep &apos; fdep(x, t, h, m) {(h,m)}⊆d + E wsib &apos; fsib(x, t, h, s, m) {(h,s),(h,m)}⊆d + E wgrd &apos; fgrd(x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep(.), fsib(.) and fgrd(.) correspond to th</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL 2005, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
</authors>
<title>Exploiting heterogeneous treebanks for parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Iatural Language Processing of the AFILP,</booktitle>
<pages>46--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="4468" citStr="Niu et al. (2009)" startWordPosition="655" endWordPosition="658">dea for multiple treebank exploitation is treebank conversion. First, the annotations in the source treebank are converted into the style of the target treebank. Then, both the converted treebank and the target treebank are combined. Finally, the combined treebank are used to train a better parser. However, the inconsistencies among different treebanks are normally nontrivial, which makes rule-based conversion infeasible. For example, a number of inconsistencies between CTB5 and CDT are lexicon-sensitive, that is, they adopt different annotations for some particular lexicons (or word senses). Niu et al. (2009) use sophisticated strategies to reduce the noises of the converted treebank after automatic treebank conversion. The present paper proposes a simple and effective framework for this problem. The proposed framework avoids directly addressing the difficult annotation transformation problem, but focuses on modeling the annotation inconsistencies using transformation patterns (TP). The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees. We conduct extensive experiments using CDT as the</context>
<context position="7220" citStr="Niu et al. (2009)" startWordPosition="1078" endWordPosition="1081">treebanks, and boost the stateof-the-art parsing accuracy using QG features. Second, we explore much richer QG features to fully exploit the knowledge of the source treebank. These features are tailored to the dependency parsing problem. In summary, the present work makes substantial progress in modeling structural annotation inconsistencies with QG features for parsing. Previous work on treebank conversion primarily focuses on converting one grammar formalism of a treebank into another and then conducting a study on the converted treebank (Collins et al., 1999; Xia et al., 2008). The work by Niu et al. (2009) is, to our knowledge, the only study to date that combines the converted treebank with the existing target treebank. They automatically convert the dependency-structure CDT into the phrase-structure style of CTB5 using a statistical constituency parser trained on CTB5. Their experiments show that the combined treebank can significantly improve the performance of constituency parsers. However, their method requires several sophisticated strategies, such as corpus weighting and score interpolation, to reduce the influence of conversion errors. Instead of using the noisy converted treebank as ad</context>
<context position="17907" citStr="Niu et al., 2009" startWordPosition="2940" endWordPosition="2943"> 1990s. For the target treebank, we use two widely used versions of Penn Chinese Treebank, i.e., CTB5 and CTB6, which consist of Xinhua newswire, Hong Kong news and articles from Sinarama news magazine (Xue et al., 2005). To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. CTB6 is used as the Chinese data set in the CoNLL 2009 shared task (Hajiˇc et al., 2009). Therefore, we adopt the same setting. CDT and CTB5/6 adopt different POS tag sets, and converting from one tag set to another is difficult (Niu et al., 2009).5 To overcome this problem, we use the People’s Daily corpus (PD),6 a large-scale corpus annotated with word segmentation and POS tags, to train a statistical POS tagger. The tagger produces a universal layer of POS tags for both the source and target treebanks. Based on the common tags, the source parser projects the source annotations into the target treebanks. PD comprises approximately 300 thousand sentences of with approximately 7 million words from the first half of 1998 of People’s Daily. Table 3 summarizes the data sets used in the present work. CTB5X is the same with CTB5 but follows</context>
<context position="29392" citStr="Niu et al. (2009)" startWordPosition="4944" endWordPosition="4947">fective and robust. We list the top three systems of the CoNLL 2009 shared task in Table 8, showing that our approach also advances the stateof-the-art parsing accuracy on this data set.10 10We reproduce their UASs using the data released by the organizer: http://ufal.mff.cuni.cz/conll2009-st/results/ results.php. The parsing accuracies of the top systems may be underestimated since the accuracy of the provided POS tags in CoNLL 2009 is only 92.38% on the test set, while the POS tagger used in our experiments reaches 94.08%. Table 9: Parsing accuracy (UAS) comparison on the test set of CTB5X. Niu et al. (2009) use the maximum entropy inspired generative parser (GP) of Charniak (2000) as their constituent parser. 5.5 Comparison with Treebank Conversion As discussed in Section 2, Niu et al. (2009) automatically convert the dependency-structure CDT to the phrase-structure annotation style of CTB5X and use the converted treebank as additional labeled data. We convert their phrase-structure results on CTB5Xtest into dependency structures using the same headfinding rules. To compare with their results, we run our baseline and QG-enhanced O2 parsers on CTB5X. Table 9 presents the results.11 The indirect c</context>
<context position="30620" citStr="Niu et al. (2009)" startWordPosition="5124" endWordPosition="5127">ndicates that our approach can achieve larger improvement than their treebank conversion based method. 6 Conclusions The current paper proposes a simple and effective framework for exploiting multiple large-scale treebanks of different annotation styles. We design rich TPs to model the annotation inconsistencies and consequently propose QG features based on these TPs. Extensive experiments show that our approach can effectively utilize the syntactic knowledge from another treebank and significantly improve the stateof-the-art parsing accuracy. 11We thank the authors for sharing their results. Niu et al. (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing. They find a less improvement on F score with RP than with GP (0.9% vs. 1.1%). We refer to their Table 5 and 6 for details. Models Ours GP (Niu et al., 2009) 84.16 82.42 86.67 (+2.51) 84.06 (+1.64) baseline with another treebank 682 Acknowledgments This work was supported by National Natural Science Foundation of China (NSFC) via grant 61133012, the National “863” Major Projects via grant 2011AA01A207, and the National “863” Leading Technology Research Project via grant 2012AA011102. </context>
</contexts>
<marker>Niu, Wang, Wu, 2009</marker>
<rawString>Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Exploiting heterogeneous treebanks for parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Iatural Language Processing of the AFILP, pages 46–54, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>950--958</pages>
<contexts>
<context position="8385" citStr="Nivre and McDonald, 2008" startWordPosition="1256" endWordPosition="1259">errors. Instead of using the noisy converted treebank as additional training data, our approach allows the QGpromote trade and industry V n c n ROO WO 1FC391 91X112 *fl3 T-R4 vv NN cc NN OBJ NMOD NMOD ROO LAD vOB cOO 676 enhanced parsing models to softly learn the systematic inconsistencies based on QG features, making our approach simpler and more robust. Our approach is also intuitively related to stacked learning (SL), a machine learning framework that has recently been applied to dependency parsing to integrate two main-stream parsing models, i.e., graph-based and transition-based models (Nivre and McDonald, 2008; Martins et al., 2008). However, the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies. 3 Dependency Parsing Given an input sentence x = w0w,...wn and its POS tag sequence t = t0t,...tn, the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m, l) : 0 &lt; h &lt; n, 0 &lt; m &lt; n, l C G}, where (h, m, l) indicates an directed arc from the head word (also called father) wh to the modifier (also called child or dependent) wm with a dependency label l, and G is the label </context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings ofACL 2008, pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<contexts>
<context position="10128" citStr="Nivre, 2003" startWordPosition="1570" endWordPosition="1571">red into the scores of some small parts (subtrees). Scorebs(x, t, d) = wbs &apos; fbs(x, t, d) E= wpart &apos; fpart(x, t, p) p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs(.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. dependency sibling grandparent Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4).4 For the O2 model, the score function is rewr</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-intensive methods for testing hypotheses: An introduction.</title>
<date>1989</date>
<tech>(ISBN 0471611360 ).</tech>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<location>New York. Book</location>
<contexts>
<context position="20168" citStr="Noreen, 1989" startWordPosition="3376" endWordPosition="3377"> indicates that the features listed in the corresponding column are also conjoined with dir(h, m) ◦ dist(h, m) to form new features. Corpus Train Dev Test PD 281,311 5,000 10,000 CDT 55,500 1,500 3,000 CTB5 16,091 803 1,910 CTB5X 18,104 352 348 CTB6 22,277 1,762 2,556 Table 3: Data used in this work (in sentence number). We adopt unlabeled attachment score (UAS) as the primary evaluation metric. We also use Root accuracy (RA) and complete match rate (CM) to give more insights. All metrics exclude punctuation. We adopt Dan Bikel’s randomized parsing evaluation comparator for significance test (Noreen, 1989).7 For all models used in current work (POS tagging and parsing), we adopt averaged perceptron to train the feature weights (Collins, 2002). We train each model for 10 iterations and select the parameters that perform best on the development set. 5.1 Preliminaries This subsection describes how we project the source annotations into the target treebanks. First, we train a statistical POS tagger on the training set of PD, which we name TaggerPD.8 The tagging accuracy on the test set of PD is 98.30%. We then use TaggerPD to produce POS tags for all the treebanks (CDT, CTB5, and CTB6). Based on th</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-intensive methods for testing hypotheses: An introduction. John Wiley &amp; Sons, Inc., New York. Book (ISBN 0471611360 ).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou Qiang</author>
</authors>
<title>Annotation scheme for chinese treebank.</title>
<date>2004</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="2980" citStr="Qiang, 2004" startWordPosition="432" endWordPosition="433">reebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which we leave as future work. Despite the divergence of annotation philosophy, these treebanks contain rich human knowledge on the Chinese syntax, thereby having a great deal of common ground. Therefore, exploiting multiple treebanks is very attractive for boosting parsing accuracy. Figure 1 gives an example with different an675 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 675–684, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Example </context>
</contexts>
<marker>Qiang, 2004</marker>
<rawString>Zhou Qiang. 2004. Annotation scheme for chinese treebank. Journal of Chinese Information Processing, 18(4):1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>23--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="11198" citStr="Smith and Eisner (2006)" startWordPosition="1745" endWordPosition="1748"> scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4).4 For the O2 model, the score function is rewritten as: EScorebs(x, t, d) = wdep &apos; fdep(x, t, h, m) {(h,m)}⊆d + E wsib &apos; fsib(x, t, h, s, m) {(h,s),(h,m)}⊆d + E wgrd &apos; fgrd(x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep(.), fsib(.) and fgrd(.) correspond to the features for the three kinds of scoring parts. We adopt the standard features following Li et al. (2011). For the O1 and O2sib models, the above formula is modified by deactivating the extra parts. 4 Dependency Parsing with QG Features Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′, a QG defines a monolingual grammar that generates translations of x′, which can be denoted by p(x, d, a|x′, d′), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decoding procedure (Koo and Collins, 2010). g h M M h s h M 677 ψ8,(d &apos;,g,h,m) </context>
<context position="12868" citStr="Smith and Eisner, 2006" startWordPosition="2070" endWordPosition="2073">using CDT as the source treebank and CTB5 as the target. A TP comprises two syntactic structures, one in the source side and the other in the target side, and denotes the process by which the left-side subtree is transformed into the right-side structure. Functions Wde.(.), Wsib(.), and Wgrd(.) return the specific TP type for a candidate scoring part according to the source tree d′. Figure 3: Framework of our approach. portion of d′, and the construction of d can be inspired by arbitrary substructures of d′. To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem considers one single sentence (x = x′), and the alignment a is trivial. Figure 3 shows the framework of our approach. First, we train a statistical parser on the source treebank, which is called the source pars</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David Smith and Jason Eisner. 2006. Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings on the Workshop on Statistical Machine Translation, pages 23–30, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<pages>822--831</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5753" citStr="Smith and Eisner (2009)" startWordPosition="846" endWordPosition="849">6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. proach also outperforms the treebank conversion approach of Niu et al. (2009). 2 Related Work The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of parsing. Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. The first part of their work is closely connected with our work, but with a few important differences. First, they conduct simulated experiments on one treebank by ma</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David A. Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of the 2009 Conference on Empirical Methods in Iatural Language Processing, pages 822–831, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy model? a quasisynchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007Joint Conference on Empirical Methods in Iatural Language Processing and Computational Iatural Language Learning (EMILP-CoILL),</booktitle>
<pages>22--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="12954" citStr="Wang et al., 2007" startWordPosition="2084" endWordPosition="2087">tures, one in the source side and the other in the target side, and denotes the process by which the left-side subtree is transformed into the right-side structure. Functions Wde.(.), Wsib(.), and Wgrd(.) return the specific TP type for a candidate scoring part according to the source tree d′. Figure 3: Framework of our approach. portion of d′, and the construction of d can be inspired by arbitrary substructures of d′. To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem considers one single sentence (x = x′), and the alignment a is trivial. Figure 3 shows the framework of our approach. First, we train a statistical parser on the source treebank, which is called the source parser. The source parser is then used to parse the whole target treebank. At this point, </context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? a quasisynchronous grammar for QA. In Proceedings of the 2007Joint Conference on Empirical Methods in Iatural Language Processing and Computational Iatural Language Learning (EMILP-CoILL), pages 22–32, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to simplify sentences with quasi-synchronous grammar and integer programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<pages>409--420</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="13011" citStr="Woodsend and Lapata, 2011" startWordPosition="2091" endWordPosition="2094">e target side, and denotes the process by which the left-side subtree is transformed into the right-side structure. Functions Wde.(.), Wsib(.), and Wgrd(.) return the specific TP type for a candidate scoring part according to the source tree d′. Figure 3: Framework of our approach. portion of d′, and the construction of d can be inspired by arbitrary substructures of d′. To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem considers one single sentence (x = x′), and the alignment a is trivial. Figure 3 shows the framework of our approach. First, we train a statistical parser on the source treebank, which is called the source parser. The source parser is then used to parse the whole target treebank. At this point, the target treebank contains two sets of annotations, one</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing, pages 409–420, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharma</author>
</authors>
<title>Towards a multirepresentational treebank. In</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories.</booktitle>
<marker>Sharma, 2008</marker>
<rawString>Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer, and Dipti Misra. Sharma. 2008. Towards a multirepresentational treebank. In In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<booktitle>In Iatural Language Engineering,</booktitle>
<volume>11</volume>
<pages>207--238</pages>
<contexts>
<context position="2823" citStr="Xue et al., 2005" startWordPosition="405" endWordPosition="408">odels, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which we leave as future work. Despite the divergence of annotation philosophy, these treebanks contain rich human knowledge on the Chinese syntax, thereby having a great deal of common ground. Therefore, exploiting multiple treebanks is very attractive for boosting parsing accuracy. Figure 1 gives an example with different an675 Proceedings of the 50th Annual Meeting of the Associati</context>
<context position="17510" citStr="Xue et al., 2005" startWordPosition="2872" endWordPosition="2875"> E wqg-dep · fqg-dep(x, t, d′, h, m) {(h,m)}⊆d + � wqg-sib · fqg-sib(x, t, d′, h, s, m) {(h,s),(h,m)}⊆d + � wqg-grd · fqg-grd(x, t, d′, 9, h, m) {(g,h),(h,m)}⊆d which resembles the baseline model and can be naturally handled by the decoding algorithms. 5 Experiments and Analysis We use the CDT as the source treebank (Liu et al., 2006). CDT consists of 60,000 sentences from the People’s Daily in 1990s. For the target treebank, we use two widely used versions of Penn Chinese Treebank, i.e., CTB5 and CTB6, which consist of Xinhua newswire, Hong Kong news and articles from Sinarama news magazine (Xue et al., 2005). To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. CTB6 is used as the Chinese data set in the CoNLL 2009 shared task (Hajiˇc et al., 2009). Therefore, we adopt the same setting. CDT and CTB5/6 adopt different POS tag sets, and converting from one tag set to another is difficult (Niu et al., 2009).5 To overcome this problem, we use the People’s Daily corpus (PD),6 a large-scale corpus annotated with word segmentation and POS tags, to train a statistical POS tagger. The tagger produces a universal</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. In Iatural Language Engineering, volume 11, pages 207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWPT</booktitle>
<pages>195--206</pages>
<contexts>
<context position="10114" citStr="Yamada and Matsumoto, 2003" startWordPosition="1566" endWordPosition="1569">f a dependency tree is factored into the scores of some small parts (subtrees). Scorebs(x, t, d) = wbs &apos; fbs(x, t, d) E= wpart &apos; fpart(x, t, p) p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs(.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. dependency sibling grandparent Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4).4 For the O2 model, the score fu</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings ofIWPT 2003, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>888--896</pages>
<contexts>
<context position="5398" citStr="Zhang and Clark, 2008" startWordPosition="794" endWordPosition="797">eling the annotation inconsistencies using transformation patterns (TP). The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees. We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. proach also outperforms the treebank conversion approach of Niu et al. (2009). 2 Related Work The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handlin</context>
<context position="17591" citStr="Zhang and Clark (2008" startWordPosition="2885" endWordPosition="2888">, h, s, m) {(h,s),(h,m)}⊆d + � wqg-grd · fqg-grd(x, t, d′, 9, h, m) {(g,h),(h,m)}⊆d which resembles the baseline model and can be naturally handled by the decoding algorithms. 5 Experiments and Analysis We use the CDT as the source treebank (Liu et al., 2006). CDT consists of 60,000 sentences from the People’s Daily in 1990s. For the target treebank, we use two widely used versions of Penn Chinese Treebank, i.e., CTB5 and CTB6, which consist of Xinhua newswire, Hong Kong news and articles from Sinarama news magazine (Xue et al., 2005). To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. CTB6 is used as the Chinese data set in the CoNLL 2009 shared task (Hajiˇc et al., 2009). Therefore, we adopt the same setting. CDT and CTB5/6 adopt different POS tag sets, and converting from one tag set to another is difficult (Niu et al., 2009).5 To overcome this problem, we use the People’s Daily corpus (PD),6 a large-scale corpus annotated with word segmentation and POS tags, to train a statistical POS tagger. The tagger produces a universal layer of POS tags for both the source and target treebanks. Based on the common </context>
<context position="21084" citStr="Zhang and Clark (2008" startWordPosition="3522" endWordPosition="3525">we project the source annotations into the target treebanks. First, we train a statistical POS tagger on the training set of PD, which we name TaggerPD.8 The tagging accuracy on the test set of PD is 98.30%. We then use TaggerPD to produce POS tags for all the treebanks (CDT, CTB5, and CTB6). Based on the common POS tags, we train a second-order source parser (O2) on CDT, denoted by ParserCDT .The UAS on CDT-test is 84.45%. We then use ParserCDT to parse CTB5 and CTB6. 7http://www.cis.upenn.edu/[normal-wave˜] dbikel/software.html 8We adopt the Chinese-oriented POS tagging features proposed in Zhang and Clark (2008a). Models without QG with QG O2 86.13 86.44 (+0.31, p = 0.06) O2sib 85.63 86.17 (+0.54, p = 0.003) O1 83.16 84.40 (+1.24, p &lt; 10−5) Li11 86.18 — Z&amp;N11 86.00 — Table 4: Parsing accuracy (UAS) comparison on CTB5- test with gold-standard POS tags. Li11 refers to the second-order graph-based model of Li et al. (2011), whereas Z&amp;N11 is the feature-rich transition-based model of Zhang and Nivre (2011). At this point, both CTB5 and CTB6 contain dependency structures conforming to the style of CDT. 5.2 CTB5 as the Target Treebank Table 4 shows the results when the gold-standard POS tags of CTB5 are a</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008a. Joint word segmentation and POS tagging using a single perceptron. In Proceedings ofACL-08: HLT, pages 888–896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<pages>562--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="5398" citStr="Zhang and Clark, 2008" startWordPosition="794" endWordPosition="797">eling the annotation inconsistencies using transformation patterns (TP). The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees. We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. proach also outperforms the treebank conversion approach of Niu et al. (2009). 2 Related Work The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handlin</context>
<context position="17591" citStr="Zhang and Clark (2008" startWordPosition="2885" endWordPosition="2888">, h, s, m) {(h,s),(h,m)}⊆d + � wqg-grd · fqg-grd(x, t, d′, 9, h, m) {(g,h),(h,m)}⊆d which resembles the baseline model and can be naturally handled by the decoding algorithms. 5 Experiments and Analysis We use the CDT as the source treebank (Liu et al., 2006). CDT consists of 60,000 sentences from the People’s Daily in 1990s. For the target treebank, we use two widely used versions of Penn Chinese Treebank, i.e., CTB5 and CTB6, which consist of Xinhua newswire, Hong Kong news and articles from Sinarama news magazine (Xue et al., 2005). To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. CTB6 is used as the Chinese data set in the CoNLL 2009 shared task (Hajiˇc et al., 2009). Therefore, we adopt the same setting. CDT and CTB5/6 adopt different POS tag sets, and converting from one tag set to another is difficult (Niu et al., 2009).5 To overcome this problem, we use the People’s Daily corpus (PD),6 a large-scale corpus annotated with word segmentation and POS tags, to train a statistical POS tagger. The tagger produces a universal layer of POS tags for both the source and target treebanks. Based on the common </context>
<context position="21084" citStr="Zhang and Clark (2008" startWordPosition="3522" endWordPosition="3525">we project the source annotations into the target treebanks. First, we train a statistical POS tagger on the training set of PD, which we name TaggerPD.8 The tagging accuracy on the test set of PD is 98.30%. We then use TaggerPD to produce POS tags for all the treebanks (CDT, CTB5, and CTB6). Based on the common POS tags, we train a second-order source parser (O2) on CDT, denoted by ParserCDT .The UAS on CDT-test is 84.45%. We then use ParserCDT to parse CTB5 and CTB6. 7http://www.cis.upenn.edu/[normal-wave˜] dbikel/software.html 8We adopt the Chinese-oriented POS tagging features proposed in Zhang and Clark (2008a). Models without QG with QG O2 86.13 86.44 (+0.31, p = 0.06) O2sib 85.63 86.17 (+0.54, p = 0.003) O1 83.16 84.40 (+1.24, p &lt; 10−5) Li11 86.18 — Z&amp;N11 86.00 — Table 4: Parsing accuracy (UAS) comparison on CTB5- test with gold-standard POS tags. Li11 refers to the second-order graph-based model of Li et al. (2011), whereas Z&amp;N11 is the feature-rich transition-based model of Zhang and Nivre (2011). At this point, both CTB5 and CTB6 contain dependency structures conforming to the style of CDT. 5.2 CTB5 as the Target Treebank Table 4 shows the results when the gold-standard POS tags of CTB5 are a</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008b. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the 2008 Conference on Empirical Methods in Iatural Language Processing, pages 562–571, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="21483" citStr="Zhang and Nivre (2011)" startWordPosition="3591" endWordPosition="3594">he UAS on CDT-test is 84.45%. We then use ParserCDT to parse CTB5 and CTB6. 7http://www.cis.upenn.edu/[normal-wave˜] dbikel/software.html 8We adopt the Chinese-oriented POS tagging features proposed in Zhang and Clark (2008a). Models without QG with QG O2 86.13 86.44 (+0.31, p = 0.06) O2sib 85.63 86.17 (+0.54, p = 0.003) O1 83.16 84.40 (+1.24, p &lt; 10−5) Li11 86.18 — Z&amp;N11 86.00 — Table 4: Parsing accuracy (UAS) comparison on CTB5- test with gold-standard POS tags. Li11 refers to the second-order graph-based model of Li et al. (2011), whereas Z&amp;N11 is the feature-rich transition-based model of Zhang and Nivre (2011). At this point, both CTB5 and CTB6 contain dependency structures conforming to the style of CDT. 5.2 CTB5 as the Target Treebank Table 4 shows the results when the gold-standard POS tags of CTB5 are adopted by the parsing models. We aim to analyze the efficacy of QG features under the ideal scenario wherein the parsing models suffer from no error propagation of POS tagging. We determine that our baseline O2 model achieves comparable accuracy with the state-of-theart parsers. We also find that QG features can boost the parsing accuracy by a large margin when the baseline parser is weak (O1). T</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Li Cai</author>
</authors>
<title>Exploiting web-derived selectional preference to improve statistical dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1556--1565</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2328" citStr="Zhou et al., 2011" startWordPosition="324" endWordPosition="327">reebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 *Correspondence author: tliu@ir.hit.edu.cn 1Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) a</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1556–1565, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>