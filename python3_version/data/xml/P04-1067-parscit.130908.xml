<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<note confidence="0.78697475">
A Geometric View on Bilingual Lexicon Extraction from Comparable
Corpora
E. Gaussier†, J.-M. Renders†, I. Matveeva*, C. Goutte†, H. D´ejean††Xerox Research Centre Europe
6, Chemin de Maupertuis — 38320 Meylan, France
</note>
<email confidence="0.715032">
Eric.Gaussier@xrce.xerox.com
</email>
<affiliation confidence="0.988504">
*Dept of Computer Science, University of Chicago
</affiliation>
<address confidence="0.617081">
1100 E. 58th St. Chicago, IL 60637 USA
</address>
<email confidence="0.998421">
matveeva@cs.uchicago.edu
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999745">
We present a geometric view on bilingual lexicon
extraction from comparable corpora, which allows
to re-interpret the methods proposed so far and iden-
tify unresolved problems. This motivates three new
methods that aim at solving these problems. Empir-
ical evaluation shows the strengths and weaknesses
of these methods, as well as a significant gain in the
accuracy of extracted lexicons.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871266666667">
Comparable corpora contain texts written in differ-
ent languages that, roughly speaking, ”talk about
the same thing”. In comparison to parallel corpora,
ie corpora which are mutual translations, compara-
ble corpora have not received much attention from
the research community, and very few methods have
been proposed to extract bilingual lexicons from
such corpora. However, except for those found in
translation services or in a few international organ-
isations, which, by essence, produce parallel docu-
mentations, most existing multilingual corpora are
not parallel, but comparable. This concern is re-
flected in major evaluation conferences on cross-
language information retrieval (CLIR), e.g. CLEF&apos;,
which only use comparable corpora for their multi-
lingual tracks.
We adopt here a geometric view on bilingual lex-
icon extraction from comparable corpora which al-
lows one to re-interpret the methods proposed thus
far and formulate new ones inspired by latent se-
mantic analysis (LSA), which was developed within
the information retrieval (IR) community to treat
synonymous and polysemous terms (Deerwester et
al., 1990). We will explain in this paper the moti-
vations behind the use of such methods for bilin-
gual lexicon extraction from comparable corpora,
and show how to apply them. Section 2 is devoted to
the presentation of the standard approach, ie the ap-
proach adopted by most researchers so far, its geo-
metric interpretation, and the unresolved synonymy
</bodyText>
<footnote confidence="0.930252">
1http://clef.iei.pi.cnr.it:2002/
</footnote>
<bodyText confidence="0.999599428571429">
and polysemy problems. Sections 3 to 4 then de-
scribe three new methods aiming at addressing the
issues raised by synonymy and polysemy: in sec-
tion 3 we introduce an extension of the standard ap-
proach, and show in appendix A how this approach
relates to the probabilistic method proposed in (De-
jean et al., 2002); in section 4, we present a bilin-
gual extension to LSA, namely canonical correla-
tion analysis and its kernel version; lastly, in sec-
tion 5, we formulate the problem in terms of prob-
abilistic LSA and review different associated simi-
larities. Section 6 is then devoted to a large-scale
evaluation of the different methods proposed. Open
issues are then discussed in section 7.
</bodyText>
<sectionHeader confidence="0.924262" genericHeader="introduction">
2 Standard approach
</sectionHeader>
<bodyText confidence="0.999923307692308">
Bilingual lexicon extraction from comparable cor-
pora has been studied by a number of researchers,
(Rapp, 1995; Peters and Picchi, 1995; Tanaka and
Iwasaki, 1996; Shahzad et al., 1999; Fung, 2000,
among others). Their work relies on the assump-
tion that if two words are mutual translations, then
their more frequent collocates (taken here in a very
broad sense) are likely to be mutual translations as
well. Based on this assumption, the standard ap-
proach builds context vectors for each source and
target word, translates the target context vectors us-
ing a general bilingual dictionary, and compares the
translation with the source context vector:
</bodyText>
<listItem confidence="0.999109333333333">
1. For each source word v (resp. target word w),
build a context vector v (resp. 2Vi) consisting
in the measure of association of each word e
(resp. f) in the context of v (resp. w), a(v, e).
2. Translate the context vectors with a general
bilingual dictionary D, accumulating the con-
tributions from words that yield identical trans-
lations.
3. Compute the similarity between source word v
</listItem>
<bodyText confidence="0.938835714285714">
and target word w using a similarity measures,
such as the Dice or Jaccard coefficients, or the
cosine measure.
As the dot-product plays a central role in all these
measures, we consider, without loss of generality,
the similarity given by the dot-product between v
and the translation of W:
</bodyText>
<equation confidence="0.9957066">
���� �
(�� v , tr(w)) =
e
�= a(v, e) a(w, f) (1)
(e,f)ED
</equation>
<bodyText confidence="0.999938153846154">
Because of the translation step, only the pairs (e, f)
that are present in the dictionary contribute to the
dot-product.
Note that this approach requires some general
bilingual dictionary as initial seed. One way to cir-
cumvent this requirement consists in automatically
building a seed lexicon based on spelling and cog-
nates clues (Koehn and Knight, 2002). Another ap-
proach directly tackles the problem from scratch by
searching for a translation mapping which optimally
preserves the intralingual association measure be-
tween words (Diab and Finch, 2000): the under-
lying assumption is that pairs of words which are
highly associated in one language should have trans-
lations that are highly associated in the other lan-
guage. In this latter case, the association measure
is defined as the Spearman rank order correlation
between their context vectors restricted to “periph-
eral tokens” (highly frequent words). The search
method is based on a gradient descent algorithm, by
iteratively changing the mapping of a single word
until (locally) minimizing the sum of squared differ-
ences between the association measure of all pairs
of words in one language and the association mea-
sure of the pairs of translated words obtained by the
current mapping.
</bodyText>
<subsectionHeader confidence="0.991071">
2.1 Geometric presentation
</subsectionHeader>
<bodyText confidence="0.977568733333333">
We denote by si,1 &lt; i &lt; p and tj,1 &lt; j &lt; q the
source and target words in the bilingual dictionary
D. D is a set of n translation pairs (si, tj), and
may be represented as a p x q matrix M, such that
Mij = 1 iff (si, tj) E D (and 0 otherwise).2
Assuming there are m distinct source words
e1, · · · , em and r distinct target words f1, · · · , fr in
the corpus, figure 1 illustrates the geometric view of
the standard method.
The association measure a(v, e) may be viewed
as the coordinates of the m-dimensional context
vector v in the vector space formed by the or-
thogonal basis (e1, · · · , em). The dot-product in (1)
only involves source dictionary entries. The corre-
sponding dimensions are selected by an orthogonal
</bodyText>
<footnote confidence="0.9447745">
2The extension to weighted dictionary entries Mij E [0, 1]
is straightforward but not considered here for clarity.
</footnote>
<bodyText confidence="0.987245333333333">
projection on the sub-space formed by (s1, · · · , sp),
using a p x m projection matrix P3. Note that
(s1, · · · , sp), being a sub-family of (e1, · · · , em), is
an orthogonal basis of the new sub-space. Similarly,
V is projected on the dictionary entries (t1, · · · , tQ)
using a q x r orthogonal projection matrix Pt. As
M encodes the relationship between the source and
target entries of the dictionary, equation 1 may be
rewritten as:
</bodyText>
<equation confidence="0.967767">
S(v, w) = (V , tr(w)) = (P3��
���� v )T M (PtV) (2)
</equation>
<bodyText confidence="0.9980616">
where T denotes transpose. In addition, notice that
M can be rewritten as STT, with S an n x p and
T an n x q matrix encoding the relations between
words and pairs in the bilingual dictionary (e.g. Ski
is 1 iff si is in the kth translation pair). Hence:
</bodyText>
<equation confidence="0.611514">
S(v,w)=vTPT3 STTPtzWi =(SP3�� v ,TPt�� w ) (3)
</equation>
<bodyText confidence="0.9994936">
which shows that the standard approach amounts to
performing a dot-product in the vector space formed
by the n pairs ((s1, tl), · · · , (sp, tk)), which are as-
sumed to be orthogonal, and correspond to transla-
tion pairs.
</bodyText>
<subsectionHeader confidence="0.998185">
2.2 Problems with the standard approach
</subsectionHeader>
<bodyText confidence="0.999928793103448">
There are two main potential problems associated
with the use of a bilingual dictionary.
Coverage. This is a problem if too few corpus
words are covered by the dictionary. However, if
the context is large enough, some context words
are bound to belong to the general language, so a
general bilingual dictionary should be suitable. We
thus expect the standard approach to cope well with
the coverage problem, at least for frequent words.
For rarer words, we can bootstrap the bilingual dic-
tionary by iteratively augmenting it with the most
probable translations found in the corpus.
Polysemy/synonymy. Because all entries on ei-
ther side of the bilingual dictionary are treated as or-
thogonal dimensions in the standard methods, prob-
lems may arise when several entries have the same
meaning (synonymy), or when an entry has sev-
eral meanings (polysemy), especially when only
one meaning is represented in the corpus.
Ideally, the similarities wrt synonyms should not
be independent, but the standard method fails to ac-
count for that. The axes corresponding to synonyms
si and sj are orthogonal, so that projections of a
context vector on si and sj will in general be uncor-
related. Therefore, a context vector that is similar to
si may not necessarily be similar to sj.
A similar situation arises for polysemous entries.
Suppose the word bank appears as both financial in-
stitution (French: banque) and ground near a river
</bodyText>
<equation confidence="0.856325714285714">
�a(v, e) a(w, f)
f,(e,f)inD
e2
P
e1 s 1
(s ,t )
1 i
v&amp;quot;
(s 1 ,t 1)
(s ,t )
p k
tp f2
P
t1
</equation>
<figure confidence="0.930679818181818">
S
T
s p
v’
w&amp;quot;
w’
fr
w
f1
em
v
</figure>
<figureCaption confidence="0.999968">
Figure 1: Geometric view of the standard approach
</figureCaption>
<bodyText confidence="0.996009692307692">
(French: berge), but only the pair (banque, bank)
is in the bilingual dictionary. The standard method
will deem similar river, which co-occurs with bank,
and argent (money), which co-occurs with banque.
In both situations, however, the context vectors of
the dictionary entries provide some additional infor-
mation: for synonyms si and sj, it is likely that i
and �−sj are similar; for polysemy, if the context vec-
tors banque and −−�
−−−−� bank have few translations pairs in
common, it is likely that banque and bank are used
with somewhat different meanings. The following
methods try to leverage this additional information.
</bodyText>
<sectionHeader confidence="0.861679" genericHeader="method">
3 Extension of the standard approach
</sectionHeader>
<bodyText confidence="0.999207318181819">
The fact that synonyms may be captured through
similarity of context vectors leads us to question
the projection that is made in the standard method,
and to replace it with a mapping into the sub-space
formed by the context vectors of the dictionary en-
tries, that is, instead of projecting v on the sub-
space formed by (s1, · · · , sp), we now map it onto
the sub-space generated by (−�s1, · · · , *−sp). With this
mapping, we try to find a vector space in which syn-
onymous dictionary entries are close to each other,
while polysemous ones still select different neigh-
bors. This time, if v is close to i and Tj, si and
sj being synonyms, the translations of both si and
sj will be used to find those words w close to v.
Figure 2 illustrates this process. By denoting Qs,
respectively Qt, such a mapping in the source (resp.
target) side, and using the same translation mapping
(S, T) as above, the similarity between source and
target words becomes:
S(v, w)=(SQs−�v , TQt−�w )=−�vTQTs STTQt−�w (4)
A natural choice for Qs (and similarly for Qt) is the
following m x p matrix:
</bodyText>
<equation confidence="0.838488666666667">
�
�
Qs = RT s = �
</equation>
<bodyText confidence="0.946388882352941">
3This assumption has been experimentally validated in sev-
eral studies, e.g. (Grefenstette, 1994; Lewis et al., 1967).
but other choices, such as a pseudo-inverse of Rs,
are possible. Note however that computing the
pseudo-inverse of Rs is a complex operation, while
the above projection is straightforward (the columns
of Q correspond to the context vectors of the dic-
tionary words). In appendix A we show how this
method generalizes over the probabilistic approach
presented in (Dejean et al., 2002). The above
method bears similarities with the one described
in (Besanc¸on et al., 1999), where a matrix similar
to Qs is used to build a new term-document ma-
trix. However, the motivations behind their work
and ours differ, as do the derivations and the gen-
eral framework, which justifies e.g. the choice of
the pseudo-inverse of Rs in our case.
</bodyText>
<sectionHeader confidence="0.97787" genericHeader="method">
4 Canonical correlation analysis
</sectionHeader>
<bodyText confidence="0.99996525">
The data we have at our disposal can naturally be
represented as an n x (m + r) matrix in which
the rows correspond to translation pairs, and the
columns to source and target vocabularies:
</bodyText>
<equation confidence="0.97812125">
e1 ··· em f1 ··· fr
· · · · · · · · ·
... ... ...
· · · · · · · · ·
</equation>
<bodyText confidence="0.999974882352941">
where (s(k), t(k)) is just a renumbering of the trans-
lation pairs (si, tj).
Matrix C shows that each translation pair sup-
ports two views, provided by the context vectors in
the source and target languages. Each view is con-
nected to the other by the translation pair it repre-
sents. The statistical technique of canonical corre-
lation analysis (CCA) can be used to identify direc-
tions in the source view (first m columns of C) and
target view (last r columns of C) that are maximally
correlated, ie “behave in the same way” wrt the
translation pairs. We are thus looking for directions
in the source and target vector spaces (defined by
the orthogonal bases (e1, · · · , em) and (f1, · · · , fr))
such that the projections of the translation pairs on
these directions are maximally correlated. Intu-
itively, those directions define latent semantic axes
</bodyText>
<equation confidence="0.962763083333333">
a(s1,e1) ··· a(sp,e1)
... ... ...
a(s1, em) ··· a(sp, em)
�
� �
C =
· · · · · · · · ·
... ... ...
· · · · · · · · ·
(s(1), t(1))
...
(s(n), t(n))
</equation>
<figureCaption confidence="0.99012">
Figure 2: Geometric view of the extended approach
</figureCaption>
<figure confidence="0.997960184210526">
i
s
em
em
f2
(s ,t )
1 i
(s ,t )
p k
w&amp;quot;
v&amp;quot;
(s 1,t 1)
T
fr
f t
2
tk
1
w
Qt
t2
tq
f1
f1
fr
e2
s2
Qs
v
s
e1
p
e1
s1
S
v
e2
w
</figure>
<bodyText confidence="0.9944">
that capture the implicit relations between transla-
tion pairs, and induce a natural mapping across lan-
guages. Denoting by s and t the directions in the
source and target spaces, respectively, this may be
formulated as:
</bodyText>
<equation confidence="0.808164">
 i(s,-§&apos; (i))(t, T (i))
i(s, -§, (i)) j(t, T (j))
</equation>
<bodyText confidence="0.999705555555556">
As in principal component analysis, once the first
two directions (1s, 1t ) have been identified, the pro-
cess can be repeated in the sub-space orthogonal
to the one formed by the already identified direc-
tions. However, a general solution based on a set of
eigenvalues can be proposed. Following e.g. (Bach
and Jordan, 2001), the above problem can be re-
formulated as the following generalized eigenvalue
problem:
</bodyText>
<equation confidence="0.983976">
B  = D  (5)
</equation>
<bodyText confidence="0.979155">
where, denoting again Rs and Rt the first m and last
r (respectively) columns of C, we define:
</bodyText>
<equation confidence="0.984502888888889">

0 RtR t RsR s
RsR s RtR 0
t
 (RsR 
s )2  s 
0
D = ,  =
0 (RtR t )2 t
</equation>
<bodyText confidence="0.999606642857143">
The standard approach to solve eq. 5 is to per-
form an incomplete Cholesky decomposition of a
regularized form of D (Bach and Jordan, 2001).
This yields pairs of source and target directions
(1s, 1t ), · · · , (ls, lt) that define a new sub-space in
which to project words from each language. This
sub-space plays the same role as the sub-space de-
fined by translation pairs in the standard method, al-
though with CCA, it is derived from the corpus via
the context vectors of the translation pairs. Once
projected, words from different languages can be
compared through their dot-product or cosine. De-
noting °s = s1 ,... s l , and °t =t , . . . t ,
the similarity becomes (figure 3):
</bodyText>
<equation confidence="0.736713">
S(v, w) = (°sV , °tV) = v°s °tV (6)
</equation>
<bodyText confidence="0.9999344">
The number l of vectors retained in each language
directly defines the dimensions of the final sub-
space used for comparing words across languages.
CCA and its kernelised version were used in (Vi-
nokourov et al., 2002) as a way to build a cross-
lingual information retrieval system from parallel
corpora. We show here that it can be used to in-
fer language-independent semantic representations
from comparable corpora, which induce a similarity
between words in the source and target languages.
</bodyText>
<sectionHeader confidence="0.981676" genericHeader="method">
5 Multilingual probabilistic latent
semantic analysis
</sectionHeader>
<bodyText confidence="0.99998705">
The matrix C described above encodes in each row
k the context vectors of the source (first m columns)
and target (last r columns) of each translation pair.
Ideally, we would like to cluster this matrix such
that translation pairs with synonymous words ap-
pear in the same cluster, while translation pairs with
polysemous words appear in different clusters (soft
clustering). Furthermore, because of the symmetry
between the roles played by translation pairs and vo-
cabulary words (synonymous and polysemous vo-
cabulary words should also behave as described
above), we want the clustering to behave symmet-
rically with respect to translation pairs and vocabu-
lary words. One well-motivated method that fulfills
all the above criteria is Probabilistic Latent Seman-
tic Analysis (PLSA) (Hofmann, 1999).
Assuming that C encodes the co-occurrences be-
tween vocabulary words w and translation pairs d,
PLSA models the probability of co-occurrence w
and d via latent classes :
</bodyText>
<equation confidence="0.941586">

P(w, d) = P() P(w|) P(d|) (7)
</equation>
<bodyText confidence="0.999758333333333">
where, for a given class, words and translation pairs
are assumed to be independently generated from
class-conditional probabilities P(w|) and P(d|).
Note here that the latter distribution is language-
independent, and that the same latent classes are
used for the two languages. The parameters of the
model are obtained by maximizing the likelihood of
the observed data (matrix C) through Expectation-
Maximisation algorithm (Dempster et al., 1977). In
</bodyText>
<figure confidence="0.996174128205128">
 = max
s,t
B =
,
f2
(2 2)
st
w&amp;quot;
f 1
2 t
(1s, 1t )
f1
it
(ls, lt)
l t
fr
(CCA)
w
v&amp;quot;
2
t
2
s
v
em
is
1
s
l s
e2
(CCA)
e1
e2
v
e1
em
f1
fr
w
</figure>
<figureCaption confidence="0.999955">
Figure 3: Geometric view of the Canonical Correlation Analysis approach
</figureCaption>
<bodyText confidence="0.959241933333333">
addition, in order to reduce the sensitivity to initial
conditions, we use a deterministic annealing scheme
(Ueda and Nakano, 1995). The update formulas for
the EM algorithm are given in appendix B.
This model can identify relevant bilingual latent
classes, but does not directly define a similarity be-
tween words across languages. That may be done
by using Fisher kernels as described below.
Associated similarities: Fisher kernels
Fisher kernels (Jaakkola and Haussler, 1999) de-
rive a similarity measure from a probabilistic model.
They are useful whenever a direct similarity be-
tween observed feature is hard to define or in-
sufficient. Denoting e(w) = lnP(w|B) the log-
likelihood for example w, the Fisher kernel is:
</bodyText>
<equation confidence="0.995582">
K(w1, w2) = VE(w1)TIF−1Vf(w2) (8)
</equation>
<bodyText confidence="0.960468285714286">
The Fisher information matrix IF =
�V�(x)V�(x)T�
E keeps the kernel indepen-
dent of reparameterisation. With a suitable
parameterisation, we assume IF pz� 1. For PLSA
(Hofmann, 2000), the Fisher kernel between two
words w1 and w2 becomes:
</bodyText>
<equation confidence="0.9989">
P(a|d,w1)P(a|d,w2)
P(d|a)
</equation>
<bodyText confidence="0.9901128">
where d ranges over the translation pairs. The
Fisher kernel performs a dot-product in a vector
space defined by the parameters of the model. With
only one class, the expression of the Fisher kernel
(9) reduces to:
</bodyText>
<equation confidence="0.808398333333333">
E
K(w1, w2) = 1 +
d
</equation>
<bodyText confidence="0.999675111111111">
Apart from the additional intercept (’1’), this is
exactly the similarity provided by the standard
method, with associations given by scaled empir-
ical frequencies a(w, d) = P�(d|w)/�IP(d). Ac-
cordingly, we expect that the standard method and
the Fisher kernel with one class should have simi-
lar behaviors. In addition to the above kernel, we
consider two additional versions, obtained:through
normalisation (NFK) and exponentiation (EFK):
</bodyText>
<equation confidence="0.9989835">
K(w1, w2)
NFK(w1, w2) = (10)
�IK(w1)K(w2)
2(K(w1)+K(w2)−2K(w1,w2))
EFK(w1,w2) = e
1
</equation>
<bodyText confidence="0.944282">
where K(w) stands for K(w, w).
</bodyText>
<sectionHeader confidence="0.994458" genericHeader="method">
6 Experiments and results
</sectionHeader>
<bodyText confidence="0.9999838">
We conducted experiments on an English-French
corpus derived from the data used in the multi-
lingual track of CLEF2003, corresponding to the
newswire of months May 1994 and December 1994
of the Los Angeles Times (1994, English) and Le
Monde (1994, French). As our bilingual dictionary,
we used the ELRA multilingual dictionary,4 which
contains ca. 13,500 entries with at least one match
in our corpus. In addition, the following linguis-
tic preprocessing steps were performed on both the
corpus and the dictionary: tokenisation, lemmatisa-
tion and POS-tagging. Only lexical words (nouns,
verbs, adverbs, adjectives) were indexed and only
single word entries in the dicitonary were retained.
Infrequent words (occurring less than 5 times) were
discarded when building the indexing terms and the
dictionary entries. After these steps our corpus con-
tains 34,966 distinct English words, and 21,140 dis-
tinct French words, leading to ca. 25,000 English
and 13,000 French words not present in the dictio-
nary.
To evaluate the performance of our extraction
methods, we randomly split the dictionaries into a
training set with 12,255 entries, and a test set with
1,245 entries. The split is designed in such a way
that all pairs corresponding to the same source word
are in the same set (training or test). All methods
use the training set as the sole available resource
and predict the most likely translations of the terms
in the source language (English) belonging to the
</bodyText>
<footnote confidence="0.981213">
4Available through www.elra.info
</footnote>
<equation confidence="0.998291125">
EK(w1, w2) =
P(a|w1)P(a|w2)
(9)
P(�)
+E P�(d|w1) E
d P� (d|w2)
P�(d|w1) P�(d|w2)
P(d)
</equation>
<bodyText confidence="0.99999143939394">
test set. The context vectors were defined by com-
puting the mutual information association measure
between terms occurring in the same context win-
dow of size 5 (ie. by considering a neighborhood of
+/- 2 words around the current word), and summing
it over all contexts of the corpora. Different associ-
ation measures and context sizes were assessed and
the above settings turned out to give the best perfor-
mance even if the optimum is relatively flat. For
memory space and computational efficiency rea-
sons, context vectors were pruned so that, for each
term, the remaining components represented at least
90 percent of the total mutual information. After
pruning, the context vectors were normalised so that
their Euclidean norm is equal to 1. The PLSA-based
methods used the raw co-occurrence counts as asso-
ciation measure, to be consistent with the underly-
ing generative model. In addition, for the extended
method, we retained only the N (N = 200 is the
value which yielded the best results in our experi-
ments) dictionary entries closest to source and tar-
get words when doing the projection with Q. As
discussed below, this allows us to get rid of spuri-
ous relationships.
The upper part of table 1 summarizes the results
we obtained, measured in terms of F-1 score for
different lengths of the candidate list, from 20 to
500. For each length, precision is based on the num-
ber of lists that contain an actual translation of the
source word, whereas recall is based on the num-
ber of translations provided in the reference set and
found in the list. Note that our results differ from the
ones previously published, which can be explained
by the fact that first our corpus is relatively small
compared to others, second that our evaluation re-
lies on a large number of candidates, which can oc-
cur as few as 5 times in the corpus, whereas previous
evaluations were based on few, high frequent terms,
and third that we do not use the same bilingual dic-
tionary, the coverage of which being an important
factor in the quality of the results obtained. Long
candidate lists are justified by CLIR considerations,
where longer lists might be preferred over shorter
ones for query expansion purposes. For PLSA, the
normalised Fisher kernels provided the best results,
and increasing the number of latent classes did not
lead in our case to improved results. We thus dis-
play here the results obtained with the normalised
version of the Fisher kernel, using only one compo-
nent. For CCA, we empirically optimised the num-
ber of dimensions to be used, and display the results
obtained with the optimal value (l = 300).
As one can note, the extended approach yields
the best results in terms of F1-score. However, its
performance for the first 20 candidates are below
the standard approach and comparable to the PLSA-
based method. Indeed, the standard approach leads
to higher precision at the top of the list, but lower
recall overall. This suggests that we could gain in
performance by re-ranking the candidates of the ex-
tended approach with the standard and PLSA meth-
ods. The lower part of table 1 shows that this is
indeed the case. The average precision goes up
from 0.4 to 0.44 through this combination, and the
F1-score is significantly improved for all the length
ranges we considered (bold line in table 1).
</bodyText>
<sectionHeader confidence="0.99765" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9999343">
Extended method As one could expect, the ex-
tended approach improves the recall of our bilingual
lexicon extraction system. Contrary to the standard
approach, in the extended approach, all the dictio-
nary words, present or not in the context vector of a
given word, can be used to translate it. This leads to
a noise problem since spurious relations are bound
to be detected. The restriction we impose on the
translation pairs to be used (N nearest neighbors)
directly aims at selecting only the translation pairs
which are in true relation with the word to be trans-
lated.
Multilingual PLSA Even though theoretically
well-founded, PLSA does not lead to improved per-
formance. When used alone, it performs slightly
below the standard method, for different numbers
of components, and performs similarly to the stan-
dard method when used in combination with the
extended method. We believe the use of mere co-
occurrence counts gives a disadvantage to PLSA
over other methods, which can rely on more sophis-
ticated measures. Furthermore, the complexity of
the final vector space (several millions of dimen-
sions) in which the comparison is done entails a
longer processing time, which renders this method
less attractive than the standard or extended ones.
Canonical correlation analysis The results we ob-
tain with CCA and its kernel version are disappoint-
ing. As already noted, CCA does not directly solve
the problems we mentioned, and our results show
that CCA does not provide a good alternative to the
standard method. Here again, we may suffer from a
noise problem, since each canonical direction is de-
fined by a linear combination that can involve many
different vocabulary words.
Overall, starting with an average precision of 0.35
as provided by the standard approach, we were able
to increase it to 0.44 with the methods we consider.
Furthermore, we have shown here that such an im-
provement could be achieved with relatively simple
</bodyText>
<table confidence="0.999721444444444">
20 60 100 160 200 260 300 400 500 Avg.Prec.
standard 0.14 0.20 0.24 0.29 0.30 0.33 0.35 0.38 0.40 0.35
Ext (N=500) 0.11 0.21 0.27 0.32 0.34 0.38 0.41 0.45 0.50 0.40
CCA (l=300) 0.04 0.10 0.14 0.20 0.22 0.26 0.29 0.35 0.41 0.25
NFK(k=1) 0.10 0.15 0.20 0.23 0.26 0.27 0.28 0.32 0.34 0.30
Ext + standard 0.16 0.26 0.32 0.37 0.40 0.44 0.45 0.47 0.50 0.44
Ext + NFK(k=1) 0.13 0.23 0.28 0.33 0.38 0.42 0.44 0.48 0.50 0.42
Ext + NFK(k=4) 0.13 0.22 0.26 0.33 0.37 0.40 0.42 0.47 0.50 0.41
Ext + NFK (k=16) 0.12 0.20 0.25 0.32 0.36 0.40 0.42 0.47 0.50 0.40
</table>
<tableCaption confidence="0.8559095">
Table 1: Results of the different methods; F-1 score at different number of candidate translations. Ext refers
to the extended approach, whereas NFK stands for normalised Fisher kernel.
</tableCaption>
<bodyText confidence="0.997818545454545">
methods. Nevertheless, there are still a number of
issues that need be addressed. The most impor-
tant one concerns the combination of the different
methods, which could be optimised on a validation
set. Such a combination could involve Fisher ker-
nels with different latent classes in a first step, and
a final combination of the different methods. How-
ever, the results we obtained so far suggest that the
rank of the candidates is an important feature. It is
thus not guaranteed that we can gain over the com-
bination we used here.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="method">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999978928571428">
We have shown in this paper how the problem of
bilingual lexicon extraction from comparable cor-
pora could be interpreted in geometric terms, and
how this view led to the formulation of new solu-
tions. We have evaluated the methods we propose
on a comparable corpus extracted from the CLEF
colection, and shown the strengths and weaknesses
of each method. Our final results show that the com-
bination of relatively simple methods helps improve
the average precision of bilingual lexicon extrac-
tion methods from comparale corpora by 10 points.
We hope this work will help pave the way towards
a new generation of cross-lingual information re-
trieval systems.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999945857142857">
We thank J.-C. Chappelier and M. Rajman who
pointed to us the similarity between our extended
method and the model DSIR (distributional seman-
tics information retrieval), and provided us with
useful comments on a first draft of this paper. We
also want to thank three anonymous reviewers for
useful comments on a first version of this paper.
</bodyText>
<sectionHeader confidence="0.999219" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.976258827160494">
F. R. Bach and M. I. Jordan. 2001. Kernel inde-
pendent component analysis. Journal ofMachine
Learning Research.
R. Besanc¸on, M. Rajman, and J.-C. Chappelier.
1999. Textual similarities based on a distribu-
tional approach. In Proceedings of the Tenth In-
ternational Workshop on Database and Expert
Systems Applications (DEX’99), Florence, Italy.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
latent semantic analysis. Journal ofthe American
Society for Information Science, 41(6):391–407.
H. Dejean, E. Gaussier, and F. Sadat. 2002. An ap-
proach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In
International Conference on Computational Lin-
guistics, COLING’02.
A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the
Royal Statistical Society, Series B, 39(1):1–38.
Mona Diab and Steve Finch. 2000. A statisti-
cal word-level translation model for compara-
ble corpora. In Proceeding of the Conference
on Content-Based Multimedia Information Ac-
cess (RIAO).
Pascale Fung. 2000. A statistical view on bilingual
lexicon extraction - from parallel corpora to non-
parallel corpora. In J. V´eronis, editor, Parallel
Text Processing. Kluwer Academic Publishers.
G. Grefenstette. 1994. Explorations in Automatic
Thesaurus Construction. Kluwer Academic Pub-
lishers.
Thomas Hofmann. 1999. Probabilistic latent se-
mantic analysis. In Proceedings of the Fifteenth
Conference on Uncertainty in Artificial Intelli-
gence, pages 289–296. Morgan Kaufmann.
Thomas Hofmann. 2000. Learning the similarity of
documents: An information-geometric approach
to document retrieval and categorization. In Ad-
vances in Neural Information Processing Systems
12, page 914. MIT Press.
Tommi S. Jaakkola and David Haussler. 1999. Ex-
ploiting generative models in discriminative clas-
sifiers. In Advances in Neural Information Pro-
cessing Systems 11, pages 487–493.
Philipp Koehn and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora.
In ACL 2002 Workshop on Unsupervised Lexical
Acquisition.
P.A.W. Lewis, P.B. Baxendale, and J.L. Ben-
net. 1967. Statistical discrimination of the
synonym/antonym relationship between words.
Journal of the ACM.
C. Peters and E. Picchi. 1995. Capturing the com-
parable: A system for querying comparable text
corpora. In JADT’95 - 3rd International Con-
ference on Statistical Analysis of Textual Data,
pages 255–262.
R. Rapp. 1995. Identifying word translations in
nonparallel texts. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics.
I. Shahzad, K. Ohtake, S. Masuyama, and K. Ya-
mamoto. 1999. Identifying translations of com-
pound nouns using non-aligned corpora. In Pro-
ceedings of the Workshop MAL’99, pages 108–
113.
K. Tanaka and Hideya Iwasaki. 1996. Extraction of
lexical translations from non-aligned corpora. In
International Conference on Computational Lin-
guistics, COLING’96.
Naonori Ueda and Ryohei Nakano. 1995. Deter-
ministic annealing variant of the EM algorithm.
In Advances in Neural Information Processing
Systems 7, pages 545–552.
A. Vinokourov, J. Shawe-Taylor, and N. Cristian-
ini. 2002. Finding language-independent seman-
tic representation of text using kernel canonical
correlation analysis. In Advances in Neural In-
formation Processing Systems 12.
</reference>
<sectionHeader confidence="0.6402355" genericHeader="method">
Appendix A: probabilistic interpretation of
the extension of standard approach
</sectionHeader>
<bodyText confidence="0.999943125">
As in section 3, SQs-e is an n-dimensional vector,
defined over ((s1, tl), · · · , (sp, tk)). The coordinate
of SQsv on the axis corresponding to the transla-
tion pair (si, tj) is (Z, v ) (the one for TQtV on
the same axis being (t1 , V)). Thus, equation 4 can
be rewritten as:
which we can normalised in order to get a probabil-
ity distribution, leading to:
</bodyText>
<equation confidence="0.922859">
S(v, w) _  P(v)P(si|v)P(w|tj)P(tj)
(si,tj)
</equation>
<bodyText confidence="0.936899875">
By imposing P(tj) to be uniform, and by denoting
C a translation pair, one arrives at:
S(v, w) a P(v)P(C|v)P(w|C)
C
with the interpretation that only the source, resp.
target, word in C is relevant for P(C|v), resp.
P(w|C). Now, if we are looking for those ws clos-
est to a given v, we rely on:
</bodyText>
<equation confidence="0.808531333333333">

S(w|v) a
C
</equation>
<bodyText confidence="0.999921">
which is the probabilistic model adopted in (Dejean
et al., 2002). This latter model is thus a special case
of the extension we propose.
</bodyText>
<sectionHeader confidence="0.927068" genericHeader="method">
Appendix B: update formulas for PLSA
</sectionHeader>
<bodyText confidence="0.998875666666667">
The deterministic annealing EM algorithm for
PLSA (Hofmann, 1999) leads to the following equa-
tions for iteration t and temperature :
</bodyText>
<equation confidence="0.9909215">
P()P(w|)P(d|)
P(|w, d) _
P()P(w|)P(d|)
P(t+) () _ (w d) n(w, d) n( w, d)P( |w, d)
(w,d)
d n(w, d)P(|w, d)
P(t+)(w|) _
(w,d) n(w, d)P(|w, d)
P(t+) (d|) _ E w n(w,d)Pal , d d)
(w,d) n(w,d)P ( l
</equation>
<bodyText confidence="0.999963">
where n(w, d) is the number of co-occurrences be-
tween w and d. Parameters are obtained by iterating
eqs 11–11 for each , 0 &lt;  &lt; 1.
</bodyText>
<equation confidence="0.957543">
P(C|v)P(w|C)
S(v, w) _  (i, v )(��tj ,Vi )
(si,tj)
</equation>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.521675">
<title confidence="0.8493625">A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora</title>
<author confidence="0.710547">J-M I C H Research Centre Europe</author>
<address confidence="0.809872">6, Chemin de Maupertuis — 38320 Meylan, France</address>
<email confidence="0.990095">Eric.Gaussier@xrce.xerox.com</email>
<affiliation confidence="0.983475">of Computer Science, University of Chicago</affiliation>
<address confidence="0.996378">1100 E. 58th St. Chicago, IL 60637 USA</address>
<email confidence="0.999728">matveeva@cs.uchicago.edu</email>
<abstract confidence="0.999647555555556">We present a geometric view on bilingual lexicon extraction from comparable corpora, which allows to re-interpret the methods proposed so far and identify unresolved problems. This motivates three new methods that aim at solving these problems. Empirical evaluation shows the strengths and weaknesses of these methods, as well as a significant gain in the accuracy of extracted lexicons.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F R Bach</author>
<author>M I Jordan</author>
</authors>
<title>Kernel independent component analysis.</title>
<date>2001</date>
<journal>Journal ofMachine Learning Research.</journal>
<contexts>
<context position="13762" citStr="Bach and Jordan, 2001" startWordPosition="2419" endWordPosition="2422"> p e1 s1 S v e2 w that capture the implicit relations between translation pairs, and induce a natural mapping across languages. Denoting by s and t the directions in the source and target spaces, respectively, this may be formulated as:  i(s,-§&apos; (i))(t, T (i)) i(s, -§, (i)) j(t, T (j)) As in principal component analysis, once the first two directions (1s, 1t ) have been identified, the process can be repeated in the sub-space orthogonal to the one formed by the already identified directions. However, a general solution based on a set of eigenvalues can be proposed. Following e.g. (Bach and Jordan, 2001), the above problem can be reformulated as the following generalized eigenvalue problem: B  = D  (5) where, denoting again Rs and Rt the first m and last r (respectively) columns of C, we define:  0 RtR t RsR s RsR s RtR 0 t  (RsR  s )2  s  0 D = ,  = 0 (RtR t )2 t The standard approach to solve eq. 5 is to perform an incomplete Cholesky decomposition of a regularized form of D (Bach and Jordan, 2001). This yields pairs of source and target directions (1s, 1t ), · · · , (ls, lt) that define a new sub-space in which to project words from each language. This sub-space plays </context>
</contexts>
<marker>Bach, Jordan, 2001</marker>
<rawString>F. R. Bach and M. I. Jordan. 2001. Kernel independent component analysis. Journal ofMachine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Besanc¸on</author>
<author>M Rajman</author>
<author>J-C Chappelier</author>
</authors>
<title>Textual similarities based on a distributional approach.</title>
<date>1999</date>
<booktitle>In Proceedings of the Tenth International Workshop on Database and Expert Systems Applications (DEX’99),</booktitle>
<location>Florence, Italy.</location>
<marker>Besanc¸on, Rajman, Chappelier, 1999</marker>
<rawString>R. Besanc¸on, M. Rajman, and J.-C. Chappelier. 1999. Textual similarities based on a distributional approach. In Proceedings of the Tenth International Workshop on Database and Expert Systems Applications (DEX’99), Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal ofthe American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1886" citStr="Deerwester et al., 1990" startWordPosition="274" endWordPosition="277">allel documentations, most existing multilingual corpora are not parallel, but comparable. This concern is reflected in major evaluation conferences on crosslanguage information retrieval (CLIR), e.g. CLEF&apos;, which only use comparable corpora for their multilingual tracks. We adopt here a geometric view on bilingual lexicon extraction from comparable corpora which allows one to re-interpret the methods proposed thus far and formulate new ones inspired by latent semantic analysis (LSA), which was developed within the information retrieval (IR) community to treat synonymous and polysemous terms (Deerwester et al., 1990). We will explain in this paper the motivations behind the use of such methods for bilingual lexicon extraction from comparable corpora, and show how to apply them. Section 2 is devoted to the presentation of the standard approach, ie the approach adopted by most researchers so far, its geometric interpretation, and the unresolved synonymy 1http://clef.iei.pi.cnr.it:2002/ and polysemy problems. Sections 3 to 4 then describe three new methods aiming at addressing the issues raised by synonymy and polysemy: in section 3 we introduce an extension of the standard approach, and show in appendix A h</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal ofthe American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dejean</author>
<author>E Gaussier</author>
<author>F Sadat</author>
</authors>
<title>An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.</title>
<date>2002</date>
<booktitle>In International Conference on Computational Linguistics, COLING’02.</booktitle>
<contexts>
<context position="2572" citStr="Dejean et al., 2002" startWordPosition="388" endWordPosition="392">such methods for bilingual lexicon extraction from comparable corpora, and show how to apply them. Section 2 is devoted to the presentation of the standard approach, ie the approach adopted by most researchers so far, its geometric interpretation, and the unresolved synonymy 1http://clef.iei.pi.cnr.it:2002/ and polysemy problems. Sections 3 to 4 then describe three new methods aiming at addressing the issues raised by synonymy and polysemy: in section 3 we introduce an extension of the standard approach, and show in appendix A how this approach relates to the probabilistic method proposed in (Dejean et al., 2002); in section 4, we present a bilingual extension to LSA, namely canonical correlation analysis and its kernel version; lastly, in section 5, we formulate the problem in terms of probabilistic LSA and review different associated similarities. Section 6 is then devoted to a large-scale evaluation of the different methods proposed. Open issues are then discussed in section 7. 2 Standard approach Bilingual lexicon extraction from comparable corpora has been studied by a number of researchers, (Rapp, 1995; Peters and Picchi, 1995; Tanaka and Iwasaki, 1996; Shahzad et al., 1999; Fung, 2000, among ot</context>
<context position="11355" citStr="Dejean et al., 2002" startWordPosition="1943" endWordPosition="1946"> )=−�vTQTs STTQt−�w (4) A natural choice for Qs (and similarly for Qt) is the following m x p matrix: � � Qs = RT s = � 3This assumption has been experimentally validated in several studies, e.g. (Grefenstette, 1994; Lewis et al., 1967). but other choices, such as a pseudo-inverse of Rs, are possible. Note however that computing the pseudo-inverse of Rs is a complex operation, while the above projection is straightforward (the columns of Q correspond to the context vectors of the dictionary words). In appendix A we show how this method generalizes over the probabilistic approach presented in (Dejean et al., 2002). The above method bears similarities with the one described in (Besanc¸on et al., 1999), where a matrix similar to Qs is used to build a new term-document matrix. However, the motivations behind their work and ours differ, as do the derivations and the general framework, which justifies e.g. the choice of the pseudo-inverse of Rs in our case. 4 Canonical correlation analysis The data we have at our disposal can naturally be represented as an n x (m + r) matrix in which the rows correspond to translation pairs, and the columns to source and target vocabularies: e1 ··· em f1 ··· fr · · · · · · </context>
</contexts>
<marker>Dejean, Gaussier, Sadat, 2002</marker>
<rawString>H. Dejean, E. Gaussier, and F. Sadat. 2002. An approach based on multilingual thesauri and model combination for bilingual lexicon extraction. In International Conference on Computational Linguistics, COLING’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="16766" citStr="Dempster et al., 1977" startWordPosition="2937" endWordPosition="2940">-occurrences between vocabulary words w and translation pairs d, PLSA models the probability of co-occurrence w and d via latent classes :  P(w, d) = P() P(w|) P(d|) (7) where, for a given class, words and translation pairs are assumed to be independently generated from class-conditional probabilities P(w|) and P(d|). Note here that the latter distribution is languageindependent, and that the same latent classes are used for the two languages. The parameters of the model are obtained by maximizing the likelihood of the observed data (matrix C) through ExpectationMaximisation algorithm (Dempster et al., 1977). In  = max s,t B = , f2 (2 2) st w&amp;quot; f 1 2 t (1s, 1t ) f1 it (ls, lt) l t fr (CCA) w v&amp;quot; 2 t 2 s v em is 1 s l s e2 (CCA) e1 e2 v e1 em f1 fr w Figure 3: Geometric view of the Canonical Correlation Analysis approach addition, in order to reduce the sensitivity to initial conditions, we use a deterministic annealing scheme (Ueda and Nakano, 1995). The update formulas for the EM algorithm are given in appendix B. This model can identify relevant bilingual latent classes, but does not directly define a similarity between words across languages. That may be done by using Fisher ker</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Steve Finch</author>
</authors>
<title>A statistical word-level translation model for comparable corpora.</title>
<date>2000</date>
<booktitle>In Proceeding of the Conference on Content-Based Multimedia Information Access (RIAO).</booktitle>
<contexts>
<context position="4908" citStr="Diab and Finch, 2000" startWordPosition="777" endWordPosition="780">ion of W: ���� � (�� v , tr(w)) = e �= a(v, e) a(w, f) (1) (e,f)ED Because of the translation step, only the pairs (e, f) that are present in the dictionary contribute to the dot-product. Note that this approach requires some general bilingual dictionary as initial seed. One way to circumvent this requirement consists in automatically building a seed lexicon based on spelling and cognates clues (Koehn and Knight, 2002). Another approach directly tackles the problem from scratch by searching for a translation mapping which optimally preserves the intralingual association measure between words (Diab and Finch, 2000): the underlying assumption is that pairs of words which are highly associated in one language should have translations that are highly associated in the other language. In this latter case, the association measure is defined as the Spearman rank order correlation between their context vectors restricted to “peripheral tokens” (highly frequent words). The search method is based on a gradient descent algorithm, by iteratively changing the mapping of a single word until (locally) minimizing the sum of squared differences between the association measure of all pairs of words in one language and t</context>
</contexts>
<marker>Diab, Finch, 2000</marker>
<rawString>Mona Diab and Steve Finch. 2000. A statistical word-level translation model for comparable corpora. In Proceeding of the Conference on Content-Based Multimedia Information Access (RIAO).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A statistical view on bilingual lexicon extraction - from parallel corpora to nonparallel corpora.</title>
<date>2000</date>
<booktitle>Parallel Text Processing.</booktitle>
<editor>In J. V´eronis, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3162" citStr="Fung, 2000" startWordPosition="488" endWordPosition="489">Dejean et al., 2002); in section 4, we present a bilingual extension to LSA, namely canonical correlation analysis and its kernel version; lastly, in section 5, we formulate the problem in terms of probabilistic LSA and review different associated similarities. Section 6 is then devoted to a large-scale evaluation of the different methods proposed. Open issues are then discussed in section 7. 2 Standard approach Bilingual lexicon extraction from comparable corpora has been studied by a number of researchers, (Rapp, 1995; Peters and Picchi, 1995; Tanaka and Iwasaki, 1996; Shahzad et al., 1999; Fung, 2000, among others). Their work relies on the assumption that if two words are mutual translations, then their more frequent collocates (taken here in a very broad sense) are likely to be mutual translations as well. Based on this assumption, the standard approach builds context vectors for each source and target word, translates the target context vectors using a general bilingual dictionary, and compares the translation with the source context vector: 1. For each source word v (resp. target word w), build a context vector v (resp. 2Vi) consisting in the measure of association of each word e (res</context>
</contexts>
<marker>Fung, 2000</marker>
<rawString>Pascale Fung. 2000. A statistical view on bilingual lexicon extraction - from parallel corpora to nonparallel corpora. In J. V´eronis, editor, Parallel Text Processing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Construction.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="10950" citStr="Grefenstette, 1994" startWordPosition="1879" endWordPosition="1880">s. This time, if v is close to i and Tj, si and sj being synonyms, the translations of both si and sj will be used to find those words w close to v. Figure 2 illustrates this process. By denoting Qs, respectively Qt, such a mapping in the source (resp. target) side, and using the same translation mapping (S, T) as above, the similarity between source and target words becomes: S(v, w)=(SQs−�v , TQt−�w )=−�vTQTs STTQt−�w (4) A natural choice for Qs (and similarly for Qt) is the following m x p matrix: � � Qs = RT s = � 3This assumption has been experimentally validated in several studies, e.g. (Grefenstette, 1994; Lewis et al., 1967). but other choices, such as a pseudo-inverse of Rs, are possible. Note however that computing the pseudo-inverse of Rs is a complex operation, while the above projection is straightforward (the columns of Q correspond to the context vectors of the dictionary words). In appendix A we show how this method generalizes over the probabilistic approach presented in (Dejean et al., 2002). The above method bears similarities with the one described in (Besanc¸on et al., 1999), where a matrix similar to Qs is used to build a new term-document matrix. However, the motivations behind</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Explorations in Automatic Thesaurus Construction. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>289--296</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="16112" citStr="Hofmann, 1999" startWordPosition="2836" endWordPosition="2837"> to cluster this matrix such that translation pairs with synonymous words appear in the same cluster, while translation pairs with polysemous words appear in different clusters (soft clustering). Furthermore, because of the symmetry between the roles played by translation pairs and vocabulary words (synonymous and polysemous vocabulary words should also behave as described above), we want the clustering to behave symmetrically with respect to translation pairs and vocabulary words. One well-motivated method that fulfills all the above criteria is Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999). Assuming that C encodes the co-occurrences between vocabulary words w and translation pairs d, PLSA models the probability of co-occurrence w and d via latent classes :  P(w, d) = P() P(w|) P(d|) (7) where, for a given class, words and translation pairs are assumed to be independently generated from class-conditional probabilities P(w|) and P(d|). Note here that the latter distribution is languageindependent, and that the same latent classes are used for the two languages. The parameters of the model are obtained by maximizing the likelihood of the observed data (matrix C) through Exp</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, pages 289–296. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Learning the similarity of documents: An information-geometric approach to document retrieval and categorization.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 12,</booktitle>
<pages>914</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="17932" citStr="Hofmann, 2000" startWordPosition="3147" endWordPosition="3148"> languages. That may be done by using Fisher kernels as described below. Associated similarities: Fisher kernels Fisher kernels (Jaakkola and Haussler, 1999) derive a similarity measure from a probabilistic model. They are useful whenever a direct similarity between observed feature is hard to define or insufficient. Denoting e(w) = lnP(w|B) the loglikelihood for example w, the Fisher kernel is: K(w1, w2) = VE(w1)TIF−1Vf(w2) (8) The Fisher information matrix IF = �V�(x)V�(x)T� E keeps the kernel independent of reparameterisation. With a suitable parameterisation, we assume IF pz� 1. For PLSA (Hofmann, 2000), the Fisher kernel between two words w1 and w2 becomes: P(a|d,w1)P(a|d,w2) P(d|a) where d ranges over the translation pairs. The Fisher kernel performs a dot-product in a vector space defined by the parameters of the model. With only one class, the expression of the Fisher kernel (9) reduces to: E K(w1, w2) = 1 + d Apart from the additional intercept (’1’), this is exactly the similarity provided by the standard method, with associations given by scaled empirical frequencies a(w, d) = P�(d|w)/�IP(d). Accordingly, we expect that the standard method and the Fisher kernel with one class should h</context>
</contexts>
<marker>Hofmann, 2000</marker>
<rawString>Thomas Hofmann. 2000. Learning the similarity of documents: An information-geometric approach to document retrieval and categorization. In Advances in Neural Information Processing Systems 12, page 914. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi S Jaakkola</author>
<author>David Haussler</author>
</authors>
<title>Exploiting generative models in discriminative classifiers.</title>
<date>1999</date>
<booktitle>In Advances in Neural Information Processing Systems 11,</booktitle>
<pages>487--493</pages>
<contexts>
<context position="17475" citStr="Jaakkola and Haussler, 1999" startWordPosition="3071" endWordPosition="3074">l t fr (CCA) w v&amp;quot; 2 t 2 s v em is 1 s l s e2 (CCA) e1 e2 v e1 em f1 fr w Figure 3: Geometric view of the Canonical Correlation Analysis approach addition, in order to reduce the sensitivity to initial conditions, we use a deterministic annealing scheme (Ueda and Nakano, 1995). The update formulas for the EM algorithm are given in appendix B. This model can identify relevant bilingual latent classes, but does not directly define a similarity between words across languages. That may be done by using Fisher kernels as described below. Associated similarities: Fisher kernels Fisher kernels (Jaakkola and Haussler, 1999) derive a similarity measure from a probabilistic model. They are useful whenever a direct similarity between observed feature is hard to define or insufficient. Denoting e(w) = lnP(w|B) the loglikelihood for example w, the Fisher kernel is: K(w1, w2) = VE(w1)TIF−1Vf(w2) (8) The Fisher information matrix IF = �V�(x)V�(x)T� E keeps the kernel independent of reparameterisation. With a suitable parameterisation, we assume IF pz� 1. For PLSA (Hofmann, 2000), the Fisher kernel between two words w1 and w2 becomes: P(a|d,w1)P(a|d,w2) P(d|a) where d ranges over the translation pairs. The Fisher kernel</context>
</contexts>
<marker>Jaakkola, Haussler, 1999</marker>
<rawString>Tommi S. Jaakkola and David Haussler. 1999. Exploiting generative models in discriminative classifiers. In Advances in Neural Information Processing Systems 11, pages 487–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In ACL 2002 Workshop on Unsupervised Lexical Acquisition.</booktitle>
<contexts>
<context position="4709" citStr="Koehn and Knight, 2002" startWordPosition="748" endWordPosition="751">cients, or the cosine measure. As the dot-product plays a central role in all these measures, we consider, without loss of generality, the similarity given by the dot-product between v and the translation of W: ���� � (�� v , tr(w)) = e �= a(v, e) a(w, f) (1) (e,f)ED Because of the translation step, only the pairs (e, f) that are present in the dictionary contribute to the dot-product. Note that this approach requires some general bilingual dictionary as initial seed. One way to circumvent this requirement consists in automatically building a seed lexicon based on spelling and cognates clues (Koehn and Knight, 2002). Another approach directly tackles the problem from scratch by searching for a translation mapping which optimally preserves the intralingual association measure between words (Diab and Finch, 2000): the underlying assumption is that pairs of words which are highly associated in one language should have translations that are highly associated in the other language. In this latter case, the association measure is defined as the Spearman rank order correlation between their context vectors restricted to “peripheral tokens” (highly frequent words). The search method is based on a gradient descen</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In ACL 2002 Workshop on Unsupervised Lexical Acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A W Lewis</author>
<author>P B Baxendale</author>
<author>J L Bennet</author>
</authors>
<title>Statistical discrimination of the synonym/antonym relationship between words.</title>
<date>1967</date>
<journal>Journal of the ACM.</journal>
<contexts>
<context position="10971" citStr="Lewis et al., 1967" startWordPosition="1881" endWordPosition="1884">s close to i and Tj, si and sj being synonyms, the translations of both si and sj will be used to find those words w close to v. Figure 2 illustrates this process. By denoting Qs, respectively Qt, such a mapping in the source (resp. target) side, and using the same translation mapping (S, T) as above, the similarity between source and target words becomes: S(v, w)=(SQs−�v , TQt−�w )=−�vTQTs STTQt−�w (4) A natural choice for Qs (and similarly for Qt) is the following m x p matrix: � � Qs = RT s = � 3This assumption has been experimentally validated in several studies, e.g. (Grefenstette, 1994; Lewis et al., 1967). but other choices, such as a pseudo-inverse of Rs, are possible. Note however that computing the pseudo-inverse of Rs is a complex operation, while the above projection is straightforward (the columns of Q correspond to the context vectors of the dictionary words). In appendix A we show how this method generalizes over the probabilistic approach presented in (Dejean et al., 2002). The above method bears similarities with the one described in (Besanc¸on et al., 1999), where a matrix similar to Qs is used to build a new term-document matrix. However, the motivations behind their work and ours </context>
</contexts>
<marker>Lewis, Baxendale, Bennet, 1967</marker>
<rawString>P.A.W. Lewis, P.B. Baxendale, and J.L. Bennet. 1967. Statistical discrimination of the synonym/antonym relationship between words. Journal of the ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Peters</author>
<author>E Picchi</author>
</authors>
<title>Capturing the comparable: A system for querying comparable text corpora.</title>
<date>1995</date>
<booktitle>In JADT’95 - 3rd International Conference on Statistical Analysis of Textual Data,</booktitle>
<pages>255--262</pages>
<contexts>
<context position="3102" citStr="Peters and Picchi, 1995" startWordPosition="476" endWordPosition="479">dix A how this approach relates to the probabilistic method proposed in (Dejean et al., 2002); in section 4, we present a bilingual extension to LSA, namely canonical correlation analysis and its kernel version; lastly, in section 5, we formulate the problem in terms of probabilistic LSA and review different associated similarities. Section 6 is then devoted to a large-scale evaluation of the different methods proposed. Open issues are then discussed in section 7. 2 Standard approach Bilingual lexicon extraction from comparable corpora has been studied by a number of researchers, (Rapp, 1995; Peters and Picchi, 1995; Tanaka and Iwasaki, 1996; Shahzad et al., 1999; Fung, 2000, among others). Their work relies on the assumption that if two words are mutual translations, then their more frequent collocates (taken here in a very broad sense) are likely to be mutual translations as well. Based on this assumption, the standard approach builds context vectors for each source and target word, translates the target context vectors using a general bilingual dictionary, and compares the translation with the source context vector: 1. For each source word v (resp. target word w), build a context vector v (resp. 2Vi) </context>
</contexts>
<marker>Peters, Picchi, 1995</marker>
<rawString>C. Peters and E. Picchi. 1995. Capturing the comparable: A system for querying comparable text corpora. In JADT’95 - 3rd International Conference on Statistical Analysis of Textual Data, pages 255–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Identifying word translations in nonparallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3077" citStr="Rapp, 1995" startWordPosition="474" endWordPosition="475">how in appendix A how this approach relates to the probabilistic method proposed in (Dejean et al., 2002); in section 4, we present a bilingual extension to LSA, namely canonical correlation analysis and its kernel version; lastly, in section 5, we formulate the problem in terms of probabilistic LSA and review different associated similarities. Section 6 is then devoted to a large-scale evaluation of the different methods proposed. Open issues are then discussed in section 7. 2 Standard approach Bilingual lexicon extraction from comparable corpora has been studied by a number of researchers, (Rapp, 1995; Peters and Picchi, 1995; Tanaka and Iwasaki, 1996; Shahzad et al., 1999; Fung, 2000, among others). Their work relies on the assumption that if two words are mutual translations, then their more frequent collocates (taken here in a very broad sense) are likely to be mutual translations as well. Based on this assumption, the standard approach builds context vectors for each source and target word, translates the target context vectors using a general bilingual dictionary, and compares the translation with the source context vector: 1. For each source word v (resp. target word w), build a cont</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>R. Rapp. 1995. Identifying word translations in nonparallel texts. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Shahzad</author>
<author>K Ohtake</author>
<author>S Masuyama</author>
<author>K Yamamoto</author>
</authors>
<title>Identifying translations of compound nouns using non-aligned corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop MAL’99,</booktitle>
<pages>108--113</pages>
<contexts>
<context position="3150" citStr="Shahzad et al., 1999" startWordPosition="484" endWordPosition="487">c method proposed in (Dejean et al., 2002); in section 4, we present a bilingual extension to LSA, namely canonical correlation analysis and its kernel version; lastly, in section 5, we formulate the problem in terms of probabilistic LSA and review different associated similarities. Section 6 is then devoted to a large-scale evaluation of the different methods proposed. Open issues are then discussed in section 7. 2 Standard approach Bilingual lexicon extraction from comparable corpora has been studied by a number of researchers, (Rapp, 1995; Peters and Picchi, 1995; Tanaka and Iwasaki, 1996; Shahzad et al., 1999; Fung, 2000, among others). Their work relies on the assumption that if two words are mutual translations, then their more frequent collocates (taken here in a very broad sense) are likely to be mutual translations as well. Based on this assumption, the standard approach builds context vectors for each source and target word, translates the target context vectors using a general bilingual dictionary, and compares the translation with the source context vector: 1. For each source word v (resp. target word w), build a context vector v (resp. 2Vi) consisting in the measure of association of each</context>
</contexts>
<marker>Shahzad, Ohtake, Masuyama, Yamamoto, 1999</marker>
<rawString>I. Shahzad, K. Ohtake, S. Masuyama, and K. Yamamoto. 1999. Identifying translations of compound nouns using non-aligned corpora. In Proceedings of the Workshop MAL’99, pages 108– 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tanaka</author>
<author>Hideya Iwasaki</author>
</authors>
<title>Extraction of lexical translations from non-aligned corpora.</title>
<date>1996</date>
<booktitle>In International Conference on Computational Linguistics, COLING’96.</booktitle>
<contexts>
<context position="3128" citStr="Tanaka and Iwasaki, 1996" startWordPosition="480" endWordPosition="483">elates to the probabilistic method proposed in (Dejean et al., 2002); in section 4, we present a bilingual extension to LSA, namely canonical correlation analysis and its kernel version; lastly, in section 5, we formulate the problem in terms of probabilistic LSA and review different associated similarities. Section 6 is then devoted to a large-scale evaluation of the different methods proposed. Open issues are then discussed in section 7. 2 Standard approach Bilingual lexicon extraction from comparable corpora has been studied by a number of researchers, (Rapp, 1995; Peters and Picchi, 1995; Tanaka and Iwasaki, 1996; Shahzad et al., 1999; Fung, 2000, among others). Their work relies on the assumption that if two words are mutual translations, then their more frequent collocates (taken here in a very broad sense) are likely to be mutual translations as well. Based on this assumption, the standard approach builds context vectors for each source and target word, translates the target context vectors using a general bilingual dictionary, and compares the translation with the source context vector: 1. For each source word v (resp. target word w), build a context vector v (resp. 2Vi) consisting in the measure </context>
</contexts>
<marker>Tanaka, Iwasaki, 1996</marker>
<rawString>K. Tanaka and Hideya Iwasaki. 1996. Extraction of lexical translations from non-aligned corpora. In International Conference on Computational Linguistics, COLING’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naonori Ueda</author>
<author>Ryohei Nakano</author>
</authors>
<title>Deterministic annealing variant of the EM algorithm.</title>
<date>1995</date>
<booktitle>In Advances in Neural Information Processing Systems 7,</booktitle>
<pages>545--552</pages>
<contexts>
<context position="17129" citStr="Ueda and Nakano, 1995" startWordPosition="3018" endWordPosition="3021">on is languageindependent, and that the same latent classes are used for the two languages. The parameters of the model are obtained by maximizing the likelihood of the observed data (matrix C) through ExpectationMaximisation algorithm (Dempster et al., 1977). In  = max s,t B = , f2 (2 2) st w&amp;quot; f 1 2 t (1s, 1t ) f1 it (ls, lt) l t fr (CCA) w v&amp;quot; 2 t 2 s v em is 1 s l s e2 (CCA) e1 e2 v e1 em f1 fr w Figure 3: Geometric view of the Canonical Correlation Analysis approach addition, in order to reduce the sensitivity to initial conditions, we use a deterministic annealing scheme (Ueda and Nakano, 1995). The update formulas for the EM algorithm are given in appendix B. This model can identify relevant bilingual latent classes, but does not directly define a similarity between words across languages. That may be done by using Fisher kernels as described below. Associated similarities: Fisher kernels Fisher kernels (Jaakkola and Haussler, 1999) derive a similarity measure from a probabilistic model. They are useful whenever a direct similarity between observed feature is hard to define or insufficient. Denoting e(w) = lnP(w|B) the loglikelihood for example w, the Fisher kernel is: K(w1, w2) = </context>
</contexts>
<marker>Ueda, Nakano, 1995</marker>
<rawString>Naonori Ueda and Ryohei Nakano. 1995. Deterministic annealing variant of the EM algorithm. In Advances in Neural Information Processing Systems 7, pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vinokourov</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Finding language-independent semantic representation of text using kernel canonical correlation analysis.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 12.</booktitle>
<contexts>
<context position="14990" citStr="Vinokourov et al., 2002" startWordPosition="2660" endWordPosition="2664">same role as the sub-space defined by translation pairs in the standard method, although with CCA, it is derived from the corpus via the context vectors of the translation pairs. Once projected, words from different languages can be compared through their dot-product or cosine. Denoting °s = s1 ,... s l , and °t =t , . . . t , the similarity becomes (figure 3): S(v, w) = (°sV , °tV) = v°s °tV (6) The number l of vectors retained in each language directly defines the dimensions of the final subspace used for comparing words across languages. CCA and its kernelised version were used in (Vinokourov et al., 2002) as a way to build a crosslingual information retrieval system from parallel corpora. We show here that it can be used to infer language-independent semantic representations from comparable corpora, which induce a similarity between words in the source and target languages. 5 Multilingual probabilistic latent semantic analysis The matrix C described above encodes in each row k the context vectors of the source (first m columns) and target (last r columns) of each translation pair. Ideally, we would like to cluster this matrix such that translation pairs with synonymous words appear in the same</context>
</contexts>
<marker>Vinokourov, Shawe-Taylor, Cristianini, 2002</marker>
<rawString>A. Vinokourov, J. Shawe-Taylor, and N. Cristianini. 2002. Finding language-independent semantic representation of text using kernel canonical correlation analysis. In Advances in Neural Information Processing Systems 12.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>