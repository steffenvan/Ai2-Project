<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000035">
<title confidence="0.9975775">
Trace Prediction and Recovery
With Unlexicalized PCFGs and Slash Features
</title>
<author confidence="0.995684">
Helmut Schmid
</author>
<affiliation confidence="0.998537">
IMS, University of Stuttgart
</affiliation>
<email confidence="0.988832">
schmid@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.982457" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959681818182">
This paper describes a parser which gen-
erates parse trees with empty elements in
which traces and fillers are co-indexed.
The parser is an unlexicalized PCFG
parser which is guaranteed to return the
most probable parse. The grammar is
extracted from a version of the PENN
treebank which was automatically anno-
tated with features in the style of Klein
and Manning (2003). The annotation in-
cludes GPSG-style slash features which
link traces and fillers, and other features
which improve the general parsing accu-
racy. In an evaluation on the PENN tree-
bank (Marcus et al., 1993), the parser
outperformed other unlexicalized PCFG
parsers in terms of labeled bracketing f-
score. Its results for the empty cate-
gory prediction task and the trace-filler co-
indexation task exceed all previously re-
ported results with 84.1% and 77.4% f-
score, respectively.
</bodyText>
<sectionHeader confidence="0.99516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999787923076923">
Empty categories (also called null elements) are
used in the annotation of the PENN treebank (Mar-
cus et al., 1993) in order to represent syntactic
phenomena like constituent movement (e.g. wh-
extraction), discontinuous constituents, and miss-
ing elements (PRO elements, empty complemen-
tizers and relative pronouns). Moved constituents
are co-indexed with a trace which is located at
the position where the moved constituent is to be
interpreted. Figure 1 shows an example of con-
stituent movement in a relative clause.
Empty categories provide important informa-
tion for the semantic interpretation, in particular
</bodyText>
<figureCaption confidence="0.999437">
Figure 1: Co-indexation of traces and fillers
</figureCaption>
<bodyText confidence="0.99944292">
for determining the predicate-argument structure
of a sentence. However, most broad-coverage sta-
tistical parsers (Collins, 1997; Charniak, 2000,
and others) which are trained on the PENN tree-
bank generate parse trees without empty cate-
gories. In order to augment such parsers with
empty category prediction, three rather different
strategies have been proposed: (i) pre-processing
of the input sentence with a tagger which inserts
empty categories into the input string of the parser
(Dienes and Dubey, 2003b; Dienes and Dubey,
2003a). The parser treats the empty elements
like normal input tokens. (ii) post-processing
of the parse trees with a pattern matcher which
adds empty categories after parsing (Johnson,
2001; Campbell, 2004; Levy and Manning, 2004)
(iii) in-processing of the empty categories with a
slash percolation mechanism (Dienes and Dubey,
2003b; Dienes and Dubey, 2003a). The empty el-
ements are here generated by the grammar.
Good results have been obtained with all three
approaches, but (Dienes and Dubey, 2003b) re-
ported that in their experiments, the in-processing
of the empty categories only worked with lexi-
calized parsing. They explain that their unlex-
</bodyText>
<figure confidence="0.96429632">
IN
of
WHNP
WDT
which
NP
NP
NNS
things
WHPP-1
S
VP
JJ
unaware
PP
-NONE-
*T*-1
SBAR
VBP
are
ADJP-PRD
NP-SBJ
PRP
they
177
</figure>
<note confidence="0.8575695">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 177–184,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.9995446875">
icalized PCFG parser produced poor results be-
cause the beam search strategy applied there elim-
inated many correct constituents with empty ele-
ments. The scores of these constituents were too
low compared with the scores of constituents with-
out empty elements. They speculated that “doing
an exhaustive search might help” here.
In this paper, we confirm this hypothesis and
show that it is possible to accurately predict empty
categories with unlexicalized PCFG parsing and
slash features if the true Viterbi parse is com-
puted. In our experiments, we used the BitPar
parser (Schmid, 2004) and a PCFG which was ex-
tracted from a version of the PENN treebank that
was automatically annotated with features in the
style of (Klein and Manning, 2003).
</bodyText>
<sectionHeader confidence="0.970233" genericHeader="method">
2 Feature Annotation
</sectionHeader>
<bodyText confidence="0.939839842105263">
A context-free grammar which generates empty
categories has to make sure that a filler exists for
each trace and vice versa. A well-known tech-
nique which enforces this constraint is the GPSG-
style percolation of a slash feature: All con-
stituents on the direct path from the trace to the
filler are annotated with a special feature which
represents the category of the filler as shown in fig-
ure 2. In order to restore the original treebank an-
Figure 2: Slash features: The filler node of cate-
gory WHNP is linked to the trace node via perco-
lation of a slash feature. The trace node is labeled
with *T*.
of the trace, i.e. the filler c-comman
ds the trace
node, but in case of clausal fillers it is also possi-
is the sentence
She had
he informed her *-
</bodyText>
<equation confidence="0.672679">
1
</equation>
<bodyText confidence="0.9047496">
whose parse tree is shown in
figure 3.
Besides the slash features, we used other fea-
tures in order to improve the parsing accuracy of
the PCFG, inspired by the work of Klein and Man-
ning (2003). The most important ones of these
will now be described in detail. Sec-
tion 4.3 shows the impact of these features on
labeled bracketing accuracy and empty category
pre
</bodyText>
<figure confidence="0.908385404255319">
“S-1
–
– kidneytrouble”
features1
diction.
NP
VP/WHPP
VBP
are JJ PP/WHPP
unaware -NONE-/WHPP
*T*/WHPP
NP
NNS
things
IN
of
WHNP
WDT
which
NP-SBJ
PRP
they
ADJP-PRD/WHPP
SBAR
WHPP/WHPP
S/WHPP
d passive VPs.
tween imperatives, finite clauses, an
d several types
of small clauses.
PP, ADVP, RB an
d NP-ADV were annotated with
a parent feature (cf. Johnson (1998)). The
parent features distinguish between verbal (VP)
178 ble that the filler dominates the trace. An example
adjectival (ADJP, WHADJP), adverbial (ADVP,
WHADVP), nominal (NP, WHNP, QP), preposi-
tional (PP) and other parents.
es.
between be, do, have, an
d full verbs.
3s (n3s) if they are headed by a verb with part
-of-
speech VBZ (VBP).
node of the category
POS (possessive marker) are
marked with a genitive flag.
</figure>
<bodyText confidence="0.982259954545455">
Base NPs NPs dominating a node of category
NN, NNS, NNP, NNPS, DT, CD, JJ, JJR, JJS, PRP,
RB, or EX are marked as base NPs.
notation with co-reference indices from the repre-
sentation with slash features, the parse tree has to
be traversed starting at a trace node and following
the nodes annotated with the respective filler cate-
gory until the filler node is encountered. Normally,
the filler node is a sister node of an ancestor node
VP feature VPs were annotated with a feature
that distinguishes between finite, infinitive, to-
infinitive, gerund, past participle, an
S feature The S node feature distinguishes be-
Parent features Modifier categories like SBAR,
PENN tags The PENN treebank annotation uses
semantic tags to refine syntactic categories. Most
parsers ignore this information. We preserved
the tags ADV, CLR, DIR, EXT, IMP, LGS, LOC,
MNR, NOM, PRD, PRP, SBJ and TMP in combi-
nation with selected categori
Auxiliary feature We added a feature to the
part-of-speech tags of verbs in order to distinguish
</bodyText>
<figure confidence="0.7505085">
Agreement feature Finite VPs are marked with
Genitive feature NP nodes which dominate a
&apos;The complete annotation program is available
from the author’s home page at http://www.ims.uni-
stuttgart.de/ schmid
,
</figure>
<figureCaption confidence="0.99588">
Figure 3: Example of a filler which dominates its trace
</figureCaption>
<figure confidence="0.9970822">
S-1
NP-SBJ
PRP
She
VP
0
S
-NONE-
*T*-1
VP
VBD
had
:
S
PRN
:
–
NP-SBJ
PRP
he
VBD
informed
NP
PRP
her
NP
–
NN
kidney
NN
trouble
SBAR
-NONE-
.
.
</figure>
<bodyText confidence="0.997190742424243">
IN feature The part-of-speech tags of the 45
most frequent prepositions were lexicalized by
adding the preposition as a feature. The new part-
of-speech tag of the preposition “by” is “IN/by”.
Irregular adverbs The part-of-speech tags of
the adverbs “as”, “so”, “about”, and “not” were
also lexicalized.
Currency feature NP and QP nodes are marked
with a currency flag if they dominate a node of
category $, #, or SYM.
Percent feature Nodes of the category NP or
QP are marked with a percent flag if they dominate
the subtree (NN %). Any node which immediately
dominates the token %, is marked, as well.
Punctuation feature Nodes which dominate
sentential punctuation (.?!) are marked.
DT feature Nodes of category DT are split into
indefinite articles (a, an), definite articles (the),
and demonstratives (this, that, those, these).
WH feature The wh-tags (WDT, WP, WRB,
WDT) of the words which, what, who, how, and
that are also lexicalized.
Colon feature The part-of-speech tag ’:’ was re-
placed with “;”, “–” or “...” if it dominated a cor-
responding token.
DomV feature Nodes of a non-verbal syntactic
category are marked with a feature if they domi-
nate a node of category VP, SINV, S, SQ, SBAR,
or SBARQ.
Gap feature S nodes dominating an empty NP
are marked with the feature gap.
Subcategorization feature The part-of-speech
tags of verbs are annotated with a feature which
encodes the sequence of arguments. The encod-
ing maps reflexive NPs to r, NP/NP-PRD/SBAR-
NOM to n, ADJP-PRD to j, ADVP-PRD to a,
PRT to t, PP/PP-DIR to p, SBAR/SBAR-CLR to
b, S/fin to sf, S/ppres/gap to sg, S/to/gap to st,
other S nodes to so, VP/ppres to vg, VP/ppast to
vn, VP/pas to vp, VP/inf to vi, and other VPs to
vo. A verb with an NP and a PP argument, for
instance, is annotated with the feature np.
Adjectives, adverbs, and nouns may also get a
subcat feature which encodes a single argument
using a less fine-grained encoding which maps PP
to p, NP to n, S to s, and SBAR to b. A node of
category NN or NNS e.g. is marked with a subcat
feature if it is followed by an argument category
unless the argument is a PP which is headed by
the preposition of.
RC feature In relative clauses with an empty
relative pronoun of category WHADVP, we mark
the SBAR node of the relative clause, the NP node
to which it is attached, and its head child of cate-
gory NN or NNS, if the head word is either way,
ways, reason, reasons, day, days, time, moment,
place, or position. This feature helps the parser
to correctly insert WHADVP rather than WHNP.
Figure 4 shows a sample tree.
TMP features Each node on the path between
an NP-TMP or PP-TMP node and its nominal head
is labeled with the feature tmp. This feature helps
the parser to identify temporal NPs and PPs.
MNR and EXT features Similarly, each node
on the path between an NP-EXT, NP-MNR or
ADVP-TMP node and its head is labeled with the
</bodyText>
<figure confidence="0.984793125">
179
NP
NP/x
NN/x
time
ADVP-TMP
-NONE-
*T*-1
</figure>
<figureCaption confidence="0.990841">
Figure 4: Annotation of relative clauses with
empty relative pronoun of category WHADVP
</figureCaption>
<figure confidence="0.996939857142857">
SBAR/x
WHADVP-1
-NONE-
0
NP-SBJ
-NONE-
*
TO
to
S
VB
relax
VP
VP
</figure>
<bodyText confidence="0.996824825396825">
feature ext or mnr.
ADJP features Nodes of category ADJP which
are dominated by an NP node are labeled with the
feature “post” if they are in final position and the
feature “attr” otherwise.
JJ feature Nodes of category JJ which are dom-
inated by an ADJP-PRD node are labeled with the
feature “prd”.
JJ-tmp feature JJ nodes which are dominated
by an NP-TMP node and which themselves dom-
inate one of the words “last”, “next”, “late”, “pre-
vious”, “early”, or “past” are labeled with tmp.
QP feature If some node dominates an NP node
followed by an NP-ADV node as in (NP (NP one
dollar) (NP-ADV a day)), the first child NP node
is labeled with the feature “qp”. If the parent is an
NP node, it is also labeled with “qp”.
NP-pp feature NP nodes which dominate a PP
node are labeled with the feature pp. If this PP
itself is headed by the preposition of, then it is an-
notated with the feature of.
MWL feature In adverbial phrases which nei-
ther dominate an adverb nor another adverbial
phrase, we lexicalize the part-of-speech tags of a
small set of words like “least” (at least), “kind”, or
“sort” which appear frequently in such adverbial
phrases.
Case feature Pronouns like he or him , but not
ambiguous pronouns like it are marked with nom
or acc, respectively.
Expletives If a subject NP dominates an NP
which consists of the pronoun it, and an S-trace in
sentences like It is important to..., the dominated
NP is marked with the feature expl.
LST feature The parent nodes of LST nodes2
are marked with the feature lst.
Complex conjunctions In SBAR constituents
starting with an IN and an NN child node (usu-
ally indicating one of the two complex conjunc-
tions “in order to” or “in case of”), we mark the
NN child with the feature sbar.
LGS feature The PENN treebank marks the
logical subject of passive clauses which are real-
ized by a by-PP with the semantic tag LGS. We
move this tag to the dominating PP.
OC feature Verbs are marked with an object
control feature if they have an NP argument which
dominates an NP filler and an S argument which
dominates an NP trace. An example is the sen-
tence She asked him to come.
Corrections The part-of-speech tags of the
PENN treebank are not always correct. Some of
the errors (like the tag NNS in VP-initial position)
can be identified and corrected automatically in
the training data. Correcting tags did not always
improve parsing accuracy, so it was done selec-
tively.
The gap and domV features described above
were also used by Klein and Manning (2003).
All features were automatically added to the
PENN treebank by means of an annotation pro-
gram. Figure 5 shows an example of an annotated
parse tree.
</bodyText>
<sectionHeader confidence="0.985624" genericHeader="method">
3 Parameter Smoothing
</sectionHeader>
<bodyText confidence="0.999982071428572">
We extracted the grammar from sections 2–21 of
the annotated version of the PENN treebank. In
order to increase the coverage of the grammar,
we selectively applied markovization to the gram-
mar (cf. Klein and Manning (2003)) by replacing
long infrequent rules with a set of binary rules.
Markovization was only applied if none of the
non-terminals on the right hand side of the rule
had a slash feature in order to avoid negative ef-
fects on the slash feature percolation mechanism.
The probabilities of the grammar rules were
directly estimated with relative frequencies. No
smoothing was applied, here. The lexical prob-
abilities, on the other hand, were smoothed with
</bodyText>
<footnote confidence="0.487673">
2LST annotates the list symbol in enumerations.
</footnote>
<page confidence="0.552121">
180
</page>
<figureCaption confidence="0.940234">
Figure 5: An Annotated Parse Tree
</figureCaption>
<figure confidence="0.999027151515151">
S/fin/.
VBZ/pst
’s
S_&lt;S&gt;
-NONE-_&lt;S&gt;
*EXP*_#&lt;S&gt;
NP/base/3s/expl
PRP/expl
It
IN/up
up
TO
to
PP/PP
NP/base
PRP
you
NP-SBJ
-NONE-
*
VP/to
VV/r
protect
NP/refl/base
PRP/refl
yourself
TO
to
VP/inf
NP-SBJ/3s/domV_&lt;S&gt;
VP/3s+&lt;S&gt;
S/to/gap+#&lt;S&gt;
PP/V
</figure>
<bodyText confidence="0.999450869565217">
the following technique which was adopted from
Klein and Manning (2003). Each word is assigned
to one of 216 word classes. The word classes
are defined with regular expressions. Examples
are the class [A-Za-z0-9-]+-oldwhich con-
tains the word 20-year-old, the class [a-z][a-
z]+ifies which contains clarifies, and a class
which contains a list of capitalized adjectives like
Advanced. The word classes are ordered. If a
string is matched by the regular expressions of
more than one word class, then it is assigned to the
first of these word classes. For each word class,
we compute part-of-speech probabilities with rel-
ative frequencies. The part-of-speech frequen-
cies of a word are smoothed by adding
the part-of-speech probability of the word
class according to equation 1 in order to ob-
tain the smoothed frequency . The part-of-
speech probability of the word class is weighted
by a parameter whose value was set to 4 after
testing on held-out data. The lexical probabilities
are finally estimated from the smoothed frequen-
cies according to equation 2.
</bodyText>
<sectionHeader confidence="0.997653" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9856316875">
In our experiments, we used the usual splitting of
the PENN treebank into training data (sections 2–
21), held-out data (section 22), and test data (sec-
tion 23).
The grammar extracted from the automatically
annotated version of the training corpus contained
52,297 rules with 3,453 different non-terminals.
Subtrees which dominated only empty categories
were collapsed into a single empty element sym-
bol. The parser skips over these symbols during
parsing, but adds them to the output parse. Over-
all, there were 308 different empty element sym-
bols in the grammar.
Parsing section 23 took 169 minutes on a Dual-
Opteron system with 2.2 GHz CPUs, which is
about 4.2 seconds per sentence.
</bodyText>
<table confidence="0.995561666666667">
precision recall f-score
this paper 86.9 86.3 86.6
Klein/Manning 86.3 85.1 85.7
</table>
<tableCaption confidence="0.9605725">
Table 1: Labeled bracketing accuracy on sec-
tion 23
</tableCaption>
<bodyText confidence="0.8741885">
Table 1 shows the labeled bracketing accuracy
of the parser on the whole section 23 and com-
pares it to the results reported in Klein and Man-
ning (2003) for sentences with up to 100 words.
</bodyText>
<subsectionHeader confidence="0.995584">
4.1 Empty Category Prediction
</subsectionHeader>
<bodyText confidence="0.99979552631579">
Table 2 reports the accuracy of the parser in the
empty category (EC) prediction task for ECs oc-
curring more than 6 times. Following Johnson
(2001), an empty category was considered cor-
rect if the treebank parse contained an empty node
of the same category at the same string position.
Empty SBAR nodes which dominate an empty S
node are treated as a single empty element and
listed as SBAR-S in table 2.
Frequent types of empty elements are recog-
nized quite reliably. Exceptions are the traces
of adverbial and prepositional phrases where the
recall was only 65% and 48%, respectively, and
empty relative pronouns of type WHNP and
WHADVP with f-scores around 60%. A couple of
empty relative pronouns of type WHADVP were
mis-analyzed as WHNP which explains why the
precision is higher than the recall for WHADVP,
but vice versa for WHNP.
</bodyText>
<page confidence="0.533023">
181
</page>
<table confidence="0.999920181818182">
prec. recall f-sc. freq.
NP * 87.0 85.9 86.5 1607
NP *T* 84.9 87.6 86.2 508
0 95.2 89.7 92.3 416
*U* 95.3 93.8 94.5 388
ADVP *T* 80.3 64.7 71.7 170
S *T* 86.7 93.8 90.1 160
SBAR-S *T* 88.5 76.7 82.1 120
WHNP 0 57.6 63.6 60.4 107
WHADVP 0 75.0 50.0 60.0 36
PP *ICH* 11.1 3.4 5.3 29
PP *T* 73.7 48.3 58.3 29
SBAR *EXP* 28.6 12.5 17.4 16
VP *?* 33.3 40.0 36.4 15
S *ICH* 61.5 57.1 59.3 14
S *EXP* 66.7 71.4 69.0 14
SBAR *ICH* 60.0 25.0 35.3 12
NP *?* 50.0 9.1 15.4 11
ADJP *T* 100.0 77.8 87.5 9
SBAR-S *?* 66.7 25.0 36.4 8
VP *T* 100.0 37.5 54.5 8
overall 86.0 82.3 84.1 3716
</table>
<tableCaption confidence="0.995924">
Table 2: Accuracy of empty category prediction
</tableCaption>
<bodyText confidence="0.9985633">
on section 23. The first column shows the type of
the empty element and – except for empty comple-
mentizers and empty units – also the category. The
last column shows the frequency in the test data.
The accuracy of the pseudo attachment labels
*RNR*, *ICH*, *EXP*, and *PPA* was gener-
ally low with a precision of 41%, recall of 21%,
and f-score of 28%. Empty elements with a test
corpus frequency below 8 were almost never gen-
erated by the parser.
</bodyText>
<subsectionHeader confidence="0.999058">
4.2 Co-Indexation
</subsectionHeader>
<bodyText confidence="0.942060666666667">
Table 3 shows the accuracy of the parser on the
co-indexation task. A co-indexation of a trace and
a filler is represented by a 5-tuple consisting of
the category and the string position of the trace,
as well as the category, start and end position of
the filler. A co-indexation is judged correct if the
treebank parse contains the same 5-tuple.
For NP3 and S4 traces of type ‘*T*’, the co-
indexation results are quite good with 85% and
92% f-score, respectively. For ‘*T*’-traces of
3NP traces of type *T* result from wh-extraction in ques-
tions and relative clauses and from fronting.
4S traces of type *T* occur in sentences with quoted
speech like the sentence “That’s true!”, he said *T*.
other categories and for NP traces of type ‘*’,5 the
parser shows high precision, but moderate recall.
The recall of infrequent types of empty elements
is again low, as in the recognition task.
</bodyText>
<table confidence="0.997783266666667">
prec. rec. f-sc. freq.
NP * 81.1 72.1 76.4 1140
WH NP *T* 83.7 86.8 85.2 507
S *T* 92.0 91.0 91.5 277
WH ADVP *T* 78.6 63.2 70.1 163
PP *ICH* 14.3 3.4 5.6 29
WH PP *T* 68.8 50.0 57.9 22
SBAR *EXP* 25.0 12.5 16.7 16
S *ICH* 57.1 53.3 55.2 15
S *EXP* 66.7 71.4 69.0 14
SBAR *ICH* 60.0 25.0 35.3 12
VP *T* 33.3 12.5 18.2 8
ADVP *T* 60.0 42.9 50.0 7
PP *T* 100.0 28.6 44.4 7
overall 81.7 73.5 77.4 2264
</table>
<tableCaption confidence="0.998314">
Table 3: Co-indexation accuracy on section 23.
</tableCaption>
<bodyText confidence="0.997363615384616">
The first column shows the category and type of
the trace. If the filler category of the filler is dif-
ferent from the category of the trace, it is added in
front. The filler category is abbreviated to “WH”
if the rest is identical to the trace category. The
last column shows the frequency in the test data.
In order to get an impression how often EC pre-
diction errors resulted from misplacement rather
than omission, we computed EC prediction accu-
racies without comparing the EC positions. We
observed the largest f-score increase for ADVP
*T* and PP *T*, where attachment ambiguities
are likely, and for VP *?* which is infrequent.
</bodyText>
<subsectionHeader confidence="0.999153">
4.3 Feature Evaluation
</subsectionHeader>
<bodyText confidence="0.858957333333333">
We ran a series of evaluations on held-out data in
order to determine the impact of the different fea-
tures which we described in section 2 on the pars-
ing accuracy. In each run, we deleted one of the
features and measured how the accuracy changed
compared to the baseline system with all features.
The results are shown in table 4.
5The trace type ‘*’ combines two types of traces with
different linguistic properties, namely empty objects of pas-
sive constructions which are co-indexed with the subject, and
empty subjects of participial and infinitive clauses which are
co-indexed with an NP of the matrix clause.
</bodyText>
<table confidence="0.991902895833333">
182
Feature LB EC CI
slash feature 0.43 – –
VP features 2.93 6.38 5.46
PENN tags 2.34 4.54 6.75
IN feature 2.02 2.57 5.63
S features 0.49 3.08 4.13
V subcat feature 0.68 3.17 2.94
punctuation feat. 0.82 1.11 1.86
all PENN tags 0.84 0.69 2.03
domV feature 1.76 0.15 0.00
gap feature 0.04 1.20 1.32
DT feature 0.57 0.44 0.99
RC feature 0.00 1.11 1.10
colon feature 0.41 0.84 0.44
ADV parent 0.50 0.04 0.93
auxiliary feat. 0.40 0.29 0.77
SBAR parent 0.45 0.24 0.71
agreement feat. 0.05 0.52 1.15
ADVP subcat feat. 0.33 0.32 0.55
genitive feat. 0.39 0.29 0.44
NP subcat feat. 0.33 0.08 0.76
no-tmp 0.14 0.90 0.16
base NP feat. 0.47 -0.24 0.55
tag correction 0.13 0.37 0.44
irr. adverb feat. 0.04 0.56 0.39
PP parent 0.08 0.04 0.82
ADJP features 0.14 0.41 0.33
currency feat. 0.06 0.82 0.00
qp feature 0.13 0.14 0.50
PP tmp feature -0.24 0.65 0.60
WH feature 0.11 0.25 0.27
percent feat. 0.34 -0.10 0.10
NP-ADV parent f. 0.07 0.14 0.39
MNR feature 0.08 0.35 0.11
JJ feature 0.08 0.18 0.27
case feature 0.05 0.14 0.27
Expletive feat. -0.01 0.16 0.27
LGS feature 0.17 0.07 0.00
ADJ subcat 0.00 0.00 0.33
OC feature 0.00 0.00 0.22
JJ-tmp feat. 0.09 0.00 0.00
refl. pronoun 0.02 -0.03 0.16
EXT feature -0.04 0.09 0.16
MWL feature 0.05 0.00 0.00
complex conj. f. 0.07 -0.07 0.00
LST feature 0.12 -0.12 -0.11
NP-pp feature 0.13 -0.57 -0.39
</table>
<tableCaption confidence="0.962381">
Table 4: Differences between the baseline f-scores
</tableCaption>
<bodyText confidence="0.980099666666667">
for labeled bracketing, EC prediction, and co-
indexation (CI) and the f-scores without the spec-
ified feature.
</bodyText>
<sectionHeader confidence="0.997153" genericHeader="evaluation">
5 Comparison
</sectionHeader>
<bodyText confidence="0.998242857142857">
Table 7 compares the empty category prediction
results of our parser with those reported in John-
son (2001), Dienes and Dubey (2003b) and Camp-
bell (2004). In terms of recall and f-score, our
parser outperforms the other parsers. In terms of
precision, the tagger of Dienes and Dubey is the
best, but its recall is the lowest of all systems.
</bodyText>
<table confidence="0.9962648">
prec. recall f-score
this paper 86.0 82.3 84.1
Campbell 85.2 81.7 83.4
Dienes &amp; Dubey 86.5 72.9 79.1
Johnson 85 74 79
</table>
<tableCaption confidence="0.87796">
Table 5: Accuracy of empty category prediction
on section 23
</tableCaption>
<bodyText confidence="0.9759934">
The good performance of our parser on the
empty element recognition task is remarkable con-
sidering the fact that its performance on the la-
beled bracketing task is 3% lower than that of the
Charniak (2000) parser used by Campbell (2004).
</bodyText>
<table confidence="0.996895166666667">
prec. recall f-score
this paper 81.7 73.5 77.4
Campbell 78.3 75.1 76.7
Dienes &amp; Dubey (b) 81.5 68.7 74.6
Dienes &amp; Dubey (a) 80.5 66.0 72.6
Johnson 73 63 68
</table>
<tableCaption confidence="0.998631">
Table 6: Co-indexation accuracy on section 23
</tableCaption>
<bodyText confidence="0.994879052631579">
Table 6 compares our co-indexation results with
those reported in Johnson (2001), Dienes and
Dubey (2003b), Dienes and Dubey (2003a), and
Campbell (2004). Our parser achieves the highest
precision and f-score. Campbell (2004) reports a
higher recall, but lower precision.
Table 7 shows the trace prediction accuracies
of our parser, Johnson’s (2001) parser with parser
input and perfect input, and Campbell’s (2004)
parser with perfect input. The accuracy of John-
son’s parser is consistently lower than that of
the other parsers and it has particular difficulties
with ADVP traces, SBAR traces, and empty rela-
tive pronouns (WHNP 0). Campbell’s parser and
our parser cannot be directly compared, but when
we take the respective performance difference to
Johnson’s parser as evidence, we might conclude
that Campbell’s parser works particularly well on
NP *, *U*, and WHNP 0, whereas our system
</bodyText>
<page confidence="0.789172">
183
</page>
<table confidence="0.9988072">
paper J1 J2 C
NP * 83.2 82 91 97.5
NP *T* 86.2 81 91 96.2
0 92.3 88 96 98.5
*U* 94.5 92 95 98.6
ADVP *T* 71.7 56 66 79.9
S *T* 90.1 88 90 92.7
SBAR-S *T* 82.1 70 74 84.4
WHNP 0 60.4 47 77 92.4
WHADVP 0 60.0 – – 73.3
</table>
<tableCaption confidence="0.991118">
Table 7: Comparison of the empty category pre-
</tableCaption>
<bodyText confidence="0.9703795">
diction accuracies for different categories in this
paper (paper), in (Johnson, 2001) with parser input
(J1), in (Johnson, 2001) with perfect input (J2),
and in (Campbell, 2004) with perfect input.
is slightly better on empty complementizers (0),
ADVP traces, and SBAR traces.
</bodyText>
<sectionHeader confidence="0.998535" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.999998392857143">
We presented an unlexicalized PCFG parser which
applies a slash feature percolation mechanism to
generate parse trees with empty elements and co-
indexation of traces and fillers. The grammar
was extracted from a version of the PENN tree-
bank which was annotated with slash features and
a set of other features that were added in order
to improve the general parsing accuracy. The
parser computes true Viterbi parses unlike most
other parsers for treebank grammars which are not
guaranteed to produce the most likely parse tree
because they apply pruning strategies like beam
search.
We evaluated the parser using the standard
PENN treebank training and test data. The labeled
bracketing f-score of 86.6% is – to our knowl-
edge – the best f-score reported for unlexical-
ized PCFGs, exceeding that of Klein and Man-
ning (2003) by almost 1%. On the empty cate-
gory prediction task, our parser outperforms the
best previously reported system (Campbell, 2004)
by 0.7% reaching an f-score of 84.1%, although
the general parsing accuracy of our unlexicalized
parser is 3% lower than that of the parser used by
Campbell (2004). Our parser also ranks highest
in terms of the co-indexation accuracy with 77.4%
f-score, again outperforming the system of Camp-
bell (2004) by 0.7%.
</bodyText>
<sectionHeader confidence="0.996465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998351934782609">
Richard Campbell. 2004. Using linguistic principles
to recover empty categories. In Proceedings of the
42nd Annual Meeting of the ACL, pages 645–652,
Barcelona, Spain.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meet-
ing of the North American Chapter of the Associ-
ation for Computational Linguistics (ANLP-NAACL
2000), pages 132–139, Seattle, Washington.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the ACL, Madrid, Spain.
Péter Dienes and Amit Dubey. 2003a. Antecedent
recovery: Experiments with a trace tagger. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, Sapporo,
Japan.
Péter Dienes and Amit Dubey. 2003b. Deep syntac-
tic processing by combining shallow methods. In
Proceedings of the 41st Annual Meeting of the ACL,
pages 431–438, Sapporo, Japan.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613–632.
Mark Johnson. 2001. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 39th Annual Meet-
ing of the ACL, pages 136–143, Toulouse, France.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the ACL, pages 423–430,
Sapporo, Japan.
Roger Levy and Christopher D. Manning. 2004. Deep
dependencies from context-free statistical parsers:
Correcting the surface dependency approximation.
In Proceedings of the 42nd Annual Meeting of the
ACL, pages 327–334, Barcelona, Spain.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313–330, June.
Helmut Schmid. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In Proceedings of the 20th International Conference
on Computational Linguistics (COLING 2004), vol-
ume 1, pages 162–168, Geneva, Switzerland.
</reference>
<page confidence="0.939712">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.789143">
<title confidence="0.993131">Trace Prediction and Recovery With Unlexicalized PCFGs and Slash Features</title>
<author confidence="0.987456">Helmut Schmid</author>
<affiliation confidence="0.999168">IMS, University of Stuttgart</affiliation>
<email confidence="0.98823">schmid@ims.uni-stuttgart.de</email>
<abstract confidence="0.991119434782609">This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed. The parser is an unlexicalized PCFG parser which is guaranteed to return the most probable parse. The grammar is extracted from a version of the PENN treebank which was automatically annotated with features in the style of Klein and Manning (2003). The annotation includes GPSG-style slash features which link traces and fillers, and other features which improve the general parsing accuracy. In an evaluation on the PENN treebank (Marcus et al., 1993), the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore. Its results for the empty category prediction task and the trace-filler coindexation task exceed all previously reported results with 84.1% and 77.4% fscore, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard Campbell</author>
</authors>
<title>Using linguistic principles to recover empty categories.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>645--652</pages>
<location>Barcelona,</location>
<contexts>
<context position="2403" citStr="Campbell, 2004" startWordPosition="365" endWordPosition="366">s, 1997; Charniak, 2000, and others) which are trained on the PENN treebank generate parse trees without empty categories. In order to augment such parsers with empty category prediction, three rather different strategies have been proposed: (i) pre-processing of the input sentence with a tagger which inserts empty categories into the input string of the parser (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The parser treats the empty elements like normal input tokens. (ii) post-processing of the parse trees with a pattern matcher which adds empty categories after parsing (Johnson, 2001; Campbell, 2004; Levy and Manning, 2004) (iii) in-processing of the empty categories with a slash percolation mechanism (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The empty elements are here generated by the grammar. Good results have been obtained with all three approaches, but (Dienes and Dubey, 2003b) reported that in their experiments, the in-processing of the empty categories only worked with lexicalized parsing. They explain that their unlexIN of WHNP WDT which NP NP NNS things WHPP-1 S VP JJ unaware PP -NONE*T*-1 SBAR VBP are ADJP-PRD NP-SBJ PRP they 177 Proceedings of the 21st International </context>
<context position="22222" citStr="Campbell (2004)" startWordPosition="3849" endWordPosition="3851">0.27 LGS feature 0.17 0.07 0.00 ADJ subcat 0.00 0.00 0.33 OC feature 0.00 0.00 0.22 JJ-tmp feat. 0.09 0.00 0.00 refl. pronoun 0.02 -0.03 0.16 EXT feature -0.04 0.09 0.16 MWL feature 0.05 0.00 0.00 complex conj. f. 0.07 -0.07 0.00 LST feature 0.12 -0.12 -0.11 NP-pp feature 0.13 -0.57 -0.39 Table 4: Differences between the baseline f-scores for labeled bracketing, EC prediction, and coindexation (CI) and the f-scores without the specified feature. 5 Comparison Table 7 compares the empty category prediction results of our parser with those reported in Johnson (2001), Dienes and Dubey (2003b) and Campbell (2004). In terms of recall and f-score, our parser outperforms the other parsers. In terms of precision, the tagger of Dienes and Dubey is the best, but its recall is the lowest of all systems. prec. recall f-score this paper 86.0 82.3 84.1 Campbell 85.2 81.7 83.4 Dienes &amp; Dubey 86.5 72.9 79.1 Johnson 85 74 79 Table 5: Accuracy of empty category prediction on section 23 The good performance of our parser on the empty element recognition task is remarkable considering the fact that its performance on the labeled bracketing task is 3% lower than that of the Charniak (2000) parser used by Campbell (200</context>
<context position="24363" citStr="Campbell, 2004" startWordPosition="4225" endWordPosition="4226">ective performance difference to Johnson’s parser as evidence, we might conclude that Campbell’s parser works particularly well on NP *, *U*, and WHNP 0, whereas our system 183 paper J1 J2 C NP * 83.2 82 91 97.5 NP *T* 86.2 81 91 96.2 0 92.3 88 96 98.5 *U* 94.5 92 95 98.6 ADVP *T* 71.7 56 66 79.9 S *T* 90.1 88 90 92.7 SBAR-S *T* 82.1 70 74 84.4 WHNP 0 60.4 47 77 92.4 WHADVP 0 60.0 – – 73.3 Table 7: Comparison of the empty category prediction accuracies for different categories in this paper (paper), in (Johnson, 2001) with parser input (J1), in (Johnson, 2001) with perfect input (J2), and in (Campbell, 2004) with perfect input. is slightly better on empty complementizers (0), ADVP traces, and SBAR traces. 6 Summary We presented an unlexicalized PCFG parser which applies a slash feature percolation mechanism to generate parse trees with empty elements and coindexation of traces and fillers. The grammar was extracted from a version of the PENN treebank which was annotated with slash features and a set of other features that were added in order to improve the general parsing accuracy. The parser computes true Viterbi parses unlike most other parsers for treebank grammars which are not guaranteed to </context>
</contexts>
<marker>Campbell, 2004</marker>
<rawString>Richard Campbell. 2004. Using linguistic principles to recover empty categories. In Proceedings of the 42nd Annual Meeting of the ACL, pages 645–652, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (ANLP-NAACL</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="1812" citStr="Charniak, 2000" startWordPosition="273" endWordPosition="274">.g. whextraction), discontinuous constituents, and missing elements (PRO elements, empty complementizers and relative pronouns). Moved constituents are co-indexed with a trace which is located at the position where the moved constituent is to be interpreted. Figure 1 shows an example of constituent movement in a relative clause. Empty categories provide important information for the semantic interpretation, in particular Figure 1: Co-indexation of traces and fillers for determining the predicate-argument structure of a sentence. However, most broad-coverage statistical parsers (Collins, 1997; Charniak, 2000, and others) which are trained on the PENN treebank generate parse trees without empty categories. In order to augment such parsers with empty category prediction, three rather different strategies have been proposed: (i) pre-processing of the input sentence with a tagger which inserts empty categories into the input string of the parser (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The parser treats the empty elements like normal input tokens. (ii) post-processing of the parse trees with a pattern matcher which adds empty categories after parsing (Johnson, 2001; Campbell, 2004; Levy an</context>
<context position="22793" citStr="Charniak (2000)" startWordPosition="3952" endWordPosition="3953">Dienes and Dubey (2003b) and Campbell (2004). In terms of recall and f-score, our parser outperforms the other parsers. In terms of precision, the tagger of Dienes and Dubey is the best, but its recall is the lowest of all systems. prec. recall f-score this paper 86.0 82.3 84.1 Campbell 85.2 81.7 83.4 Dienes &amp; Dubey 86.5 72.9 79.1 Johnson 85 74 79 Table 5: Accuracy of empty category prediction on section 23 The good performance of our parser on the empty element recognition task is remarkable considering the fact that its performance on the labeled bracketing task is 3% lower than that of the Charniak (2000) parser used by Campbell (2004). prec. recall f-score this paper 81.7 73.5 77.4 Campbell 78.3 75.1 76.7 Dienes &amp; Dubey (b) 81.5 68.7 74.6 Dienes &amp; Dubey (a) 80.5 66.0 72.6 Johnson 73 63 68 Table 6: Co-indexation accuracy on section 23 Table 6 compares our co-indexation results with those reported in Johnson (2001), Dienes and Dubey (2003b), Dienes and Dubey (2003a), and Campbell (2004). Our parser achieves the highest precision and f-score. Campbell (2004) reports a higher recall, but lower precision. Table 7 shows the trace prediction accuracies of our parser, Johnson’s (2001) parser with par</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (ANLP-NAACL 2000), pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="1796" citStr="Collins, 1997" startWordPosition="271" endWordPosition="272">ent movement (e.g. whextraction), discontinuous constituents, and missing elements (PRO elements, empty complementizers and relative pronouns). Moved constituents are co-indexed with a trace which is located at the position where the moved constituent is to be interpreted. Figure 1 shows an example of constituent movement in a relative clause. Empty categories provide important information for the semantic interpretation, in particular Figure 1: Co-indexation of traces and fillers for determining the predicate-argument structure of a sentence. However, most broad-coverage statistical parsers (Collins, 1997; Charniak, 2000, and others) which are trained on the PENN treebank generate parse trees without empty categories. In order to augment such parsers with empty category prediction, three rather different strategies have been proposed: (i) pre-processing of the input sentence with a tagger which inserts empty categories into the input string of the parser (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The parser treats the empty elements like normal input tokens. (ii) post-processing of the parse trees with a pattern matcher which adds empty categories after parsing (Johnson, 2001; Campbel</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Péter Dienes</author>
<author>Amit Dubey</author>
</authors>
<title>Antecedent recovery: Experiments with a trace tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2176" citStr="Dienes and Dubey, 2003" startWordPosition="329" endWordPosition="332">important information for the semantic interpretation, in particular Figure 1: Co-indexation of traces and fillers for determining the predicate-argument structure of a sentence. However, most broad-coverage statistical parsers (Collins, 1997; Charniak, 2000, and others) which are trained on the PENN treebank generate parse trees without empty categories. In order to augment such parsers with empty category prediction, three rather different strategies have been proposed: (i) pre-processing of the input sentence with a tagger which inserts empty categories into the input string of the parser (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The parser treats the empty elements like normal input tokens. (ii) post-processing of the parse trees with a pattern matcher which adds empty categories after parsing (Johnson, 2001; Campbell, 2004; Levy and Manning, 2004) (iii) in-processing of the empty categories with a slash percolation mechanism (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The empty elements are here generated by the grammar. Good results have been obtained with all three approaches, but (Dienes and Dubey, 2003b) reported that in their experiments, the in-processing of the empty catego</context>
<context position="22200" citStr="Dienes and Dubey (2003" startWordPosition="3844" endWordPosition="3847">7 Expletive feat. -0.01 0.16 0.27 LGS feature 0.17 0.07 0.00 ADJ subcat 0.00 0.00 0.33 OC feature 0.00 0.00 0.22 JJ-tmp feat. 0.09 0.00 0.00 refl. pronoun 0.02 -0.03 0.16 EXT feature -0.04 0.09 0.16 MWL feature 0.05 0.00 0.00 complex conj. f. 0.07 -0.07 0.00 LST feature 0.12 -0.12 -0.11 NP-pp feature 0.13 -0.57 -0.39 Table 4: Differences between the baseline f-scores for labeled bracketing, EC prediction, and coindexation (CI) and the f-scores without the specified feature. 5 Comparison Table 7 compares the empty category prediction results of our parser with those reported in Johnson (2001), Dienes and Dubey (2003b) and Campbell (2004). In terms of recall and f-score, our parser outperforms the other parsers. In terms of precision, the tagger of Dienes and Dubey is the best, but its recall is the lowest of all systems. prec. recall f-score this paper 86.0 82.3 84.1 Campbell 85.2 81.7 83.4 Dienes &amp; Dubey 86.5 72.9 79.1 Johnson 85 74 79 Table 5: Accuracy of empty category prediction on section 23 The good performance of our parser on the empty element recognition task is remarkable considering the fact that its performance on the labeled bracketing task is 3% lower than that of the Charniak (2000) parser</context>
</contexts>
<marker>Dienes, Dubey, 2003</marker>
<rawString>Péter Dienes and Amit Dubey. 2003a. Antecedent recovery: Experiments with a trace tagger. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Péter Dienes</author>
<author>Amit Dubey</author>
</authors>
<title>Deep syntactic processing by combining shallow methods.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>431--438</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2176" citStr="Dienes and Dubey, 2003" startWordPosition="329" endWordPosition="332">important information for the semantic interpretation, in particular Figure 1: Co-indexation of traces and fillers for determining the predicate-argument structure of a sentence. However, most broad-coverage statistical parsers (Collins, 1997; Charniak, 2000, and others) which are trained on the PENN treebank generate parse trees without empty categories. In order to augment such parsers with empty category prediction, three rather different strategies have been proposed: (i) pre-processing of the input sentence with a tagger which inserts empty categories into the input string of the parser (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The parser treats the empty elements like normal input tokens. (ii) post-processing of the parse trees with a pattern matcher which adds empty categories after parsing (Johnson, 2001; Campbell, 2004; Levy and Manning, 2004) (iii) in-processing of the empty categories with a slash percolation mechanism (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The empty elements are here generated by the grammar. Good results have been obtained with all three approaches, but (Dienes and Dubey, 2003b) reported that in their experiments, the in-processing of the empty catego</context>
<context position="22200" citStr="Dienes and Dubey (2003" startWordPosition="3844" endWordPosition="3847">7 Expletive feat. -0.01 0.16 0.27 LGS feature 0.17 0.07 0.00 ADJ subcat 0.00 0.00 0.33 OC feature 0.00 0.00 0.22 JJ-tmp feat. 0.09 0.00 0.00 refl. pronoun 0.02 -0.03 0.16 EXT feature -0.04 0.09 0.16 MWL feature 0.05 0.00 0.00 complex conj. f. 0.07 -0.07 0.00 LST feature 0.12 -0.12 -0.11 NP-pp feature 0.13 -0.57 -0.39 Table 4: Differences between the baseline f-scores for labeled bracketing, EC prediction, and coindexation (CI) and the f-scores without the specified feature. 5 Comparison Table 7 compares the empty category prediction results of our parser with those reported in Johnson (2001), Dienes and Dubey (2003b) and Campbell (2004). In terms of recall and f-score, our parser outperforms the other parsers. In terms of precision, the tagger of Dienes and Dubey is the best, but its recall is the lowest of all systems. prec. recall f-score this paper 86.0 82.3 84.1 Campbell 85.2 81.7 83.4 Dienes &amp; Dubey 86.5 72.9 79.1 Johnson 85 74 79 Table 5: Accuracy of empty category prediction on section 23 The good performance of our parser on the empty element recognition task is remarkable considering the fact that its performance on the labeled bracketing task is 3% lower than that of the Charniak (2000) parser</context>
</contexts>
<marker>Dienes, Dubey, 2003</marker>
<rawString>Péter Dienes and Amit Dubey. 2003b. Deep syntactic processing by combining shallow methods. In Proceedings of the 41st Annual Meeting of the ACL, pages 431–438, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="5394" citStr="Johnson (1998)" startWordPosition="876" endWordPosition="877">prove the parsing accuracy of the PCFG, inspired by the work of Klein and Manning (2003). The most important ones of these will now be described in detail. Section 4.3 shows the impact of these features on labeled bracketing accuracy and empty category pre “S-1 – – kidneytrouble” features1 diction. NP VP/WHPP VBP are JJ PP/WHPP unaware -NONE-/WHPP *T*/WHPP NP NNS things IN of WHNP WDT which NP-SBJ PRP they ADJP-PRD/WHPP SBAR WHPP/WHPP S/WHPP d passive VPs. tween imperatives, finite clauses, an d several types of small clauses. PP, ADVP, RB an d NP-ADV were annotated with a parent feature (cf. Johnson (1998)). The parent features distinguish between verbal (VP) 178 ble that the filler dominates the trace. An example adjectival (ADJP, WHADJP), adverbial (ADVP, WHADVP), nominal (NP, WHNP, QP), prepositional (PP) and other parents. es. between be, do, have, an d full verbs. 3s (n3s) if they are headed by a verb with part -ofspeech VBZ (VBP). node of the category POS (possessive marker) are marked with a genitive flag. Base NPs NPs dominating a node of category NN, NNS, NNP, NNPS, DT, CD, JJ, JJR, JJS, PRP, RB, or EX are marked as base NPs. notation with co-reference indices from the representation w</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>A simple pattern-matching algorithm for recovering empty nodes and their antecedents.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL,</booktitle>
<pages>136--143</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="2387" citStr="Johnson, 2001" startWordPosition="363" endWordPosition="364">parsers (Collins, 1997; Charniak, 2000, and others) which are trained on the PENN treebank generate parse trees without empty categories. In order to augment such parsers with empty category prediction, three rather different strategies have been proposed: (i) pre-processing of the input sentence with a tagger which inserts empty categories into the input string of the parser (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The parser treats the empty elements like normal input tokens. (ii) post-processing of the parse trees with a pattern matcher which adds empty categories after parsing (Johnson, 2001; Campbell, 2004; Levy and Manning, 2004) (iii) in-processing of the empty categories with a slash percolation mechanism (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The empty elements are here generated by the grammar. Good results have been obtained with all three approaches, but (Dienes and Dubey, 2003b) reported that in their experiments, the in-processing of the empty categories only worked with lexicalized parsing. They explain that their unlexIN of WHNP WDT which NP NP NNS things WHPP-1 S VP JJ unaware PP -NONE*T*-1 SBAR VBP are ADJP-PRD NP-SBJ PRP they 177 Proceedings of the 21s</context>
<context position="16184" citStr="Johnson (2001)" startWordPosition="2748" endWordPosition="2749">rsing section 23 took 169 minutes on a DualOpteron system with 2.2 GHz CPUs, which is about 4.2 seconds per sentence. precision recall f-score this paper 86.9 86.3 86.6 Klein/Manning 86.3 85.1 85.7 Table 1: Labeled bracketing accuracy on section 23 Table 1 shows the labeled bracketing accuracy of the parser on the whole section 23 and compares it to the results reported in Klein and Manning (2003) for sentences with up to 100 words. 4.1 Empty Category Prediction Table 2 reports the accuracy of the parser in the empty category (EC) prediction task for ECs occurring more than 6 times. Following Johnson (2001), an empty category was considered correct if the treebank parse contained an empty node of the same category at the same string position. Empty SBAR nodes which dominate an empty S node are treated as a single empty element and listed as SBAR-S in table 2. Frequent types of empty elements are recognized quite reliably. Exceptions are the traces of adverbial and prepositional phrases where the recall was only 65% and 48%, respectively, and empty relative pronouns of type WHNP and WHADVP with f-scores around 60%. A couple of empty relative pronouns of type WHADVP were mis-analyzed as WHNP which</context>
<context position="22176" citStr="Johnson (2001)" startWordPosition="3841" endWordPosition="3843">re 0.05 0.14 0.27 Expletive feat. -0.01 0.16 0.27 LGS feature 0.17 0.07 0.00 ADJ subcat 0.00 0.00 0.33 OC feature 0.00 0.00 0.22 JJ-tmp feat. 0.09 0.00 0.00 refl. pronoun 0.02 -0.03 0.16 EXT feature -0.04 0.09 0.16 MWL feature 0.05 0.00 0.00 complex conj. f. 0.07 -0.07 0.00 LST feature 0.12 -0.12 -0.11 NP-pp feature 0.13 -0.57 -0.39 Table 4: Differences between the baseline f-scores for labeled bracketing, EC prediction, and coindexation (CI) and the f-scores without the specified feature. 5 Comparison Table 7 compares the empty category prediction results of our parser with those reported in Johnson (2001), Dienes and Dubey (2003b) and Campbell (2004). In terms of recall and f-score, our parser outperforms the other parsers. In terms of precision, the tagger of Dienes and Dubey is the best, but its recall is the lowest of all systems. prec. recall f-score this paper 86.0 82.3 84.1 Campbell 85.2 81.7 83.4 Dienes &amp; Dubey 86.5 72.9 79.1 Johnson 85 74 79 Table 5: Accuracy of empty category prediction on section 23 The good performance of our parser on the empty element recognition task is remarkable considering the fact that its performance on the labeled bracketing task is 3% lower than that of th</context>
<context position="24271" citStr="Johnson, 2001" startWordPosition="4210" endWordPosition="4211">0). Campbell’s parser and our parser cannot be directly compared, but when we take the respective performance difference to Johnson’s parser as evidence, we might conclude that Campbell’s parser works particularly well on NP *, *U*, and WHNP 0, whereas our system 183 paper J1 J2 C NP * 83.2 82 91 97.5 NP *T* 86.2 81 91 96.2 0 92.3 88 96 98.5 *U* 94.5 92 95 98.6 ADVP *T* 71.7 56 66 79.9 S *T* 90.1 88 90 92.7 SBAR-S *T* 82.1 70 74 84.4 WHNP 0 60.4 47 77 92.4 WHADVP 0 60.0 – – 73.3 Table 7: Comparison of the empty category prediction accuracies for different categories in this paper (paper), in (Johnson, 2001) with parser input (J1), in (Johnson, 2001) with perfect input (J2), and in (Campbell, 2004) with perfect input. is slightly better on empty complementizers (0), ADVP traces, and SBAR traces. 6 Summary We presented an unlexicalized PCFG parser which applies a slash feature percolation mechanism to generate parse trees with empty elements and coindexation of traces and fillers. The grammar was extracted from a version of the PENN treebank which was annotated with slash features and a set of other features that were added in order to improve the general parsing accuracy. The parser computes true</context>
</contexts>
<marker>Johnson, 2001</marker>
<rawString>Mark Johnson. 2001. A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In Proceedings of the 39th Annual Meeting of the ACL, pages 136–143, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3904" citStr="Klein and Manning, 2003" startWordPosition="606" endWordPosition="609">tituents with empty elements. The scores of these constituents were too low compared with the scores of constituents without empty elements. They speculated that “doing an exhaustive search might help” here. In this paper, we confirm this hypothesis and show that it is possible to accurately predict empty categories with unlexicalized PCFG parsing and slash features if the true Viterbi parse is computed. In our experiments, we used the BitPar parser (Schmid, 2004) and a PCFG which was extracted from a version of the PENN treebank that was automatically annotated with features in the style of (Klein and Manning, 2003). 2 Feature Annotation A context-free grammar which generates empty categories has to make sure that a filler exists for each trace and vice versa. A well-known technique which enforces this constraint is the GPSGstyle percolation of a slash feature: All constituents on the direct path from the trace to the filler are annotated with a special feature which represents the category of the filler as shown in figure 2. In order to restore the original treebank anFigure 2: Slash features: The filler node of category WHNP is linked to the trace node via percolation of a slash feature. The trace node</context>
<context position="12777" citStr="Klein and Manning (2003)" startWordPosition="2188" endWordPosition="2191"> this tag to the dominating PP. OC feature Verbs are marked with an object control feature if they have an NP argument which dominates an NP filler and an S argument which dominates an NP trace. An example is the sentence She asked him to come. Corrections The part-of-speech tags of the PENN treebank are not always correct. Some of the errors (like the tag NNS in VP-initial position) can be identified and corrected automatically in the training data. Correcting tags did not always improve parsing accuracy, so it was done selectively. The gap and domV features described above were also used by Klein and Manning (2003). All features were automatically added to the PENN treebank by means of an annotation program. Figure 5 shows an example of an annotated parse tree. 3 Parameter Smoothing We extracted the grammar from sections 2–21 of the annotated version of the PENN treebank. In order to increase the coverage of the grammar, we selectively applied markovization to the grammar (cf. Klein and Manning (2003)) by replacing long infrequent rules with a set of binary rules. Markovization was only applied if none of the non-terminals on the right hand side of the rule had a slash feature in order to avoid negative</context>
<context position="14011" citStr="Klein and Manning (2003)" startWordPosition="2385" endWordPosition="2388">n the slash feature percolation mechanism. The probabilities of the grammar rules were directly estimated with relative frequencies. No smoothing was applied, here. The lexical probabilities, on the other hand, were smoothed with 2LST annotates the list symbol in enumerations. 180 Figure 5: An Annotated Parse Tree S/fin/. VBZ/pst ’s S_&lt;S&gt; -NONE-_&lt;S&gt; *EXP*_#&lt;S&gt; NP/base/3s/expl PRP/expl It IN/up up TO to PP/PP NP/base PRP you NP-SBJ -NONE* VP/to VV/r protect NP/refl/base PRP/refl yourself TO to VP/inf NP-SBJ/3s/domV_&lt;S&gt; VP/3s+&lt;S&gt; S/to/gap+#&lt;S&gt; PP/V the following technique which was adopted from Klein and Manning (2003). Each word is assigned to one of 216 word classes. The word classes are defined with regular expressions. Examples are the class [A-Za-z0-9-]+-oldwhich contains the word 20-year-old, the class [a-z][az]+ifies which contains clarifies, and a class which contains a list of capitalized adjectives like Advanced. The word classes are ordered. If a string is matched by the regular expressions of more than one word class, then it is assigned to the first of these word classes. For each word class, we compute part-of-speech probabilities with relative frequencies. The part-of-speech frequencies of a </context>
<context position="15970" citStr="Klein and Manning (2003)" startWordPosition="2708" endWordPosition="2712"> categories were collapsed into a single empty element symbol. The parser skips over these symbols during parsing, but adds them to the output parse. Overall, there were 308 different empty element symbols in the grammar. Parsing section 23 took 169 minutes on a DualOpteron system with 2.2 GHz CPUs, which is about 4.2 seconds per sentence. precision recall f-score this paper 86.9 86.3 86.6 Klein/Manning 86.3 85.1 85.7 Table 1: Labeled bracketing accuracy on section 23 Table 1 shows the labeled bracketing accuracy of the parser on the whole section 23 and compares it to the results reported in Klein and Manning (2003) for sentences with up to 100 words. 4.1 Empty Category Prediction Table 2 reports the accuracy of the parser in the empty category (EC) prediction task for ECs occurring more than 6 times. Following Johnson (2001), an empty category was considered correct if the treebank parse contained an empty node of the same category at the same string position. Empty SBAR nodes which dominate an empty S node are treated as a single empty element and listed as SBAR-S in table 2. Frequent types of empty elements are recognized quite reliably. Exceptions are the traces of adverbial and prepositional phrases</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the ACL, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>Deep dependencies from context-free statistical parsers: Correcting the surface dependency approximation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>327--334</pages>
<location>Barcelona,</location>
<contexts>
<context position="2428" citStr="Levy and Manning, 2004" startWordPosition="367" endWordPosition="370">k, 2000, and others) which are trained on the PENN treebank generate parse trees without empty categories. In order to augment such parsers with empty category prediction, three rather different strategies have been proposed: (i) pre-processing of the input sentence with a tagger which inserts empty categories into the input string of the parser (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The parser treats the empty elements like normal input tokens. (ii) post-processing of the parse trees with a pattern matcher which adds empty categories after parsing (Johnson, 2001; Campbell, 2004; Levy and Manning, 2004) (iii) in-processing of the empty categories with a slash percolation mechanism (Dienes and Dubey, 2003b; Dienes and Dubey, 2003a). The empty elements are here generated by the grammar. Good results have been obtained with all three approaches, but (Dienes and Dubey, 2003b) reported that in their experiments, the in-processing of the empty categories only worked with lexicalized parsing. They explain that their unlexIN of WHNP WDT which NP NP NNS things WHPP-1 S VP JJ unaware PP -NONE*T*-1 SBAR VBP are ADJP-PRD NP-SBJ PRP they 177 Proceedings of the 21st International Conference on Computation</context>
</contexts>
<marker>Levy, Manning, 2004</marker>
<rawString>Roger Levy and Christopher D. Manning. 2004. Deep dependencies from context-free statistical parsers: Correcting the surface dependency approximation. In Proceedings of the 42nd Annual Meeting of the ACL, pages 327–334, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="728" citStr="Marcus et al., 1993" startWordPosition="110" endWordPosition="113">tuttgart schmid@ims.uni-stuttgart.de Abstract This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed. The parser is an unlexicalized PCFG parser which is guaranteed to return the most probable parse. The grammar is extracted from a version of the PENN treebank which was automatically annotated with features in the style of Klein and Manning (2003). The annotation includes GPSG-style slash features which link traces and fillers, and other features which improve the general parsing accuracy. In an evaluation on the PENN treebank (Marcus et al., 1993), the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore. Its results for the empty category prediction task and the trace-filler coindexation task exceed all previously reported results with 84.1% and 77.4% fscore, respectively. 1 Introduction Empty categories (also called null elements) are used in the annotation of the PENN treebank (Marcus et al., 1993) in order to represent syntactic phenomena like constituent movement (e.g. whextraction), discontinuous constituents, and missing elements (PRO elements, empty complementizers and relative pronouns). M</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<volume>1</volume>
<pages>162--168</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="3748" citStr="Schmid, 2004" startWordPosition="580" endWordPosition="581"> Computational Linguistics icalized PCFG parser produced poor results because the beam search strategy applied there eliminated many correct constituents with empty elements. The scores of these constituents were too low compared with the scores of constituents without empty elements. They speculated that “doing an exhaustive search might help” here. In this paper, we confirm this hypothesis and show that it is possible to accurately predict empty categories with unlexicalized PCFG parsing and slash features if the true Viterbi parse is computed. In our experiments, we used the BitPar parser (Schmid, 2004) and a PCFG which was extracted from a version of the PENN treebank that was automatically annotated with features in the style of (Klein and Manning, 2003). 2 Feature Annotation A context-free grammar which generates empty categories has to make sure that a filler exists for each trace and vice versa. A well-known technique which enforces this constraint is the GPSGstyle percolation of a slash feature: All constituents on the direct path from the trace to the filler are annotated with a special feature which represents the category of the filler as shown in figure 2. In order to restore the o</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), volume 1, pages 162–168, Geneva, Switzerland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>