<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.997683">
Incorporating Temporal and Semantic Information with Eye Gaze for
Automatic Word Acquisition in Multimodal Conversational Systems
</title>
<author confidence="0.999028">
Shaolin Qu Joyce Y. Chai
</author>
<affiliation confidence="0.99641">
Department of Computer Science and Engineering
Michigan State University
</affiliation>
<address confidence="0.983097">
East Lansing, MI 48824
</address>
<email confidence="0.999841">
{qushaoli,jchai}@cse.msu.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994556">
One major bottleneck in conversational sys-
tems is their incapability in interpreting un-
expected user language inputs such as out-of-
vocabulary words. To overcome this problem,
conversational systems must be able to learn
new words automatically during human ma-
chine conversation. Motivated by psycholin-
guistic findings on eye gaze and human lan-
guage processing, we are developing tech-
niques to incorporate human eye gaze for au-
tomatic word acquisition in multimodal con-
versational systems. This paper investigates
the use of temporal alignment between speech
and eye gaze and the use of domain knowl-
edge in word acquisition. Our experiment re-
sults indicate that eye gaze provides a poten-
tial channel for automatically acquiring new
words. The use of extra temporal and domain
knowledge can significantly improve acquisi-
tion performance.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981391304348">
Interpreting human language is a challenging prob-
lem in human machine conversational systems due
to the flexibility of human language behavior. When
the encountered vocabulary is outside of the sys-
tem’s knowledge, conversational systems tend to
fail. It is desirable that conversational systems can
learn new words automatically during human ma-
chine conversation. While automatic word acquisi-
tion in general is quite challenging, multimodal con-
versational systems offer an unique opportunity to
explore word acquisition. In a multimodal conversa-
tional system where users can talk and interact with
a graphical display, users’ eye gaze, which occurs
naturally with speech production, provides a poten-
tial channel for the system to learn new words auto-
matically during human machine conversation.
Psycholinguistic studies have shown that eye gaze
is tightly linked to human language processing. Eye
gaze is one of the reliable indicators of what a per-
son is “thinking about” (Henderson and Ferreira,
2004). The direction of eye gaze carries informa-
tion about the focus of the user’s attention (Just and
Carpenter, 1976). The perceived visual context in-
fluences spoken word recognition and mediates syn-
tactic processing of spoken sentences (Tanenhaus et
al., 1995). In addition, directly before speaking a
word, the eyes move to the mentioned object (Grif-
fin and Bock, 2000).
Motivated by these psycholinguistic findings, we
are investigating the use of eye gaze for automatic
word acquisition in multimodal conversation. Par-
ticulary, this paper investigates the use of tempo-
ral information about speech and eye gaze and do-
main semantic relatedness for automatic word ac-
quisition. The domain semantic and temporal in-
formation are incorporated in statistical translation
models for word acquisition. Our experiments show
that the use of domain semantic and temporal infor-
mation significantly improves word acquisition per-
formance.
In the following sections, we first describe the ba-
sic translation models for word acquisition. Then,
we describe the enhanced models that incorporate
temporal and semantic information about speech
and eye gaze for word acquisition. Finally, we
present the results of empirical evaluation.
</bodyText>
<page confidence="0.975974">
244
</page>
<note confidence="0.9319715">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 244–253,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<figure confidence="0.777857">
(a) Raw gaze points (b) Processed gaze fixations
</figure>
<figureCaption confidence="0.999225">
Figure 1: Domain scene with a user’s gaze fixations
</figureCaption>
<sectionHeader confidence="0.999769" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99996040625">
Word acquisition by grounding words to visual en-
tities has been studied in many language ground-
ing systems. For example, given speech paired with
video images of single objects, mutual information
between audio and visual signals was used to acquire
words by associating acoustic phone sequences with
the visual prototypes (e.g., color, size, shape) of ob-
jects (Roy and Pentland, 2002). Generative mod-
els were used to acquire words by associating words
with image regions given parallel data of pictures
and description text (Barnard et al., 2003). Differ-
ent from these works, in our work, the visual atten-
tion foci accompanying speech are indicated by eye
gaze. Eye gaze is an implicit and subconscious in-
put, which brings additional challenges in word ac-
quisition.
Eye gaze has been explored for word acquisition
in previous work. In (Yu and Ballard, 2004), given
speech paired with eye gaze information and video
images, a translation model was used to acquire
words by associating acoustic phone sequences with
visual representations of objects and actions. A re-
cent investigation on word acquisition from tran-
scribed speech and eye gaze in human machine con-
versation was reported in (Liu et al., 2007). In this
work, a translation model was developed to asso-
ciate words with visual objects on a graphical dis-
play. Different from these previous works, here
we investigate the incorporation of extra knowledge,
specifically speech-gaze temporal information and
domain knowledge, with eye gaze to facilitate word
acquisition.
</bodyText>
<sectionHeader confidence="0.995779" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.999956666666667">
We recruited users to interact with a simplified mul-
timodal conversational system to collect speech and
eye gaze data.
</bodyText>
<subsectionHeader confidence="0.994485">
3.1 Domain
</subsectionHeader>
<bodyText confidence="0.999968125">
We are working on a 3D room decoration domain.
Figure 1 shows the 3D room scene that was shown
to the user in the experiments. There are 28 3D
objects (bed, chairs, paintings, lamp, etc.) in the
room scene. During the human machine conversa-
tion, the system verbally asked the user a question
(e.g., “what do you dislike about the arrangement
of the room?”) or issued a request (e.g., “describe
the left wall”) about the room. The user provided
responses by speaking to the system.
During the experiments, users’ speech was
recorded through an open microphone and users’
eye gaze was captured by an Eye Link II eye tracker.
Eye gaze data consists of the screen coordinates of
each gaze point that was captured by the eye tracker
at a sampling rate of 250hz.
</bodyText>
<subsectionHeader confidence="0.999846">
3.2 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999489">
As for speech data, we collected 357 spoken utter-
ances from 7 users’ experiments. The vocabulary
size is 480, among which 227 words are nouns and
adjectives. We manually transcribed the collected
speech.
As for gaze data, the first step is to identify gaze
fixation from raw gaze points. As shown in Fig-
ure 1(a), the collected raw gaze points are very noisy.
They can not be used directly for identifying gaze
fixated entities in the scene. We processed the raw
</bodyText>
<page confidence="0.997245">
245
</page>
<bodyText confidence="0.999477636363636">
gaze data to eliminate invalid and saccadic gaze
points. Invalid gaze points occur when users look
off the screen. Saccadic gaze points occur during
ballistic eye movements between gaze fixations. Vi-
sion studies have shown that no visual processing
occurs in the human mind during saccades (i.e., sac-
cadic suppression) (Matin, 1974). Since eyes do not
stay still but rather make small, frequent jerky move-
ments, we average nearby gaze points to better iden-
tify gaze fixations. The processed eye gaze fixations
are shown in Figure 1(b).
</bodyText>
<equation confidence="0.77213375">
2572 2872 3170 3528 3736
[19] [22] [ ] [10] [10] [10] [fixated entity ID]
[11] [11] [11]
( [19] – bed_frame; [22] – door; [10] – bedroom; [11] – chandelier)
</equation>
<figureCaption confidence="0.992591">
Figure 2: Parallel speech and gaze streams
</figureCaption>
<bodyText confidence="0.994567916666667">
Figure 2 shows an excerpt of the collected speech
and gaze fixation in one experiment. In the speech
stream, each word starts at a particular timestamp.
In the gaze stream, each gaze fixation has a start-
ing timestamp ts and an ending timestamp te. Each
gaze fixation also has a list of fixated entities (3D ob-
jects). An entity e on the graphical display is fixated
by gaze fixation f if the area of e contains fixation
point of f.
Given the collected speech and gaze fixations, we
build parallel speech-gaze data set as follows. For
each spoken utterance and its accompanying gaze
fixations, we construct a pair of word sequence and
entity sequence (w, e). The word sequence w con-
sists of only nouns and adjectives in the utterance.
Each gaze fixation results in a fixated entity in the
entity sequence e. When multiple entities are fix-
ated by one gaze fixation due to the overlapping of
the entities, the forefront one is chosen. Also, we
merge the neighboring gaze fixations that contain
the same fixated entities. For the parallel speech and
gaze streams shown in Figure 2, the resulting word
sequence is w = [room chandelier] and the entity
sequence is e = [bed frame door chandelier].
</bodyText>
<sectionHeader confidence="0.9964765" genericHeader="method">
4 Translation Models for Automatic Word
Acquisition
</sectionHeader>
<bodyText confidence="0.9999743">
Since we are working on conversational systems
where users interact with a visual scene, we consider
the task of word acquisition as associating words
with visual entities in the domain. Given the par-
allel speech and gaze fixated entities {(w, e)}, we
formulate word acquisition as a translation problem
and use translation models to estimate word-entity
association probabilities p(w|e). The words with the
highest association probabilities are chosen as ac-
quired words for entity e.
</bodyText>
<subsectionHeader confidence="0.970315">
4.1 Base Model I
</subsectionHeader>
<bodyText confidence="0.999345">
Using the translation model I (Brown et al., 1993),
where each word is equally likely to be aligned with
each entity, we have
</bodyText>
<equation confidence="0.99960075">
1 m l
p(w|e) = (l + 1)
m H I:p(wj  |ei) (1)
j=1 i=0
</equation>
<bodyText confidence="0.9999216">
where l and m are the lengths of entity and word
sequences respectively. This is the model used in
(Liu et al., 2007) and (Yu and Ballard, 2004). We
refer to this model as Model-1 throughout the rest
of this paper.
</bodyText>
<subsectionHeader confidence="0.984093">
4.2 Base Model II
</subsectionHeader>
<bodyText confidence="0.999913333333333">
Using the translation model II (Brown et al., 1993),
where alignments are dependent on word/entity po-
sitions and word/entity sequence lengths, we have
</bodyText>
<equation confidence="0.985879">
p(aj = i|j, m, l)p(wj|ei) (2)
</equation>
<bodyText confidence="0.999857375">
where aj = i means that wj is aligned with ei.
When aj = 0, wj is not aligned with any entity (e0
represents a null entity). We refer to this model as
Model-2.
Compared to Model-1, Model-2 considers the or-
dering of words and entities in word acquisition.
EM algorithms are used to estimate the probabilities
p(w|e) in the translation models.
</bodyText>
<sectionHeader confidence="0.806018" genericHeader="method">
5 Using Speech-Gaze Temporal
Information for Word Acquisition
</sectionHeader>
<bodyText confidence="0.9986">
In Model-2, word-entity alignments are estimated
from co-occurring word and entity sequences in an
</bodyText>
<figure confidence="0.9120595">
This room has a chandelier
speech stream
f: gaze fixation
(ms)
1668 2096 2692 3252
gaze stream
ts te
(ms)
</figure>
<equation confidence="0.886925857142857">
l
m
H
j=1
p(w|e) =
I:
i=0
</equation>
<page confidence="0.990156">
246
</page>
<bodyText confidence="0.9833374">
unsupervised way. The estimated alignments are de-
pendent on where the words/entities appear in the
word/entity sequences, not on when those words and
gaze fixated entities actually occur. Motivated by the
finding that users move their eyes to the mentioned
object directly before speaking a word (Griffin and
Bock, 2000), we make the word-entity alignments
dependent on their temporal relation in a new model
(referred as Model-2t):
pt(aj = i|j, e, w)p(wj|ei) (3)
where pt(aj = i|j, e, w) is the temporal alignment
probability computed based on the temporal dis-
tance between entity ei and word wj.
We define the temporal distance between ei and
wj as
</bodyText>
<equation confidence="0.999634">
d(ei,wj) =
{ 0 ts(ei) ≤ ts(wj) ≤ te(ei)
te(ei) − ts(wj) ts(wj) &gt; te(ei)
ts(ei) − ts(wj) ts(wj) &lt; ts(ei)
</equation>
<bodyText confidence="0.999945384615385">
where ts(wj) is the starting timestamp (ms) of word
wj, ts(ei) and te(ei) are the starting and ending
timestamps (ms) of gaze fixation on entity ei.
The alignment of word wj and entity ei is de-
cided by their temporal distance d(ei,wj). Based
on the psycholinguistic finding that eye gaze hap-
pens before a spoken word, wj is not allowed to
be aligned with ei when wj happens earlier than ei
(i.e., d(ei, wj) &gt; 0). When wj happens no earlier
than ei (i.e., d(ei, wj) ≤ 0), the closer they are, the
more likely they are aligned. Specifically, the tem-
poral alignment probability of wj and ei in each co-
occurring instance (w, e) is computed as
</bodyText>
<equation confidence="0.9804064">
pt(aj = i|j, e, w) =
�
0 d(ei, wj) &gt; 0
exp[α-d(ei,wj)] e d &lt; 0
Ei exp[α-d(ei,wj)] ( z, w 7)
</equation>
<bodyText confidence="0.9923425">
where α is a constant for scaling d(ei, wj). In our
experiments, α is set to 0.005.
An EM algorithm is used to estimate probabilities
p(w|e) in Model-2t.
</bodyText>
<figure confidence="0.9584404">
140
120
100
80
60
40
20
0
−5000 −4000 −3000 −2000 −1000 0 1000
temporal distance of aligned word and entity (ms)
</figure>
<figureCaption confidence="0.9989785">
Figure 3: Histogram of truly aligned word and entity
pairs over temporal distance (bin width = 200ms)
</figureCaption>
<bodyText confidence="0.999967166666667">
For the purpose of evaluation, we manually anno-
tated the truly aligned word and entity pairs. Fig-
ure 3 shows the histogram of those truly aligned
word and entity pairs over the temporal distance of
aligned word and entity. We can observe in the fig-
ure that 1) almost no eye gaze happens after a spo-
ken word, and 2) the number of word-entity pairs
with closer temporal distance is generally larger than
the number of those with farther temporal distance.
This is consistent with our modeling of the tempo-
ral alignment probability of word and entity (Equa-
tion (5)).
</bodyText>
<sectionHeader confidence="0.791366" genericHeader="method">
6 Using Domain Semantic Relatedness for
</sectionHeader>
<subsectionHeader confidence="0.719817">
Word Acquisition
</subsectionHeader>
<bodyText confidence="0.999820375">
Speech-gaze temporal alignment and occurrence
statistics sometimes are not sufficient to associate
words to an entity correctly. For example, suppose
a user says “there is a lamp on the dresser” while
looking at a lamp object on a table object. Due
to their co-occurring with the lamp object, words
dresser and lamp are both likely to be associated
with the lamp object in the translation models. As
a result, word dresser is likely to be incorrectly ac-
quired for the lamp object. For the same reason, the
word lamp could be acquired incorrectly for the ta-
ble object. To solve this type of association prob-
lem, the semantic knowledge about the domain and
words can be helpful. For example, the knowledge
that the word lamp is more semantically related to
the object lamp can help the system avoid associat-
</bodyText>
<figure confidence="0.8964749">
l
m
H
j=1
p(w|e) =
�
i=0
(4)
(5)
alignment count
</figure>
<page confidence="0.981352">
247
</page>
<bodyText confidence="0.999958409090909">
ing the word dresser to the lamp object. Therefore,
we are interested in investigating the use of semantic
knowledge in word acquisition.
On one hand, each conversational system has a
domain model, which is the knowledge representa-
tion about its domain such as the types of objects
and their properties and relations. On the other hand,
there are available resources about domain indepen-
dent lexical knowledge (e.g., WordNet (Fellbaum,
1998)). The question is whether we can utilize the
domain model and external lexical knowledge re-
source to improve word acquisition. To address this
question, we link the domain concepts in the domain
model with WordNet concepts, and define semantic
relatedness of word and entity to help the system ac-
quire domain semantically compatible words.
In the following sections, we first describe our
domain modeling, then define the semantic related-
ness of word and entity based on domain modeling
and WordNet semantic lexicon, and finally describe
different ways of using the semantic relatedness of
word and entity to help word acquisition.
</bodyText>
<subsectionHeader confidence="0.990407">
6.1 Domain Modeling
</subsectionHeader>
<bodyText confidence="0.9999914375">
We model the 3D room decoration domain as shown
in Figure 4. The domain model contains all do-
main related semantic concepts. These concepts are
linked to the WordNet concepts (i.e., synsets in the
format of “word#part-of-speech#sense-id”). Each of
the entities in the domain has one or more properties
(e.g., semantic type, color, size) that are denoted by
domain concepts. For example, the entity dresser 1
has domain concepts SEM DRESSER and COLOR.
These domain concepts are linked to “dresser#n#4”
and “color#n#1” in WordNet.
Note that in the domain model, the domain con-
cepts are not specific to a certain entity, they are gen-
eral concepts for a certain type of entity. Multiple
entities of the same type have the same properties
and share the same set of domain concepts.
</bodyText>
<subsectionHeader confidence="0.998011">
6.2 Semantic Relatedness of Word and Entity
</subsectionHeader>
<bodyText confidence="0.9998025">
We compute the semantic relatedness of a word w
and an entity e based on the semantic similarity be-
tween w and the properties of e. Specifically, se-
mantic relatedness SR(e, w) is defined as
</bodyText>
<equation confidence="0.899564">
sim(s(ce),sj(w)) (6)
</equation>
<figureCaption confidence="0.958176">
Figure 4: Domain model with domain concepts linked to
WordNet synsets
</figureCaption>
<bodyText confidence="0.999986230769231">
where ce is the i-th property of entity e, s(ce) is the
synset of property ce as designed in domain model,
sj(w) is the j-th synset of word w as defined in
WordNet, and sim(·, ·) is the similarity score of two
synsets.
We computed the similarity score of two synsets
based on the path length between them. The similar-
ity score is inversely proportional to the number of
nodes along the shortest path between the synsets as
defined in WordNet. When the two synsets are the
same, they have the maximal similarity score of 1.
The WordNet-Similarity tool (Pedersen et al., 2004)
was used for the synset similarity computation.
</bodyText>
<subsectionHeader confidence="0.986828">
6.3 Word Acquisition with Word-Entity
Semantic Relatedness
</subsectionHeader>
<bodyText confidence="0.999841">
We can use the semantic relatedness of word and
entity to help the system acquire semantically com-
patible words for each entity, and therefore improve
word acquisition performance. The semantic relat-
edness can be applied for word acquisition in two
ways: post process learned word-entity association
probabilities by rescoring them with semantic relat-
edness, or directly affect the learning of word-entity
associations by constraining the alignment of word
and entity in the translation models.
</bodyText>
<subsectionHeader confidence="0.96871">
6.3.1 Rescoring with semantic relatedness
</subsectionHeader>
<bodyText confidence="0.960728333333333">
In the acquired word list for an entity ez, each
word wj has an association probability p(wj|ez) that
is learned from a translation model. We use the
</bodyText>
<figure confidence="0.982964947368421">
Domain
concepts:
WordNet
concepts:
Entities:
SEM_DRESSER
“dresser#n#4”
“picture#n#2” “size#n#1”
dresser_1
Domain Model
COLOR
“color#n#1”
SEM_BED
“bed#n#1”
bed_frame
COLOR
SIZE
SR(e, w) = max
zJ
</figure>
<page confidence="0.98505">
248
</page>
<bodyText confidence="0.998097333333333">
semantic relatedness SR(ei, wj) to redistribute the
probability mass for each wj. The new association
probability is given by:
</bodyText>
<equation confidence="0.9638065">
p�(wj|ei) = p(wj|ei)SR(ei, wj) (7)
Ej p(wj  |ei)SR(ei, wj)
</equation>
<subsectionHeader confidence="0.9540235">
6.3.2 Semantic alignment constraint in
translation model
</subsectionHeader>
<bodyText confidence="0.9993572">
When used to constrain the word-entity alignment
in the translation model, semantic relatedness can be
used alone or used together with speech-gaze tempo-
ral information to decide the alignment probability
of word and entity.
</bodyText>
<listItem confidence="0.901396">
• Using only semantic relatedness to constrain
word-entity alignments in Model-2s, we have
</listItem>
<equation confidence="0.992597">
ps(aj = i|j, e, w)p(wj|ei)
(8)
</equation>
<bodyText confidence="0.9878565">
where ps(aj = i|j, e, w) is the alignment prob-
ability based on semantic relatedness,
</bodyText>
<equation confidence="0.9702395">
SR(ei, wj)
ps(aj = i|j, e, w) = Ei SR(ei, wj) (9)
</equation>
<bodyText confidence="0.507470666666667">
• Using semantic relatedness and temporal infor-
mation to constrain word-entity alignments in
Model-2ts, we have
</bodyText>
<sectionHeader confidence="0.930456" genericHeader="method">
7 Grounding Words to Domain Concepts
</sectionHeader>
<bodyText confidence="0.999922269230769">
As discussed above, based on translation models, we
can incorporate temporal and domain semantic in-
formation to obtain p(w|e). This probability only
provides a means to ground words to entities. In
conversational systems, the ultimate goal of word
acquisition is to make the system understand the se-
mantic meaning of new words. Word acquisition by
grounding words to objects is not always sufficient
for identifying their semantic meanings. Suppose
the word green is grounded to a green chair object,
so is the word chair. Although the system is aware
that green is some word describing the green chair,
it does not know that word green refers to the chair’s
color while the word chair refers to the chair’s se-
mantic type. Thus, after learning the word-entity as-
sociations p(w|e) by the translation models, we need
to further ground words to domain concepts of entity
properties.
We further apply WordNet to ground words to do-
main concepts. For each entity e, based on asso-
ciation probabilities p(w|e), we can choose the n-
best words as acquired words for e. Those n-best
words have the n highest association probabilities.
For each word w acquired for e, the grounded con-
cept ce for w is chosen as the one that has the highest
semantic relatedness with w:
</bodyText>
<equation confidence="0.9905692">
max sim(s(cie), sj(w)) (12)
j
l
m
H
j=1
p(w|e) =
�
i=0
ce = arg max
i
p(w|e) = m l pts(aj = i|j, e, w)p(wj|ei) where sim(s(cie),sj(w)) is the semantic similarity
H � score defined in Equation (6).
j=1 i=0
(10)
</equation>
<bodyText confidence="0.999814666666667">
where pts(aj = i|j, e, w) is the alignment
probability that is decided by both temporal re-
lation and semantic relatedness of ei and wj,
</bodyText>
<equation confidence="0.982307333333333">
pts(aj = i|j, e, w) =
ps(aj = i|j,e,w)pt(aj = i|j,e,w) (11)
Ei ps(aj = i|j, e, w)pt(aj = i|j, e, w)
</equation>
<bodyText confidence="0.962879333333333">
where ps(aj = i|j, e, w) is the semantic align-
ment probability in Equation (9), and pt(aj =
i|j, e, w) is the temporal alignment probability
given in Equation (5).
EM algorithms are used to estimate p(w|e) in
Model-2s and Model-2ts.
</bodyText>
<sectionHeader confidence="0.996935" genericHeader="method">
8 Evaluation
</sectionHeader>
<bodyText confidence="0.999958">
We evaluate word acquisition performance of differ-
ent models on the data collected from our user stud-
ies (see Section 3).
</bodyText>
<subsectionHeader confidence="0.987325">
8.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999245">
The following metrics are used to evaluate the words
acquired for domain concepts (i.e., entity properties)
Iciel.
</bodyText>
<listItem confidence="0.96901">
• Precision
</listItem>
<bodyText confidence="0.890148">
EEEi # words correctly acquired for ci
e e
Ei # words acquired for cie
e
</bodyText>
<page confidence="0.994881">
249
</page>
<listItem confidence="0.760509">
• Recall
</listItem>
<equation confidence="0.764871428571429">
EEEi # words correctly acquired for ci
e e
Ei # ground-truth1 words of cie
e
• F-measure
2 x precision x recall
precision + recall
</equation>
<bodyText confidence="0.999291333333333">
The metrics of precision, recall, and F-measure
are based on the n-best words acquired for the entity
properties. Therefore, we have different precision,
recall, and F-measure when n changes.
The metrics of precision, recall, and F-measure
only provide evaluation on the top n candidate
words. To measure the acquisition performance on
the entire ranked list of candidate words, we define
a new metric as follows:
</bodyText>
<listItem confidence="0.993298">
• Mean Reciprocal Rank Rate (MRRR)
</listItem>
<subsectionHeader confidence="0.996653">
8.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.997399">
To compare the effects of different speech-gaze
alignments on word acquisition, we evaluate the fol-
lowing models:
</bodyText>
<listItem confidence="0.9991227">
• Model-1 – base model I without word-entity
alignment (Equation (1)).
• Model-2 – base model II with positional align-
ment (Equation (2)).
• Model-2t – enhanced model with temporal
alignment (Equation (3)).
• Model-2s – enhanced model with semantic
alignment (Equation (8)).
• Model-2ts – enhanced model with both tempo-
ral and semantic alignment (Equation (10)).
</listItem>
<bodyText confidence="0.8634315">
To compare the different ways of incorporating
semantic relatedness in word acquisition as dis-
cussed in Section 6.3.1, we also evaluate the follow-
ing models:
</bodyText>
<table confidence="0.795486666666667">
Ee �Ne 1 • Model-1-r – Model-1 with semantic relatedness
MRRR = i=1 index(wie) rescoring of word-entity association.
ENe 1
i=1 i
#e • Model-2t-r – Model-2t with semantic related-
ness rescoring of word-entity association.
</table>
<bodyText confidence="0.999877470588235">
where Ne is the number of all ground-truth
words {wie} for entity e, index(wie) is the in-
dex of word wie in the ranked list of candidate
words for entity e.
Entities may have a different number of ground-
truth words. For each entity e, we calculate a Recip-
rocal Rank Rate (RRR), which measures how close
the ranks of the ground-truth words in the candidate
word list is to the best scenario where the top Ne
words are the ground-truth words for e. RRR is in
the range of (0, 1]. The higher the RRR, the better
is the word acquisition performance. The average of
RRRs across all entities gives the Mean Reciprocal
Rank Rate (MRRR).
Note that MRRR is directly based on the learned
word-entity associations p(w|e), it is in fact a mea-
sure of grounding words to entities.
</bodyText>
<footnote confidence="0.916329">
1The ground-truth words were compiled and agreed upon by
two human judges.
</footnote>
<bodyText confidence="0.997588857142857">
Figure 5 shows the results of models with differ-
ent speech-gaze alignments. Figure 6 shows the re-
sults of models with semantic relatedness rescoring.
In Figure 5 &amp; 6, n-best means the top n word candi-
dates are chosen as acquired words for each entity.
The Mean Reciprocal Rank Rates of all models are
compared in Figure 7.
</bodyText>
<subsectionHeader confidence="0.517094">
8.2.1 Results of using different speech-gaze
alignments
</subsectionHeader>
<bodyText confidence="0.9962507">
As shown in Figure 5, Model-2 does not show a
consistent improvement compared to Model-1 when
a different number of n-best words are chosen as ac-
quired words. This result shows that it is not very
helpful to consider the index-based positional align-
ment of word and entity for word acquisition.
Figure 5 also shows that models considering
temporal or/and semantic information (Model-2t,
Model-2s, Model-2ts) consistently perform better
than the models considering neither temporal nor
</bodyText>
<page confidence="0.979738">
250
</page>
<figure confidence="0.999807043478261">
precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
1 2 3 4 5 6 7 8 9 10
Model−1
Model−2
Model−2t
Model−2s
Model−2ts
recall
0.6
0.5
0.4
0.3
0.2
0.1
1 2 3 4 5 6 7 8 9 10
Model−1
Model−2
Model−2t
Model−2s
Model−2&amp;quot;
F−measure
0.55
0.45
0.35
0.25
0.5
0.4
0.3
0.2
1 2 3 4 5 6 7 8 9 10
Model−1
Model−2
Model−2t
Model−2s
Model−2ts
nest n−best nest
(a) precision (b) recall (c) F-measure
</figure>
<figureCaption confidence="0.998374">
Figure 5: Performance of word acquisition when different types of speech-gaze alignment are applied
</figureCaption>
<figure confidence="0.999822372093024">
recall
0.6
0.5
0.4
0.3
0.2
0.1
1 2 3 4 5 6 7 8 9 10
Model−1
Model−2t
Model−1−r
Model−2t−r
F−measure
0.55
0.45
0.35
0.25
0.5
0.4
0.3
0.2
1 2 3 4 5 6 7 8 9 10
Model−1
Model−2t
Model−1−r
Model−2t−r
precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
1 2 3 4 5 6 7 8 9 10
Model−1
Model−2t
Model−1−r
Model−2t−r
n−best r−best n−best
(a) precision (b) recall (c) F-measure
</figure>
<figureCaption confidence="0.998485">
Figure 6: Performance of word acquisition when semantic relatedness rescoring of word-entity association is applied
</figureCaption>
<figure confidence="0.770791">
Models
</figure>
<figureCaption confidence="0.999673">
Figure 7: MRRRs achieved by different models
</figureCaption>
<bodyText confidence="0.999501">
semantic information (Model-1, Model-2). Among
Model-2t, Model-2s, and Model-2ts, it is found that
they do not make consistent differences.
As shown in Figure 7, the MRRRs of different
models are consistent with their performances on F-
measure. A t-test has shown that the difference be-
tween the MRRRs of Model-1 and Model-2 is not
statistically significant. Compared to Model-1, t-
tests have confirmed that MRRR is significantly im-
proved by Model-2t (t = 2.27, p &lt; 0.02), Model-2s
(t = 3.40, p &lt; 0.01), and Model-2ts(t = 2.60, p &lt;
0.01). T-tests have shown no significant differences
among Model-2t, Model-2s, and Model-2ts.
</bodyText>
<sectionHeader confidence="0.4540685" genericHeader="method">
8.2.2 Results of applying semantic relatedness
rescoring
</sectionHeader>
<bodyText confidence="0.999317928571428">
Figure 6 shows that semantic relatedness rescor-
ing improves word acquisition. After semantic re-
latedness rescoring of the word-entity associations
learned by Model-1, Model-1-r improves the F-
measure consistently when a different number of
n-best words are chosen as acquired words. Com-
pared to Model-2t, Model-2t-r also improves the F-
measure consistently.
Comparing the two ways of using semantic relat-
edness for word acquisition, it is found that rescor-
ing word-entity association with semantic related-
ness works better. When semantic relatedness is
used together with temporal information to constrain
word-entity alignments in Model-2ts, word acqui-
</bodyText>
<figure confidence="0.979303">
0.8
0.75
Mean Reciprocal Rank Rate
0.7
0.65
0.6
0.55
0.5
</figure>
<table confidence="0.9471636">
M−1 M−2 M−2t M−2s M−2ts M−1−r M−2t−r
Model Rank 1 Rank 2 Rank 3 Rank 4 Rank 5
M-1 table(0.173) dresser(0.067) area(0.058) picture(0.053) dressing(0.041)
M-2t table(0.146) dresser(0.125) dressing(0.061) vanity(0.051) fact(0.050)
M-2t-r table(0.312) dresser(0.241) vanity(0.149) desk(0.047) area(0.026)
</table>
<tableCaption confidence="0.999518">
Table 1: N-best candidate words acquired for the entity dresser 1 by different models
</tableCaption>
<bodyText confidence="0.997327608695652">
sition performance is not improved compared to
Model-2t. However, using semantic relatedness to
rescore word-entity association learned by Model-
2t, Model-2t-r further improves word acquisition.
As shown in Figure 7, the MRRRs of Model-
1-r and Model-2t-r are consistent with their per-
formances on F-measure. Compared to Model-2t,
Model-2t-r improves MRRR. A t-test has confirmed
that this is a significant improvement (t = 1.97, p &lt;
0.03). Compared to Model-1, Model-1-r signifi-
cantly improves MRRR (t = 2.33, p &lt; 0.02). There
is no significant difference between Model-1-r and
Model-2t/Model-2s/Model-2ts.
In Figures 5&amp;6, we also notice that the recall
of the acquired words is still comparably low even
when 10 best word candidates are chosen for each
entity. This is mainly due to the scarcity of those
words that are not acquired in the data. Many of
the words that are not acquired appear less than 3
times in the data, which makes them unlikely to be
associated with any entity by the translation models.
When more data is available, we expect to see higher
recall.
</bodyText>
<subsectionHeader confidence="0.995597">
8.3 An Example
</subsectionHeader>
<bodyText confidence="0.981618294117647">
Table 1 shows the 5-best words acquired by different
models for the entity dresser 1 in the 3d room scene
(see Figure 1). In the table, each word is followed by
its word-entity association probability p(wIe). The
correctly acquired words are shown in bold font.
As shown in the example, the baseline Model-1
learned 2 correct words in the 5-best list. Consid-
ering speech-gaze temporal information, Model-2t
learned one more correct word vanity in the 5-best
list. With semantic relatedness rescoring, Model-
2t-r further acquired word desk in the 5-best list
because of the high semantic relatedness of word
desk and the type of entity dresser 1. Although nei-
ther Model-1 nor Model-2t successfully acquired the
word desk in the 5-best list, the rank (=7) of the word
desk in Model-2t’s n-best list is much higher than the
rank (=21) in Model-1’s n-best list.
</bodyText>
<sectionHeader confidence="0.997754" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99998752">
Motivated by the psycholinguistic findings, we in-
vestigate the use of eye gaze for automatic word ac-
quisition in multimodal conversational systems. Par-
ticularly, we investigate the use of speech-gaze tem-
poral information and word-entity semantic related-
ness to facilitate word acquisition. Our experiments
show that word acquisition is significantly improved
when temporal information is considered, which is
consistent with the previous psycholinguistic find-
ings about speech and eye gaze. Moreover, using
temporal information together with semantic relat-
edness rescoring further improves word acquisition.
Eye tracking systems are no longer bulky sys-
tems that prevent natural human machine commu-
nication. Display mounted gaze tracking systems
(e.g., Tobii) are completely non-intrusive, can toler-
ate head motion, and provide high tracking quality.
Integrating eye tracking with conversational inter-
faces is no longer beyond reach. Recent works have
shown that eye gaze can facilitate spoken language
processing in conversational systems (Qu and Chai,
2007; Prasov and Chai, 2008). Incorporating eye
gaze with automatic word acquisition provides an-
other potential approach to improve the robustness
of human machine conversation.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999892666666667">
This work was supported by IIS-0347548 and IIS-
0535112 from the National Science Foundation.
The authors would like to thank Zahar Prasov for his
contribution on data collection. The authors would
also like to thank anonymous reviewers for their
valuable comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9929505">
Kobus Barnard, Pinar Duygulu, Nando de Freitas, David
Forsyth, David Blei, and Michael I. Jordan. 2003.
</reference>
<page confidence="0.961355">
252
</page>
<reference confidence="0.999764294117647">
Matching words and pictures. Journal of Machine
Learning Research, 3:1107–1135.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Zenzi M. Griffin and Kathryn Bock. 2000. What the eyes
say about speaking. Psychological Science, 11:274–
279.
John M. Henderson and Fernanda Ferreira, editors. 2004.
The interface of language, vision, and action: Eye
movements and the visual world. New York: Taylor
&amp; Francis.
Marcel A. Just and Patricia A. Carpenter. 1976. Eye fix-
ations and cognitive processes. Cognitive Psychology,
8:441–480.
Yi Liu, Joyce Y. Chai, and Rong Jin. 2007. Au-
tomated vocabulary acquisition and interpretation in
multimodal conversational systems. In Proceedings of
the 45th Annual Meeting of the Association of Compu-
tational Linguistics (ACL).
E. Matin. 1974. Saccadic suppression: a review and an
analysis. Psychological Bulletin, 81:899–917.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity -measuring the relat-
edness of concepts. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence (AAAI-
04).
Zahar Prasov and Joyce Y. Chai. 2008. What’s in a
gaze? the role of eye-gaze in reference resolution in
multimodal conversational interfaces. In Proceedings
of ACM 12th International Conference on Intelligent
User interfaces (IUI).
Shaolin Qu and Joyce Y. Chai. 2007. An exploration
of eye gaze in spoken language processing for multi-
modal conversational interfaces. In Proceedings of the
Conference of the North America Chapter of the Asso-
ciation of Computational Linguistics (NAACL).
Deb K. Roy and Alex P. Pentland. 2002. Learning words
from sights and sounds, a computational model. Cog-
nitive Science, 26(1):113–146.
Michael K. Tanenhaus, Michael J. Spivey-Knowiton,
Kathleen M. Eberhard, and Julie C. Sedivy. 1995. In-
tegration of visual and linguistic information in spoken
language comprehension. Science, 268:1632–1634.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perceptions, 1(1):57–80.
</reference>
<page confidence="0.998943">
253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.698883">
<title confidence="0.999541">Incorporating Temporal and Semantic Information with Eye Gaze Automatic Word Acquisition in Multimodal Conversational Systems</title>
<author confidence="0.999897">Shaolin Qu Joyce Y Chai</author>
<affiliation confidence="0.933412">Department of Computer Science and Michigan State</affiliation>
<address confidence="0.75865">East Lansing, MI</address>
<abstract confidence="0.998520761904762">One major bottleneck in conversational systems is their incapability in interpreting unexpected user language inputs such as out-ofvocabulary words. To overcome this problem, conversational systems must be able to learn new words automatically during human machine conversation. Motivated by psycholinguistic findings on eye gaze and human language processing, we are developing techniques to incorporate human eye gaze for automatic word acquisition in multimodal conversational systems. This paper investigates the use of temporal alignment between speech and eye gaze and the use of domain knowledge in word acquisition. Our experiment results indicate that eye gaze provides a potential channel for automatically acquiring new words. The use of extra temporal and domain knowledge can significantly improve acquisition performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>Pinar Duygulu</author>
<author>Nando de Freitas</author>
<author>David Forsyth</author>
<author>David Blei</author>
<author>Michael I Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1107</pages>
<marker>Barnard, Duygulu, de Freitas, Forsyth, Blei, Jordan, 2003</marker>
<rawString>Kobus Barnard, Pinar Duygulu, Nando de Freitas, David Forsyth, David Blei, and Michael I. Jordan. 2003. Matching words and pictures. Journal of Machine Learning Research, 3:1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematic of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9104" citStr="Brown et al., 1993" startWordPosition="1455" endWordPosition="1458">r chandelier]. 4 Translation Models for Automatic Word Acquisition Since we are working on conversational systems where users interact with a visual scene, we consider the task of word acquisition as associating words with visual entities in the domain. Given the parallel speech and gaze fixated entities {(w, e)}, we formulate word acquisition as a translation problem and use translation models to estimate word-entity association probabilities p(w|e). The words with the highest association probabilities are chosen as acquired words for entity e. 4.1 Base Model I Using the translation model I (Brown et al., 1993), where each word is equally likely to be aligned with each entity, we have 1 m l p(w|e) = (l + 1) m H I:p(wj |ei) (1) j=1 i=0 where l and m are the lengths of entity and word sequences respectively. This is the model used in (Liu et al., 2007) and (Yu and Ballard, 2004). We refer to this model as Model-1 throughout the rest of this paper. 4.2 Base Model II Using the translation model II (Brown et al., 1993), where alignments are dependent on word/entity positions and word/entity sequence lengths, we have p(aj = i|j, m, l)p(wj|ei) (2) where aj = i means that wj is aligned with ei. When aj = 0,</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematic of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zenzi M Griffin</author>
<author>Kathryn Bock</author>
</authors>
<title>What the eyes say about speaking.</title>
<date>2000</date>
<journal>Psychological Science,</journal>
<volume>11</volume>
<pages>279</pages>
<contexts>
<context position="2511" citStr="Griffin and Bock, 2000" startWordPosition="373" endWordPosition="377">tomatically during human machine conversation. Psycholinguistic studies have shown that eye gaze is tightly linked to human language processing. Eye gaze is one of the reliable indicators of what a person is “thinking about” (Henderson and Ferreira, 2004). The direction of eye gaze carries information about the focus of the user’s attention (Just and Carpenter, 1976). The perceived visual context influences spoken word recognition and mediates syntactic processing of spoken sentences (Tanenhaus et al., 1995). In addition, directly before speaking a word, the eyes move to the mentioned object (Griffin and Bock, 2000). Motivated by these psycholinguistic findings, we are investigating the use of eye gaze for automatic word acquisition in multimodal conversation. Particulary, this paper investigates the use of temporal information about speech and eye gaze and domain semantic relatedness for automatic word acquisition. The domain semantic and temporal information are incorporated in statistical translation models for word acquisition. Our experiments show that the use of domain semantic and temporal information significantly improves word acquisition performance. In the following sections, we first describe</context>
<context position="10604" citStr="Griffin and Bock, 2000" startWordPosition="1721" endWordPosition="1724">dels. 5 Using Speech-Gaze Temporal Information for Word Acquisition In Model-2, word-entity alignments are estimated from co-occurring word and entity sequences in an This room has a chandelier speech stream f: gaze fixation (ms) 1668 2096 2692 3252 gaze stream ts te (ms) l m H j=1 p(w|e) = I: i=0 246 unsupervised way. The estimated alignments are dependent on where the words/entities appear in the word/entity sequences, not on when those words and gaze fixated entities actually occur. Motivated by the finding that users move their eyes to the mentioned object directly before speaking a word (Griffin and Bock, 2000), we make the word-entity alignments dependent on their temporal relation in a new model (referred as Model-2t): pt(aj = i|j, e, w)p(wj|ei) (3) where pt(aj = i|j, e, w) is the temporal alignment probability computed based on the temporal distance between entity ei and word wj. We define the temporal distance between ei and wj as d(ei,wj) = { 0 ts(ei) ≤ ts(wj) ≤ te(ei) te(ei) − ts(wj) ts(wj) &gt; te(ei) ts(ei) − ts(wj) ts(wj) &lt; ts(ei) where ts(wj) is the starting timestamp (ms) of word wj, ts(ei) and te(ei) are the starting and ending timestamps (ms) of gaze fixation on entity ei. The alignment of</context>
</contexts>
<marker>Griffin, Bock, 2000</marker>
<rawString>Zenzi M. Griffin and Kathryn Bock. 2000. What the eyes say about speaking. Psychological Science, 11:274– 279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Henderson</author>
<author>Fernanda Ferreira</author>
<author>editors</author>
</authors>
<title>The interface of language, vision, and action: Eye movements and the visual world.</title>
<date>2004</date>
<publisher>Taylor &amp; Francis.</publisher>
<location>New York:</location>
<marker>Henderson, Ferreira, editors, 2004</marker>
<rawString>John M. Henderson and Fernanda Ferreira, editors. 2004. The interface of language, vision, and action: Eye movements and the visual world. New York: Taylor &amp; Francis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel A Just</author>
<author>Patricia A Carpenter</author>
</authors>
<title>Eye fixations and cognitive processes. Cognitive Psychology,</title>
<date>1976</date>
<pages>8--441</pages>
<contexts>
<context position="2257" citStr="Just and Carpenter, 1976" startWordPosition="334" endWordPosition="337">to explore word acquisition. In a multimodal conversational system where users can talk and interact with a graphical display, users’ eye gaze, which occurs naturally with speech production, provides a potential channel for the system to learn new words automatically during human machine conversation. Psycholinguistic studies have shown that eye gaze is tightly linked to human language processing. Eye gaze is one of the reliable indicators of what a person is “thinking about” (Henderson and Ferreira, 2004). The direction of eye gaze carries information about the focus of the user’s attention (Just and Carpenter, 1976). The perceived visual context influences spoken word recognition and mediates syntactic processing of spoken sentences (Tanenhaus et al., 1995). In addition, directly before speaking a word, the eyes move to the mentioned object (Griffin and Bock, 2000). Motivated by these psycholinguistic findings, we are investigating the use of eye gaze for automatic word acquisition in multimodal conversation. Particulary, this paper investigates the use of temporal information about speech and eye gaze and domain semantic relatedness for automatic word acquisition. The domain semantic and temporal inform</context>
</contexts>
<marker>Just, Carpenter, 1976</marker>
<rawString>Marcel A. Just and Patricia A. Carpenter. 1976. Eye fixations and cognitive processes. Cognitive Psychology, 8:441–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Liu</author>
<author>Joyce Y Chai</author>
<author>Rong Jin</author>
</authors>
<title>Automated vocabulary acquisition and interpretation in multimodal conversational systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4858" citStr="Liu et al., 2007" startWordPosition="736" endWordPosition="739"> the visual attention foci accompanying speech are indicated by eye gaze. Eye gaze is an implicit and subconscious input, which brings additional challenges in word acquisition. Eye gaze has been explored for word acquisition in previous work. In (Yu and Ballard, 2004), given speech paired with eye gaze information and video images, a translation model was used to acquire words by associating acoustic phone sequences with visual representations of objects and actions. A recent investigation on word acquisition from transcribed speech and eye gaze in human machine conversation was reported in (Liu et al., 2007). In this work, a translation model was developed to associate words with visual objects on a graphical display. Different from these previous works, here we investigate the incorporation of extra knowledge, specifically speech-gaze temporal information and domain knowledge, with eye gaze to facilitate word acquisition. 3 Data Collection We recruited users to interact with a simplified multimodal conversational system to collect speech and eye gaze data. 3.1 Domain We are working on a 3D room decoration domain. Figure 1 shows the 3D room scene that was shown to the user in the experiments. The</context>
<context position="9348" citStr="Liu et al., 2007" startWordPosition="1507" endWordPosition="1510">domain. Given the parallel speech and gaze fixated entities {(w, e)}, we formulate word acquisition as a translation problem and use translation models to estimate word-entity association probabilities p(w|e). The words with the highest association probabilities are chosen as acquired words for entity e. 4.1 Base Model I Using the translation model I (Brown et al., 1993), where each word is equally likely to be aligned with each entity, we have 1 m l p(w|e) = (l + 1) m H I:p(wj |ei) (1) j=1 i=0 where l and m are the lengths of entity and word sequences respectively. This is the model used in (Liu et al., 2007) and (Yu and Ballard, 2004). We refer to this model as Model-1 throughout the rest of this paper. 4.2 Base Model II Using the translation model II (Brown et al., 1993), where alignments are dependent on word/entity positions and word/entity sequence lengths, we have p(aj = i|j, m, l)p(wj|ei) (2) where aj = i means that wj is aligned with ei. When aj = 0, wj is not aligned with any entity (e0 represents a null entity). We refer to this model as Model-2. Compared to Model-1, Model-2 considers the ordering of words and entities in word acquisition. EM algorithms are used to estimate the probabili</context>
</contexts>
<marker>Liu, Chai, Jin, 2007</marker>
<rawString>Yi Liu, Joyce Y. Chai, and Rong Jin. 2007. Automated vocabulary acquisition and interpretation in multimodal conversational systems. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matin</author>
</authors>
<title>Saccadic suppression: a review and an analysis.</title>
<date>1974</date>
<journal>Psychological Bulletin,</journal>
<pages>81--899</pages>
<contexts>
<context position="6906" citStr="Matin, 1974" startWordPosition="1080" endWordPosition="1081">collected speech. As for gaze data, the first step is to identify gaze fixation from raw gaze points. As shown in Figure 1(a), the collected raw gaze points are very noisy. They can not be used directly for identifying gaze fixated entities in the scene. We processed the raw 245 gaze data to eliminate invalid and saccadic gaze points. Invalid gaze points occur when users look off the screen. Saccadic gaze points occur during ballistic eye movements between gaze fixations. Vision studies have shown that no visual processing occurs in the human mind during saccades (i.e., saccadic suppression) (Matin, 1974). Since eyes do not stay still but rather make small, frequent jerky movements, we average nearby gaze points to better identify gaze fixations. The processed eye gaze fixations are shown in Figure 1(b). 2572 2872 3170 3528 3736 [19] [22] [ ] [10] [10] [10] [fixated entity ID] [11] [11] [11] ( [19] – bed_frame; [22] – door; [10] – bedroom; [11] – chandelier) Figure 2: Parallel speech and gaze streams Figure 2 shows an excerpt of the collected speech and gaze fixation in one experiment. In the speech stream, each word starts at a particular timestamp. In the gaze stream, each gaze fixation has </context>
</contexts>
<marker>Matin, 1974</marker>
<rawString>E. Matin. 1974. Saccadic suppression: a review and an analysis. Psychological Bulletin, 81:899–917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity -measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI04).</booktitle>
<contexts>
<context position="16388" citStr="Pedersen et al., 2004" startWordPosition="2729" endWordPosition="2732"> with domain concepts linked to WordNet synsets where ce is the i-th property of entity e, s(ce) is the synset of property ce as designed in domain model, sj(w) is the j-th synset of word w as defined in WordNet, and sim(·, ·) is the similarity score of two synsets. We computed the similarity score of two synsets based on the path length between them. The similarity score is inversely proportional to the number of nodes along the shortest path between the synsets as defined in WordNet. When the two synsets are the same, they have the maximal similarity score of 1. The WordNet-Similarity tool (Pedersen et al., 2004) was used for the synset similarity computation. 6.3 Word Acquisition with Word-Entity Semantic Relatedness We can use the semantic relatedness of word and entity to help the system acquire semantically compatible words for each entity, and therefore improve word acquisition performance. The semantic relatedness can be applied for word acquisition in two ways: post process learned word-entity association probabilities by rescoring them with semantic relatedness, or directly affect the learning of word-entity associations by constraining the alignment of word and entity in the translation model</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity -measuring the relatedness of concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zahar Prasov</author>
<author>Joyce Y Chai</author>
</authors>
<title>What’s in a gaze? the role of eye-gaze in reference resolution in multimodal conversational interfaces.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM 12th International Conference on Intelligent User interfaces (IUI).</booktitle>
<marker>Prasov, Chai, 2008</marker>
<rawString>Zahar Prasov and Joyce Y. Chai. 2008. What’s in a gaze? the role of eye-gaze in reference resolution in multimodal conversational interfaces. In Proceedings of ACM 12th International Conference on Intelligent User interfaces (IUI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaolin Qu</author>
<author>Joyce Y Chai</author>
</authors>
<title>An exploration of eye gaze in spoken language processing for multimodal conversational interfaces.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the North America Chapter of the Association of Computational Linguistics (NAACL).</booktitle>
<marker>Qu, Chai, 2007</marker>
<rawString>Shaolin Qu and Joyce Y. Chai. 2007. An exploration of eye gaze in spoken language processing for multimodal conversational interfaces. In Proceedings of the Conference of the North America Chapter of the Association of Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deb K Roy</author>
<author>Alex P Pentland</author>
</authors>
<title>Learning words from sights and sounds, a computational model.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="4037" citStr="Roy and Pentland, 2002" startWordPosition="601" endWordPosition="604">ds in Natural Language Processing, pages 244–253, Honolulu, October 2008.c�2008 Association for Computational Linguistics (a) Raw gaze points (b) Processed gaze fixations Figure 1: Domain scene with a user’s gaze fixations 2 Related Work Word acquisition by grounding words to visual entities has been studied in many language grounding systems. For example, given speech paired with video images of single objects, mutual information between audio and visual signals was used to acquire words by associating acoustic phone sequences with the visual prototypes (e.g., color, size, shape) of objects (Roy and Pentland, 2002). Generative models were used to acquire words by associating words with image regions given parallel data of pictures and description text (Barnard et al., 2003). Different from these works, in our work, the visual attention foci accompanying speech are indicated by eye gaze. Eye gaze is an implicit and subconscious input, which brings additional challenges in word acquisition. Eye gaze has been explored for word acquisition in previous work. In (Yu and Ballard, 2004), given speech paired with eye gaze information and video images, a translation model was used to acquire words by associating </context>
</contexts>
<marker>Roy, Pentland, 2002</marker>
<rawString>Deb K. Roy and Alex P. Pentland. 2002. Learning words from sights and sounds, a computational model. Cognitive Science, 26(1):113–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J Spivey-Knowiton</author>
<author>Kathleen M Eberhard</author>
<author>Julie C Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>268--1632</pages>
<contexts>
<context position="2401" citStr="Tanenhaus et al., 1995" startWordPosition="355" endWordPosition="358">ich occurs naturally with speech production, provides a potential channel for the system to learn new words automatically during human machine conversation. Psycholinguistic studies have shown that eye gaze is tightly linked to human language processing. Eye gaze is one of the reliable indicators of what a person is “thinking about” (Henderson and Ferreira, 2004). The direction of eye gaze carries information about the focus of the user’s attention (Just and Carpenter, 1976). The perceived visual context influences spoken word recognition and mediates syntactic processing of spoken sentences (Tanenhaus et al., 1995). In addition, directly before speaking a word, the eyes move to the mentioned object (Griffin and Bock, 2000). Motivated by these psycholinguistic findings, we are investigating the use of eye gaze for automatic word acquisition in multimodal conversation. Particulary, this paper investigates the use of temporal information about speech and eye gaze and domain semantic relatedness for automatic word acquisition. The domain semantic and temporal information are incorporated in statistical translation models for word acquisition. Our experiments show that the use of domain semantic and temporal</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowiton, Eberhard, Sedivy, 1995</marker>
<rawString>Michael K. Tanenhaus, Michael J. Spivey-Knowiton, Kathleen M. Eberhard, and Julie C. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Dana H Ballard</author>
</authors>
<title>A multimodal learning interface for grounding spoken language in sensory perceptions.</title>
<date>2004</date>
<journal>ACM Transactions on Applied Perceptions,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="4510" citStr="Yu and Ballard, 2004" startWordPosition="680" endWordPosition="683">d to acquire words by associating acoustic phone sequences with the visual prototypes (e.g., color, size, shape) of objects (Roy and Pentland, 2002). Generative models were used to acquire words by associating words with image regions given parallel data of pictures and description text (Barnard et al., 2003). Different from these works, in our work, the visual attention foci accompanying speech are indicated by eye gaze. Eye gaze is an implicit and subconscious input, which brings additional challenges in word acquisition. Eye gaze has been explored for word acquisition in previous work. In (Yu and Ballard, 2004), given speech paired with eye gaze information and video images, a translation model was used to acquire words by associating acoustic phone sequences with visual representations of objects and actions. A recent investigation on word acquisition from transcribed speech and eye gaze in human machine conversation was reported in (Liu et al., 2007). In this work, a translation model was developed to associate words with visual objects on a graphical display. Different from these previous works, here we investigate the incorporation of extra knowledge, specifically speech-gaze temporal informatio</context>
<context position="9375" citStr="Yu and Ballard, 2004" startWordPosition="1512" endWordPosition="1515">lel speech and gaze fixated entities {(w, e)}, we formulate word acquisition as a translation problem and use translation models to estimate word-entity association probabilities p(w|e). The words with the highest association probabilities are chosen as acquired words for entity e. 4.1 Base Model I Using the translation model I (Brown et al., 1993), where each word is equally likely to be aligned with each entity, we have 1 m l p(w|e) = (l + 1) m H I:p(wj |ei) (1) j=1 i=0 where l and m are the lengths of entity and word sequences respectively. This is the model used in (Liu et al., 2007) and (Yu and Ballard, 2004). We refer to this model as Model-1 throughout the rest of this paper. 4.2 Base Model II Using the translation model II (Brown et al., 1993), where alignments are dependent on word/entity positions and word/entity sequence lengths, we have p(aj = i|j, m, l)p(wj|ei) (2) where aj = i means that wj is aligned with ei. When aj = 0, wj is not aligned with any entity (e0 represents a null entity). We refer to this model as Model-2. Compared to Model-1, Model-2 considers the ordering of words and entities in word acquisition. EM algorithms are used to estimate the probabilities p(w|e) in the translat</context>
</contexts>
<marker>Yu, Ballard, 2004</marker>
<rawString>Chen Yu and Dana H. Ballard. 2004. A multimodal learning interface for grounding spoken language in sensory perceptions. ACM Transactions on Applied Perceptions, 1(1):57–80.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>