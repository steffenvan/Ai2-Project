<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000061">
<title confidence="0.992751">
Human Evaluation of Article and Noun Number Usage:
Influences of Context and Construction Variability
</title>
<author confidence="0.974235">
John Lee
</author>
<affiliation confidence="0.927599">
Spoken Language Systems
</affiliation>
<address confidence="0.799511">
MIT CSAIL
Cambridge, MA 02139, USA
</address>
<email confidence="0.998922">
jsylee@csail.mit.edu
</email>
<author confidence="0.668255">
Joel Tetreault
</author>
<affiliation confidence="0.494301">
Educational Testing Service
</affiliation>
<address confidence="0.504622">
Princeton, NJ 08541
</address>
<email confidence="0.990682">
jtetreault@ets.org
</email>
<author confidence="0.917003">
Martin Chodorow
</author>
<affiliation confidence="0.860078">
Hunter College of CUNY
</affiliation>
<address confidence="0.6182735">
New York, NY 10021
martin.chodorow@
</address>
<email confidence="0.791916">
hunter.cuny.edu
</email>
<sectionHeader confidence="0.986222" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998616">
Evaluating systems that correct errors in
non-native writing is difficult because of
the possibility of multiple correct answers
and the variability in human agreement.
This paper seeks to improve the best prac-
tice of such evaluation by analyzing the
frequency of multiple correct answers and
identifying factors that influence agree-
ment levels in judging the usage of articles
and noun number.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932161290322">
In recent years, systems have been developed with
the long-term goal of detecting, in the writing of
non-native speakers, usage errors involving arti-
cles, prepositions and noun number (Knight and
Chander, 1994; Minnen et al., 2000; Lee, 2004;
Han et al., 2005; Peng and Araki, 2005; Brockett
et al., 2006; Turner and Charniak, 2007). These
systems should, ideally, be evaluated on a cor-
pus of learners’ writing, annotated with accept-
able corrections. However, since such corpora are
expensive to compile, many researchers have in-
stead resorted to measuring the accuracy of pre-
dicting what a native writer originally wrote in
well-formed text. This type of evaluation effec-
tively makes the assumption that there is one cor-
rect form of native usage per context, which may
not always be the case.
Two studies have already challenged this “sin-
gle correct construction” assumption by compar-
ing the output of a system to the original text.
In (Tetreault and Chodorow, 2008), two human
judges were presented with 200 sentences and, for
each sentence, they were asked to select which
preposition (either the writer’s preposition, or the
system’s) better fits the context. In 28% of the
cases where the writer and the system differed, the
human raters found the system’s prediction to be
equal to or better than the writer’s original prepo-
sition. (Lee and Seneff, 2006) found similar re-
sults on the sentence level in a task that evaluated
many different parts of speech.
</bodyText>
<figure confidence="0.630811666666667">
Percentage Article Number Example
42.5% null singular stone
22.7% the singular the stone
17.6% null plural stones
11.4% a/an singular a stone
5.7% the plural the stones
</figure>
<tableCaption confidence="0.9089">
Table 1: Distribution of the five article-number
</tableCaption>
<bodyText confidence="0.86890575">
constructions of head nouns, based on 8 million
examples extracted from the MetaMetrics Lexile
Corpus. The various constructions are illustrated
with the noun “stone”.
</bodyText>
<sectionHeader confidence="0.983655" genericHeader="method">
2 Research Questions
</sectionHeader>
<bodyText confidence="0.981152652173913">
It is clear that using what the author wrote as
the gold standard can underestimate the sys-
tem’s performance, and that multiple correct an-
swers should be annotated. Using this annotation
scheme, however, raises two questions that have
not yet been thoroughly researched: (1) what is the
human agreement level on such annotation? (2)
what factors might influence the agreement level?
In this paper, we consider two factors: the context
of a word, and the variability of its usage.
In the two studies cited above, the human judges
were shown only the target sentence and did not
take into account any constraint on the choice of
word that might be imposed by the larger con-
text. For PP attachment, human performance im-
proves when given more context (Ratnaparkhi et
al., 1994). For other linguistic phenomena, such
as article/number selection for nouns, a larger con-
text window of at least several sentences may be
required, even though some automatic methods for
exploiting context have not been shown to boost
performance (Han et al., 2005).
60 The second factor, variability of usage, may be
</bodyText>
<note confidence="0.9404275">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 60–63,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.985907">
Three years ago John Small, a sheep farmer in the Mendip
Hills, read an editorial in his local newspaper which claimed
that foxes never killed lambs. He drove down to the pa-
per’s office and presented [?], killed the night before,
to the editor.
</bodyText>
<table confidence="0.504724666666667">
NO-CONTEXT IN-CONTEXT
lamb: no no
a lamb: yes yes*
the lamb: yes no
lambs: yes yes
the lambs: yes no
</table>
<tableCaption confidence="0.8223885">
Table 2: An example of a completed annotation
item.
</tableCaption>
<bodyText confidence="0.999982909090909">
expressed as the entropy of the distribution of the
word’s constructions. Table 1 shows the over-
all distribution of five article/number constructions
for head nouns, i.e. all permissible combinations
of number (singular or plural), and article (“a/an”,
“the”, or the “null article”). A high entropy noun
such as “stone” can appear freely in all of these, ei-
ther as a count noun or a non-count noun. This
contrasts with a low entropy noun such as “pollu-
tion” which is mostly limited to two construction
types (“pollution” and “the pollution”).
In this paper, we analyze the effects of varying
context and noun entropy on human judgments of
the acceptability of article-number constructions.
As a result of this study, we hope to advance the
best practice in annotation for evaluating error de-
tection systems. §3 describes our annotation task.
In §4, we test the “single correct construction” as-
sumption for article and noun number. In §5, we
investigate to what extent context and entropy con-
strain the range of acceptable constructions and in-
fluence the level of human agreement.
</bodyText>
<sectionHeader confidence="0.965002" genericHeader="method">
3 Annotation Design
</sectionHeader>
<subsectionHeader confidence="0.999789">
3.1 Annotation Scheme
</subsectionHeader>
<bodyText confidence="0.980916787878788">
Two native speakers of English participated in
an annotation exercise, which took place in two
stages: NO-CONTEXT and IN-CONTEXT. Both
stages used a common set of sentences, each con-
taining one noun to be annotated. That noun was
replaced by the symbol [?], and the five possible
constructions, as listed in Table 1, were displayed
below the sentence to be judged.
In the NO-CONTEXT stage, only the sentence
in question and the five candidate constructions
(i.e., the bolded parts in Table 2) were shown to
the raters. They were asked to consider each of
the five constructions, and to select yes if it would
null a the
anaphoric not anaphoric
singular 2 2 2 2
plural 2 n/a 2 2
Table 3: For each noun, two sentences were se-
lected from each configuration of number, article
and anaphor.
yield a good sentence in some context, and no oth-
erwise1.
The IN-CONTEXT stage began after a few days’
break. The raters were presented with the same
sentences, but including the context, which con-
sisted of the five preceding sentences, some of
which are shown in Table 2. The raters were again
asked to select yes if the choice would yield a
good sentence given the context, and no other-
wise. Among the yes constructions, they were
asked to mark with an asterisk (yes*) the con-
struction(s) most likely to have been used in the
original text.
</bodyText>
<subsectionHeader confidence="0.999692">
3.2 Annotation Example
</subsectionHeader>
<bodyText confidence="0.999931818181818">
In Table 2, “lambs” are mentioned in the context,
but only in the generic sense. Therefore, the [?]
in the sentence must be indefinite, resulting in yes
for both “a lamb” and “lambs”. Of these two con-
structions, the singular was judged more likely to
have been the writer’s choice.
If the context is removed, then the [?] in the
sentence could be anaphoric, and so “the lamb”
and “the lambs” are also possible. Finally, regard-
less of context, the null singular “lamb” is not ac-
ceptable.
</bodyText>
<subsectionHeader confidence="0.994002">
3.3 Item Selection
</subsectionHeader>
<bodyText confidence="0.958795142857143">
All items were drawn from the Grade 10 material
in the 2.5M-sentence MetaMetrics Lexile corpus.
To avoid artificially inflating the agreement level,
we excluded noun phrases whose article or num-
ber can be predicted with very high confidence,
such as proper nouns, pronouns and non-count
nouns. Noun phrases with certain words, such as
non-article determiners (e.g., this car), possessive
pronouns (e.g., his car), cardinal numbers (e.g.,
one car) or quantifiers (e.g., some cars), also fall
into this category. Most of these preclude the arti-
cles a and the.
1Originally, a third response category was offered to the
rater to mark constructions that fell in a grey area between
</bodyText>
<page confidence="0.988881">
6
</page>
<bodyText confidence="0.311007">
yes and no. This category was merged with yes.
</bodyText>
<table confidence="0.99934625">
Rater NO-CONTEXT IN-CONTEXT
yes no yes no
R1 62.4% 37.6% 29.3% 70.7%
R2 51.8% 48.2% 39.2% 60.8%
</table>
<tableCaption confidence="0.807682">
Table 4: Breakdown of the annotations by rater
and by stage. See §4 for a discussion.
</tableCaption>
<bodyText confidence="0.999655409090909">
Once these easy cases were filtered out, the head
nouns in the corpus were divided into five sets ac-
cording to their dominant construction. Each set
was then ranked according to the entropy of the
distribution of their constructions. Low entropy
typically means that there is one particular con-
struction whose frequency dwarfs the others’, such
as the singular definite for “sun”. High entropy
means that the five constructions are more evenly
represented in the corpus; these are mostly generic
objects that can be definite or indefinite, singular
or plural, such as “stone”. For each of the five
constructions, the three nouns with the highest en-
tropies, and three with the lowest, were selected.
This yielded a total of 15 “high-entropy” and 15
“low-entropy” nouns.
For each noun, 14 sentences were drawn ac-
cording to the breakdown in Table 3, ensuring a
balanced representation of the article and num-
ber used in the original text, and the presence of
anaphoric references2. A total of 368 items3 were
generated.
</bodyText>
<sectionHeader confidence="0.997772" genericHeader="method">
4 Multiple Correct Constructions
</sectionHeader>
<bodyText confidence="0.999975285714286">
We first establish the reliability of the annotation
by measuring agreement with the original text,
then show how and when multiple correct con-
structions can arise. All results in this section are
from the IN-CONTEXT stage.
Since the items were drawn from well-formed
text, each noun’s original construction should be
marked yes. The two raters assigned yes to the
original construction 80% and 95% of the time,
respectively. These can be viewed as the upper
bound of system performance if we assume there
can be only one correct construction. A stricter
ceiling can be obtained by considering how of-
ten the yes* constructions overlap with the orig-
</bodyText>
<footnote confidence="0.9898685">
2For practical reasons, we have restricted the study of con-
text to direct anaphoric references, i.e., where the same head
noun has already occurred in the context.
3In theory, there should be 420 items, but some of the con-
figurations in Table 3 are missing for certain nouns, mostly
the low-entropy ones.
</footnote>
<table confidence="0.976272">
NO-CONTEXT IN-CONTEXT
R1:↓ R2:→ yes no yes no
yes 846 302 462 77
no 108 584 260 1041
</table>
<tableCaption confidence="0.844952">
Table 5: The confusion tables of the two raters for
the two stages.
</tableCaption>
<bodyText confidence="0.998047631578947">
inal one4. The yes* items overlapped with the
original 72% and 83% of the time, respectively.
These relatively high figures serve as evidence of
the quality of the annotation.
Both raters frequently found more than one
valid construction — 18% of the time if only
considering yes*, and 49% if considering both
yes and yes*. The implication for auto-
matic system evaluation is that one could po-
tentially underestimate a system’s performance
by as much as 18%, if not more. For both
raters, the most frequent combinations of yes*
constructions were {null-plural,the-plural}, {a-
singular,the-singular}, {a-singular,null-plural},
and {the-singular,the-plural}. From the stand-
point of designing a grammar-checking system,
a system should be less confident in proposing
change from one construction to another within
the same construction pair.
</bodyText>
<sectionHeader confidence="0.629105" genericHeader="method">
5 Sources of Variation in Agreement
</sectionHeader>
<bodyText confidence="0.99972845">
It is unavoidable for agreement levels to be af-
fected by how accepting or imaginative the in-
dividual raters are. In the NO-CONTEXT stage,
Rater 1 awarded more yes’s than Rater 2, per-
haps attributable to her ability to imagine suitable
contexts for some of the less likely constructions.
In the IN-CONTEXT stage, Rater 1 used yes more
sparingly than Rater 2. This reflects their different
judgments on where to draw the line among con-
structions in the grey area between acceptable and
unacceptable.
We have identified, however, two other factors
that led to variations in the agreement level: the
amount of context available, and the distribution
of the noun itself in the English language. Careful
consideration of these factors should lead to better
agreement.
Availability of Context As shown in Table 4, for
both raters, the context sharply reduced the num-
ber of correct constructions. The confusion tables
</bodyText>
<footnote confidence="0.9847455">
4Both raters assigned yes* to an average of 1.2 construc-
tions per item.
</footnote>
<page confidence="0.997632">
62
</page>
<bodyText confidence="0.9999421875">
for the two raters are shown in Table 5. For the
NO-CONTEXT stage, they agreed 78% of the time
and the kappa statistic was 0.55. When context is
provided, human judgment can be expected to in-
crease. Indeed, for the IN-CONTEXT stage, agree-
ment rose to 82% and kappa to 0.605.
Another kind of context — previous mention
of the noun — also increases agreement. Among
nouns originally constructed with “the”, the kappa
statistics for those with direct anaphora was 0.63,
but only 0.52 for those without6.
Most previous research on article-number pre-
diction has only used features extracted from the
target sentence. These results suggest that using
features from a wider context should improve
performance.
Noun Construction Entropy For the low-entropy
nouns, we found a marked difference in human
agreement among the constructions depending on
their frequencies. For the most frequent construc-
tion in a noun’s distribution, the kappa was 0.78;
for the four remaining constructions, which are
much more rare, the kappa was only 0.527. They
probably constitute “border-line” cases for which
the line between yes and no was often hard to
draw, leading to the lower kappa.
Entropy can thus serve as an additional factor
when a system decides whether or not to mark a
usage as an error. For low-entropy nouns, the sys-
tem should be more confident of predicting a fre-
quent construction, but more wary of suggesting
the other constructions.
</bodyText>
<sectionHeader confidence="0.999202" genericHeader="conclusions">
6 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.9999775">
We conducted a human annotation exercise on ar-
ticle and noun number usage, making two obser-
vations that can help improve the evaluation pro-
cedure for this task. First, although the context
substantially reduces the range of acceptable an-
swers, there are still often multiple acceptable an-
swers given a context; second, the level of human
agreement is influenced by the availability of the
</bodyText>
<footnote confidence="0.664282">
5This kappa value is on the boundary between “moderate”
and “substantial” agreement on the scale proposed in (Landis
and Koch, 1977). The difference between the kappa values
for the NO-CONTEXT and IN-CONTEXT stages approaches
statistical significance, z = 1.71, p &lt; 0.10.
6The difference between these kappa values is statistically
significant, z = 2.06, p &lt; 0.05.
7The two kappa values are significantly different, z =
4.35, p &lt; 0.001.
</footnote>
<bodyText confidence="0.999748333333333">
context and the distribution of the noun’s construc-
tions.
These observations should help improve not
only the evaluation procedure but also the design
of error correction systems for articles and noun
number. Entropy, for example, can be incorpo-
rated into the estimation of a system’s confidence
in its prediction. More sophisticated contextual
features, beyond simply noting that a noun has
been previously mentioned (Han et al., 2005; Lee,
2004), can also potentially reduce uncertainty and
improve system performance.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998741">
We thank the two annotators, Sarah Ohls and Wa-
verely VanWinkle.
</bodyText>
<sectionHeader confidence="0.999396" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915666666667">
C. Brockett, W. Dolan, and M. Gamon. 2006. Cor-
recting ESL Errors using Phrasal SMT Techniques.
Proc. ACL.
N.-R. Han, M. Chodorow, and C. Leacock. 2005.
Detecting Errors in English Article Usage by Non-
Native Speakers. Natural Language Engineering,
1(1):1–15.
K. Knight and I. Chander. 1994. Automated Postedit-
ing of Documents. Proc. AAAI.
J. R. Landis and G. G. Koch. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics 33:159–174.
J. Lee. 2004. Automatic Article Restoration. Proc.
HLT-NAACL Student Research Workshop.
J. Lee and S. Seneff. 2006. Automatic Grammar Cor-
rection for Second-Language Learners. Proc. Inter-
speech.
G. Minnen, F. Bond, and A. Copestake. 2000.
Memory-based Learning for Article Generation.
Proc. CoNLL/LLL.
J. Peng and K. Araki. 2005. Correction of Arti-
cle Errors in Machine Translation Using Web-based
Model. Proc. IEEE NLP-KE.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
Maximum Entropy Model for Prepositional Phrase
Attachment. Proc. ARPA Workshop on Human Lan-
guage Technology.
J. Tetreault and M. Chodorow. 2008. Native Judg-
ments of Non-Native Usage. Proc. COLING Work-
shop on Human Judgements in Computational Lin-
guistics.
J. Turner and E. Charniak. 2007. Language Modeling
for Determiner Selection. Proc. HLT-NAACL.
</reference>
<page confidence="0.993465">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.165181">
<title confidence="0.967671">Human Evaluation of Article and Noun Number Influences of Context and Construction Variability</title>
<author confidence="0.917027">John</author>
<affiliation confidence="0.8521385">Spoken Language MIT</affiliation>
<address confidence="0.99943">Cambridge, MA 02139,</address>
<email confidence="0.999881">jsylee@csail.mit.edu</email>
<author confidence="0.96379">Joel</author>
<affiliation confidence="0.966418">Educational Testing</affiliation>
<address confidence="0.959253">Princeton, NJ</address>
<email confidence="0.930409">jtetreault@ets.org</email>
<degree confidence="0.7526735">Martin Hunter College of</degree>
<author confidence="0.432229">New York</author>
<author confidence="0.432229">NY</author>
<email confidence="0.997947">hunter.cuny.edu</email>
<abstract confidence="0.998709818181818">Evaluating systems that correct errors in non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Brockett</author>
<author>W Dolan</author>
<author>M Gamon</author>
</authors>
<date>2006</date>
<booktitle>Correcting ESL Errors using Phrasal SMT Techniques. Proc. ACL.</booktitle>
<contexts>
<context position="1087" citStr="Brockett et al., 2006" startWordPosition="157" endWordPosition="160">ibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged this “single correct construction” assumption by comparing the output of a</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>C. Brockett, W. Dolan, and M. Gamon. 2006. Correcting ESL Errors using Phrasal SMT Techniques. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N-R Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting Errors in English Article Usage by NonNative Speakers.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1042" citStr="Han et al., 2005" startWordPosition="149" endWordPosition="152">writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged this “single correct construc</context>
<context position="3691" citStr="Han et al., 2005" startWordPosition="582" endWordPosition="585"> the context of a word, and the variability of its usage. In the two studies cited above, the human judges were shown only the target sentence and did not take into account any constraint on the choice of word that might be imposed by the larger context. For PP attachment, human performance improves when given more context (Ratnaparkhi et al., 1994). For other linguistic phenomena, such as article/number selection for nouns, a larger context window of at least several sentences may be required, even though some automatic methods for exploiting context have not been shown to boost performance (Han et al., 2005). 60 The second factor, variability of usage, may be Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 60–63, Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Three years ago John Small, a sheep farmer in the Mendip Hills, read an editorial in his local newspaper which claimed that foxes never killed lambs. He drove down to the paper’s office and presented [?], killed the night before, to the editor. NO-CONTEXT IN-CONTEXT lamb: no no a lamb: yes yes* the lamb: yes no lambs: yes yes the lambs: yes no Table 2: An example of a completed annotation item. expre</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2005</marker>
<rawString>N.-R. Han, M. Chodorow, and C. Leacock. 2005. Detecting Errors in English Article Usage by NonNative Speakers. Natural Language Engineering, 1(1):1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automated Postediting of Documents.</title>
<date>1994</date>
<booktitle>Proc.</booktitle>
<publisher>AAAI.</publisher>
<contexts>
<context position="992" citStr="Knight and Chander, 1994" startWordPosition="139" endWordPosition="142">ract Evaluating systems that correct errors in non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies hav</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>K. Knight and I. Chander. 1994. Automated Postediting of Documents. Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data. Biometrics 33:159–174.</title>
<date>1977</date>
<contexts>
<context position="14166" citStr="Landis and Koch, 1977" startWordPosition="2328" endWordPosition="2331">quent construction, but more wary of suggesting the other constructions. 6 Conclusions &amp; Future Work We conducted a human annotation exercise on article and noun number usage, making two observations that can help improve the evaluation procedure for this task. First, although the context substantially reduces the range of acceptable answers, there are still often multiple acceptable answers given a context; second, the level of human agreement is influenced by the availability of the 5This kappa value is on the boundary between “moderate” and “substantial” agreement on the scale proposed in (Landis and Koch, 1977). The difference between the kappa values for the NO-CONTEXT and IN-CONTEXT stages approaches statistical significance, z = 1.71, p &lt; 0.10. 6The difference between these kappa values is statistically significant, z = 2.06, p &lt; 0.05. 7The two kappa values are significantly different, z = 4.35, p &lt; 0.001. context and the distribution of the noun’s constructions. These observations should help improve not only the evaluation procedure but also the design of error correction systems for articles and noun number. Entropy, for example, can be incorporated into the estimation of a system’s confidence</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics 33:159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
</authors>
<title>Automatic Article Restoration.</title>
<date>2004</date>
<booktitle>Proc. HLT-NAACL Student Research Workshop.</booktitle>
<contexts>
<context position="1024" citStr="Lee, 2004" startWordPosition="147" endWordPosition="148">non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged this “singl</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>J. Lee. 2004. Automatic Article Restoration. Proc. HLT-NAACL Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>Automatic Grammar Correction for Second-Language Learners.</title>
<date>2006</date>
<tech>Proc. Interspeech.</tech>
<contexts>
<context position="2143" citStr="Lee and Seneff, 2006" startWordPosition="330" endWordPosition="333">e usage per context, which may not always be the case. Two studies have already challenged this “single correct construction” assumption by comparing the output of a system to the original text. In (Tetreault and Chodorow, 2008), two human judges were presented with 200 sentences and, for each sentence, they were asked to select which preposition (either the writer’s preposition, or the system’s) better fits the context. In 28% of the cases where the writer and the system differed, the human raters found the system’s prediction to be equal to or better than the writer’s original preposition. (Lee and Seneff, 2006) found similar results on the sentence level in a task that evaluated many different parts of speech. Percentage Article Number Example 42.5% null singular stone 22.7% the singular the stone 17.6% null plural stones 11.4% a/an singular a stone 5.7% the plural the stones Table 1: Distribution of the five article-number constructions of head nouns, based on 8 million examples extracted from the MetaMetrics Lexile Corpus. The various constructions are illustrated with the noun “stone”. 2 Research Questions It is clear that using what the author wrote as the gold standard can underestimate the sys</context>
</contexts>
<marker>Lee, Seneff, 2006</marker>
<rawString>J. Lee and S. Seneff. 2006. Automatic Grammar Correction for Second-Language Learners. Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>F Bond</author>
<author>A Copestake</author>
</authors>
<title>Memory-based Learning for Article Generation.</title>
<date>2000</date>
<booktitle>Proc. CoNLL/LLL.</booktitle>
<contexts>
<context position="1013" citStr="Minnen et al., 2000" startWordPosition="143" endWordPosition="146">at correct errors in non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged </context>
</contexts>
<marker>Minnen, Bond, Copestake, 2000</marker>
<rawString>G. Minnen, F. Bond, and A. Copestake. 2000. Memory-based Learning for Article Generation. Proc. CoNLL/LLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peng</author>
<author>K Araki</author>
</authors>
<title>Correction of Article Errors in Machine Translation Using Web-based Model.</title>
<date>2005</date>
<booktitle>Proc. IEEE NLP-KE.</booktitle>
<contexts>
<context position="1064" citStr="Peng and Araki, 2005" startWordPosition="153" endWordPosition="156">lt because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged this “single correct construction” assumption by co</context>
</contexts>
<marker>Peng, Araki, 2005</marker>
<rawString>J. Peng and K. Araki. 2005. Correction of Article Errors in Machine Translation Using Web-based Model. Proc. IEEE NLP-KE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>J Reynar</author>
<author>S Roukos</author>
</authors>
<title>A Maximum Entropy Model for Prepositional Phrase Attachment.</title>
<date>1994</date>
<booktitle>Proc. ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="3425" citStr="Ratnaparkhi et al., 1994" startWordPosition="540" endWordPosition="543">ld be annotated. Using this annotation scheme, however, raises two questions that have not yet been thoroughly researched: (1) what is the human agreement level on such annotation? (2) what factors might influence the agreement level? In this paper, we consider two factors: the context of a word, and the variability of its usage. In the two studies cited above, the human judges were shown only the target sentence and did not take into account any constraint on the choice of word that might be imposed by the larger context. For PP attachment, human performance improves when given more context (Ratnaparkhi et al., 1994). For other linguistic phenomena, such as article/number selection for nouns, a larger context window of at least several sentences may be required, even though some automatic methods for exploiting context have not been shown to boost performance (Han et al., 2005). 60 The second factor, variability of usage, may be Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 60–63, Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Three years ago John Small, a sheep farmer in the Mendip Hills, read an editorial in his local newspaper which claimed that foxes never k</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A Maximum Entropy Model for Prepositional Phrase Attachment. Proc. ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>Native Judgments of Non-Native Usage.</title>
<date>2008</date>
<booktitle>Proc. COLING Workshop on Human Judgements in Computational Linguistics.</booktitle>
<contexts>
<context position="1750" citStr="Tetreault and Chodorow, 2008" startWordPosition="265" endWordPosition="268">e systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged this “single correct construction” assumption by comparing the output of a system to the original text. In (Tetreault and Chodorow, 2008), two human judges were presented with 200 sentences and, for each sentence, they were asked to select which preposition (either the writer’s preposition, or the system’s) better fits the context. In 28% of the cases where the writer and the system differed, the human raters found the system’s prediction to be equal to or better than the writer’s original preposition. (Lee and Seneff, 2006) found similar results on the sentence level in a task that evaluated many different parts of speech. Percentage Article Number Example 42.5% null singular stone 22.7% the singular the stone 17.6% null plura</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>J. Tetreault and M. Chodorow. 2008. Native Judgments of Non-Native Usage. Proc. COLING Workshop on Human Judgements in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turner</author>
<author>E Charniak</author>
</authors>
<title>Language Modeling for Determiner Selection.</title>
<date>2007</date>
<booktitle>Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="1115" citStr="Turner and Charniak, 2007" startWordPosition="161" endWordPosition="164">rect answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged this “single correct construction” assumption by comparing the output of a system to the original text</context>
</contexts>
<marker>Turner, Charniak, 2007</marker>
<rawString>J. Turner and E. Charniak. 2007. Language Modeling for Determiner Selection. Proc. HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>