<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011121">
<title confidence="0.996943">
Computing Confidence Scores for All Sub Parse Trees
</title>
<author confidence="0.999231">
Feng Lin Fuliang Weng
</author>
<affiliation confidence="0.9976195">
Department of Computer Science and Engineering Research and Technology Center
Fudan University Robert Bosch LLC
</affiliation>
<address confidence="0.745971">
Shanghai 200433, P.R. China Palo Alto, CA, 94303, USA
</address>
<email confidence="0.997242">
fenglin@fudan.edu.cn fuliang.weng@us.bosch.com
</email>
<sectionHeader confidence="0.995608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999488428571429">
Computing confidence scores for applica-
tions, such as dialogue system, informa-
tion retrieving and extraction, is an active
research area. However, its focus has been
primarily on computing word-, concept-,
or utterance-level confidences. Motivated
by the need from sophisticated dialogue
systems for more effective dialogs, we
generalize the confidence annotation to all
the subtrees, the first effort in this line of
research. The other contribution of this
work is that we incorporated novel long
distance features to address challenges in
computing multi-level confidence scores.
Using Conditional Maximum Entropy
(CME) classifier with all the selected fea-
tures, we reached an annotation error rate
of 26.0% in the SWBD corpus, compared
with a subtree error rate of 41.91%, a
closely related benchmark with the
Charniak parser from (Kahn et al., 2005).
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987296969697">
There has been a good amount of interest in ob-
taining confidence scores for improving word or
utterance accuracy, dialogue systems, information
retrieving &amp; extraction, and machine translation
(Zhang and Rudnicky, 2001; Guillevic et al., 2002;
Gabsdil et al., 2003; Ueffing et al., 2007).
However, these confidence scores are limited to
relatively simple systems, such as command-n-
control dialogue systems. For more sophisticated
dialogue systems (e.g., Weng et al., 2007), identi-
fication of reliable phrases must be performed at
different granularity to ensure effective and
friendly dialogues. For example, in a request of
MP3 music domain “Play a rock song by Cher”, if
we want to communicate to the user that the sys-
tem is not confident of the phrase “a rock song,”
the confidence scores for each word, the artist
name “Cher,” and the whole sentence would not be
enough. For tasks of information extraction, when
extracted content has internal structures, confi-
dence scores for such phrases are very useful for
reliable returns.
As a first attempt in this research, we generalize
confidence annotation algorithms to all sub parse
trees and tested on a human-human conversational
corpus, the SWBD. Technically, we also introduce
a set of long distance features to address the chal-
lenges in computing multi-level confidence scores.
This paper is organized as follows: Section 2 in-
troduces the tasks and the representation for parse
trees; Section 3 presents the features used in the
algorithm; Section 4 describes the experiments in
the SWBD corpus; Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.680244" genericHeader="method">
2 Computing Confidence Scores for
</sectionHeader>
<subsectionHeader confidence="0.817918">
Parse Trees
</subsectionHeader>
<bodyText confidence="0.999939">
The confidence of a sub-tree is defined as the pos-
terior probability of its correctness, given all the
available information. It is P(sp is correct |x) – the
posterior probability that the parse sub-tree sp is
correct, given related information x. In real appli-
cations, typically a threshold or cutoff t is needed:
</bodyText>
<equation confidence="0.862968333333333">
sp is correct, if P(sp is correct |x) ≥ t (1)
jincorrec, if P(sp is correct |x) &lt; t
⎧
</equation>
<page confidence="0.990379">
217
</page>
<reference confidence="0.2266705">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 217–220,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.978427">
In this work, the probability P(sp is correct |x) is
calculated using CME modeling framework:
</bodyText>
<equation confidence="0.998847">
⎛ ⎞
( ) ( )
1 ( )⎟ ⎟
exp ⎜
P y x
 |= ∑λ j fj x y
, (2)
Z x ⎜ ⎝ j ⎠
</equation>
<bodyText confidence="0.999978538461539">
where y∈{sp is correct, sp is incorrect}, x is the
syntactic context of the parse sub-tree sp, fj are the
features, λj are the corresponding weights, and Z(x)
is the normalization factor.
The parse trees used in our system are lexical-
ized binary trees. However, the confidence compu-
tation is independent of any parsing method used
in generating the parse tree as long as it generates
the binary dependency relations. An example of
the lexicalized binary trees is given in Figure 1,
where three important components are illustrated:
the left sub-tree, the right sub-trees, and the
marked head and dependency relation.
</bodyText>
<figureCaption confidence="0.993899">
Figure 1. Example of parse sub-tree’s structure for
phrase “three star Chinese restaurant”
</figureCaption>
<bodyText confidence="0.9997325">
Because the parse tree is already given, a bot-
tom-up left-right algorithm is used to traverse
through the parse tree: for each subtree, compute
its confidence, and annotate it as correct or wrong.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999524892307692">
Four major categories of features are used, includ-
ing, words, POS tags, scores and syntactic infor-
mation. Due to the space limitation, we only give a
detailed description of the most important one1,
lexical-syntactic features.
The lexical-syntactic features include lexical,
POS tag, and syntactic features. Word and POS tag
features include the head and modifier words of the
parse sub-tree and the two children of the root, as
well as their combinations. The POS tags and hier-
archical POS tags of the corresponding words are
1 The other important one is the dependency score, which is
the conditional probability of the last dependency relation in
the subtree, given its left and right child trees
also considered to avoid data sparseness. The
adopted hierarchical tags are: Verb-related (V),
Noun-related (N), Adjectives (ADJ), and Adverbs
(ADV), similar to (Zhang et al, 2006).
Long distance structural features in statistical
parsing lead to significant improvements (Collins
et al., 2000; Charniak et al., 2005). We incorporate
some of the reported features in the feature space
to be explored, and they are enriched with different
POS categories and grammatical types. Two eam-
ples are given below.
One example is the Single-Level Joint Head
and Dependency Relation (SL-JHD). This feature
is pairing the head word of a given sub-tree with its
last dependency relation. To address the data
sparseness problem, two additional SL-JHD fea-
tures are considered: a pair of the POS tag of the
head of a given sub-tree and its dependency rela-
tion, a pair of the hierarchical POS tag of the head
of a given sub-tree and its dependency relation. For
example, for the top node in Figure 2, (restaurant
NCOMP), (NN, NCOMP), and (N, NCOMP) are
the examples for the three SL-JHD features. To
compute the confidence score of the sub-tree, we
include the three JHD features for the top node,
and the JHD features for its two children. Thus, for
the sub-tree in Figure 2, the following nine JHD
features are included in the feature space, i.e., (res-
taurant NCOMP), (NN, NCOMP), (N, NCOMP),
(restaurant NMOD), (NN NMOD), (N NMOD),
(with POBJ), (IN POBJ), and (ADV POBJ).
The other example feature is Multi-Level Joint
Head and Dependency Relation (ML-JHD), which
takes into consideration the dependency relations
at multiple levels. This feature is an extension of
SL-JHD. Instead of including only single level
head and dependency relations, the ML-JHD fea-
ture includes the hierarchical POS tag of the head
and dependency relations for all the levels of a
given sub-tree. For example, given the sub-tree in
Figure 3, (NCOMP, N, NMOD, N, NMOD, N,
POBJ, ADV, NMOD, N) is the ML-JHD feature
for the top node (marked by the dashed circle).
In addition, three types of features are included:
dependency relations, neighbors of the head of the
current subtree, and the sizes of the sub-tree and its
left and right children. The dependency relations
include the top one in the subtree. The neighbors
are typically within a preset distance from the head
word. The sizes refer to the numbers of words or
non-terminals in the subtree and its children.
</bodyText>
<figure confidence="0.999566375">
Left Sub-tree
NP (restaurant)
NMOD
Right Sub-tree
NP (star)
NMOD
NP (restaurant)
NMOD
CD
NN
NNP
NN
three
star
Chinese
restaurant
</figure>
<page confidence="0.793487">
218
</page>
<figureCaption confidence="0.999647">
Figure 2. SL-JHD Features
Figure 3. ML-JHD Features
</figureCaption>
<sectionHeader confidence="0.99866" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.998622487804878">
Experiments were conducted to see the perform-
ance of our algorithm in human to human dialogs –
the ultimate goal of a dialogue system. In our work,
we use a version of the Charniak’s parser from
(Aug. 16, 2005) to parse the re-segmented SWBD
corpus (Kahn et al., 2005), and extract the parse
sub-trees from the parse trees as experimental data.
The parser’s training procedure is the same as
(Kahn et al., 2005). The only difference is that they
use golden edits in the parsing experiments while
we delete all the edits in the UW Switchboard cor-
pus. The F-score of the parsing result of the
Charniak parser without edits is 88.24%.
The Charniak parser without edits is used to
parse the training data, testing data and tuning data.
We remove the sentences with only one word and
delete the interjections in the hypothesis parse trees.
Finally, we extract parse sub-trees from these hy-
pothesis parse trees. Based on the gold parse trees,
a parse sub-tree is labeled with 1 (correct), if it has
all the words, their POS tags and syntactic struc-
tures correct. Otherwise, it is 0 (incorrect). Among
the 424,614 parse sub-trees from the training data,
316,182 sub-trees are labeled with 1; among the
38,774 parse sub-trees from testing data, 22,521
ones are labeled with 1; and among the 67,464
parse sub-trees from the tuning data, 38,619 ones
are labeled with 1. In the testing data, there are
5,590 sentences, and the percentage of complete
bracket match2 is 57.11%, and the percentage of
parse sub-trees with correct labels at the sentence
level is 48.57%. The percentage of correct parse
sub-trees is lower than that of the complete bracket
match due to its stricter requirements.
Table 1 shows our analysis of the testing data.
There, the first column indicates the phrase length
categories from the parse sub-trees. Among all the
parse trees in the test data, 82.84% (first two rows)
have a length equal to or shorter than 10 words.
We converted the original parse sub-trees from the
Charniak parser into binary trees.
</bodyText>
<table confidence="0.9994182">
Length Sub-tree Types Number Ratio
&lt;=10 Correct 21,593 55.70%
Incorrect 10,525 27.14%
&gt;10 Correct 928 2.39%
Incorrect 5,728 14.77%
</table>
<tableCaption confidence="0.999885">
Table 1. The analysis of testing data.
</tableCaption>
<bodyText confidence="0.950189">
We apply the model (2) from section 2 on the
above data for all the following experiments. The
performance is measured based on the confidence
annotation error rate (Zhang and Rudnicky, 2001).
Annot.Error = NumberOf SubtreesAnnotatedAsIncorrect
</bodyText>
<subsectionHeader confidence="0.576637">
Total Number Of Subtrees
</subsectionHeader>
<bodyText confidence="0.965338">
Two sets of experiments are designed to demon-
strate the improvements of our confidence comput-
ing algorithm, as well as the newly introduced
features (see Table 2 and Table 3).
Experiments were conducted to evaluate the ef-
fectiveness of each feature category for the sub-
tree level confidence annotation on SWBD corpus
(Table 2). The baseline system uses the conven-
tional features: words and POS tags. Additional
feature categories are included separately. The syn-
tactic feature category shows the biggest improve-
ment among all the categories.
To see the additive effect of the feature spaces
for the multi-level confidence annotation, another
set of experiments were performed (Table 3).
Three feature spaces are included incrementally:
dependency score, hierarchical tags and syntactic
features. Each category provides sizable reduction
in error rate. Totally, it reduces the error rate by
2 Complete bracket match is the percentage of sentences where
bracketing recall and precision are both 100%.
</bodyText>
<figure confidence="0.999078303030303">
NP (service)
DT
IN
NN
a NNP
with JJ
NN
CUISINENAME restaurant
good service
NP (restaurant)
NP (restaurant)
NCOMP Relation_Head
NP (restaurant)
PP (with)
NMOD POBJ
NMOD
NMOD
NP (restaurant)
NCOMP
NP (restaurant)
PP (with)
NMOD POBJ
DT
NMOD
NMOD
a NNP
CUISINENAME restaurant
good service
NN
NN
NP (restaurant)
IN NP (service)
with JJ
</figure>
<page confidence="0.994485">
219
</page>
<table confidence="0.974338">
Feature Space Description Annot. Error Relative Error Decrease
Baseline Base features: Words, POS tag 36.2% \
Set 1 Base features + Dependency score 32.8% 9.4%
Set 2 Base features + Hierarchical tags 35.3% 2.5%
Set 3 Base features + Syntactic features 29.3% 19.1%
</table>
<tableCaption confidence="0.999677">
Table 2. Comparison of different feature space (on SWBD corpus).
</tableCaption>
<table confidence="0.999900666666667">
Feature Space Description Annot. Error Relative Error Decrease
Baseline Base features: Words, POS tag 36.2% \
Set 4 + Dependency score 32.8% 9.4%
Set 5 + Dependency score + hierarchical tags 32.7% 9.7%
Set 6 + Dependency score + hierarchical tags 26.0% 28.2%
+ syntactic features
</table>
<tableCaption confidence="0.999704">
Table 3. Summary of experiment results with different feature space (on SWBD corpus).
</tableCaption>
<bodyText confidence="0.999679636363636">
10.2%, corresponding to 28.2% of a relative error
reduction over the baseline. The best result of an-
notation error rate is 26% for Switchboard data,
which is significantly lower than the 41.91% sub-
tree parsing error rate (see Table 1: 41.91% =
27.14%+14.77%). So, our algorithm would also
help the best parsing algorithms during rescoring
(Charniak et al., 2005; McClosky et al., 2006).
We list the performance of the parse sub-trees
with different lengths for Set 6 in Table 4, using
the F-score as the evaluation measure.
</bodyText>
<table confidence="0.9982286">
Length Sub-tree Category F-score
&lt;=10 Correct 82.3%
Incorrect 45.9%
&gt;10 Correct 33.1%
Incorrect 86.1%
</table>
<tableCaption confidence="0.994256">
Table 4. F-scores for various lengths in Set 15.
</tableCaption>
<bodyText confidence="0.9999547">
The F-score difference between the ones with
correct labels and the ones with incorrect labels are
significant. We suspect that it is caused by the dif-
ferent amount of training data. Therefore, we sim-
ply duplicated the training data for the sub-trees
with incorrect labels. For the sub-trees of length
equal to or less than 10 words, this training method
leads to a 79.8% F-score for correct labels, and a
61.4% F-score for incorrect labels, which is much
more balanced than those in the first set of results.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999892142857143">
In this paper, we generalized confidence annota-
tion algorithms to multiple-level parse trees and
demonstrated the significant benefits of using long
distance features in SWBD corpora. It is foresee-
able that multi-level confidence annotation can be
used for many other language applications such as
parsing, or information retrieval.
</bodyText>
<sectionHeader confidence="0.999081" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999586777777778">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. Proc.
ACL, pages 173–180.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. Proc. ICML, pages 175–182.
Malte Gabsdil and Johan Bos. 2003. Combining Acoustic Con-
fidence Scores with Deep Semantic Analysis for Clarifica-
tion Dialogues. Proc. IWCS, pages 137-150.
Didier Guillevic, et al. 2002. Robust semantic confidence
scoring. Proc. ICSLP, pages 853-856.
Jeremy G. Kahn, et al. 2005. Effective Use of Prosody in Pars-
ing Conversational Speech. Proc. EMNLP, pages 233-240.
David McClosky, Eugene Charniak and Mark Johnson. 2006.
Reranking and Self-Training for Parser Adaptation. Proc.
COLING-ACL, pages 337-344.
Nicola Ueffing and Hermann Ney. 2007. Word-Level Confi-
dence Estimation for Machine Translation. Computational
Linguistics, 33(1):9-40.
Fuliang Weng, et al., 2007. CHAT to Your Destination. Proc.
of the 8th SIGDial workshop on Discourse and Dialogue,
pages 79-86.
Qi Zhang, Fuliang Weng and Zhe Feng. 2006. A Pro-gressive
Feature Selection Algorithm for Ultra Large Feature
Spaces. Proc. COLING-ACL, pages 561-568.
Rong Zhang and Alexander I. Rudnicky. 2001. Word level
confidence annotation using combinations of features. Proc.
Eurospeech, pages 2105-2108.
</reference>
<page confidence="0.997588">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.755806">
<title confidence="0.999869">Computing Confidence Scores for All Sub Parse Trees</title>
<author confidence="0.999343">Feng Lin Fuliang Weng</author>
<affiliation confidence="0.9983995">Department of Computer Science and Engineering Research and Technology Center Fudan University Robert Bosch LLC</affiliation>
<address confidence="0.999805">Shanghai 200433, P.R. China Palo Alto, CA, 94303, USA</address>
<email confidence="0.998033">fenglin@fudan.edu.cnfuliang.weng@us.bosch.com</email>
<abstract confidence="0.990117619047619">Computing confidence scores for applications, such as dialogue system, information retrieving and extraction, is an active research area. However, its focus has been primarily on computing word-, concept-, or utterance-level confidences. Motivated by the need from sophisticated dialogue systems for more effective dialogs, we generalize the confidence annotation to all the subtrees, the first effort in this line of research. The other contribution of this work is that we incorporated novel long distance features to address challenges in computing multi-level confidence scores. Using Conditional Maximum Entropy (CME) classifier with all the selected features, we reached an annotation error rate of 26.0% in the SWBD corpus, compared with a subtree error rate of 41.91%, a closely related benchmark with the</abstract>
<note confidence="0.905587">Charniak parser from (Kahn et al., 2005).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>217--220</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 217–220,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>Proc. ACL,</booktitle>
<pages>173--180</pages>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine nbest parsing and MaxEnt discriminative reranking. Proc. ACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>Proc. ICML,</booktitle>
<pages>175--182</pages>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. Proc. ICML, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Gabsdil</author>
<author>Johan Bos</author>
</authors>
<title>Combining Acoustic Confidence Scores with Deep Semantic Analysis for Clarification Dialogues.</title>
<date>2003</date>
<booktitle>Proc. IWCS,</booktitle>
<pages>137--150</pages>
<marker>Gabsdil, Bos, 2003</marker>
<rawString>Malte Gabsdil and Johan Bos. 2003. Combining Acoustic Confidence Scores with Deep Semantic Analysis for Clarification Dialogues. Proc. IWCS, pages 137-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Didier Guillevic</author>
</authors>
<title>Robust semantic confidence scoring.</title>
<date>2002</date>
<booktitle>Proc. ICSLP,</booktitle>
<pages>853--856</pages>
<marker>Guillevic, 2002</marker>
<rawString>Didier Guillevic, et al. 2002. Robust semantic confidence scoring. Proc. ICSLP, pages 853-856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy G Kahn</author>
</authors>
<date>2005</date>
<booktitle>Effective Use of Prosody in Parsing Conversational Speech. Proc. EMNLP,</booktitle>
<pages>233--240</pages>
<marker>Kahn, 2005</marker>
<rawString>Jeremy G. Kahn, et al. 2005. Effective Use of Prosody in Parsing Conversational Speech. Proc. EMNLP, pages 233-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and Self-Training for Parser Adaptation.</title>
<date>2006</date>
<booktitle>Proc. COLING-ACL,</booktitle>
<pages>337--344</pages>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak and Mark Johnson. 2006. Reranking and Self-Training for Parser Adaptation. Proc. COLING-ACL, pages 337-344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<date>2007</date>
<booktitle>Word-Level Confidence Estimation for Machine Translation. Computational Linguistics,</booktitle>
<pages>33--1</pages>
<marker>Ueffing, Ney, 2007</marker>
<rawString>Nicola Ueffing and Hermann Ney. 2007. Word-Level Confidence Estimation for Machine Translation. Computational Linguistics, 33(1):9-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuliang Weng</author>
</authors>
<title>CHAT to Your Destination.</title>
<date>2007</date>
<booktitle>Proc. of the 8th SIGDial workshop on Discourse and Dialogue,</booktitle>
<pages>79--86</pages>
<marker>Weng, 2007</marker>
<rawString>Fuliang Weng, et al., 2007. CHAT to Your Destination. Proc. of the 8th SIGDial workshop on Discourse and Dialogue, pages 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
</authors>
<title>Fuliang Weng and Zhe Feng.</title>
<date>2006</date>
<booktitle>Proc. COLING-ACL,</booktitle>
<pages>561--568</pages>
<marker>Zhang, 2006</marker>
<rawString>Qi Zhang, Fuliang Weng and Zhe Feng. 2006. A Pro-gressive Feature Selection Algorithm for Ultra Large Feature Spaces. Proc. COLING-ACL, pages 561-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Zhang</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Word level confidence annotation using combinations of features.</title>
<date>2001</date>
<booktitle>Proc. Eurospeech,</booktitle>
<pages>2105--2108</pages>
<contexts>
<context position="1385" citStr="Zhang and Rudnicky, 2001" startWordPosition="198" endWordPosition="201">s that we incorporated novel long distance features to address challenges in computing multi-level confidence scores. Using Conditional Maximum Entropy (CME) classifier with all the selected features, we reached an annotation error rate of 26.0% in the SWBD corpus, compared with a subtree error rate of 41.91%, a closely related benchmark with the Charniak parser from (Kahn et al., 2005). 1 Introduction There has been a good amount of interest in obtaining confidence scores for improving word or utterance accuracy, dialogue systems, information retrieving &amp; extraction, and machine translation (Zhang and Rudnicky, 2001; Guillevic et al., 2002; Gabsdil et al., 2003; Ueffing et al., 2007). However, these confidence scores are limited to relatively simple systems, such as command-ncontrol dialogue systems. For more sophisticated dialogue systems (e.g., Weng et al., 2007), identification of reliable phrases must be performed at different granularity to ensure effective and friendly dialogues. For example, in a request of MP3 music domain “Play a rock song by Cher”, if we want to communicate to the user that the system is not confident of the phrase “a rock song,” the confidence scores for each word, the artist </context>
</contexts>
<marker>Zhang, Rudnicky, 2001</marker>
<rawString>Rong Zhang and Alexander I. Rudnicky. 2001. Word level confidence annotation using combinations of features. Proc. Eurospeech, pages 2105-2108.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>