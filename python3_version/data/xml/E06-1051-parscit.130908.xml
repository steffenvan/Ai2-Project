<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000735">
<title confidence="0.9981985">
Exploiting Shallow Linguistic Information for
Relation Extraction from Biomedical Literature
</title>
<author confidence="0.882885">
Claudio Giuliano and Alberto Lavelli and Lorenza Romano
</author>
<affiliation confidence="0.679354">
ITC-irst
</affiliation>
<author confidence="0.634301">
Via Sommarive, 18
38050, Povo (TN)
</author>
<affiliation confidence="0.760186">
Italy
</affiliation>
<email confidence="0.990388">
{giuliano,lavelli,romano}@itc.it
</email>
<sectionHeader confidence="0.995446" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879785714286">
We propose an approach for extracting re-
lations between entities from biomedical
literature based solely on shallow linguis-
tic information. We use a combination of
kernel functions to integrate two different
information sources: (i) the whole sen-
tence where the relation appears, and (ii)
the local contexts around the interacting
entities. We performed experiments on ex-
tracting gene and protein interactions from
two different data sets. The results show
that our approach outperforms most of the
previous methods based on syntactic and
semantic information.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999627350877193">
Information Extraction (IE) is the process of find-
ing relevant entities and their relationships within
textual documents. Applications of IE range from
Semantic Web to Bioinformatics. For example,
there is an increasing interest in automatically
extracting relevant information from biomedi-
cal literature. Recent evaluation campaigns on
bio-entity recognition, such as BioCreAtIvE and
JNLPBA 2004 shared task, have shown that sev-
eral systems are able to achieve good performance
(even if it is a bit worse than that reported on news
articles). However, relation identification is more
useful from an applicative perspective but it is still
a considerable challenge for automatic tools.
In this work, we propose a supervised machine
learning approach to relation extraction which is
applicable even when (deep) linguistic process-
ing is not available or reliable. In particular, we
explore a kernel-based approach based solely on
shallow linguistic processing, such as tokeniza-
tion, sentence splitting, Part-of-Speech (PoS) tag-
ging and lemmatization.
Kernel methods (Shawe-Taylor and Cristianini,
2004) show their full potential when an explicit
computation of the feature map becomes compu-
tationally infeasible, due to the high or even infi-
nite dimension of the feature space. For this rea-
son, kernels have been recently used to develop
innovative approaches to relation extraction based
on syntactic information, in which the examples
preserve their original representations (i.e. parse
trees) and are compared by the kernel function
(Zelenko et al., 2003; Culotta and Sorensen, 2004;
Zhao and Grishman, 2005).
Despite the positive results obtained exploiting
syntactic information, we claim that there is still
room for improvement relying exclusively on shal-
low linguistic information for two main reasons.
First of all, previous comparative evaluations put
more stress on the deep linguistic approaches and
did not put as much effort on developing effec-
tive methods based on shallow linguistic informa-
tion. A second reason concerns the fact that syn-
tactic parsing is not always robust enough to deal
with real-world sentences. This may prevent ap-
proaches based on syntactic features from produc-
ing any result. Another related issue concerns the
fact that parsers are available only for few lan-
guages and may not produce reliable results when
used on domain specific texts (as is the case of
the biomedical literature). For example, most of
the participants at the Learning Language in Logic
(LLL) challenge on Genic Interaction Extraction
(see Section 4.2) were unable to successfully ex-
ploit linguistic information provided by parsers. It
is still an open issue whether the use of domain-
specific treebanks (such as the Genia treebank1)
</bodyText>
<footnote confidence="0.982359">
1http://www-tsujii.is.s.u-tokyo.ac.jp/
</footnote>
<page confidence="0.997743">
401
</page>
<bodyText confidence="0.999908971428572">
can be successfully exploited to overcome this
problem. Therefore it is essential to better investi-
gate the potential of approaches based exclusively
on simple linguistic features.
In our approach we use a combination of ker-
nel functions to represent two distinct informa-
tion sources: the global context where entities ap-
pear and their local contexts. The whole sentence
where the entities appear (global context) is used
to discover the presence of a relation between two
entities, similarly to what was done by Bunescu
and Mooney (2005b). Windows of limited size
around the entities (local contexts) provide use-
ful clues to identify the roles of the entities within
a relation. The approach has some resemblance
with what was proposed by Roth and Yih (2002).
The main difference is that we perform the extrac-
tion task in a single step via a combined kernel,
while they used two separate classifiers to identify
entities and relations and their output is later com-
bined with a probabilistic global inference.
We evaluated our relation extraction algorithm
on two biomedical data sets (i.e. the AImed cor-
pus and the LLL challenge data set; see Section
4). The motivations for using these benchmarks
derive from the increasing applicative interest in
tools able to extract relations between relevant en-
tities in biomedical texts and, consequently, from
the growing availability of annotated data sets.
The experiments show clearly that our approach
consistently improves previous results. Surpris-
ingly, it outperforms most of the systems based on
syntactic or semantic information, even when this
information is manually annotated (i.e. the LLL
challenge).
</bodyText>
<sectionHeader confidence="0.953432" genericHeader="introduction">
2 Problem Formalization
</sectionHeader>
<bodyText confidence="0.9512379375">
The problem considered here is that of iden-
tifying interactions between genes and proteins
from biomedical literature. More specifically, we
performed experiments on two slightly different
benchmark data sets (see Section 4 for a detailed
description). In the former (AImed) gene/protein
interactions are annotated without distinguishing
the type and roles of the two interacting entities.
The latter (LLL challenge) is more realistic (and
complex) because it also aims at identifying the
roles played by the interacting entities (agent and
target). For example, in Figure 1 three entities
are mentioned and two of the six ordered pairs of
GENIA/topics/Corpus/GTB.html
entities actually interact: (sigma(K), cwlH) and
(gerE, cwlH).
</bodyText>
<figureCaption confidence="0.990303">
Figure 1: A sentence with two relations, R12 and
R32, between three entities, E1, E2 and E3.
</figureCaption>
<bodyText confidence="0.998983">
In our approach we cast relation extraction as a
classification problem, in which examples are gen-
erated from sentences as follows.
First of all, we describe the complex case,
namely the protein/gene interactions (LLL chal-
lenge). For this data set entity recognition is per-
formed using a dictionary of protein and gene
names in which the type of the entities is unknown.
We generate examples for all the sentences con-
taining at least two entities. Thus the number of
examples generated for each sentence is given by
the combinations of distinct entities (N) selected
two at a time, i.e. NC2. For example, as the sen-
tence shown in Figure 1 contains three entities, the
total number of examples generated is 3C2 = 3. In
each example we assign the attribute CANDIDATE
to each of the candidate interacting entities, while
the other entities in the example are assigned the
attribute OTHER, meaning that they do not partici-
pate in the relation. If a relation holds between the
two candidate interacting entities the example is
labeled 1 or 2 (according to the roles of the inter-
acting entities, agent and target, i.e. to the direc-
tion of the relation); 0 otherwise. Figure 2 shows
the examples generated from the sentence in Fig-
ure 1.
</bodyText>
<figureCaption confidence="0.8851225">
Figure 2: The three protein-gene examples gener-
ated from the sentence in Figure 1.
</figureCaption>
<bodyText confidence="0.989357">
Note that in generating the examples from the
sentence in Figure 1 we did not create three neg-
</bodyText>
<page confidence="0.99797">
402
</page>
<bodyText confidence="0.999642304347826">
ative examples (there are six potential ordered re-
lations between three entities), thereby implicitly
under-sampling the data set. This allows us to
make the classification task simpler without loos-
ing information. As a matter of fact, generating
examples for each ordered pair of entities would
produce two subsets of the same size containing
similar examples (differing only for the attributes
CANDIDATE and OTHER), but with different clas-
sification labels. Furthermore, under-sampling al-
lows us to halve the data set size and reduce the
data skewness.
For the protein-protein interaction task (AImed)
we use the correct entities provided by the manual
annotation. As said at the beginning of this sec-
tion, this task is simpler than the LLL challenge
because there is no distinction between types (all
entities are proteins) and roles (the relation is sym-
metric). As a consequence, the examples are gen-
erated as described above with the following dif-
ference: an example is labeled 1 if a relation holds
between the two candidate interacting entities; 0
otherwise.
</bodyText>
<sectionHeader confidence="0.9782845" genericHeader="method">
3 Kernel Methods for Relation
Extraction
</sectionHeader>
<bodyText confidence="0.999953">
The basic idea behind kernel methods is to embed
the input data into a suitable feature space F via
a mapping function 0 : X → F, and then use
a linear algorithm for discovering nonlinear pat-
terns. Instead of using the explicit mapping 0, we
can use a kernel function K : X x X → R, that
corresponds to the inner product in a feature space
which is, in general, different from the input space.
Kernel methods allow us to design a modular
system, in which the kernel function acts as an
interface between the data and the learning algo-
rithm. Thus the kernel function is the only domain
specific module of the system, while the learning
algorithm is a general purpose component. Po-
tentially any kernel function can work with any
kernel-based algorithm. In our approach we use
Support Vector Machines (Vapnik, 1998).
In order to implement the approach based on
shallow linguistic information we employed a
linear combination of kernels. Different works
(Gliozzo et al., 2005; Zhao and Grishman, 2005;
Culotta and Sorensen, 2004) empirically demon-
strate the effectiveness of combining kernels in
this way, showing that the combined kernel always
improves the performance of the individual ones.
In addition, this formulation allows us to evalu-
ate the individual contribution of each informa-
tion source. We designed two families of kernels:
Global Context kernels and Local Context kernels,
in which each single kernel is explicitly calculated
as follows
</bodyText>
<equation confidence="0.994519">
K(x1, x2) = (0(x1), 0(x2)) , (1)
II0(x1)IIII0(x2)II
</equation>
<bodyText confidence="0.9999647">
where 0(·) is the embedding vector and II · II is the
2-norm. The kernel is normalized (divided) by the
product of the norms of embedding vectors. The
normalization factor plays an important role in al-
lowing us to integrate information from heteroge-
neous feature spaces. Even though the resulting
feature space has high dimensionality, an efficient
computation of Equation 1 can be carried out ex-
plicitly since the input representations defined be-
low are extremely sparse.
</bodyText>
<subsectionHeader confidence="0.983373">
3.1 Global Context Kernel
</subsectionHeader>
<bodyText confidence="0.9976764">
In (Bunescu and Mooney, 2005b), the authors ob-
served that a relation between two entities is gen-
erally expressed using only words that appear si-
multaneously in one of the following three pat-
terns:
Fore-Between: tokens before and between the
two candidate interacting entities. For in-
stance: binding of [P1] to [P2], interaction in-
volving [P1] and [P2], association of [P1] by
[P2].
Between: only tokens between the two candidate
interacting entities. For instance: [P1] asso-
ciates with [P2], [P1] binding to [P2], [P1],
inhibitor of [P2].
Between-After: tokens between and after the two
candidate interacting entities. For instance:
[P1] - [P2] association, [P1] and [P2] interact,
[P1] has influence on [P2] binding.
Our global context kernels operate on the patterns
above, where each pattern is represented using a
bag-of-words instead of sparse subsequences of
words, PoS tags, entity and chunk types, or Word-
Net synsets as in (Bunescu and Mooney, 2005b).
More formally, given a relation example R, we
represent a pattern P as a row vector
</bodyText>
<equation confidence="0.998156">
0P (R) = (tf(t1, P), tf(t2, P), ... , tf(tl, P)) E Rl, (2)
</equation>
<bodyText confidence="0.999609">
where the function tf(ti, P) records how many
times a particular token tz is used in P. Note that,
</bodyText>
<page confidence="0.996196">
403
</page>
<bodyText confidence="0.99979">
this approach differs from the standard bag-of-
words as punctuation and stop words are included
in OP, while the entities (with attribute CANDI-
DATE and OTHER) are not. To improve the clas-
sification performance, we have further extended
OP to embed n-grams of (contiguous) tokens (up
to n = 3). By substituting OP into Equation 1, we
obtain the n-gram kernel Kn, which counts com-
mon uni-grams, bi-grams, ... , n-grams that two
patterns have in common2. The Global Context
kernel KGC(R1, R2) is then defined as
</bodyText>
<equation confidence="0.998055">
KFB(R1, R2) + KB(R1, R2) + KBA(R1, R2), (3)
</equation>
<bodyText confidence="0.999053">
where KFB, KB and KBA are n-gram kernels
that operate on the Fore-Between, Between and
Between-After patterns respectively.
</bodyText>
<subsectionHeader confidence="0.986">
3.2 Local Context Kernel
</subsectionHeader>
<bodyText confidence="0.995382210526316">
The type of the candidate interacting entities can
provide useful clues for detecting the agent and
target of the relation, as well as the presence of the
relation itself. As the type is not known, we use
the information provided by the two local contexts
of the candidate interacting entities, called left and
right local context respectively. As typically done
in entity recognition, we represent each local con-
text by using the following basic features:
Token The token itself.
Lemma The lemma of the token.
PoS The PoS tag of the token.
Orthographic This feature maps each token into
equivalence classes that encode attributes
such as capitalization, punctuation, numerals
and so on.
Formally, given a relation example R, a local con-
text L = t_w, ... , t_1, t0, t+1, ... , t+w is repre-
sented as a row vector
</bodyText>
<equation confidence="0.99827">
ψL(R) = (f1(L), f2(L),..., fm(L)) E {0, 1lm, (4)
</equation>
<bodyText confidence="0.9983765">
where fi is a feature function that returns 1 if it is
active in the specified position of L, 0 otherwise3.
The Local Context kernel KLC(R1, R2) is defined
as
</bodyText>
<equation confidence="0.997068">
Kleft(R1, R2) + Kright(R1, R2), (5)
</equation>
<bodyText confidence="0.999981">
where Kleft and Kright are defined by substituting
the embedding of the left and right local context
into Equation 1 respectively.
</bodyText>
<footnote confidence="0.597713333333333">
2In the literature, it is also called n-spectrum kernel.
3In the reported experiments, we used a context window
of f2 tokens around the candidate entity.
</footnote>
<bodyText confidence="0.998414">
Notice that KLC differs substantially from
KGC as it considers the ordering of the tokens and
the feature space is enriched with PoS, lemma and
orthographic features.
</bodyText>
<subsectionHeader confidence="0.994525">
3.3 Shallow Linguistic Kernel
</subsectionHeader>
<bodyText confidence="0.932463">
Finally, the Shallow Linguistic kernel
</bodyText>
<equation confidence="0.9217535">
KSL(R1, R2) is defined as
KGC(R1, R2) + KLC(R1, R2). (6)
</equation>
<bodyText confidence="0.999923333333333">
It follows directly from the explicit construction
of the feature space and from closure properties of
kernels that KSL is a valid kernel.
</bodyText>
<sectionHeader confidence="0.99551" genericHeader="method">
4 Data sets
</sectionHeader>
<bodyText confidence="0.99995025">
The two data sets used for the experiments concern
the same domain (i.e. gene/protein interactions).
However, they present a crucial difference which
makes it worthwhile to show the experimental re-
sults on both of them. In one case (AImed) in-
teractions are considered symmetric, while in the
other (LLL challenge) agents and targets of genic
interactions have to be identified.
</bodyText>
<subsectionHeader confidence="0.985949">
4.1 AImed corpus
</subsectionHeader>
<bodyText confidence="0.999972090909091">
The first data set used in the experiments is the
AImed corpus4, previously used for training pro-
tein interaction extraction systems in (Bunescu et
al., 2005; Bunescu and Mooney, 2005b). It con-
sists of 225 Medline abstracts: 200 are known
to describe interactions between human proteins,
while the other 25 do not refer to any interaction.
There are 4,084 protein references and around
1,000 tagged interactions in this data set. In this
data set there is no distinction between genes and
proteins and the relations are symmetric.
</bodyText>
<subsectionHeader confidence="0.979507">
4.2 LLL Challenge
</subsectionHeader>
<bodyText confidence="0.9999579">
This data set was used in the Learning Language
in Logic (LLL) challenge on Genic Interaction
extraction5 (Ned´ellec, 2005). The objective of
the challenge was to evaluate the performance of
systems based on machine learning techniques to
identify gene/protein interactions and their roles,
agent or target. The data set was collected by
querying Medline on Bacillus subtilis transcrip-
tion and sporulation. It is divided in a training set
(80 sentences describing 271 interactions) and a
</bodyText>
<footnote confidence="0.99581875">
4ftp://ftp.cs.utexas.edu/pub/mooney/
bio-data/interactions.tar.gz
5http://genome.jouy.inra.fr/texte/
LLLchallenge/
</footnote>
<page confidence="0.997136">
404
</page>
<bodyText confidence="0.9999415">
test set (87 sentences describing 106 interactions).
Differently from the training set, the test set con-
tains sentences without interactions. The data set
is decomposed in two subsets of increasing diffi-
culty. The first subset does not include corefer-
ences, while the second one includes simple cases
of coreference, mainly appositions. Both subsets
are available with different kinds of annotation:
basic and enriched. The former includes word and
sentence segmentation. The latter also includes
manually checked information, such as lemma and
syntactic dependencies. A dictionary of named
entities (including typographical variants and syn-
onyms) is associated to the data set.
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999648333333333">
Before describing the results of the experiments,
a note concerning the evaluation methodology.
There are different ways of evaluating perfor-
mance in extracting information, as noted in
(Lavelli et al., 2004) for the extraction of slot
fillers in the Seminar Announcement and the Job
Posting data sets. Adapting the proposed classi-
fication to relation extraction, the following two
cases can be identified:
</bodyText>
<listItem confidence="0.922025">
• One Answer per Occurrence in the Document
– OAOD (each individual occurrence of a
protein interaction has to be extracted from
the document);
• One Answer per Relation in a given Docu-
ment – OARD (where two occurrences of the
same protein interaction are considered one
correct answer).
</listItem>
<bodyText confidence="0.976372090909091">
Figure 3 shows a fragment of tagged text drawn
from the AImed corpus. It contains three different
interactions between pairs of proteins, for a total
of seven occurrences of interactions. For example,
there are three occurrences of the interaction be-
tween IGF-IR and p52Shc (i.e. number 1, 3 and
7). If we adopt the OAOD methodology, all the
seven occurrences have to be extracted to achieve
the maximum score. On the other hand, if we use
the OARD methodology, only one occurrence for
each interaction has to be extracted to maximize
the score.
On the AImed data set both evaluations were
performed, while on the LLL challenge only the
OAOD evaluation methodology was performed
because this is the only one provided by the eval-
uation server of the challenge.
Figure 3: Fragment of the AImed corpus with all
proteins and their interactions tagged. The pro-
tein names have been highlighted in bold face and
their same subscript numbers indicate interaction
between the proteins.
</bodyText>
<subsectionHeader confidence="0.980309">
5.1 Implementation Details
</subsectionHeader>
<bodyText confidence="0.999878">
All the experiments were performed using the
SVM package LIBSVM6 customized to embed our
own kernel. For the LLL challenge submission,
we optimized the regularization parameter C by
10-fold cross validation; while we used its default
value for the AImed experiment. In both exper-
iments, we set the cost-factor WZ to be the ratio
between the number of negative and positive ex-
amples.
</bodyText>
<subsectionHeader confidence="0.969141">
5.2 Results on AImed
</subsectionHeader>
<bodyText confidence="0.99992352631579">
KSL performance was first evaluated on the
AImed data set (Section 4.1). We first give an
evaluation of the kernel combination and then we
compare our results with the Subsequence Ker-
nel for Relation Extraction (ERK) described in
(Bunescu and Mooney, 2005b). All experiments
are conducted using 10-fold cross validation on
the same data splitting used in (Bunescu et al.,
2005; Bunescu and Mooney, 2005b).
Table 1 shows the performance of the three ker-
nels defined in Section 3 for protein-protein in-
teractions using the two evaluation methodologies
described above.
We report in Figure 4 the precision-recall curves
of ERK and KSL using OARD evaluation method-
ology (the evaluation performed by Bunescu and
Mooney (2005b)). As in (Bunescu et al., 2005;
Bunescu and Mooney, 2005b), the graph points are
obtained by varying the threshold on the classifi-
</bodyText>
<footnote confidence="0.9914825">
6http://www.csie.ntu.edu.tw/˜cjlin/
libsvm/
</footnote>
<page confidence="0.995486">
405
</page>
<table confidence="0.999392818181818">
OAOD
Kernel Precision Recall Fl
KGC 57.7 60.1 58.9
KLC 37.3 56.3 44.9
KSL 60.9 57.2 59.0
OARD
Kernel Precision Recall Fl
KGC 58.9 66.2 62.2
KLC 44.8 67.8 54.0
KSL 64.5 63.2 63.9
ERK 65.0 46.4 54.2
</table>
<tableCaption confidence="0.656743166666667">
Table 1: Performance on the AImed data set us-
ing the two evaluation methodologies, OAOD and
OARD.
cation confidence7. The results clearly show that
KSL outperforms ERK, especially in term of re-
call (see Table 1).
</tableCaption>
<figure confidence="0.84759">
KSL vs. ERK
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.9101385">
Figure 4: Precision-recall curves on the AImed
data set using OARD evaluation methodology.
</figureCaption>
<bodyText confidence="0.96200025">
Finally, Figure 5 shows the learning curve of the
combined kernel KSL using the OARD evaluation
methodology. The curve reaches a plateau with
around 100 Medline abstracts.
</bodyText>
<subsectionHeader confidence="0.990695">
5.3 Results on LLL challenge
</subsectionHeader>
<bodyText confidence="0.999836777777778">
The system was evaluated on the “basic” version
of the LLL challenge data set (Section 4.2).
Table 2 shows the results of KSL returned by
the scoring service8 for the three subsets of the
training set (with and without coreferences, and
with their union). Table 3 shows the best results
obtained at the official competition performed in
April 2005. Comparing the results we see that
KSL trained on each subset outperforms the best
</bodyText>
<footnote confidence="0.95576625">
7For this purpose the probability estimate output of LIB-
SVM is used.
8http://genome.jouy.inra.fr/texte/
LLLchallenge/scoringService.php
</footnote>
<figure confidence="0.8417735">
0 50 100 150 200
Number of documents
</figure>
<figureCaption confidence="0.7655915">
Figure 5: KSL learning curve on the AImed data
set using OARD evaluation methodology.
</figureCaption>
<table confidence="0.99957675">
Coref. Precision Recall Fl
all 56.0 61.4 58.6
with 29.0 31.0 30.0
without 54.8 62.9 58.6
</table>
<tableCaption confidence="0.9867065">
Table 2: KSL performance on the LLL challenge
test set using only the basic linguistic information.
</tableCaption>
<bodyText confidence="0.999709769230769">
systems of the LLL challenge9. Notice that the
best results at the challenge were obtained by dif-
ferent groups and exploiting the linguistic “en-
riched” version of the data set. As observed in
(Ned´ellec, 2005), the scores obtained using the
training set without coreferences and the whole
training set are similar.
We also report in Table 4 an analysis of the ker-
nel combination. Given that we are interested here
in the contribution of each kernel, we evaluated
the experiments by 10-fold cross-validation on the
whole training set avoiding the submission pro-
cess.
</bodyText>
<subsectionHeader confidence="0.998878">
5.4 Discussion of Results
</subsectionHeader>
<bodyText confidence="0.854771357142857">
The experimental results show that the combined
kernel KSL outperforms the basic kernels KGC
and KLC on both data sets. In particular, precision
significantly increases at the expense of a lower re-
call. High precision is particularly advantageous
when extracting knowledge from large corpora,
because it avoids overloading end users with too
many false positives.
Although the basic kernels were designed to
model complementary aspects of the task (i.e.
9After the challenge deadline, Reidel and Klein (2005)
achieved a significant improvement, Fl = 68.4% (without
coreferences) and Fl = 64.7% (with and without corefer-
ences).
</bodyText>
<figure confidence="0.998672875">
Precision
0.8
0.6
0.4
0.2
0
1
ERK
KSL
1
0.8
0.6
F1
0.4
0.2
0
</figure>
<page confidence="0.996779">
406
</page>
<table confidence="0.999955428571428">
Test set Coref. Precision Recall Fl
Enriched all 55.6 53.0 54.3
with 29.0 31.0 24.4
without 60.9 46.2 52.6
Basic all n/a n/a n/a
with 14.0 82.7 24.0
without 50.0 53.8 51.8
</table>
<tableCaption confidence="0.994029">
Table 3: Best performance on basic and enriched
test sets obtained by participants in the official
competition at the LLL challenge.
</tableCaption>
<table confidence="0.99994875">
Kernel Precision Recall Fl
KGC 55.1 66.3 60.2
KLC 44.8 60.1 53.8
KSL 62.1 61.3 61.7
</table>
<tableCaption confidence="0.886186333333333">
Table 4: Comparison of the performance of kernel
combination on the LLL challenge using 10-fold
cross validation.
</tableCaption>
<bodyText confidence="0.999969166666666">
presence of the relation and roles of the interact-
ing entities), they perform reasonably well even
when considered separately. In particular, KGC
achieved good performance on both data sets. This
result was not expected on the LLL challenge be-
cause this task requires not only to recognize the
presence of relationships between entities but also
to identify their roles. On the other hand, the out-
comes of KLC on the AImed data set show that
such kernel helps to identify the presence of rela-
tionships as well.
At first glance, it may seem strange that KGC
outperforms ERK on AImed, as the latter ap-
proach exploits a richer representation: sparse
sub-sequences of words, PoS tags, entity and
chunk types, or WordNet synsets. However, an
approach based on n-grams is sufficient to identify
the presence of a relationship. This result sounds
less surprising, if we recall that both approaches
cast the relation extraction problem as a text cate-
gorization task. Approaches to text categorization
based on rich linguistic information have obtained
less accuracy than the traditional bag-of-words ap-
proach (e.g. (Koster and Seutter, 2003)). Shallow
linguistics information seems to be more effective
to model the local context of the entities.
Finally, we obtained worse results performing
dimensionality reduction either based on generic
linguistic assumptions (e.g. by removing words
from stop lists or with certain PoS tags) or using
statistical methods (e.g. tf.idf weighting schema).
This may be explained by the fact that, in tasks like
entity recognition and relation extraction, useful
clues are also provided by high frequency tokens,
such as stop words or punctuation marks, and by
the relative positions in which they appear.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999595">
First of all, the obvious references for our work
are the approaches evaluated on AImed and LLL
challenge data sets.
In (Bunescu and Mooney, 2005b), the authors
present a generalized subsequence kernel that
works with sparse sequences containing combina-
tions of words and PoS tags.
The best results on the LLL challenge were ob-
tained by the group from the University of Ed-
inburgh (Reidel and Klein, 2005), which used
Markov Logic, a framework that combines log-
linear models and First Order Logic, to create a
set of weighted clauses which can classify pairs of
gene named entities as genic interactions. These
clauses are based on chains of syntactic and se-
mantic relations in the parse or Discourse Repre-
sentation Structure (DRS) of a sentence, respec-
tively.
Other relevant approaches include those that
adopt kernel methods to perform relation extrac-
tion. Zelenko et al. (2003) describe a relation ex-
traction algorithm that uses a tree kernel defined
over a shallow parse tree representation of sen-
tences. The approach is vulnerable to unrecover-
able parsing errors. Culotta and Sorensen (2004)
describe a slightly generalized version of this ker-
nel based on dependency trees, in which a bag-of-
words kernel is used to compensate for errors in
syntactic analysis. A further extension is proposed
by Zhao and Grishman (2005). They use compos-
ite kernels to integrate information from different
syntactic sources (tokenization, sentence parsing,
and deep dependency analysis) so that process-
ing errors occurring at one level may be overcome
by information from other levels. Bunescu and
Mooney (2005a) present an alternative approach
which uses information concentrated in the short-
est path in the dependency tree between the two
entities.
As mentioned in Section 1, another relevant ap-
proach is presented in (Roth and Yih, 2002). Clas-
sifiers that identify entities and relations among
them are first learned from local information in
the sentence. This information, along with con-
straints induced among entity types and relations,
is used to perform global probabilistic inference
</bodyText>
<page confidence="0.99471">
407
</page>
<bodyText confidence="0.9997675">
that accounts for the mutual dependencies among
the entities.
All the previous approaches have been evalu-
ated on different data sets so that it is not possi-
ble to have a clear idea of which approach is better
than the other.
</bodyText>
<sectionHeader confidence="0.996963" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999975842105263">
The good results obtained using only shallow lin-
guistic features provide a higher baseline against
which it is possible to measure improvements ob-
tained using methods based on deep linguistic pro-
cessing. In the near future, we plan to extend our
work in several ways.
First, we would like to evaluate the contribu-
tion of syntactic information to relation extraction
from biomedical literature. With this aim, we will
integrate the output of a parser (possibly trained on
a domain-specific resource such the Genia Tree-
bank). Second, we plan to test the portability of
our model on ACE and MUC data sets. Third,
we would like to use a named entity recognizer
instead of assuming that entities are already ex-
tracted or given by a dictionary. Our long term
goal is to populate databases and ontologies by
extracting information from large text collections
such as Medline.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998931857142857">
We would like to thank Razvan Bunescu for pro-
viding detailed information about the AImed data
set and the settings of the experiments. Clau-
dio Giuliano and Lorenza Romano have been sup-
ported by the ONTOTEXT project, funded by the
Autonomous Province of Trento under the FUP-
2004 research program.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999206014925373">
Razvan Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, Van-
couver, B.C, October.
Razvan Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In
Proceedings of the 19th Conference on Neural In-
formation Processing Systems, Vancouver, British
Columbia.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139–155. Special Issue on Sum-
marization and Information Extraction from Medi-
cal Documents.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL 2004), Barcelona,
Spain.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strappar-
ava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor, Michigan, June.
Cornelis H. A. Koster and Mark Seutter. 2003. Taming
wild phrases. In Advances in Information Retrieval,
25th European Conference on IR Research (ECIR
2003), pages 161–176, Pisa, Italy.
Alberto Lavelli, Mary Elaine Califf, Fabio Ciravegna,
Dayne Freitag, Claudio Giuliano, Nicholas Kushm-
erick, and Lorenza Romano. 2004. IE evaluation:
Criticisms and recommendations. In Proceedings of
the AAAI 2004 Workshop on Adaptive Text Extrac-
tion and Mining (ATEM 2004), San Jose, California.
Claire Ned´ellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the ICML-2005 Workshop on Learning Lan-
guage in Logic (LLL05), pages 31–37, Bonn, Ger-
many, August.
Sebastian Reidel and Ewan Klein. 2005. Genic
interaction extraction with semantic and syntactic
chains. In Proceedings of the ICML-2005 Workshop
on Learning Language in Logic (LLL05), pages 69–
74, Bonn, Germany, August.
D. Roth and W. Yih. 2002. Probabilistic reasoning
for entity &amp; relation recognition. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING-02), Taipei, Taiwan.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, New York, NY, USA.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for information
extraction. Journal of Machine Learning Research,
3:1083–1106.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor, Michigan, June.
</reference>
<page confidence="0.997814">
408
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.381832">
<title confidence="0.9995615">Exploiting Shallow Linguistic Information for Relation Extraction from Biomedical Literature</title>
<author confidence="0.999875">Claudio Giuliano</author>
<author confidence="0.999875">Alberto Lavelli</author>
<author confidence="0.999875">Lorenza Romano</author>
<affiliation confidence="0.463776">ITC-irst</affiliation>
<address confidence="0.765573333333333">Via Sommarive, 18 38050, Povo (TN) Italy</address>
<abstract confidence="0.9973326">We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information. We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities. We performed experiments on extracting gene and protein interactions from two different data sets. The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, B.C,</location>
<contexts>
<context position="4137" citStr="Bunescu and Mooney (2005" startWordPosition="615" endWordPosition="618">fic treebanks (such as the Genia treebank1) 1http://www-tsujii.is.s.u-tokyo.ac.jp/ 401 can be successfully exploited to overcome this problem. Therefore it is essential to better investigate the potential of approaches based exclusively on simple linguistic features. In our approach we use a combination of kernel functions to represent two distinct information sources: the global context where entities appear and their local contexts. The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities, similarly to what was done by Bunescu and Mooney (2005b). Windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within a relation. The approach has some resemblance with what was proposed by Roth and Yih (2002). The main difference is that we perform the extraction task in a single step via a combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with a probabilistic global inference. We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data se</context>
<context position="10652" citStr="Bunescu and Mooney, 2005" startWordPosition="1672" endWordPosition="1675">e kernel is explicitly calculated as follows K(x1, x2) = (0(x1), 0(x2)) , (1) II0(x1)IIII0(x2)II where 0(·) is the embedding vector and II · II is the 2-norm. The kernel is normalized (divided) by the product of the norms of embedding vectors. The normalization factor plays an important role in allowing us to integrate information from heterogeneous feature spaces. Even though the resulting feature space has high dimensionality, an efficient computation of Equation 1 can be carried out explicitly since the input representations defined below are extremely sparse. 3.1 Global Context Kernel In (Bunescu and Mooney, 2005b), the authors observed that a relation between two entities is generally expressed using only words that appear simultaneously in one of the following three patterns: Fore-Between: tokens before and between the two candidate interacting entities. For instance: binding of [P1] to [P2], interaction involving [P1] and [P2], association of [P1] by [P2]. Between: only tokens between the two candidate interacting entities. For instance: [P1] associates with [P2], [P1] binding to [P2], [P1], inhibitor of [P2]. Between-After: tokens between and after the two candidate interacting entities. For insta</context>
<context position="14894" citStr="Bunescu and Mooney, 2005" startWordPosition="2384" endWordPosition="2387">rnels that KSL is a valid kernel. 4 Data sets The two data sets used for the experiments concern the same domain (i.e. gene/protein interactions). However, they present a crucial difference which makes it worthwhile to show the experimental results on both of them. In one case (AImed) interactions are considered symmetric, while in the other (LLL challenge) agents and targets of genic interactions have to be identified. 4.1 AImed corpus The first data set used in the experiments is the AImed corpus4, previously used for training protein interaction extraction systems in (Bunescu et al., 2005; Bunescu and Mooney, 2005b). It consists of 225 Medline abstracts: 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction. There are 4,084 protein references and around 1,000 tagged interactions in this data set. In this data set there is no distinction between genes and proteins and the relations are symmetric. 4.2 LLL Challenge This data set was used in the Learning Language in Logic (LLL) challenge on Genic Interaction extraction5 (Ned´ellec, 2005). The objective of the challenge was to evaluate the performance of systems based on machine learning technique</context>
<context position="18918" citStr="Bunescu and Mooney, 2005" startWordPosition="3012" endWordPosition="3015">using the SVM package LIBSVM6 customized to embed our own kernel. For the LLL challenge submission, we optimized the regularization parameter C by 10-fold cross validation; while we used its default value for the AImed experiment. In both experiments, we set the cost-factor WZ to be the ratio between the number of negative and positive examples. 5.2 Results on AImed KSL performance was first evaluated on the AImed data set (Section 4.1). We first give an evaluation of the kernel combination and then we compare our results with the Subsequence Kernel for Relation Extraction (ERK) described in (Bunescu and Mooney, 2005b). All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b). Table 1 shows the performance of the three kernels defined in Section 3 for protein-protein interactions using the two evaluation methodologies described above. We report in Figure 4 the precision-recall curves of ERK and KSL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b)). As in (Bunescu et al., 2005; Bunescu and Mooney, 2005b), the graph points are obtained by varying the threshold on the classifi6htt</context>
<context position="24861" citStr="Bunescu and Mooney, 2005" startWordPosition="3977" endWordPosition="3980">sults performing dimensionality reduction either based on generic linguistic assumptions (e.g. by removing words from stop lists or with certain PoS tags) or using statistical methods (e.g. tf.idf weighting schema). This may be explained by the fact that, in tasks like entity recognition and relation extraction, useful clues are also provided by high frequency tokens, such as stop words or punctuation marks, and by the relative positions in which they appear. 6 Related Work First of all, the obvious references for our work are the approaches evaluated on AImed and LLL challenge data sets. In (Bunescu and Mooney, 2005b), the authors present a generalized subsequence kernel that works with sparse sequences containing combinations of words and PoS tags. The best results on the LLL challenge were obtained by the group from the University of Edinburgh (Reidel and Klein, 2005), which used Markov Logic, a framework that combines loglinear models and First Order Logic, to create a set of weighted clauses which can classify pairs of gene named entities as genic interactions. These clauses are based on chains of syntactic and semantic relations in the parse or Discourse Representation Structure (DRS) of a sentence,</context>
<context position="26312" citStr="Bunescu and Mooney (2005" startWordPosition="4209" endWordPosition="4212">e tree representation of sentences. The approach is vulnerable to unrecoverable parsing errors. Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-ofwords kernel is used to compensate for errors in syntactic analysis. A further extension is proposed by Zhao and Grishman (2005). They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so that processing errors occurring at one level may be overcome by information from other levels. Bunescu and Mooney (2005a) present an alternative approach which uses information concentrated in the shortest path in the dependency tree between the two entities. As mentioned in Section 1, another relevant approach is presented in (Roth and Yih, 2002). Classifiers that identify entities and relations among them are first learned from local information in the sentence. This information, along with constraints induced among entity types and relations, is used to perform global probabilistic inference 407 that accounts for the mutual dependencies among the entities. All the previous approaches have been evaluated on </context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond J. Mooney. 2005a. A shortest path dependency kernel for relation extraction. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th Conference on Neural Information Processing Systems,</booktitle>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="4137" citStr="Bunescu and Mooney (2005" startWordPosition="615" endWordPosition="618">fic treebanks (such as the Genia treebank1) 1http://www-tsujii.is.s.u-tokyo.ac.jp/ 401 can be successfully exploited to overcome this problem. Therefore it is essential to better investigate the potential of approaches based exclusively on simple linguistic features. In our approach we use a combination of kernel functions to represent two distinct information sources: the global context where entities appear and their local contexts. The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities, similarly to what was done by Bunescu and Mooney (2005b). Windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within a relation. The approach has some resemblance with what was proposed by Roth and Yih (2002). The main difference is that we perform the extraction task in a single step via a combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with a probabilistic global inference. We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data se</context>
<context position="10652" citStr="Bunescu and Mooney, 2005" startWordPosition="1672" endWordPosition="1675">e kernel is explicitly calculated as follows K(x1, x2) = (0(x1), 0(x2)) , (1) II0(x1)IIII0(x2)II where 0(·) is the embedding vector and II · II is the 2-norm. The kernel is normalized (divided) by the product of the norms of embedding vectors. The normalization factor plays an important role in allowing us to integrate information from heterogeneous feature spaces. Even though the resulting feature space has high dimensionality, an efficient computation of Equation 1 can be carried out explicitly since the input representations defined below are extremely sparse. 3.1 Global Context Kernel In (Bunescu and Mooney, 2005b), the authors observed that a relation between two entities is generally expressed using only words that appear simultaneously in one of the following three patterns: Fore-Between: tokens before and between the two candidate interacting entities. For instance: binding of [P1] to [P2], interaction involving [P1] and [P2], association of [P1] by [P2]. Between: only tokens between the two candidate interacting entities. For instance: [P1] associates with [P2], [P1] binding to [P2], [P1], inhibitor of [P2]. Between-After: tokens between and after the two candidate interacting entities. For insta</context>
<context position="14894" citStr="Bunescu and Mooney, 2005" startWordPosition="2384" endWordPosition="2387">rnels that KSL is a valid kernel. 4 Data sets The two data sets used for the experiments concern the same domain (i.e. gene/protein interactions). However, they present a crucial difference which makes it worthwhile to show the experimental results on both of them. In one case (AImed) interactions are considered symmetric, while in the other (LLL challenge) agents and targets of genic interactions have to be identified. 4.1 AImed corpus The first data set used in the experiments is the AImed corpus4, previously used for training protein interaction extraction systems in (Bunescu et al., 2005; Bunescu and Mooney, 2005b). It consists of 225 Medline abstracts: 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction. There are 4,084 protein references and around 1,000 tagged interactions in this data set. In this data set there is no distinction between genes and proteins and the relations are symmetric. 4.2 LLL Challenge This data set was used in the Learning Language in Logic (LLL) challenge on Genic Interaction extraction5 (Ned´ellec, 2005). The objective of the challenge was to evaluate the performance of systems based on machine learning technique</context>
<context position="18918" citStr="Bunescu and Mooney, 2005" startWordPosition="3012" endWordPosition="3015">using the SVM package LIBSVM6 customized to embed our own kernel. For the LLL challenge submission, we optimized the regularization parameter C by 10-fold cross validation; while we used its default value for the AImed experiment. In both experiments, we set the cost-factor WZ to be the ratio between the number of negative and positive examples. 5.2 Results on AImed KSL performance was first evaluated on the AImed data set (Section 4.1). We first give an evaluation of the kernel combination and then we compare our results with the Subsequence Kernel for Relation Extraction (ERK) described in (Bunescu and Mooney, 2005b). All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b). Table 1 shows the performance of the three kernels defined in Section 3 for protein-protein interactions using the two evaluation methodologies described above. We report in Figure 4 the precision-recall curves of ERK and KSL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b)). As in (Bunescu et al., 2005; Bunescu and Mooney, 2005b), the graph points are obtained by varying the threshold on the classifi6htt</context>
<context position="24861" citStr="Bunescu and Mooney, 2005" startWordPosition="3977" endWordPosition="3980">sults performing dimensionality reduction either based on generic linguistic assumptions (e.g. by removing words from stop lists or with certain PoS tags) or using statistical methods (e.g. tf.idf weighting schema). This may be explained by the fact that, in tasks like entity recognition and relation extraction, useful clues are also provided by high frequency tokens, such as stop words or punctuation marks, and by the relative positions in which they appear. 6 Related Work First of all, the obvious references for our work are the approaches evaluated on AImed and LLL challenge data sets. In (Bunescu and Mooney, 2005b), the authors present a generalized subsequence kernel that works with sparse sequences containing combinations of words and PoS tags. The best results on the LLL challenge were obtained by the group from the University of Edinburgh (Reidel and Klein, 2005), which used Markov Logic, a framework that combines loglinear models and First Order Logic, to create a set of weighted clauses which can classify pairs of gene named entities as genic interactions. These clauses are based on chains of syntactic and semantic relations in the parse or Discourse Representation Structure (DRS) of a sentence,</context>
<context position="26312" citStr="Bunescu and Mooney (2005" startWordPosition="4209" endWordPosition="4212">e tree representation of sentences. The approach is vulnerable to unrecoverable parsing errors. Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-ofwords kernel is used to compensate for errors in syntactic analysis. A further extension is proposed by Zhao and Grishman (2005). They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so that processing errors occurring at one level may be overcome by information from other levels. Bunescu and Mooney (2005a) present an alternative approach which uses information concentrated in the shortest path in the dependency tree between the two entities. As mentioned in Section 1, another relevant approach is presented in (Roth and Yih, 2002). Classifiers that identify entities and relations among them are first learned from local information in the sentence. This information, along with constraints induced among entity types and relations, is used to perform global probabilistic inference 407 that accounts for the mutual dependencies among the entities. All the previous approaches have been evaluated on </context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond J. Mooney. 2005b. Subsequence kernels for relation extraction. In Proceedings of the 19th Conference on Neural Information Processing Systems, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Ruifang Ge</author>
<author>Rohit J Kate</author>
<author>Edward M Marcotte</author>
<author>Raymond J Mooney</author>
<author>Arun K Ramani</author>
<author>Yuk Wah Wong</author>
</authors>
<title>Comparative experiments on learning information extractors for proteins and their interactions.</title>
<date>2005</date>
<journal>Artificial Intelligence in Medicine,</journal>
<booktitle>Special Issue on Summarization and Information Extraction from Medical Documents.</booktitle>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="14868" citStr="Bunescu et al., 2005" startWordPosition="2380" endWordPosition="2383">osure properties of kernels that KSL is a valid kernel. 4 Data sets The two data sets used for the experiments concern the same domain (i.e. gene/protein interactions). However, they present a crucial difference which makes it worthwhile to show the experimental results on both of them. In one case (AImed) interactions are considered symmetric, while in the other (LLL challenge) agents and targets of genic interactions have to be identified. 4.1 AImed corpus The first data set used in the experiments is the AImed corpus4, previously used for training protein interaction extraction systems in (Bunescu et al., 2005; Bunescu and Mooney, 2005b). It consists of 225 Medline abstracts: 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction. There are 4,084 protein references and around 1,000 tagged interactions in this data set. In this data set there is no distinction between genes and proteins and the relations are symmetric. 4.2 LLL Challenge This data set was used in the Learning Language in Logic (LLL) challenge on Genic Interaction extraction5 (Ned´ellec, 2005). The objective of the challenge was to evaluate the performance of systems based on </context>
<context position="19039" citStr="Bunescu et al., 2005" startWordPosition="3031" endWordPosition="3034">zation parameter C by 10-fold cross validation; while we used its default value for the AImed experiment. In both experiments, we set the cost-factor WZ to be the ratio between the number of negative and positive examples. 5.2 Results on AImed KSL performance was first evaluated on the AImed data set (Section 4.1). We first give an evaluation of the kernel combination and then we compare our results with the Subsequence Kernel for Relation Extraction (ERK) described in (Bunescu and Mooney, 2005b). All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b). Table 1 shows the performance of the three kernels defined in Section 3 for protein-protein interactions using the two evaluation methodologies described above. We report in Figure 4 the precision-recall curves of ERK and KSL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b)). As in (Bunescu et al., 2005; Bunescu and Mooney, 2005b), the graph points are obtained by varying the threshold on the classifi6http://www.csie.ntu.edu.tw/˜cjlin/ libsvm/ 405 OAOD Kernel Precision Recall Fl KGC 57.7 60.1 58.9 KLC 37.3 56.3 44.9 KSL 60.</context>
</contexts>
<marker>Bunescu, Ge, Kate, Marcotte, Mooney, Ramani, Wong, 2005</marker>
<rawString>Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M. Marcotte, Raymond J. Mooney, Arun K. Ramani, and Yuk Wah Wong. 2005. Comparative experiments on learning information extractors for proteins and their interactions. Artificial Intelligence in Medicine, 33(2):139–155. Special Issue on Summarization and Information Extraction from Medical Documents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2400" citStr="Culotta and Sorensen, 2004" startWordPosition="344" endWordPosition="347">, such as tokenization, sentence splitting, Part-of-Speech (PoS) tagging and lemmatization. Kernel methods (Shawe-Taylor and Cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space. For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhao and Grishman, 2005). Despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons. First of all, previous comparative evaluations put more stress on the deep linguistic approaches and did not put as much effort on developing effective methods based on shallow linguistic information. A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences. This may prevent approaches based on syntactic featur</context>
<context position="9648" citStr="Culotta and Sorensen, 2004" startWordPosition="1514" endWordPosition="1517">el methods allow us to design a modular system, in which the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function is the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our approach we use Support Vector Machines (Vapnik, 1998). In order to implement the approach based on shallow linguistic information we employed a linear combination of kernels. Different works (Gliozzo et al., 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004) empirically demonstrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source. We designed two families of kernels: Global Context kernels and Local Context kernels, in which each single kernel is explicitly calculated as follows K(x1, x2) = (0(x1), 0(x2)) , (1) II0(x1)IIII0(x2)II where 0(·) is the embedding vector and II · II is the 2-norm. The kernel is normalized (divided) by the product of the norm</context>
<context position="25811" citStr="Culotta and Sorensen (2004)" startWordPosition="4131" endWordPosition="4134">odels and First Order Logic, to create a set of weighted clauses which can classify pairs of gene named entities as genic interactions. These clauses are based on chains of syntactic and semantic relations in the parse or Discourse Representation Structure (DRS) of a sentence, respectively. Other relevant approaches include those that adopt kernel methods to perform relation extraction. Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences. The approach is vulnerable to unrecoverable parsing errors. Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-ofwords kernel is used to compensate for errors in syntactic analysis. A further extension is proposed by Zhao and Grishman (2005). They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so that processing errors occurring at one level may be overcome by information from other levels. Bunescu and Mooney (2005a) present an alternative approach which uses information concentrated in the shortest path in the </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005),</booktitle>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="9594" citStr="Gliozzo et al., 2005" startWordPosition="1506" endWordPosition="1509">n general, different from the input space. Kernel methods allow us to design a modular system, in which the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function is the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our approach we use Support Vector Machines (Vapnik, 1998). In order to implement the approach based on shallow linguistic information we employed a linear combination of kernels. Different works (Gliozzo et al., 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004) empirically demonstrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source. We designed two families of kernels: Global Context kernels and Local Context kernels, in which each single kernel is explicitly calculated as follows K(x1, x2) = (0(x1), 0(x2)) , (1) II0(x1)IIII0(x2)II where 0(·) is the embedding vector and II · II is the 2-norm. The ker</context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelis H A Koster</author>
<author>Mark Seutter</author>
</authors>
<title>Taming wild phrases.</title>
<date>2003</date>
<booktitle>In Advances in Information Retrieval, 25th European Conference on IR Research (ECIR</booktitle>
<pages>161--176</pages>
<location>Pisa, Italy.</location>
<contexts>
<context position="24102" citStr="Koster and Seutter, 2003" startWordPosition="3858" endWordPosition="3861">l. At first glance, it may seem strange that KGC outperforms ERK on AImed, as the latter approach exploits a richer representation: sparse sub-sequences of words, PoS tags, entity and chunk types, or WordNet synsets. However, an approach based on n-grams is sufficient to identify the presence of a relationship. This result sounds less surprising, if we recall that both approaches cast the relation extraction problem as a text categorization task. Approaches to text categorization based on rich linguistic information have obtained less accuracy than the traditional bag-of-words approach (e.g. (Koster and Seutter, 2003)). Shallow linguistics information seems to be more effective to model the local context of the entities. Finally, we obtained worse results performing dimensionality reduction either based on generic linguistic assumptions (e.g. by removing words from stop lists or with certain PoS tags) or using statistical methods (e.g. tf.idf weighting schema). This may be explained by the fact that, in tasks like entity recognition and relation extraction, useful clues are also provided by high frequency tokens, such as stop words or punctuation marks, and by the relative positions in which they appear. 6</context>
</contexts>
<marker>Koster, Seutter, 2003</marker>
<rawString>Cornelis H. A. Koster and Mark Seutter. 2003. Taming wild phrases. In Advances in Information Retrieval, 25th European Conference on IR Research (ECIR 2003), pages 161–176, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Lavelli</author>
<author>Mary Elaine Califf</author>
<author>Fabio Ciravegna</author>
<author>Dayne Freitag</author>
<author>Claudio Giuliano</author>
<author>Nicholas Kushmerick</author>
<author>Lorenza Romano</author>
</authors>
<title>IE evaluation: Criticisms and recommendations.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI 2004 Workshop on Adaptive Text Extraction and Mining (ATEM 2004),</booktitle>
<location>San Jose, California.</location>
<contexts>
<context position="16768" citStr="Lavelli et al., 2004" startWordPosition="2657" endWordPosition="2660">ncludes simple cases of coreference, mainly appositions. Both subsets are available with different kinds of annotation: basic and enriched. The former includes word and sentence segmentation. The latter also includes manually checked information, such as lemma and syntactic dependencies. A dictionary of named entities (including typographical variants and synonyms) is associated to the data set. 5 Experiments Before describing the results of the experiments, a note concerning the evaluation methodology. There are different ways of evaluating performance in extracting information, as noted in (Lavelli et al., 2004) for the extraction of slot fillers in the Seminar Announcement and the Job Posting data sets. Adapting the proposed classification to relation extraction, the following two cases can be identified: • One Answer per Occurrence in the Document – OAOD (each individual occurrence of a protein interaction has to be extracted from the document); • One Answer per Relation in a given Document – OARD (where two occurrences of the same protein interaction are considered one correct answer). Figure 3 shows a fragment of tagged text drawn from the AImed corpus. It contains three different interactions be</context>
</contexts>
<marker>Lavelli, Califf, Ciravegna, Freitag, Giuliano, Kushmerick, Romano, 2004</marker>
<rawString>Alberto Lavelli, Mary Elaine Califf, Fabio Ciravegna, Dayne Freitag, Claudio Giuliano, Nicholas Kushmerick, and Lorenza Romano. 2004. IE evaluation: Criticisms and recommendations. In Proceedings of the AAAI 2004 Workshop on Adaptive Text Extraction and Mining (ATEM 2004), San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Ned´ellec</author>
</authors>
<title>Learning language in logic -genic interaction extraction challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the ICML-2005 Workshop on Learning Language in Logic (LLL05),</booktitle>
<pages>31--37</pages>
<location>Bonn, Germany,</location>
<marker>Ned´ellec, 2005</marker>
<rawString>Claire Ned´ellec. 2005. Learning language in logic -genic interaction extraction challenge. In Proceedings of the ICML-2005 Workshop on Learning Language in Logic (LLL05), pages 31–37, Bonn, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Reidel</author>
<author>Ewan Klein</author>
</authors>
<title>Genic interaction extraction with semantic and syntactic chains.</title>
<date>2005</date>
<booktitle>In Proceedings of the ICML-2005 Workshop on Learning Language in Logic (LLL05),</booktitle>
<pages>69--74</pages>
<location>Bonn, Germany,</location>
<contexts>
<context position="22282" citStr="Reidel and Klein (2005)" startWordPosition="3554" endWordPosition="3557">s by 10-fold cross-validation on the whole training set avoiding the submission process. 5.4 Discussion of Results The experimental results show that the combined kernel KSL outperforms the basic kernels KGC and KLC on both data sets. In particular, precision significantly increases at the expense of a lower recall. High precision is particularly advantageous when extracting knowledge from large corpora, because it avoids overloading end users with too many false positives. Although the basic kernels were designed to model complementary aspects of the task (i.e. 9After the challenge deadline, Reidel and Klein (2005) achieved a significant improvement, Fl = 68.4% (without coreferences) and Fl = 64.7% (with and without coreferences). Precision 0.8 0.6 0.4 0.2 0 1 ERK KSL 1 0.8 0.6 F1 0.4 0.2 0 406 Test set Coref. Precision Recall Fl Enriched all 55.6 53.0 54.3 with 29.0 31.0 24.4 without 60.9 46.2 52.6 Basic all n/a n/a n/a with 14.0 82.7 24.0 without 50.0 53.8 51.8 Table 3: Best performance on basic and enriched test sets obtained by participants in the official competition at the LLL challenge. Kernel Precision Recall Fl KGC 55.1 66.3 60.2 KLC 44.8 60.1 53.8 KSL 62.1 61.3 61.7 Table 4: Comparison of the </context>
<context position="25120" citStr="Reidel and Klein, 2005" startWordPosition="4020" endWordPosition="4023">asks like entity recognition and relation extraction, useful clues are also provided by high frequency tokens, such as stop words or punctuation marks, and by the relative positions in which they appear. 6 Related Work First of all, the obvious references for our work are the approaches evaluated on AImed and LLL challenge data sets. In (Bunescu and Mooney, 2005b), the authors present a generalized subsequence kernel that works with sparse sequences containing combinations of words and PoS tags. The best results on the LLL challenge were obtained by the group from the University of Edinburgh (Reidel and Klein, 2005), which used Markov Logic, a framework that combines loglinear models and First Order Logic, to create a set of weighted clauses which can classify pairs of gene named entities as genic interactions. These clauses are based on chains of syntactic and semantic relations in the parse or Discourse Representation Structure (DRS) of a sentence, respectively. Other relevant approaches include those that adopt kernel methods to perform relation extraction. Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentenc</context>
</contexts>
<marker>Reidel, Klein, 2005</marker>
<rawString>Sebastian Reidel and Ewan Klein. 2005. Genic interaction extraction with semantic and syntactic chains. In Proceedings of the ICML-2005 Workshop on Learning Language in Logic (LLL05), pages 69– 74, Bonn, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Probabilistic reasoning for entity &amp; relation recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING-02),</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4359" citStr="Roth and Yih (2002)" startWordPosition="652" endWordPosition="655">clusively on simple linguistic features. In our approach we use a combination of kernel functions to represent two distinct information sources: the global context where entities appear and their local contexts. The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities, similarly to what was done by Bunescu and Mooney (2005b). Windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within a relation. The approach has some resemblance with what was proposed by Roth and Yih (2002). The main difference is that we perform the extraction task in a single step via a combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with a probabilistic global inference. We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data set; see Section 4). The motivations for using these benchmarks derive from the increasing applicative interest in tools able to extract relations between relevant entities in biomedical texts and, consequently, from the gro</context>
<context position="26542" citStr="Roth and Yih, 2002" startWordPosition="4246" endWordPosition="4249"> is used to compensate for errors in syntactic analysis. A further extension is proposed by Zhao and Grishman (2005). They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so that processing errors occurring at one level may be overcome by information from other levels. Bunescu and Mooney (2005a) present an alternative approach which uses information concentrated in the shortest path in the dependency tree between the two entities. As mentioned in Section 1, another relevant approach is presented in (Roth and Yih, 2002). Classifiers that identify entities and relations among them are first learned from local information in the sentence. This information, along with constraints induced among entity types and relations, is used to perform global probabilistic inference 407 that accounts for the mutual dependencies among the entities. All the previous approaches have been evaluated on different data sets so that it is not possible to have a clear idea of which approach is better than the other. 7 Conclusions and Future Work The good results obtained using only shallow linguistic features provide a higher baseli</context>
</contexts>
<marker>Roth, Yih, 2002</marker>
<rawString>D. Roth and W. Yih. 2002. Probabilistic reasoning for entity &amp; relation recognition. In Proceedings of the 19th International Conference on Computational Linguistics (COLING-02), Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1917" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="269" endWordPosition="272">d performance (even if it is a bit worse than that reported on news articles). However, relation identification is more useful from an applicative perspective but it is still a considerable challenge for automatic tools. In this work, we propose a supervised machine learning approach to relation extraction which is applicable even when (deep) linguistic processing is not available or reliable. In particular, we explore a kernel-based approach based solely on shallow linguistic processing, such as tokenization, sentence splitting, Part-of-Speech (PoS) tagging and lemmatization. Kernel methods (Shawe-Taylor and Cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space. For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhao and Grishman, 2005). Despite the positive results obtained exploiting syntactic information, we claim that the</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley and Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="9435" citStr="Vapnik, 1998" startWordPosition="1484" endWordPosition="1485">nstead of using the explicit mapping 0, we can use a kernel function K : X x X → R, that corresponds to the inner product in a feature space which is, in general, different from the input space. Kernel methods allow us to design a modular system, in which the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function is the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our approach we use Support Vector Machines (Vapnik, 1998). In order to implement the approach based on shallow linguistic information we employed a linear combination of kernels. Different works (Gliozzo et al., 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004) empirically demonstrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source. We designed two families of kernels: Global Context kernels and Local Context kernels, in which each single kerne</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik. 1998. Statistical Learning Theory. John Wiley and Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for information extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="2372" citStr="Zelenko et al., 2003" startWordPosition="340" endWordPosition="343"> linguistic processing, such as tokenization, sentence splitting, Part-of-Speech (PoS) tagging and lemmatization. Kernel methods (Shawe-Taylor and Cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space. For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhao and Grishman, 2005). Despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons. First of all, previous comparative evaluations put more stress on the deep linguistic approaches and did not put as much effort on developing effective methods based on shallow linguistic information. A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences. This may prevent approach</context>
<context position="25595" citStr="Zelenko et al. (2003)" startWordPosition="4097" endWordPosition="4100"> of words and PoS tags. The best results on the LLL challenge were obtained by the group from the University of Edinburgh (Reidel and Klein, 2005), which used Markov Logic, a framework that combines loglinear models and First Order Logic, to create a set of weighted clauses which can classify pairs of gene named entities as genic interactions. These clauses are based on chains of syntactic and semantic relations in the parse or Discourse Representation Structure (DRS) of a sentence, respectively. Other relevant approaches include those that adopt kernel methods to perform relation extraction. Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences. The approach is vulnerable to unrecoverable parsing errors. Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-ofwords kernel is used to compensate for errors in syntactic analysis. A further extension is proposed by Zhao and Grishman (2005). They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so tha</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for information extraction. Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005),</booktitle>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2426" citStr="Zhao and Grishman, 2005" startWordPosition="348" endWordPosition="351">ence splitting, Part-of-Speech (PoS) tagging and lemmatization. Kernel methods (Shawe-Taylor and Cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space. For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhao and Grishman, 2005). Despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons. First of all, previous comparative evaluations put more stress on the deep linguistic approaches and did not put as much effort on developing effective methods based on shallow linguistic information. A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences. This may prevent approaches based on syntactic features from producing any resu</context>
<context position="9619" citStr="Zhao and Grishman, 2005" startWordPosition="1510" endWordPosition="1513">rom the input space. Kernel methods allow us to design a modular system, in which the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function is the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our approach we use Support Vector Machines (Vapnik, 1998). In order to implement the approach based on shallow linguistic information we employed a linear combination of kernels. Different works (Gliozzo et al., 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004) empirically demonstrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source. We designed two families of kernels: Global Context kernels and Local Context kernels, in which each single kernel is explicitly calculated as follows K(x1, x2) = (0(x1), 0(x2)) , (1) II0(x1)IIII0(x2)II where 0(·) is the embedding vector and II · II is the 2-norm. The kernel is normalized (divide</context>
<context position="26039" citStr="Zhao and Grishman (2005)" startWordPosition="4169" endWordPosition="4172"> Representation Structure (DRS) of a sentence, respectively. Other relevant approaches include those that adopt kernel methods to perform relation extraction. Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences. The approach is vulnerable to unrecoverable parsing errors. Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-ofwords kernel is used to compensate for errors in syntactic analysis. A further extension is proposed by Zhao and Grishman (2005). They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so that processing errors occurring at one level may be overcome by information from other levels. Bunescu and Mooney (2005a) present an alternative approach which uses information concentrated in the shortest path in the dependency tree between the two entities. As mentioned in Section 1, another relevant approach is presented in (Roth and Yih, 2002). Classifiers that identify entities and relations among them are first learned from local inform</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), Ann Arbor, Michigan, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>