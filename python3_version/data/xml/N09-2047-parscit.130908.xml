<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000864">
<title confidence="0.721791333333333">
MICA: A Probabilistic Dependency Parser
Based on Tree Insertion Grammars
Application Note
Srinivas Bangalore Pierre Boulllier
AT&amp;T Labs – Research INRIA
Florham Park, NJ, USA Rocquencourt, France
</title>
<email confidence="0.897622">
srini@research.att.com Pierre.Boullier@inria.fr
</email>
<author confidence="0.940275">
Alexis Nasr Owen Rambow Benoit Sagot
</author>
<affiliation confidence="0.535165">
Aix-Marseille Universit´e CCLS, Columbia Univserity INRIA
</affiliation>
<address confidence="0.475104">
Marseille, France New York, NY, USA Rocquencourt, France
</address>
<email confidence="0.996597">
alexis.nasr@lif.univ-mrs.fr rambow@ccls.columbia.edu benoit.sagot@inria.fr
</email>
<sectionHeader confidence="0.995564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99454375">
MICA is a dependency parser which returns
deep dependency representations, is fast, has
state-of-the-art performance, and is freely
available.
</bodyText>
<sectionHeader confidence="0.994248" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.810581863636363">
This application note presents a freely avail-
able parser, MICA (Marseille-INRIA-Columbia-
AT&amp;T).1 MICA has several key characteristics that
make it appealing to researchers in NLP who need
an off-the-shelf parser.
• MICA returns a deep dependency parse, in
which dependency is defined in terms of lex-
ical predicate-argument structure, not in terms
of surface-syntactic features such as subject-verb
agreement. Function words such as auxiliaries
and determiners depend on their lexical head, and
strongly governed prepositions (such as to for give)
are treated as co-heads rather than as syntactic heads
in their own right. For example, John is giving books
to Mary gets the following analysis (the arc label is
on the terminal).
giving
John is books to Mary
arc=0 arc=adj arc=1 arc=co-head arc=2
The arc labels for the three arguments John,
books, and Mary do not change when the sentence
is passivized or Mary undergoes dative shift.
</bodyText>
<footnote confidence="0.984674">
1We would like to thank Ryan Roth for contributing the
MALT data.
</footnote>
<listItem confidence="0.93431925">
• MICA is based on an explicit phrase-structure
tree grammar extracted from the Penn Treebank.
Therefore, MICA can associate dependency parses
with rich linguistic information such as voice, the
presence of empty subjects (PRO), wh-movement,
and whether a verb heads a relative clause.
• MICA is fast (450 words per second plus 6 sec-
onds initialization on a standard high-end machine
on sentences with fewer than 200 words) and has
state-of-the-art performance (87.6% unlabeled de-
pendency accuracy, see Section 5).
• MICA consists of two processes: the supertag-
</listItem>
<bodyText confidence="0.853972833333333">
ger, which associates tags representing rich syntac-
tic information with the input word sequence, and
the actual parser, which derives the syntactic struc-
ture from the n-best chosen supertags. Only the su-
pertagger uses lexical information, the parser only
sees the supertag hypotheses.
</bodyText>
<listItem confidence="0.938895833333333">
• MICA returns n-best parses for arbitrary n;
parse trees are associated with probabilities. A
packed forest can also be returned.
• MICA is freely available2, easy to install under
Linux, and easy to use. (Input is one sentence per
line with no special tokenization required.)
</listItem>
<bodyText confidence="0.999868222222222">
There is an enormous amount of related work,
and we can mention only the most salient, given
space constraints. Our parser is very similar to the
work of (Shen and Joshi, 2005). They do not em-
ploy a supertagging step, and we do not restrict our
trees to spinal projections. Other parsers using su-
pertagging include the LDA of Bangalore and Joshi
(1999), the CCG-based parser of Clark and Curran
(2004), and the constraint-based approach of Wang
</bodyText>
<footnote confidence="0.96883">
2http://www1.ccls.columbia.edu/˜rambow/mica.html
</footnote>
<page confidence="0.958173">
185
</page>
<note confidence="0.366184">
Proceedings of NAACL HLT 2009: Short Papers, pages 185–188,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996595">
and Harper (2004). Widely used dependency parsers
which generate deep dependency representations in-
clude Minipar (Lin, 1994), which uses a declarative
grammar, and the Stanford parser (Levy and Man-
ning, 2004), which performs a conversion from a
standard phrase-structure parse. All of these systems
generate dependency structures which are slightly
different from MICA’s, so that direct comparison
is difficult. For comparison purposes, we therefore
use the MALT parser generator (Nivre et al., 2004),
which allows us to train a dependency parser on our
own dependency structures. MALT has been among
the top performers in the CoNLL dependency pars-
ing competitions.
</bodyText>
<sectionHeader confidence="0.73194" genericHeader="introduction">
2 Supertags and Supertagging
</sectionHeader>
<bodyText confidence="0.9999636">
Supertags are elementary trees of a lexicalized
tree grammar such as a Tree-Adjoining Gram-
mar (TAG) (Joshi, 1987). Unlike context-free gram-
mar rules which are single level trees, supertags are
multi-level trees which encapsulate both predicate-
argument structure of the anchor lexeme (by includ-
ing nodes at which its arguments must substitute)
and morpho-syntactic constraints such as subject-
verb agreement within the supertag associated with
the anchor. There are a number of supertags for each
lexeme to account for the different syntactic trans-
formations (relative clause, wh-question, passiviza-
tion etc.). For example, the verb give will be associ-
ated with at least these two trees, which we will call
tdi and tdi-dat. (There are also many other trees.)
</bodyText>
<equation confidence="0.7908115">
tdi tdi-dat
NP2 ↓
</equation>
<bodyText confidence="0.993189135135135">
to
Supertagging is the task of disambiguating among
the set of supertags associated with each word in
a sentence, given the context of the sentence. In
order to arrive at a complete parse, the only step
remaining after supertagging is establishing the at-
tachments among the supertags. Hence the result of
supertagging is termed as an “almost parse” (Banga-
lore and Joshi, 1999).
The set of supertags is derived from the Penn
Treebank using the approach of Chen (2001). This
extraction procedure results in a supertag set of
4,727 supertags and about one million words of su-
pertag annotated corpus. We use 950,028 annotated
words for training (Sections 02-21) and 46,451 (Sec-
tion 00) annotated words for testing in our exper-
iments. We estimate the probability of a tag se-
quence directly as in discriminative classification
approaches. In such approaches, the context of the
word being supertagged is encoded as features for
the classifier. Given the large scale multiclass la-
beling nature of the supertagging task, we train su-
pertagging models as one-vs-rest binary classifica-
tion problems. Detailed supertagging experiment re-
sults are reported in (Bangalore et al., 2005) which
we summarize here. We use the lexical, part-of-
speech attributes from the left and right context
in a 6-word window and the lexical, orthographic
(e.g. capitalization, prefix, suffix, digit) and part-
of-speech attributes of the word being supertagged.
Crucially, this set does not use the supertags for the
words in the history. Thus during decoding the su-
pertag assignment is done locally and does not need
a dynamic programming search. We trained a Max-
ent model with such features using the labeled data
set mentioned above and achieve an error rate of
11.48% on the test set.
</bodyText>
<sectionHeader confidence="0.991975" genericHeader="method">
3 Grammars and Models
</sectionHeader>
<bodyText confidence="0.9999768125">
MICA grammars are extracted in a three steps pro-
cess. In a first step, a Tree Insertion Grammar (TIG)
(Schabes and Waters, 1995) is extracted from the
treebank, along with a table of counts. This is the
grammar that is used for supertagging, as described
↓in Section 2. In a second step, the TIG and the count
table are used to build a PCFG. During the last step,
the PCFG is “specialized” in order to model more
finely some lexico-syntactic phenomena. The sec-
ond and third steps are discussed in this section.
The extracted TIG is transformed into a PCFG
which generates strings of supertags as follows. Ini-
tial elementary trees (which are substituted) yield
rules whose left hand side is the root category of
the elementary tree. Left (respectively right) aux-
iliary trees (the trees for which the foot node is the
</bodyText>
<equation confidence="0.997863857142857">
S
NP0 ↓ VP
V♦ NP1 ↓ PP
S
NP0 ↓ VP
V♦ NP2 ↓NP1
P
</equation>
<page confidence="0.974434">
186
</page>
<bodyText confidence="0.999884625">
left (resp. right) daughter of the root) give birth to
rules whose left-hand side is of the form Xl (resp.
Xr), where X is the root category of the elementary
tree. The right hand side of each rule is built during
a top down traversal of the corresponding elemen-
tary tree. For every node of the tree visited, a new
symbol is added to the right hand side of rule, from
left to right, as follows:
</bodyText>
<listItem confidence="0.97554075">
• The anchor of the elementary tree adds the su-
pertag (i.e., the name of the tree), which is a terminal
symbol, to the context-free rule.
• A substitution node in the elementary tree adds
its nonterminal symbol to the context-free rule.
• A interior node in the elementary tree at which
adjunction may occur adds to the context-free rule
the nonterminal symbol Xr∗ or X ∗
</listItem>
<bodyText confidence="0.99985131372549">
l , where X is the
node’s nonterminal symbol, and l (resp. r) indicates
whether it is a left (resp. right) adjunction. Each
interior node is visited twice, the first time from the
left, and then from the right. A set of non-lexicalized
rules (i.e., rules that do not generate a terminal sym-
bol) allow us to generate zero or more trees anchored
by Xl from the symbol X ∗
l . No adjunction, the first
adjunction, and the second adjunction are modeled
explicitly in the grammar and the associated prob-
abilistic model, while the third and all subsequent
adjunctions are modeled together.
This conversion method is basically the same as
that presented in (Schabes and Waters, 1995), ex-
cept that our PCFG models multiple adjunctions at
the same node by positions (a concern Schabes and
Waters (1995) do not share, of course). Our PCFG
construction differs from that of Hwa (2001) in that
she does not allow multiple adjunction at one node
(Schabes and Shieber, 1994) (which we do since we
are interested in the derivation structure as a repre-
sentation of linguistic dependency). For more in-
formation about the positional model of adjunction
and a discussion of an alternate model, the “bigram
model”, see (Nasr and Rambow, 2006).
Tree tdi from Section 2 gives rise to the following
rule (where tdi and tCO are terminal symbols and
the rest are nonterminals): S → S∗l NP VP∗l V∗l tdi
V∗r NP PP∗ l P∗l tCO P∗r NP PP∗ r VP∗ r S∗r
The probabilities of the PCFG rules are estimated
using maximum likelihood. The probabilistic model
refers only to supertag names, not to words. In the
basic model, the probability of the adjunction or sub-
stitution of an elementary tree (the daughter) in an-
other elementary tree (the mother) only depends on
the nonterminal, and does not depend on the mother
nor on the node on which the attachment is per-
formed in the mother elementary tree. It is well
known that such a dependency is important for an
adequate probabilistic modelling of syntax. In order
to introduce such a dependency, we condition an at-
tachment on the mother and on the node on which
the attachment is performed, an operation that we
call mother specialization. Mother specialization is
performed by adding to all nonterminals the name of
the mother and the address of a node. The special-
ization of a grammar increase vastly the number of
symbols and rules and provoke severe data sparse-
ness problems, this is why only a subset of the sym-
bols are specialized.
</bodyText>
<sectionHeader confidence="0.996609" genericHeader="method">
4 Parser
</sectionHeader>
<bodyText confidence="0.999123066666667">
SYNTAX (Boullier and Deschamp, 1988) is a sys-
tem used to generate lexical and syntactic analyzers
(parsers) (both deterministic and non-deterministic)
for all kind of context-free grammars (CFGs) as
well as some classes of contextual grammars. It
has been under development at INRIA for several
decades. SYNTAX handles most classes of determin-
istic (unambiguous) grammars (LR, LALR, RLR)
as well as general context-free grammars. The
non-deterministic features include, among others,
an Earley-like parser generator used for natural lan-
guage processing (Boullier, 2003).
Like most SYNTAX Earley-like parsers, the archi-
tecture of MICA’s PCFG-based parser is the follow-
ing:
</bodyText>
<listItem confidence="0.999533833333333">
• The Earley-like parser proper computes a shared
parse forest that represents in a factorized (polyno-
mial) way all possible parse trees according to the
underlying (non-probabilistic) CFG that represents
the TIG;
• Filtering and/or decoration modules are applied
</listItem>
<bodyText confidence="0.937717285714286">
on the shared parse forest; in MICA’s case, an n-
best module is applied, followed by a dependency
extractor that relies on the TIG structure of the CFG.
The Earley-like parser relies on Earley’s algo-
rithm (Earley, 1970). However, several optimiza-
tions have been applied, including guiding tech-
niques (Boullier, 2003), extensive static (offline)
</bodyText>
<page confidence="0.995579">
187
</page>
<bodyText confidence="0.99997128">
computations over the grammar, and efficient data
structures. Moreover, Earley’s algorithm has been
extended so as to handle input DAGs (and not only
sequences of forms). A particular effort has been
made to handle huge grammars (over 1 million
symbol occurrences in the grammar), thanks to ad-
vanced dynamic lexicalization techniques (Boullier
and Sagot, 2007). The resulting efficiency is satisfy-
ing: with standard ambiguous NLP grammars, huge
shared parse forest (over 1010 trees) are often gener-
ated in a few dozens of milliseconds.
Within MICA, the first module that is applied on
top of the shared parse forest is SYNTAX’s n-best
module. This module adapts and implements the al-
gorithm of (Huang and Chiang, 2005) for efficient
n-best trees extraction from a shared parse forest. In
practice, and within the current version of MICA,
this module is usually used with n = 1, which iden-
tifies the optimal tree w.r.t. the probabilistic model
embedded in the original PCFG; other values can
also be used. Once the n-best trees have been ex-
tracted, the dependency extractor module transforms
each of these trees into a dependency tree, by ex-
ploiting the fact that the CFG used for parsing has
been built from a TIG.
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="conclusions">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999518071428571">
We compare MICA to the MALT parser. Both
parsers are trained on sections 02-21 of our de-
pendency version of the WSJ PennTreebank, and
tested on Section 00, not counting true punctuation.
“Predicted” refers to tags (PTB-tagset POS and su-
pertags) predicted by our taggers; “Gold” refers to
the gold POS and supertags. We tested MALT using
only POS tags (MALT-POS), and POS tags as well
as 1-best supertags (MALT-all). We provide unla-
beled (“Un”) and labeled (“Lb”) dependency accu-
racy (%). As we can see, the predicted supertags do
not help MALT. MALT is significantly slower than
MICA, running at about 30 words a second (MICA:
450 words a second).
</bodyText>
<table confidence="0.9571955">
Pred MICA MALT -POS MALT -all
Gold Pred Gold Pred Gold
Lb 85.8 97.3 86.9 87.4 86.8 96.9
Un 87.6 97.6 88.9 89.3 88.5 97.2
</table>
<sectionHeader confidence="0.962349" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996267115384615">
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237–266.
Srinivas Bangalore, Patrick Haffner, and Ga¨el Emami.
2005. Factoring global inference by enriching local rep-
resentations. Technical report, AT&amp;T Labs – Reserach.
Pierre Boullier and Philippe Deschamp.
1988. Le syst`eme SYNTAXTM – manuel
d’utilisation et de mise en œuvre sous UNIXTM.
http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.
Pierre Boullier and Benoit Sagot. 2007. Are very large
grammars computationnaly tractable? In Proceedings of
IWPT’07, Prague, Czech Republic.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of the 7th International Workshop on =20 Pars-
ing Technologies, pages 43–54, Nancy, France.
John Chen. 2001. Towards Efficient Statistical Parsing
Using Lexicalized Grammatical Information. Ph.D. the-
sis, University of Delaware.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In ACL’04.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communication of the ACM, 13(2):94–102.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT’05, Vancouver, Canada.
Rebecca Hwa. 2001. Learning Probabilistic Lexicalized
Grammars for Natural Language Processing. Ph.D. the-
sis, Harvard University.
Aravind K. Joshi. 1987. An introduction to Tree Ad-
joining Grammars. In A. Manaster-Ramer, editor, Math-
ematics of Language. John Benjamins, Amsterdam.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: Correct-
ing the surface dependency approximation. In ACL’04.
Dekang Lin. 1994. PRINCIPAR—an efficient, broad-
coverage, principle-based parser. In Coling’94.
Alexis Nasr and Owen Rambow. 2006. Parsing with
lexicalized probabilistic recursive transition networks. In
Finite-State Methods and Natural Language Processing,
Springer Verlag Lecture Notes in Commputer Science.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In CoNLL-2004.
Yves Schabes and Stuart Shieber. 1994. An alternative
conception of tree-adjoining derivation. Computational
Linguistics, 1(20):91–124.
Yves Schabes and Richard C. Waters. 1995. Tree Inser-
tion Grammar. Computational Linguistics, 21(4).
Libin Shen and Aravind Joshi. 2005. Incremental ltag
parsing. In HLT-EMNLP’05.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proceed-
ings of the ACL Workshop on Incremental Parsing.
</reference>
<page confidence="0.997463">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.041945">
<title confidence="0.997047">MICA: A Probabilistic Dependency Based on Tree Insertion Grammars Application Note</title>
<author confidence="0.987595">Srinivas Bangalore Pierre Boulllier</author>
<affiliation confidence="0.983048">AT&amp;T Labs – Research INRIA</affiliation>
<address confidence="0.861466">Florham Park, NJ, USA Rocquencourt, France</address>
<email confidence="0.74297">srini@research.att.comPierre.Boullier@inria.fr</email>
<author confidence="0.997659">Alexis Nasr Owen Rambow Benoit Sagot</author>
<affiliation confidence="0.997919">Aix-Marseille Universit´e CCLS, Columbia Univserity INRIA</affiliation>
<address confidence="0.986832">Marseille, France New York, NY, USA Rocquencourt, France</address>
<email confidence="0.994964">alexis.nasr@lif.univ-mrs.frrambow@ccls.columbia.edubenoit.sagot@inria.fr</email>
<abstract confidence="0.992702147260274">MICA is a dependency parser which returns deep dependency representations, is fast, has state-of-the-art performance, and is freely available. 1 Overview This application note presents a freely available parser, MICA (Marseille-INRIA-Columbia- MICA has several key characteristics that make it appealing to researchers in NLP who need an off-the-shelf parser. • MICA returns a deep dependency parse, in which dependency is defined in terms of lexical predicate-argument structure, not in terms of surface-syntactic features such as subject-verb agreement. Function words such as auxiliaries and determiners depend on their lexical head, and governed prepositions (such as are treated as co-heads rather than as syntactic heads their own right. For example, is giving books Mary the following analysis (the arc label is on the terminal). giving John is books to Mary arc=0 arc=adj arc=1 arc=co-head arc=2 arc labels for the three arguments and not change when the sentence passivized or dative shift. would like to thank Ryan Roth for contributing the MALT data. • MICA is based on an explicit phrase-structure tree grammar extracted from the Penn Treebank. Therefore, MICA can associate dependency parses with rich linguistic information such as voice, the of empty subjects (PRO), and whether a verb heads a relative clause. • MICA is fast (450 words per second plus 6 seconds initialization on a standard high-end machine on sentences with fewer than 200 words) and has state-of-the-art performance (87.6% unlabeled dependency accuracy, see Section 5). • MICA consists of two processes: the supertagger, which associates tags representing rich syntactic information with the input word sequence, and the actual parser, which derives the syntactic strucfrom the chosen supertags. Only the supertagger uses lexical information, the parser only sees the supertag hypotheses. MICA returns n-best parses for arbitrary parse trees are associated with probabilities. A packed forest can also be returned. MICA is freely easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 185 of NAACL HLT 2009: Short pages Colorado, June 2009. Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MICA’s, so that direct comparison is difficult. For comparison purposes, we therefore use the MALT parser generator (Nivre et al., 2004), which allows us to train a dependency parser on our own dependency structures. MALT has been among the top performers in the CoNLL dependency parsing competitions. 2 Supertags and Supertagging Supertags are elementary trees of a lexicalized tree grammar such as a Tree-Adjoining Grammar (TAG) (Joshi, 1987). Unlike context-free grammar rules which are single level trees, supertags are multi-level trees which encapsulate both predicateargument structure of the anchor lexeme (by including nodes at which its arguments must substitute) and morpho-syntactic constraints such as subjectverb agreement within the supertag associated with the anchor. There are a number of supertags for each lexeme to account for the different syntactic trans- (relative clause, passivizaetc.). For example, the verb be associated with at least these two trees, which we will call tdi and tdi-dat. (There are also many other trees.) tdi tdi-dat to the task of disambiguating among the set of supertags associated with each word in a sentence, given the context of the sentence. In order to arrive at a complete parse, the only step remaining after supertagging is establishing the attachments among the supertags. Hence the result of supertagging is termed as an “almost parse” (Bangalore and Joshi, 1999). The set of supertags is derived from the Penn Treebank using the approach of Chen (2001). This extraction procedure results in a supertag set of 4,727 supertags and about one million words of supertag annotated corpus. We use 950,028 annotated words for training (Sections 02-21) and 46,451 (Section 00) annotated words for testing in our experiments. We estimate the probability of a tag sequence directly as in discriminative classification approaches. In such approaches, the context of the word being supertagged is encoded as features for the classifier. Given the large scale multiclass labeling nature of the supertagging task, we train supertagging models as one-vs-rest binary classification problems. Detailed supertagging experiment results are reported in (Bangalore et al., 2005) which we summarize here. We use the lexical, part-ofspeech attributes from the left and right context a window and the lexical, orthographic (e.g. capitalization, prefix, suffix, digit) and partof-speech attributes of the word being supertagged. Crucially, this set does not use the supertags for the words in the history. Thus during decoding the supertag assignment is done locally and does not need a dynamic programming search. We trained a Maxent model with such features using the labeled data set mentioned above and achieve an error rate of 11.48% on the test set. 3 Grammars and Models MICA grammars are extracted in a three steps process. In a first step, a Tree Insertion Grammar (TIG) (Schabes and Waters, 1995) is extracted from the treebank, along with a table of counts. This is the grammar that is used for supertagging, as described Section 2. In a second step, the TIG and the count table are used to build a PCFG. During the last step, the PCFG is “specialized” in order to model more finely some lexico-syntactic phenomena. The second and third steps are discussed in this section. The extracted TIG is transformed into a PCFG which generates strings of supertags as follows. Initial elementary trees (which are substituted) yield rules whose left hand side is the root category of the elementary tree. Left (respectively right) auxiliary trees (the trees for which the foot node is the S S P 186 left (resp. right) daughter of the root) give birth to whose left-hand side is of the form where the root category of the elementary tree. The right hand side of each rule is built during a top down traversal of the corresponding elementary tree. For every node of the tree visited, a new symbol is added to the right hand side of rule, from left to right, as follows: • The anchor of the elementary tree adds the supertag (i.e., the name of the tree), which is a terminal symbol, to the context-free rule. • A substitution node in the elementary tree adds its nonterminal symbol to the context-free rule. • A interior node in the elementary tree at which adjunction may occur adds to the context-free rule nonterminal symbol or where the node’s nonterminal symbol, and l (resp. r) indicates whether it is a left (resp. right) adjunction. Each interior node is visited twice, the first time from the left, and then from the right. A set of non-lexicalized rules (i.e., rules that do not generate a terminal symbol) allow us to generate zero or more trees anchored the symbol No adjunction, the first adjunction, and the second adjunction are modeled explicitly in the grammar and the associated probabilistic model, while the third and all subsequent adjunctions are modeled together. This conversion method is basically the same as that presented in (Schabes and Waters, 1995), except that our PCFG models multiple adjunctions at the same node by positions (a concern Schabes and Waters (1995) do not share, of course). Our PCFG construction differs from that of Hwa (2001) in that she does not allow multiple adjunction at one node (Schabes and Shieber, 1994) (which we do since we are interested in the derivation structure as a representation of linguistic dependency). For more information about the positional model of adjunction and a discussion of an alternate model, the “bigram model”, see (Nasr and Rambow, 2006). Tree tdi from Section 2 gives rise to the following rule (where tdi and tCO are terminal symbols and rest are nonterminals): S NP tdi NP ltCO NP rr The probabilities of the PCFG rules are estimated using maximum likelihood. The probabilistic model refers only to supertag names, not to words. In the basic model, the probability of the adjunction or substitution of an elementary tree (the daughter) in another elementary tree (the mother) only depends on the nonterminal, and does not depend on the mother nor on the node on which the attachment is performed in the mother elementary tree. It is well known that such a dependency is important for an adequate probabilistic modelling of syntax. In order to introduce such a dependency, we condition an attachment on the mother and on the node on which the attachment is performed, an operation that we call mother specialization. Mother specialization is performed by adding to all nonterminals the name of the mother and the address of a node. The specialization of a grammar increase vastly the number of symbols and rules and provoke severe data sparseness problems, this is why only a subset of the symbols are specialized. 4 Parser and Deschamp, 1988) is a system used to generate lexical and syntactic analyzers (parsers) (both deterministic and non-deterministic) for all kind of context-free grammars (CFGs) as well as some classes of contextual grammars. It has been under development at INRIA for several most classes of deterministic (unambiguous) grammars (LR, LALR, RLR) as well as general context-free grammars. The non-deterministic features include, among others, an Earley-like parser generator used for natural language processing (Boullier, 2003). most parsers, the architecture of MICA’s PCFG-based parser is the following: • The Earley-like parser proper computes a shared parse forest that represents in a factorized (polynoway parse trees according to the underlying (non-probabilistic) CFG that represents the TIG; • Filtering and/or decoration modules are applied the shared parse forest; in MICA’s case, an best module is applied, followed by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, several optimizations have been applied, including guiding techniques (Boullier, 2003), extensive static (offline) 187 computations over the grammar, and efficient data structures. Moreover, Earley’s algorithm has been extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge parse forest (over trees) are often generated in a few dozens of milliseconds. Within MICA, the first module that is applied on of the shared parse forest is module. This module adapts and implements the algorithm of (Huang and Chiang, 2005) for efficient trees extraction from a shared parse forest. In practice, and within the current version of MICA, module is usually used with which identifies the optimal tree w.r.t. the probabilistic model embedded in the original PCFG; other values can be used. Once the trees have been extracted, the dependency extractor module transforms each of these trees into a dependency tree, by exploiting the fact that the CFG used for parsing has been built from a TIG. 5 Evaluation We compare MICA to the MALT parser. Both parsers are trained on sections 02-21 of our dependency version of the WSJ PennTreebank, and tested on Section 00, not counting true punctuation. “Predicted” refers to tags (PTB-tagset POS and supertags) predicted by our taggers; “Gold” refers to the gold POS and supertags. We tested MALT using only POS tags (MALT-POS), and POS tags as well as 1-best supertags (MALT-all). We provide unlabeled (“Un”) and labeled (“Lb”) dependency accuracy (%). As we can see, the predicted supertags do not help MALT. MALT is significantly slower than MICA, running at about 30 words a second (MICA: 450 words a second).</abstract>
<note confidence="0.836742696969697">Pred MICA MALTPred -POS Gold MALTPred Gold Gold Lb 85.8 97.3 86.9 87.4 86.8 96.9 Un 87.6 97.6 88.9 89.3 88.5 97.2 References Srinivas Bangalore and Aravind Joshi. 1999. Supertag- An approach to almost parsing. 25(2):237–266. Srinivas Bangalore, Patrick Haffner, and Ga¨el Emami. 2005. Factoring global inference by enriching local representations. Technical report, AT&amp;T Labs – Reserach. Pierre Boullier and Philippe Deschamp. Le syst`eme – et de mise en œuvre sous http://syntax.gforge.inria.fr/syntax3.8-manual.pdf. Pierre Boullier and Benoit Sagot. 2007. Are very large computationnaly tractable? In of Prague, Czech Republic. Boullier. 2003. Guided Earley parsing. In Proceedings of the 7th International Workshop on =20 Parspages 43–54, Nancy, France. Chen. 2001. Efficient Statistical Parsing Lexicalized Grammatical Ph.D. thesis, University of Delaware. Stephen Clark and James R. Curran. 2004. Parsing the using CCG and log-linear models. In Jay Earley. 1970. An efficient context-free parsing algoof the 13(2):94–102. Liang Huang and David Chiang. 2005. Better k-best In of Vancouver, Canada. Hwa. 2001. Probabilistic Lexicalized for Natural Language Ph.D. thesis, Harvard University.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="3109" citStr="Bangalore and Joshi (1999)" startWordPosition="469" endWordPosition="472">CA returns n-best parses for arbitrary n; parse trees are associated with probabilities. A packed forest can also be returned. • MICA is freely available2, easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2http://www1.ccls.columbia.edu/˜rambow/mica.html 185 Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate depende</context>
<context position="5240" citStr="Bangalore and Joshi, 1999" startWordPosition="791" endWordPosition="795">erent syntactic transformations (relative clause, wh-question, passivization etc.). For example, the verb give will be associated with at least these two trees, which we will call tdi and tdi-dat. (There are also many other trees.) tdi tdi-dat NP2 ↓ to Supertagging is the task of disambiguating among the set of supertags associated with each word in a sentence, given the context of the sentence. In order to arrive at a complete parse, the only step remaining after supertagging is establishing the attachments among the supertags. Hence the result of supertagging is termed as an “almost parse” (Bangalore and Joshi, 1999). The set of supertags is derived from the Penn Treebank using the approach of Chen (2001). This extraction procedure results in a supertag set of 4,727 supertags and about one million words of supertag annotated corpus. We use 950,028 annotated words for training (Sections 02-21) and 46,451 (Section 00) annotated words for testing in our experiments. We estimate the probability of a tag sequence directly as in discriminative classification approaches. In such approaches, the context of the word being supertagged is encoded as features for the classifier. Given the large scale multiclass label</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Patrick Haffner</author>
<author>Ga¨el Emami</author>
</authors>
<title>Factoring global inference by enriching local representations.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>AT&amp;T Labs – Reserach. Pierre Boullier and Philippe Deschamp.</institution>
<marker>Bangalore, Haffner, Ga¨el Emami, 2005</marker>
<rawString>Srinivas Bangalore, Patrick Haffner, and Ga¨el Emami. 2005. Factoring global inference by enriching local representations. Technical report, AT&amp;T Labs – Reserach. Pierre Boullier and Philippe Deschamp.</rawString>
</citation>
<citation valid="true">
<title>Le syst`eme SYNTAXTM – manuel d’utilisation et de mise en œuvre sous UNIXTM.</title>
<date>1988</date>
<note>http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.</note>
<marker>1988</marker>
<rawString>1988. Le syst`eme SYNTAXTM – manuel d’utilisation et de mise en œuvre sous UNIXTM. http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
<author>Benoit Sagot</author>
</authors>
<title>Are very large grammars computationnaly tractable?</title>
<date>2007</date>
<booktitle>In Proceedings of IWPT’07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12379" citStr="Boullier and Sagot, 2007" startWordPosition="2002" endWordPosition="2005"> a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, several optimizations have been applied, including guiding techniques (Boullier, 2003), extensive static (offline) 187 computations over the grammar, and efficient data structures. Moreover, Earley’s algorithm has been extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 1010 trees) are often generated in a few dozens of milliseconds. Within MICA, the first module that is applied on top of the shared parse forest is SYNTAX’s n-best module. This module adapts and implements the algorithm of (Huang and Chiang, 2005) for efficient n-best trees extraction from a shared parse forest. In practice, and within the current version of MICA, this module is usually used with n = 1, which identifies the optimal tree w.r.t. the probabilistic model embedded in the o</context>
</contexts>
<marker>Boullier, Sagot, 2007</marker>
<rawString>Pierre Boullier and Benoit Sagot. 2007. Are very large grammars computationnaly tractable? In Proceedings of IWPT’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
</authors>
<title>Guided Earley parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th International Workshop on =20 Parsing Technologies,</booktitle>
<pages>43--54</pages>
<location>Nancy, France.</location>
<contexts>
<context position="11304" citStr="Boullier, 2003" startWordPosition="1839" endWordPosition="1840">bset of the symbols are specialized. 4 Parser SYNTAX (Boullier and Deschamp, 1988) is a system used to generate lexical and syntactic analyzers (parsers) (both deterministic and non-deterministic) for all kind of context-free grammars (CFGs) as well as some classes of contextual grammars. It has been under development at INRIA for several decades. SYNTAX handles most classes of deterministic (unambiguous) grammars (LR, LALR, RLR) as well as general context-free grammars. The non-deterministic features include, among others, an Earley-like parser generator used for natural language processing (Boullier, 2003). Like most SYNTAX Earley-like parsers, the architecture of MICA’s PCFG-based parser is the following: • The Earley-like parser proper computes a shared parse forest that represents in a factorized (polynomial) way all possible parse trees according to the underlying (non-probabilistic) CFG that represents the TIG; • Filtering and/or decoration modules are applied on the shared parse forest; in MICA’s case, an nbest module is applied, followed by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, seve</context>
</contexts>
<marker>Boullier, 2003</marker>
<rawString>Pierre Boullier. 2003. Guided Earley parsing. In Proceedings of the 7th International Workshop on =20 Parsing Technologies, pages 43–54, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
</authors>
<title>Towards Efficient Statistical Parsing Using Lexicalized Grammatical Information.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Delaware.</institution>
<contexts>
<context position="5330" citStr="Chen (2001)" startWordPosition="810" endWordPosition="811"> give will be associated with at least these two trees, which we will call tdi and tdi-dat. (There are also many other trees.) tdi tdi-dat NP2 ↓ to Supertagging is the task of disambiguating among the set of supertags associated with each word in a sentence, given the context of the sentence. In order to arrive at a complete parse, the only step remaining after supertagging is establishing the attachments among the supertags. Hence the result of supertagging is termed as an “almost parse” (Bangalore and Joshi, 1999). The set of supertags is derived from the Penn Treebank using the approach of Chen (2001). This extraction procedure results in a supertag set of 4,727 supertags and about one million words of supertag annotated corpus. We use 950,028 annotated words for training (Sections 02-21) and 46,451 (Section 00) annotated words for testing in our experiments. We estimate the probability of a tag sequence directly as in discriminative classification approaches. In such approaches, the context of the word being supertagged is encoded as features for the classifier. Given the large scale multiclass labeling nature of the supertagging task, we train supertagging models as one-vs-rest binary cl</context>
</contexts>
<marker>Chen, 2001</marker>
<rawString>John Chen. 2001. Towards Efficient Statistical Parsing Using Lexicalized Grammatical Information. Ph.D. thesis, University of Delaware.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models. In ACL’04. Jay Earley.</title>
<date>2004</date>
<journal>Communication of the ACM,</journal>
<booktitle>In Proceedings of IWPT’05,</booktitle>
<tech>Ph.D. thesis,</tech>
<volume>13</volume>
<issue>2</issue>
<institution>Harvard University.</institution>
<location>Vancouver, Canada. Rebecca Hwa.</location>
<contexts>
<context position="3158" citStr="Clark and Curran (2004)" startWordPosition="477" endWordPosition="480">s are associated with probabilities. A packed forest can also be returned. • MICA is freely available2, easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2http://www1.ccls.columbia.edu/˜rambow/mica.html 185 Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from </context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In ACL’04. Jay Earley. 1970. An efficient context-free parsing algorithm. Communication of the ACM, 13(2):94–102. Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of IWPT’05, Vancouver, Canada. Rebecca Hwa. 2001. Learning Probabilistic Lexicalized Grammars for Natural Language Processing. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An introduction to Tree Adjoining Grammars. In</title>
<date>1987</date>
<booktitle>In ACL’04. Dekang Lin.</booktitle>
<editor>A. Manaster-Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam. Roger Levy and Christopher</editor>
<contexts>
<context position="4202" citStr="Joshi, 1987" startWordPosition="627" endWordPosition="628">nning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MICA’s, so that direct comparison is difficult. For comparison purposes, we therefore use the MALT parser generator (Nivre et al., 2004), which allows us to train a dependency parser on our own dependency structures. MALT has been among the top performers in the CoNLL dependency parsing competitions. 2 Supertags and Supertagging Supertags are elementary trees of a lexicalized tree grammar such as a Tree-Adjoining Grammar (TAG) (Joshi, 1987). Unlike context-free grammar rules which are single level trees, supertags are multi-level trees which encapsulate both predicateargument structure of the anchor lexeme (by including nodes at which its arguments must substitute) and morpho-syntactic constraints such as subjectverb agreement within the supertag associated with the anchor. There are a number of supertags for each lexeme to account for the different syntactic transformations (relative clause, wh-question, passivization etc.). For example, the verb give will be associated with at least these two trees, which we will call tdi and </context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Aravind K. Joshi. 1987. An introduction to Tree Adjoining Grammars. In A. Manaster-Ramer, editor, Mathematics of Language. John Benjamins, Amsterdam. Roger Levy and Christopher Manning. 2004. Deep dependencies from context-free statistical parsers: Correcting the surface dependency approximation. In ACL’04. Dekang Lin. 1994. PRINCIPAR—an efficient, broadcoverage, principle-based parser. In Coling’94.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Parsing with lexicalized probabilistic recursive transition networks.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<booktitle>In Finite-State Methods and Natural Language Processing, Springer Verlag Lecture Notes in Commputer Science. Joakim Nivre,</booktitle>
<volume>1</volume>
<issue>20</issue>
<location>Johan Hall, and</location>
<contexts>
<context position="9497" citStr="Nasr and Rambow, 2006" startWordPosition="1533" endWordPosition="1536">method is basically the same as that presented in (Schabes and Waters, 1995), except that our PCFG models multiple adjunctions at the same node by positions (a concern Schabes and Waters (1995) do not share, of course). Our PCFG construction differs from that of Hwa (2001) in that she does not allow multiple adjunction at one node (Schabes and Shieber, 1994) (which we do since we are interested in the derivation structure as a representation of linguistic dependency). For more information about the positional model of adjunction and a discussion of an alternate model, the “bigram model”, see (Nasr and Rambow, 2006). Tree tdi from Section 2 gives rise to the following rule (where tdi and tCO are terminal symbols and the rest are nonterminals): S → S∗l NP VP∗l V∗l tdi V∗r NP PP∗ l P∗l tCO P∗r NP PP∗ r VP∗ r S∗r The probabilities of the PCFG rules are estimated using maximum likelihood. The probabilistic model refers only to supertag names, not to words. In the basic model, the probability of the adjunction or substitution of an elementary tree (the daughter) in another elementary tree (the mother) only depends on the nonterminal, and does not depend on the mother nor on the node on which the attachment is</context>
</contexts>
<marker>Nasr, Rambow, 2006</marker>
<rawString>Alexis Nasr and Owen Rambow. 2006. Parsing with lexicalized probabilistic recursive transition networks. In Finite-State Methods and Natural Language Processing, Springer Verlag Lecture Notes in Commputer Science. Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In CoNLL-2004. Yves Schabes and Stuart Shieber. 1994. An alternative conception of tree-adjoining derivation. Computational Linguistics, 1(20):91–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Incremental ltag parsing.</title>
<date>1995</date>
<booktitle>Tree Insertion Grammar. Computational Linguistics, 21(4). Libin Shen and Aravind</booktitle>
<contexts>
<context position="6768" citStr="Schabes and Waters, 1995" startWordPosition="1043" endWordPosition="1046">-word window and the lexical, orthographic (e.g. capitalization, prefix, suffix, digit) and partof-speech attributes of the word being supertagged. Crucially, this set does not use the supertags for the words in the history. Thus during decoding the supertag assignment is done locally and does not need a dynamic programming search. We trained a Maxent model with such features using the labeled data set mentioned above and achieve an error rate of 11.48% on the test set. 3 Grammars and Models MICA grammars are extracted in a three steps process. In a first step, a Tree Insertion Grammar (TIG) (Schabes and Waters, 1995) is extracted from the treebank, along with a table of counts. This is the grammar that is used for supertagging, as described ↓in Section 2. In a second step, the TIG and the count table are used to build a PCFG. During the last step, the PCFG is “specialized” in order to model more finely some lexico-syntactic phenomena. The second and third steps are discussed in this section. The extracted TIG is transformed into a PCFG which generates strings of supertags as follows. Initial elementary trees (which are substituted) yield rules whose left hand side is the root category of the elementary tr</context>
<context position="8951" citStr="Schabes and Waters, 1995" startWordPosition="1441" endWordPosition="1444"> (resp. r) indicates whether it is a left (resp. right) adjunction. Each interior node is visited twice, the first time from the left, and then from the right. A set of non-lexicalized rules (i.e., rules that do not generate a terminal symbol) allow us to generate zero or more trees anchored by Xl from the symbol X ∗ l . No adjunction, the first adjunction, and the second adjunction are modeled explicitly in the grammar and the associated probabilistic model, while the third and all subsequent adjunctions are modeled together. This conversion method is basically the same as that presented in (Schabes and Waters, 1995), except that our PCFG models multiple adjunctions at the same node by positions (a concern Schabes and Waters (1995) do not share, of course). Our PCFG construction differs from that of Hwa (2001) in that she does not allow multiple adjunction at one node (Schabes and Shieber, 1994) (which we do since we are interested in the derivation structure as a representation of linguistic dependency). For more information about the positional model of adjunction and a discussion of an alternate model, the “bigram model”, see (Nasr and Rambow, 2006). Tree tdi from Section 2 gives rise to the following </context>
</contexts>
<marker>Schabes, Waters, 1995</marker>
<rawString>Yves Schabes and Richard C. Waters. 1995. Tree Insertion Grammar. Computational Linguistics, 21(4). Libin Shen and Aravind Joshi. 2005. Incremental ltag parsing. In HLT-EMNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary P Harper</author>
</authors>
<title>A statistical constraint dependency grammar (CDG) parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Workshop on Incremental Parsing.</booktitle>
<marker>Wang, Harper, 2004</marker>
<rawString>Wen Wang and Mary P. Harper. 2004. A statistical constraint dependency grammar (CDG) parser. In Proceedings of the ACL Workshop on Incremental Parsing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>