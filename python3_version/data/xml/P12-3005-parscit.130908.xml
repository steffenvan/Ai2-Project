<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.094776">
<title confidence="0.973052">
langid.py: An Off-the-shelf Language Identification Tool
</title>
<author confidence="0.934365">
Marco Lui and Timothy Baldwin
</author>
<affiliation confidence="0.930310666666667">
NICTA VRL
Department of Computing and Information Systems
University of Melbourne, VIC 3010, Australia
</affiliation>
<email confidence="0.99776">
mhlui@unimelb.edu.au, tb@ldwin.net
</email>
<sectionHeader confidence="0.993875" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999140363636364">
We present langid.py, an off-the-shelf lan-
guage identification tool. We discuss the de-
sign and implementation of langid.py, and
provide an empirical comparison on 5 long-
document datasets, and 2 datasets from the mi-
croblog domain. We find that langid.py
maintains consistently high accuracy across
all domains, making it ideal for end-users that
require language identification without want-
ing to invest in preparation of in-domain train-
ing data.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961695652174">
Language identification (LangID) is the task of de-
termining the natural language that a document is
written in. It is a key step in automatic processing
of real-world data, where a multitude of languages
may be present. Natural language processing tech-
niques typically pre-suppose that all documents be-
ing processed are written in a given language (e.g.
English), but as focus shifts onto processing docu-
ments from internet sources such as microblogging
services, this becomes increasingly difficult to guar-
antee. Language identification is also a key compo-
nent of many web services. For example, the lan-
guage that a web page is written in is an important
consideration in determining whether it is likely to
be of interest to a particular user of a search engine,
and automatic identification is an essential step in
building language corpora from the web. It has prac-
tical implications for social networking and social
media, where it may be desirable to organize com-
ments and other user-generated content by language.
It also has implications for accessibility, since it en-
ables automatic determination of the target language
for automatic machine translation purposes.
</bodyText>
<page confidence="0.968944">
25
</page>
<bodyText confidence="0.984470133333333">
Many applications could potentially benefit from
automatic language identification, but building a
customized solution per-application is prohibitively
expensive, especially if human annotation is re-
quired to produce a corpus of language-labelled
training documents from the application domain.
What is required is thus a generic language identi-
fication tool that is usable off-the-shelf, i.e. with no
end-user training and minimal configuration.
In this paper, we present langid.py, a LangID
tool with the following characteristics: (1) fast,
(2) usable off-the-shelf, (3) unaffected by domain-
specific features (e.g. HTML, XML, markdown),
(4) single file with minimal dependencies, and (5)
flexible interface
</bodyText>
<sectionHeader confidence="0.996688" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.994708285714286">
langid.py is trained over a naive Bayes clas-
sifier with a multinomial event model (McCallum
and Nigam, 1998), over a mixture of byte n-grams
(1&lt;n&lt;4). One key difference from conventional
text categorization solutions is that langid.py
was designed to be used off-the-shelf. Since
langid.py implements a supervised classifier,
this presents two primary challenges: (1) a pre-
trained model must be distributed with the classi-
fier, and (2) the model must generalize to data from
different domains, meaning that in its default con-
figuration, it must have good accuracy over inputs
as diverse as web pages, newspaper articles and mi-
croblog messages. (1) is mostly a practical consid-
eration, and so we will address it in Section 3. In
order to address (2), we integrate information about
the language identification task from a variety of do-
mains by using LD feature selection (Lui and Bald-
win, 2011).
Lui and Baldwin (2011) showed that it is rela-
tively easy to attain high accuracy for language iden-
</bodyText>
<note confidence="0.79864">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 25–30,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<table confidence="0.999279375">
Dataset Documents Langs Doc Length (bytes)
EUROGOV 1500 10 1.7×104 ±3.9×104
TCL 3174 60 2.6×103 ±3.8×103
WIKIPEDIA 4963 67 1.5×103 ±4.1×103
EMEA 19988 22 2.9×105 ±7.9×105
EUROPARL 20828 22 1.7×102 ±1.6×102
T-BE 9659 6 1.0×102 ±3.2×101
T-SC 5000 5 8.8×101 ±3.9×101
</table>
<tableCaption confidence="0.999932">
Table 1: Summary of the LangID datasets
</tableCaption>
<bodyText confidence="0.999943458333333">
tification in a traditional text categorization setting,
where we have in-domain training data. The task be-
comes much harder when trying to perform domain
adaptation, that is, trying to use model parameters
learned in one domain to classify data from a dif-
ferent domain. GD feature selection addresses this
problem by focusing on key features that are relevant
to the language identification task. It is based on In-
formation Gain (IG), originally introduced as a split-
ting criteria for decision trees (Quinlan, 1986), and
later shown to be effective for feature selection in
text categorization (Yang and Pedersen, 1997; For-
man, 2003). GD represents the difference in IG with
respect to language and domain. Features with a
high GD score are informative about language with-
out being informative about domain. For practi-
cal reasons, before the IG calculation the candidate
feature set is pruned by means of a term-frequency
based feature selection.
Lui and Baldwin (2011) presented empirical evi-
dence that GD feature selection was effective for do-
main adaptation in language identification. This re-
sult is further supported by our evaluation, presented
in Section 5.
</bodyText>
<sectionHeader confidence="0.966788" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.999757789473685">
The full langid.py package consists of the
language identifier langid.py, as well as two
support modules LDfeatureselect.py and
train.py.
langid.py is the single file which packages the
language identification tool, and the only file needed
to use langid.py for off-the-shelf language iden-
tification. It comes with an embedded model which
covers 97 languages using training data drawn from
5 domains. Tokenization and feature selection are
carried out in a single pass over the input document
via Aho-Corasick string matching (Aho and Cora-
sick, 1975). The Aho-Corasick string matching al-
gorithm processes an input by means of a determin-
istic finite automaton (DFA). Some states of the au-
tomaton are associated with the completion of one
of the n-grams selected through GD feature selec-
tion. Thus, we can obtain our document represen-
tation by simply counting the number of times the
DFA enters particular states while processing our in-
put. The DFA and the associated mapping from state
to n-gram are constructed during the training phase,
and embedded as part of the pre-trained model.
The naive Bayes classifier is implemented using
numpy,1 the de-facto numerical computation pack-
age for Python. numpy is free and open source, and
available for all major platforms. Using numpy in-
troduces a dependency on a library that is not in the
Python standard library. This is a reasonable trade-
off, as numpy provides us with an optimized im-
plementation of matrix operations, which allows us
to implement fast naive Bayes classification while
maintaining the single-file concept of langid.py.
langid.py can be used in the three ways:
Command-line tool: langid.py supports an
interactive mode with a text prompt and line-by-line
classification. This mode is suitable for quick in-
teractive queries, as well as for demonstration pur-
poses. langid.py also supports language identi-
fication of entire files via redirection. This allows a
user to interactively explore data, as well as to inte-
grate language identification into a pipeline of other
unix-style tools. However, use via redirection is
not recommended for large quantities of documents
as each invocation requires the trained model to be
unpacked into memory. Where large quantities of
documents are being processed, use as a library or
web service is preferred as the model will only be
unpacked once upon initialization.
Python library: langid.py can be imported as
a Python module, and provides a function that ac-
cepts text and returns the identified language of the
text. This use of langid.py is the fastest in a
single-processor setting as it incurs the least over-
head.
Web service: langid.py can be started as a
web service with a command-line switch. This
</bodyText>
<footnote confidence="0.996763">
1http://numpy.scipy.org
</footnote>
<page confidence="0.996951">
26
</page>
<bodyText confidence="0.999961121212121">
allows language identitication by means of HTTP
PUT and HTTP POST requests, which return JSON-
encoded responses. This is the preferred method of
using langid.py from other programming envi-
ronments, as most languages include libraries for in-
teracting with web services over HTTP. It also al-
lows the language identification service to be run as
a network/internet service. Finally, langid.py is
WSGI-compliant,2 so it can be deployed in a WSGI-
compliant web server. This provides an easy way to
achieve parallelism by leveraging existing technolo-
gies to manage load balancing and utilize multiple
processors in the handling of multiple concurrent re-
quests for a service.
LDfeatureselect.py implements the GD
feature selection. The calculation of term frequency
is done in constant memory by index inversion
through a MapReduce-style sharding approach. The
calculation of information gain is also chunked to
limit peak memory use, and furthermore it is paral-
lelized to make full use of modern multiprocessor
systems. LDfeatureselect.py produces a list
of byte n-grams ranked by their GD score.
train.py implements estimation of parameters
for the multinomial naive Bayes model, as well as
the construction of the DFA for the Aho-Corasick
string matching algorithm. Its input is a list of byte
patterns representing a feature set (such as that se-
lected via LDfeatureselect.py), and a corpus
of training documents. It produces the final model as
a single compressed, encoded string, which can be
saved to an external file and used by langid.py
via a command-line option.
</bodyText>
<sectionHeader confidence="0.987396" genericHeader="method">
4 Training Data
</sectionHeader>
<bodyText confidence="0.999976">
langid.py is distributed with an embedded
model trained using the multi-domain language
identification corpus of Lui and Baldwin (2011).
This corpus contains documents in a total of 97 lan-
guages. The data is drawn from 5 different do-
mains: government documents, software documen-
tation, newswire, online encyclopedia and an inter-
net crawl, though no domain covers the full set of
languages by itself, and some languages are present
only in a single domain. More details about this cor-
pus are given in Lui and Baldwin (2011).
</bodyText>
<footnote confidence="0.928075">
2http://www.wsgi.org
</footnote>
<bodyText confidence="0.9999365">
We do not perform explicit encoding detection,
but we do not assume that all the data is in the same
encoding. Previous research has shown that explicit
encoding detection is not needed for language iden-
tification (Baldwin and Lui, 2010). Our training data
consists mostly of UTF8-encoded documents, but
some of our evaluation datasets contain a mixture
of encodings.
</bodyText>
<sectionHeader confidence="0.998519" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999205">
In order to benchmark langid.py, we carried out
an empirical evaluation using a number of language-
labelled datasets. We compare the empirical results
obtained from langid.py to those obtained from
other language identification toolkits which incor-
porate a pre-trained model, and are thus usable off-
the-shelf for language identification. These tools are
listed in Table 3.
</bodyText>
<subsectionHeader confidence="0.989499">
5.1 Off-the-shelf LangID tools
</subsectionHeader>
<bodyText confidence="0.99997948">
TextCat is an implementation of the method of
Cavnar and Trenkle (1994) by Gertjan van Noord.
It has traditionally been the de facto LangID tool of
choice in research, and is the basis of language iden-
tification/filtering in the ClueWeb09 Dataset (Callan
and Hoy, 2009) and CorpusBuilder (Ghani et al.,
2004). It includes support for training with user-
supplied data.
LangDetect implements a Naive Bayes classi-
fier, using a character n-gram based representation
without feature selection, with a set of normaliza-
tion heuristics to improve accuracy. It is trained on
data from Wikipedia,3 and can be trained with user-
supplied data.
CLD is a port of the embedded language identi-
fier in Google’s Chromium browser, maintained by
Mike McCandless. Not much is known about the
internal design of the tool, and there is no support
provided for re-training it.
The datasets come from a variety of domains,
such as newswire (TCL), biomedical corpora
(EMEA), government documents (EUROGOV, EU-
ROPARL) and microblog services (T-BE, T-SC). A
number of these datasets have been previously used
in language identification research. We provide a
</bodyText>
<footnote confidence="0.990537">
3http://www.wikipedia.org
</footnote>
<page confidence="0.970627">
27
</page>
<table confidence="0.999884333333333">
Test Dataset langid.py LangDetect TextCat DAcc CLD
Accuracy docs/s DAcc Slowdown DAcc Slowdown Slowdown
EUROGOV 0.987 70.5 +0.005 1.1x −0.046 31.1x −0.004 0.5x
TCL 0.904 185.4 −0.086 2.1x −0.299 24.2x −0.172 0.5x
WIKIPEDIA 0.913 227.6 −0.046 2.5x −0.207 99.9x −0.082 0.9x
EMEA 0.934 7.7 −0.820 0.2x −0.572 6.3x +0.044 0.3x
EUROPARL 0.992 294.3 +0.001 3.6x −0.186 115.4x −0.010 0.2x
T-BE 0.941 367.9 −0.016 4.4x −0.210 144.1x −0.081 0.7x
T-SC 0.886 298.2 −0.038 2.9x −0.235 34.2x −0.120 0.2x
</table>
<tableCaption confidence="0.778806">
Table 2: Comparison of standalone classification tools, in terms of accuracy and speed (documents/second), relative
to langid.py
Tool Languages URL
</tableCaption>
<footnote confidence="0.618033">
langid.py 97 http://www.csse.unimelb.edu.au/research/lt/resources/langid/
LangDetect 53 http://code.google.com/p/language-detection/
TextCat 75 http://odur.let.rug.nl/vannoord/TextCat/
CLD 64+ http://code.google.com/p/chromium-compact-language-detector/
</footnote>
<tableCaption confidence="0.997225">
Table 3: Summary of the LangID tools compared
</tableCaption>
<bodyText confidence="0.999810866666667">
brief summary of the characteristics of each dataset
in Table 1.
The datasets we use for evaluation are differ-
ent from and independent of the datasets from
which the embedded model of langid.py was
produced. In Table 2, we report the accuracy of
each tool, measured as the proportion of documents
from each dataset that are correctly classified. We
present the absolute accuracy and performance for
langid.py, and relative accuracy and slowdown
for the other systems. For this experiment, we used
a machine with 2 Intel Xeon E5540 processors and
24GB of RAM. We only utilized a single core, as
none of the language identification tools tested are
inherently multicore.
</bodyText>
<subsectionHeader confidence="0.999009">
5.2 Comparison on standard datasets
</subsectionHeader>
<bodyText confidence="0.998077418604651">
We compared the four systems on datasets used in
previous language identification research (Baldwin
and Lui, 2010) (EUROGOV, TCL, WIKIPEDIA), as
well as an extract from a biomedical parallel cor-
pus (Tiedemann, 2009) (EMEA) and a corpus of
samples from the Europarl Parallel Corpus (Koehn,
2005) (EUROPARL). The sample of EUROPARL
we use was originally prepared by Shuyo Nakatani
(author of LangDetect) as a validation set.
langid.py compares very favorably with other
language identification tools. It outperforms
TextCat in terms of speed and accuracy on all of
the datasets considered. langid.py is generally
orders of magnitude faster than TextCat, but this
advantage is reduced on larger documents. This is
primarily due to the design of TextCat, which re-
quires that the supplied models be read from file for
each document classified.
langid.py generally outperforms
LangDetect, except in datasets derived from
government documents (EUROGOV, EUROPARL).
However, the difference in accuracy between
langid.py and LangDetect on such datasets
is very small, and langid.py is generally faster.
An abnormal result was obtained when testing
LangDetect on the EMEA corpus. Here,
LangDetect is much faster, but has extremely
poor accuracy (0.114). Analysis of the results re-
veals that the majority of documents were classified
as Polish. We suspect that this is due to the early
termination criteria employed by LangDetect,
together with specific characteristics of the corpus.
TextCat also performed very poorly on this
corpus (accuracy 0.362). However, it is important
to note that langid.py and CLD both performed
very well, providing evidence that it is possible to
build a generic language identifier that is insensitive
to domain-specific characteristics.
langid.py also compares well with CLD. It is
generally more accurate, although CLD does bet-
ter on the EMEA corpus. This may reveal some
insight into the design of CLD, which is likely to
have been tuned for language identification of web
</bodyText>
<page confidence="0.996429">
28
</page>
<bodyText confidence="0.999949333333333">
pages. The EMEA corpus is heavy in XML markup,
which CLD and langid.py both successfully ig-
nore. One area where CLD outperforms all other sys-
tems is in its speed. However, this increase in speed
comes at the cost of decreased accuracy in other do-
mains, as we will see in Section 5.3.
</bodyText>
<subsectionHeader confidence="0.999307">
5.3 Comparison on microblog messages
</subsectionHeader>
<bodyText confidence="0.999993277777778">
The size of the input text is known to play a sig-
nificant role in the accuracy of automatic language
identification, with accuracy decreasing on shorter
input documents (Cavnar and Trenkle, 1994; Sibun
and Reynar, 1996; Baldwin and Lui, 2010).
Recently, language identification of short strings
has generated interest in the research community.
Hammarstrom (2007) described a method that aug-
mented a dictionary with an affix table, and tested it
over synthetic data derived from a parallel bible cor-
pus. Ceylan and Kim (2009) compared a number of
methods for identifying the language of search en-
gine queries of 2 to 3 words. They develop a method
which uses a decision tree to integrate outputs from
several different language identification approaches.
Vatanen et al. (2010) focus on messages of 5–21
characters, using n-gram language models over data
drawn from UDHR in a naive Bayes classifier.
A recent application where language identifica-
tion is an open issue is over the rapidly-increasing
volume of data being generated by social media.
Microblog services such as Twitter4 allow users to
post short text messages. Twitter has a worldwide
user base, evidenced by the large array of languages
present on Twitter (Carter et al., to appear). It is es-
timated that half the messages on Twitter are not in
English. 5
This new domain presents a significant challenge
for automatic language identification, due to the
much shorter ‘documents’ to be classified, and is
compounded by the lack of language-labelled in-
domain data for training and validation. This has led
to recent research focused specifically on the task of
language identification of Twitter messages. Carter
et al. (to appear) improve language identification in
Twitter messages by augmenting standard methods
</bodyText>
<footnote confidence="0.99355775">
4http://www.twitter.com
5http://semiocast.com/downloads/
Semiocast_Half_of_messages_on_Twitter_
are_not_in_English_20100224.pdf
</footnote>
<bodyText confidence="0.999978708333333">
with language identification priors based on a user’s
previous messages and by the content of links em-
bedded in messages. Tromp and Pechenizkiy (2011)
present a method for language identification of short
text messages by means of a graph structure.
Despite the recently published results on language
identification of microblog messages, there is no
dedicated off-the-shelf system to perform the task.
We thus examine the accuracy and performance of
using generic language identification tools to iden-
tify the language of microblog messages. It is im-
portant to note that none of the systems we test have
been specifically tuned for the microblog domain.
Furthermore, they do not make use of any non-
textual information such as author and link-based
priors (Carter et al., to appear).
We make use of two datasets of Twitter messages
kindly provided to us by other researchers. The first
is T-BE (Tromp and Pechenizkiy, 2011), which con-
tains 9659 messages in 6 European languages. The
second is T-SC (Carter et al., to appear), which con-
tains 5000 messages in 5 European languages.
We find that over both datasets, langid.py has
better accuracy than any of the other systems tested.
On T-BE, Tromp and Pechenizkiy (2011) report
accuracy between 0.92 and 0.98 depending on the
parametrization of their system, which was tuned
specifically for classifying short text messages. In
its off-the-shelf configuration, langid.py attains
an accuracy of 0.94, making it competitive with
the customized solution of Tromp and Pechenizkiy
(2011).
On T-SC, Carter et al. (to appear) report over-
all accuracy of 0.90 for TextCat in the off-the-
shelf configuration, and up to 0.92 after the inclusion
of priors based on (domain-specific) extra-textual
information. In our experiments, the accuracy of
TextCat is much lower (0.654). This is because
Carter et al. (to appear) constrained TextCat to
output only the set of 5 languages they considered.
Our results show that it is possible for a generic lan-
guage identification tool to attain reasonably high
accuracy (0.89) without artificially constraining the
set of languages to be considered, which corre-
sponds more closely to the demands of automatic
language identification to real-world data sources,
where there is generally no prior knowledge of the
languages present.
</bodyText>
<page confidence="0.995084">
29
</page>
<bodyText confidence="0.999985333333334">
We also observe that while CLD is still the fastest
classifier, this has come at the cost of accuracy in an
alternative domain such as Twitter messages, where
both langid.py and LangDetect attain better
accuracy than CLD.
An interesting point of comparison between the
Twitter datasets is how the accuracy of all systems
is generally higher on T-BE than on T-SC, despite
them covering essentially the same languages (T-BE
includes Italian, whereas T-SC does not). This is
likely to be because the T-BE dataset was produced
using a semi-automatic method which involved a
language identification step using the method of
Cavnar and Trenkle (1994) (E Tromp, personal com-
munication, July 6 2011). This may also explain
why TextCat, which is also based on Cavnar and
Trenkle’s work, has unusually high accuracy on this
dataset.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999262">
In this paper, we presented langid.py, an off-the-
shelf language identification solution. We demon-
strated the robustness of the tool over a range of test
corpora of both long and short documents (including
micro-blogs).
</bodyText>
<sectionHeader confidence="0.997291" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9917012">
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999224938461539">
Alfred V. Aho and Margaret J. Corasick. 1975. Efficient
string matching: an aid to bibliographic search. Com-
munications of the ACM, 18(6):333–340, June.
Timothy Baldwin and Marco Lui. 2010. Language iden-
tification: The long and the short of the matter. In Pro-
ceedings of NAACL HLT 2010, pages 229–237, Los
Angeles, USA.
Jamie Callan and Mark Hoy, 2009. ClueWeb09
Dataset. Available at http://boston.lti.cs.
cmu.edu/Data/clueweb09/.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
to appear. Microblog language identification: Over-
coming the limitations of short, unedited and idiomatic
text. Language Resources and Evaluation Journal.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on DocumentAnalysis and Informa-
tion Retrieval, Las Vegas, USA.
Hakan Ceylan and Yookyung Kim. 2009. Language
identification of search engine queries. In Proceedings
ofACL2009, pages 1066–1074, Singapore.
George Forman. 2003. An Extensive Empirical Study
of Feature Selection Metrics for Text Classification.
Journal of Machine Learning Research, 3(7-8):1289–
1305, October.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2004.
Building Minority Language Corpora by Learning to
Generate Web Search Queries. Knowledge and Infor-
mation Systems, 7(1):56–83, February.
Harald Hammarstrom. 2007. A Fine-Grained Model for
Language Identication. In Proceedings of iNEWS07,
pages 14–20.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. MT summit, 11.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of 5th International Joint Conference on Nat-
ural Language Processing, pages 553–561, Chiang
Mai, Thailand.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text classifi-
cation. In Proceedings of the AAAI-98 Workshop on
Learning for Text Categorization, Madison, USA.
J.R. Quinlan. 1986. Induction of Decision Trees. Ma-
chine Learning, 1(1):81–106, October.
Penelope Sibun and Jeffrey C. Reynar. 1996. Language
determination: Examining the issues. In Proceedings
of the 5th Annual Symposium on Document Analysis
and Information Retrieval, pages 125–135, Las Vegas,
USA.
J¨org Tiedemann. 2009. News from OPUS - A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. Recent Advances in Natural Language Process-
ing, V:237–248.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
Based N-gram Language Identification on Short Texts.
In Proceedings of Benelearn 2011, pages 27–35, The
Hague, Netherlands.
Tommi Vatanen, Jaakko J. Vayrynen, and Sami Virpioja.
2010. Language identification of short text segments
with n-gram models. In Proceedings of LREC 2010,
pages 3423–3430.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings ofICML 97.
</reference>
<page confidence="0.998785">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.483874">
<title confidence="0.999508">An Off-the-shelf Language Identification Tool</title>
<author confidence="0.999022">Marco Lui</author>
<author confidence="0.999022">Timothy</author>
<affiliation confidence="0.831723">NICTA Department of Computing and Information University of Melbourne, VIC 3010,</affiliation>
<email confidence="0.957034">mhlui@unimelb.edu.au,tb@ldwin.net</email>
<abstract confidence="0.997641833333333">present an off-the-shelf language identification tool. We discuss the deand implementation of and provide an empirical comparison on 5 longdocument datasets, and 2 datasets from the midomain. We find that maintains consistently high accuracy across all domains, making it ideal for end-users that require language identification without wanting to invest in preparation of in-domain training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Margaret J Corasick</author>
</authors>
<title>Efficient string matching: an aid to bibliographic search.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="5822" citStr="Aho and Corasick, 1975" startWordPosition="892" endWordPosition="896">pported by our evaluation, presented in Section 5. 3 System Architecture The full langid.py package consists of the language identifier langid.py, as well as two support modules LDfeatureselect.py and train.py. langid.py is the single file which packages the language identification tool, and the only file needed to use langid.py for off-the-shelf language identification. It comes with an embedded model which covers 97 languages using training data drawn from 5 domains. Tokenization and feature selection are carried out in a single pass over the input document via Aho-Corasick string matching (Aho and Corasick, 1975). The Aho-Corasick string matching algorithm processes an input by means of a deterministic finite automaton (DFA). Some states of the automaton are associated with the completion of one of the n-grams selected through GD feature selection. Thus, we can obtain our document representation by simply counting the number of times the DFA enters particular states while processing our input. The DFA and the associated mapping from state to n-gram are constructed during the training phase, and embedded as part of the pre-trained model. The naive Bayes classifier is implemented using numpy,1 the de-fa</context>
</contexts>
<marker>Aho, Corasick, 1975</marker>
<rawString>Alfred V. Aho and Margaret J. Corasick. 1975. Efficient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333–340, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language identification: The long and the short of the matter.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL HLT 2010,</booktitle>
<pages>229--237</pages>
<location>Los Angeles, USA.</location>
<contexts>
<context position="10369" citStr="Baldwin and Lui, 2010" startWordPosition="1620" endWordPosition="1623">cuments in a total of 97 languages. The data is drawn from 5 different domains: government documents, software documentation, newswire, online encyclopedia and an internet crawl, though no domain covers the full set of languages by itself, and some languages are present only in a single domain. More details about this corpus are given in Lui and Baldwin (2011). 2http://www.wsgi.org We do not perform explicit encoding detection, but we do not assume that all the data is in the same encoding. Previous research has shown that explicit encoding detection is not needed for language identification (Baldwin and Lui, 2010). Our training data consists mostly of UTF8-encoded documents, but some of our evaluation datasets contain a mixture of encodings. 5 Evaluation In order to benchmark langid.py, we carried out an empirical evaluation using a number of languagelabelled datasets. We compare the empirical results obtained from langid.py to those obtained from other language identification toolkits which incorporate a pre-trained model, and are thus usable offthe-shelf for language identification. These tools are listed in Table 3. 5.1 Off-the-shelf LangID tools TextCat is an implementation of the method of Cavnar </context>
<context position="13831" citStr="Baldwin and Lui, 2010" startWordPosition="2132" endWordPosition="2135"> produced. In Table 2, we report the accuracy of each tool, measured as the proportion of documents from each dataset that are correctly classified. We present the absolute accuracy and performance for langid.py, and relative accuracy and slowdown for the other systems. For this experiment, we used a machine with 2 Intel Xeon E5540 processors and 24GB of RAM. We only utilized a single core, as none of the language identification tools tested are inherently multicore. 5.2 Comparison on standard datasets We compared the four systems on datasets used in previous language identification research (Baldwin and Lui, 2010) (EUROGOV, TCL, WIKIPEDIA), as well as an extract from a biomedical parallel corpus (Tiedemann, 2009) (EMEA) and a corpus of samples from the Europarl Parallel Corpus (Koehn, 2005) (EUROPARL). The sample of EUROPARL we use was originally prepared by Shuyo Nakatani (author of LangDetect) as a validation set. langid.py compares very favorably with other language identification tools. It outperforms TextCat in terms of speed and accuracy on all of the datasets considered. langid.py is generally orders of magnitude faster than TextCat, but this advantage is reduced on larger documents. This is pri</context>
<context position="16278" citStr="Baldwin and Lui, 2010" startWordPosition="2521" endWordPosition="2524">y to have been tuned for language identification of web 28 pages. The EMEA corpus is heavy in XML markup, which CLD and langid.py both successfully ignore. One area where CLD outperforms all other systems is in its speed. However, this increase in speed comes at the cost of decreased accuracy in other domains, as we will see in Section 5.3. 5.3 Comparison on microblog messages The size of the input text is known to play a significant role in the accuracy of automatic language identification, with accuracy decreasing on shorter input documents (Cavnar and Trenkle, 1994; Sibun and Reynar, 1996; Baldwin and Lui, 2010). Recently, language identification of short strings has generated interest in the research community. Hammarstrom (2007) described a method that augmented a dictionary with an affix table, and tested it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compared a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different language identification approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, using n-gram language model</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In Proceedings of NAACL HLT 2010, pages 229–237, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Callan</author>
<author>Mark Hoy</author>
</authors>
<date>2009</date>
<note>ClueWeb09 Dataset. Available at http://boston.lti.cs. cmu.edu/Data/clueweb09/.</note>
<contexts>
<context position="11185" citStr="Callan and Hoy, 2009" startWordPosition="1746" endWordPosition="1749">empirical evaluation using a number of languagelabelled datasets. We compare the empirical results obtained from langid.py to those obtained from other language identification toolkits which incorporate a pre-trained model, and are thus usable offthe-shelf for language identification. These tools are listed in Table 3. 5.1 Off-the-shelf LangID tools TextCat is an implementation of the method of Cavnar and Trenkle (1994) by Gertjan van Noord. It has traditionally been the de facto LangID tool of choice in research, and is the basis of language identification/filtering in the ClueWeb09 Dataset (Callan and Hoy, 2009) and CorpusBuilder (Ghani et al., 2004). It includes support for training with usersupplied data. LangDetect implements a Naive Bayes classifier, using a character n-gram based representation without feature selection, with a set of normalization heuristics to improve accuracy. It is trained on data from Wikipedia,3 and can be trained with usersupplied data. CLD is a port of the embedded language identifier in Google’s Chromium browser, maintained by Mike McCandless. Not much is known about the internal design of the tool, and there is no support provided for re-training it. The datasets come </context>
</contexts>
<marker>Callan, Hoy, 2009</marker>
<rawString>Jamie Callan and Mark Hoy, 2009. ClueWeb09 Dataset. Available at http://boston.lti.cs. cmu.edu/Data/clueweb09/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Simon Carter</author>
</authors>
<title>Wouter Weerkamp, and Manos Tsagkias. to appear. Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation Journal.</title>
<marker>Carter, </marker>
<rawString>Simon Carter, Wouter Weerkamp, and Manos Tsagkias. to appear. Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>Ngram-based text categorization.</title>
<date>1994</date>
<booktitle>In Proceedings of the Third Symposium on DocumentAnalysis and Information Retrieval,</booktitle>
<location>Las Vegas, USA.</location>
<contexts>
<context position="10987" citStr="Cavnar and Trenkle (1994)" startWordPosition="1713" endWordPosition="1716">, 2010). Our training data consists mostly of UTF8-encoded documents, but some of our evaluation datasets contain a mixture of encodings. 5 Evaluation In order to benchmark langid.py, we carried out an empirical evaluation using a number of languagelabelled datasets. We compare the empirical results obtained from langid.py to those obtained from other language identification toolkits which incorporate a pre-trained model, and are thus usable offthe-shelf for language identification. These tools are listed in Table 3. 5.1 Off-the-shelf LangID tools TextCat is an implementation of the method of Cavnar and Trenkle (1994) by Gertjan van Noord. It has traditionally been the de facto LangID tool of choice in research, and is the basis of language identification/filtering in the ClueWeb09 Dataset (Callan and Hoy, 2009) and CorpusBuilder (Ghani et al., 2004). It includes support for training with usersupplied data. LangDetect implements a Naive Bayes classifier, using a character n-gram based representation without feature selection, with a set of normalization heuristics to improve accuracy. It is trained on data from Wikipedia,3 and can be trained with usersupplied data. CLD is a port of the embedded language id</context>
<context position="16230" citStr="Cavnar and Trenkle, 1994" startWordPosition="2513" endWordPosition="2516">ome insight into the design of CLD, which is likely to have been tuned for language identification of web 28 pages. The EMEA corpus is heavy in XML markup, which CLD and langid.py both successfully ignore. One area where CLD outperforms all other systems is in its speed. However, this increase in speed comes at the cost of decreased accuracy in other domains, as we will see in Section 5.3. 5.3 Comparison on microblog messages The size of the input text is known to play a significant role in the accuracy of automatic language identification, with accuracy decreasing on shorter input documents (Cavnar and Trenkle, 1994; Sibun and Reynar, 1996; Baldwin and Lui, 2010). Recently, language identification of short strings has generated interest in the research community. Hammarstrom (2007) described a method that augmented a dictionary with an affix table, and tested it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compared a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different language identification approaches. Vatanen et al. (2010) focus on messages</context>
<context position="20893" citStr="Cavnar and Trenkle (1994)" startWordPosition="3238" endWordPosition="3241">t while CLD is still the fastest classifier, this has come at the cost of accuracy in an alternative domain such as Twitter messages, where both langid.py and LangDetect attain better accuracy than CLD. An interesting point of comparison between the Twitter datasets is how the accuracy of all systems is generally higher on T-BE than on T-SC, despite them covering essentially the same languages (T-BE includes Italian, whereas T-SC does not). This is likely to be because the T-BE dataset was produced using a semi-automatic method which involved a language identification step using the method of Cavnar and Trenkle (1994) (E Tromp, personal communication, July 6 2011). This may also explain why TextCat, which is also based on Cavnar and Trenkle’s work, has unusually high accuracy on this dataset. 6 Conclusion In this paper, we presented langid.py, an off-theshelf language identification solution. We demonstrated the robustness of the tool over a range of test corpora of both long and short documents (including micro-blogs). Acknowledgments NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council throug</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. Ngram-based text categorization. In Proceedings of the Third Symposium on DocumentAnalysis and Information Retrieval, Las Vegas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hakan Ceylan</author>
<author>Yookyung Kim</author>
</authors>
<title>Language identification of search engine queries.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL2009,</booktitle>
<pages>1066--1074</pages>
<contexts>
<context position="16561" citStr="Ceylan and Kim (2009)" startWordPosition="2565" endWordPosition="2568"> accuracy in other domains, as we will see in Section 5.3. 5.3 Comparison on microblog messages The size of the input text is known to play a significant role in the accuracy of automatic language identification, with accuracy decreasing on shorter input documents (Cavnar and Trenkle, 1994; Sibun and Reynar, 1996; Baldwin and Lui, 2010). Recently, language identification of short strings has generated interest in the research community. Hammarstrom (2007) described a method that augmented a dictionary with an affix table, and tested it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compared a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different language identification approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, using n-gram language models over data drawn from UDHR in a naive Bayes classifier. A recent application where language identification is an open issue is over the rapidly-increasing volume of data being generated by social media. Microblog services such as Twitter4 allow users to post short text messages. Tw</context>
</contexts>
<marker>Ceylan, Kim, 2009</marker>
<rawString>Hakan Ceylan and Yookyung Kim. 2009. Language identification of search engine queries. In Proceedings ofACL2009, pages 1066–1074, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
</authors>
<title>An Extensive Empirical Study of Feature Selection Metrics for Text Classification.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--7</pages>
<contexts>
<context position="4718" citStr="Forman, 2003" startWordPosition="723" endWordPosition="725">tional text categorization setting, where we have in-domain training data. The task becomes much harder when trying to perform domain adaptation, that is, trying to use model parameters learned in one domain to classify data from a different domain. GD feature selection addresses this problem by focusing on key features that are relevant to the language identification task. It is based on Information Gain (IG), originally introduced as a splitting criteria for decision trees (Quinlan, 1986), and later shown to be effective for feature selection in text categorization (Yang and Pedersen, 1997; Forman, 2003). GD represents the difference in IG with respect to language and domain. Features with a high GD score are informative about language without being informative about domain. For practical reasons, before the IG calculation the candidate feature set is pruned by means of a term-frequency based feature selection. Lui and Baldwin (2011) presented empirical evidence that GD feature selection was effective for domain adaptation in language identification. This result is further supported by our evaluation, presented in Section 5. 3 System Architecture The full langid.py package consists of the lan</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>George Forman. 2003. An Extensive Empirical Study of Feature Selection Metrics for Text Classification. Journal of Machine Learning Research, 3(7-8):1289– 1305, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rayid Ghani</author>
<author>Rosie Jones</author>
<author>Dunja Mladenic</author>
</authors>
<title>Building Minority Language Corpora by Learning to Generate Web Search Queries.</title>
<date>2004</date>
<journal>Knowledge and Information Systems,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="11224" citStr="Ghani et al., 2004" startWordPosition="1752" endWordPosition="1755">nguagelabelled datasets. We compare the empirical results obtained from langid.py to those obtained from other language identification toolkits which incorporate a pre-trained model, and are thus usable offthe-shelf for language identification. These tools are listed in Table 3. 5.1 Off-the-shelf LangID tools TextCat is an implementation of the method of Cavnar and Trenkle (1994) by Gertjan van Noord. It has traditionally been the de facto LangID tool of choice in research, and is the basis of language identification/filtering in the ClueWeb09 Dataset (Callan and Hoy, 2009) and CorpusBuilder (Ghani et al., 2004). It includes support for training with usersupplied data. LangDetect implements a Naive Bayes classifier, using a character n-gram based representation without feature selection, with a set of normalization heuristics to improve accuracy. It is trained on data from Wikipedia,3 and can be trained with usersupplied data. CLD is a port of the embedded language identifier in Google’s Chromium browser, maintained by Mike McCandless. Not much is known about the internal design of the tool, and there is no support provided for re-training it. The datasets come from a variety of domains, such as news</context>
</contexts>
<marker>Ghani, Jones, Mladenic, 2004</marker>
<rawString>Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2004. Building Minority Language Corpora by Learning to Generate Web Search Queries. Knowledge and Information Systems, 7(1):56–83, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Hammarstrom</author>
</authors>
<title>A Fine-Grained Model for Language Identication.</title>
<date>2007</date>
<booktitle>In Proceedings of iNEWS07,</booktitle>
<pages>14--20</pages>
<contexts>
<context position="16399" citStr="Hammarstrom (2007)" startWordPosition="2538" endWordPosition="2539">d.py both successfully ignore. One area where CLD outperforms all other systems is in its speed. However, this increase in speed comes at the cost of decreased accuracy in other domains, as we will see in Section 5.3. 5.3 Comparison on microblog messages The size of the input text is known to play a significant role in the accuracy of automatic language identification, with accuracy decreasing on shorter input documents (Cavnar and Trenkle, 1994; Sibun and Reynar, 1996; Baldwin and Lui, 2010). Recently, language identification of short strings has generated interest in the research community. Hammarstrom (2007) described a method that augmented a dictionary with an affix table, and tested it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compared a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different language identification approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, using n-gram language models over data drawn from UDHR in a naive Bayes classifier. A recent application where language identification is an open is</context>
</contexts>
<marker>Hammarstrom, 2007</marker>
<rawString>Harald Hammarstrom. 2007. A Fine-Grained Model for Language Identication. In Proceedings of iNEWS07, pages 14–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<journal>MT summit,</journal>
<volume>11</volume>
<contexts>
<context position="14011" citStr="Koehn, 2005" startWordPosition="2163" endWordPosition="2164">ormance for langid.py, and relative accuracy and slowdown for the other systems. For this experiment, we used a machine with 2 Intel Xeon E5540 processors and 24GB of RAM. We only utilized a single core, as none of the language identification tools tested are inherently multicore. 5.2 Comparison on standard datasets We compared the four systems on datasets used in previous language identification research (Baldwin and Lui, 2010) (EUROGOV, TCL, WIKIPEDIA), as well as an extract from a biomedical parallel corpus (Tiedemann, 2009) (EMEA) and a corpus of samples from the Europarl Parallel Corpus (Koehn, 2005) (EUROPARL). The sample of EUROPARL we use was originally prepared by Shuyo Nakatani (author of LangDetect) as a validation set. langid.py compares very favorably with other language identification tools. It outperforms TextCat in terms of speed and accuracy on all of the datasets considered. langid.py is generally orders of magnitude faster than TextCat, but this advantage is reduced on larger documents. This is primarily due to the design of TextCat, which requires that the supplied models be read from file for each document classified. langid.py generally outperforms LangDetect, except in d</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. MT summit, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>Cross-domain feature selection for language identification.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>553--561</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="3488" citStr="Lui and Baldwin, 2011" startWordPosition="527" endWordPosition="531">f. Since langid.py implements a supervised classifier, this presents two primary challenges: (1) a pretrained model must be distributed with the classifier, and (2) the model must generalize to data from different domains, meaning that in its default configuration, it must have good accuracy over inputs as diverse as web pages, newspaper articles and microblog messages. (1) is mostly a practical consideration, and so we will address it in Section 3. In order to address (2), we integrate information about the language identification task from a variety of domains by using LD feature selection (Lui and Baldwin, 2011). Lui and Baldwin (2011) showed that it is relatively easy to attain high accuracy for language idenProceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 25–30, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Dataset Documents Langs Doc Length (bytes) EUROGOV 1500 10 1.7×104 ±3.9×104 TCL 3174 60 2.6×103 ±3.8×103 WIKIPEDIA 4963 67 1.5×103 ±4.1×103 EMEA 19988 22 2.9×105 ±7.9×105 EUROPARL 20828 22 1.7×102 ±1.6×102 T-BE 9659 6 1.0×102 ±3.2×101 T-SC 5000 5 8.8×101 ±3.9×101 Table 1: Summary of the LangID datasets tifi</context>
<context position="5054" citStr="Lui and Baldwin (2011)" startWordPosition="776" endWordPosition="779">are relevant to the language identification task. It is based on Information Gain (IG), originally introduced as a splitting criteria for decision trees (Quinlan, 1986), and later shown to be effective for feature selection in text categorization (Yang and Pedersen, 1997; Forman, 2003). GD represents the difference in IG with respect to language and domain. Features with a high GD score are informative about language without being informative about domain. For practical reasons, before the IG calculation the candidate feature set is pruned by means of a term-frequency based feature selection. Lui and Baldwin (2011) presented empirical evidence that GD feature selection was effective for domain adaptation in language identification. This result is further supported by our evaluation, presented in Section 5. 3 System Architecture The full langid.py package consists of the language identifier langid.py, as well as two support modules LDfeatureselect.py and train.py. langid.py is the single file which packages the language identification tool, and the only file needed to use langid.py for off-the-shelf language identification. It comes with an embedded model which covers 97 languages using training data dra</context>
<context position="9722" citStr="Lui and Baldwin (2011)" startWordPosition="1512" endWordPosition="1515">n.py implements estimation of parameters for the multinomial naive Bayes model, as well as the construction of the DFA for the Aho-Corasick string matching algorithm. Its input is a list of byte patterns representing a feature set (such as that selected via LDfeatureselect.py), and a corpus of training documents. It produces the final model as a single compressed, encoded string, which can be saved to an external file and used by langid.py via a command-line option. 4 Training Data langid.py is distributed with an embedded model trained using the multi-domain language identification corpus of Lui and Baldwin (2011). This corpus contains documents in a total of 97 languages. The data is drawn from 5 different domains: government documents, software documentation, newswire, online encyclopedia and an internet crawl, though no domain covers the full set of languages by itself, and some languages are present only in a single domain. More details about this corpus are given in Lui and Baldwin (2011). 2http://www.wsgi.org We do not perform explicit encoding detection, but we do not assume that all the data is in the same encoding. Previous research has shown that explicit encoding detection is not needed for </context>
</contexts>
<marker>Lui, Baldwin, 2011</marker>
<rawString>Marco Lui and Timothy Baldwin. 2011. Cross-domain feature selection for language identification. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 553–561, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for Naive Bayes text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI-98 Workshop on Learning for Text Categorization,</booktitle>
<location>Madison, USA.</location>
<contexts>
<context position="2703" citStr="McCallum and Nigam, 1998" startWordPosition="400" endWordPosition="403">o produce a corpus of language-labelled training documents from the application domain. What is required is thus a generic language identification tool that is usable off-the-shelf, i.e. with no end-user training and minimal configuration. In this paper, we present langid.py, a LangID tool with the following characteristics: (1) fast, (2) usable off-the-shelf, (3) unaffected by domainspecific features (e.g. HTML, XML, markdown), (4) single file with minimal dependencies, and (5) flexible interface 2 Methodology langid.py is trained over a naive Bayes classifier with a multinomial event model (McCallum and Nigam, 1998), over a mixture of byte n-grams (1&lt;n&lt;4). One key difference from conventional text categorization solutions is that langid.py was designed to be used off-the-shelf. Since langid.py implements a supervised classifier, this presents two primary challenges: (1) a pretrained model must be distributed with the classifier, and (2) the model must generalize to data from different domains, meaning that in its default configuration, it must have good accuracy over inputs as diverse as web pages, newspaper articles and microblog messages. (1) is mostly a practical consideration, and so we will address </context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In Proceedings of the AAAI-98 Workshop on Learning for Text Categorization, Madison, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of Decision Trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="4600" citStr="Quinlan, 1986" startWordPosition="705" endWordPosition="706">T-BE 9659 6 1.0×102 ±3.2×101 T-SC 5000 5 8.8×101 ±3.9×101 Table 1: Summary of the LangID datasets tification in a traditional text categorization setting, where we have in-domain training data. The task becomes much harder when trying to perform domain adaptation, that is, trying to use model parameters learned in one domain to classify data from a different domain. GD feature selection addresses this problem by focusing on key features that are relevant to the language identification task. It is based on Information Gain (IG), originally introduced as a splitting criteria for decision trees (Quinlan, 1986), and later shown to be effective for feature selection in text categorization (Yang and Pedersen, 1997; Forman, 2003). GD represents the difference in IG with respect to language and domain. Features with a high GD score are informative about language without being informative about domain. For practical reasons, before the IG calculation the candidate feature set is pruned by means of a term-frequency based feature selection. Lui and Baldwin (2011) presented empirical evidence that GD feature selection was effective for domain adaptation in language identification. This result is further sup</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>J.R. Quinlan. 1986. Induction of Decision Trees. Machine Learning, 1(1):81–106, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Sibun</author>
<author>Jeffrey C Reynar</author>
</authors>
<title>Language determination: Examining the issues.</title>
<date>1996</date>
<booktitle>In Proceedings of the 5th Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>125--135</pages>
<location>Las Vegas, USA.</location>
<contexts>
<context position="16254" citStr="Sibun and Reynar, 1996" startWordPosition="2517" endWordPosition="2520">n of CLD, which is likely to have been tuned for language identification of web 28 pages. The EMEA corpus is heavy in XML markup, which CLD and langid.py both successfully ignore. One area where CLD outperforms all other systems is in its speed. However, this increase in speed comes at the cost of decreased accuracy in other domains, as we will see in Section 5.3. 5.3 Comparison on microblog messages The size of the input text is known to play a significant role in the accuracy of automatic language identification, with accuracy decreasing on shorter input documents (Cavnar and Trenkle, 1994; Sibun and Reynar, 1996; Baldwin and Lui, 2010). Recently, language identification of short strings has generated interest in the research community. Hammarstrom (2007) described a method that augmented a dictionary with an affix table, and tested it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compared a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different language identification approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, usi</context>
</contexts>
<marker>Sibun, Reynar, 1996</marker>
<rawString>Penelope Sibun and Jeffrey C. Reynar. 1996. Language determination: Examining the issues. In Proceedings of the 5th Annual Symposium on Document Analysis and Information Retrieval, pages 125–135, Las Vegas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing, V:237–248.</booktitle>
<contexts>
<context position="13932" citStr="Tiedemann, 2009" startWordPosition="2150" endWordPosition="2151">ch dataset that are correctly classified. We present the absolute accuracy and performance for langid.py, and relative accuracy and slowdown for the other systems. For this experiment, we used a machine with 2 Intel Xeon E5540 processors and 24GB of RAM. We only utilized a single core, as none of the language identification tools tested are inherently multicore. 5.2 Comparison on standard datasets We compared the four systems on datasets used in previous language identification research (Baldwin and Lui, 2010) (EUROGOV, TCL, WIKIPEDIA), as well as an extract from a biomedical parallel corpus (Tiedemann, 2009) (EMEA) and a corpus of samples from the Europarl Parallel Corpus (Koehn, 2005) (EUROPARL). The sample of EUROPARL we use was originally prepared by Shuyo Nakatani (author of LangDetect) as a validation set. langid.py compares very favorably with other language identification tools. It outperforms TextCat in terms of speed and accuracy on all of the datasets considered. langid.py is generally orders of magnitude faster than TextCat, but this advantage is reduced on larger documents. This is primarily due to the design of TextCat, which requires that the supplied models be read from file for ea</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces. Recent Advances in Natural Language Processing, V:237–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tromp</author>
<author>Mykola Pechenizkiy</author>
</authors>
<title>GraphBased N-gram Language Identification on Short Texts.</title>
<date>2011</date>
<booktitle>In Proceedings of Benelearn</booktitle>
<pages>27--35</pages>
<location>The Hague, Netherlands.</location>
<contexts>
<context position="18091" citStr="Tromp and Pechenizkiy (2011)" startWordPosition="2792" endWordPosition="2795">ter ‘documents’ to be classified, and is compounded by the lack of language-labelled indomain data for training and validation. This has led to recent research focused specifically on the task of language identification of Twitter messages. Carter et al. (to appear) improve language identification in Twitter messages by augmenting standard methods 4http://www.twitter.com 5http://semiocast.com/downloads/ Semiocast_Half_of_messages_on_Twitter_ are_not_in_English_20100224.pdf with language identification priors based on a user’s previous messages and by the content of links embedded in messages. Tromp and Pechenizkiy (2011) present a method for language identification of short text messages by means of a graph structure. Despite the recently published results on language identification of microblog messages, there is no dedicated off-the-shelf system to perform the task. We thus examine the accuracy and performance of using generic language identification tools to identify the language of microblog messages. It is important to note that none of the systems we test have been specifically tuned for the microblog domain. Furthermore, they do not make use of any nontextual information such as author and link-based p</context>
<context position="19471" citStr="Tromp and Pechenizkiy (2011)" startWordPosition="3011" endWordPosition="3014">nd Pechenizkiy, 2011), which contains 9659 messages in 6 European languages. The second is T-SC (Carter et al., to appear), which contains 5000 messages in 5 European languages. We find that over both datasets, langid.py has better accuracy than any of the other systems tested. On T-BE, Tromp and Pechenizkiy (2011) report accuracy between 0.92 and 0.98 depending on the parametrization of their system, which was tuned specifically for classifying short text messages. In its off-the-shelf configuration, langid.py attains an accuracy of 0.94, making it competitive with the customized solution of Tromp and Pechenizkiy (2011). On T-SC, Carter et al. (to appear) report overall accuracy of 0.90 for TextCat in the off-theshelf configuration, and up to 0.92 after the inclusion of priors based on (domain-specific) extra-textual information. In our experiments, the accuracy of TextCat is much lower (0.654). This is because Carter et al. (to appear) constrained TextCat to output only the set of 5 languages they considered. Our results show that it is possible for a generic language identification tool to attain reasonably high accuracy (0.89) without artificially constraining the set of languages to be considered, which </context>
</contexts>
<marker>Tromp, Pechenizkiy, 2011</marker>
<rawString>Erik Tromp and Mykola Pechenizkiy. 2011. GraphBased N-gram Language Identification on Short Texts. In Proceedings of Benelearn 2011, pages 27–35, The Hague, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi Vatanen</author>
<author>Jaakko J Vayrynen</author>
<author>Sami Virpioja</author>
</authors>
<title>Language identification of short text segments with n-gram models.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>3423--3430</pages>
<contexts>
<context position="16812" citStr="Vatanen et al. (2010)" startWordPosition="2606" endWordPosition="2609">put documents (Cavnar and Trenkle, 1994; Sibun and Reynar, 1996; Baldwin and Lui, 2010). Recently, language identification of short strings has generated interest in the research community. Hammarstrom (2007) described a method that augmented a dictionary with an affix table, and tested it over synthetic data derived from a parallel bible corpus. Ceylan and Kim (2009) compared a number of methods for identifying the language of search engine queries of 2 to 3 words. They develop a method which uses a decision tree to integrate outputs from several different language identification approaches. Vatanen et al. (2010) focus on messages of 5–21 characters, using n-gram language models over data drawn from UDHR in a naive Bayes classifier. A recent application where language identification is an open issue is over the rapidly-increasing volume of data being generated by social media. Microblog services such as Twitter4 allow users to post short text messages. Twitter has a worldwide user base, evidenced by the large array of languages present on Twitter (Carter et al., to appear). It is estimated that half the messages on Twitter are not in English. 5 This new domain presents a significant challenge for auto</context>
</contexts>
<marker>Vatanen, Vayrynen, Virpioja, 2010</marker>
<rawString>Tommi Vatanen, Jaakko J. Vayrynen, and Sami Virpioja. 2010. Language identification of short text segments with n-gram models. In Proceedings of LREC 2010, pages 3423–3430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings ofICML 97.</booktitle>
<contexts>
<context position="4703" citStr="Yang and Pedersen, 1997" startWordPosition="719" endWordPosition="722">ets tification in a traditional text categorization setting, where we have in-domain training data. The task becomes much harder when trying to perform domain adaptation, that is, trying to use model parameters learned in one domain to classify data from a different domain. GD feature selection addresses this problem by focusing on key features that are relevant to the language identification task. It is based on Information Gain (IG), originally introduced as a splitting criteria for decision trees (Quinlan, 1986), and later shown to be effective for feature selection in text categorization (Yang and Pedersen, 1997; Forman, 2003). GD represents the difference in IG with respect to language and domain. Features with a high GD score are informative about language without being informative about domain. For practical reasons, before the IG calculation the candidate feature set is pruned by means of a term-frequency based feature selection. Lui and Baldwin (2011) presented empirical evidence that GD feature selection was effective for domain adaptation in language identification. This result is further supported by our evaluation, presented in Section 5. 3 System Architecture The full langid.py package cons</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proceedings ofICML 97.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>