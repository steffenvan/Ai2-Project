<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.9915655">
The Imagination of Crowds: Conversational AAC Language Modeling using
Crowdsourcing and Large Data Sources
</title>
<author confidence="0.988516">
Keith Vertanen Per Ola Kristensson
</author>
<affiliation confidence="0.999504">
Department of Computer Science School of Computer Science
Princeton University University of St Andrews
</affiliation>
<email confidence="0.998823">
vertanen@princeton.edu pok@st-andrews.ac.uk
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999734923076923">
Augmented and alternative communication
(AAC) devices enable users with certain com-
munication disabilities to participate in every-
day conversations. Such devices often rely
on statistical language models to improve text
entry by offering word predictions. These
predictions can be improved if the language
model is trained on data that closely reflects
the style of the users’ intended communica-
tions. Unfortunately, there is no large dataset
consisting of genuine AAC messages. In this
paper we demonstrate how we can crowd-
source the creation of a large set of fictional
AAC messages. We show that these messages
model conversational AAC better than the cur-
rently used datasets based on telephone con-
versations or newswire text. We leverage our
crowdsourced messages to intelligently select
sentences from much larger sets of Twitter,
blog and Usenet data. Compared to a model
trained only on telephone transcripts, our best
performing model reduced perplexity on three
test sets of AAC-like communications by 60–
82% relative. This translated to a potential
keystroke savings in a predictive keyboard in-
terface of 5–11%.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997474902439025">
Users with certain communication disabilities
rely on augmented and alternative communication
(AAC) devices to take part in everyday conversa-
tions. Often these devices consist of a predictive
text input method coupled with text-to-speech out-
put. Unfortunately, the text entry rates provided by
700
AAC devices are typically low, between 0.5 and 16
words-per-minute (Trnka et al., 2009).
As a consequence, researchers have made nu-
merous efforts to increase AAC text entry rates by
employing a variety of improved language model-
ing techniques. Examples of approaches include
adapting the language model to recently used words
(Wandmacher et al., 2008; Trnka, 2008), using syn-
tactic information (Hunnicutt, 1989; Garay-Vitoria
and Gonz´alez-Abascal, 1997), using semantic in-
formation (Wandmacher and Antoine, 2007; Li
and Hirst, 2005), and modeling topics (Lesher and
Rinkus, 2002; Trnka et al., 2006). For a recent sur-
vey, see Garay-Vitoria and Abascal (2006).
While such language model improvement tech-
niques are undoubtedly helpful, certainly they can
all benefit from starting with a long-span language
model trained on large amounts of closely matched
data. For AAC devices this means closely modeling
everyday face-to-face communications. However,
a long-standing problem in the field is the lack of
good data sources that adequately model such AAC
communications. Due to privacy-reasons and other
ethical concerns, there is no large dataset consist-
ing of genuine AAC messages. Therefore, previous
research has used transcripts of telephone conversa-
tions or newswire text. However, these data sources
are unlikely to be an ideal basis for AAC language
models.
In this paper we show that it is possible to signif-
icantly improve conversational AAC language mod-
eling by first crowdsourcing the creation of a fic-
tional collection of AAC messages on the Amazon
Mechanical Turk microtask market. Using a care-
</bodyText>
<note confidence="0.9483405">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 700–711,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999855928571429">
fully designed microtask we collected 5890 mes-
sages from 298 unique workers. As we will see,
word-for-word these fictional AAC messages are
better at predicting AAC test sets than a wide-range
of other text sources. Further, we demonstrate that
Twitter, blog and Usenet data outperform telephone
transcripts or newswire text.
While our crowdsourced AAC data is better than
other text sources, it is too small to train high-quality
long-span language models. We therefore investi-
gate how to use our crowdsourced collection to in-
telligently select AAC-like sentences from Twitter,
blog and Usenet data. We compare a variety of
different techniques for doing this intelligent selec-
tion. We find that the best selection technique is the
recently proposed cross-entropy difference method
(Moore and Lewis, 2010). Using this method, we
build a compact and well-performing mixture model
from the Twitter, blog and Usenet sentences most
similar to our crowdsourced data.
We evaluate our mixture model on four different
test sets. On the three most AAC-like test sets, we
found substantial reductions in not only perplexity
but also in potential keystroke savings when used
in a predictive keyboard interface. Finally, to aid
other AAC researchers, we have publicly released
our crowdsourced AAC collection, word lists and
best-performing language models1.
</bodyText>
<sectionHeader confidence="0.755731" genericHeader="method">
2 Crowdsourcing AAC-like Messages
</sectionHeader>
<bodyText confidence="0.999816666666667">
As we mentioned in the introduction, there are un-
fortunately no publicly available sources of gen-
uine conversational AAC messages. We conjectured
we could create surrogate data by asking workers
on Amazon Mechanical Turk to imagine they were
a user of an AAC device and having them invent
things they might want to say. While crowdsourcing
is commonly used for simple human computation
tasks, such as labeling images and transcribing au-
dio, it is an open research question whether we can
leverage workers’ creativity to invent plausible and
useful AAC-like messages. In this section, we de-
scribe our carefully constructed microtask and com-
pare how well our collected messages correspond to
communications from actual AAC users.
</bodyText>
<footnote confidence="0.965389">
1http://www.aactext.org/imagine/
</footnote>
<figureCaption confidence="0.99232625">
Figure 1: The interface for HITs of type 1 in our
crowdsourced data collection.
Figure 2: The interface for HITs of type 2 in our
crowdsourced data collection.
</figureCaption>
<subsectionHeader confidence="0.998246">
2.1 Collection Tasks
</subsectionHeader>
<bodyText confidence="0.999956">
To collect our data, we used two different types
of human intelligence tasks (HITs). In type 1, the
workers were told to imagine that due to an accident
or medical condition they had to use a communica-
tion device to speak for them. Workers were asked
to invent a plausible communication. Workers were
prevented from pasting text. After several pilot ex-
periments, we arrived at the instructions shown in
figure 1.
In type 2, a worker first judged the plausibility
of a communication written by a previous worker
(figure 2). After judging, the worker was asked
to “invent a completely new communication” as if
the worker was the AAC user. Workers were pre-
vented from pasting text or typing the identical text
as the one just judged. The same communication
</bodyText>
<page confidence="0.967684">
701
</page>
<bodyText confidence="0.923541730769231">
was judged by three separate workers. In this work Is the dog friendly?
we did not make use of these judgments. Can I have some water please?
2.2 Data Cleaning I need to start making a shopping list soon.
While most workers produced plausible and often What I would really like right now is a plate of fruit.
creative communications, some workers entered ob- Who will drive me to the doctor’s office tomorrow?
vious garbage. These workers were identified by a
quick visual scan of the submitted communications.
We rejected the work of 9% of the workers in type
1 and 4% of the workers in type 2. After removing
these workers, we had 2481 communications from
type 1 and 4440 communications from type 2.
After combining the data from all accepted HITs,
we conducted further semi-automatic data clean-
ing. We first manually reviewed communications
sorted by worker. We removed workers whose text
was non-fluent English or not plausible (e.g. some
workers entered news headlines or proverbs). Iden-
tical communications from the same worker were
removed. We removed communications with an
out-of-vocabulary (OOV) rate of over 20% with re-
spect to a large word list of 330K words obtained
from human-edited dictionaries2. We also removed
communications that were all in upper case, con-
tained common texting abbreviations (e.g. “plz”,
“ru”, “2day”), communications over 80 characters,
and communications with excessive letter repeti-
tions (e.g. “yippeeee”). After cleaning, we had 5890
messages from 298 unique workers.
2.3 Results
Tables 1 and 2 show some example communications
obtained in each HIT type. Sometimes, but not al-
ways, type 2 resulted in the worker writing a similar
communication as the one judged. This is a mixed
blessing. While it may reduce the diversity of com-
munications, we found that workers were more ea-
ger to accept HITs of type 2. The average HIT com-
pletion time was also shorter, 24 seconds in type 2
versus 36 seconds in type 1. While we initially paid
$0.04/HIT for both types, we found in subsequent
rounds that we could pay $0.02/HIT for type 2. We
also had to reject less work in type 2 and qualita-
tively found the communications to be more AAC-
like. Since workers had to imagine themselves in a
Table 1: Example communications from type 1.
Can you bring my slippers?
I am cold, is there another blanket.
How did Pam take the news?
Bring the fuzzy slippers here.
Did you have breakfast?
why are you so late?
I am pretty hungry, can we go eat?
I had bacon eggs and hashbrowns for breakfast.
</bodyText>
<tableCaption confidence="0.679017">
Table 2: Example communications from type 2. The
</tableCaption>
<table confidence="0.869609096774194">
text in bold is the message workers judged. It is fol-
lowed in plain text by the workers’ new messages.
very unfamiliar situation, it appears that providing a
concrete example was helpful to workers.
3 Comparison of Training Sources
In this section, we compare the predictive perfor-
mance of language models trained on our Turk AAC
data with models trained on other text sources. We
use the following training sets:
• NEWS – Newspaper articles from the CSR-III
(Graff et al., 1995) and Gigaword corpora (Graff,
2003). 60M sentences, 1323M words.
• WIKIPEDIA – Current articles and discussion
threads from a snapshot of Wikipedia (January 3,
2008). 24M sentences, 452M words.
• USENET – Messages from a Usenet corpus
(Shaoul and Westbury, 2009). 123M sentences,
1847M words.
• SWITCHBOARD – Transcripts of 2217 telephone
conversations from the Switchboard corpus (God-
frey et al., 1992). Due to its conversational style,
this corpus has been popular for AAC language
modeling (Lesher and Rinkus, 2002; Trnka et al.,
2009). 0.2M sentences, 2.6M words.
• BLOG – Blog posts from the ICWSM corpus
(Burton et al., 2009). 25M sentences, 387M
words.
2We combined Wiktionary, Webster’s dictionary provided
by Project Gutenberg, the CMU pronouncing dictionary and
GNU aspell.
702
</table>
<listItem confidence="0.9295685">
• TWITTER – We collected Twitter messages via
the streaming API between December 2010 and
March 2011. We used the free Twitter stream
which provides access to 5% of all tweets. Twit-
ter may be particularly well suited for modeling
AAC communications as tweets are short typed
messages that are often informal person-to-person
communications. Twitter has previously been
proposed as a candidate for modeling conversa-
tions, see for example Ritter et al. (2010). 7M
sentences, 55M words.
• TURKTRAIN – Communications from 80% of the
workers in our crowdsourced collection. 4981
sentences, 24860 words.
</listItem>
<bodyText confidence="0.999281666666666">
WIKIPEDIA, USENET, BLOG and TWITTER all
consisted of raw text that required significant filter-
ing to eliminate garbage, spam, repeated messages,
XML tags, non-English text, etc. Given the large
amount of data available, our approach was to throw
away any text that did not appear to be a sensible
English sentence. For example, we eliminated any
sentence having a large number of words not in our
330K word list.
</bodyText>
<subsectionHeader confidence="0.999933">
3.1 Test Sets
</subsectionHeader>
<bodyText confidence="0.999917">
We evaluated our models on the following test sets:
</bodyText>
<listItem confidence="0.901826722222222">
• COMM – Sentences written in response to hy-
pothetical communication situations collected by
Venkatagiri (1999). We removed nine sentences
containing numbers. This set is used throughout
the paper. 251 sentences, 1789 words.
• SPECIALISTS – Context specific phrases sug-
gested by AAC specialists3. This set is used
throughout the paper. 952 sentences, 3842 words.
• TURKDEV – Communications from 10% of the
workers in our crowdsourced collection (disjoint
from TURKTRAIN and TURKTEST). This set will
be used for initial evaluations and also to tune our
models. 551 sentences, 2916 words.
• TURKTEST – Communications from 10% of the
workers in our crowdsourced collection (disjoint
from TURKTRAIN and TURKDEV). This set is
used only in the final evaluation section. 563 sen-
tences, 2721 words.
</listItem>
<footnote confidence="0.670406">
3http://aac.unl.edu/vocabulary.html
</footnote>
<table confidence="0.313811666666667">
Test set Sentence
COMM I love your new haircut.
COMM How many children do you have?
</table>
<bodyText confidence="0.972215">
SPECIALISTS Are you sure you don’t mind?
SPECIALISTS I’ll keep an eye on that for you
SWITCHTEST yeah he’s a good actor though
SWITCHTEST what did she have like
</bodyText>
<tableCaption confidence="0.950851">
Table 3: Examples from three of our test sets.
</tableCaption>
<listItem confidence="0.983027">
• SWITCHTEST – Transcripts of three Switchboard
conversations (disjoint from the SWITCHBOARD
training set). This is the same set used in Trnka et
al. (2009). We dropped one sentence containing a
</listItem>
<bodyText confidence="0.929526166666667">
dash. This set is only used in the final evaluation
section. 59 sentences, 508 words.
TURKDEV and TURKTEST contain text similar
to table 1 and 2. Table 3 shows some examples from
the other three test sets. Sentences in COMM tended
to be richer in vocabulary and subject matter than
those in SPECIALISTS. The SPECIALISTS sentences
tended to be general phrases that avoided mention-
ing specific situations, proper names, etc. Sentences
in SWITCHTEST exhibited phenomena typical of
human-to-human voice conversations (filler words,
backchannels, interruptions, etc).
</bodyText>
<subsectionHeader confidence="0.988169">
3.2 Language Model Training
</subsectionHeader>
<bodyText confidence="0.999977263157895">
All language models were trained using the SRILM
toolkit (Stolcke, 2002). All models used interpo-
lated modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1998). In this sec-
tion, we trained 3-gram language models with no
count-cutoffs. All text was converted to lowercase
and we removed punctuation except for apostrophes.
We believe punctuation would likely slow down a
user’s conversation for only a small potential advan-
tage (e.g. improving text-to-speech prosody).
All models used a vocabulary of 63K words in-
cluding an unknown word. We obtained our vocab-
ulary by taking all words occurring in TURKTRAIN
and all words occurring four or more times in the
TWITTER training set. We restricted our vocabu-
lary to words from our large list of 330K words.
This restriction prevented the inclusion of com-
mon misspellings prevalent in many of our train-
ing sets. Our 63K vocabulary resulted in low OOV
</bodyText>
<page confidence="0.989415">
703
</page>
<figure confidence="0.999893224489796">
Average perplexity
0 500 1000 1500
●
●
● ● ●
● ● ● ● ● ●
●
News
Wikipedia
Usenet
Switchboard
Blog
Twitter
TurkTrain
Average perplexity
0 500 1000 1500
●
●
●
● ●
● ● ● ● ● ●
●
News
Wikipedia
Usenet
Switchboard
Blog
Twitter
TurkTrain
Average perplexity
0 500 1000 1500
●
●
News
Wikipedia
Usenet
Switchboard
Blog
Twitter
TurkTrain
4 6 8 10 12 14 16 18 20 22 24
Words of training data (K)
(a) TURKDEV test set
4 6 8 10 12 14 16 18 20 22 24
Words of training data (K)
(b) COMM test set
4 6 8 10 12 14 16 18 20 22 24
Words of training data (K)
(c) SPECIALISTS test set
</figure>
<figureCaption confidence="0.969241333333333">
Figure 3: Perplexity of language models trained on the same amount of data from different sources. The
perplexity is the average of 20 models trained on random subsets of the training data (one standard deviation
error bars).
</figureCaption>
<bodyText confidence="0.854352333333333">
rates for all test sets: COMM 0%, SPECIALISTS
0.05%, TURKDEV 0.1%, TURKTEST 0.07%, and
SWITCHTEST 0.8%.
</bodyText>
<subsectionHeader confidence="0.999239">
3.3 Small Training Size Experiment
</subsectionHeader>
<bodyText confidence="0.998724230769231">
We trained language models on each dataset, vary-
ing the number of training words from 4K to 24K
(the limit of the TURKTRAIN set). For each dataset
and training amount, we built 20 different models by
choosing sentences from the full training set at ran-
dom. We computed the mean and standard deviation
of the per-word perplexity of the set of 20 models.
As shown in figure 3, word-for-word the TURK-
TRAIN data was superior for our three most AAC-
like test sets. Thus it appears our crowdsourcing pro-
cedure was successful at generating AAC-like data.
TWITTER was consistently the second best. BLOG,
USENET and SWITCHBOARD also performed well.
</bodyText>
<subsectionHeader confidence="0.994994">
3.4 Large Training Size Experiment
</subsectionHeader>
<bodyText confidence="0.999835666666667">
The previous experiment used a small amount of
training data. We selected the best three datasets
having tens of millions of words of training data:
USENET, BLOG, and TWITTER. As in the previ-
ous experiment, we computed the mean and stan-
dard deviation of the per-word perplexity of a set
of 20 models. Increasing the amount of training
data substantially reduced perplexity compared to
our small TURKTRAIN collection (figure 4). Tweets
were clearly well suited for modeling AAC-like text
as 3M words of TWITTER data was better than 40M
words of BLOG data.
</bodyText>
<subsectionHeader confidence="0.994571">
3.5 Comparison with Real AAC Data
</subsectionHeader>
<bodyText confidence="0.999995545454546">
Beukelman et al. (1984) analyzed the communica-
tions made by five nonspeaking adults over 14 days.
All users were experienced using a tape-typewriter
AAC device. Beukelman gives a ranked list of the
top 500 words, the frequency of the top 20 words,
and statistics calculated on the communications.
For the top 10 words in Beukelman’s AAC user
data, we computed the probability of each word in
our various datasets (figure 5). As shown, some
words such as “to” and “a” occur with similar fre-
quency across all datasets. Some words such as
“the” are overrepresented in data such as news text.
Other words such as “I” and “you” are much more
variable. Our Turk data has the closest matching
frequency for the most popular word “I”. Interest-
ingly, our Turk data shows a much higher probabil-
ity for “you” than the AAC data. We believe this re-
sulted from the situation we asked workers to imag-
ine (i.e. communicating via a letter-at-a-time scan-
ning interface). Workers presumed in such a situa-
tion they would need to ask others to do many tasks.
We observed many requests in the data such as “Can
</bodyText>
<page confidence="0.979974">
704
</page>
<figure confidence="0.999822575757576">
Average perplexity
0 50 100 150 200
●
● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ●
●
Usenet
Blog
Twitter
Average perplexity
0 50 100 150 200
●
● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ●
●
Usenet
Blog
Twitter
Average perplexity
0 50 100 150 200
●
Usenet
Blog
Twitter
0 10 20 30 40
Words of training data (M)
(a) TURKDEV test set
0 10 20 30 40
Words of training data (M)
(b) COMM test set
0 10 20 30 40
Words of training data (M)
(c) SPECIALISTS test set
</figure>
<figureCaption confidence="0.998769">
Figure 4: Perplexity of language models trained on increasing amounts of data from three different training
sources. Results on the TURKDEV, COMM and SPECIALISTS test sets.
</figureCaption>
<bodyText confidence="0.7547">
i to you the a it my and in is
</bodyText>
<figureCaption confidence="0.999338">
Figure 5: The unigram probabilities of the top 10 words reported by Beukelman et al. (1984).
</figureCaption>
<figure confidence="0.997131470588235">
Unigram probability
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
AAC users, Beukelman
Turk workers
Switchboard
Twitter
Blog
Usenet
Wikipedia
News
</figure>
<bodyText confidence="0.998345565217391">
you change my sheets?” and “Can you walk the dog
for me?”
Beukelman reports 33% of all communications
could be made using only the top 500 words. The
same 500 words allowed writing of 34% of our Turk
communications. Other datasets exhibited much
lower percentages. Note that this is at least partially
due to the longer sentences present in some datasets.
Unfortunately, Beukelman does not report the aver-
age communication length. Our Turk communica-
tions were 5.0 words on average. The next shortest
dataset was TWITTER with 7.5 words per communi-
cation. Despite their short average length, only 10%
of Tweets could be written using the top 500 words.
Beukelman reports that 80% of words in the AAC
users’ communications were in the top 500 words.
81% of the words in our crowdsourced data were in
this word list. For comparison, only 65% of words
in our TWITTER data were in the 500 word vocabu-
lary. While our TURKTRAIN set contains only 2141
unique words, this may in fact be good since it has
been argued that rare words have received too much
attention in AAC (Baker et al., 2000).
</bodyText>
<sectionHeader confidence="0.971105" genericHeader="method">
4 Using Large Datasets Effectively
</sectionHeader>
<bodyText confidence="0.999990363636364">
In the previous section, we found our crowdsourced
data was good at predicting AAC-like test sets.
However, in order to build a good long-span lan-
guage model, we would require millions of such
communications. Crowdsourcing such a large col-
lection would be prohibitively expensive. There-
fore, we instead investigated how to use our crowd-
sourced data to intelligently select AAC-like data
from other large datasets. For large datasets, we
used TWITTER, BLOG and USENET as they were
both large and well-matched to AAC data.
</bodyText>
<subsectionHeader confidence="0.999752">
4.1 Selecting AAC-like Data
</subsectionHeader>
<bodyText confidence="0.980953">
For each training sentence, we calculated three val-
ues:
</bodyText>
<listItem confidence="0.9436175">
• WER – The minimum word error rate between
the training sentence and one of the crowdsourced
</listItem>
<page confidence="0.988014">
705
</page>
<figure confidence="0.99983475">
Perplexity
40 60 80 100 120 140
●
●●●●●●●●●●●●●●●●●●●●● ● ●● ● ● ● ●
Entropy pruning
Count cutoff pruning
● Cross−entropy selection
WER selection
Cross−entropy difference
Perplexity
40 60 80 100 120 140
●
● ● ● ● ● ● ● ● ● ● ● ● ● ●
Entropy pruning
Count cutoff pruning
● Cross−entropy selection
WER selection
Cross−entropy difference
Perplexity
40 60 80 100 120 140
●
●
●
●
●● ● ● ● ● ● ● ● ●
● ● ● ● ●
Entropy pruning
Count cutoff pruning
● Cross−entropy selection
WER selection
Cross−entropy difference
0 5 10 15 20 25
Language model parameters (M)
(a) TWITTER
0 5 10 15 20 25
Language model parameters (M)
(b) BLOG
0 5 10 15 20 25
Language model parameters (M)
(c) USENET
</figure>
<figureCaption confidence="0.999975">
Figure 6: Perplexity on TURKDEV using different data selection and pruning techniques.
</figureCaption>
<bodyText confidence="0.999156">
communications. This is the minimum number of
words that must be inserted, substituted or deleted
to transform the training sentence into a TURK-
TRAIN sentence divided by the number of words
in the TURKTRAIN sentence. For example, the
training sentence “I didn’t sleep well Monday
night either” was given a WER of 0.33 because
two word-changes transformed it into a message
written by a worker: “I didn’t sleep well last
night”.
</bodyText>
<listItem confidence="0.92315625">
• Cross-entropy, in-domain – The average per-word
cross-entropy of the training sentence under a 3-
gram model trained on TURKTRAIN.
• Cross-entropy, background – The average per-
word cross-entropy of the training sentence un-
der a 3-gram model trained on a random portion
of the training set. The random portion was the
same size as TURKTRAIN.
</listItem>
<bodyText confidence="0.999773181818182">
We used these values to limit training to only
AAC-like sentences. We tried three different selec-
tion methods. In WER selection, only sentences be-
low a threshold on the word error rate were kept in
the training data. This tends to find variants of exist-
ing communications in our Turk collection.
In cross-entropy selection, we used only sen-
tences below a threshold on the per-word cross-
entropy with respect to a TURKTRAIN language
model. This is equivalent to placing a threshold on
the perplexity. Previously this technique has been
used to improve language models based on web data
(Bulyko et al., 2007; Gao et al., 2002) and to con-
struct domain-specific models (Lin et al., 1997).
In cross-entropy difference selection, a sentence’s
score is the in-domain cross-entropy minus the back-
ground cross-entropy (Moore and Lewis, 2010).
This technique has been used to supplement Euro-
pean parliamentary text (48M words) with newswire
data (3.4B words) (Moore and Lewis, 2010). We
were curious how this technique would work given
our much smaller in-domain set of 24K words.
</bodyText>
<subsectionHeader confidence="0.998522">
4.2 Data Selection and Pruning
</subsectionHeader>
<bodyText confidence="0.9999122">
We built models selecting sentences below different
thresholds on the WER, in-domain cross-entropy, or
cross-entropy difference. For comparison, we also
pruned our models using conventional count-cutoff
and entropy pruning (Stolcke, 1998). During en-
tropy pruning, we used a Good-Turing estimated
model for computing the history marginals as the
lower-order Kneser-Ney distributions are unsuitable
for this purpose (Chelba et al., 2010).
We calculated the perplexity of each model on
three test sets. We also tallied the number of model
parameters (all n-gram probabilities plus all backoff
weights). On TURKDEV, cross-entropy difference
selection performed the best for all models sizes and
for all training sets (figure 6). We also found cross-
</bodyText>
<page confidence="0.997408">
706
</page>
<figureCaption confidence="0.8856515">
Figure 7: Perplexity on TURKDEV varying the
cross-entropy difference threshold.
</figureCaption>
<bodyText confidence="0.99986675">
entropy difference was the best on COMM, reducing
perplexity by 10–20% relative compared to cross-
entropy selection. Results on SPECIALISTS showed
that WER and both forms of cross-entropy selection
performed similarly. All three data selection meth-
ods were superior to count-cutoff or entropy prun-
ing. We use cross-entropy difference selection for
the remainder of this paper.
</bodyText>
<subsectionHeader confidence="0.999956">
4.3 Model Order and Optimal Thresholds
</subsectionHeader>
<bodyText confidence="0.999974">
We created 2-gram, 3-gram and 4-gram models on
TWITTER, BLOG, and USENET using a range of
cross-entropy difference thresholds. 4-gram models
slightly outperformed 3-gram models (figure 7). The
optimal threshold for 4-gram models were as fol-
lows: TWITTER 0.0, BLOG -0.4, and USENET -0.7.
These thresholds resulted in using 20% of TWIT-
TER, 5% of BLOG, and 1% of USENET.
</bodyText>
<subsectionHeader confidence="0.993282">
4.4 Mixture Model
</subsectionHeader>
<bodyText confidence="0.997632111111111">
We created a mixture model using linear interpo-
lation from the TWITTER, USENET and BLOG 4-
gram models created with each set’s optimal thresh-
old. The mixture weights were optimized with re-
spect to TURKDEV using SRILM. The final mix-
ture weights were: TWITTER 0.42, BLOG 0.29, and
USENET 0.29. Our final 4-gram mixture model had
43M total parameters and a compressed disk size of
316 MB.
</bodyText>
<sectionHeader confidence="0.995946" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.8623945">
In this section, we compare our mixture model
against baseline models. We show performance with
</bodyText>
<page confidence="0.985483">
707
</page>
<bodyText confidence="0.992721">
respect to usage in a typical AAC text entry interface
based on word prediction.
</bodyText>
<subsectionHeader confidence="0.993225">
5.1 Predictive Text Entry
</subsectionHeader>
<bodyText confidence="0.999982807692308">
Many AAC communication devices use word pre-
dictions. In a word prediction interface users type
letters and the interface offers word completions
based on the prefix of the current word and often the
prior text. By selecting one of the predictions, the
user can potentially save keystrokes as compared to
typing out every letter of each word.
We assume a hypothetical predictive keyboard in-
terface that displays five word predictions. Our key-
board makes predictions based on up to three words
of prior context. Our keyboard predicts words even
before the first letter of a new word is typed. As
a user types letters, predictions are limited to words
consistent with the typed letters. If the system makes
a correct prediction, we assume it takes only one
keystroke to enter the word and any following space.
We only predict words in our 63K word vocab-
ulary (empty prediction slots are possible). We dis-
play a word even if it was already a proposed predic-
tion for a shorter prefix of the current word. The first
word in a sentence is conditioned on the sentence-
start pseudo-word. If an out-of-vocabulary word is
typed, the word is replaced in the language model’s
context with the unknown pseudo-word.
We evaluate our predictive keyboard using the
common metric of keystroke savings (KS):
</bodyText>
<equation confidence="0.95812">
I Ikp ))
KS = 1 − × 100%,
ka
</equation>
<bodyText confidence="0.999917333333333">
where kp is the number of keystrokes required with
word predictions and ka is the number of keystrokes
required without word prediction.
</bodyText>
<subsectionHeader confidence="0.998203">
5.2 Predictive Performance Experiment
</subsectionHeader>
<bodyText confidence="0.9994128">
We compared our mixture model using cross-
entropy difference selection with three baseline
models trained on all of TWITTER, SWITCHBOARD
and TURKTRAIN. The baseline models were un-
pruned 4-gram models trained using interpolated
modified Kneser-Ney smoothing. They had 72M,
5M, and 129K parameters respectively.
As shown in table 4, our mixture model per-
formed the best on the three most AAC-like test
sets (COMM, SPECIALISTS, and TURKTEST). The
</bodyText>
<figure confidence="0.899862588235294">
−1 0 1 −1 0 1 −1.4 −0.4 0.4
Threshold Threshold Threshold
(a) TWITTER (b) BLOG (c) USENET
Perplexity
40 50 60 70 80 90 100
●
●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
2−gram
● 3−gram
4−gram
2−gram
● 3−gram
4−gram
2−gram
● 3−gram
4−gram
</figure>
<table confidence="0.997212">
LM Test set PPL KS
Mixture COMM 47.9 62.5%
Twitter COMM 55.9 60.9%
Switchboard COMM 151.1 54.4%
Turk COMM 165.9 52.7%
Mixture SPECIALISTS 25.7 63.1%
Twitter SPECIALISTS 27.3 61.9%
Switchboard SPECIALISTS 64.5 57.7%
Turk SPECIALISTS 85.9 52.8%
Mixture TURKTEST 31.2 62.0%
Twitter TURKTEST 42.3 59.3%
Switchboard TURKTEST 172.5 50.6%
Turk TURKTEST 51.0 57.6%
Mixture SWITCHTEST 174.3 52.8%
Twitter SWITCHTEST 142.6 54.9%
Switchboard SWITCHTEST 79.2 58.8%
Turk SWITCHTEST 642.5 42.9%
</table>
<tableCaption confidence="0.988088">
Table 4: Perplexity (PPL) and keystroke savings
</tableCaption>
<bodyText confidence="0.999239833333334">
(KS) of different language models on four test sets.
The bold line shows the best performing language
model on each test set.
mixture model provided substantial increases in
keystroke savings compared to a model trained
solely on Switchboard. The mixture model also per-
formed better than simply training a model on a
large amount of Twitter data. The model trained on
only 24K words of Turk data did surprisingly well
given its extremely limited training data.
Our Switchboard model performed the best on
SWITCHTEST with a keystroke savings of 58.8%.
For comparison, past work reported a keystroke sav-
ings of 55.7% on SWITCHTEST using a 3-gram
model trained on Switchboard (Trnka et al., 2009).
While our mixture model performed less well on
SWITCHTEST (52.8%), it is likely the other three
test sets better represent AAC communications.
</bodyText>
<subsectionHeader confidence="0.973221">
5.3 Larger Mixture Model Experiment
</subsectionHeader>
<bodyText confidence="0.999822">
Our mixture language model used the best thresh-
olds with respect to TURKDEV. This resulted in
throwing away most of the training data. This might
be suboptimal in practice if an AAC user’s com-
munications are somewhat different or more diverse
than the language generated by the Turk workers.
We trained a series of mixture models in which
we varied the cross-entropy difference thresholds
</bodyText>
<figure confidence="0.641151">
Change from optimal thresholds
</figure>
<figureCaption confidence="0.599225666666667">
Figure 8: Keystroke savings on mixture models
varying a constant added to the optimal thresholds
with respect to TURKDEV.
</figureCaption>
<bodyText confidence="0.999983333333333">
by adding a constant to all three thresholds. The
mixture weights for each new model were opti-
mized with respect to TURKDEV. Using somewhat
larger models did improve keystroke savings for all
test sets except for TURKTEST (figure 8). How-
ever, using too large thresholds eventually hurt per-
formance except on SWITCHTEST. Performance
on SWITCHTEST steadily increased from 52.8% to
56.6%. These gains however came at the cost of big-
ger models. The model using +1.0 of the optimal
thresholds had 384M parameters and a compressed
size of 3.0 GB.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999994727272727">
Given the ethical implications of collecting mes-
sages from actual AAC users, it is unlikely that a
large corpus of genuine AAC messages will ever be
available to researchers. An important finding in
this paper is that crowdsourcing can be an effective
way to obtain surrogate data for improving AAC lan-
guage models. Another finding is that Twitter pro-
vides a continuous stream of large amounts of very
AAC-like data. Twitter also has the advantage of al-
lowing models to be continually updated to reflect
current events, new vocabulary, etc.
</bodyText>
<subsectionHeader confidence="0.990574">
6.1 Limitations and Implications
</subsectionHeader>
<bodyText confidence="0.999768">
We collected data from a large number of workers,
some of whom may have written only a single com-
</bodyText>
<figure confidence="0.998212333333333">
−0.5 0.0 0.5 1.0
Keystroke savings (%)
50 55 60 65
● ● ●
●
● ● ●
●
●
● ● ● ● ●
●
Specialist
Comm
TurkTest
SwitchTest
●
</figure>
<page confidence="0.982049">
708
</page>
<bodyText confidence="0.993824673469388">
munication. This may have resulted in more mes- 7 Conclusions
sages about simple situations and perceived needs In this paper we have shown how workers’ creativity
which could differ from true AAC usage. on a microtask crowdsourcing market can be used
Our data does not contain long-term two-sided to create fictional but plausible AAC communica-
conversations. Thus it may not be as useful for eval- tions. We have demonstrated that these messages
uating techniques that adapt to past messages or that model conversational AAC better than the currently
use the conversation partner’s communications. used datasets based on telephone conversations or
We asked workers to imagine they were using newswire text. We used our new crowdsourced
a scanning-style AAC device. We believe this led dataset to intelligently select sentences from Twit-
workers to presume they would require assistance ter, blog and Usenet data.
in many routine physical tasks. Our workers were We compared a variety of different techniques for
(presumably) without cognitive or language impair- intelligent training data selection. We found that
ments. Thus our collection is more representative even for our small amount of in-domain data, the
of one subgroup of AAC communicators (scanning recently proposed cross-entropy difference method
users with normal cognitive function and language was consistently the best (Moore and Lewis, 2010).
skills). By modifying the situation given to workers, Finally, compared to a model trained only on
it is likely we can expand our collection to better rep- Switchboard, our best performing model reduced
resent other groups of AAC users, such as those us- perplexity by 60-82% relative on three AAC-like test
ing predictive keyboards or eye-trackers. However, sets. This translated to a potential keystroke savings
obtaining data representative of users with cognitive in a predictive keyboard interface of 5–11%.
or language impairments via crowdsourcing would In conclusion, we have shown how to create long-
probably be difficult. span AAC language models using openly avail-
While we were unable to obtain real AAC mes- able resources. Our models significantly outperform
sages for testing, we believe the COMM and SPE- models trained on the commonly used data sources
CIALIST test sets provide a good indication of the of telephone transcripts and newswire text. To aid
real-world potential for our methods. Our collected other researchers, we have publicly released our
Turk data was compared with reported data from ac- crowdsourced AAC collection, word lists and best-
tual AAC users (though this comparison was neces- performing models. We hope complementary tech-
sarily coarse-grained). We hope that by releasing niques such as topic modeling and language model
our data and models it may be possible for those adaptation will provide additive gains to those ob-
privy to real AAC communications to validate and tained by training models on large amounts of AAC-
report about the techniques described in this paper. like data. We plan to use our models to design and
We evaluated our models in terms of perplexity test new interfaces that enable faster communication
and keystrokes savings within the auspices of a pre- for AAC users.
dictive keyboard. Further work is needed to inves- Acknowledgments
tigate how our numeric gains translate to real-world We thank Keith Trnka and Horabail Venkatagiri for
benefits to users. However, past work indicates more their assistance. This work was supported by the En-
accurate predictions do in fact yield improvements gineering and Physical Sciences Research Council
in human performance (Trnka et al., 2009). (grant number EP/H027408/1).
Finally, while the predictive keyboard is a com- References
monly studied interface, it is not appropriate for all Bruce Baker, Katya Hill, and Richard Devylder. 2000.
AAC users. Eye-tracker users may prefer an in- Core vocabulary is the same across environments. In
terface such as Dasher (Ward and MacKay, 2002). California State University at Northridge Conference.
Single-switch users may prefer an interface such as David R. Beukelman, Kathryn M. Yorkston, Miguel
Nomon (Broderick and MacKay, 2009). Any AAC Poblete, and Carlos Naranjo. 1984. Frequency of
interface based on word- or letter-based predictions
stands to benefit from the methods described in this
paper.
709
</bodyText>
<reference confidence="0.998560752380952">
word occurrence in communication samples produced
by adult communication aid users. Journal of Speech
and Hearing Disorders, 49:360–367.
Tamara Broderick and David J. C. MacKay. 2009. Fast
and flexible selection with a single switch. PLoS ONE,
4(10):e7481.
Ivan Bulyko, Mari Ostendorf, Manhung Siu, Tim Ng,
Andreas Stolcke, and ¨Ozg¨ur C¸etin. 2007. Web
resources for language modeling in conversational
speech recognition. ACM Transactions on Speech and
Language Processing, 5(1):1–25.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r dataset. In Proceedings of the
3rd Annual Conference on Weblogs and Social Media.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and Kneser-Ney smoothing. In Proceedings of the
International Conference on Spoken Language Pro-
cessing, pages 2422–2425.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical report, Computer Science Group,
Harvard University.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statistical
language modeling for chinese. ACM Transactions on
Asian Language Information Processing, 1:3–33.
Nestor Garay-Vitoria and Julio Abascal. 2006. Text pre-
diction systems: A survey. Universal Access in the
Information Society, 4:188–203.
Nestor Garay-Vitoria and Julio Gonz´alez-Abascal. 1997.
Intelligent word-prediction to enhance text input rate.
In Proceedings of the 2nd ACM International Confer-
ence on Intelligent User Interfaces, pages 241–244.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. Proceedings of the IEEE
Conference on Acoustics, Speech, and Signal Process-
ing, pages 517–520.
David Graff, Roni Rosenfeld, and Doug Pau. 1995.
CSR-III text. Linguistic Data Consortium, Philadel-
phia, PA, USA.
David Graff. 2003. English gigaword corpus. Linguistic
Data Consortium, Philadelphia, PA, USA.
Sheri Hunnicutt. 1989. Using syntactic and semantic in-
formation in a word prediction aid. In Proceedings of
the 1st European Conference on Speech Communica-
tion and Technology, pages 1191–1193.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proceedings of the IEEE Conference on Acoustics,
Speech, and Signal Processing, pages 181–184.
Gregory W. Lesher and Gerard J. Rinkus. 2002.
Domain-specific word prediction for augmentative
communication. In Proceedings of the RESNA 2002
Annual Conference.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In Proceedings of the 7th
International ACM SIGACCESS Conference on Com-
puters and Accessibility, pages 121–128.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Ker-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document classifi-
cation and multiple domain-specific language models.
In Proceedings of the 5th European Conference on
Speech Communication and Technology, pages 1463–
1466.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the 48th Annual Meeting of the Association of
Computational Linguistics, pages 220–224.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL 2010, pages 172–180.
Cyrus Shaoul and Chris Westbury. 2009. A USENET
corpus (2005-2009). University of Alberta, Canada.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270–274.
Andreas Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proceedings of the 7th Annual In-
ternational Conference on Spoken Language Process-
ing, pages 901–904.
Keith Trnka, Debra Yarrington, and Christopher Penning-
ton. 2006. Topic modeling in fringe word prediction
for AAC. In Proceedings of the 11th ACM Interna-
tional Conference on Intelligent User Interfaces, pages
276–278.
Keith Trnka, John McCaw, Debra Yarrington, Kathleen F.
McCoy, and Christopher Pennington. 2009. User in-
teraction with word prediction: The effects of predic-
tion quality. ACM Transactions on Accessible Com-
puting, 1:17:1–17:34.
Keith Trnka. 2008. Adaptive language modeling for
word prediction. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics on Human Language Technologies: Student Re-
search Workshop, pages 61–66.
Horabail Venkatagiri. 1999. Efficient keyboard layouts
for sequential access in augmentative and alternative
communication. Augmentative and Alternative Com-
munication, 15(2):126–134.
Tonio Wandmacher and Jean-Yves Antoine. 2007.
Methods to integrate a language model with semantic
</reference>
<page confidence="0.963682">
710
</page>
<reference confidence="0.986937181818182">
information for a word prediction component. Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 506–513.
Tonio Wandmacher, Jean-Yves Antoine, Franck Poirier,
and Jean-Paul D´eparte. 2008. SIBYLLE, an assis-
tive communication system adapting to the context and
its user. ACM Transactions on Accessible Computing,
1:6:1–6:30.
D. J. Ward and D. J. C. MacKay. 2002. Fast hands-free
writing by gaze direction. Nature, 418(6900):838.
</reference>
<page confidence="0.997928">
711
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912181">
<title confidence="0.9998885">The Imagination of Crowds: Conversational AAC Language Modeling using Crowdsourcing and Large Data Sources</title>
<author confidence="0.999749">Keith Vertanen Per Ola Kristensson</author>
<affiliation confidence="0.999484">Department of Computer Science School of Computer Science Princeton University University of St Andrews</affiliation>
<email confidence="0.984442">vertanen@princeton.edupok@st-andrews.ac.uk</email>
<abstract confidence="0.997301888888889">Augmented and alternative communication (AAC) devices enable users with certain communication disabilities to participate in everyday conversations. Such devices often rely on statistical language models to improve text entry by offering word predictions. These predictions can be improved if the language model is trained on data that closely reflects the style of the users’ intended communications. Unfortunately, there is no large dataset consisting of genuine AAC messages. In this paper we demonstrate how we can crowdsource the creation of a large set of fictional AAC messages. We show that these messages model conversational AAC better than the currently used datasets based on telephone conversations or newswire text. We leverage our crowdsourced messages to intelligently select sentences from much larger sets of Twitter, blog and Usenet data. Compared to a model trained only on telephone transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–11%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>word occurrence in communication samples produced by adult communication aid users.</title>
<journal>Journal of Speech and Hearing Disorders,</journal>
<pages>49--360</pages>
<marker></marker>
<rawString>word occurrence in communication samples produced by adult communication aid users. Journal of Speech and Hearing Disorders, 49:360–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Broderick</author>
<author>David J C MacKay</author>
</authors>
<title>Fast and flexible selection with a single switch.</title>
<date>2009</date>
<journal>PLoS ONE,</journal>
<volume>4</volume>
<issue>10</issue>
<marker>Broderick, MacKay, 2009</marker>
<rawString>Tamara Broderick and David J. C. MacKay. 2009. Fast and flexible selection with a single switch. PLoS ONE, 4(10):e7481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Bulyko</author>
<author>Mari Ostendorf</author>
<author>Manhung Siu</author>
<author>Tim Ng</author>
<author>Andreas Stolcke</author>
<author>¨Ozg¨ur C¸etin</author>
</authors>
<title>Web resources for language modeling in conversational speech recognition.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Bulyko, Ostendorf, Siu, Ng, Stolcke, C¸etin, 2007</marker>
<rawString>Ivan Bulyko, Mari Ostendorf, Manhung Siu, Tim Ng, Andreas Stolcke, and ¨Ozg¨ur C¸etin. 2007. Web resources for language modeling in conversational speech recognition. ACM Transactions on Speech and Language Processing, 5(1):1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Burton</author>
<author>Akshay Java</author>
<author>Ian Soboroff</author>
</authors>
<date>2009</date>
<booktitle>The ICWSM 2009 Spinn3r dataset. In Proceedings of the 3rd Annual Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="10289" citStr="Burton et al., 2009" startWordPosition="1633" endWordPosition="1636">rd corpora (Graff, 2003). 60M sentences, 1323M words. • WIKIPEDIA – Current articles and discussion threads from a snapshot of Wikipedia (January 3, 2008). 24M sentences, 452M words. • USENET – Messages from a Usenet corpus (Shaoul and Westbury, 2009). 123M sentences, 1847M words. • SWITCHBOARD – Transcripts of 2217 telephone conversations from the Switchboard corpus (Godfrey et al., 1992). Due to its conversational style, this corpus has been popular for AAC language modeling (Lesher and Rinkus, 2002; Trnka et al., 2009). 0.2M sentences, 2.6M words. • BLOG – Blog posts from the ICWSM corpus (Burton et al., 2009). 25M sentences, 387M words. 2We combined Wiktionary, Webster’s dictionary provided by Project Gutenberg, the CMU pronouncing dictionary and GNU aspell. 702 • TWITTER – We collected Twitter messages via the streaming API between December 2010 and March 2011. We used the free Twitter stream which provides access to 5% of all tweets. Twitter may be particularly well suited for modeling AAC communications as tweets are short typed messages that are often informal person-to-person communications. Twitter has previously been proposed as a candidate for modeling conversations, see for example Ritter</context>
</contexts>
<marker>Burton, Java, Soboroff, 2009</marker>
<rawString>Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r dataset. In Proceedings of the 3rd Annual Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Thorsten Brants</author>
<author>Will Neveitt</author>
<author>Peng Xu</author>
</authors>
<title>Study on interaction between entropy pruning and Kneser-Ney smoothing.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>2422--2425</pages>
<contexts>
<context position="23392" citStr="Chelba et al., 2010" startWordPosition="3874" endWordPosition="3877">with newswire data (3.4B words) (Moore and Lewis, 2010). We were curious how this technique would work given our much smaller in-domain set of 24K words. 4.2 Data Selection and Pruning We built models selecting sentences below different thresholds on the WER, in-domain cross-entropy, or cross-entropy difference. For comparison, we also pruned our models using conventional count-cutoff and entropy pruning (Stolcke, 1998). During entropy pruning, we used a Good-Turing estimated model for computing the history marginals as the lower-order Kneser-Ney distributions are unsuitable for this purpose (Chelba et al., 2010). We calculated the perplexity of each model on three test sets. We also tallied the number of model parameters (all n-gram probabilities plus all backoff weights). On TURKDEV, cross-entropy difference selection performed the best for all models sizes and for all training sets (figure 6). We also found cross706 Figure 7: Perplexity on TURKDEV varying the cross-entropy difference threshold. entropy difference was the best on COMM, reducing perplexity by 10–20% relative compared to crossentropy selection. Results on SPECIALISTS showed that WER and both forms of cross-entropy selection performed </context>
</contexts>
<marker>Chelba, Brants, Neveitt, Xu, 2010</marker>
<rawString>Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng Xu. 2010. Study on interaction between entropy pruning and Kneser-Ney smoothing. In Proceedings of the International Conference on Spoken Language Processing, pages 2422–2425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua T Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Computer Science Group, Harvard University.</institution>
<contexts>
<context position="13607" citStr="Chen and Goodman, 1998" startWordPosition="2155" endWordPosition="2158">able 3 shows some examples from the other three test sets. Sentences in COMM tended to be richer in vocabulary and subject matter than those in SPECIALISTS. The SPECIALISTS sentences tended to be general phrases that avoided mentioning specific situations, proper names, etc. Sentences in SWITCHTEST exhibited phenomena typical of human-to-human voice conversations (filler words, backchannels, interruptions, etc). 3.2 Language Model Training All language models were trained using the SRILM toolkit (Stolcke, 2002). All models used interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). In this section, we trained 3-gram language models with no count-cutoffs. All text was converted to lowercase and we removed punctuation except for apostrophes. We believe punctuation would likely slow down a user’s conversation for only a small potential advantage (e.g. improving text-to-speech prosody). All models used a vocabulary of 63K words including an unknown word. We obtained our vocabulary by taking all words occurring in TURKTRAIN and all words occurring four or more times in the TWITTER training set. We restricted our vocabulary to words from our large list of 330K words. This re</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua T. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Joshua Goodman</author>
<author>Mingjing Li</author>
<author>KaiFu Lee</author>
</authors>
<title>Toward a unified approach to statistical language modeling for chinese.</title>
<date>2002</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<pages>1--3</pages>
<contexts>
<context position="22479" citStr="Gao et al., 2002" startWordPosition="3740" endWordPosition="3743">d these values to limit training to only AAC-like sentences. We tried three different selection methods. In WER selection, only sentences below a threshold on the word error rate were kept in the training data. This tends to find variants of existing communications in our Turk collection. In cross-entropy selection, we used only sentences below a threshold on the per-word crossentropy with respect to a TURKTRAIN language model. This is equivalent to placing a threshold on the perplexity. Previously this technique has been used to improve language models based on web data (Bulyko et al., 2007; Gao et al., 2002) and to construct domain-specific models (Lin et al., 1997). In cross-entropy difference selection, a sentence’s score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010). This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). We were curious how this technique would work given our much smaller in-domain set of 24K words. 4.2 Data Selection and Pruning We built models selecting sentences below different thresholds on the WER, in-domain cross-entropy, or cross-entropy diffe</context>
</contexts>
<marker>Gao, Goodman, Li, Lee, 2002</marker>
<rawString>Jianfeng Gao, Joshua Goodman, Mingjing Li, and KaiFu Lee. 2002. Toward a unified approach to statistical language modeling for chinese. ACM Transactions on Asian Language Information Processing, 1:3–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nestor Garay-Vitoria</author>
<author>Julio Abascal</author>
</authors>
<title>Text prediction systems: A survey.</title>
<date>2006</date>
<booktitle>Universal Access in the Information Society,</booktitle>
<pages>4--188</pages>
<contexts>
<context position="2394" citStr="Garay-Vitoria and Abascal (2006)" startWordPosition="350" endWordPosition="353">ow, between 0.5 and 16 words-per-minute (Trnka et al., 2009). As a consequence, researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques. Examples of approaches include adapting the language model to recently used words (Wandmacher et al., 2008; Trnka, 2008), using syntactic information (Hunnicutt, 1989; Garay-Vitoria and Gonz´alez-Abascal, 1997), using semantic information (Wandmacher and Antoine, 2007; Li and Hirst, 2005), and modeling topics (Lesher and Rinkus, 2002; Trnka et al., 2006). For a recent survey, see Garay-Vitoria and Abascal (2006). While such language model improvement techniques are undoubtedly helpful, certainly they can all benefit from starting with a long-span language model trained on large amounts of closely matched data. For AAC devices this means closely modeling everyday face-to-face communications. However, a long-standing problem in the field is the lack of good data sources that adequately model such AAC communications. Due to privacy-reasons and other ethical concerns, there is no large dataset consisting of genuine AAC messages. Therefore, previous research has used transcripts of telephone conversations</context>
</contexts>
<marker>Garay-Vitoria, Abascal, 2006</marker>
<rawString>Nestor Garay-Vitoria and Julio Abascal. 2006. Text prediction systems: A survey. Universal Access in the Information Society, 4:188–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nestor Garay-Vitoria</author>
<author>Julio Gonz´alez-Abascal</author>
</authors>
<title>Intelligent word-prediction to enhance text input rate.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd ACM International Conference on Intelligent User Interfaces,</booktitle>
<pages>241--244</pages>
<marker>Garay-Vitoria, Gonz´alez-Abascal, 1997</marker>
<rawString>Nestor Garay-Vitoria and Julio Gonz´alez-Abascal. 1997. Intelligent word-prediction to enhance text input rate. In Proceedings of the 2nd ACM International Conference on Intelligent User Interfaces, pages 241–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Godfrey</author>
<author>E C Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="10061" citStr="Godfrey et al., 1992" startWordPosition="1593" endWordPosition="1597">he predictive performance of language models trained on our Turk AAC data with models trained on other text sources. We use the following training sets: • NEWS – Newspaper articles from the CSR-III (Graff et al., 1995) and Gigaword corpora (Graff, 2003). 60M sentences, 1323M words. • WIKIPEDIA – Current articles and discussion threads from a snapshot of Wikipedia (January 3, 2008). 24M sentences, 452M words. • USENET – Messages from a Usenet corpus (Shaoul and Westbury, 2009). 123M sentences, 1847M words. • SWITCHBOARD – Transcripts of 2217 telephone conversations from the Switchboard corpus (Godfrey et al., 1992). Due to its conversational style, this corpus has been popular for AAC language modeling (Lesher and Rinkus, 2002; Trnka et al., 2009). 0.2M sentences, 2.6M words. • BLOG – Blog posts from the ICWSM corpus (Burton et al., 2009). 25M sentences, 387M words. 2We combined Wiktionary, Webster’s dictionary provided by Project Gutenberg, the CMU pronouncing dictionary and GNU aspell. 702 • TWITTER – We collected Twitter messages via the streaming API between December 2010 and March 2011. We used the free Twitter stream which provides access to 5% of all tweets. Twitter may be particularly well suite</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pages 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Roni Rosenfeld</author>
<author>Doug Pau</author>
</authors>
<date>1995</date>
<booktitle>CSR-III text. Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="9658" citStr="Graff et al., 1995" startWordPosition="1532" endWordPosition="1535">so late? I am pretty hungry, can we go eat? I had bacon eggs and hashbrowns for breakfast. Table 2: Example communications from type 2. The text in bold is the message workers judged. It is followed in plain text by the workers’ new messages. very unfamiliar situation, it appears that providing a concrete example was helpful to workers. 3 Comparison of Training Sources In this section, we compare the predictive performance of language models trained on our Turk AAC data with models trained on other text sources. We use the following training sets: • NEWS – Newspaper articles from the CSR-III (Graff et al., 1995) and Gigaword corpora (Graff, 2003). 60M sentences, 1323M words. • WIKIPEDIA – Current articles and discussion threads from a snapshot of Wikipedia (January 3, 2008). 24M sentences, 452M words. • USENET – Messages from a Usenet corpus (Shaoul and Westbury, 2009). 123M sentences, 1847M words. • SWITCHBOARD – Transcripts of 2217 telephone conversations from the Switchboard corpus (Godfrey et al., 1992). Due to its conversational style, this corpus has been popular for AAC language modeling (Lesher and Rinkus, 2002; Trnka et al., 2009). 0.2M sentences, 2.6M words. • BLOG – Blog posts from the ICW</context>
</contexts>
<marker>Graff, Rosenfeld, Pau, 1995</marker>
<rawString>David Graff, Roni Rosenfeld, and Doug Pau. 1995. CSR-III text. Linguistic Data Consortium, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English gigaword corpus. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="9693" citStr="Graff, 2003" startWordPosition="1539" endWordPosition="1540">? I had bacon eggs and hashbrowns for breakfast. Table 2: Example communications from type 2. The text in bold is the message workers judged. It is followed in plain text by the workers’ new messages. very unfamiliar situation, it appears that providing a concrete example was helpful to workers. 3 Comparison of Training Sources In this section, we compare the predictive performance of language models trained on our Turk AAC data with models trained on other text sources. We use the following training sets: • NEWS – Newspaper articles from the CSR-III (Graff et al., 1995) and Gigaword corpora (Graff, 2003). 60M sentences, 1323M words. • WIKIPEDIA – Current articles and discussion threads from a snapshot of Wikipedia (January 3, 2008). 24M sentences, 452M words. • USENET – Messages from a Usenet corpus (Shaoul and Westbury, 2009). 123M sentences, 1847M words. • SWITCHBOARD – Transcripts of 2217 telephone conversations from the Switchboard corpus (Godfrey et al., 1992). Due to its conversational style, this corpus has been popular for AAC language modeling (Lesher and Rinkus, 2002; Trnka et al., 2009). 0.2M sentences, 2.6M words. • BLOG – Blog posts from the ICWSM corpus (Burton et al., 2009). 25</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English gigaword corpus. Linguistic Data Consortium, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheri Hunnicutt</author>
</authors>
<title>Using syntactic and semantic information in a word prediction aid.</title>
<date>1989</date>
<booktitle>In Proceedings of the 1st European Conference on Speech Communication and Technology,</booktitle>
<pages>1191--1193</pages>
<contexts>
<context position="2145" citStr="Hunnicutt, 1989" startWordPosition="315" endWordPosition="316"> (AAC) devices to take part in everyday conversations. Often these devices consist of a predictive text input method coupled with text-to-speech output. Unfortunately, the text entry rates provided by 700 AAC devices are typically low, between 0.5 and 16 words-per-minute (Trnka et al., 2009). As a consequence, researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques. Examples of approaches include adapting the language model to recently used words (Wandmacher et al., 2008; Trnka, 2008), using syntactic information (Hunnicutt, 1989; Garay-Vitoria and Gonz´alez-Abascal, 1997), using semantic information (Wandmacher and Antoine, 2007; Li and Hirst, 2005), and modeling topics (Lesher and Rinkus, 2002; Trnka et al., 2006). For a recent survey, see Garay-Vitoria and Abascal (2006). While such language model improvement techniques are undoubtedly helpful, certainly they can all benefit from starting with a long-span language model trained on large amounts of closely matched data. For AAC devices this means closely modeling everyday face-to-face communications. However, a long-standing problem in the field is the lack of good </context>
</contexts>
<marker>Hunnicutt, 1989</marker>
<rawString>Sheri Hunnicutt. 1989. Using syntactic and semantic information in a word prediction aid. In Proceedings of the 1st European Conference on Speech Communication and Technology, pages 1191–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="13582" citStr="Kneser and Ney, 1995" startWordPosition="2151" endWordPosition="2154">ar to table 1 and 2. Table 3 shows some examples from the other three test sets. Sentences in COMM tended to be richer in vocabulary and subject matter than those in SPECIALISTS. The SPECIALISTS sentences tended to be general phrases that avoided mentioning specific situations, proper names, etc. Sentences in SWITCHTEST exhibited phenomena typical of human-to-human voice conversations (filler words, backchannels, interruptions, etc). 3.2 Language Model Training All language models were trained using the SRILM toolkit (Stolcke, 2002). All models used interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). In this section, we trained 3-gram language models with no count-cutoffs. All text was converted to lowercase and we removed punctuation except for apostrophes. We believe punctuation would likely slow down a user’s conversation for only a small potential advantage (e.g. improving text-to-speech prosody). All models used a vocabulary of 63K words including an unknown word. We obtained our vocabulary by taking all words occurring in TURKTRAIN and all words occurring four or more times in the TWITTER training set. We restricted our vocabulary to words from our large li</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory W Lesher</author>
<author>Gerard J Rinkus</author>
</authors>
<title>Domain-specific word prediction for augmentative communication.</title>
<date>2002</date>
<booktitle>In Proceedings of the RESNA 2002 Annual Conference.</booktitle>
<contexts>
<context position="2314" citStr="Lesher and Rinkus, 2002" startWordPosition="336" endWordPosition="339">ately, the text entry rates provided by 700 AAC devices are typically low, between 0.5 and 16 words-per-minute (Trnka et al., 2009). As a consequence, researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques. Examples of approaches include adapting the language model to recently used words (Wandmacher et al., 2008; Trnka, 2008), using syntactic information (Hunnicutt, 1989; Garay-Vitoria and Gonz´alez-Abascal, 1997), using semantic information (Wandmacher and Antoine, 2007; Li and Hirst, 2005), and modeling topics (Lesher and Rinkus, 2002; Trnka et al., 2006). For a recent survey, see Garay-Vitoria and Abascal (2006). While such language model improvement techniques are undoubtedly helpful, certainly they can all benefit from starting with a long-span language model trained on large amounts of closely matched data. For AAC devices this means closely modeling everyday face-to-face communications. However, a long-standing problem in the field is the lack of good data sources that adequately model such AAC communications. Due to privacy-reasons and other ethical concerns, there is no large dataset consisting of genuine AAC messag</context>
<context position="10175" citStr="Lesher and Rinkus, 2002" startWordPosition="1612" endWordPosition="1615">ces. We use the following training sets: • NEWS – Newspaper articles from the CSR-III (Graff et al., 1995) and Gigaword corpora (Graff, 2003). 60M sentences, 1323M words. • WIKIPEDIA – Current articles and discussion threads from a snapshot of Wikipedia (January 3, 2008). 24M sentences, 452M words. • USENET – Messages from a Usenet corpus (Shaoul and Westbury, 2009). 123M sentences, 1847M words. • SWITCHBOARD – Transcripts of 2217 telephone conversations from the Switchboard corpus (Godfrey et al., 1992). Due to its conversational style, this corpus has been popular for AAC language modeling (Lesher and Rinkus, 2002; Trnka et al., 2009). 0.2M sentences, 2.6M words. • BLOG – Blog posts from the ICWSM corpus (Burton et al., 2009). 25M sentences, 387M words. 2We combined Wiktionary, Webster’s dictionary provided by Project Gutenberg, the CMU pronouncing dictionary and GNU aspell. 702 • TWITTER – We collected Twitter messages via the streaming API between December 2010 and March 2011. We used the free Twitter stream which provides access to 5% of all tweets. Twitter may be particularly well suited for modeling AAC communications as tweets are short typed messages that are often informal person-to-person comm</context>
</contexts>
<marker>Lesher, Rinkus, 2002</marker>
<rawString>Gregory W. Lesher and Gerard J. Rinkus. 2002. Domain-specific word prediction for augmentative communication. In Proceedings of the RESNA 2002 Annual Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Li</author>
<author>Graeme Hirst</author>
</authors>
<title>Semantic knowledge in word completion.</title>
<date>2005</date>
<booktitle>In Proceedings of the 7th International ACM SIGACCESS Conference on Computers and Accessibility,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="2268" citStr="Li and Hirst, 2005" startWordPosition="329" endWordPosition="332">upled with text-to-speech output. Unfortunately, the text entry rates provided by 700 AAC devices are typically low, between 0.5 and 16 words-per-minute (Trnka et al., 2009). As a consequence, researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques. Examples of approaches include adapting the language model to recently used words (Wandmacher et al., 2008; Trnka, 2008), using syntactic information (Hunnicutt, 1989; Garay-Vitoria and Gonz´alez-Abascal, 1997), using semantic information (Wandmacher and Antoine, 2007; Li and Hirst, 2005), and modeling topics (Lesher and Rinkus, 2002; Trnka et al., 2006). For a recent survey, see Garay-Vitoria and Abascal (2006). While such language model improvement techniques are undoubtedly helpful, certainly they can all benefit from starting with a long-span language model trained on large amounts of closely matched data. For AAC devices this means closely modeling everyday face-to-face communications. However, a long-standing problem in the field is the lack of good data sources that adequately model such AAC communications. Due to privacy-reasons and other ethical concerns, there is no </context>
</contexts>
<marker>Li, Hirst, 2005</marker>
<rawString>Jianhua Li and Graeme Hirst. 2005. Semantic knowledge in word completion. In Proceedings of the 7th International ACM SIGACCESS Conference on Computers and Accessibility, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung-Chien Lin</author>
<author>Chi-Lung Tsai</author>
<author>Lee-Feng Chien</author>
<author>KerJiann Chen</author>
<author>Lin-Shan Lee</author>
</authors>
<title>Chinese language model adaptation based on document classification and multiple domain-specific language models.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology,</booktitle>
<pages>1463--1466</pages>
<contexts>
<context position="22538" citStr="Lin et al., 1997" startWordPosition="3750" endWordPosition="3753">. We tried three different selection methods. In WER selection, only sentences below a threshold on the word error rate were kept in the training data. This tends to find variants of existing communications in our Turk collection. In cross-entropy selection, we used only sentences below a threshold on the per-word crossentropy with respect to a TURKTRAIN language model. This is equivalent to placing a threshold on the perplexity. Previously this technique has been used to improve language models based on web data (Bulyko et al., 2007; Gao et al., 2002) and to construct domain-specific models (Lin et al., 1997). In cross-entropy difference selection, a sentence’s score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010). This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). We were curious how this technique would work given our much smaller in-domain set of 24K words. 4.2 Data Selection and Pruning We built models selecting sentences below different thresholds on the WER, in-domain cross-entropy, or cross-entropy difference. For comparison, we also pruned our models using conv</context>
</contexts>
<marker>Lin, Tsai, Chien, Chen, Lee, 1997</marker>
<rawString>Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, KerJiann Chen, and Lin-Shan Lee. 1997. Chinese language model adaptation based on document classification and multiple domain-specific language models. In Proceedings of the 5th European Conference on Speech Communication and Technology, pages 1463– 1466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>220--224</pages>
<contexts>
<context position="4343" citStr="Moore and Lewis, 2010" startWordPosition="646" endWordPosition="649">de-range of other text sources. Further, we demonstrate that Twitter, blog and Usenet data outperform telephone transcripts or newswire text. While our crowdsourced AAC data is better than other text sources, it is too small to train high-quality long-span language models. We therefore investigate how to use our crowdsourced collection to intelligently select AAC-like sentences from Twitter, blog and Usenet data. We compare a variety of different techniques for doing this intelligent selection. We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). Using this method, we build a compact and well-performing mixture model from the Twitter, blog and Usenet sentences most similar to our crowdsourced data. We evaluate our mixture model on four different test sets. On the three most AAC-like test sets, we found substantial reductions in not only perplexity but also in potential keystroke savings when used in a predictive keyboard interface. Finally, to aid other AAC researchers, we have publicly released our crowdsourced AAC collection, word lists and best-performing language models1. 2 Crowdsourcing AAC-like Messages As we mentioned in the i</context>
<context position="22687" citStr="Moore and Lewis, 2010" startWordPosition="3770" endWordPosition="3773">g data. This tends to find variants of existing communications in our Turk collection. In cross-entropy selection, we used only sentences below a threshold on the per-word crossentropy with respect to a TURKTRAIN language model. This is equivalent to placing a threshold on the perplexity. Previously this technique has been used to improve language models based on web data (Bulyko et al., 2007; Gao et al., 2002) and to construct domain-specific models (Lin et al., 1997). In cross-entropy difference selection, a sentence’s score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010). This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). We were curious how this technique would work given our much smaller in-domain set of 24K words. 4.2 Data Selection and Pruning We built models selecting sentences below different thresholds on the WER, in-domain cross-entropy, or cross-entropy difference. For comparison, we also pruned our models using conventional count-cutoff and entropy pruning (Stolcke, 1998). During entropy pruning, we used a Good-Turing estimated model for computing the history ma</context>
<context position="32062" citStr="Moore and Lewis, 2010" startWordPosition="5298" endWordPosition="5301">ataset to intelligently select sentences from Twitworkers to presume they would require assistance ter, blog and Usenet data. in many routine physical tasks. Our workers were We compared a variety of different techniques for (presumably) without cognitive or language impair- intelligent training data selection. We found that ments. Thus our collection is more representative even for our small amount of in-domain data, the of one subgroup of AAC communicators (scanning recently proposed cross-entropy difference method users with normal cognitive function and language was consistently the best (Moore and Lewis, 2010). skills). By modifying the situation given to workers, Finally, compared to a model trained only on it is likely we can expand our collection to better rep- Switchboard, our best performing model reduced resent other groups of AAC users, such as those us- perplexity by 60-82% relative on three AAC-like test ing predictive keyboards or eye-trackers. However, sets. This translated to a potential keystroke savings obtaining data representative of users with cognitive in a predictive keyboard interface of 5–11%. or language impairments via crowdsourcing would In conclusion, we have shown how to c</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics, pages 220–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>172--180</pages>
<contexts>
<context position="10903" citStr="Ritter et al. (2010)" startWordPosition="1727" endWordPosition="1730"> 2009). 25M sentences, 387M words. 2We combined Wiktionary, Webster’s dictionary provided by Project Gutenberg, the CMU pronouncing dictionary and GNU aspell. 702 • TWITTER – We collected Twitter messages via the streaming API between December 2010 and March 2011. We used the free Twitter stream which provides access to 5% of all tweets. Twitter may be particularly well suited for modeling AAC communications as tweets are short typed messages that are often informal person-to-person communications. Twitter has previously been proposed as a candidate for modeling conversations, see for example Ritter et al. (2010). 7M sentences, 55M words. • TURKTRAIN – Communications from 80% of the workers in our crowdsourced collection. 4981 sentences, 24860 words. WIKIPEDIA, USENET, BLOG and TWITTER all consisted of raw text that required significant filtering to eliminate garbage, spam, repeated messages, XML tags, non-English text, etc. Given the large amount of data available, our approach was to throw away any text that did not appear to be a sensible English sentence. For example, we eliminated any sentence having a large number of words not in our 330K word list. 3.1 Test Sets We evaluated our models on the f</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Proceedings of HLT-NAACL 2010, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Shaoul</author>
<author>Chris Westbury</author>
</authors>
<title>A USENET corpus (2005-2009).</title>
<date>2009</date>
<institution>University of Alberta, Canada.</institution>
<contexts>
<context position="9920" citStr="Shaoul and Westbury, 2009" startWordPosition="1573" endWordPosition="1576">iar situation, it appears that providing a concrete example was helpful to workers. 3 Comparison of Training Sources In this section, we compare the predictive performance of language models trained on our Turk AAC data with models trained on other text sources. We use the following training sets: • NEWS – Newspaper articles from the CSR-III (Graff et al., 1995) and Gigaword corpora (Graff, 2003). 60M sentences, 1323M words. • WIKIPEDIA – Current articles and discussion threads from a snapshot of Wikipedia (January 3, 2008). 24M sentences, 452M words. • USENET – Messages from a Usenet corpus (Shaoul and Westbury, 2009). 123M sentences, 1847M words. • SWITCHBOARD – Transcripts of 2217 telephone conversations from the Switchboard corpus (Godfrey et al., 1992). Due to its conversational style, this corpus has been popular for AAC language modeling (Lesher and Rinkus, 2002; Trnka et al., 2009). 0.2M sentences, 2.6M words. • BLOG – Blog posts from the ICWSM corpus (Burton et al., 2009). 25M sentences, 387M words. 2We combined Wiktionary, Webster’s dictionary provided by Project Gutenberg, the CMU pronouncing dictionary and GNU aspell. 702 • TWITTER – We collected Twitter messages via the streaming API between De</context>
</contexts>
<marker>Shaoul, Westbury, 2009</marker>
<rawString>Cyrus Shaoul and Chris Westbury. 2009. A USENET corpus (2005-2009). University of Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proceedings of DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="23195" citStr="Stolcke, 1998" startWordPosition="3847" endWordPosition="3848">sentence’s score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010). This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). We were curious how this technique would work given our much smaller in-domain set of 24K words. 4.2 Data Selection and Pruning We built models selecting sentences below different thresholds on the WER, in-domain cross-entropy, or cross-entropy difference. For comparison, we also pruned our models using conventional count-cutoff and entropy pruning (Stolcke, 1998). During entropy pruning, we used a Good-Turing estimated model for computing the history marginals as the lower-order Kneser-Ney distributions are unsuitable for this purpose (Chelba et al., 2010). We calculated the perplexity of each model on three test sets. We also tallied the number of model parameters (all n-gram probabilities plus all backoff weights). On TURKDEV, cross-entropy difference selection performed the best for all models sizes and for all training sets (figure 6). We also found cross706 Figure 7: Perplexity on TURKDEV varying the cross-entropy difference threshold. entropy di</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th Annual International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="13500" citStr="Stolcke, 2002" startWordPosition="2141" endWordPosition="2142">on section. 59 sentences, 508 words. TURKDEV and TURKTEST contain text similar to table 1 and 2. Table 3 shows some examples from the other three test sets. Sentences in COMM tended to be richer in vocabulary and subject matter than those in SPECIALISTS. The SPECIALISTS sentences tended to be general phrases that avoided mentioning specific situations, proper names, etc. Sentences in SWITCHTEST exhibited phenomena typical of human-to-human voice conversations (filler words, backchannels, interruptions, etc). 3.2 Language Model Training All language models were trained using the SRILM toolkit (Stolcke, 2002). All models used interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). In this section, we trained 3-gram language models with no count-cutoffs. All text was converted to lowercase and we removed punctuation except for apostrophes. We believe punctuation would likely slow down a user’s conversation for only a small potential advantage (e.g. improving text-to-speech prosody). All models used a vocabulary of 63K words including an unknown word. We obtained our vocabulary by taking all words occurring in TURKTRAIN and all words occurring four or more times in</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of the 7th Annual International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Trnka</author>
<author>Debra Yarrington</author>
<author>Christopher Pennington</author>
</authors>
<title>Topic modeling in fringe word prediction for AAC.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th ACM International Conference on Intelligent User Interfaces,</booktitle>
<pages>276--278</pages>
<contexts>
<context position="2335" citStr="Trnka et al., 2006" startWordPosition="340" endWordPosition="343">es provided by 700 AAC devices are typically low, between 0.5 and 16 words-per-minute (Trnka et al., 2009). As a consequence, researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques. Examples of approaches include adapting the language model to recently used words (Wandmacher et al., 2008; Trnka, 2008), using syntactic information (Hunnicutt, 1989; Garay-Vitoria and Gonz´alez-Abascal, 1997), using semantic information (Wandmacher and Antoine, 2007; Li and Hirst, 2005), and modeling topics (Lesher and Rinkus, 2002; Trnka et al., 2006). For a recent survey, see Garay-Vitoria and Abascal (2006). While such language model improvement techniques are undoubtedly helpful, certainly they can all benefit from starting with a long-span language model trained on large amounts of closely matched data. For AAC devices this means closely modeling everyday face-to-face communications. However, a long-standing problem in the field is the lack of good data sources that adequately model such AAC communications. Due to privacy-reasons and other ethical concerns, there is no large dataset consisting of genuine AAC messages. Therefore, previo</context>
</contexts>
<marker>Trnka, Yarrington, Pennington, 2006</marker>
<rawString>Keith Trnka, Debra Yarrington, and Christopher Pennington. 2006. Topic modeling in fringe word prediction for AAC. In Proceedings of the 11th ACM International Conference on Intelligent User Interfaces, pages 276–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Trnka</author>
<author>John McCaw</author>
<author>Debra Yarrington</author>
<author>Kathleen F McCoy</author>
<author>Christopher Pennington</author>
</authors>
<title>User interaction with word prediction: The effects of prediction quality.</title>
<date>2009</date>
<journal>ACM Transactions on Accessible Computing,</journal>
<pages>1--17</pages>
<contexts>
<context position="1822" citStr="Trnka et al., 2009" startWordPosition="264" endWordPosition="267">transcripts, our best performing model reduced perplexity on three test sets of AAC-like communications by 60– 82% relative. This translated to a potential keystroke savings in a predictive keyboard interface of 5–11%. 1 Introduction Users with certain communication disabilities rely on augmented and alternative communication (AAC) devices to take part in everyday conversations. Often these devices consist of a predictive text input method coupled with text-to-speech output. Unfortunately, the text entry rates provided by 700 AAC devices are typically low, between 0.5 and 16 words-per-minute (Trnka et al., 2009). As a consequence, researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques. Examples of approaches include adapting the language model to recently used words (Wandmacher et al., 2008; Trnka, 2008), using syntactic information (Hunnicutt, 1989; Garay-Vitoria and Gonz´alez-Abascal, 1997), using semantic information (Wandmacher and Antoine, 2007; Li and Hirst, 2005), and modeling topics (Lesher and Rinkus, 2002; Trnka et al., 2006). For a recent survey, see Garay-Vitoria and Abascal (2006). While such language model </context>
<context position="10196" citStr="Trnka et al., 2009" startWordPosition="1616" endWordPosition="1619"> training sets: • NEWS – Newspaper articles from the CSR-III (Graff et al., 1995) and Gigaword corpora (Graff, 2003). 60M sentences, 1323M words. • WIKIPEDIA – Current articles and discussion threads from a snapshot of Wikipedia (January 3, 2008). 24M sentences, 452M words. • USENET – Messages from a Usenet corpus (Shaoul and Westbury, 2009). 123M sentences, 1847M words. • SWITCHBOARD – Transcripts of 2217 telephone conversations from the Switchboard corpus (Godfrey et al., 1992). Due to its conversational style, this corpus has been popular for AAC language modeling (Lesher and Rinkus, 2002; Trnka et al., 2009). 0.2M sentences, 2.6M words. • BLOG – Blog posts from the ICWSM corpus (Burton et al., 2009). 25M sentences, 387M words. 2We combined Wiktionary, Webster’s dictionary provided by Project Gutenberg, the CMU pronouncing dictionary and GNU aspell. 702 • TWITTER – We collected Twitter messages via the streaming API between December 2010 and March 2011. We used the free Twitter stream which provides access to 5% of all tweets. Twitter may be particularly well suited for modeling AAC communications as tweets are short typed messages that are often informal person-to-person communications. Twitter h</context>
<context position="12798" citStr="Trnka et al. (2009)" startWordPosition="2033" endWordPosition="2036">ed collection (disjoint from TURKTRAIN and TURKDEV). This set is used only in the final evaluation section. 563 sentences, 2721 words. 3http://aac.unl.edu/vocabulary.html Test set Sentence COMM I love your new haircut. COMM How many children do you have? SPECIALISTS Are you sure you don’t mind? SPECIALISTS I’ll keep an eye on that for you SWITCHTEST yeah he’s a good actor though SWITCHTEST what did she have like Table 3: Examples from three of our test sets. • SWITCHTEST – Transcripts of three Switchboard conversations (disjoint from the SWITCHBOARD training set). This is the same set used in Trnka et al. (2009). We dropped one sentence containing a dash. This set is only used in the final evaluation section. 59 sentences, 508 words. TURKDEV and TURKTEST contain text similar to table 1 and 2. Table 3 shows some examples from the other three test sets. Sentences in COMM tended to be richer in vocabulary and subject matter than those in SPECIALISTS. The SPECIALISTS sentences tended to be general phrases that avoided mentioning specific situations, proper names, etc. Sentences in SWITCHTEST exhibited phenomena typical of human-to-human voice conversations (filler words, backchannels, interruptions, etc)</context>
<context position="28582" citStr="Trnka et al., 2009" startWordPosition="4733" endWordPosition="4736">he best performing language model on each test set. mixture model provided substantial increases in keystroke savings compared to a model trained solely on Switchboard. The mixture model also performed better than simply training a model on a large amount of Twitter data. The model trained on only 24K words of Turk data did surprisingly well given its extremely limited training data. Our Switchboard model performed the best on SWITCHTEST with a keystroke savings of 58.8%. For comparison, past work reported a keystroke savings of 55.7% on SWITCHTEST using a 3-gram model trained on Switchboard (Trnka et al., 2009). While our mixture model performed less well on SWITCHTEST (52.8%), it is likely the other three test sets better represent AAC communications. 5.3 Larger Mixture Model Experiment Our mixture language model used the best thresholds with respect to TURKDEV. This resulted in throwing away most of the training data. This might be suboptimal in practice if an AAC user’s communications are somewhat different or more diverse than the language generated by the Turk workers. We trained a series of mixture models in which we varied the cross-entropy difference thresholds Change from optimal thresholds</context>
<context position="34312" citStr="Trnka et al., 2009" startWordPosition="5653" endWordPosition="5656">lan to use our models to design and We evaluated our models in terms of perplexity test new interfaces that enable faster communication and keystrokes savings within the auspices of a pre- for AAC users. dictive keyboard. Further work is needed to inves- Acknowledgments tigate how our numeric gains translate to real-world We thank Keith Trnka and Horabail Venkatagiri for benefits to users. However, past work indicates more their assistance. This work was supported by the Enaccurate predictions do in fact yield improvements gineering and Physical Sciences Research Council in human performance (Trnka et al., 2009). (grant number EP/H027408/1). Finally, while the predictive keyboard is a com- References monly studied interface, it is not appropriate for all Bruce Baker, Katya Hill, and Richard Devylder. 2000. AAC users. Eye-tracker users may prefer an in- Core vocabulary is the same across environments. In terface such as Dasher (Ward and MacKay, 2002). California State University at Northridge Conference. Single-switch users may prefer an interface such as David R. Beukelman, Kathryn M. Yorkston, Miguel Nomon (Broderick and MacKay, 2009). Any AAC Poblete, and Carlos Naranjo. 1984. Frequency of interfac</context>
</contexts>
<marker>Trnka, McCaw, Yarrington, McCoy, Pennington, 2009</marker>
<rawString>Keith Trnka, John McCaw, Debra Yarrington, Kathleen F. McCoy, and Christopher Pennington. 2009. User interaction with word prediction: The effects of prediction quality. ACM Transactions on Accessible Computing, 1:17:1–17:34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Trnka</author>
</authors>
<title>Adaptive language modeling for word prediction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Student Research Workshop,</booktitle>
<pages>61--66</pages>
<contexts>
<context position="2099" citStr="Trnka, 2008" startWordPosition="309" endWordPosition="310"> on augmented and alternative communication (AAC) devices to take part in everyday conversations. Often these devices consist of a predictive text input method coupled with text-to-speech output. Unfortunately, the text entry rates provided by 700 AAC devices are typically low, between 0.5 and 16 words-per-minute (Trnka et al., 2009). As a consequence, researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques. Examples of approaches include adapting the language model to recently used words (Wandmacher et al., 2008; Trnka, 2008), using syntactic information (Hunnicutt, 1989; Garay-Vitoria and Gonz´alez-Abascal, 1997), using semantic information (Wandmacher and Antoine, 2007; Li and Hirst, 2005), and modeling topics (Lesher and Rinkus, 2002; Trnka et al., 2006). For a recent survey, see Garay-Vitoria and Abascal (2006). While such language model improvement techniques are undoubtedly helpful, certainly they can all benefit from starting with a long-span language model trained on large amounts of closely matched data. For AAC devices this means closely modeling everyday face-to-face communications. However, a long-stan</context>
</contexts>
<marker>Trnka, 2008</marker>
<rawString>Keith Trnka. 2008. Adaptive language modeling for word prediction. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Student Research Workshop, pages 61–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horabail Venkatagiri</author>
</authors>
<title>Efficient keyboard layouts for sequential access in augmentative and alternative communication. Augmentative and Alternative Communication,</title>
<date>1999</date>
<contexts>
<context position="11634" citStr="Venkatagiri (1999)" startWordPosition="1847" endWordPosition="1848">981 sentences, 24860 words. WIKIPEDIA, USENET, BLOG and TWITTER all consisted of raw text that required significant filtering to eliminate garbage, spam, repeated messages, XML tags, non-English text, etc. Given the large amount of data available, our approach was to throw away any text that did not appear to be a sensible English sentence. For example, we eliminated any sentence having a large number of words not in our 330K word list. 3.1 Test Sets We evaluated our models on the following test sets: • COMM – Sentences written in response to hypothetical communication situations collected by Venkatagiri (1999). We removed nine sentences containing numbers. This set is used throughout the paper. 251 sentences, 1789 words. • SPECIALISTS – Context specific phrases suggested by AAC specialists3. This set is used throughout the paper. 952 sentences, 3842 words. • TURKDEV – Communications from 10% of the workers in our crowdsourced collection (disjoint from TURKTRAIN and TURKTEST). This set will be used for initial evaluations and also to tune our models. 551 sentences, 2916 words. • TURKTEST – Communications from 10% of the workers in our crowdsourced collection (disjoint from TURKTRAIN and TURKDEV). Th</context>
</contexts>
<marker>Venkatagiri, 1999</marker>
<rawString>Horabail Venkatagiri. 1999. Efficient keyboard layouts for sequential access in augmentative and alternative communication. Augmentative and Alternative Communication, 15(2):126–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tonio Wandmacher</author>
<author>Jean-Yves Antoine</author>
</authors>
<title>Methods to integrate a language model with semantic information for a word prediction component.</title>
<date>2007</date>
<booktitle>Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>506--513</pages>
<contexts>
<context position="2247" citStr="Wandmacher and Antoine, 2007" startWordPosition="325" endWordPosition="328">redictive text input method coupled with text-to-speech output. Unfortunately, the text entry rates provided by 700 AAC devices are typically low, between 0.5 and 16 words-per-minute (Trnka et al., 2009). As a consequence, researchers have made numerous efforts to increase AAC text entry rates by employing a variety of improved language modeling techniques. Examples of approaches include adapting the language model to recently used words (Wandmacher et al., 2008; Trnka, 2008), using syntactic information (Hunnicutt, 1989; Garay-Vitoria and Gonz´alez-Abascal, 1997), using semantic information (Wandmacher and Antoine, 2007; Li and Hirst, 2005), and modeling topics (Lesher and Rinkus, 2002; Trnka et al., 2006). For a recent survey, see Garay-Vitoria and Abascal (2006). While such language model improvement techniques are undoubtedly helpful, certainly they can all benefit from starting with a long-span language model trained on large amounts of closely matched data. For AAC devices this means closely modeling everyday face-to-face communications. However, a long-standing problem in the field is the lack of good data sources that adequately model such AAC communications. Due to privacy-reasons and other ethical c</context>
</contexts>
<marker>Wandmacher, Antoine, 2007</marker>
<rawString>Tonio Wandmacher and Jean-Yves Antoine. 2007. Methods to integrate a language model with semantic information for a word prediction component. Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 506–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tonio Wandmacher</author>
<author>Jean-Yves Antoine</author>
<author>Franck Poirier</author>
<author>Jean-Paul D´eparte</author>
</authors>
<title>SIBYLLE, an assistive communication system adapting to the context and its user.</title>
<date>2008</date>
<journal>ACM Transactions on Accessible Computing,</journal>
<pages>1--6</pages>
<marker>Wandmacher, Antoine, Poirier, D´eparte, 2008</marker>
<rawString>Tonio Wandmacher, Jean-Yves Antoine, Franck Poirier, and Jean-Paul D´eparte. 2008. SIBYLLE, an assistive communication system adapting to the context and its user. ACM Transactions on Accessible Computing, 1:6:1–6:30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Ward</author>
<author>D J C MacKay</author>
</authors>
<title>Fast hands-free writing by gaze direction.</title>
<date>2002</date>
<journal>Nature,</journal>
<volume>418</volume>
<issue>6900</issue>
<marker>Ward, MacKay, 2002</marker>
<rawString>D. J. Ward and D. J. C. MacKay. 2002. Fast hands-free writing by gaze direction. Nature, 418(6900):838.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>