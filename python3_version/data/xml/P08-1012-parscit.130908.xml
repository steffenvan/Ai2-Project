<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.791343">
Bayesian Learning of Non-compositional Phrases with Synchronous Parsing
</title>
<author confidence="0.995178">
Hao Zhang
</author>
<affiliation confidence="0.997203">
Computer Science Department
University of Rochester
</affiliation>
<address confidence="0.714858">
Rochester, NY 14627
</address>
<email confidence="0.996615">
zhanghao@cs.rochester.edu
</email>
<author confidence="0.989346">
Robert C. Moore
</author>
<affiliation confidence="0.957148">
Microsoft Research
</affiliation>
<address confidence="0.9252705">
One Microsoft Way
Redmond, WA 98052 USA
</address>
<email confidence="0.995716">
bobmoore@microsoft.com
</email>
<sectionHeader confidence="0.998572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999712944444444">
We combine the strengths of Bayesian mod-
eling and synchronous grammar in unsu-
pervised learning of basic translation phrase
pairs. The structured space of a synchronous
grammar is a natural fit for phrase pair proba-
bility estimation, though the search space can
be prohibitively large. Therefore we explore
efficient algorithms for pruning this space that
lead to empirically effective results. Incorpo-
rating a sparse prior using Variational Bayes,
biases the models toward generalizable, parsi-
monious parameter sets, leading to significant
improvements in word alignment. This pref-
erence for sparse solutions together with ef-
fective pruning methods forms a phrase align-
ment regimen that produces better end-to-end
translations than standard word alignment ap-
proaches.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869545454546">
Most state-of-the-art statistical machine transla-
tion systems are based on large phrase tables ex-
tracted from parallel text using word-level align-
ments. These word-level alignments are most of-
ten obtained using Expectation Maximization on the
conditional generative models of Brown et al. (1993)
and Vogel et al. (1996). As these word-level align-
ment models restrict the word alignment complex-
ity by requiring each target word to align to zero
or one source words, results are improved by align-
ing both source-to-target as well as target-to-source,
</bodyText>
<page confidence="0.995644">
97
</page>
<author confidence="0.362029">
Chris Quirk
</author>
<affiliation confidence="0.317578">
Microsoft Research
</affiliation>
<address confidence="0.510103">
One Microsoft Way
Redmond, WA 98052 USA
</address>
<email confidence="0.965198">
chrisq@microsoft.com
</email>
<author confidence="0.974439">
Daniel Gildea
</author>
<affiliation confidence="0.841898666666667">
Computer Science Department
University of Rochester
Rochester, NY 14627
</affiliation>
<email confidence="0.935657">
gildea@cs.rochester.edu
</email>
<bodyText confidence="0.999700967741936">
then heuristically combining these alignments. Fi-
nally, the set of phrases consistent with the word
alignments are extracted from every sentence pair;
these form the basis of the decoding process. While
this approach has been very successful, poor word-
level alignments are nonetheless a common source
of error in machine translation systems.
A natural solution to several of these issues is
unite the word-level and phrase-level models into
one learning procedure. Ideally, such a procedure
would remedy the deficiencies of word-level align-
ment models, including the strong restrictions on
the form of the alignment, and the strong inde-
pendence assumption between words. Furthermore
it would obviate the need for heuristic combina-
tion of word alignments. A unified procedure may
also improve the identification of non-compositional
phrasal translations, and the attachment decisions
for unaligned words.
In this direction, Expectation Maximization at
the phrase level was proposed by Marcu and Wong
(2002), who, however, experienced two major dif-
ficulties: computational complexity and controlling
overfitting. Computational complexity arises from
the exponentially large number of decompositions
of a sentence pair into phrase pairs; overfitting is a
problem because as EM attempts to maximize the
likelihood of its training data, it prefers to directly
explain a sentence pair with a single phrase pair.
In this paper, we attempt to address these two is-
sues in order to apply EM above the word level.
</bodyText>
<note confidence="0.818978">
Proceedings of ACL-08: HLT, pages 97–105,
</note>
<page confidence="0.547884">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999969538461539">
We attack computational complexity by adopting
the polynomial-time Inversion Transduction Gram-
mar framework, and by only learning small non-
compositional phrases. We address the tendency of
EM to overfit by using Bayesian methods, where
sparse priors assign greater mass to parameter vec-
tors with fewer non-zero values therefore favoring
shorter, more frequent phrases. We test our model
by extracting longer phrases from our model’s align-
ments using traditional phrase extraction, and find
that a phrase table based on our system improves MT
results over a phrase table extracted from traditional
word-level alignments.
</bodyText>
<sectionHeader confidence="0.986075" genericHeader="introduction">
2 Phrasal Inversion Transduction
Grammar
</sectionHeader>
<bodyText confidence="0.984676714285714">
We use a phrasal extension of Inversion Transduc-
tion Grammar (Wu, 1997) as the generative frame-
work. Our ITG has two nonterminals: X and
C, where X represents compositional phrase pairs
that can have recursive structures and C is the pre-
terminal over terminal phrase pairs. There are three
rules with X on the left-hand side:
</bodyText>
<equation confidence="0.985855666666667">
X —* [X X],
X —* (X X),
X —* C.
</equation>
<bodyText confidence="0.999986375">
The first two rules are the straight rule and in-
verted rule respectively. They split the left-hand side
constituent which represents a phrase pair into two
smaller phrase pairs on the right-hand side and order
them according to one of the two possible permuta-
tions. The rewriting process continues until the third
rule is invoked. C is our unique pre-terminal for
generating terminal multi-word pairs:
</bodyText>
<equation confidence="0.7995">
C —* e/f.
</equation>
<bodyText confidence="0.999995777777778">
We parameterize our probabilistic model in the
manner of a PCFG: we associate a multinomial dis-
tribution with each nonterminal, where each out-
come in this distribution corresponds to an expan-
sion of that nonterminal. Specifically, we place one
multinomial distribution θX over the three expan-
sions of the nonterminal X, and another multinomial
distribution θC over the expansions of C. Thus, the
parameters in our model can be listed as
</bodyText>
<equation confidence="0.863622">
θX = (Phi, PD, PC),
</equation>
<bodyText confidence="0.990107">
where Phi is for the inverted rule, PD for the straight
rule, PC for the third rule, satisfying Phi+PD+PC =
1, and
</bodyText>
<equation confidence="0.984424">
θC = (P(e/f), P(e′/f′), ... ),
</equation>
<bodyText confidence="0.99992740625">
where &amp;/f P(e/f) = 1 is a multinomial distribu-
tion over phrase pairs.
This is our model in a nutshell. We can train
this model using a two-dimensional extension of the
inside-outside algorithm on bilingual data, assuming
every phrase pair that can appear as a leaf in a parse
tree of the grammar a valid candidate. However, it is
easy to show that the maximum likelihood training
will lead to the saturated solution where PC = 1 —
each sentence pair is generated by a single phrase
spanning the whole sentence. From the computa-
tional point of view, the full EM algorithm runs in
O(n6) where n is the average length of the two in-
put sentences, which is too slow in practice.
The key is to control the number of parameters,
and therefore the size of the set of candidate phrases.
We deal with this problem in two directions. First
we change the objective function by incorporating
a prior over the phrasal parameters. This has the
effect of preferring parameter vectors in θC with
fewer non-zero values. Our second approach was
to constrain the search space using simpler align-
ment models, which has the further benefit of signif-
icantly speeding up training. First we train a lower
level word alignment model, then we place hard con-
straints on the phrasal alignment space using confi-
dent word links from this simpler model. Combining
the two approaches, we have a staged training pro-
cedure going from the simplest unconstrained word
based model to a constrained Bayesian word-level
ITG model, and finally proceeding to a constrained
Bayesian phrasal model.
</bodyText>
<sectionHeader confidence="0.992848" genericHeader="method">
3 Variational Bayes for ITG
</sectionHeader>
<bodyText confidence="0.99970125">
Goldwater and Griffiths (2007) and Johnson (2007)
show that modifying an HMM to include a sparse
prior over its parameters and using Bayesian esti-
mation leads to improved accuracy for unsupervised
part-of-speech tagging. In this section, we describe
a Bayesian estimator for ITG: we select parame-
ters that optimize the probability of the data given
a prior. The traditional estimation method for word
</bodyText>
<page confidence="0.997887">
98
</page>
<bodyText confidence="0.999941821428571">
alignment models is the EM algorithm (Brown et
al., 1993) which iteratively updates parameters to
maximize the likelihood of the data. The drawback
of maximum likelihood is obvious for phrase-based
models. If we do not put any constraint on the dis-
tribution of phrases, EM overfits the data by mem-
orizing every sentence pair. A sparse prior over a
multinomial distribution such as the distribution of
phrase pairs may bias the estimator toward skewed
distributions that generalize better. In the context of
phrasal models, this means learning the more repre-
sentative phrases in the space of all possible phrases.
The Dirichlet distribution, which is parameter-
ized by a vector of real values often interpreted as
pseudo-counts, is a natural choice for the prior, for
two main reasons. First, the Dirichlet is conjugate
to the multinomial distribution, meaning that if we
select a Dirichlet prior and a multinomial likelihood
function, the posterior distribution will again be a
Dirichlet. This makes parameter estimation quite
simple. Second, Dirichlet distributions with small,
non-zero parameters place more probability mass on
multinomials on the edges or faces of the probabil-
ity simplex, distributions with fewer non-zero pa-
rameters. Starting from the model from Section 2,
we propose the following Bayesian extension, where
A — Dir(B) means the random variable A is dis-
tributed according to a Dirichlet with parameter B:
</bodyText>
<equation confidence="0.999568333333333">
θX  |αX — Dir(αX),
θC  |αC — Dir(αC),
[X X]
(X X) X — Multi(θX),
C
e/f  |C — Multi(θC).
</equation>
<bodyText confidence="0.999989111111111">
The parameters αX and αC control the sparsity of
the two distributions in our model. One is the distri-
bution of the three possible branching choices. The
other is the distribution of the phrase pairs. αC is
crucial, since the multinomial it is controlling has a
high dimension. By adjusting αC to a very small
number, we hope to place more posterior mass on
parsimonious solutions with fewer but more confi-
dent and general phrase pairs.
Having defined the Bayesian model, it remains
to decide the inference procedure. We chose Vari-
ational Bayes, for its procedural similarity to EM
and ease of implementation. Another potential op-
tion would be Gibbs sampling (or some other sam-
pling technique). However, in experiments in un-
supervised POS tag learning using HMM structured
models, Johnson (2007) shows that VB is more ef-
fective than Gibbs sampling in approaching distribu-
tions that agree with the Zipf’s law, which is promi-
nent in natural languages.
Kurihara and Sato (2006) describe VB for PCFGs,
showing the only need is to change the M step of
the EM algorithm. As in the case of maximum like-
lihood estimation, Bayesian estimation for ITGs is
very similar to PCFGs, which follows due to the
strong isomorphism between the two models. Spe-
cific to our ITG case, the M step becomes:
</bodyText>
<equation confidence="0.997423875">
P˜ (l+1)exp(ψ(E(X — [X X]) + αX))
[] =exp(ψ(E(X) + sαX)) ,
P˜(l+1) = exp(ψ(E(X — (X X)) + αX))
0 exp(ψ(E(X) + sαX)) ,
p(l+1) _ exp(ψ(E(X — C) + αX))
PC&amp;quot; exp(ψ(E(X) + sαX)) ,
P˜(l+1)(e/f) = exp(ψ(E(e/f) + αC))
exp(ψ(E(C) + mαC)),
</equation>
<bodyText confidence="0.998776153846154">
where ψ is the digamma function (Beal, 2003), s =
3 is the number of right-hand-sides for X, and m is
the number of observed phrase pairs in the data. The
sole difference between EM and VB with a sparse
prior α is that the raw fractional counts c are re-
placed by exp(ψ(c + α)), an operation that resem-
bles smoothing. As pointed out by Johnson (2007),
in effect this expression adds to c a small value that
asymptotically approaches α — 0.5 as c approaches
oc, and 0 as c approaches 0. For small values of
α the net effect is the opposite of typical smooth-
ing, since it tends to redistribute probably mass away
from unlikely events onto more likely ones.
</bodyText>
<sectionHeader confidence="0.99172" genericHeader="method">
4 Bitext Pruning Strategy
</sectionHeader>
<bodyText confidence="0.999630666666667">
ITG is slow mainly because it considers every pair of
spans in two sentences as a possible chart element.
In reality, the set of useful chart elements is much
</bodyText>
<page confidence="0.99276">
99
</page>
<bodyText confidence="0.998735727272727">
smaller than the possible scriptO(n4), where n is
the average sentence length. Pruning the span pairs
(bitext cells) that can participate in a tree (either as
terminals or non-terminals) serves to not only speed
up ITG parsing, but also to provide a kind of ini-
tialization hint to the training procedures, encourag-
ing it to focus on promising regions of the alignment
space.
Given a bitext cell defined by the four boundary
indices (i, j,l, m) as shown in Figure 1a, we prune
based on a figure of merit V (i, j,l, m) approximat-
ing the utility of that cell in a full ITG parse. The
figure of merit considers the Model 1 scores of not
only the words inside a given cell, but also all the
words not included in the source and target spans, as
in Moore (2003) and Vogel (2005). Like Zhang and
Gildea (2005), it is used to prune bitext cells rather
than score phrases. The total score is the product of
the Model 1 probabilities for each column; “inside”
columns in the range [l, m] are scored according to
the sum (or maximum) of Model 1 probabilities for
[i, j], and “outside” columns use the sum (or maxi-
mum) of all probabilities not in the range [i, j].
Our pruning differs from Zhang and Gildea
(2005) in two major ways. First, we perform prun-
ing using both directions of the IBM Model 1 scores;
instead of a single figure of merit V , we have two:
VF and VB. Only those spans that pass the prun-
ing threshold in both directions are kept. Second,
we allow whole spans to be pruned. The figure of
merit for a span is VF (i, j) = maxl,m VF (i, j,l, m).
Only spans that are within some threshold of the un-
restricted Model 1 scores VF and VB are kept:
</bodyText>
<equation confidence="0.7734375">
VF (i, j) ≥ τs and VB(l,m) ≥ τs.
VF VB
</equation>
<bodyText confidence="0.9967895">
Amongst those spans retained by this first threshold,
we keep only those bitext cells satisfying both
</bodyText>
<subsectionHeader confidence="0.997948">
4.1 Fast Tic-tac-toe Pruning
</subsectionHeader>
<bodyText confidence="0.998531">
The tic-tac-toe pruning algorithm (Zhang and
Gildea, 2005) uses dynamic programming to com-
pute the product of inside and outside scores for
all cells in O(n4) time. However, even this can be
slow for large values of n. Therefore we describe an
</bodyText>
<figureCaption confidence="0.94810425">
Figure 1: (a) shows the original tic-tac-toe score for a
bitext cell (i, j,l, m). (b) demonstrates the finite state
representation using the machine in (c), assuming a fixed
source span (i, j).
</figureCaption>
<bodyText confidence="0.985008454545455">
improved algorithm with best case n3 performance.
Although the worst case performance is also O(n4),
in practice it is significantly faster.
To begin, let us restrict our attention to the for-
ward direction for a fixed source span (i, j). Prun-
ing bitext spans and cells requires VF (i, j), the score
of the best bitext cell within a given span, as well
as all cells within a given threshold of that best
score. For a fixed i and j, we need to search over
the starting and ending points l and m of the in-
side region. Note that there is an isomorphism be-
tween the set of spans and a simple finite state ma-
chine: any span (l, m) can be represented by a se-
quence of l OUTSIDE columns, followed by m−l+1
INSIDE columns, followed by n − m + 1 OUT-
SIDE columns. This simple machine has the re-
stricted form described in Figure 1c: it has three
states, L, M, and R; each transition generates ei-
ther an OUTSIDE column O or an INSIDE column
I. The cost of generating an OUTSIDE at posi-
tion a is O(a) = P(ta|NULL) + &amp;0[i,j] P(ta|sb);
likewise the cost of generating an INSIDE column
</bodyText>
<equation confidence="0.928015428571428">
is I(a) = P(ta|NULL) + Eb∈[i,j] P(ta|sb), with
≥
VB (i, j,l, m) .
VF (i, and VB(l,m) ≥ τb
VF (i, j,l, m)
100
O(0) = O(n + 1) = 1 and I(0) = I(n + 1) = 0.
</equation>
<bodyText confidence="0.9994504">
Directly computing O and I would take time
O(n2) for each source span, leading to an overall
runtime of O(n4). Luckily there are faster ways to
find the inside and outside scores. First we can pre-
compute following arrays in O(n2) time and space:
</bodyText>
<equation confidence="0.9955326">
pre[0,l] := P(tl|NULL)
pre[i, l] := pre[i − 1, l] + P(tl|si)
suf[n + 1, l] := 0
suf[i, l] := suf[i + 1, l] + P(tl|si)
Then for any (i, j), O(a) = P(ta|NULL) +
P
b6∈[i,j] P(ta|sb) = pre[i − 1, a] + suf[j + 1, a].
I(a) can be incrementally updated as the source
span varies: when i = j, I(a) = P(ta|NULL) +
P(ta|si). As j is incremented, we add P(ta|sj) to
</equation>
<bodyText confidence="0.9697698">
I(a). Thus we have linear time updates for O and I.
We can then find the best scoring sequence using
the familiar Viterbi algorithm. Let S[a, Q] be the cost
of the best scoring sequence ending at in state Q at
time a:
</bodyText>
<figure confidence="0.961178">
10 20 30 40 50
Average sentence length
</figure>
<figureCaption confidence="0.997999">
Figure 2: Speed comparison of the O(n4) tic-tac-toe
pruning algorithm, the A* top-x algorithm, and the fast
tic-tac-toe pruning. All produce the same set of bitext
cells, those within threshold of the best bitext cell.
</figureCaption>
<figure confidence="0.900520363636364">
400
200
900
700
600
500
300
800
100
0
Baseline
k-best
Fast
Pruning time (thousands of seconds)
S[0, Q] := 1 if Q = L; 0 otherwise
S[a, L] := S[a − 1, L] · O(a)
S[a, M] := max
σ∈L,M
S[a, R] := max {S[a − 1, Q]} · O(a)
σ∈M,R
{S[a − 1, Q]} · I(a)
Then VF (i, j) = S[n + 1, R], using the isomor-
</figure>
<bodyText confidence="0.98336516">
phism between state sequences and spans. This lin-
ear time algorithm allows us to compute span prun-
ing in O(n3) time. The same algorithm may be
performed using the backward figure of merit after
transposing rows and columns.
Having cast the problem in terms of finite state au-
tomata, we can use finite state algorithms for prun-
ing. For instance, fixing a source span we can enu-
merate the target spans in decreasing order by score
(Soong and Huang, 1991), stopping once we en-
counter the first span below threshold. In practice
the overhead of maintaining the priority queue out-
weighs any benefit, as seen in Figure 2.
An alternate approach that avoids this overhead is
to enumerate spans by position. Note that S[m, R] ·
Qna=m+1 O(a) is within threshold iff there is a
span with right boundary m′ &lt; m within thresh-
old. Furthermore if S[m, M] · Qna=m+1 O(a) is
within threshold, then m is the right boundary within
threshold. Using these facts, we can gradually
sweep the right boundary m from n toward 1 until
the first condition fails to hold. For each value where
the second condition holds, we pause to search for
the set of left boundaries within threshold.
Likewise for the left edge, S[l, M] · Qma=l+1 I(a) ·
</bodyText>
<equation confidence="0.96412875">
Qna=m+1 O(a) is within threshold iff there is some
l′ &lt; l identifying a span (l′, m) within threshold.
Finally if V (i, j,l, m) = S[l − 1, L] · Qma=l I(a) ·
Qna=m+1 O(a) is within threshold, then (i, j,l, m)
</equation>
<bodyText confidence="0.99851175">
is a bitext cell within threshold. For right edges that
are known to be within threshold, we can sweep the
left edges leftward until the first condition no longer
holds, keeping only those spans for which the sec-
ond condition holds.
The filtering algorithm behaves extremely well.
Although the worst case runtime is still O(n4), the
best case has improved to n3; empirically it seems to
significantly reduce the amount of time spent explor-
ing spans. Figure 2 compares the speed of the fast
tic-tac-toe algorithm against the algorithm in Zhang
and Gildea (2005).
</bodyText>
<page confidence="0.998456">
101
</page>
<figureCaption confidence="0.996963">
Figure 3: Example output from the ITG using non-compositional phrases. (a) is the Viterbi alignment from the word-
based ITG. The shaded regions indicate phrasal alignments that are allowed by the non-compositional constraint; all
other phrasal alignments will not be considered. (b) is the Viterbi alignment from the phrasal ITG, with the multi-word
alignments highlighted.
</figureCaption>
<sectionHeader confidence="0.9931265" genericHeader="method">
5 Bootstrapping Phrasal ITG from
Word-based ITG
</sectionHeader>
<bodyText confidence="0.999967666666667">
This section introduces a technique that bootstraps
candidate phrase pairs for phrase-based ITG from
word-based ITG Viterbi alignments. The word-
based ITG uses the same expansions for the non-
terminal X, but the expansions of C are limited to
generate only 1-1, 1-0, and 0-1 alignments:
</bodyText>
<equation confidence="0.604409666666667">
C → e/f,
C → e/E,
C → E/f
</equation>
<bodyText confidence="0.991095947368421">
where E indicates that no word was generated.
Broadly speaking, the goal of this section is the same
as the previous section, namely, to limit the set of
phrase pairs that needs to be considered in the train-
ing process. The tic-tac-toe pruning relies on IBM
model 1 for scoring a given aligned area. In this
part, we use word-based ITG alignments as anchor
points in the alignment space to pin down the poten-
tial phrases. The scope of iterative phrasal ITG train-
ing, therefore, is limited to determining the bound-
aries of the phrases anchored on the given one-to-
one word alignments.
The heuristic method is based on the Non-
Compositional Constraint of Cherry and Lin (2007).
Cherry and Lin (2007) use GIZA++ intersections
which have high precision as anchor points in the
bitext space to constraint ITG phrases. We use ITG
Viterbi alignments instead. The benefit is two-fold.
First of all, we do not have to run a GIZA++ aligner.
Second, we do not need to worry about non-ITG
word alignments, such as the (2, 4, 1, 3) permutation
patterns. GIZA++ does not limit the set of permu-
tations allowed during translation, so it can produce
permutations that are not reachable using an ITG.
Formally, given a word-based ITG alignment, the
bootstrapping algorithm finds all the phrase pairs
according to the definition of Och and Ney (2004)
and Chiang (2005) with the additional constraint
that each phrase pair contains at most one word
link. Mathematically, let e(i, j) count the number of
word links that are emitted from the substring ez...j,
and f(l, m) count the number of word links emit-
ted from the substring fl...ry,t. The non-compositional
phrase pairs satisfy
e(i, j) = f(l, m) G 1.
Figure 3 (a) shows all possible non-compositional
phrases given the Viterbi word alignment of the ex-
ample sentence pair.
</bodyText>
<sectionHeader confidence="0.810893" genericHeader="method">
6 Summary of the Pipeline
</sectionHeader>
<bodyText confidence="0.99969">
We summarize the pipeline of our system, demon-
strating the interactions between the three main con-
tributions of this paper: Variational Bayes, tic-tac-
toe pruning, and word-to-phrase bootstrapping. We
</bodyText>
<page confidence="0.997539">
102
</page>
<bodyText confidence="0.973329608695652">
start from sentence-aligned bilingual data and run
IBM Model 1 in both directions to obtain two trans-
lation tables. Then we use the efficient bidirectional
tic-tac-toe pruning to prune the bitext space within
each of the sentence pairs; ITG parsing will be car-
ried out on only this this sparse set of bitext cells.
The first stage of training is word-based ITG, us-
ing the standard iterative training procedure, except
VB replaces EM to focus on a sparse prior. Af-
ter several training iterations, we obtain the Viterbi
alignments on the training data according to the fi-
nal model. Now we transition into the second stage
– the phrasal training. Before the training starts,
we apply the non-compositional constraints over the
pruned bitext space to further constrain the space
of phrase pairs. Finally, we run phrasal ITG itera-
tive training using VB for a certain number of itera-
tions. In the end, a Viterbi pass for the phrasal ITG is
executed to produce the non-compositional phrasal
alignments. From this alignment, phrase pairs are
extracted in the usual manner, and a phrase-based
translation system is trained.
AER
</bodyText>
<sectionHeader confidence="0.99505" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999948625">
The training data was a subset of 175K sentence
pairs from the NIST Chinese-English training data,
automatically selected to maximize character-level
overlap with the source side of the test data. We put
a length limit of 35 on both sides, producing a train-
ing set of 141K sentence pairs. 500 Chinese-English
pairs from this set were manually aligned and used
as a gold standard.
</bodyText>
<subsectionHeader confidence="0.997785">
7.1 Word Alignment Evaluation
</subsectionHeader>
<bodyText confidence="0.999874615384615">
First, using evaluations of alignment quality, we
demonstrate the effectiveness of VB over EM, and
explore the effect of the prior.
Figure 4 examines the difference between EM and
VB with varying sparse priors for the word-based
model of ITG on the 500 sentence pairs, both af-
ter 10 iterations of training. Using EM, because of
overfitting, AER drops first and increases again as
the number of iterations varies from 1 to 10. The
lowest AER using EM is achieved after the second
iteration, which is .40. At iteration 10, AER for EM
increases to .42. On the other hand, using VB, AER
decreases monotonically over the 10 iterations and
</bodyText>
<figure confidence="0.800522">
1e-009 1e-006 0.001 1
Prior value
</figure>
<figureCaption confidence="0.9955805">
Figure 4: AER drops as αC approaches zero; a more
sparse solution leads to better results.
</figureCaption>
<bodyText confidence="0.998711266666667">
stabilizes at iteration 10. When αC is 1e − 9, VB
gets AER close to .35 at iteration 10.
As we increase the bias toward sparsity, the AER
decreases, following a long slow plateau. Although
the magnitude of improvement is not large, the trend
is encouraging.
These experiments also indicate that a very sparse
prior is needed for machine translation tasks. Un-
like Johnson (2007), who found optimal perfor-
mance when α was approximately 10−4, we ob-
served monotonic increases in performance as α
dropped. The dimensionality of this MT problem is
significantly larger than that of the sequence prob-
lem, though, therefore it may take a stronger push
from the prior to achieve the desired result.
</bodyText>
<subsectionHeader confidence="0.987072">
7.2 End-to-end Evaluation
</subsectionHeader>
<bodyText confidence="0.999968">
Given an unlimited amount of time, we would tune
the prior to maximize end-to-end performance, us-
ing an objective function such as BLEU. Unfortu-
nately these experiments are very slow. Since we
observed monotonic increases in alignment perfor-
mance with smaller values of αC, we simply fixed
the prior at a very small value (10−100) for all trans-
lation experiments. We do compare VB against EM
in terms of final BLEU scores in the translation ex-
periments to ensure that this sparse prior has a sig-
</bodyText>
<figure confidence="0.994579181818182">
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
VB
EM
</figure>
<page confidence="0.996926">
103
</page>
<bodyText confidence="0.987178520833333">
nificant impact on the output.
We also trained a baseline model with GIZA++
(Och and Ney, 2003) following a regimen of 5 it-
erations of Model 1, 5 iterations of HMM, and 5
iterations of Model 4. We computed Chinese-to-
English and English-to-Chinese word translation ta-
bles using five iterations of Model 1. These val-
ues were used to perform tic-tac-toe pruning with
τb = 1 × 10−3 and τ3 = 1 × 10−6. Over the pruned
charts, we ran 10 iterations of word-based ITG using
EM or VB. The charts were then pruned further by
applying the non-compositional constraint from the
Viterbi alignment links of that model. Finally we ran
10 iterations of phrase-based ITG over the residual
charts, using EM or VB, and extracted the Viterbi
alignments.
For translation, we used the standard phrasal de-
coding approach, based on a re-implementation of
the Pharaoh system (Koehn, 2004). The output of
the word alignment systems (GIZA++ or ITG) were
fed to a standard phrase extraction procedure that
extracted all phrases of length up to 7 and esti-
mated the conditional probabilities of source given
target and target given source using relative fre-
quencies. Thus our phrasal ITG learns only the
minimal non-compositional phrases; the standard
phrase-extraction algorithm learns larger combina-
tions of these minimal units. In addition the phrases
were annotated with lexical weights using the IBM
Model 1 tables. The decoder also used a trigram lan-
guage model trained on the target side of the training
data, as well as word count, phrase count, and distor-
tion penalty features. Minimum Error Rate training
(Och, 2003) over BLEU was used to optimize the
weights for each of these models over the develop-
ment test data.
We used the NIST 2002 evaluation datasets for
tuning and evaluation; the 10-reference develop-
ment set was used for minimum error rate training,
and the 4-reference test set was used for evaluation.
We trained several phrasal translation systems, vary-
ing only the word alignment (or phrasal alignment)
method.
Table 1 compares the four systems: the GIZA++
baseline, the ITG word-based model, the ITG multi-
word model using EM training, and the ITG multi-
word model using VB training. ITG-mwm-VB is
our best model. We see an improvement of nearly
</bodyText>
<table confidence="0.9980352">
Development Test
GIZA++ 37.46 28.24
ITG-word 35.47 26.55
ITG-mwm (VB) 39.21 29.02
ITG-mwm (EM) 39.15 28.47
</table>
<tableCaption confidence="0.9383165">
Table 1: Translation results on Chinese-English, using
the subset of training data (141K sentence pairs) that have
length limit 35 on both sides. (No length limit in transla-
tion. )
</tableCaption>
<bodyText confidence="0.9948824">
2 points dev set and nearly 1 point of improvement
on the test set. We also observe the consistent supe-
riority of VB over EM. The gain is especially large
on the test data set, indicating VB is less prone to
overfitting.
</bodyText>
<sectionHeader confidence="0.999256" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999929035714286">
We have presented an improved and more efficient
method of estimating phrase pairs directly. By both
changing the objective function to include a bias
toward sparser models and improving the pruning
techniques and efficiency, we achieve significant
gains on test data with practical speed. In addition,
these gains were shown without resorting to external
models, such as GIZA++. We have shown that VB
is both practical and effective for use in MT models.
However, our best system does not apply VB to a
single probability model, as we found an apprecia-
ble benefit from bootstrapping each model from sim-
pler models, much as the IBM word alignment mod-
els are usually trained in succession. We find that
VB alone is not sufficient to counteract the tendency
of EM to prefer analyses with smaller trees using
fewer rules and longer phrases. Both the tic-tac-toe
pruning and the non-compositional constraint ad-
dress this problem by reducing the space of possible
phrase pairs. On top of these hard constraints, the
sparse prior of VB helps make the model less prone
to overfitting to infrequent phrase pairs, and thus
improves the quality of the phrase pairs the model
learns.
Acknowledgments This work was done while the
first author was at Microsoft Research; thanks to Xi-
aodong He, Mark Johnson, and Kristina Toutanova.
The last author was supported by NSF IIS-0546554.
</bodyText>
<page confidence="0.998674">
104
</page>
<sectionHeader confidence="0.998348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915136986302">
Matthew Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience Unit, University College
London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311,
June.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 17–24, Rochester, New York, April. As-
sociation for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263–270, Ann Arbor, Michigan, USA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744–751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296–305.
Philipp Koehn. 2004. Pharaoh: A beam search de-
coder for phrase-based statistical machine translation
models. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 115–124, Washington, USA, Septem-
ber.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language. In
International Colloquium on Grammatical Inference,
pages 84–96, Tokyo, Japan.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Robert C. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In Proceedings
of EACL, Budapest, Hungary.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19–51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449, De-
cember.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings ofACL,
pages 160–167, Sapporo, Japan.
Frank Soong and Eng Huang. 1991. A tree-trellis based
fast search for finding the n best sentence hypotheses
in continuous speech recognition. In Proceedings of
ICASSP 1991.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836–741,
Copenhagen, Denmark.
Stephan Vogel. 2005. PESA: Phrase pair extraction as
sentence splitting. In MT Summit X, Phuket, Thailand.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403, Septem-
ber.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proceedings ofACL.
</reference>
<page confidence="0.999014">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.833807">
<title confidence="0.99994">Bayesian Learning of Non-compositional Phrases with Synchronous Parsing</title>
<author confidence="0.999413">Hao Zhang</author>
<affiliation confidence="0.999962">Computer Science Department University of Rochester</affiliation>
<address confidence="0.999506">Rochester, NY 14627</address>
<email confidence="0.999532">zhanghao@cs.rochester.edu</email>
<author confidence="0.998469">Robert C Moore</author>
<affiliation confidence="0.999581">Microsoft Research</affiliation>
<address confidence="0.9943275">One Microsoft Way Redmond, WA 98052 USA</address>
<email confidence="0.999891">bobmoore@microsoft.com</email>
<abstract confidence="0.99183247368421">We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs. The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large. Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results. Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Beal</author>
</authors>
<title>Variational Algorithms for Approximate Bayesian Inference.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Gatsby Computational Neuroscience Unit, University College London.</institution>
<contexts>
<context position="10538" citStr="Beal, 2003" startWordPosition="1690" endWordPosition="1691">ges. Kurihara and Sato (2006) describe VB for PCFGs, showing the only need is to change the M step of the EM algorithm. As in the case of maximum likelihood estimation, Bayesian estimation for ITGs is very similar to PCFGs, which follows due to the strong isomorphism between the two models. Specific to our ITG case, the M step becomes: P˜ (l+1)exp(ψ(E(X — [X X]) + αX)) [] =exp(ψ(E(X) + sαX)) , P˜(l+1) = exp(ψ(E(X — (X X)) + αX)) 0 exp(ψ(E(X) + sαX)) , p(l+1) _ exp(ψ(E(X — C) + αX)) PC&amp;quot; exp(ψ(E(X) + sαX)) , P˜(l+1)(e/f) = exp(ψ(E(e/f) + αC)) exp(ψ(E(C) + mαC)), where ψ is the digamma function (Beal, 2003), s = 3 is the number of right-hand-sides for X, and m is the number of observed phrase pairs in the data. The sole difference between EM and VB with a sparse prior α is that the raw fractional counts c are replaced by exp(ψ(c + α)), an operation that resembles smoothing. As pointed out by Johnson (2007), in effect this expression adds to c a small value that asymptotically approaches α — 0.5 as c approaches oc, and 0 as c approaches 0. For small values of α the net effect is the opposite of typical smoothing, since it tends to redistribute probably mass away from unlikely events onto more lik</context>
</contexts>
<marker>Beal, 2003</marker>
<rawString>Matthew Beal. 2003. Variational Algorithms for Approximate Bayesian Inference. Ph.D. thesis, Gatsby Computational Neuroscience Unit, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1365" citStr="Brown et al. (1993)" startWordPosition="188" endWordPosition="191">models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, 97 Chris Quirk Microsoft Research One Microsoft Way Redmond, WA 98052 USA chrisq@microsoft.com Daniel Gildea Computer Science Department University of Rochester Rochester, NY 14627 gildea@cs.rochester.edu then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sente</context>
<context position="7528" citStr="Brown et al., 1993" startWordPosition="1181" endWordPosition="1184"> word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word 98 alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any constraint on the distribution of phrases, EM overfits the data by memorizing every sentence pair. A sparse prior over a multinomial distribution such as the distribution of phrase pairs may bias the estimator toward skewed distributions that generalize better. In the context of phrasal models, this means learning the more representative phrases in the space of all possible phrases. The Dirichlet distribution, which is paramet</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="19572" citStr="Cherry and Lin (2007)" startWordPosition="3368" endWordPosition="3371">ly speaking, the goal of this section is the same as the previous section, namely, to limit the set of phrase pairs that needs to be considered in the training process. The tic-tac-toe pruning relies on IBM model 1 for scoring a given aligned area. In this part, we use word-based ITG alignments as anchor points in the alignment space to pin down the potential phrases. The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-toone word alignments. The heuristic method is based on the NonCompositional Constraint of Cherry and Lin (2007). Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phras</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17–24, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="20247" citStr="Chiang (2005)" startWordPosition="3484" endWordPosition="3485">gh precision as anchor points in the bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ez...j, and f(l, m) count the number of word links emitted from the substring fl...ry,t. The non-compositional phrase pairs satisfy e(i, j) = f(l, m) G 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main contributions of this </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263–270, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>744--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7097" citStr="Goldwater and Griffiths (2007)" startWordPosition="1111" endWordPosition="1114">r non-zero values. Our second approach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word 98 alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>296--305</pages>
<contexts>
<context position="7116" citStr="Johnson (2007)" startWordPosition="1116" endWordPosition="1117">ach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word 98 alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any constraint on the </context>
<context position="9777" citStr="Johnson (2007)" startWordPosition="1549" endWordPosition="1550">f the phrase pairs. αC is crucial, since the multinomial it is controlling has a high dimension. By adjusting αC to a very small number, we hope to place more posterior mass on parsimonious solutions with fewer but more confident and general phrase pairs. Having defined the Bayesian model, it remains to decide the inference procedure. We chose Variational Bayes, for its procedural similarity to EM and ease of implementation. Another potential option would be Gibbs sampling (or some other sampling technique). However, in experiments in unsupervised POS tag learning using HMM structured models, Johnson (2007) shows that VB is more effective than Gibbs sampling in approaching distributions that agree with the Zipf’s law, which is prominent in natural languages. Kurihara and Sato (2006) describe VB for PCFGs, showing the only need is to change the M step of the EM algorithm. As in the case of maximum likelihood estimation, Bayesian estimation for ITGs is very similar to PCFGs, which follows due to the strong isomorphism between the two models. Specific to our ITG case, the M step becomes: P˜ (l+1)exp(ψ(E(X — [X X]) + αX)) [] =exp(ψ(E(X) + sαX)) , P˜(l+1) = exp(ψ(E(X — (X X)) + αX)) 0 exp(ψ(E(X) + sα</context>
<context position="23613" citStr="Johnson (2007)" startWordPosition="4049" endWordPosition="4050">ER for EM increases to .42. On the other hand, using VB, AER decreases monotonically over the 10 iterations and 1e-009 1e-006 0.001 1 Prior value Figure 4: AER drops as αC approaches zero; a more sparse solution leads to better results. stabilizes at iteration 10. When αC is 1e − 9, VB gets AER close to .35 at iteration 10. As we increase the bias toward sparsity, the AER decreases, following a long slow plateau. Although the magnitude of improvement is not large, the trend is encouraging. These experiments also indicate that a very sparse prior is needed for machine translation tasks. Unlike Johnson (2007), who found optimal performance when α was approximately 10−4, we observed monotonic increases in performance as α dropped. The dimensionality of this MT problem is significantly larger than that of the sequence problem, though, therefore it may take a stronger push from the prior to achieve the desired result. 7.2 End-to-end Evaluation Given an unlimited amount of time, we would tune the prior to maximize end-to-end performance, using an objective function such as BLEU. Unfortunately these experiments are very slow. Since we observed monotonic increases in alignment performance with smaller v</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>115--124</pages>
<location>Washington, USA,</location>
<contexts>
<context position="25360" citStr="Koehn, 2004" startWordPosition="4351" endWordPosition="4352"> translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τ3 = 1 × 10−6. Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB, and extracted the Viterbi alignments. For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system (Koehn, 2004). The output of the word alignment systems (GIZA++ or ITG) were fed to a standard phrase extraction procedure that extracted all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target sid</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA), pages 115–124, Washington, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenichi Kurihara</author>
<author>Taisuke Sato</author>
</authors>
<title>Variational bayesian grammar induction for natural language.</title>
<date>2006</date>
<booktitle>In International Colloquium on Grammatical Inference,</booktitle>
<pages>84--96</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="9956" citStr="Kurihara and Sato (2006)" startWordPosition="1578" endWordPosition="1581">mass on parsimonious solutions with fewer but more confident and general phrase pairs. Having defined the Bayesian model, it remains to decide the inference procedure. We chose Variational Bayes, for its procedural similarity to EM and ease of implementation. Another potential option would be Gibbs sampling (or some other sampling technique). However, in experiments in unsupervised POS tag learning using HMM structured models, Johnson (2007) shows that VB is more effective than Gibbs sampling in approaching distributions that agree with the Zipf’s law, which is prominent in natural languages. Kurihara and Sato (2006) describe VB for PCFGs, showing the only need is to change the M step of the EM algorithm. As in the case of maximum likelihood estimation, Bayesian estimation for ITGs is very similar to PCFGs, which follows due to the strong isomorphism between the two models. Specific to our ITG case, the M step becomes: P˜ (l+1)exp(ψ(E(X — [X X]) + αX)) [] =exp(ψ(E(X) + sαX)) , P˜(l+1) = exp(ψ(E(X — (X X)) + αX)) 0 exp(ψ(E(X) + sαX)) , p(l+1) _ exp(ψ(E(X — C) + αX)) PC&amp;quot; exp(ψ(E(X) + sαX)) , P˜(l+1)(e/f) = exp(ψ(E(e/f) + αC)) exp(ψ(E(C) + mαC)), where ψ is the digamma function (Beal, 2003), s = 3 is the num</context>
</contexts>
<marker>Kurihara, Sato, 2006</marker>
<rawString>Kenichi Kurihara and Taisuke Sato. 2006. Variational bayesian grammar induction for natural language. In International Colloquium on Grammatical Inference, pages 84–96, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2829" citStr="Marcu and Wong (2002)" startWordPosition="405" endWordPosition="408">ues is unite the word-level and phrase-level models into one learning procedure. Ideally, such a procedure would remedy the deficiencies of word-level alignment models, including the strong restrictions on the form of the alignment, and the strong independence assumption between words. Furthermore it would obviate the need for heuristic combination of word alignments. A unified procedure may also improve the identification of non-compositional phrasal translations, and the attachment decisions for unaligned words. In this direction, Expectation Maximization at the phrase level was proposed by Marcu and Wong (2002), who, however, experienced two major difficulties: computational complexity and controlling overfitting. Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair. In this paper, we attempt to address these two issues in order to apply EM above the word level. Proceedings of ACL-08: HLT, pages 97–105, Columbus, Ohio, USA, June 2008. c�2008 Association for Compu</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Learning translations of namedentity phrases from parallel corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="12091" citStr="Moore (2003)" startWordPosition="1980" endWordPosition="1981">tree (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j,l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j,l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and</context>
</contexts>
<marker>Moore, 2003</marker>
<rawString>Robert C. Moore. 2003. Learning translations of namedentity phrases from parallel corpora. In Proceedings of EACL, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="24592" citStr="Och and Ney, 2003" startWordPosition="4217" endWordPosition="4220">unt of time, we would tune the prior to maximize end-to-end performance, using an objective function such as BLEU. Unfortunately these experiments are very slow. Since we observed monotonic increases in alignment performance with smaller values of αC, we simply fixed the prior at a very small value (10−100) for all translation experiments. We do compare VB against EM in terms of final BLEU scores in the translation experiments to ensure that this sparse prior has a sig0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 VB EM 103 nificant impact on the output. We also trained a baseline model with GIZA++ (Och and Ney, 2003) following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. We computed Chinese-toEnglish and English-to-Chinese word translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τ3 = 1 × 10−6. Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB,</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="20229" citStr="Och and Ney (2004)" startWordPosition="3479" endWordPosition="3482">rsections which have high precision as anchor points in the bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ez...j, and f(l, m) count the number of word links emitted from the substring fl...ry,t. The non-compositional phrase pairs satisfy e(i, j) = f(l, m) G 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main cont</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="26093" citStr="Och, 2003" startWordPosition="4469" endWordPosition="4470">d all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target side of the training data, as well as word count, phrase count, and distortion penalty features. Minimum Error Rate training (Och, 2003) over BLEU was used to optimize the weights for each of these models over the development test data. We used the NIST 2002 evaluation datasets for tuning and evaluation; the 10-reference development set was used for minimum error rate training, and the 4-reference test set was used for evaluation. We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method. Table 1 compares the four systems: the GIZA++ baseline, the ITG word-based model, the ITG multiword model using EM training, and the ITG multiword model using VB training. ITG-mwm-VB is our </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Soong</author>
<author>Eng Huang</author>
</authors>
<title>A tree-trellis based fast search for finding the n best sentence hypotheses in continuous speech recognition.</title>
<date>1991</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<contexts>
<context position="16633" citStr="Soong and Huang, 1991" startWordPosition="2859" endWordPosition="2862">otherwise S[a, L] := S[a − 1, L] · O(a) S[a, M] := max σ∈L,M S[a, R] := max {S[a − 1, Q]} · O(a) σ∈M,R {S[a − 1, Q]} · I(a) Then VF (i, j) = S[n + 1, R], using the isomorphism between state sequences and spans. This linear time algorithm allows us to compute span pruning in O(n3) time. The same algorithm may be performed using the backward figure of merit after transposing rows and columns. Having cast the problem in terms of finite state automata, we can use finite state algorithms for pruning. For instance, fixing a source span we can enumerate the target spans in decreasing order by score (Soong and Huang, 1991), stopping once we encounter the first span below threshold. In practice the overhead of maintaining the priority queue outweighs any benefit, as seen in Figure 2. An alternate approach that avoids this overhead is to enumerate spans by position. Note that S[m, R] · Qna=m+1 O(a) is within threshold iff there is a span with right boundary m′ &lt; m within threshold. Furthermore if S[m, M] · Qna=m+1 O(a) is within threshold, then m is the right boundary within threshold. Using these facts, we can gradually sweep the right boundary m from n toward 1 until the first condition fails to hold. For each </context>
</contexts>
<marker>Soong, Huang, 1991</marker>
<rawString>Frank Soong and Eng Huang. 1991. A tree-trellis based fast search for finding the n best sentence hypotheses in continuous speech recognition. In Proceedings of ICASSP 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>836--741</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1389" citStr="Vogel et al. (1996)" startWordPosition="193" endWordPosition="196">ble, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, 97 Chris Quirk Microsoft Research One Microsoft Way Redmond, WA 98052 USA chrisq@microsoft.com Daniel Gildea Computer Science Department University of Rochester Rochester, NY 14627 gildea@cs.rochester.edu then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of COLING, pages 836–741, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>PESA: Phrase pair extraction as sentence splitting.</title>
<date>2005</date>
<booktitle>In MT Summit X,</booktitle>
<location>Phuket, Thailand.</location>
<contexts>
<context position="12108" citStr="Vogel (2005)" startWordPosition="1983" endWordPosition="1984">erminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j,l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j,l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB. Only those s</context>
</contexts>
<marker>Vogel, 2005</marker>
<rawString>Stephan Vogel. 2005. PESA: Phrase pair extraction as sentence splitting. In MT Summit X, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4182" citStr="Wu, 1997" startWordPosition="610" endWordPosition="611">arning small noncompositional phrases. We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases. We test our model by extracting longer phrases from our model’s alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments. 2 Phrasal Inversion Transduction Grammar We use a phrasal extension of Inversion Transduction Grammar (Wu, 1997) as the generative framework. Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs. There are three rules with X on the left-hand side: X —* [X X], X —* (X X), X —* C. The first two rules are the straight rule and inverted rule respectively. They split the left-hand side constituent which represents a phrase pair into two smaller phrase pairs on the right-hand side and order them according to one of the two possible permutations. The rewriting process continues until the third</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="12138" citStr="Zhang and Gildea (2005)" startWordPosition="1986" endWordPosition="1989">minals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j,l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j,l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB. Only those spans that pass the pruning thr</context>
<context position="18162" citStr="Zhang and Gildea (2005)" startWordPosition="3132" endWordPosition="3135">ma=l I(a) · Qna=m+1 O(a) is within threshold, then (i, j,l, m) is a bitext cell within threshold. For right edges that are known to be within threshold, we can sweep the left edges leftward until the first condition no longer holds, keeping only those spans for which the second condition holds. The filtering algorithm behaves extremely well. Although the worst case runtime is still O(n4), the best case has improved to n3; empirically it seems to significantly reduce the amount of time spent exploring spans. Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005). 101 Figure 3: Example output from the ITG using non-compositional phrases. (a) is the Viterbi alignment from the wordbased ITG. The shaded regions indicate phrasal alignments that are allowed by the non-compositional constraint; all other phrasal alignments will not be considered. (b) is the Viterbi alignment from the phrasal ITG, with the multi-word alignments highlighted. 5 Bootstrapping Phrasal ITG from Word-based ITG This section introduces a technique that bootstraps candidate phrase pairs for phrase-based ITG from word-based ITG Viterbi alignments. The wordbased ITG uses the same expan</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>