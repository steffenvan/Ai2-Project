<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000063">
<title confidence="0.85342">
Can Natural Language Processing Become Natural Language Coaching?
</title>
<author confidence="0.360718">
Marti A. Hearst
</author>
<address confidence="0.628354">
UC Berkeley
Berkeley, CA 94720
</address>
<email confidence="0.98817">
hearst@berkeley.edu
</email>
<sectionHeader confidence="0.993642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972090909091">
How we teach and learn is undergoing a
revolution, due to changes in technology
and connectivity. Education may be one
of the best application areas for advanced
NLP techniques, and NLP researchers
have much to contribute to this problem,
especially in the areas of learning to write,
mastery learning, and peer learning. In
this paper I consider what happens when
we convert natural language processors
into natural language coaches.
</bodyText>
<sectionHeader confidence="0.8527295" genericHeader="keywords">
1 Why Should You Care, NLP
Researcher?
</sectionHeader>
<bodyText confidence="0.999913088235294">
There is a revolution in learning underway. Stu-
dents are taking Massive Open Online Courses as
well as online tutorials and paid online courses.
Technology and connectivity makes it possible for
students to learn from anywhere in the world, at
any time, to fit their schedules. And in today’s
knowledge-based economy, going to school only
in one’s early years is no longer enough; in future
most people are going to need continuous, life-
long education.
Students are changing too — they expect to
interact with information and technology. For-
tunately, pedagogical research shows significant
benefits of active learning over passive methods.
The modern view of teaching means students work
actively in class, talk with peers, and are coached
more than graded by their instructors.
In this new world of education, there is a great
need for NLP research to step in and help. I hope
in this paper to excite colleagues about the pos-
sibilities and suggest a few new ways of looking
at them. I do not attempt to cover the field of
language and learning comprehensively, nor do I
claim there is no work in the field. In fact there
is quite a bit, such as a recent special issue on lan-
guage learning resources (Sharoff et al., 2014), the
long running ACL workshops on Building Edu-
cational Applications using NLP (Tetreault et al.,
2015), and a recent shared task competition on
grammatical error detection for second language
learners (Ng et al., 2014). But I hope I am cast-
ing a few interesting thoughts in this direction for
those colleagues who are not focused on this par-
ticular topic.
</bodyText>
<sectionHeader confidence="0.972201" genericHeader="method">
2 How Awkward
</sectionHeader>
<bodyText confidence="0.999955709677419">
Perhaps the least useful feedback that an instructor
writes next to a block of prose on a learner’s essay
is ‘awkward’. We know what this means: some-
thing about this text does not read fluently. But this
is not helpful feedback; if the student knew how to
make the wording flow, he or she would have writ-
ten it fluently in the first place! Useful feedback is
actionable: it provides steps to take to make im-
provements.
A challenge for the field of NLP is how to build
writing tutors or coaches – as opposed to graders
or scorers. There is a vast difference between a
tool that performs an assessment of writing and
one that coaches students to help them as they are
attempting to write.
Current practice uses the output of scorers to
give students a target to aim for: revise your essay
to get a higher score. An alternative is to design
a system that watches alongside a learner as they
write an essay, and coaches their work at all levels
of construction – phrase level, clause level, sen-
tence level, discourse level, paragraph level, and
essay level.
Grammar checking technology has been excel-
lent for years now (Heidorn, 2000), but instead of
just showing the right answer as grammar check-
ers do, a grammar coach should give hints and
scaffolding the way a tutor would – not giving the
answer explicitly, but showing the path and letting
the learner fill in the missing information. When
the learner makes incorrect choices, the parser
</bodyText>
<page confidence="0.914993">
1245
</page>
<note confidence="0.975402666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1245–1252,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.989177388888889">
can teach principles and lessons for the concep-
tual stage that the learner is currently at. Different
grammars could be developed for learners at dif-
ferent competency levels, as well as for different
first-second language pairings in the case of sec-
ond language learning.
This suggests a different approach for building
a parser than what is the current standard. I am
not claiming that this has not been suggested in
the past; for instance Schwind (1988) designed a
parser to explain errors to learners. However, be-
cause of the renewed interest in technology for
teaching, this may be a pivotal time to recon-
sider how we develop parsing technology: perhaps
we should think fundamentally about parsers as
coaches rather than parsers as critics.
This inversion can apply to other aspects of
NLP technology as well. For instance, Dale and
Kilgarriff (2011) have held a series of workshop
to produce algorithms to identify errors introduced
into texts by non-native writers in the warmly
named “Helping Our Own” shared task (Dale et
al., 2012). Using the technology developed for
tasks like these, the challenge is to go beyond rec-
ognizing and correcting the errors to helping the
writer understand why the choices they are making
are not correct. Another option is to target practice
questions tailored for learners based on errors in a
fun manner (as described below).
Of course, for decades, the field of Intelligent
Tutoring Systems (ITS) (VanLehn, 2011) has de-
veloped technology for this purpose, so what is
new about what I am suggesting? First, we know
as NLP researchers that language analysis requires
specific technology beyond standard algorithms,
and so advances in Intelligent Tutoring Systems
on language problems most likely requires col-
laboration with experts in NLP. And, apparently
such collaborations have not been as robust as they
might be (Borin, 2002; Meurers, 2012). So there
is an opportunity for new advances at the intersec-
tion of these two fields.
And second, the newly expanded interest in on-
line learning and technology makes possible the
access of information about student writing be-
havior on a large scale that was not possible in
the past. Imagine thousands of students in cas-
caded waves, tasked with writing essays on the
same topic, and receiving real-time suggestions
from different algorithms. The first wave of stu-
dent responses to the feedback would be used to
Figure 1: Wordcraft user interface showing a farm
scene with four characters, a fully formed sen-
tence, the word tray with candidate additional
words colored by part of speech, and tool bar.
When the child completes a sentence correctly, the
corresponding action is animated.
improve the algorithms and these results would be
fed into the next wave of student work, and so on.
Students and instructors could be encouraged to
give feedback via the user interface. Very rapid
cycles of iteration should lead to accelerated im-
provements in understanding of how the interfaces
and the algorithms could be improved. A revo-
lution in understanding of how to coach student
writing could result!
Algorithms could be designed to give feedback
for partially completed work: partially written
sentences in the case of a parser; partially com-
pleted paragraphs in the case of a discourse writ-
ing tool, and so on, rather than only assessing
completed work after the fact.
</bodyText>
<sectionHeader confidence="0.992058" genericHeader="method">
3 Karaoke Anyone?
</sectionHeader>
<bodyText confidence="0.999904833333333">
Beyond learning to write, new technology is
changing other aspects of language learning in
ways that should excite NLP researchers. In or-
der to write well, a student must have a good vo-
cabulary and must know syntax. Learning words
and syntax requires exposure to language in many
</bodyText>
<page confidence="0.984">
1246
</page>
<bodyText confidence="0.99997427027027">
contexts, both spoken and written, for a student’s
primary language was well as for learning a sec-
ond language.
Although computerized vocabulary tools have
been around for quite some time, the rise of mo-
bile, connected applications, the serious games
movement, and the idea of “microtasks” which
are done during interstices of time while out and
about during the day, opens the door to new ways
to expose students to repetitive learning tasks for
acquiring language (Edge et al., 2011). Some of
the most innovative approaches for teaching lan-
guage combine mobile apps with multimedia in-
formation.
For example, the Tip Tap Tones project (Edge
et al., 2012) attempts to help learners reduce the
the challenge of mastering a foreign phonetic sys-
tem by microtasking with minute-long episodes of
mobile gaming. This work focuses in particular
on helping learners acquire the tonal sound system
of Mandarin Chinese and combines gesture swipes
with audio on a smartphone.
The ToneWars app (Head et al., 2014) takes
this idea one step farther by linking second lan-
guage learners with native speakers in real time
to play a Tretis-like game against one another to
better learn Chinese pronunciation. The second
language learner feels especially motivated when
they are able to beat the native speaker, and the
native speaker contributes their expert tone record-
ings to the database, fine-tunes their understanding
of their own language, and enjoys the benefits of
tutoring others in a fun context.
Going beyond phonemes, the DuoLingo
second-language learning application (von Ahn,
2013) teaches syntax as well as vocabulary
through a game-based interface. For instance,
one of Duolingo’s games consists of a display of
a sentence in one language, and a jumbled list
of words in the opposing language presented as
cards to be dragged and dropped onto a tray in the
correct order to form a sentence. In some cases
the user must select between two confounding
choices, such as the articles “le” or “la” to modify
French nouns.
Our work on a game for children called Word-
Craft takes this idea one step further (Anand et al.,
2015) (see Figure 1). Children manipulate word
cards to build sentences which, when grammati-
cally well formed, come to life in a storybook-like
animated world to illustrate their meaning. Pre-
liminary studies of the use of Wordcraft found that
children between the ages of 4 and 8 were able to
observe how different sentence constructions re-
sulted in different meanings and encouraged chil-
dren to engage in metalinguistic discourse, espe-
cially when playing the game with another child.
A karaoke-style video simulation is used by the
Engkoo system to teach English to Chinese speak-
ers (Wang et al., 2012). The interface not only
generates audio for the English words, but also
shows the lip and facial shapes necessary for form-
ing English words using a 3D simulated model lip-
syncing the words in a highly realistic manner. To
generate a large number of sample sentences, the
text was drawn from bilingual sentence pairs from
the web.
These technologies have only become feasible
recently because of the combination of multime-
dia, fast audio and image processing, fast network
connectivity, and a connected population. NLP re-
searchers may want to let their imaginations con-
sider the possibilities that arise from this new and
potent combination.
</bodyText>
<sectionHeader confidence="0.938314" genericHeader="method">
4 Closing the Cheese Gap
</sectionHeader>
<bodyText confidence="0.999896846153846">
Salman Kahn, the creator of Kahn Academy, talks
about the “Swiss cheese” model of learning in
which students learn something only partly before
they are forced to move on to the next topic, build-
ing knowledge on a foundation filled with holes,
like the cheese of the same name (Khan, 2012).
This is akin to learning to ride a bicycle without
perfecting the balancing part. In standard school-
ing, students are made to move one from one les-
son to the next even if they only got 70, 80, 90%
correct on the test. By contrast, mastery learn-
ing requires a deep understanding, working with
knowledge and probing it from every angle, try-
ing out the ideas and applying them to solve real
problems.
In many cases, mastery learning also requires
practicing with dozens, hundreds, or even thou-
sands of different examples, and getting feedback
on those examples. Automation can help with
mastery learning by generating personalized prac-
tice examples that challenge and interest students.
Automatically generated examples also reduce the
cost of creating new questions for instructors who
are concerned about answer sharing among stu-
dents from previous runs of a course.
Recently, sophisticated techniques developed in
</bodyText>
<page confidence="0.960112">
1247
</page>
<bodyText confidence="0.999939549019608">
the programming languages field have begun to be
applied to automate repetitive and structured tasks
in education, including problem generation, solu-
tion generation, and feedback generation for com-
puter science and logic topics (Gulwani, 2014).
Closer to the subject at hand is the automated
generation of mathematical word problems that
are organized around themes of interest to kids,
such as “School of Wizardry” (Polozov et al.,
2015). The method allows the student to specify
personal preferences about the world and charac-
ters, and then creates mini “plots” for each word
problem by enforcing coherence across the sen-
tences using constraints in a logic programming
paradigm combined with hand-crafted discourse
tropes (constraints on logical graphs) and a natu-
ral language generation step. A sample generated
word problem is
Professor Alice assigns Elliot to make a
luck potion. He had to spend 9 hours
first reading the recipe in the textbook.
He spends several hours brewing 11 por-
tions of it. The potion has to be brewed
for 3 hours per portion. How many
hours did Elliot spend in total?
Results are close in terms of comprehensibility
and solubility to those of a textbook. The project’s
ultimate goal is to have the word problems actu-
ally tell a coherent story, but that challenge is still
an open one. But the programs can generate an
infinite number of problems with solutions. Other
work by the same research team generated person-
alized algebraic equation problems in a game en-
vironment and showed that students could achieve
mastery learning in 90 minutes or less during an
organized educational campaign (Liu et al., 2015).
Another way that NLP can help with mastery
learning is to aid instructors in the providing of
feedback on short answer test questions. There
has been significant work in this space (Kukich,
2000; Hirschman et al., 2000). The standard ap-
proach builds on the classic successful model of
essay scoring which compares the student’s text to
model essays using a similarity-based technique
such as LSA (Landauer et al., 2000; Mohler and
Mihalcea, 2009) or careful authoring of the answer
(Leacock and Chodorow, 2003).
Recent techniques pair with learning techniques
like Inductive Logic Programming with instructor
editing to induce logic rules that describe permis-
sible answers with high accuracy (Willis, 2015).
Unfortunately most approaches require quite a
large number of students’ answers to be marked
up manually by the instructor before the feedback
is accurate enough to be reliably used for a given
question; a recent study found on the order of 500-
800 items per question had to be marked up at
minimum in order to obtain acceptable correla-
tions with human scorers (Heilman and Madnani,
2015). This high initial cost makes the develop-
ment of hundreds of practice questions for a given
conceptual unit a daunting task for instructors.
Recent research in Learning at Scale has pro-
duced some interesting approaches to improving
“feedback at scale.” One approach (Brooks et al.,
2014) uses a variation on hierarchical text cluster-
ing in tandem with a custom user interface that al-
lows instructors to rapidly view clusters and deter-
mine which contain correct answers, incorrect an-
swers, and partially correct answers. This greatly
speeds up the markup time and allows instructors
to assign explanations to a large group of answers
with a click of a button.
An entirely different approach to providing
feedback that is becoming heavily used in Massive
Open Online Courses is peer feedback, in which
students assign grades or give feedback to other
students on their work (Hicks et al., 2015). Re-
searchers have studied how to refine the process
of peer feedback to train students to produce re-
views that come within a grade point of that of in-
structors, with the aid of carefully designed rubrics
(Kulkarni et al., 2013).
However, to ensure accurate feedback, several
peer assessments per assignment are needed in ad-
dition to a training exercise, and students some-
times complain about workload. To reduce the ef-
fort, Kulkarni et al. (2014) experimented with a
workflow that uses machine grading as a first step.
After training a machine learning algorithm for a
given assignment, assignments are scored by the
algorithm. The less confident the algorithm is in
its score, the more students are assigned to grade
the assignment, but high-confidence assignments
may need only one peer grader. This step was
found to successfully reduce the amount of feed-
back needed to be done with a moderate decrease
in grading performance. That said, the algorithm
did require the instructors to mark up 500 sam-
ple assignments, and there is room for improve-
ment in the algorithm in other ways, since only
a first pass at NLP techniques was used to date.
</bodyText>
<page confidence="0.958583">
1248
</page>
<bodyText confidence="0.996107">
Nonetheless, mixing machine and peer grading is
a promising technique to explore, as it has been
found to be useful in other contexts (Nguyen and
Litman, 2014; Kukich, 2000).
</bodyText>
<sectionHeader confidence="0.928828" genericHeader="method">
5 Are You a FakeBot?
</sectionHeader>
<bodyText confidence="0.999992355555556">
Why is the completion rate of MOOCs so low?
This question vexes proponents and opponents of
MOOCs alike. Counting the window shopping en-
rollees of a MOOC who do not complete a course
is akin to counting everyone who visits a col-
lege campus as a failed graduate of that univer-
sity; many people are just checking the course out
(Jordan, 2014). That said, although the anytime,
anywhere aspect of online courses works well for
many busy professionals who are self-directed, re-
search shows that most people need to learn in an
environment that includes interacting with other
people.
Learning with others can refer to instructors and
tutors, and online tutoring systems have had suc-
cess comparable to that of human tutors in some
cases (VanLehn, 2011; Aleven et al., 2004). But
another important component of learning with oth-
ers refers to learning with other students. Lit-
erally hundreds of research papers show that an
effective way to help students learn is to have
them talk together in small groups, called struc-
tured peer learning, collaborative learning, or co-
operative learning (Johnson et al., 1991; Lord,
1998). In the classroom, this consists of activities
in which students confer in small groups to discuss
conceptual questions and to engage in problem-
solving. Studies and meta-analyses show the sig-
nificant pedagogical benefit of peer learning in-
cluding improved critical thinking skills, retention
of learned information, interest in subject matter,
and class morale (Hake, 1998; Millis and Cottell,
1998; Springer et al., 1999; Smith et al., 2009;
Deslauriers et al., 2011). Even studies of intelli-
gent tutoring systems find it hard to do better than
just having students discuss homework problems
in a structured setting online (Kumar et al., 2007).
The reasons for the success of peer learning in-
clude: students are at similar levels of understand-
ing that experts can no longer relate to well, people
learn material better when they have to explain it
to others, and identify the gaps in their current un-
derstanding, and the techniques of structured peer
learning introduce activities and incentives to help
students help one another.
</bodyText>
<equation confidence="0.58208975">
S2 I think E is the right answer
S1 Hi, I think E is right, too
S3 Hi! This seems to be a nurture vs nature
question.
S3 Can scent be learned, or only at birth?
S2 Yeah, but answer A supports the author’s
conclusion
S1 I felt that about A too
</equation>
<bodyText confidence="0.854161333333333">
S2 But the question was, which statement
would weaken the author’s conclusion
S3 So I choose A, showing that scent can be
learned at not only AT BIRTH.
S2 That’s why I think E is right
S3 Are you real, or fake?
</bodyText>
<listItem confidence="0.376366157894737">
S2 real
S1 I didn’t think that b or d had anything to
do with the statement
S3 Actually what you said makes sense.
S1 So, do we all agree that E was the correct
answer?
S2 I think so, yes.
S3 But I’m sticking with A since “no other
water could stimulate olfactory sites” abd
I suggests that other water could be de-
tected.
S3 *and
S1 I thought about c for awhile but it didn’t
really seem to have anything to do with
the topic of scent
S3 It has to be A or E. Other ones don’t have
anything do do with the question.
S2 but that “no other water” thing applies
equally well to E
</listItem>
<equation confidence="0.514052666666667">
S3 E is still about spawing ground water, I
think. this is a confusing question.
S1 I thought E contradicted the statement the
most
S2 me too
S3 I loving hits with other mturkers
</equation>
<tableCaption confidence="0.809197">
Table 1: Transcript of a conversation among three
</tableCaption>
<bodyText confidence="0.9950084">
crowdworkers who discussed the options for a
multiple choice question for a GMAT logical rea-
soning task. Note the meta-discussion about the
prevalence of robots on the crowdsourcing plat-
form.
</bodyText>
<page confidence="0.989528">
1249
</page>
<bodyText confidence="0.999917333333334">
In our MOOCChat research, we were inter-
ested in bringing structured peer learning into the
MOOC setting. We first tried out the idea on
a crowdsourcing platform (Coetzee et al., 2015),
showing that when groups of 3 workers discussed
challenging problems together, and especially if
they were incentivized to help each other arrive
at the correct answer, they achieved better results
than working alone. (A sample conversation is
shown in Table 1.) We also found that provid-
ing a mini-lesson in which workers consider the
principles underlying the tested concept and jus-
tify their answers leads to further improvements,
and combining the mini-lesson with the discussion
of the corresponding multiple-choice question in a
group of 3 leads to significant improvements on
that question. Crowd workers also expressed pos-
itive subjective responses to the peer interactions,
suggesting that discussions can improve morale in
remote work or learning settings.
When we tested the synchronous small-group
discussions in a live MOOC we found that, for
those students that were successfully placed into a
group of 3 for discussion, they were quite positive
about the experience (Lim et al., 2014). However,
there are significant challenges in getting students
to coordinate synchronously in very large low-cost
courses (Kotturi et al., 2015).
There is much NLP research to be done to en-
hance the online dialogues that are associated with
student discussion text beyond the traditional role
of intelligent tutoring systems. One idea is to mon-
itor discussions in real time and try to shape the
way the group works together (Tausczik and Pen-
nebaker, 2013). Another idea is to automatically
assess if students are discussing content at appro-
priate levels on Bloom’s taxonomy of educational
objectives (Krathwohl, 2002).
In our MOOCChat work with triad discussions
we observed that more workers will change their
answer from an incorrect to a correct one if at least
one member of the group starts out correct than
if no one is correct initially (Hearst et al., 2015).
We also noticed that if all group members start
out with the same answer — right or wrong — no
one is likely to change their answer in any direc-
tion. This behavior pattern suggests an interesting
idea for large scale online group discussions that
are not feasible in in-person environments: dy-
namically assign students to groups depending on
what their initial answers to questions are, and dy-
namically regroup students according to the mis-
conceptions and correct conceptions they have.
Rather than building an intelligent tutoring sys-
tem to prompt students with just the right state-
ment at just the right time, a more successful strat-
egy might be to mix students with other poeple
who for that particular discussion point have the
just the right level of conceptual understanding to
move the group forward.
</bodyText>
<sectionHeader confidence="0.99633" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9995145">
In this paper I am suggesting inverting the stan-
dard mode of our field from that of processing,
correcting, identifying, and generating aspects of
language to one of recognizing what a person is
doing with language: NLP algorithms as coaches
rather than critics. I have outlined a number of
specific suggestions for research that are currently
outside the mainstream of NLP research but which
pose challenges that I think some of my colleagues
will find interesting. Among these are text ana-
lyzers that explain what is wrong with an essay at
the clause, sentence, discourse level as the student
writes it, promoting mastery learning by generat-
ing unlimited practice problems, with answers, in
a form that makes practice fun, and using NLP to
improve the manner in which peers learning takes
place online. The field of learning and education
is being disrupted, and NLP researchers should be
helping push the frontiers.
Acknowledgements I thank the ACL program
chairs Michael Strube and Chengqing Zong for
inviting me to write this paper and keynote talk,
Lucy Vanderwende for suggested references, and
colleagues at NAACL 2015 for discussing these
ideas with me. This research is supported in part
by a Google Social Interactions Grant.
</bodyText>
<sectionHeader confidence="0.988099" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.943781833333333">
Vincent Aleven, Amy Ogan, Octav Popescu, Cristen
Torrey, and Kenneth Koedinger. 2004. Evaluat-
ing the effectiveness of a tutorial dialogue system
for self-explanation. In Intelligent tutoring systems,
pages 443–454. Springer.
Divya Anand, Shreyas, Sonali Sharma, Victor
Starostenko, Ashley DeSouza, Kimiko Ryokai, and
Marti A. Hearst. 2015. Wordcraft: Playing with
Sentence Structure. Under review.
Lars Borin. 2002. What have you done for me lately?
The fickle alignment of NLP and CALL. Reports
from Uppsala Learning Lab.
</reference>
<page confidence="0.973295">
1250
</page>
<note confidence="0.89308475">
Michael Brooks, Sumit Basu, Charles Jacobs, and Lucy
Vanderwende. 2014. Divide and correct: Using
clusters to grade short answers at scale. In Pro-
ceedings of the first ACM conference on Learn-
</note>
<reference confidence="0.985350723809524">
ing@Scale, pages 89–98. ACM.
D Coetzee, Seongtaek Lim, Armando Fox, Bjorn Hart-
mann, and Marti A Hearst. 2015. Structuring inter-
actions for large-scale synchronous peer learning. In
Proceedings of the 18th ACM Conference on Com-
puter Supported Cooperative Work &amp; Social Com-
puting, pages 1139–1152. ACM.
Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, pages 242–249. Association for
Computational Linguistics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Seventh Workshop on Building Educa-
tional Applications Using NLP, pages 54–62. Asso-
ciation for Computational Linguistics.
Louis Deslauriers, Ellen Schelew, and Carl Wieman.
2011. Improved learning in a large-enrollment
physics class. Science, 332(6031):862–864.
Darren Edge, Elly Searle, Kevin Chiu, Jing Zhao, and
James A Landay. 2011. Micromandarin: mobile
language learning in context. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems, pages 3169–3178. ACM.
Darren Edge, Kai-Yin Cheng, Michael Whitney, Yao
Qian, Zhijie Yan, and Frank Soong. 2012. Tip tap
tones: mobile microtraining of mandarin sounds. In
Proceedings of the 14th International Conference on
Human-Computer Interaction with Mobile Devices
and Services, pages 427–430. ACM.
Sumit Gulwani. 2014. Example-based learning in
computer-aided stem education. Communications of
the ACM, 57(8):70–80.
Richard R Hake. 1998. Interactive-engagement ver-
sus traditional methods: A six-thousand-student sur-
vey of mechanics test data for introductory physics
courses. American Journal of Physics, 66(1):64–74.
Andrew Head, Yi Xu, and Jingtao Wang. 2014.
Tonewars: Connecting language learners and native
speakers through collaborative mobile games. In In-
telligent Tutoring Systems, pages 368–377. Springer.
Marti A Hearst, Armando Fox, D Coetzee, and Bjo-
ern Hartmann. 2015. All it takes is one: Evidence
for a strategy for seeding large scale peer learning
interactions. In Proceedings of the Second (2015)
ACM Conference on Learning@Scale, pages 381–
383. ACM.
George Heidorn. 2000. Intelligent writing assistance.
Handbook of Natural Language Processing, pages
181–207.
Michael Heilman and Nitin Madnani. 2015. The im-
pact of training data on automated short answer scor-
ing performance. In Proceedings of the Tenth Work-
shop on Building Educational Applications Using
NLP. Association for Computational Linguistics.
Catherine M Hicks, C Ailie Fraser, Purvi Desai, and
Scott Klemmer. 2015. Do numeric ratings im-
pact peer reviewers? In Proceedings of the Second
(2015) ACM Conference on Learning@ Scale, pages
359–362. ACM.
Lynette Hirschman, Eric Breck, Marc Light, John D
Burger, and Lisa Ferro. 2000. Automated grading
of short-answer tests. Intelligent Systems and their
Applications, IEEE, 15(5):22–37.
David W Johnson, Roger T Johnson, and Karl Aldrich
Smith. 1991. Active learning: Cooperation in the
college classroom. Interaction Book Company Ed-
ina, MN.
Katy Jordan. 2014. Initial trends in enrolment and
completion of massive open online courses. The In-
ternational Review Of Research In Open And Dis-
tributed Learning, 15(1).
Salman Khan. 2012. The One World Schoolhouse: Ed-
ucation Reimagined. Twelve.
Yasmine Kotturi, Chinmay Kulkarni, Michael S Bern-
stein, and Scott Klemmer. 2015. Structure and mes-
saging techniques for online peer learning systems
that increase stickiness. In Proceedings of the Sec-
ond (2015) ACM Conference on Learning@ Scale,
pages 31–38. ACM.
David R Krathwohl. 2002. A revision of Bloom’s
taxonomy: An overview. Theory into practice,
41(4):212–218.
Karen Kukich. 2000. Beyond automated essay scor-
ing. IEEE Intelligent Systems and their Applica-
tions, IEEE, 15(5):22–27.
Chinmay Kulkarni, Koh Pang Wei, Huy Le, Daniel
Chia, Kathryn Papadopoulos, Justin Cheng, Daphne
Koller, and Scott R Klemmer. 2013. Peer and
self assessment in massive online classes. In
ACM Transactions on Computer Human Interaction
(TOCHI), volume 20. ACM.
Chinmay E Kulkarni, Richard Socher, Michael S Bern-
stein, and Scott R Klemmer. 2014. Scaling short-
answer grading by combining peer assessment with
algorithmic scoring. In Proceedings of the first
ACM conference on Learning@Scale, pages 99–
108. ACM.
Rohit Kumar, Carolyn Penstein Ros´e, Yi-Chia Wang,
Mahesh Joshi, and Allen Robinson. 2007. Tutorial
dialogue as adaptive collaborative learning support.
Frontiers in Artificial Intelligence and Applications,
158:383.
</reference>
<page confidence="0.782438">
1251
</page>
<reference confidence="0.999736042553192">
Thomas K Landauer, Darrell Laham, and Peter W
Foltz. 2000. The intelligent essay assessor. IEEE
Intelligent Systems and their Applications, IEEE,
15(5):22–27.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389–405.
Seongtaek Lim, Derrick Coetzee, Bjoern Hartmann,
Armando Fox, and Marti A Hearst. 2014. Initial ex-
periences with small group discussions in moocs. In
Proceedings of the first ACM conference on Learn-
ing@Scale, pages 151–152. ACM.
Yun-En Liu, Christy Ballweber, Eleanor O’rourke,
Eric Butler, Phonraphee Thummaphan, and Zoran
Popovi´c. 2015. Large-scale educational campaigns.
ACM Transactions on Computer-Human Interaction
(TOCHI), 22(2):8.
Thomas Lord. 1998. Cooperative learning that re-
ally works in biology teaching: using constructivist-
based activities to challenge student teams. The
American Biology Teacher, 60(8):580–588.
Detmar Meurers. 2012. Natural language processing
and language learning. In The Encyclopedia of Ap-
plied Linguistics. Wiley Online Library.
Barbara J Millis and Philip G Cottell. 1998. Coop-
erative learning for higher education faculty. Oryx
Press (Phoenix, Ariz.).
Michael Mohler and Rada Mihalcea. 2009. Text-to-
text semantic similarity for automatic short answer
grading. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 567–575. Association
for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Chris-
tian Hadiwinoto, and Joel Tetreault. 2014. The
CoNLL-2013 shared task on grammatical error cor-
rection. In Proceedings of the Eighteenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task (CoNLL-2014 Shared Task), pages
1–12.
Huy V Nguyen and Diane J Litman. 2014. Improving
peer feedback prediction: The sentence level is right.
ACL 2014, page 99.
Oleksandr Polozov, Eleanor ORourke, Adam M Smith,
Luke Zettlemoyer, Sumit Gulwani, and Zoran
Popovic. 2015. Personalized mathematical word
problem generation. In Proceedings of the 24th
International Joint Conference on Artificial Intelli-
gence (IJCAI 2015). To appear.
Camilla Schwind. 1988. Sensitive parsing: error anal-
ysis and explanation in an intelligent language tutor-
ing system. In Proceedings of the 12th conference
on Computational Linguistics-Volume 2, pages 608–
613. Association for Computational Linguistics.
Serge Sharoff, Stefania Spina, and Sofie Johansson
Kokkinakis. 2014. Introduction to the special issue
on resources and tools for language learners. Lan-
guage Resources and Evaluation, 48(1):1–3.
Michelle K Smith, William B Wood, Wendy K Adams,
Carl Wieman, Jennifer K Knight, Nancy Guild, and
Tin Tin Su. 2009. Why peer discussion improves
student performance on in-class concept questions.
Science, 323(5910):122–124.
Leonard Springer, Mary Elizabeth Stanne, and
Samuel S Donovan. 1999. Effects of small-group
learning on undergraduates in science, mathematics,
engineering, and technology: A meta-analysis. Re-
view of educational research, 69(1):21–51.
Yla R Tausczik and James W Pennebaker. 2013. Im-
proving teamwork using real-time language feed-
back. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, pages 459–
468. ACM.
Joel Tetreault, Jill Burstein, and Claudia Leacock.
2015. Proceedings of the Tenth Workshop on Build-
ing Educational Applications Using NLP. Associa-
tion for Computational Linguistics.
Kurt VanLehn. 2011. The relative effectiveness of
human tutoring, intelligent tutoring systems, and
other tutoring systems. Educational Psychologist,
46(4):197–221.
Luis von Ahn. 2013. Duolingo: learn a language for
free while helping to translate the web. In Proceed-
ings of the 2013 International Conference on Intel-
ligent User Interfaces, pages 1–2. ACM.
Lijuan Wang, Yao Qian, Matthew R Scott, Gang Chen,
and Frank K Soong. 2012. Computer-assisted
audiovisual language learning (with online video).
Computer, 45(6):38–47.
Alistair Willis. 2015. Using nlp to support scalable
assessment of short free text responses. In Proceed-
ings of the Tenth Workshop on Building Educational
Applications Using NLP. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.993024">
1252
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997907">Can Natural Language Processing Become Natural Language Coaching?</title>
<author confidence="0.998134">A Marti</author>
<affiliation confidence="0.997656">UC</affiliation>
<address confidence="0.944897">Berkeley, CA</address>
<email confidence="0.998933">hearst@berkeley.edu</email>
<abstract confidence="0.98978916304348">How we teach and learn is undergoing a revolution, due to changes in technology and connectivity. Education may be one of the best application areas for advanced NLP techniques, and NLP researchers have much to contribute to this problem, especially in the areas of learning to write, mastery learning, and peer learning. In this paper I consider what happens when we convert natural language processors into natural language coaches. 1 Why Should You Care, NLP Researcher? There is a revolution in learning underway. Students are taking Massive Open Online Courses as well as online tutorials and paid online courses. Technology and connectivity makes it possible for students to learn from anywhere in the world, at any time, to fit their schedules. And in today’s knowledge-based economy, going to school only in one’s early years is no longer enough; in future most people are going to need continuous, lifelong education. Students are changing too — they expect to interact with information and technology. Fortunately, pedagogical research shows significant benefits of active learning over passive methods. The modern view of teaching means students work actively in class, talk with peers, and are coached more than graded by their instructors. In this new world of education, there is a great need for NLP research to step in and help. I hope in this paper to excite colleagues about the possibilities and suggest a few new ways of looking at them. I do not attempt to cover the field of language and learning comprehensively, nor do I claim there is no work in the field. In fact there is quite a bit, such as a recent special issue on language learning resources (Sharoff et al., 2014), the long running ACL workshops on Building Educational Applications using NLP (Tetreault et al., 2015), and a recent shared task competition on grammatical error detection for second language learners (Ng et al., 2014). But I hope I am casting a few interesting thoughts in this direction for those colleagues who are not focused on this particular topic. 2 How Awkward Perhaps the least useful feedback that an instructor writes next to a block of prose on a learner’s essay We know what this means: something about this text does not read fluently. But this is not helpful feedback; if the student knew how to make the wording flow, he or she would have written it fluently in the first place! Useful feedback is it provides steps to take to make improvements. A challenge for the field of NLP is how to build as opposed to There is a vast difference between a tool that performs an assessment of writing and one that coaches students to help them as they are attempting to write. Current practice uses the output of scorers to give students a target to aim for: revise your essay to get a higher score. An alternative is to design a system that watches alongside a learner as they write an essay, and coaches their work at all levels of construction – phrase level, clause level, sentence level, discourse level, paragraph level, and essay level. Grammar checking technology has been excellent for years now (Heidorn, 2000), but instead of just showing the right answer as grammar checkers do, a grammar coach should give hints and scaffolding the way a tutor would – not giving the answer explicitly, but showing the path and letting the learner fill in the missing information. When the learner makes incorrect choices, the parser 1245 Proceedings of the 53rd Annual Meeting of the Association for Computational the 7th International Joint Conference on Natural Language pages China, July 26-31, 2015. Association for Computational Linguistics can teach principles and lessons for the conceptual stage that the learner is currently at. Different grammars could be developed for learners at different competency levels, as well as for different first-second language pairings in the case of second language learning. This suggests a different approach for building a parser than what is the current standard. I am not claiming that this has not been suggested in the past; for instance Schwind (1988) designed a parser to explain errors to learners. However, because of the renewed interest in technology for teaching, this may be a pivotal time to reconsider how we develop parsing technology: perhaps we should think fundamentally about parsers as coaches rather than parsers as critics. This inversion can apply to other aspects of NLP technology as well. For instance, Dale and Kilgarriff (2011) have held a series of workshop to produce algorithms to identify errors introduced into texts by non-native writers in the warmly named “Helping Our Own” shared task (Dale et al., 2012). Using the technology developed for tasks like these, the challenge is to go beyond recognizing and correcting the errors to helping the writer understand why the choices they are making are not correct. Another option is to target practice questions tailored for learners based on errors in a fun manner (as described below). Of course, for decades, the field of Intelligent Tutoring Systems (ITS) (VanLehn, 2011) has developed technology for this purpose, so what is new about what I am suggesting? First, we know as NLP researchers that language analysis requires specific technology beyond standard algorithms, and so advances in Intelligent Tutoring Systems on language problems most likely requires collaboration with experts in NLP. And, apparently such collaborations have not been as robust as they might be (Borin, 2002; Meurers, 2012). So there is an opportunity for new advances at the intersection of these two fields. And second, the newly expanded interest in online learning and technology makes possible the access of information about student writing behavior on a large scale that was not possible in the past. Imagine thousands of students in cascaded waves, tasked with writing essays on the same topic, and receiving real-time suggestions from different algorithms. The first wave of student responses to the feedback would be used to Figure 1: Wordcraft user interface showing a farm scene with four characters, a fully formed sentence, the word tray with candidate additional words colored by part of speech, and tool bar. When the child completes a sentence correctly, the corresponding action is animated. improve the algorithms and these results would be fed into the next wave of student work, and so on. Students and instructors could be encouraged to give feedback via the user interface. Very rapid cycles of iteration should lead to accelerated improvements in understanding of how the interfaces and the algorithms could be improved. A revolution in understanding of how to coach student writing could result! Algorithms could be designed to give feedback for partially completed work: partially written sentences in the case of a parser; partially completed paragraphs in the case of a discourse writing tool, and so on, rather than only assessing completed work after the fact. 3 Karaoke Anyone? Beyond learning to write, new technology is changing other aspects of language learning in ways that should excite NLP researchers. In order to write well, a student must have a good vocabulary and must know syntax. Learning words and syntax requires exposure to language in many 1246 contexts, both spoken and written, for a student’s primary language was well as for learning a second language. Although computerized vocabulary tools have been around for quite some time, the rise of mobile, connected applications, the serious games movement, and the idea of “microtasks” which are done during interstices of time while out and about during the day, opens the door to new ways to expose students to repetitive learning tasks for acquiring language (Edge et al., 2011). Some of the most innovative approaches for teaching language combine mobile apps with multimedia information. For example, the Tip Tap Tones project (Edge et al., 2012) attempts to help learners reduce the the challenge of mastering a foreign phonetic system by microtasking with minute-long episodes of mobile gaming. This work focuses in particular on helping learners acquire the tonal sound system of Mandarin Chinese and combines gesture swipes with audio on a smartphone. The ToneWars app (Head et al., 2014) takes this idea one step farther by linking second language learners with native speakers in real time to play a Tretis-like game against one another to better learn Chinese pronunciation. The second language learner feels especially motivated when they are able to beat the native speaker, and the native speaker contributes their expert tone recordings to the database, fine-tunes their understanding of their own language, and enjoys the benefits of tutoring others in a fun context. Going beyond phonemes, the DuoLingo second-language learning application (von Ahn, 2013) teaches syntax as well as vocabulary through a game-based interface. For instance, one of Duolingo’s games consists of a display of a sentence in one language, and a jumbled list of words in the opposing language presented as cards to be dragged and dropped onto a tray in the correct order to form a sentence. In some cases the user must select between two confounding choices, such as the articles “le” or “la” to modify French nouns. Our work on a game for children called Word- Craft takes this idea one step further (Anand et al., 2015) (see Figure 1). Children manipulate word cards to build sentences which, when grammatically well formed, come to life in a storybook-like animated world to illustrate their meaning. Preliminary studies of the use of Wordcraft found that children between the ages of 4 and 8 were able to observe how different sentence constructions resulted in different meanings and encouraged children to engage in metalinguistic discourse, especially when playing the game with another child. A karaoke-style video simulation is used by the Engkoo system to teach English to Chinese speakers (Wang et al., 2012). The interface not only generates audio for the English words, but also shows the lip and facial shapes necessary for forming English words using a 3D simulated model lipsyncing the words in a highly realistic manner. To generate a large number of sample sentences, the text was drawn from bilingual sentence pairs from the web. These technologies have only become feasible recently because of the combination of multimedia, fast audio and image processing, fast network connectivity, and a connected population. NLP researchers may want to let their imaginations consider the possibilities that arise from this new and potent combination. 4 Closing the Cheese Gap Salman Kahn, the creator of Kahn Academy, talks about the “Swiss cheese” model of learning in which students learn something only partly before they are forced to move on to the next topic, building knowledge on a foundation filled with holes, like the cheese of the same name (Khan, 2012). This is akin to learning to ride a bicycle without perfecting the balancing part. In standard schooling, students are made to move one from one lesson to the next even if they only got 70, 80, 90% on the test. By contrast, learna deep understanding, working with knowledge and probing it from every angle, trying out the ideas and applying them to solve real problems. In many cases, mastery learning also requires practicing with dozens, hundreds, or even thousands of different examples, and getting feedback on those examples. Automation can help with mastery learning by generating personalized practice examples that challenge and interest students. Automatically generated examples also reduce the cost of creating new questions for instructors who are concerned about answer sharing among students from previous runs of a course. Recently, sophisticated techniques developed in 1247 the programming languages field have begun to be applied to automate repetitive and structured tasks in education, including problem generation, solution generation, and feedback generation for computer science and logic topics (Gulwani, 2014). Closer to the subject at hand is the automated generation of mathematical word problems that are organized around themes of interest to kids, such as “School of Wizardry” (Polozov et al., 2015). The method allows the student to specify personal preferences about the world and characters, and then creates mini “plots” for each word problem by enforcing coherence across the sentences using constraints in a logic programming paradigm combined with hand-crafted discourse tropes (constraints on logical graphs) and a natural language generation step. A sample generated word problem is Professor Alice assigns Elliot to make a luck potion. He had to spend 9 hours first reading the recipe in the textbook. He spends several hours brewing 11 portions of it. The potion has to be brewed for 3 hours per portion. How many hours did Elliot spend in total? Results are close in terms of comprehensibility and solubility to those of a textbook. The project’s ultimate goal is to have the word problems actually tell a coherent story, but that challenge is still an open one. But the programs can generate an infinite number of problems with solutions. Other work by the same research team generated personalized algebraic equation problems in a game environment and showed that students could achieve mastery learning in 90 minutes or less during an organized educational campaign (Liu et al., 2015). Another way that NLP can help with mastery learning is to aid instructors in the providing of feedback on short answer test questions. There has been significant work in this space (Kukich, 2000; Hirschman et al., 2000). The standard approach builds on the classic successful model of essay scoring which compares the student’s text to model essays using a similarity-based technique such as LSA (Landauer et al., 2000; Mohler and Mihalcea, 2009) or careful authoring of the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be marked up manually by the instructor before the feedback is accurate enough to be reliably used for a given question; a recent study found on the order of 500- 800 items per question had to be marked up at minimum in order to obtain acceptable correlations with human scorers (Heilman and Madnani, 2015). This high initial cost makes the development of hundreds of practice questions for a given conceptual unit a daunting task for instructors. Recent research in Learning at Scale has produced some interesting approaches to improving “feedback at scale.” One approach (Brooks et al., 2014) uses a variation on hierarchical text clustering in tandem with a custom user interface that allows instructors to rapidly view clusters and determine which contain correct answers, incorrect answers, and partially correct answers. This greatly speeds up the markup time and allows instructors to assign explanations to a large group of answers with a click of a button. An entirely different approach to providing feedback that is becoming heavily used in Massive Open Online Courses is peer feedback, in which students assign grades or give feedback to other students on their work (Hicks et al., 2015). Researchers have studied how to refine the process of peer feedback to train students to produce reviews that come within a grade point of that of instructors, with the aid of carefully designed rubrics (Kulkarni et al., 2013). However, to ensure accurate feedback, several peer assessments per assignment are needed in addition to a training exercise, and students sometimes complain about workload. To reduce the effort, Kulkarni et al. (2014) experimented with a workflow that uses machine grading as a first step. After training a machine learning algorithm for a given assignment, assignments are scored by the algorithm. The less confident the algorithm is in its score, the more students are assigned to grade the assignment, but high-confidence assignments may need only one peer grader. This step was found to successfully reduce the amount of feedback needed to be done with a moderate decrease in grading performance. That said, the algorithm did require the instructors to mark up 500 sample assignments, and there is room for improvement in the algorithm in other ways, since only a first pass at NLP techniques was used to date. 1248 Nonetheless, mixing machine and peer grading is a promising technique to explore, as it has been found to be useful in other contexts (Nguyen and Litman, 2014; Kukich, 2000). 5 Are You a FakeBot? Why is the completion rate of MOOCs so low? This question vexes proponents and opponents of MOOCs alike. Counting the window shopping enrollees of a MOOC who do not complete a course is akin to counting everyone who visits a college campus as a failed graduate of that university; many people are just checking the course out (Jordan, 2014). That said, although the anytime, anywhere aspect of online courses works well for many busy professionals who are self-directed, research shows that most people need to learn in an environment that includes interacting with other people. Learning with others can refer to instructors and tutors, and online tutoring systems have had success comparable to that of human tutors in some cases (VanLehn, 2011; Aleven et al., 2004). But another important component of learning with others refers to learning with other students. Literally hundreds of research papers show that an effective way to help students learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer learning including improved critical thinking skills, retention of learned information, interest in subject matter, and class morale (Hake, 1998; Millis and Cottell, 1998; Springer et al., 1999; Smith et al., 2009; Deslauriers et al., 2011). Even studies of intelligent tutoring systems find it hard to do better than just having students discuss homework problems in a structured setting online (Kumar et al., 2007). The reasons for the success of peer learning include: students are at similar levels of understanding that experts can no longer relate to well, people learn material better when they have to explain it to others, and identify the gaps in their current understanding, and the techniques of structured peer learning introduce activities and incentives to help students help one another. S2 I think E is the right answer S1 Hi, I think E is right, too S3 Hi! This seems to be a nurture vs nature question. S3 Can scent be learned, or only at birth? S2 Yeah, but answer A supports the author’s conclusion S1 I felt that about A too S2 But the question was, which statement would weaken the author’s conclusion S3 So I choose A, showing that scent can be learned at not only AT BIRTH. S2 That’s why I think E is right S3 Are you real, or fake? S2 real S1 I didn’t think that b or d had anything to do with the statement S3 Actually what you said makes sense. S1 So, do we all agree that E was the correct answer? S2 I think so, yes. S3 But I’m sticking with A since “no other water could stimulate olfactory sites” abd I suggests that other water could be detected. S3 *and S1 I thought about c for awhile but it didn’t really seem to have anything to do with the topic of scent S3 It has to be A or E. Other ones don’t have anything do do with the question. S2 but that “no other water” thing applies equally well to E S3 E is still about spawing ground water, I think. this is a confusing question. S1 I thought E contradicted the statement the most S2 me too S3 I loving hits with other mturkers Table 1: Transcript of a conversation among three crowdworkers who discussed the options for a multiple choice question for a GMAT logical reasoning task. Note the meta-discussion about the prevalence of robots on the crowdsourcing platform. 1249 In our MOOCChat research, we were interested in bringing structured peer learning into the MOOC setting. We first tried out the idea on a crowdsourcing platform (Coetzee et al., 2015), showing that when groups of 3 workers discussed challenging problems together, and especially if they were incentivized to help each other arrive at the correct answer, they achieved better results than working alone. (A sample conversation is shown in Table 1.) We also found that provida which workers consider the principles underlying the tested concept and justify their answers leads to further improvements, and combining the mini-lesson with the discussion of the corresponding multiple-choice question in a group of 3 leads to significant improvements on that question. Crowd workers also expressed positive subjective responses to the peer interactions, suggesting that discussions can improve morale in remote work or learning settings. When we tested the synchronous small-group discussions in a live MOOC we found that, for those students that were successfully placed into a group of 3 for discussion, they were quite positive about the experience (Lim et al., 2014). However, there are significant challenges in getting students to coordinate synchronously in very large low-cost courses (Kotturi et al., 2015). There is much NLP research to be done to enhance the online dialogues that are associated with student discussion text beyond the traditional role of intelligent tutoring systems. One idea is to monitor discussions in real time and try to shape the way the group works together (Tausczik and Pennebaker, 2013). Another idea is to automatically assess if students are discussing content at appropriate levels on Bloom’s taxonomy of educational objectives (Krathwohl, 2002). In our MOOCChat work with triad discussions we observed that more workers will change their answer from an incorrect to a correct one if at least one member of the group starts out correct than if no one is correct initially (Hearst et al., 2015). We also noticed that if all group members start out with the same answer — right or wrong — no one is likely to change their answer in any direction. This behavior pattern suggests an interesting idea for large scale online group discussions that are not feasible in in-person environments: dynamically assign students to groups depending on what their initial answers to questions are, and dynamically regroup students according to the misconceptions and correct conceptions they have. Rather than building an intelligent tutoring system to prompt students with just the right statement at just the right time, a more successful strategy might be to mix students with other poeple who for that particular discussion point have the just the right level of conceptual understanding to move the group forward. 6 Conclusions In this paper I am suggesting inverting the standard mode of our field from that of processing, correcting, identifying, and generating aspects of language to one of recognizing what a person is doing with language: NLP algorithms as coaches rather than critics. I have outlined a number of specific suggestions for research that are currently outside the mainstream of NLP research but which pose challenges that I think some of my colleagues will find interesting. Among these are text analyzers that explain what is wrong with an essay at the clause, sentence, discourse level as the student writes it, promoting mastery learning by generating unlimited practice problems, with answers, in a form that makes practice fun, and using NLP to improve the manner in which peers learning takes place online. The field of learning and education is being disrupted, and NLP researchers should be helping push the frontiers. thank the ACL program chairs Michael Strube and Chengqing Zong for inviting me to write this paper and keynote talk, Lucy Vanderwende for suggested references, and colleagues at NAACL 2015 for discussing these ideas with me. This research is supported in part by a Google Social Interactions Grant.</abstract>
<title confidence="0.581066">References</title>
<author confidence="0.766365">Vincent Aleven</author>
<author confidence="0.766365">Amy Ogan</author>
<author confidence="0.766365">Octav Popescu</author>
<author confidence="0.766365">Cristen</author>
<abstract confidence="0.8393465">Torrey, and Kenneth Koedinger. 2004. Evaluating the effectiveness of a tutorial dialogue system self-explanation. In tutoring pages 443–454. Springer.</abstract>
<address confidence="0.3635165">Divya Anand, Shreyas, Sonali Sharma, Victor Starostenko, Ashley DeSouza, Kimiko Ryokai, and</address>
<abstract confidence="0.694119">A. Hearst. 2015. Playing with Under review. Borin. 2002. have you done for me lately? fickle alignment of NLP and Reports from Uppsala Learning Lab. 1250 Michael Brooks, Sumit Basu, Charles Jacobs, and Lucy Vanderwende. 2014. Divide and correct: Using to grade short answers at scale. In Proceedings of the first ACM conference on Learnpages 89–98. ACM.</abstract>
<note confidence="0.932996344827586">D Coetzee, Seongtaek Lim, Armando Fox, Bjorn Hartmann, and Marti A Hearst. 2015. Structuring interactions for large-scale synchronous peer learning. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Compages 1139–1152. ACM. Robert Dale and Adam Kilgarriff. 2011. Helping our The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Lanpages 242–249. Association for Computational Linguistics. Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A report on the preposition and error correction shared task. In Proceedings of the Seventh Workshop on Building Educa- Applications Using pages 54–62. Association for Computational Linguistics. Louis Deslauriers, Ellen Schelew, and Carl Wieman. 2011. Improved learning in a large-enrollment class. 332(6031):862–864. Darren Edge, Elly Searle, Kevin Chiu, Jing Zhao, and James A Landay. 2011. Micromandarin: mobile learning in context. In of the SIGCHI Conference on Human Factors in Computpages 3169–3178. ACM. Darren Edge, Kai-Yin Cheng, Michael Whitney, Yao Qian, Zhijie Yan, and Frank Soong. 2012. Tip tap tones: mobile microtraining of mandarin sounds. In Proceedings of the 14th International Conference on</note>
<title confidence="0.57613">Human-Computer Interaction with Mobile Devices</title>
<abstract confidence="0.956956428571428">pages 427–430. ACM. Sumit Gulwani. 2014. Example-based learning in stem education. of 57(8):70–80. Richard R Hake. 1998. Interactive-engagement versus traditional methods: A six-thousand-student survey of mechanics test data for introductory physics</abstract>
<note confidence="0.7225757">Journal of 66(1):64–74. Andrew Head, Yi Xu, and Jingtao Wang. 2014. Tonewars: Connecting language learners and native through collaborative mobile games. In In- Tutoring pages 368–377. Springer. Marti A Hearst, Armando Fox, D Coetzee, and Bjoern Hartmann. 2015. All it takes is one: Evidence for a strategy for seeding large scale peer learning In of the Second (2015) Conference on pages 381–</note>
<abstract confidence="0.57678125">383. ACM. George Heidorn. 2000. Intelligent writing assistance. of Natural Language pages 181–207. Michael Heilman and Nitin Madnani. 2015. The impact of training data on automated short answer scorperformance. In of the Tenth Workshop on Building Educational Applications Using</abstract>
<note confidence="0.894630714285714">Association for Computational Linguistics. Catherine M Hicks, C Ailie Fraser, Purvi Desai, and Scott Klemmer. 2015. Do numeric ratings impeer reviewers? In of the Second ACM Conference on Learning@ pages 359–362. ACM. Lynette Hirschman, Eric Breck, Marc Light, John D Burger, and Lisa Ferro. 2000. Automated grading short-answer tests. Systems and their 15(5):22–37. David W Johnson, Roger T Johnson, and Karl Aldrich 1991. learning: Cooperation in the Interaction Book Company Edina, MN. Katy Jordan. 2014. Initial trends in enrolment and of massive open online courses. International Review Of Research In Open And Dis- 15(1). Khan. 2012. One World Schoolhouse: Ed- Twelve. Yasmine Kotturi, Chinmay Kulkarni, Michael S Bern-</note>
<abstract confidence="0.7245448">stein, and Scott Klemmer. 2015. Structure and messaging techniques for online peer learning systems increase stickiness. In of the Sec- (2015) ACM Conference on Learning@ pages 31–38. ACM.</abstract>
<note confidence="0.821781307692308">David R Krathwohl. 2002. A revision of Bloom’s An overview. into 41(4):212–218. Karen Kukich. 2000. Beyond automated essay scor- Intelligent Systems and their Applica- 15(5):22–27. Chinmay Kulkarni, Koh Pang Wei, Huy Le, Daniel Chia, Kathryn Papadopoulos, Justin Cheng, Daphne Koller, and Scott R Klemmer. 2013. Peer and self assessment in massive online classes. ACM Transactions on Computer Human Interaction volume 20. ACM. Chinmay E Kulkarni, Richard Socher, Michael S Bern-</note>
<abstract confidence="0.8255256">stein, and Scott R Klemmer. 2014. Scaling shortanswer grading by combining peer assessment with scoring. In of the first conference on pages 99– 108. ACM. Rohit Kumar, Carolyn Penstein Ros´e, Yi-Chia Wang, Mahesh Joshi, and Allen Robinson. 2007. Tutorial dialogue as adaptive collaborative learning support. in Artificial Intelligence and 158:383.</abstract>
<date confidence="0.652723">1251</date>
<note confidence="0.941621333333333">Thomas K Landauer, Darrell Laham, and Peter W 2000. The intelligent essay assessor. Systems and their Applications, 15(5):22–27. Claudia Leacock and Martin Chodorow. 2003. C-rater: scoring of short-answer questions. Comand the 37(4):389–405. Seongtaek Lim, Derrick Coetzee, Bjoern Hartmann, Armando Fox, and Marti A Hearst. 2014. Initial experiences with small group discussions in moocs. In Proceedings of the first ACM conference on Learnpages 151–152. ACM. Yun-En Liu, Christy Ballweber, Eleanor O’rourke, Eric Butler, Phonraphee Thummaphan, and Zoran Popovi´c. 2015. Large-scale educational campaigns. ACM Transactions on Computer-Human Interaction 22(2):8. Thomas Lord. 1998. Cooperative learning that re-</note>
<abstract confidence="0.7300485">ally works in biology teaching: using constructivistactivities to challenge student teams. Biology 60(8):580–588. Detmar Meurers. 2012. Natural language processing language learning. In Encyclopedia of Ap- Wiley Online Library.</abstract>
<note confidence="0.978854666666667">J Millis and Philip G Cottell. 1998. Cooplearning for higher education Oryx Press (Phoenix, Ariz.). Michael Mohler and Rada Mihalcea. 2009. Text-totext semantic similarity for automatic short answer In of the 12th Conference of the European Chapter of the Association for Compages 567–575. Association for Computational Linguistics. Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2014. The CoNLL-2013 shared task on grammatical error cor- In of the Eighteenth Conference on Computational Natural Language Learn- Shared Task (CoNLL-2014 Shared pages 1–12. Huy V Nguyen and Diane J Litman. 2014. Improving peer feedback prediction: The sentence level is right. page 99. Oleksandr Polozov, Eleanor ORourke, Adam M Smith, Luke Zettlemoyer, Sumit Gulwani, and Zoran Popovic. 2015. Personalized mathematical word generation. In of the 24th International Joint Conference on Artificial Intelli- (IJCAI To appear. Camilla Schwind. 1988. Sensitive parsing: error analysis and explanation in an intelligent language tutorsystem. In of the 12th conference Computational Linguistics-Volume pages 608– 613. Association for Computational Linguistics.</note>
<intro confidence="0.640556">Serge Sharoff, Stefania Spina, and Sofie Johansson</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vincent Aleven</author>
<author>Amy Ogan</author>
<author>Octav Popescu</author>
<author>Cristen Torrey</author>
<author>Kenneth Koedinger</author>
</authors>
<title>Evaluating the effectiveness of a tutorial dialogue system for self-explanation. In Intelligent tutoring systems,</title>
<date>2004</date>
<pages>443--454</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17910" citStr="Aleven et al., 2004" startWordPosition="2963" endWordPosition="2966"> MOOC who do not complete a course is akin to counting everyone who visits a college campus as a failed graduate of that university; many people are just checking the course out (Jordan, 2014). That said, although the anytime, anywhere aspect of online courses works well for many busy professionals who are self-directed, research shows that most people need to learn in an environment that includes interacting with other people. Learning with others can refer to instructors and tutors, and online tutoring systems have had success comparable to that of human tutors in some cases (VanLehn, 2011; Aleven et al., 2004). But another important component of learning with others refers to learning with other students. Literally hundreds of research papers show that an effective way to help students learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer learning including impr</context>
</contexts>
<marker>Aleven, Ogan, Popescu, Torrey, Koedinger, 2004</marker>
<rawString>Vincent Aleven, Amy Ogan, Octav Popescu, Cristen Torrey, and Kenneth Koedinger. 2004. Evaluating the effectiveness of a tutorial dialogue system for self-explanation. In Intelligent tutoring systems, pages 443–454. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Divya Anand</author>
<author>Sonali Sharma Shreyas</author>
<author>Victor Starostenko</author>
<author>Ashley DeSouza</author>
<author>Kimiko Ryokai</author>
<author>Marti A Hearst</author>
</authors>
<title>Wordcraft: Playing with Sentence Structure. Under review.</title>
<date>2015</date>
<contexts>
<context position="9692" citStr="Anand et al., 2015" startWordPosition="1606" endWordPosition="1609">ond phonemes, the DuoLingo second-language learning application (von Ahn, 2013) teaches syntax as well as vocabulary through a game-based interface. For instance, one of Duolingo’s games consists of a display of a sentence in one language, and a jumbled list of words in the opposing language presented as cards to be dragged and dropped onto a tray in the correct order to form a sentence. In some cases the user must select between two confounding choices, such as the articles “le” or “la” to modify French nouns. Our work on a game for children called WordCraft takes this idea one step further (Anand et al., 2015) (see Figure 1). Children manipulate word cards to build sentences which, when grammatically well formed, come to life in a storybook-like animated world to illustrate their meaning. Preliminary studies of the use of Wordcraft found that children between the ages of 4 and 8 were able to observe how different sentence constructions resulted in different meanings and encouraged children to engage in metalinguistic discourse, especially when playing the game with another child. A karaoke-style video simulation is used by the Engkoo system to teach English to Chinese speakers (Wang et al., 2012). </context>
</contexts>
<marker>Anand, Shreyas, Starostenko, DeSouza, Ryokai, Hearst, 2015</marker>
<rawString>Divya Anand, Shreyas, Sonali Sharma, Victor Starostenko, Ashley DeSouza, Kimiko Ryokai, and Marti A. Hearst. 2015. Wordcraft: Playing with Sentence Structure. Under review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Borin</author>
</authors>
<title>What have you done for me lately? The fickle alignment of NLP and CALL. Reports from Uppsala Learning Lab. ing@Scale,</title>
<date>2002</date>
<pages>89--98</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5788" citStr="Borin, 2002" startWordPosition="961" endWordPosition="962"> is to target practice questions tailored for learners based on errors in a fun manner (as described below). Of course, for decades, the field of Intelligent Tutoring Systems (ITS) (VanLehn, 2011) has developed technology for this purpose, so what is new about what I am suggesting? First, we know as NLP researchers that language analysis requires specific technology beyond standard algorithms, and so advances in Intelligent Tutoring Systems on language problems most likely requires collaboration with experts in NLP. And, apparently such collaborations have not been as robust as they might be (Borin, 2002; Meurers, 2012). So there is an opportunity for new advances at the intersection of these two fields. And second, the newly expanded interest in online learning and technology makes possible the access of information about student writing behavior on a large scale that was not possible in the past. Imagine thousands of students in cascaded waves, tasked with writing essays on the same topic, and receiving real-time suggestions from different algorithms. The first wave of student responses to the feedback would be used to Figure 1: Wordcraft user interface showing a farm scene with four charac</context>
</contexts>
<marker>Borin, 2002</marker>
<rawString>Lars Borin. 2002. What have you done for me lately? The fickle alignment of NLP and CALL. Reports from Uppsala Learning Lab. ing@Scale, pages 89–98. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Coetzee</author>
<author>Seongtaek Lim</author>
<author>Armando Fox</author>
<author>Bjorn Hartmann</author>
<author>Marti A Hearst</author>
</authors>
<title>Structuring interactions for large-scale synchronous peer learning.</title>
<date>2015</date>
<booktitle>In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing,</booktitle>
<pages>1139--1152</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="20930" citStr="Coetzee et al., 2015" startWordPosition="3500" endWordPosition="3503">y well to E S3 E is still about spawing ground water, I think. this is a confusing question. S1 I thought E contradicted the statement the most S2 me too S3 I loving hits with other mturkers Table 1: Transcript of a conversation among three crowdworkers who discussed the options for a multiple choice question for a GMAT logical reasoning task. Note the meta-discussion about the prevalence of robots on the crowdsourcing platform. 1249 In our MOOCChat research, we were interested in bringing structured peer learning into the MOOC setting. We first tried out the idea on a crowdsourcing platform (Coetzee et al., 2015), showing that when groups of 3 workers discussed challenging problems together, and especially if they were incentivized to help each other arrive at the correct answer, they achieved better results than working alone. (A sample conversation is shown in Table 1.) We also found that providing a mini-lesson in which workers consider the principles underlying the tested concept and justify their answers leads to further improvements, and combining the mini-lesson with the discussion of the corresponding multiple-choice question in a group of 3 leads to significant improvements on that question. </context>
</contexts>
<marker>Coetzee, Lim, Fox, Hartmann, Hearst, 2015</marker>
<rawString>D Coetzee, Seongtaek Lim, Armando Fox, Bjorn Hartmann, and Marti A Hearst. 2015. Structuring interactions for large-scale synchronous peer learning. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing, pages 1139–1152. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping our own: The HOO 2011 pilot shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>242--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4772" citStr="Dale and Kilgarriff (2011)" startWordPosition="796" endWordPosition="799">gs in the case of second language learning. This suggests a different approach for building a parser than what is the current standard. I am not claiming that this has not been suggested in the past; for instance Schwind (1988) designed a parser to explain errors to learners. However, because of the renewed interest in technology for teaching, this may be a pivotal time to reconsider how we develop parsing technology: perhaps we should think fundamentally about parsers as coaches rather than parsers as critics. This inversion can apply to other aspects of NLP technology as well. For instance, Dale and Kilgarriff (2011) have held a series of workshop to produce algorithms to identify errors introduced into texts by non-native writers in the warmly named “Helping Our Own” shared task (Dale et al., 2012). Using the technology developed for tasks like these, the challenge is to go beyond recognizing and correcting the errors to helping the writer understand why the choices they are making are not correct. Another option is to target practice questions tailored for learners based on errors in a fun manner (as described below). Of course, for decades, the field of Intelligent Tutoring Systems (ITS) (VanLehn, 2011</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2011. Helping our own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 242–249. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ilya Anisimoff</author>
<author>George Narroway</author>
</authors>
<title>HOO 2012: A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>54--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4958" citStr="Dale et al., 2012" startWordPosition="827" endWordPosition="830">past; for instance Schwind (1988) designed a parser to explain errors to learners. However, because of the renewed interest in technology for teaching, this may be a pivotal time to reconsider how we develop parsing technology: perhaps we should think fundamentally about parsers as coaches rather than parsers as critics. This inversion can apply to other aspects of NLP technology as well. For instance, Dale and Kilgarriff (2011) have held a series of workshop to produce algorithms to identify errors introduced into texts by non-native writers in the warmly named “Helping Our Own” shared task (Dale et al., 2012). Using the technology developed for tasks like these, the challenge is to go beyond recognizing and correcting the errors to helping the writer understand why the choices they are making are not correct. Another option is to target practice questions tailored for learners based on errors in a fun manner (as described below). Of course, for decades, the field of Intelligent Tutoring Systems (ITS) (VanLehn, 2011) has developed technology for this purpose, so what is new about what I am suggesting? First, we know as NLP researchers that language analysis requires specific technology beyond stand</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A report on the preposition and determiner error correction shared task. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54–62. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louis Deslauriers</author>
<author>Ellen Schelew</author>
<author>Carl Wieman</author>
</authors>
<title>Improved learning in a large-enrollment physics class.</title>
<date>2011</date>
<journal>Science,</journal>
<volume>332</volume>
<issue>6031</issue>
<contexts>
<context position="18727" citStr="Deslauriers et al., 2011" startWordPosition="3090" endWordPosition="3093">have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer learning including improved critical thinking skills, retention of learned information, interest in subject matter, and class morale (Hake, 1998; Millis and Cottell, 1998; Springer et al., 1999; Smith et al., 2009; Deslauriers et al., 2011). Even studies of intelligent tutoring systems find it hard to do better than just having students discuss homework problems in a structured setting online (Kumar et al., 2007). The reasons for the success of peer learning include: students are at similar levels of understanding that experts can no longer relate to well, people learn material better when they have to explain it to others, and identify the gaps in their current understanding, and the techniques of structured peer learning introduce activities and incentives to help students help one another. S2 I think E is the right answer S1 </context>
</contexts>
<marker>Deslauriers, Schelew, Wieman, 2011</marker>
<rawString>Louis Deslauriers, Ellen Schelew, and Carl Wieman. 2011. Improved learning in a large-enrollment physics class. Science, 332(6031):862–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Edge</author>
<author>Elly Searle</author>
<author>Kevin Chiu</author>
<author>Jing Zhao</author>
<author>James A Landay</author>
</authors>
<title>Micromandarin: mobile language learning in context.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>3169--3178</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8060" citStr="Edge et al., 2011" startWordPosition="1337" endWordPosition="1340">ll, a student must have a good vocabulary and must know syntax. Learning words and syntax requires exposure to language in many 1246 contexts, both spoken and written, for a student’s primary language was well as for learning a second language. Although computerized vocabulary tools have been around for quite some time, the rise of mobile, connected applications, the serious games movement, and the idea of “microtasks” which are done during interstices of time while out and about during the day, opens the door to new ways to expose students to repetitive learning tasks for acquiring language (Edge et al., 2011). Some of the most innovative approaches for teaching language combine mobile apps with multimedia information. For example, the Tip Tap Tones project (Edge et al., 2012) attempts to help learners reduce the the challenge of mastering a foreign phonetic system by microtasking with minute-long episodes of mobile gaming. This work focuses in particular on helping learners acquire the tonal sound system of Mandarin Chinese and combines gesture swipes with audio on a smartphone. The ToneWars app (Head et al., 2014) takes this idea one step farther by linking second language learners with native sp</context>
</contexts>
<marker>Edge, Searle, Chiu, Zhao, Landay, 2011</marker>
<rawString>Darren Edge, Elly Searle, Kevin Chiu, Jing Zhao, and James A Landay. 2011. Micromandarin: mobile language learning in context. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 3169–3178. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Edge</author>
<author>Kai-Yin Cheng</author>
<author>Michael Whitney</author>
<author>Yao Qian</author>
<author>Zhijie Yan</author>
<author>Frank Soong</author>
</authors>
<title>Tip tap tones: mobile microtraining of mandarin sounds.</title>
<date>2012</date>
<booktitle>In Proceedings of the 14th International Conference on Human-Computer Interaction with Mobile Devices and Services,</booktitle>
<pages>427--430</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8230" citStr="Edge et al., 2012" startWordPosition="1365" endWordPosition="1368">r a student’s primary language was well as for learning a second language. Although computerized vocabulary tools have been around for quite some time, the rise of mobile, connected applications, the serious games movement, and the idea of “microtasks” which are done during interstices of time while out and about during the day, opens the door to new ways to expose students to repetitive learning tasks for acquiring language (Edge et al., 2011). Some of the most innovative approaches for teaching language combine mobile apps with multimedia information. For example, the Tip Tap Tones project (Edge et al., 2012) attempts to help learners reduce the the challenge of mastering a foreign phonetic system by microtasking with minute-long episodes of mobile gaming. This work focuses in particular on helping learners acquire the tonal sound system of Mandarin Chinese and combines gesture swipes with audio on a smartphone. The ToneWars app (Head et al., 2014) takes this idea one step farther by linking second language learners with native speakers in real time to play a Tretis-like game against one another to better learn Chinese pronunciation. The second language learner feels especially motivated when they</context>
</contexts>
<marker>Edge, Cheng, Whitney, Qian, Yan, Soong, 2012</marker>
<rawString>Darren Edge, Kai-Yin Cheng, Michael Whitney, Yao Qian, Zhijie Yan, and Frank Soong. 2012. Tip tap tones: mobile microtraining of mandarin sounds. In Proceedings of the 14th International Conference on Human-Computer Interaction with Mobile Devices and Services, pages 427–430. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumit Gulwani</author>
</authors>
<title>Example-based learning in computer-aided stem education.</title>
<date>2014</date>
<journal>Communications of the ACM,</journal>
<volume>57</volume>
<issue>8</issue>
<contexts>
<context position="12409" citStr="Gulwani, 2014" startWordPosition="2049" endWordPosition="2050"> examples. Automation can help with mastery learning by generating personalized practice examples that challenge and interest students. Automatically generated examples also reduce the cost of creating new questions for instructors who are concerned about answer sharing among students from previous runs of a course. Recently, sophisticated techniques developed in 1247 the programming languages field have begun to be applied to automate repetitive and structured tasks in education, including problem generation, solution generation, and feedback generation for computer science and logic topics (Gulwani, 2014). Closer to the subject at hand is the automated generation of mathematical word problems that are organized around themes of interest to kids, such as “School of Wizardry” (Polozov et al., 2015). The method allows the student to specify personal preferences about the world and characters, and then creates mini “plots” for each word problem by enforcing coherence across the sentences using constraints in a logic programming paradigm combined with hand-crafted discourse tropes (constraints on logical graphs) and a natural language generation step. A sample generated word problem is Professor Al</context>
</contexts>
<marker>Gulwani, 2014</marker>
<rawString>Sumit Gulwani. 2014. Example-based learning in computer-aided stem education. Communications of the ACM, 57(8):70–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard R Hake</author>
</authors>
<title>Interactive-engagement versus traditional methods: A six-thousand-student survey of mechanics test data for introductory physics courses.</title>
<date>1998</date>
<journal>American Journal of Physics,</journal>
<volume>66</volume>
<issue>1</issue>
<contexts>
<context position="18631" citStr="Hake, 1998" startWordPosition="3076" endWordPosition="3077">dreds of research papers show that an effective way to help students learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer learning including improved critical thinking skills, retention of learned information, interest in subject matter, and class morale (Hake, 1998; Millis and Cottell, 1998; Springer et al., 1999; Smith et al., 2009; Deslauriers et al., 2011). Even studies of intelligent tutoring systems find it hard to do better than just having students discuss homework problems in a structured setting online (Kumar et al., 2007). The reasons for the success of peer learning include: students are at similar levels of understanding that experts can no longer relate to well, people learn material better when they have to explain it to others, and identify the gaps in their current understanding, and the techniques of structured peer learning introduce a</context>
</contexts>
<marker>Hake, 1998</marker>
<rawString>Richard R Hake. 1998. Interactive-engagement versus traditional methods: A six-thousand-student survey of mechanics test data for introductory physics courses. American Journal of Physics, 66(1):64–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Head</author>
<author>Yi Xu</author>
<author>Jingtao Wang</author>
</authors>
<title>Tonewars: Connecting language learners and native speakers through collaborative mobile games.</title>
<date>2014</date>
<booktitle>In Intelligent Tutoring Systems,</booktitle>
<pages>368--377</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="8576" citStr="Head et al., 2014" startWordPosition="1420" endWordPosition="1423">to new ways to expose students to repetitive learning tasks for acquiring language (Edge et al., 2011). Some of the most innovative approaches for teaching language combine mobile apps with multimedia information. For example, the Tip Tap Tones project (Edge et al., 2012) attempts to help learners reduce the the challenge of mastering a foreign phonetic system by microtasking with minute-long episodes of mobile gaming. This work focuses in particular on helping learners acquire the tonal sound system of Mandarin Chinese and combines gesture swipes with audio on a smartphone. The ToneWars app (Head et al., 2014) takes this idea one step farther by linking second language learners with native speakers in real time to play a Tretis-like game against one another to better learn Chinese pronunciation. The second language learner feels especially motivated when they are able to beat the native speaker, and the native speaker contributes their expert tone recordings to the database, fine-tunes their understanding of their own language, and enjoys the benefits of tutoring others in a fun context. Going beyond phonemes, the DuoLingo second-language learning application (von Ahn, 2013) teaches syntax as well </context>
</contexts>
<marker>Head, Xu, Wang, 2014</marker>
<rawString>Andrew Head, Yi Xu, and Jingtao Wang. 2014. Tonewars: Connecting language learners and native speakers through collaborative mobile games. In Intelligent Tutoring Systems, pages 368–377. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
<author>Armando Fox</author>
<author>D Coetzee</author>
<author>Bjoern Hartmann</author>
</authors>
<title>All it takes is one: Evidence for a strategy for seeding large scale peer learning interactions.</title>
<date>2015</date>
<booktitle>In Proceedings of the Second (2015) ACM Conference on Learning@Scale,</booktitle>
<pages>381--383</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="22797" citStr="Hearst et al., 2015" startWordPosition="3797" endWordPosition="3800">ent discussion text beyond the traditional role of intelligent tutoring systems. One idea is to monitor discussions in real time and try to shape the way the group works together (Tausczik and Pennebaker, 2013). Another idea is to automatically assess if students are discussing content at appropriate levels on Bloom’s taxonomy of educational objectives (Krathwohl, 2002). In our MOOCChat work with triad discussions we observed that more workers will change their answer from an incorrect to a correct one if at least one member of the group starts out correct than if no one is correct initially (Hearst et al., 2015). We also noticed that if all group members start out with the same answer — right or wrong — no one is likely to change their answer in any direction. This behavior pattern suggests an interesting idea for large scale online group discussions that are not feasible in in-person environments: dynamically assign students to groups depending on what their initial answers to questions are, and dynamically regroup students according to the misconceptions and correct conceptions they have. Rather than building an intelligent tutoring system to prompt students with just the right statement at just th</context>
</contexts>
<marker>Hearst, Fox, Coetzee, Hartmann, 2015</marker>
<rawString>Marti A Hearst, Armando Fox, D Coetzee, and Bjoern Hartmann. 2015. All it takes is one: Evidence for a strategy for seeding large scale peer learning interactions. In Proceedings of the Second (2015) ACM Conference on Learning@Scale, pages 381– 383. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Heidorn</author>
</authors>
<title>Intelligent writing assistance.</title>
<date>2000</date>
<booktitle>Handbook of Natural Language Processing,</booktitle>
<pages>181--207</pages>
<contexts>
<context position="3340" citStr="Heidorn, 2000" startWordPosition="569" endWordPosition="570">s or scorers. There is a vast difference between a tool that performs an assessment of writing and one that coaches students to help them as they are attempting to write. Current practice uses the output of scorers to give students a target to aim for: revise your essay to get a higher score. An alternative is to design a system that watches alongside a learner as they write an essay, and coaches their work at all levels of construction – phrase level, clause level, sentence level, discourse level, paragraph level, and essay level. Grammar checking technology has been excellent for years now (Heidorn, 2000), but instead of just showing the right answer as grammar checkers do, a grammar coach should give hints and scaffolding the way a tutor would – not giving the answer explicitly, but showing the path and letting the learner fill in the missing information. When the learner makes incorrect choices, the parser 1245 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1245–1252, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics can teach principles a</context>
</contexts>
<marker>Heidorn, 2000</marker>
<rawString>George Heidorn. 2000. Intelligent writing assistance. Handbook of Natural Language Processing, pages 181–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Nitin Madnani</author>
</authors>
<title>The impact of training data on automated short answer scoring performance.</title>
<date>2015</date>
<booktitle>In Proceedings of the Tenth Workshop on Building Educational Applications Using NLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14903" citStr="Heilman and Madnani, 2015" startWordPosition="2457" endWordPosition="2460">the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be marked up manually by the instructor before the feedback is accurate enough to be reliably used for a given question; a recent study found on the order of 500- 800 items per question had to be marked up at minimum in order to obtain acceptable correlations with human scorers (Heilman and Madnani, 2015). This high initial cost makes the development of hundreds of practice questions for a given conceptual unit a daunting task for instructors. Recent research in Learning at Scale has produced some interesting approaches to improving “feedback at scale.” One approach (Brooks et al., 2014) uses a variation on hierarchical text clustering in tandem with a custom user interface that allows instructors to rapidly view clusters and determine which contain correct answers, incorrect answers, and partially correct answers. This greatly speeds up the markup time and allows instructors to assign explana</context>
</contexts>
<marker>Heilman, Madnani, 2015</marker>
<rawString>Michael Heilman and Nitin Madnani. 2015. The impact of training data on automated short answer scoring performance. In Proceedings of the Tenth Workshop on Building Educational Applications Using NLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine M Hicks</author>
<author>C Ailie Fraser</author>
<author>Purvi Desai</author>
<author>Scott Klemmer</author>
</authors>
<title>Do numeric ratings impact peer reviewers?</title>
<date>2015</date>
<booktitle>In Proceedings of the Second (2015) ACM Conference on Learning@ Scale,</booktitle>
<pages>359--362</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="15796" citStr="Hicks et al., 2015" startWordPosition="2603" endWordPosition="2606"> et al., 2014) uses a variation on hierarchical text clustering in tandem with a custom user interface that allows instructors to rapidly view clusters and determine which contain correct answers, incorrect answers, and partially correct answers. This greatly speeds up the markup time and allows instructors to assign explanations to a large group of answers with a click of a button. An entirely different approach to providing feedback that is becoming heavily used in Massive Open Online Courses is peer feedback, in which students assign grades or give feedback to other students on their work (Hicks et al., 2015). Researchers have studied how to refine the process of peer feedback to train students to produce reviews that come within a grade point of that of instructors, with the aid of carefully designed rubrics (Kulkarni et al., 2013). However, to ensure accurate feedback, several peer assessments per assignment are needed in addition to a training exercise, and students sometimes complain about workload. To reduce the effort, Kulkarni et al. (2014) experimented with a workflow that uses machine grading as a first step. After training a machine learning algorithm for a given assignment, assignments </context>
</contexts>
<marker>Hicks, Fraser, Desai, Klemmer, 2015</marker>
<rawString>Catherine M Hicks, C Ailie Fraser, Purvi Desai, and Scott Klemmer. 2015. Do numeric ratings impact peer reviewers? In Proceedings of the Second (2015) ACM Conference on Learning@ Scale, pages 359–362. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Eric Breck</author>
<author>Marc Light</author>
<author>John D Burger</author>
<author>Lisa Ferro</author>
</authors>
<title>Automated grading of short-answer tests. Intelligent Systems and their Applications,</title>
<date>2000</date>
<pages>15--5</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="14025" citStr="Hirschman et al., 2000" startWordPosition="2317" endWordPosition="2320">lems actually tell a coherent story, but that challenge is still an open one. But the programs can generate an infinite number of problems with solutions. Other work by the same research team generated personalized algebraic equation problems in a game environment and showed that students could achieve mastery learning in 90 minutes or less during an organized educational campaign (Liu et al., 2015). Another way that NLP can help with mastery learning is to aid instructors in the providing of feedback on short answer test questions. There has been significant work in this space (Kukich, 2000; Hirschman et al., 2000). The standard approach builds on the classic successful model of essay scoring which compares the student’s text to model essays using a similarity-based technique such as LSA (Landauer et al., 2000; Mohler and Mihalcea, 2009) or careful authoring of the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be marked up manually by the</context>
</contexts>
<marker>Hirschman, Breck, Light, Burger, Ferro, 2000</marker>
<rawString>Lynette Hirschman, Eric Breck, Marc Light, John D Burger, and Lisa Ferro. 2000. Automated grading of short-answer tests. Intelligent Systems and their Applications, IEEE, 15(5):22–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David W Johnson</author>
<author>Roger T Johnson</author>
<author>Karl Aldrich Smith</author>
</authors>
<title>Active learning: Cooperation in the college classroom.</title>
<date>1991</date>
<publisher>Interaction Book Company</publisher>
<location>Edina, MN.</location>
<contexts>
<context position="18245" citStr="Johnson et al., 1991" startWordPosition="3017" endWordPosition="3020">ws that most people need to learn in an environment that includes interacting with other people. Learning with others can refer to instructors and tutors, and online tutoring systems have had success comparable to that of human tutors in some cases (VanLehn, 2011; Aleven et al., 2004). But another important component of learning with others refers to learning with other students. Literally hundreds of research papers show that an effective way to help students learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer learning including improved critical thinking skills, retention of learned information, interest in subject matter, and class morale (Hake, 1998; Millis and Cottell, 1998; Springer et al., 1999; Smith et al., 2009; Deslauriers et al., 2011). Even studies of intelligent tutoring systems find it hard to do better than just having students discuss homework pr</context>
</contexts>
<marker>Johnson, Johnson, Smith, 1991</marker>
<rawString>David W Johnson, Roger T Johnson, and Karl Aldrich Smith. 1991. Active learning: Cooperation in the college classroom. Interaction Book Company Edina, MN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katy Jordan</author>
</authors>
<title>Initial trends in enrolment and completion of massive open online courses.</title>
<date>2014</date>
<booktitle>The International Review Of Research In Open And Distributed Learning,</booktitle>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="17482" citStr="Jordan, 2014" startWordPosition="2896" endWordPosition="2897">other ways, since only a first pass at NLP techniques was used to date. 1248 Nonetheless, mixing machine and peer grading is a promising technique to explore, as it has been found to be useful in other contexts (Nguyen and Litman, 2014; Kukich, 2000). 5 Are You a FakeBot? Why is the completion rate of MOOCs so low? This question vexes proponents and opponents of MOOCs alike. Counting the window shopping enrollees of a MOOC who do not complete a course is akin to counting everyone who visits a college campus as a failed graduate of that university; many people are just checking the course out (Jordan, 2014). That said, although the anytime, anywhere aspect of online courses works well for many busy professionals who are self-directed, research shows that most people need to learn in an environment that includes interacting with other people. Learning with others can refer to instructors and tutors, and online tutoring systems have had success comparable to that of human tutors in some cases (VanLehn, 2011; Aleven et al., 2004). But another important component of learning with others refers to learning with other students. Literally hundreds of research papers show that an effective way to help s</context>
</contexts>
<marker>Jordan, 2014</marker>
<rawString>Katy Jordan. 2014. Initial trends in enrolment and completion of massive open online courses. The International Review Of Research In Open And Distributed Learning, 15(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salman Khan</author>
</authors>
<title>The One World Schoolhouse: Education Reimagined.</title>
<date>2012</date>
<publisher>Twelve.</publisher>
<contexts>
<context position="11245" citStr="Khan, 2012" startWordPosition="1866" endWordPosition="1867">gies have only become feasible recently because of the combination of multimedia, fast audio and image processing, fast network connectivity, and a connected population. NLP researchers may want to let their imaginations consider the possibilities that arise from this new and potent combination. 4 Closing the Cheese Gap Salman Kahn, the creator of Kahn Academy, talks about the “Swiss cheese” model of learning in which students learn something only partly before they are forced to move on to the next topic, building knowledge on a foundation filled with holes, like the cheese of the same name (Khan, 2012). This is akin to learning to ride a bicycle without perfecting the balancing part. In standard schooling, students are made to move one from one lesson to the next even if they only got 70, 80, 90% correct on the test. By contrast, mastery learning requires a deep understanding, working with knowledge and probing it from every angle, trying out the ideas and applying them to solve real problems. In many cases, mastery learning also requires practicing with dozens, hundreds, or even thousands of different examples, and getting feedback on those examples. Automation can help with mastery learni</context>
</contexts>
<marker>Khan, 2012</marker>
<rawString>Salman Khan. 2012. The One World Schoolhouse: Education Reimagined. Twelve.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasmine Kotturi</author>
<author>Chinmay Kulkarni</author>
<author>Michael S Bernstein</author>
<author>Scott Klemmer</author>
</authors>
<title>Structure and messaging techniques for online peer learning systems that increase stickiness.</title>
<date>2015</date>
<booktitle>In Proceedings of the Second (2015) ACM Conference on Learning@ Scale,</booktitle>
<pages>31--38</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="22076" citStr="Kotturi et al., 2015" startWordPosition="3675" endWordPosition="3678">estion in a group of 3 leads to significant improvements on that question. Crowd workers also expressed positive subjective responses to the peer interactions, suggesting that discussions can improve morale in remote work or learning settings. When we tested the synchronous small-group discussions in a live MOOC we found that, for those students that were successfully placed into a group of 3 for discussion, they were quite positive about the experience (Lim et al., 2014). However, there are significant challenges in getting students to coordinate synchronously in very large low-cost courses (Kotturi et al., 2015). There is much NLP research to be done to enhance the online dialogues that are associated with student discussion text beyond the traditional role of intelligent tutoring systems. One idea is to monitor discussions in real time and try to shape the way the group works together (Tausczik and Pennebaker, 2013). Another idea is to automatically assess if students are discussing content at appropriate levels on Bloom’s taxonomy of educational objectives (Krathwohl, 2002). In our MOOCChat work with triad discussions we observed that more workers will change their answer from an incorrect to a cor</context>
</contexts>
<marker>Kotturi, Kulkarni, Bernstein, Klemmer, 2015</marker>
<rawString>Yasmine Kotturi, Chinmay Kulkarni, Michael S Bernstein, and Scott Klemmer. 2015. Structure and messaging techniques for online peer learning systems that increase stickiness. In Proceedings of the Second (2015) ACM Conference on Learning@ Scale, pages 31–38. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Krathwohl</author>
</authors>
<title>A revision of Bloom’s taxonomy: An overview. Theory into practice,</title>
<date>2002</date>
<pages>41--4</pages>
<contexts>
<context position="22549" citStr="Krathwohl, 2002" startWordPosition="3754" endWordPosition="3755">However, there are significant challenges in getting students to coordinate synchronously in very large low-cost courses (Kotturi et al., 2015). There is much NLP research to be done to enhance the online dialogues that are associated with student discussion text beyond the traditional role of intelligent tutoring systems. One idea is to monitor discussions in real time and try to shape the way the group works together (Tausczik and Pennebaker, 2013). Another idea is to automatically assess if students are discussing content at appropriate levels on Bloom’s taxonomy of educational objectives (Krathwohl, 2002). In our MOOCChat work with triad discussions we observed that more workers will change their answer from an incorrect to a correct one if at least one member of the group starts out correct than if no one is correct initially (Hearst et al., 2015). We also noticed that if all group members start out with the same answer — right or wrong — no one is likely to change their answer in any direction. This behavior pattern suggests an interesting idea for large scale online group discussions that are not feasible in in-person environments: dynamically assign students to groups depending on what the</context>
</contexts>
<marker>Krathwohl, 2002</marker>
<rawString>David R Krathwohl. 2002. A revision of Bloom’s taxonomy: An overview. Theory into practice, 41(4):212–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Beyond automated essay scoring.</title>
<date>2000</date>
<journal>IEEE Intelligent Systems and their Applications, IEEE,</journal>
<volume>15</volume>
<issue>5</issue>
<contexts>
<context position="14000" citStr="Kukich, 2000" startWordPosition="2315" endWordPosition="2316"> the word problems actually tell a coherent story, but that challenge is still an open one. But the programs can generate an infinite number of problems with solutions. Other work by the same research team generated personalized algebraic equation problems in a game environment and showed that students could achieve mastery learning in 90 minutes or less during an organized educational campaign (Liu et al., 2015). Another way that NLP can help with mastery learning is to aid instructors in the providing of feedback on short answer test questions. There has been significant work in this space (Kukich, 2000; Hirschman et al., 2000). The standard approach builds on the classic successful model of essay scoring which compares the student’s text to model essays using a similarity-based technique such as LSA (Landauer et al., 2000; Mohler and Mihalcea, 2009) or careful authoring of the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be </context>
<context position="17119" citStr="Kukich, 2000" startWordPosition="2829" endWordPosition="2830">to grade the assignment, but high-confidence assignments may need only one peer grader. This step was found to successfully reduce the amount of feedback needed to be done with a moderate decrease in grading performance. That said, the algorithm did require the instructors to mark up 500 sample assignments, and there is room for improvement in the algorithm in other ways, since only a first pass at NLP techniques was used to date. 1248 Nonetheless, mixing machine and peer grading is a promising technique to explore, as it has been found to be useful in other contexts (Nguyen and Litman, 2014; Kukich, 2000). 5 Are You a FakeBot? Why is the completion rate of MOOCs so low? This question vexes proponents and opponents of MOOCs alike. Counting the window shopping enrollees of a MOOC who do not complete a course is akin to counting everyone who visits a college campus as a failed graduate of that university; many people are just checking the course out (Jordan, 2014). That said, although the anytime, anywhere aspect of online courses works well for many busy professionals who are self-directed, research shows that most people need to learn in an environment that includes interacting with other peopl</context>
</contexts>
<marker>Kukich, 2000</marker>
<rawString>Karen Kukich. 2000. Beyond automated essay scoring. IEEE Intelligent Systems and their Applications, IEEE, 15(5):22–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chinmay Kulkarni</author>
<author>Koh Pang Wei</author>
<author>Huy Le</author>
<author>Daniel Chia</author>
<author>Kathryn Papadopoulos</author>
<author>Justin Cheng</author>
<author>Daphne Koller</author>
<author>Scott R Klemmer</author>
</authors>
<title>Peer and self assessment in massive online classes. In</title>
<date>2013</date>
<journal>ACM Transactions on Computer Human Interaction (TOCHI),</journal>
<volume>20</volume>
<publisher>ACM.</publisher>
<contexts>
<context position="16024" citStr="Kulkarni et al., 2013" startWordPosition="2644" endWordPosition="2647">ally correct answers. This greatly speeds up the markup time and allows instructors to assign explanations to a large group of answers with a click of a button. An entirely different approach to providing feedback that is becoming heavily used in Massive Open Online Courses is peer feedback, in which students assign grades or give feedback to other students on their work (Hicks et al., 2015). Researchers have studied how to refine the process of peer feedback to train students to produce reviews that come within a grade point of that of instructors, with the aid of carefully designed rubrics (Kulkarni et al., 2013). However, to ensure accurate feedback, several peer assessments per assignment are needed in addition to a training exercise, and students sometimes complain about workload. To reduce the effort, Kulkarni et al. (2014) experimented with a workflow that uses machine grading as a first step. After training a machine learning algorithm for a given assignment, assignments are scored by the algorithm. The less confident the algorithm is in its score, the more students are assigned to grade the assignment, but high-confidence assignments may need only one peer grader. This step was found to success</context>
</contexts>
<marker>Kulkarni, Wei, Le, Chia, Papadopoulos, Cheng, Koller, Klemmer, 2013</marker>
<rawString>Chinmay Kulkarni, Koh Pang Wei, Huy Le, Daniel Chia, Kathryn Papadopoulos, Justin Cheng, Daphne Koller, and Scott R Klemmer. 2013. Peer and self assessment in massive online classes. In ACM Transactions on Computer Human Interaction (TOCHI), volume 20. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chinmay E Kulkarni</author>
<author>Richard Socher</author>
<author>Michael S Bernstein</author>
<author>Scott R Klemmer</author>
</authors>
<title>Scaling shortanswer grading by combining peer assessment with algorithmic scoring.</title>
<date>2014</date>
<booktitle>In Proceedings of the first ACM conference on Learning@Scale,</booktitle>
<pages>99--108</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="16243" citStr="Kulkarni et al. (2014)" startWordPosition="2679" endWordPosition="2682">is becoming heavily used in Massive Open Online Courses is peer feedback, in which students assign grades or give feedback to other students on their work (Hicks et al., 2015). Researchers have studied how to refine the process of peer feedback to train students to produce reviews that come within a grade point of that of instructors, with the aid of carefully designed rubrics (Kulkarni et al., 2013). However, to ensure accurate feedback, several peer assessments per assignment are needed in addition to a training exercise, and students sometimes complain about workload. To reduce the effort, Kulkarni et al. (2014) experimented with a workflow that uses machine grading as a first step. After training a machine learning algorithm for a given assignment, assignments are scored by the algorithm. The less confident the algorithm is in its score, the more students are assigned to grade the assignment, but high-confidence assignments may need only one peer grader. This step was found to successfully reduce the amount of feedback needed to be done with a moderate decrease in grading performance. That said, the algorithm did require the instructors to mark up 500 sample assignments, and there is room for improv</context>
</contexts>
<marker>Kulkarni, Socher, Bernstein, Klemmer, 2014</marker>
<rawString>Chinmay E Kulkarni, Richard Socher, Michael S Bernstein, and Scott R Klemmer. 2014. Scaling shortanswer grading by combining peer assessment with algorithmic scoring. In Proceedings of the first ACM conference on Learning@Scale, pages 99– 108. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Kumar</author>
<author>Carolyn Penstein Ros´e</author>
<author>Yi-Chia Wang</author>
<author>Mahesh Joshi</author>
<author>Allen Robinson</author>
</authors>
<title>Tutorial dialogue as adaptive collaborative learning support.</title>
<date>2007</date>
<booktitle>Frontiers in Artificial Intelligence and Applications,</booktitle>
<pages>158--383</pages>
<marker>Kumar, Ros´e, Wang, Joshi, Robinson, 2007</marker>
<rawString>Rohit Kumar, Carolyn Penstein Ros´e, Yi-Chia Wang, Mahesh Joshi, and Allen Robinson. 2007. Tutorial dialogue as adaptive collaborative learning support. Frontiers in Artificial Intelligence and Applications, 158:383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Darrell Laham</author>
<author>Peter W Foltz</author>
</authors>
<title>The intelligent essay assessor.</title>
<date>2000</date>
<journal>IEEE Intelligent Systems and their Applications, IEEE,</journal>
<volume>15</volume>
<issue>5</issue>
<contexts>
<context position="14224" citStr="Landauer et al., 2000" startWordPosition="2349" endWordPosition="2352"> personalized algebraic equation problems in a game environment and showed that students could achieve mastery learning in 90 minutes or less during an organized educational campaign (Liu et al., 2015). Another way that NLP can help with mastery learning is to aid instructors in the providing of feedback on short answer test questions. There has been significant work in this space (Kukich, 2000; Hirschman et al., 2000). The standard approach builds on the classic successful model of essay scoring which compares the student’s text to model essays using a similarity-based technique such as LSA (Landauer et al., 2000; Mohler and Mihalcea, 2009) or careful authoring of the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be marked up manually by the instructor before the feedback is accurate enough to be reliably used for a given question; a recent study found on the order of 500- 800 items per question had to be marked up at minimum in order t</context>
</contexts>
<marker>Landauer, Laham, Foltz, 2000</marker>
<rawString>Thomas K Landauer, Darrell Laham, and Peter W Foltz. 2000. The intelligent essay assessor. IEEE Intelligent Systems and their Applications, IEEE, 15(5):22–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>C-rater: Automated scoring of short-answer questions.</title>
<date>2003</date>
<journal>Computers and the Humanities,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="14316" citStr="Leacock and Chodorow, 2003" startWordPosition="2363" endWordPosition="2366">ts could achieve mastery learning in 90 minutes or less during an organized educational campaign (Liu et al., 2015). Another way that NLP can help with mastery learning is to aid instructors in the providing of feedback on short answer test questions. There has been significant work in this space (Kukich, 2000; Hirschman et al., 2000). The standard approach builds on the classic successful model of essay scoring which compares the student’s text to model essays using a similarity-based technique such as LSA (Landauer et al., 2000; Mohler and Mihalcea, 2009) or careful authoring of the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be marked up manually by the instructor before the feedback is accurate enough to be reliably used for a given question; a recent study found on the order of 500- 800 items per question had to be marked up at minimum in order to obtain acceptable correlations with human scorers (Heilman and Madnani, 2015). This high i</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>Claudia Leacock and Martin Chodorow. 2003. C-rater: Automated scoring of short-answer questions. Computers and the Humanities, 37(4):389–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seongtaek Lim</author>
<author>Derrick Coetzee</author>
<author>Bjoern Hartmann</author>
<author>Armando Fox</author>
<author>Marti A Hearst</author>
</authors>
<title>Initial experiences with small group discussions in moocs.</title>
<date>2014</date>
<booktitle>In Proceedings of the first ACM conference on Learning@Scale,</booktitle>
<pages>151--152</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="21931" citStr="Lim et al., 2014" startWordPosition="3655" endWordPosition="3658">ustify their answers leads to further improvements, and combining the mini-lesson with the discussion of the corresponding multiple-choice question in a group of 3 leads to significant improvements on that question. Crowd workers also expressed positive subjective responses to the peer interactions, suggesting that discussions can improve morale in remote work or learning settings. When we tested the synchronous small-group discussions in a live MOOC we found that, for those students that were successfully placed into a group of 3 for discussion, they were quite positive about the experience (Lim et al., 2014). However, there are significant challenges in getting students to coordinate synchronously in very large low-cost courses (Kotturi et al., 2015). There is much NLP research to be done to enhance the online dialogues that are associated with student discussion text beyond the traditional role of intelligent tutoring systems. One idea is to monitor discussions in real time and try to shape the way the group works together (Tausczik and Pennebaker, 2013). Another idea is to automatically assess if students are discussing content at appropriate levels on Bloom’s taxonomy of educational objectives</context>
</contexts>
<marker>Lim, Coetzee, Hartmann, Fox, Hearst, 2014</marker>
<rawString>Seongtaek Lim, Derrick Coetzee, Bjoern Hartmann, Armando Fox, and Marti A Hearst. 2014. Initial experiences with small group discussions in moocs. In Proceedings of the first ACM conference on Learning@Scale, pages 151–152. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-En Liu</author>
<author>Christy Ballweber</author>
</authors>
<title>Eleanor O’rourke, Eric Butler, Phonraphee Thummaphan, and Zoran Popovi´c.</title>
<date>2015</date>
<journal>ACM Transactions on Computer-Human Interaction (TOCHI),</journal>
<volume>22</volume>
<issue>2</issue>
<marker>Liu, Ballweber, 2015</marker>
<rawString>Yun-En Liu, Christy Ballweber, Eleanor O’rourke, Eric Butler, Phonraphee Thummaphan, and Zoran Popovi´c. 2015. Large-scale educational campaigns. ACM Transactions on Computer-Human Interaction (TOCHI), 22(2):8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lord</author>
</authors>
<title>Cooperative learning that really works in biology teaching: using constructivistbased activities to challenge student teams.</title>
<date>1998</date>
<journal>The American Biology Teacher,</journal>
<volume>60</volume>
<issue>8</issue>
<contexts>
<context position="18258" citStr="Lord, 1998" startWordPosition="3021" endWordPosition="3022">ed to learn in an environment that includes interacting with other people. Learning with others can refer to instructors and tutors, and online tutoring systems have had success comparable to that of human tutors in some cases (VanLehn, 2011; Aleven et al., 2004). But another important component of learning with others refers to learning with other students. Literally hundreds of research papers show that an effective way to help students learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer learning including improved critical thinking skills, retention of learned information, interest in subject matter, and class morale (Hake, 1998; Millis and Cottell, 1998; Springer et al., 1999; Smith et al., 2009; Deslauriers et al., 2011). Even studies of intelligent tutoring systems find it hard to do better than just having students discuss homework problems in a s</context>
</contexts>
<marker>Lord, 1998</marker>
<rawString>Thomas Lord. 1998. Cooperative learning that really works in biology teaching: using constructivistbased activities to challenge student teams. The American Biology Teacher, 60(8):580–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detmar Meurers</author>
</authors>
<title>Natural language processing and language learning.</title>
<date>2012</date>
<booktitle>In The Encyclopedia of Applied Linguistics.</booktitle>
<publisher>Wiley Online Library.</publisher>
<contexts>
<context position="5804" citStr="Meurers, 2012" startWordPosition="963" endWordPosition="964"> practice questions tailored for learners based on errors in a fun manner (as described below). Of course, for decades, the field of Intelligent Tutoring Systems (ITS) (VanLehn, 2011) has developed technology for this purpose, so what is new about what I am suggesting? First, we know as NLP researchers that language analysis requires specific technology beyond standard algorithms, and so advances in Intelligent Tutoring Systems on language problems most likely requires collaboration with experts in NLP. And, apparently such collaborations have not been as robust as they might be (Borin, 2002; Meurers, 2012). So there is an opportunity for new advances at the intersection of these two fields. And second, the newly expanded interest in online learning and technology makes possible the access of information about student writing behavior on a large scale that was not possible in the past. Imagine thousands of students in cascaded waves, tasked with writing essays on the same topic, and receiving real-time suggestions from different algorithms. The first wave of student responses to the feedback would be used to Figure 1: Wordcraft user interface showing a farm scene with four characters, a fully fo</context>
</contexts>
<marker>Meurers, 2012</marker>
<rawString>Detmar Meurers. 2012. Natural language processing and language learning. In The Encyclopedia of Applied Linguistics. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Millis</author>
<author>Philip G Cottell</author>
</authors>
<title>Cooperative learning for higher education faculty.</title>
<date>1998</date>
<publisher>Oryx Press</publisher>
<location>(Phoenix, Ariz.).</location>
<contexts>
<context position="18657" citStr="Millis and Cottell, 1998" startWordPosition="3078" endWordPosition="3081">earch papers show that an effective way to help students learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer learning including improved critical thinking skills, retention of learned information, interest in subject matter, and class morale (Hake, 1998; Millis and Cottell, 1998; Springer et al., 1999; Smith et al., 2009; Deslauriers et al., 2011). Even studies of intelligent tutoring systems find it hard to do better than just having students discuss homework problems in a structured setting online (Kumar et al., 2007). The reasons for the success of peer learning include: students are at similar levels of understanding that experts can no longer relate to well, people learn material better when they have to explain it to others, and identify the gaps in their current understanding, and the techniques of structured peer learning introduce activities and incentives t</context>
</contexts>
<marker>Millis, Cottell, 1998</marker>
<rawString>Barbara J Millis and Philip G Cottell. 1998. Cooperative learning for higher education faculty. Oryx Press (Phoenix, Ariz.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>Rada Mihalcea</author>
</authors>
<title>Text-totext semantic similarity for automatic short answer grading.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>567--575</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14252" citStr="Mohler and Mihalcea, 2009" startWordPosition="2353" endWordPosition="2356"> equation problems in a game environment and showed that students could achieve mastery learning in 90 minutes or less during an organized educational campaign (Liu et al., 2015). Another way that NLP can help with mastery learning is to aid instructors in the providing of feedback on short answer test questions. There has been significant work in this space (Kukich, 2000; Hirschman et al., 2000). The standard approach builds on the classic successful model of essay scoring which compares the student’s text to model essays using a similarity-based technique such as LSA (Landauer et al., 2000; Mohler and Mihalcea, 2009) or careful authoring of the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be marked up manually by the instructor before the feedback is accurate enough to be reliably used for a given question; a recent study found on the order of 500- 800 items per question had to be marked up at minimum in order to obtain acceptable correlat</context>
</contexts>
<marker>Mohler, Mihalcea, 2009</marker>
<rawString>Michael Mohler and Rada Mihalcea. 2009. Text-totext semantic similarity for automatic short answer grading. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 567–575. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Yuanbin Wu</author>
<author>Christian Hadiwinoto</author>
<author>Joel Tetreault</author>
</authors>
<title>The CoNLL-2013 shared task on grammatical error correction.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task),</booktitle>
<pages>1--12</pages>
<contexts>
<context position="2058" citStr="Ng et al., 2014" startWordPosition="335" endWordPosition="338">t need for NLP research to step in and help. I hope in this paper to excite colleagues about the possibilities and suggest a few new ways of looking at them. I do not attempt to cover the field of language and learning comprehensively, nor do I claim there is no work in the field. In fact there is quite a bit, such as a recent special issue on language learning resources (Sharoff et al., 2014), the long running ACL workshops on Building Educational Applications using NLP (Tetreault et al., 2015), and a recent shared task competition on grammatical error detection for second language learners (Ng et al., 2014). But I hope I am casting a few interesting thoughts in this direction for those colleagues who are not focused on this particular topic. 2 How Awkward Perhaps the least useful feedback that an instructor writes next to a block of prose on a learner’s essay is ‘awkward’. We know what this means: something about this text does not read fluently. But this is not helpful feedback; if the student knew how to make the wording flow, he or she would have written it fluently in the first place! Useful feedback is actionable: it provides steps to take to make improvements. A challenge for the field of </context>
</contexts>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2014. The CoNLL-2013 shared task on grammatical error correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task), pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huy V Nguyen</author>
<author>Diane J Litman</author>
</authors>
<title>Improving peer feedback prediction: The sentence level is right. ACL</title>
<date>2014</date>
<pages>99</pages>
<contexts>
<context position="17104" citStr="Nguyen and Litman, 2014" startWordPosition="2825" endWordPosition="2828">re students are assigned to grade the assignment, but high-confidence assignments may need only one peer grader. This step was found to successfully reduce the amount of feedback needed to be done with a moderate decrease in grading performance. That said, the algorithm did require the instructors to mark up 500 sample assignments, and there is room for improvement in the algorithm in other ways, since only a first pass at NLP techniques was used to date. 1248 Nonetheless, mixing machine and peer grading is a promising technique to explore, as it has been found to be useful in other contexts (Nguyen and Litman, 2014; Kukich, 2000). 5 Are You a FakeBot? Why is the completion rate of MOOCs so low? This question vexes proponents and opponents of MOOCs alike. Counting the window shopping enrollees of a MOOC who do not complete a course is akin to counting everyone who visits a college campus as a failed graduate of that university; many people are just checking the course out (Jordan, 2014). That said, although the anytime, anywhere aspect of online courses works well for many busy professionals who are self-directed, research shows that most people need to learn in an environment that includes interacting w</context>
</contexts>
<marker>Nguyen, Litman, 2014</marker>
<rawString>Huy V Nguyen and Diane J Litman. 2014. Improving peer feedback prediction: The sentence level is right. ACL 2014, page 99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oleksandr Polozov</author>
<author>Eleanor ORourke</author>
<author>Adam M Smith</author>
<author>Luke Zettlemoyer</author>
<author>Sumit Gulwani</author>
<author>Zoran Popovic</author>
</authors>
<title>Personalized mathematical word problem generation.</title>
<date>2015</date>
<booktitle>In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<note>To appear.</note>
<contexts>
<context position="12604" citStr="Polozov et al., 2015" startWordPosition="2079" endWordPosition="2082">t of creating new questions for instructors who are concerned about answer sharing among students from previous runs of a course. Recently, sophisticated techniques developed in 1247 the programming languages field have begun to be applied to automate repetitive and structured tasks in education, including problem generation, solution generation, and feedback generation for computer science and logic topics (Gulwani, 2014). Closer to the subject at hand is the automated generation of mathematical word problems that are organized around themes of interest to kids, such as “School of Wizardry” (Polozov et al., 2015). The method allows the student to specify personal preferences about the world and characters, and then creates mini “plots” for each word problem by enforcing coherence across the sentences using constraints in a logic programming paradigm combined with hand-crafted discourse tropes (constraints on logical graphs) and a natural language generation step. A sample generated word problem is Professor Alice assigns Elliot to make a luck potion. He had to spend 9 hours first reading the recipe in the textbook. He spends several hours brewing 11 portions of it. The potion has to be brewed for 3 ho</context>
</contexts>
<marker>Polozov, ORourke, Smith, Zettlemoyer, Gulwani, Popovic, 2015</marker>
<rawString>Oleksandr Polozov, Eleanor ORourke, Adam M Smith, Luke Zettlemoyer, Sumit Gulwani, and Zoran Popovic. 2015. Personalized mathematical word problem generation. In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI 2015). To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Camilla Schwind</author>
</authors>
<title>Sensitive parsing: error analysis and explanation in an intelligent language tutoring system.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th conference on Computational Linguistics-Volume 2,</booktitle>
<pages>608--613</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4373" citStr="Schwind (1988)" startWordPosition="733" endWordPosition="734">rnational Joint Conference on Natural Language Processing, pages 1245–1252, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics can teach principles and lessons for the conceptual stage that the learner is currently at. Different grammars could be developed for learners at different competency levels, as well as for different first-second language pairings in the case of second language learning. This suggests a different approach for building a parser than what is the current standard. I am not claiming that this has not been suggested in the past; for instance Schwind (1988) designed a parser to explain errors to learners. However, because of the renewed interest in technology for teaching, this may be a pivotal time to reconsider how we develop parsing technology: perhaps we should think fundamentally about parsers as coaches rather than parsers as critics. This inversion can apply to other aspects of NLP technology as well. For instance, Dale and Kilgarriff (2011) have held a series of workshop to produce algorithms to identify errors introduced into texts by non-native writers in the warmly named “Helping Our Own” shared task (Dale et al., 2012). Using the tec</context>
</contexts>
<marker>Schwind, 1988</marker>
<rawString>Camilla Schwind. 1988. Sensitive parsing: error analysis and explanation in an intelligent language tutoring system. In Proceedings of the 12th conference on Computational Linguistics-Volume 2, pages 608– 613. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
<author>Stefania Spina</author>
<author>Sofie Johansson Kokkinakis</author>
</authors>
<title>Introduction to the special issue on resources and tools for language learners.</title>
<date>2014</date>
<journal>Language Resources and Evaluation,</journal>
<volume>48</volume>
<issue>1</issue>
<contexts>
<context position="1838" citStr="Sharoff et al., 2014" startWordPosition="301" endWordPosition="304"> active learning over passive methods. The modern view of teaching means students work actively in class, talk with peers, and are coached more than graded by their instructors. In this new world of education, there is a great need for NLP research to step in and help. I hope in this paper to excite colleagues about the possibilities and suggest a few new ways of looking at them. I do not attempt to cover the field of language and learning comprehensively, nor do I claim there is no work in the field. In fact there is quite a bit, such as a recent special issue on language learning resources (Sharoff et al., 2014), the long running ACL workshops on Building Educational Applications using NLP (Tetreault et al., 2015), and a recent shared task competition on grammatical error detection for second language learners (Ng et al., 2014). But I hope I am casting a few interesting thoughts in this direction for those colleagues who are not focused on this particular topic. 2 How Awkward Perhaps the least useful feedback that an instructor writes next to a block of prose on a learner’s essay is ‘awkward’. We know what this means: something about this text does not read fluently. But this is not helpful feedback;</context>
</contexts>
<marker>Sharoff, Spina, Kokkinakis, 2014</marker>
<rawString>Serge Sharoff, Stefania Spina, and Sofie Johansson Kokkinakis. 2014. Introduction to the special issue on resources and tools for language learners. Language Resources and Evaluation, 48(1):1–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle K Smith</author>
<author>William B Wood</author>
<author>Wendy K Adams</author>
<author>Carl Wieman</author>
<author>Jennifer K Knight</author>
<author>Nancy Guild</author>
<author>Tin Tin Su</author>
</authors>
<title>Why peer discussion improves student performance on in-class concept questions.</title>
<date>2009</date>
<journal>Science,</journal>
<volume>323</volume>
<issue>5910</issue>
<contexts>
<context position="18700" citStr="Smith et al., 2009" startWordPosition="3086" endWordPosition="3089">tudents learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer learning including improved critical thinking skills, retention of learned information, interest in subject matter, and class morale (Hake, 1998; Millis and Cottell, 1998; Springer et al., 1999; Smith et al., 2009; Deslauriers et al., 2011). Even studies of intelligent tutoring systems find it hard to do better than just having students discuss homework problems in a structured setting online (Kumar et al., 2007). The reasons for the success of peer learning include: students are at similar levels of understanding that experts can no longer relate to well, people learn material better when they have to explain it to others, and identify the gaps in their current understanding, and the techniques of structured peer learning introduce activities and incentives to help students help one another. S2 I thin</context>
</contexts>
<marker>Smith, Wood, Adams, Wieman, Knight, Guild, Su, 2009</marker>
<rawString>Michelle K Smith, William B Wood, Wendy K Adams, Carl Wieman, Jennifer K Knight, Nancy Guild, and Tin Tin Su. 2009. Why peer discussion improves student performance on in-class concept questions. Science, 323(5910):122–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Springer</author>
<author>Mary Elizabeth Stanne</author>
<author>Samuel S Donovan</author>
</authors>
<title>Effects of small-group learning on undergraduates in science, mathematics, engineering, and technology: A meta-analysis. Review of educational research,</title>
<date>1999</date>
<pages>69--1</pages>
<contexts>
<context position="18680" citStr="Springer et al., 1999" startWordPosition="3082" endWordPosition="3085">effective way to help students learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer learning including improved critical thinking skills, retention of learned information, interest in subject matter, and class morale (Hake, 1998; Millis and Cottell, 1998; Springer et al., 1999; Smith et al., 2009; Deslauriers et al., 2011). Even studies of intelligent tutoring systems find it hard to do better than just having students discuss homework problems in a structured setting online (Kumar et al., 2007). The reasons for the success of peer learning include: students are at similar levels of understanding that experts can no longer relate to well, people learn material better when they have to explain it to others, and identify the gaps in their current understanding, and the techniques of structured peer learning introduce activities and incentives to help students help on</context>
</contexts>
<marker>Springer, Stanne, Donovan, 1999</marker>
<rawString>Leonard Springer, Mary Elizabeth Stanne, and Samuel S Donovan. 1999. Effects of small-group learning on undergraduates in science, mathematics, engineering, and technology: A meta-analysis. Review of educational research, 69(1):21–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>Improving teamwork using real-time language feedback.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>459--468</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="22387" citStr="Tausczik and Pennebaker, 2013" startWordPosition="3728" endWordPosition="3732">a live MOOC we found that, for those students that were successfully placed into a group of 3 for discussion, they were quite positive about the experience (Lim et al., 2014). However, there are significant challenges in getting students to coordinate synchronously in very large low-cost courses (Kotturi et al., 2015). There is much NLP research to be done to enhance the online dialogues that are associated with student discussion text beyond the traditional role of intelligent tutoring systems. One idea is to monitor discussions in real time and try to shape the way the group works together (Tausczik and Pennebaker, 2013). Another idea is to automatically assess if students are discussing content at appropriate levels on Bloom’s taxonomy of educational objectives (Krathwohl, 2002). In our MOOCChat work with triad discussions we observed that more workers will change their answer from an incorrect to a correct one if at least one member of the group starts out correct than if no one is correct initially (Hearst et al., 2015). We also noticed that if all group members start out with the same answer — right or wrong — no one is likely to change their answer in any direction. This behavior pattern suggests an inte</context>
</contexts>
<marker>Tausczik, Pennebaker, 2013</marker>
<rawString>Yla R Tausczik and James W Pennebaker. 2013. Improving teamwork using real-time language feedback. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 459– 468. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Jill Burstein</author>
<author>Claudia Leacock</author>
</authors>
<date>2015</date>
<booktitle>Proceedings of the Tenth Workshop on Building Educational Applications Using NLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1942" citStr="Tetreault et al., 2015" startWordPosition="317" endWordPosition="320">ss, talk with peers, and are coached more than graded by their instructors. In this new world of education, there is a great need for NLP research to step in and help. I hope in this paper to excite colleagues about the possibilities and suggest a few new ways of looking at them. I do not attempt to cover the field of language and learning comprehensively, nor do I claim there is no work in the field. In fact there is quite a bit, such as a recent special issue on language learning resources (Sharoff et al., 2014), the long running ACL workshops on Building Educational Applications using NLP (Tetreault et al., 2015), and a recent shared task competition on grammatical error detection for second language learners (Ng et al., 2014). But I hope I am casting a few interesting thoughts in this direction for those colleagues who are not focused on this particular topic. 2 How Awkward Perhaps the least useful feedback that an instructor writes next to a block of prose on a learner’s essay is ‘awkward’. We know what this means: something about this text does not read fluently. But this is not helpful feedback; if the student knew how to make the wording flow, he or she would have written it fluently in the first</context>
</contexts>
<marker>Tetreault, Burstein, Leacock, 2015</marker>
<rawString>Joel Tetreault, Jill Burstein, and Claudia Leacock. 2015. Proceedings of the Tenth Workshop on Building Educational Applications Using NLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt VanLehn</author>
</authors>
<title>The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems.</title>
<date>2011</date>
<journal>Educational Psychologist,</journal>
<volume>46</volume>
<issue>4</issue>
<contexts>
<context position="5373" citStr="VanLehn, 2011" startWordPosition="896" endWordPosition="897">arriff (2011) have held a series of workshop to produce algorithms to identify errors introduced into texts by non-native writers in the warmly named “Helping Our Own” shared task (Dale et al., 2012). Using the technology developed for tasks like these, the challenge is to go beyond recognizing and correcting the errors to helping the writer understand why the choices they are making are not correct. Another option is to target practice questions tailored for learners based on errors in a fun manner (as described below). Of course, for decades, the field of Intelligent Tutoring Systems (ITS) (VanLehn, 2011) has developed technology for this purpose, so what is new about what I am suggesting? First, we know as NLP researchers that language analysis requires specific technology beyond standard algorithms, and so advances in Intelligent Tutoring Systems on language problems most likely requires collaboration with experts in NLP. And, apparently such collaborations have not been as robust as they might be (Borin, 2002; Meurers, 2012). So there is an opportunity for new advances at the intersection of these two fields. And second, the newly expanded interest in online learning and technology makes po</context>
<context position="17888" citStr="VanLehn, 2011" startWordPosition="2961" endWordPosition="2962"> enrollees of a MOOC who do not complete a course is akin to counting everyone who visits a college campus as a failed graduate of that university; many people are just checking the course out (Jordan, 2014). That said, although the anytime, anywhere aspect of online courses works well for many busy professionals who are self-directed, research shows that most people need to learn in an environment that includes interacting with other people. Learning with others can refer to instructors and tutors, and online tutoring systems have had success comparable to that of human tutors in some cases (VanLehn, 2011; Aleven et al., 2004). But another important component of learning with others refers to learning with other students. Literally hundreds of research papers show that an effective way to help students learn is to have them talk together in small groups, called structured peer learning, collaborative learning, or cooperative learning (Johnson et al., 1991; Lord, 1998). In the classroom, this consists of activities in which students confer in small groups to discuss conceptual questions and to engage in problemsolving. Studies and meta-analyses show the significant pedagogical benefit of peer l</context>
</contexts>
<marker>VanLehn, 2011</marker>
<rawString>Kurt VanLehn. 2011. The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems. Educational Psychologist, 46(4):197–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
</authors>
<title>Duolingo: learn a language for free while helping to translate the web.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 International Conference on Intelligent User Interfaces,</booktitle>
<pages>1--2</pages>
<publisher>ACM.</publisher>
<marker>von Ahn, 2013</marker>
<rawString>Luis von Ahn. 2013. Duolingo: learn a language for free while helping to translate the web. In Proceedings of the 2013 International Conference on Intelligent User Interfaces, pages 1–2. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijuan Wang</author>
<author>Yao Qian</author>
<author>Matthew R Scott</author>
<author>Gang Chen</author>
<author>Frank K Soong</author>
</authors>
<title>Computer-assisted audiovisual language learning (with online video).</title>
<date>2012</date>
<journal>Computer,</journal>
<volume>45</volume>
<issue>6</issue>
<contexts>
<context position="10290" citStr="Wang et al., 2012" startWordPosition="1704" endWordPosition="1707">(Anand et al., 2015) (see Figure 1). Children manipulate word cards to build sentences which, when grammatically well formed, come to life in a storybook-like animated world to illustrate their meaning. Preliminary studies of the use of Wordcraft found that children between the ages of 4 and 8 were able to observe how different sentence constructions resulted in different meanings and encouraged children to engage in metalinguistic discourse, especially when playing the game with another child. A karaoke-style video simulation is used by the Engkoo system to teach English to Chinese speakers (Wang et al., 2012). The interface not only generates audio for the English words, but also shows the lip and facial shapes necessary for forming English words using a 3D simulated model lipsyncing the words in a highly realistic manner. To generate a large number of sample sentences, the text was drawn from bilingual sentence pairs from the web. These technologies have only become feasible recently because of the combination of multimedia, fast audio and image processing, fast network connectivity, and a connected population. NLP researchers may want to let their imaginations consider the possibilities that ari</context>
</contexts>
<marker>Wang, Qian, Scott, Chen, Soong, 2012</marker>
<rawString>Lijuan Wang, Yao Qian, Matthew R Scott, Gang Chen, and Frank K Soong. 2012. Computer-assisted audiovisual language learning (with online video). Computer, 45(6):38–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Willis</author>
</authors>
<title>Using nlp to support scalable assessment of short free text responses.</title>
<date>2015</date>
<booktitle>In Proceedings of the Tenth Workshop on Building Educational Applications Using NLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14512" citStr="Willis, 2015" startWordPosition="2392" endWordPosition="2393"> feedback on short answer test questions. There has been significant work in this space (Kukich, 2000; Hirschman et al., 2000). The standard approach builds on the classic successful model of essay scoring which compares the student’s text to model essays using a similarity-based technique such as LSA (Landauer et al., 2000; Mohler and Mihalcea, 2009) or careful authoring of the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be marked up manually by the instructor before the feedback is accurate enough to be reliably used for a given question; a recent study found on the order of 500- 800 items per question had to be marked up at minimum in order to obtain acceptable correlations with human scorers (Heilman and Madnani, 2015). This high initial cost makes the development of hundreds of practice questions for a given conceptual unit a daunting task for instructors. Recent research in Learning at Scale has produced some interesting </context>
</contexts>
<marker>Willis, 2015</marker>
<rawString>Alistair Willis. 2015. Using nlp to support scalable assessment of short free text responses. In Proceedings of the Tenth Workshop on Building Educational Applications Using NLP. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>