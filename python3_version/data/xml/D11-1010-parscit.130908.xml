<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006346">
<title confidence="0.984147">
Correcting Semantic Collocation Errors with L1-induced Paraphrases
</title>
<author confidence="0.985502">
Daniel Dahlmeier&apos; and Hwee Tou Ng&apos; ,2
</author>
<affiliation confidence="0.979514">
&apos;NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
</affiliation>
<email confidence="0.992694">
{danielhe,nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.998573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998378384615385">
We present a novel approach for automatic
collocation error correction in learner English
which is based on paraphrases extracted from
parallel corpora. Our key assumption is that
collocation errors are often caused by se-
mantic similarity in the first language (L1-
language) of the writer. An analysis of a
large corpus of annotated learner English con-
firms this assumption. We evaluate our ap-
proach on real-world learner data and show
that L1-induced paraphrases outperform tradi-
tional approaches based on edit distance, ho-
mophones, and WordNet synonyms.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994834375">
Grammatical error correction (GEC) is emerging as
a commercially attractive application of natural lan-
guage processing (NLP) for the booming market of
English as foreign or second language (EFL/ESL1).
The de facto standard approach to GEC is to build
a statistical model that can choose the most likely
correction from a confusion set of possible correc-
tion choices. The way the confusion set is defined
depends on the type of error. Work in context-
sensitive spelling error correction (Golding and
Roth, 1999) has traditionally focused on confusion
sets with similar spelling (e.g., {dessert, desert}) or
similar pronunciation (e.g., {there, their}). In other
words, the words in a confusion set are deemed con-
fusable because of orthographic or phonetic simi-
larity. Other work in GEC has defined the confu-
</bodyText>
<footnote confidence="0.9867685">
1For simplicity, we will collectively refer to both terms as
English as a foreign language (EFL)
</footnote>
<bodyText confidence="0.9932015">
sion sets based on syntactic similarity, for exam-
ple all English articles or the most frequent English
prepositions form a confusion set (see for example
(Tetreault et al., 2010; Rozovskaya and Roth, 2010;
Gamon, 2010; Dahlmeier and Ng, 2011) among oth-
ers).
In contrast, we investigate in this paper a class of
grammatical errors where the source of confusion is
the similar semantics of the words, rather than or-
thography, phonetics, or syntax. In particular, we
focus on collocation errors in EFL writing. The
term collocation (Firth, 1957) describes a sequence
of words that is conventionally used together in a
particular way by native speakers and appears more
often together than one would expect by chance. The
correct use of collocations is a major difficulty for
EFL students (Farghal and Obiedat, 1995).
In this work, we present a novel approach for au-
tomatic correction of collocation errors in EFL writ-
ing. Our key observation is that words are poten-
tially confusable for an EFL student if they have
similar translations in the writer’s first language (L1-
language), or in other words if they have the same
semantics in the L1-language of the writer. The
Chinese word 看 (kàn), for example, has over a
dozen translations in English, including the words
see, look, read, and watch. A Chinese speaker who
still “thinks” in Chinese has to choose from all these
possible translations when he wants to express a sen-
tence like I like to watch movies and might instead
produce a sentence like *I like to look movies. Al-
though the meanings of watch and look are simi-
lar, the former is clearly the more fluent choice in
this context. While these types of L1-transfer er-
</bodyText>
<page confidence="0.982809">
107
</page>
<note confidence="0.957951">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 107–117,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999905657894737">
rors have been known in the EFL teaching litera-
ture (Swan and Smith, 2001; Meng, 2008), research
in GEC has mostly ignored this fact.
We first analyze collocation errors in the NUS
Corpus of Learner English (NUCLE), a fully an-
notated one-million-word corpus of learner English
which we will make available to the community for
research purposes (see Section 3 for details about
the corpus). Our analysis confirms that many col-
location errors can be traced to similar translations
in the writer’s L1-language. Based on this result,
we propose a novel approach for automatic collo-
cation error correction. The key component in our
approach generates L1-induced paraphrases which
we automatically extract from an L1-English par-
allel corpus. Our proposed approach outperforms
traditional approaches based on edit distance, ho-
mophones, and WordNet synonyms on a test set of
real-world learner data in an automatic and a human
evaluation. Finally, we present a detailed analysis of
unsolved instances in our data set to highlight direc-
tions for future work.
Our work adds to a growing body of research that
leverages parallel corpora for semantic NLP tasks,
for example in word sense disambiguation (Ng et
al., 2003; Chan and Ng, 2005; Ng and Chan, 2007;
Zhong and Ng, 2009), paraphrasing (Bannard and
Callison-Burch, 2005; Liu et al., 2010a), and ma-
chine translation evaluation (Snover et al., 2009; Liu
et al., 2010b).
The remainder of this paper is organized as fol-
lows. The next section reviews related work. Sec-
tion 3 presents our analysis of collocation errors.
Section 4 describes our approach for automatic col-
location error correction. The experimental setup
and the results are described in Sections 5 and 6, re-
spectively. Section 7 provides further analysis. Sec-
tion 8 concludes the paper.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999930642857143">
In this section, we give an overview of related work
on collocation error correction. We also highlight
differences between collocation error correction and
related NLP tasks like context-sensitive spelling er-
ror correction, synonym extraction, lexical substitu-
tion, and paraphrasing.
Most work in collocation error correction has re-
lied on dictionaries or manually created databases
to generate collocation candidates (Shei and Pain,
2000; Wible et al., 2003; Futagi et al., 2008). Other
work has focused on finding candidates that collo-
cate with similar words, e.g., verbs that appear with
the same noun objects form a confusion set (Liu et
al., 2009; Wu et al., 2010). The work most similar
to ours is probably the one presented by Chang et
al. (2008), as they also use translation information to
generate collocation candidates. However, they do
not use automatically derived paraphrases from par-
allel corpora but bilingual dictionaries. Dictionaries
usually have lower coverage, do not contain longer
phrases or inflected forms, and do not provide any
translation probability estimates. Also, their work
focuses solely on verb-noun collocations, while we
target collocations of arbitrary syntactic type.
Context-sensitive spelling error correction is the
task of correcting spelling mistakes that result in
another valid word, see for example (Golding and
Roth, 1999). It has traditionally focused on a small
number of pre-defined confusion sets, like homo-
phones or frequent spelling errors. Even when the
confusion sets are formed automatically, the simi-
larity of words in a confusion set has been based
on edit distance or phonetic similarity (Carlson et
al., 2001). In contrast, we focus on words that are
confusable due to their similar semantics instead of
similar spelling or pronunciation. Also, we do not
assume that the set of confusion sets is already given
to us. Instead, we automatically extract confusable
candidates from a parallel corpus.
Synonym extraction (Wu and Zhou, 2003), lexi-
cal substitution (McCarthy and Navigli, 2007) and
paraphrasing (Madnani and Dorr, 2010) are related
to collocation correction in the sense that they try to
find semantically equivalent words or phrases. How-
ever, there is a subtle but important difference be-
tween these tasks and collocation correction. In the
former, the main criterion is whether the original
phrase and the synonym/paraphrase candidate are
substitutable, i.e., both form a grammatical sentence
when substituted for each other in a particular con-
text. In contrast, in collocation correction, we are
primarily interested in finding candidates which are
not substitutable in their English context but appear
to be substitutable in the L1-language of the writer,
i.e., one forms a grammatical English sentence but
the other does not.
</bodyText>
<page confidence="0.997333">
108
</page>
<table confidence="0.997976142857143">
Sentences 52,149
Words 1,149,100
Distinct words 27,593
Avg. sentence length (words) 22.04
Collocation errors 2,747
Avg. collocation error length (words) 1.17
Avg. correction length (words) 1.13
</table>
<tableCaption confidence="0.9869445">
Table 1: Statistics of the NUS Corpus of Learner En-
glish (NUCLE)
</tableCaption>
<subsectionHeader confidence="0.467908">
3 Analysis of EFL collocation errors
</subsectionHeader>
<bodyText confidence="0.999951927710844">
While the fact that collocation errors can be caused
by L1-transfer has been ascertained by EFL re-
searchers (Meng, 2008), we need to quantify how
frequent collocation errors can be traced to these
types of transfer errors in order to estimate how
many errors in EFL writing we can potentially hope
to correct with information about the writer’s L1-
language.
We base our analysis on the NUS Corpus of
Learner English (NUCLE). The corpus consists of
about 1,400 essays written by EFL university stu-
dents on a wide range of topics, like environmen-
tal pollution or healthcare. Most of the students are
native Chinese speakers. The corpus contains over
one million words which are completely annotated
with error tags and corrections. All annotations have
been performed by professional English instructors.
The statistics of the corpus are summarized in Ta-
ble 1. The annotation is stored in a stand-off fashion.
Each error tag consists of the start and end offset of
the annotation, the type of the error, and the appro-
priate gold correction as deemed by the annotator.
The annotators were asked to provide a correction
that would result in a grammatical sentence if the
selected word or phrase would be replaced by the
correction.
In this work, we focus on errors which have
been marked with the error tag wrong colloca-
tion/idiom/preposition. As preposition errors are not
the focus of this work, we automatically filter out
all instances which represent simple substitutions of
prepositions, using a fixed list of frequent English
prepositions. In a similar way, we filter out a small
number of article errors which were marked as collo-
cation errors. Finally, we filter out instances where
the annotated phrase or the suggested correction is
longer than 3 words, as we observe that they contain
highly context-specific corrections and are unlikely
to generalize well (e.g., “for the simple reasons that
these can help them” → “simply to”).
After filtering, we end up with 2,747 collocation
errors and their respective corrections, which ac-
count for about 6% of all errors in NUCLE. This
makes collocation errors the 7th largest class of er-
rors in the corpus after article errors, redundancies,
prepositions, noun number, verb tense, and mechan-
ics. Not counting duplicates, there are 2,412 distinct
collocation errors and corrections. Although there
are other error types which are more frequent, collo-
cation errors represent a particular challenge as the
possible corrections are not restricted to a closed set
of choices and they are directly related to seman-
tics rather than syntax. We analyzed the collocation
errors and found that they can be attributed to the
following sources of confusion:
Spelling: We suspect that an error is caused by simi-
lar orthography if the edit distance between the erro-
neous phrase and its correction is less than a certain
threshold.
Homophones: We suspect that an error is caused by
similar pronunciation if the erroneous word and its
correction have the same pronunciation. We use the
CuVPlus English dictionary (Mitton, 1992) to map
words to their phonetic representations.
Synonyms: We suspect that an error is caused by
synonymy if the erroneous word and its correction
are synonyms in WordNet (Fellbaum, 1998). We use
WordNet 3.0.
L1-transfer: We suspect that an error is caused by
L1-transfer if the erroneous phrase and its correction
share a common translation in a Chinese-English
phrase table. The details of the phrase table con-
struction are described in Section 4. We note that
although we focus on Chinese-English translation,
our method is applicable to any language pair where
parallel corpora are available.
As CuVPlus and WordNet are defined for indi-
vidual words, we extend the matching process to
phrases in the following way: two phrases A and B
are deemed homophones/synonyms if they have the
same length and the i-th word in phrase A is a ho-
mophone/synonym of the corresponding i-th word
in phrase B.
</bodyText>
<page confidence="0.995779">
109
</page>
<bodyText confidence="0.977942">
Spelling ... it received critics (criticism) as much as complaints ...
... budget for the aged to improvise (improve) other areas.
Homophones ... diverse spending can aide (aid) our country.
... insure (ensure) the safety of civilians ...
Synonyms ... rapid increment (increase) of the seniors ...
... energy that we can apply (use) in the future ...
L1-transfer ... and give (provide, 给予 ) reasonable fares to the public ...
... and concerns (attention, 关注 ) that the nation put on technology and engineering ...
</bodyText>
<tableCaption confidence="0.888317333333333">
Table 3: Examples of collocation errors with different sources of confusion. The correction is shown in parenthesis.
For L1-transfer, we also show the shared Chinese translation. The L1-transfer examples shown here do not belong to
any of the other categories.
</tableCaption>
<table confidence="0.998825090909091">
Suspected Error Source Tokens Types
Spelling 154 131
Homophones 2 2
Synonyms 74 60
L1-transfer 1016 782
L1-transfer w/o spelling 954 727
L1-transfer w/o homophones 1015 781
L1-transfer w/o synonyms 958 737
L1-transfer w/o spelling, 906 692
homophones,
synonyms
</table>
<tableCaption confidence="0.993542">
Table 2: Analysis of collocation errors. The threshold for
</tableCaption>
<bodyText confidence="0.9682819">
spelling errors is one for phrases of up to six characters
and two for the remaining phrases.
The results of the analysis are shown in Table 2.
Tokens refer to running erroneous phrase-correction
pairs including duplicates, and types refer to distinct
erroneous phrase-correction pairs. As a collocation
error can be part of more than one category, the rows
in the table do not sum up to the total number of
errors. The number of errors that can be traced to
L1-transfer greatly outnumbers all other categories.
The table also shows the number of collocation er-
rors that can be traced to L1-transfer but not the
other sources. 906 collocation errors with 692 dis-
tinct collocation error types can be attributed only to
L1-transfer but not to spelling, homophones, or syn-
onyms. Table 3 shows some examples of collocation
errors for each category from our corpus. We note
that there are also collocation error types that cannot
be traced to any of the above sources. We will return
to these errors in Section 7.
</bodyText>
<sectionHeader confidence="0.977316" genericHeader="method">
4 Correcting Collocation Errors
</sectionHeader>
<bodyText confidence="0.999849">
In this section, we propose a novel approach for cor-
recting collocation errors in EFL writing.
</bodyText>
<subsectionHeader confidence="0.964208">
4.1 L1-induced Paraphrases
</subsectionHeader>
<bodyText confidence="0.999954952380952">
We use the popular technique of paraphrasing
with parallel corpora (Bannard and Callison-Burch,
2005) to automatically find collocation candidates
from a sentence-aligned L1-English parallel corpus.
As most of the essays in our corpus are written by
native Chinese speakers, we use the FBIS Chinese-
English corpus, which consists of about 230,000
Chinese sentences (8.5 million words) from news
articles, each with a single English translation. We
tokenize and lowercase the English half of the cor-
pus in the standard way. We segment the Chinese
half of the corpus using the maximum entropy seg-
menter from (Ng and Low, 2004; Low et al., 2005).
Subsequently, we automatically align the texts at the
word level using the Berkeley aligner (Liang et al.,
2006; Haghighi et al., 2009). We extract English-L1
and L1-English phrases of up to three words from
the aligned texts using the widely used phrase ex-
traction heuristic in (Koehn et al., 2003). The para-
phrase probability of an English phrase e1 given an
English phrase e2 is defined as
</bodyText>
<equation confidence="0.912724">
�p(e1 e2) = p(e1 f)p(f e2) (1)
f
</equation>
<bodyText confidence="0.9997122">
where f denotes a foreign phrase in the L1 language.
The phrase translation probabilities p(e1 f) and
p(f e2) are estimated by maximum likelihood es-
timation and smoothed using Good-Turing smooth-
ing (Foster et al., 2006). Finally, we only keep para-
</bodyText>
<page confidence="0.989542">
110
</page>
<bodyText confidence="0.9986325">
phrases with a probability above a certain threshold
(set to 0.001 in our work).
</bodyText>
<sectionHeader confidence="0.5619535" genericHeader="method">
4.2 Collocation Correction with Phrase-based
SMT
</sectionHeader>
<bodyText confidence="0.99853425">
We implement our approach in the framework
of phrase-based statistical machine transla-
tion (SMT) (Koehn et al., 2003). Phrase-based
SMT tries to find the highest scoring translation e
given an input sentence f. The decoding process of
finding the highest scoring translation is guided by a
log-linear model which scores translation candidates
using a set of feature functions hi, i = 1, ... , n
</bodyText>
<equation confidence="0.950944">
�Aihi(e,f) . (2)
</equation>
<bodyText confidence="0.999860823529412">
Typical features include a phrase translation proba-
bility p(e|f), an inverse phrase translation probabil-
ity p(f|e), a language model score p(e), and a con-
stant phrase penalty. The optimization of the feature
weights Ai, i = 1, ... , n can be done using mini-
mum error rate training (MERT) (Och, 2003) on a
development set of input sentences and their refer-
ence translations.
Because of the great flexibility of the log-linear
model, researchers have used the framework for
other tasks outside SMT, including grammatical er-
ror correction (Brockett et al., 2006). We adopt a
similar approach in this work. We modify the phrase
table of the popular phrase-based SMT decoder
MOSES (Koehn et al., 2007) to include collocation
corrections with features derived from spelling, ho-
mophones, synonyms, and L1-induced paraphrases.
</bodyText>
<listItem confidence="0.94388172972973">
• Spelling: For each English word, the phrase ta-
ble contains entries consisting of the word itself
and each word that is within a certain edit dis-
tance from the original word. Each entry has a
constant feature of 1.0.
• Homophones: For each English word, the
phrase table contains entries consisting of the
word itself and each of the word’s homophones.
We determine homophones using the CuVPlus
dictionary. Each entry has a constant feature of
1.0.
• Synonyms: For each English word, the phrase
table contains entries consisting of the word it-
self and each of its synonyms in WordNet. If a
word has more than one sense, we consider all
its senses. Each entry has a constant feature of
1.0.
• L1-paraphrases: For each English phrase, the
phrase table contains entries consisting of the
phrase and each of its L1-derived paraphrases
as described in Section 4.1. Each entry has two
real-valued features: a paraphrase probability
according to Equation 1 and an inverse para-
phrase probability.
• Baseline We combine the phrase tables built
for spelling, homophones, and synonyms. The
combined phrase table contains three binary
features for spelling, homophones, and syn-
onyms, respectively.
• All We combine the phrase tables from
spelling, homophones, synonyms, and L1-
paraphrases. The combined phrase table con-
tains five features: three binary features for
spelling, homophones, and synonyms, and
two real-valued features for the L1-paraphrase
probability and inverse L1-paraphrase proba-
bility.
</listItem>
<bodyText confidence="0.999818">
Additionally, each phrase table contains the standard
constant phrase penalty feature. The first four ta-
bles only contain collocation candidates for individ-
ual words. We leave it to the decoder to construct
corrections for longer phrases during the decoding
process if necessary.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999821">
In this section, we empirically evaluate our approach
on real collocation errors in learner English.
</bodyText>
<subsectionHeader confidence="0.988424">
5.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999997333333333">
We randomly sample a development set of 770 sen-
tences and a test set of 856 sentences from our cor-
pus. Each sentence contains exactly one collocation
error. The sampling is performed in a way that sen-
tences from the same document cannot end up in
both the development and the test set. In order to
</bodyText>
<equation confidence="0.987099">
n
score(e|f) = exp
i=1
</equation>
<page confidence="0.971525">
111
</page>
<bodyText confidence="0.999842571428571">
keep conditions as realistic as possible, we make no
attempt to filter the test set in any way.
We build phrase tables as described in Section 4.2.
For the purpose of the experiments reported in this
paper, we only need to generate phrase table entries
for words and phrases which actually appear in the
development or test set.
</bodyText>
<subsectionHeader confidence="0.995184">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999647">
We conduct an automatic and a human evalua-
tion. Our main evaluation metric is mean recipro-
cal rank (MRR) which is the arithmetic mean of the
inverse ranks of the first correct answer returned by
the system
</bodyText>
<equation confidence="0.983975">
1 N 1
MRR = N i=1 rank(i) (3)
</equation>
<bodyText confidence="0.981971">
where N is the size of the test set. If the system did
not return a correct answer for a test instance, we set
</bodyText>
<equation confidence="0.9521965">
1 to zero.
rank(i)
</equation>
<bodyText confidence="0.913123666666667">
In the human evaluation, we additionally report
precision at rank k, k = 1, 2, 3, which we calculate
as follows:
</bodyText>
<equation confidence="0.991513666666667">
P@k = Ea∈A score(a)
(4)
|A|
</equation>
<bodyText confidence="0.999742666666667">
where A is the set of returned answers of rank k or
less and score(·) is a real-valued scoring function
between zero and one.
</bodyText>
<subsectionHeader confidence="0.998462">
5.3 Collocation Error Experiments
</subsectionHeader>
<bodyText confidence="0.999974171428571">
Automatic correction of collocation errors can con-
ceptually be divided into two steps: i) identification
of wrong collocations in the input, and ii) correc-
tion of the identified collocations. In this work, we
focus on the second step and assume that the erro-
neous collocation has already been identified. While
this might seem like a simplification, it has been the
common evaluation setup in collocation error cor-
rection (see for example (Wu et al., 2010)). It also
has a practical application where the user first selects
a word or phrase and the system displays possible
corrections.
In our experiments, we use the start and end offset
of the collocation error provided by the human anno-
tator to identify the location of the collocation error.
We fix the translation of the rest of the sentence to
its identity. We remove phrase table entries where
the phrase and the candidate correction are identi-
cal, thus practically forcing the system to change
the identified phrase. We set the distortion limit of
the decoder to zero to achieve monotone decoding.
We previously observed that word order errors are
virtually absent in our collocation errors. For the
language model, we use a 5-gram language model
trained on the English Gigaword corpus with modi-
fied Kneser-Ney smoothing. All experiments use the
same language model to allow a fair comparison.
We perform MERT training with the popular
BLEU metric (Papineni et al., 2002) on the devel-
opment set of erroneous sentences and their correc-
tions. As the search space is restricted to changing
a single phrase per sentence, training converges rel-
atively quickly after two or three iterations. After
convergence, the model can be used to automatically
correct new collocation errors.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.99974537037037">
We evaluate the performance of the proposed
method on our test set of 856 sentences, each with
one collocation error. We conduct both an automatic
and a human evaluation. In the automatic evalua-
tion, the system’s performance is measured by com-
puting the rank of the gold answer provided by the
human annotator in the n-best list of the system. We
limit the size of the n-best list to the top 100 out-
puts. If the gold answer is not found in the top 100
outputs, the rank is considered to be infinity, or in
other words, the inverse of the rank is zero. We also
report the number of test instances for which the
gold answer was ranked among the top k answers,
k = 1, 2, 3,10,100. The results of the automatic
evaluation are shown in Table 4
For collocation errors, there are usually more than
one possible correct answer. Therefore, automatic
evaluation underestimates the actual performance of
the system by only considering the single gold an-
swer as correct and all other answers as wrong. As
such, we carried out a human evaluation for the sys-
tems BASELINE and ALL. We recruited two English
speakers to judge a subset of 500 test sentences. For
each sentence, a judge was shown the original sen-
tence and the 3-best candidates of each of the two
systems. We restricted human evaluation to the 3-
best candidates, as we believe that answers at a rank
</bodyText>
<page confidence="0.995535">
112
</page>
<table confidence="0.999720857142857">
Model Rank = 1 Rank &lt; 2 Rank &lt; 3 Rank &lt; 10 Rank &lt; 100 MRR
Spelling 35 41 42 44 44 4.51
Homophones 1 1 1 1 1 0.11
Synonyms 32 47 52 60 61 4.98
Baseline 49 68 80 93 96 7.61
L1-paraphrases 93 133 154 216 243 15.43
All 112 150 166 216 241 17.21
</table>
<tableCaption confidence="0.8482776">
Table 4: Results of automatic evaluation. Columns two to six show the number of gold answers that are ranked within
the top k answers. The last column shows the mean reciprocal rank in percentage. Bigger values are better.
P(A) 0.8076
Kappa 0.6152
Table 5: Inter-annotator agreement. P(E) = 0.5.
</tableCaption>
<bodyText confidence="0.9999535625">
larger than three will not be very useful in a prac-
tical application. The candidates are displayed to-
gether in alphabetical order without any information
about their rank or which system produced them or
the gold answer by the annotator. The difference
between the candidates and the original sentence is
highlighted. The judges were asked to make a bi-
nary judgment for each of the candidates on whether
the proposed candidate is a valid correction of the
original or not. We represent valid corrections with
a score of 1.0 and invalid corrections with a score
of 0.0. Inter-annotator agreement is reported in Ta-
ble 5. The chance of agreement P(A) is the percent-
age of times that the annotators agree, and P(E) is
the expected agreement by chance, which is 0.5 in
our case. The Kappa coefficient is defined as
</bodyText>
<equation confidence="0.99735">
P(A) − P(E)
Kappa =
1 − P(E)
</equation>
<bodyText confidence="0.999960655172414">
We obtain a Kappa coefficient of 0.6152. A Kappa
coefficient between 0.6 and 0.8 is considered as
showing substantial agreement according to Landis
and Koch (1977). To compute precision at rank k,
we average the judgments. Thus, a system can re-
ceive a score of 0.0 (both judgments negative), 0.5
(judges disagree), or 1.0 (both judgments positive)
for each returned answer. To compute MRR, we
cannot simply average the judgments as MRR re-
quires binary judgments on whether an item is cor-
rect or not. Instead, we report MRR on the union and
the intersection of the judgments. In the first case,
the rank of the first correct item is the minimum
rank of any item judged correct by either judge. In
the second case, the rank of the first correct item
is the minimum rank of any item judged correct by
both judges. The results for the human evaluation
are shown in Table 6. Our best system ALL outper-
forms the BASELINE approach on all measures. It
receives a precision at rank 1 of 38.20% and a MRR
of 33.16% (intersection) and 57.26% (union). Ta-
ble 7 shows some examples from our test set.
Unfortunately, comparison of our results with pre-
vious work is complicated by the fact that there cur-
rently exists no standard data set for collocation er-
ror correction. We will make our corpus available
for research purposes in the hope that it will allow
researchers to more directly compare their results in
future.
</bodyText>
<sectionHeader confidence="0.971536" genericHeader="evaluation">
7 Analysis
</sectionHeader>
<bodyText confidence="0.999475105263158">
In this section, we analyze and categorize those test
instances for which the ALL system could not pro-
duce an acceptable correction in the top 3 candi-
dates. We manually analyze 100 test sentences for
which neither judge had deemed any candidate an-
swer to be a valid correction. Based on our findings,
we categorize the 100 sentences into eight categories
which are shown below. Table 8 shows examples
from each category.
Out-of-vocabulary (21/100) The most frequent rea-
son why the system does not produce a good correc-
tion is that the erroneous collocation is out of vocab-
ulary. These collocations often involve compound
words, like man-hours or carefully-nurturing, or in-
frequent expressions, like copy phenomena, which
do not appear in the FBIS parallel corpus. We ex-
pect that this problem can be reduced by using larger
parallel corpora for paraphrase extraction.
Near miss (18/100) The second largest category
</bodyText>
<page confidence="0.997248">
113
</page>
<table confidence="0.999041333333333">
Model Rank = 1 Rank &lt; 2 Rank &lt; 3 P@1 P@2 P@3 MRR
Baseline 43  |141 69  |201 83  |237 18.40 16.68 15.36 12.13  |36.60
All 137  |245 176  |303 204  |340 38.20 32.87 29.30 33.16  |57.26
</table>
<tableCaption confidence="0.98349">
Table 6: Results of human evaluation. Rank and MRR results are shown for the intersection (first value) and union
(second value) of human judgments.
</tableCaption>
<bodyText confidence="0.9747375625">
Original it must be clear, concise and unambiguous to prevent any off-track
Gold it must be clear, concise and unambiguous to avoid any off-track
All it must be clear, concise and unambiguous to avoid any off-track
it must be clear, concise and unambiguous to stop any off-track
it must be clear, concise and unambiguous to block any off-track
Baseline *it must be clear, concise and unambiguous to present any off-track
it must be clear, concise and unambiguous to forestall any off-track
*it must be clear, concise and unambiguous to lock any off-track
Original although many may agree that public spending on the elderly should be limited ...
Gold although many may argue that public spending on the elderly should be limited ...
All although many may believe that public spending on the elderly should be limited ...
although many may think that public spending on the elderly should be limited ...
although many may accept that public spending on the elderly should be limited ...
Baseline *although many may agreed that public spending on the elderly should be limited ...
*although many may hold that public spending on the elderly should be limited ...
*although many may agrees that public spending on the elderly should be limited ...
</bodyText>
<tableCaption confidence="0.9827565">
Table 7: Examples of test sentences with the top 3 answers of the ALL and BASELINE system. An answer judged
incorrect by at least one judge is marked with an asterisk (*).
</tableCaption>
<bodyText confidence="0.980258875">
Out of vocabulary ... many illegal copy phenomena (copy phenomena, copies) in china.
... lead to reduced man-hours (man-hours, productivity) as people fall sick .. .
Near miss ... smaller groups of people, sometimes even (more, only) individual .
... take pre-emptive actions (activities, measures) ...
Function/auxiliary words ... entertainment an elderly person can have (be, enjoy) .
... and the security issue is solved also (and, too)
Discourse specific ... make other countries respect and fear you (&lt;question mark&gt;, a country)
... will contribute nothing to the accident (explosion, problem) .
Spelling errors this incidence (rate, incident) had also resulted in 4 fatalities ...
refrigerator did not compromise (yield, comprise) of any moving parts ...
Word sense ... refers to the desire or shortage of a good (better, commodity) and ...
... members are always from different majors (major league, specialties)
Preposition constructions ... can be an area worth investing (investing, investing in)
... in spending their resources (resources, resources on)
Others this might redirect (make sound, reduce) foreign investments ...
... a trading hub since british ’s (british ’s, british) rule.
</bodyText>
<tableCaption confidence="0.9728545">
Table 8: Examples of sentences without valid corrections by the ALL model. The top-1 suggestion of the system and
the gold answer (in bold) are shown in parenthesis.
</tableCaption>
<page confidence="0.99838">
114
</page>
<bodyText confidence="0.999980041666667">
consists of instances where the system barely misses
the gold standard answer. This includes cases where
the extracted L1-paraphrases do not contain the ex-
act phrase required, e.g., the paraphrase table con-
tains even only get when the gold correction was
even → only, or the phrase table actually contains
the gold answer but fails to rank it among the top 3
answers. The first problem could be addressed by
modifying the phrase extraction heuristic to produce
more fine-grained phrase pairs. The second prob-
lem requires a better language model. Although our
language model is trained on the large English Giga-
word corpus, it is not always successful in promot-
ing the correct candidate to the top. The domain mis-
match between the newswire domain of Gigaword
and student essays could be one reason for this.
Function/auxiliary words (14/100) We observe
that collocation errors that involve function words
or auxiliary words are not handled very well by our
model. Function words and auxiliary words in En-
glish lack direct counterparts in Chinese, which is
why the word alignments and therefore the extracted
phrases for these words contain a high amount of
noise. As function words and auxiliaries are essen-
tially a closed set, it might be more promising to
build separate models with fixed confusion sets for
them.
Discourse specific (14/100) Some of the gold an-
swers are highly specific to the particular discourse
that they appear in. As our model corrects colloca-
tion errors at the sentence level, such gold answers
will be very difficult or impossible to determine cor-
rectly. Including more context beyond the sentence
level might help to overcome this problem, although
it is not easy to integrate this larger context informa-
tion.
Spelling errors (9/100) Some of the collocation er-
rors are caused by spelling mistakes, e.g., incidence
instead of incident. Although the ALL model in-
cludes candidates which are created through edit dis-
tance, paraphrase candidates created from the mis-
spelled word can dominate the top 3 ranks, e.g., rate
and frequently are paraphrases of incidence. A pos-
sible solution would be to perform spell-checking as
a separate pre-processing step prior to collocation
correction.
Word sense (7/100) Some of the failures of the
model can be attributed to ambiguous senses of the
collocation phrase. As we do not perform word
sense disambiguation in our current work, candi-
dates from other word senses can end up as the top
candidates. Including word sense disambiguation
into the model might help, although accurate word
sense disambiguation on noisy learner text may not
be easy.
Preposition constructions (6/100) Some of the col-
location errors involve preposition constructions,
e.g., the student wrote attend instead of attend
to. Because prepositions do not have a direct
counterpart in Chinese, the L1-paraphrases do not
model their semantics very well. This category is
closely related to the function/auxiliary word cate-
gory. Again, since prepositions are a closed set, it
might be more promising to build a separate model
for them.
Others (11/100) Other mistakes include collocation
errors where the gold answer slightly changed the
semantics of the target word, e.g., redirect potential
foreign investments → reduce potential foreign in-
vestments, active-passive alternation (enhanced eco-
nomics → was economical), and noun possessive er-
rors (british ’s rule → british rule).
</bodyText>
<sectionHeader confidence="0.99381" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999973846153846">
We have presented a novel approach for correcting
collocation errors in written learner text. Our ap-
proach exploits the semantic similarity of words in
the writer’s L1-language based on paraphrases ex-
tracted from an L1-English parallel corpus. Our ex-
periments on real-world learner data show that our
approach outperforms traditional approaches based
on edit distance, homophones, and synonyms by a
large margin.
In future work, we plan to extend our system to
fully automatic collocation correction that involves
both identification and correction of collocation er-
rors.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9877912">
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
</bodyText>
<page confidence="0.998658">
115
</page>
<sectionHeader confidence="0.996381" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999170481132075">
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of
ACL.
C. Brockett, W. B. Dolan, and M. Gamon. 2006. Cor-
recting ESL errors using phrasal SMT techniques. In
Proceedings of ACL.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling
up context-sensitive text correction. In Proceedings of
IAAI.
Y. S. Chan and H. T. Ng. 2005. Scaling up word sense
disambiguation via parallel texts. In Proceedings of
AAAI.
Y. C. Chang, J. S. Chang, H. J. Chen, and H. C. Liou.
2008. An automatic collocation writing assistant for
Taiwanese EFL learners: A case of corpus-based NLP
technology. Computer Assisted Language Learning,
21(3):283–299.
D. Dahlmeier and H. T. Ng. 2011. Grammatical error
correction with alternating structure optimization. In
Proceedings of ACL.
M. Farghal and H. Obiedat. 1995. Collocations: A ne-
glected variable in EFL. International Review of App-
plied Linguistics, 33.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press, Cambridge,MA.
J. R. Firth. 1957. Papers in Linguistics 1934-1951. Ox-
ford University Press, London.
G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable
smoothing for statistical machine translation. In Pro-
ceedings of EMNLP.
Y. Futagi, P. Deane, M. Chodorow, and J. Tetreault. 2008.
A computational approach to detecting collocation er-
rors in the writing of non-native speakers of English.
Journal of Computer-Assisted Learning, 21.
M. Gamon. 2010. Using mostly native data to correct
errors in learners’ writing: A meta-classifier approach.
In Proceedings of HLT-NAACL.
A. R. Golding and D. Roth. 1999. A winnow-based ap-
proach to context-sensitive spelling correction. Ma-
chine Learning, 34:107–130.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proceedings of ACL-IJCNLP.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL
demonstration session.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1).
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proceedings of HLT-NAACL.
A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated
suggestions for miscollocations. In Proceedings of the
NAACL HLT Workshop on Innovative Use of NLP for
Building Educational Applications.
C. Liu, D. Dahlmeier, and H. T. Ng. 2010a. PEM: a
paraphrase evaluation metric exploiting parallel texts.
In Proceedings of EMNLP.
C. Liu, D. Dahlmeier, and H. T. Ng. 2010b.
TESLA: Translation evaluation of sentences with
linear-programming-based analysis. In Proceedings of
WMT and MetricsMATR.
J. K. Low, H. T. Ng, and W. Guo. 2005. A maximum
entropy approach to Chinese word segmentation. In
Proceedings of the 4th SIGHAN Workshop on Chinese
Language Processing.
N. Madnani and B. J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341–387.
D. McCarthy and R. Navigli. 2007. Semeval-2007 task
10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (SemEval-2007).
J. Meng. 2008. Erroneous collocations caused by lan-
guage transfer in Chinese EFL writing. US-China For-
eign Language, 6:57–61.
R. Mitton. 1992. A description of a computer-usable dic-
tionary file based on the Oxford Advanced Learner’s
Dictionary of Current English.
H. T. Ng and Y. S. Chan. 2007. Semeval-2007 task 11:
English lexical sample task via english-chinese paral-
lel text. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval 2007).
H. T. Ng and J. K. Low. 2004. Chinese part-of-speech
tagging: One-at-a-time or all-at-once? word-based or
character-based? In Proceedings of EMNLP.
H. T. Ng, B. Wang, and Y. S. Chan. 2003. Exploiting
parallel texts for word sense disambiguation: An em-
pirical study. In Proceedings of ACL.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proceedings of ACL.
A. Rozovskaya and D. Roth. 2010. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of EMNLP.
C. C. Shei and H. Pain. 2000. An ESL writer’s collo-
cational aid. Computer Assisted Language Learning,
13.
</reference>
<page confidence="0.990062">
116
</page>
<reference confidence="0.999589608695652">
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
Fluency, adequacy, or HTER? Exploring different hu-
man judgments with a tunable MT metric. In Proceed-
ings of WMT.
M. Swan and B. Smith. 2001. Learner English: A
Teacher’s Guide to Interference and Other Problems.
Cambridge University Press, Cambridge, UK.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In Proceedings ofACL.
D. Wible, C. H. Kuo, N. L. Tsao, A. Liu, and H. L. Lin.
2003. Bootstrapping in a language learning environ-
ment. Journal of Computer-Assisted Learning, 19.
H. Wu and M. Zhou. 2003. Synonymous collocation ex-
traction using translation information. In Proceedings
of ACL.
J. C. Wu, Y. C. Chang, T. Mitamura, and J. S. Chang.
2010. Automatic collocation suggestion in academic
writing. In Proceedings of the ACL 2010 Conference
Short Papers.
Z. Zhong and H. T. Ng. 2009. Word sense disambigua-
tion for all words without hard labor. In Proceedings
of IJCAI.
</reference>
<page confidence="0.998124">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.482165">
<title confidence="0.991749">Correcting Semantic Collocation Errors with L1-induced Paraphrases</title>
<author confidence="0.789666">Tou</author>
<affiliation confidence="0.8525235">Graduate School for Integrative Sciences and of Computer Science, National University of</affiliation>
<email confidence="0.847359">danielhe@comp.nus.edu.sg</email>
<email confidence="0.847359">nght@comp.nus.edu.sg</email>
<abstract confidence="0.985941071428571">We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora. Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L1language) of the writer. An analysis of a large corpus of annotated learner English confirms this assumption. We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4934" citStr="Bannard and Callison-Burch, 2005" startWordPosition="782" endWordPosition="785">matically extract from an L1-English parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight di</context>
<context position="14850" citStr="Bannard and Callison-Burch, 2005" startWordPosition="2372" endWordPosition="2375">collocation errors with 692 distinct collocation error types can be attributed only to L1-transfer but not to spelling, homophones, or synonyms. Table 3 shows some examples of collocation errors for each category from our corpus. We note that there are also collocation error types that cannot be traced to any of the above sources. We will return to these errors in Section 7. 4 Correcting Collocation Errors In this section, we propose a novel approach for correcting collocation errors in EFL writing. 4.1 L1-induced Paraphrases We use the popular technique of paraphrasing with parallel corpora (Bannard and Callison-Burch, 2005) to automatically find collocation candidates from a sentence-aligned L1-English parallel corpus. As most of the essays in our corpus are written by native Chinese speakers, we use the FBIS ChineseEnglish corpus, which consists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>C. Bannard and C. Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockett</author>
<author>W B Dolan</author>
<author>M Gamon</author>
</authors>
<title>Correcting ESL errors using phrasal SMT techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="17170" citStr="Brockett et al., 2006" startWordPosition="2758" endWordPosition="2761">a set of feature functions hi, i = 1, ... , n �Aihi(e,f) . (2) Typical features include a phrase translation probability p(e|f), an inverse phrase translation probability p(f|e), a language model score p(e), and a constant phrase penalty. The optimization of the feature weights Ai, i = 1, ... , n can be done using minimum error rate training (MERT) (Och, 2003) on a development set of input sentences and their reference translations. Because of the great flexibility of the log-linear model, researchers have used the framework for other tasks outside SMT, including grammatical error correction (Brockett et al., 2006). We adopt a similar approach in this work. We modify the phrase table of the popular phrase-based SMT decoder MOSES (Koehn et al., 2007) to include collocation corrections with features derived from spelling, homophones, synonyms, and L1-induced paraphrases. • Spelling: For each English word, the phrase table contains entries consisting of the word itself and each word that is within a certain edit distance from the original word. Each entry has a constant feature of 1.0. • Homophones: For each English word, the phrase table contains entries consisting of the word itself and each of the word’</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>C. Brockett, W. B. Dolan, and M. Gamon. 2006. Correcting ESL errors using phrasal SMT techniques. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Carlson</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>Scaling up context-sensitive text correction.</title>
<date>2001</date>
<booktitle>In Proceedings of IAAI.</booktitle>
<contexts>
<context position="7100" citStr="Carlson et al., 2001" startWordPosition="1119" endWordPosition="1122">e any translation probability estimates. Also, their work focuses solely on verb-noun collocations, while we target collocations of arbitrary syntactic type. Context-sensitive spelling error correction is the task of correcting spelling mistakes that result in another valid word, see for example (Golding and Roth, 1999). It has traditionally focused on a small number of pre-defined confusion sets, like homophones or frequent spelling errors. Even when the confusion sets are formed automatically, the similarity of words in a confusion set has been based on edit distance or phonetic similarity (Carlson et al., 2001). In contrast, we focus on words that are confusable due to their similar semantics instead of similar spelling or pronunciation. Also, we do not assume that the set of confusion sets is already given to us. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. However, there is a subtle but important difference between these</context>
</contexts>
<marker>Carlson, Rosen, Roth, 2001</marker>
<rawString>A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up context-sensitive text correction. In Proceedings of IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
</authors>
<title>Scaling up word sense disambiguation via parallel texts.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="4846" citStr="Chan and Ng, 2005" startWordPosition="769" endWordPosition="772"> component in our approach generates L1-induced paraphrases which we automatically extract from an L1-English parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, w</context>
</contexts>
<marker>Chan, Ng, 2005</marker>
<rawString>Y. S. Chan and H. T. Ng. 2005. Scaling up word sense disambiguation via parallel texts. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Chang</author>
<author>J S Chang</author>
<author>H J Chen</author>
<author>H C Liou</author>
</authors>
<title>An automatic collocation writing assistant for Taiwanese EFL learners: A case of corpus-based NLP technology.</title>
<date>2008</date>
<journal>Computer Assisted Language Learning,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="6182" citStr="Chang et al. (2008)" startWordPosition="984" endWordPosition="987"> error correction and related NLP tasks like context-sensitive spelling error correction, synonym extraction, lexical substitution, and paraphrasing. Most work in collocation error correction has relied on dictionaries or manually created databases to generate collocation candidates (Shei and Pain, 2000; Wible et al., 2003; Futagi et al., 2008). Other work has focused on finding candidates that collocate with similar words, e.g., verbs that appear with the same noun objects form a confusion set (Liu et al., 2009; Wu et al., 2010). The work most similar to ours is probably the one presented by Chang et al. (2008), as they also use translation information to generate collocation candidates. However, they do not use automatically derived paraphrases from parallel corpora but bilingual dictionaries. Dictionaries usually have lower coverage, do not contain longer phrases or inflected forms, and do not provide any translation probability estimates. Also, their work focuses solely on verb-noun collocations, while we target collocations of arbitrary syntactic type. Context-sensitive spelling error correction is the task of correcting spelling mistakes that result in another valid word, see for example (Goldi</context>
</contexts>
<marker>Chang, Chang, Chen, Liou, 2008</marker>
<rawString>Y. C. Chang, J. S. Chang, H. J. Chen, and H. C. Liou. 2008. An automatic collocation writing assistant for Taiwanese EFL learners: A case of corpus-based NLP technology. Computer Assisted Language Learning, 21(3):283–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Grammatical error correction with alternating structure optimization.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1990" citStr="Dahlmeier and Ng, 2011" startWordPosition="299" endWordPosition="302">onally focused on confusion sets with similar spelling (e.g., {dessert, desert}) or similar pronunciation (e.g., {there, their}). In other words, the words in a confusion set are deemed confusable because of orthographic or phonetic similarity. Other work in GEC has defined the confu1For simplicity, we will collectively refer to both terms as English as a foreign language (EFL) sion sets based on syntactic similarity, for example all English articles or the most frequent English prepositions form a confusion set (see for example (Tetreault et al., 2010; Rozovskaya and Roth, 2010; Gamon, 2010; Dahlmeier and Ng, 2011) among others). In contrast, we investigate in this paper a class of grammatical errors where the source of confusion is the similar semantics of the words, rather than orthography, phonetics, or syntax. In particular, we focus on collocation errors in EFL writing. The term collocation (Firth, 1957) describes a sequence of words that is conventionally used together in a particular way by native speakers and appears more often together than one would expect by chance. The correct use of collocations is a major difficulty for EFL students (Farghal and Obiedat, 1995). In this work, we present a n</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>D. Dahlmeier and H. T. Ng. 2011. Grammatical error correction with alternating structure optimization. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Farghal</author>
<author>H Obiedat</author>
</authors>
<title>Collocations: A neglected variable in EFL.</title>
<date>1995</date>
<journal>International Review of Appplied Linguistics,</journal>
<volume>33</volume>
<contexts>
<context position="2560" citStr="Farghal and Obiedat, 1995" startWordPosition="392" endWordPosition="395">aya and Roth, 2010; Gamon, 2010; Dahlmeier and Ng, 2011) among others). In contrast, we investigate in this paper a class of grammatical errors where the source of confusion is the similar semantics of the words, rather than orthography, phonetics, or syntax. In particular, we focus on collocation errors in EFL writing. The term collocation (Firth, 1957) describes a sequence of words that is conventionally used together in a particular way by native speakers and appears more often together than one would expect by chance. The correct use of collocations is a major difficulty for EFL students (Farghal and Obiedat, 1995). In this work, we present a novel approach for automatic correction of collocation errors in EFL writing. Our key observation is that words are potentially confusable for an EFL student if they have similar translations in the writer’s first language (L1- language), or in other words if they have the same semantics in the L1-language of the writer. The Chinese word 看 (kàn), for example, has over a dozen translations in English, including the words see, look, read, and watch. A Chinese speaker who still “thinks” in Chinese has to choose from all these possible translations when he wants to exp</context>
</contexts>
<marker>Farghal, Obiedat, 1995</marker>
<rawString>M. Farghal and H. Obiedat. 1995. Collocations: A neglected variable in EFL. International Review of Appplied Linguistics, 33.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press, Cambridge,MA.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press, Cambridge,MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>Papers in Linguistics 1934-1951.</title>
<date>1957</date>
<publisher>Oxford University Press,</publisher>
<location>London.</location>
<contexts>
<context position="2290" citStr="Firth, 1957" startWordPosition="350" endWordPosition="351">llectively refer to both terms as English as a foreign language (EFL) sion sets based on syntactic similarity, for example all English articles or the most frequent English prepositions form a confusion set (see for example (Tetreault et al., 2010; Rozovskaya and Roth, 2010; Gamon, 2010; Dahlmeier and Ng, 2011) among others). In contrast, we investigate in this paper a class of grammatical errors where the source of confusion is the similar semantics of the words, rather than orthography, phonetics, or syntax. In particular, we focus on collocation errors in EFL writing. The term collocation (Firth, 1957) describes a sequence of words that is conventionally used together in a particular way by native speakers and appears more often together than one would expect by chance. The correct use of collocations is a major difficulty for EFL students (Farghal and Obiedat, 1995). In this work, we present a novel approach for automatic correction of collocation errors in EFL writing. Our key observation is that words are potentially confusable for an EFL student if they have similar translations in the writer’s first language (L1- language), or in other words if they have the same semantics in the L1-la</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth. 1957. Papers in Linguistics 1934-1951. Oxford University Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>R Kuhn</author>
<author>H Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="16037" citStr="Foster et al., 2006" startWordPosition="2572" endWordPosition="2575">cally align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as �p(e1 e2) = p(e1 f)p(f e2) (1) f where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 f) and p(f e2) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al., 2006). Finally, we only keep para110 phrases with a probability above a certain threshold (set to 0.001 in our work). 4.2 Collocation Correction with Phrase-based SMT We implement our approach in the framework of phrase-based statistical machine translation (SMT) (Koehn et al., 2003). Phrase-based SMT tries to find the highest scoring translation e given an input sentence f. The decoding process of finding the highest scoring translation is guided by a log-linear model which scores translation candidates using a set of feature functions hi, i = 1, ... , n �Aihi(e,f) . (2) Typical features include a</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable smoothing for statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Futagi</author>
<author>P Deane</author>
<author>M Chodorow</author>
<author>J Tetreault</author>
</authors>
<title>A computational approach to detecting collocation errors in the writing of non-native speakers of English.</title>
<date>2008</date>
<journal>Journal of Computer-Assisted Learning,</journal>
<volume>21</volume>
<contexts>
<context position="5909" citStr="Futagi et al., 2008" startWordPosition="934" endWordPosition="937">esults are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight differences between collocation error correction and related NLP tasks like context-sensitive spelling error correction, synonym extraction, lexical substitution, and paraphrasing. Most work in collocation error correction has relied on dictionaries or manually created databases to generate collocation candidates (Shei and Pain, 2000; Wible et al., 2003; Futagi et al., 2008). Other work has focused on finding candidates that collocate with similar words, e.g., verbs that appear with the same noun objects form a confusion set (Liu et al., 2009; Wu et al., 2010). The work most similar to ours is probably the one presented by Chang et al. (2008), as they also use translation information to generate collocation candidates. However, they do not use automatically derived paraphrases from parallel corpora but bilingual dictionaries. Dictionaries usually have lower coverage, do not contain longer phrases or inflected forms, and do not provide any translation probability </context>
</contexts>
<marker>Futagi, Deane, Chodorow, Tetreault, 2008</marker>
<rawString>Y. Futagi, P. Deane, M. Chodorow, and J. Tetreault. 2008. A computational approach to detecting collocation errors in the writing of non-native speakers of English. Journal of Computer-Assisted Learning, 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners’ writing: A meta-classifier approach.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1965" citStr="Gamon, 2010" startWordPosition="297" endWordPosition="298">) has traditionally focused on confusion sets with similar spelling (e.g., {dessert, desert}) or similar pronunciation (e.g., {there, their}). In other words, the words in a confusion set are deemed confusable because of orthographic or phonetic similarity. Other work in GEC has defined the confu1For simplicity, we will collectively refer to both terms as English as a foreign language (EFL) sion sets based on syntactic similarity, for example all English articles or the most frequent English prepositions form a confusion set (see for example (Tetreault et al., 2010; Rozovskaya and Roth, 2010; Gamon, 2010; Dahlmeier and Ng, 2011) among others). In contrast, we investigate in this paper a class of grammatical errors where the source of confusion is the similar semantics of the words, rather than orthography, phonetics, or syntax. In particular, we focus on collocation errors in EFL writing. The term collocation (Firth, 1957) describes a sequence of words that is conventionally used together in a particular way by native speakers and appears more often together than one would expect by chance. The correct use of collocations is a major difficulty for EFL students (Farghal and Obiedat, 1995). In </context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>M. Gamon. 2010. Using mostly native data to correct errors in learners’ writing: A meta-classifier approach. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A winnow-based approach to context-sensitive spelling correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--107</pages>
<contexts>
<context position="1355" citStr="Golding and Roth, 1999" startWordPosition="198" endWordPosition="201">hrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms. 1 Introduction Grammatical error correction (GEC) is emerging as a commercially attractive application of natural language processing (NLP) for the booming market of English as foreign or second language (EFL/ESL1). The de facto standard approach to GEC is to build a statistical model that can choose the most likely correction from a confusion set of possible correction choices. The way the confusion set is defined depends on the type of error. Work in contextsensitive spelling error correction (Golding and Roth, 1999) has traditionally focused on confusion sets with similar spelling (e.g., {dessert, desert}) or similar pronunciation (e.g., {there, their}). In other words, the words in a confusion set are deemed confusable because of orthographic or phonetic similarity. Other work in GEC has defined the confu1For simplicity, we will collectively refer to both terms as English as a foreign language (EFL) sion sets based on syntactic similarity, for example all English articles or the most frequent English prepositions form a confusion set (see for example (Tetreault et al., 2010; Rozovskaya and Roth, 2010; G</context>
<context position="6800" citStr="Golding and Roth, 1999" startWordPosition="1070" endWordPosition="1073">2008), as they also use translation information to generate collocation candidates. However, they do not use automatically derived paraphrases from parallel corpora but bilingual dictionaries. Dictionaries usually have lower coverage, do not contain longer phrases or inflected forms, and do not provide any translation probability estimates. Also, their work focuses solely on verb-noun collocations, while we target collocations of arbitrary syntactic type. Context-sensitive spelling error correction is the task of correcting spelling mistakes that result in another valid word, see for example (Golding and Roth, 1999). It has traditionally focused on a small number of pre-defined confusion sets, like homophones or frequent spelling errors. Even when the confusion sets are formed automatically, the similarity of words in a confusion set has been based on edit distance or phonetic similarity (Carlson et al., 2001). In contrast, we focus on words that are confusable due to their similar semantics instead of similar spelling or pronunciation. Also, we do not assume that the set of confusion sets is already given to us. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extr</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A winnow-based approach to context-sensitive spelling correction. Machine Learning, 34:107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>J Blitzer</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="15527" citStr="Haghighi et al., 2009" startWordPosition="2483" endWordPosition="2486">ntence-aligned L1-English parallel corpus. As most of the essays in our corpus are written by native Chinese speakers, we use the FBIS ChineseEnglish corpus, which consists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as �p(e1 e2) = p(e1 f)p(f e2) (1) f where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 f) and p(f e2) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al., 2006). Finally, we only keep para110 phrases with a probability above a certain threshold (set </context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="15691" citStr="Koehn et al., 2003" startWordPosition="2511" endWordPosition="2514">sists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as �p(e1 e2) = p(e1 f)p(f e2) (1) f where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 f) and p(f e2) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al., 2006). Finally, we only keep para110 phrases with a probability above a certain threshold (set to 0.001 in our work). 4.2 Collocation Correction with Phrase-based SMT We implement our approach in the framework of phrase-based statistical machine translation (</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL demonstration session.</booktitle>
<contexts>
<context position="17307" citStr="Koehn et al., 2007" startWordPosition="2782" endWordPosition="2785">phrase translation probability p(f|e), a language model score p(e), and a constant phrase penalty. The optimization of the feature weights Ai, i = 1, ... , n can be done using minimum error rate training (MERT) (Och, 2003) on a development set of input sentences and their reference translations. Because of the great flexibility of the log-linear model, researchers have used the framework for other tasks outside SMT, including grammatical error correction (Brockett et al., 2006). We adopt a similar approach in this work. We modify the phrase table of the popular phrase-based SMT decoder MOSES (Koehn et al., 2007) to include collocation corrections with features derived from spelling, homophones, synonyms, and L1-induced paraphrases. • Spelling: For each English word, the phrase table contains entries consisting of the word itself and each word that is within a certain edit distance from the original word. Each entry has a constant feature of 1.0. • Homophones: For each English word, the phrase table contains entries consisting of the word itself and each of the word’s homophones. We determine homophones using the CuVPlus dictionary. Each entry has a constant feature of 1.0. • Synonyms: For each Englis</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="25314" citStr="Landis and Koch (1977)" startWordPosition="4162" endWordPosition="4165">e candidates on whether the proposed candidate is a valid correction of the original or not. We represent valid corrections with a score of 1.0 and invalid corrections with a score of 0.0. Inter-annotator agreement is reported in Table 5. The chance of agreement P(A) is the percentage of times that the annotators agree, and P(E) is the expected agreement by chance, which is 0.5 in our case. The Kappa coefficient is defined as P(A) − P(E) Kappa = 1 − P(E) We obtain a Kappa coefficient of 0.6152. A Kappa coefficient between 0.6 and 0.8 is considered as showing substantial agreement according to Landis and Koch (1977). To compute precision at rank k, we average the judgments. Thus, a system can receive a score of 0.0 (both judgments negative), 0.5 (judges disagree), or 1.0 (both judgments positive) for each returned answer. To compute MRR, we cannot simply average the judgments as MRR requires binary judgments on whether an item is correct or not. Instead, we report MRR on the union and the intersection of the judgments. In the first case, the rank of the first correct item is the minimum rank of any item judged correct by either judge. In the second case, the rank of the first correct item is the minimum </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="15503" citStr="Liang et al., 2006" startWordPosition="2479" endWordPosition="2482">candidates from a sentence-aligned L1-English parallel corpus. As most of the essays in our corpus are written by native Chinese speakers, we use the FBIS ChineseEnglish corpus, which consists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as �p(e1 e2) = p(e1 f)p(f e2) (1) f where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 f) and p(f e2) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al., 2006). Finally, we only keep para110 phrases with a probability above a</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Liu</author>
<author>D Wible</author>
<author>N L Tsao</author>
</authors>
<title>Automated suggestions for miscollocations.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<contexts>
<context position="6080" citStr="Liu et al., 2009" startWordPosition="964" endWordPosition="967"> of related work on collocation error correction. We also highlight differences between collocation error correction and related NLP tasks like context-sensitive spelling error correction, synonym extraction, lexical substitution, and paraphrasing. Most work in collocation error correction has relied on dictionaries or manually created databases to generate collocation candidates (Shei and Pain, 2000; Wible et al., 2003; Futagi et al., 2008). Other work has focused on finding candidates that collocate with similar words, e.g., verbs that appear with the same noun objects form a confusion set (Liu et al., 2009; Wu et al., 2010). The work most similar to ours is probably the one presented by Chang et al. (2008), as they also use translation information to generate collocation candidates. However, they do not use automatically derived paraphrases from parallel corpora but bilingual dictionaries. Dictionaries usually have lower coverage, do not contain longer phrases or inflected forms, and do not provide any translation probability estimates. Also, their work focuses solely on verb-noun collocations, while we target collocations of arbitrary syntactic type. Context-sensitive spelling error correction</context>
</contexts>
<marker>Liu, Wible, Tsao, 2009</marker>
<rawString>A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated suggestions for miscollocations. In Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Liu</author>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>PEM: a paraphrase evaluation metric exploiting parallel texts.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4952" citStr="Liu et al., 2010" startWordPosition="786" endWordPosition="789">sh parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight differences between </context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2010</marker>
<rawString>C. Liu, D. Dahlmeier, and H. T. Ng. 2010a. PEM: a paraphrase evaluation metric exploiting parallel texts. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Liu</author>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>TESLA: Translation evaluation of sentences with linear-programming-based analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of WMT and MetricsMATR.</booktitle>
<contexts>
<context position="4952" citStr="Liu et al., 2010" startWordPosition="786" endWordPosition="789">sh parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight differences between </context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2010</marker>
<rawString>C. Liu, D. Dahlmeier, and H. T. Ng. 2010b. TESLA: Translation evaluation of sentences with linear-programming-based analysis. In Proceedings of WMT and MetricsMATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Low</author>
<author>H T Ng</author>
<author>W Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="15390" citStr="Low et al., 2005" startWordPosition="2461" endWordPosition="2464">ique of paraphrasing with parallel corpora (Bannard and Callison-Burch, 2005) to automatically find collocation candidates from a sentence-aligned L1-English parallel corpus. As most of the essays in our corpus are written by native Chinese speakers, we use the FBIS ChineseEnglish corpus, which consists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as �p(e1 e2) = p(e1 f)p(f e2) (1) f where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 f) and p(f e2) are estimated by maximum likelihood estimation and smoothed us</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>J. K. Low, H. T. Ng, and W. Guo. 2005. A maximum entropy approach to Chinese word segmentation. In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>B J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="7519" citStr="Madnani and Dorr, 2010" startWordPosition="1184" endWordPosition="1187">or frequent spelling errors. Even when the confusion sets are formed automatically, the similarity of words in a confusion set has been based on edit distance or phonetic similarity (Carlson et al., 2001). In contrast, we focus on words that are confusable due to their similar semantics instead of similar spelling or pronunciation. Also, we do not assume that the set of confusion sets is already given to us. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. However, there is a subtle but important difference between these tasks and collocation correction. In the former, the main criterion is whether the original phrase and the synonym/paraphrase candidate are substitutable, i.e., both form a grammatical sentence when substituted for each other in a particular context. In contrast, in collocation correction, we are primarily interested in finding candidates which are not substitutable in their English context but appear to be substit</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>N. Madnani and B. J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics, 36(3):341–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>Semeval-2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007).</booktitle>
<contexts>
<context position="7477" citStr="McCarthy and Navigli, 2007" startWordPosition="1178" endWordPosition="1181">f pre-defined confusion sets, like homophones or frequent spelling errors. Even when the confusion sets are formed automatically, the similarity of words in a confusion set has been based on edit distance or phonetic similarity (Carlson et al., 2001). In contrast, we focus on words that are confusable due to their similar semantics instead of similar spelling or pronunciation. Also, we do not assume that the set of confusion sets is already given to us. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. However, there is a subtle but important difference between these tasks and collocation correction. In the former, the main criterion is whether the original phrase and the synonym/paraphrase candidate are substitutable, i.e., both form a grammatical sentence when substituted for each other in a particular context. In contrast, in collocation correction, we are primarily interested in finding candidates which are not substitutable in thei</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy and R. Navigli. 2007. Semeval-2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Meng</author>
</authors>
<title>Erroneous collocations caused by language transfer</title>
<date>2008</date>
<booktitle>in Chinese EFL writing. US-China Foreign Language,</booktitle>
<pages>6--57</pages>
<contexts>
<context position="3705" citStr="Meng, 2008" startWordPosition="589" endWordPosition="590">o choose from all these possible translations when he wants to express a sentence like I like to watch movies and might instead produce a sentence like *I like to look movies. Although the meanings of watch and look are similar, the former is clearly the more fluent choice in this context. While these types of L1-transfer er107 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 107–117, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics rors have been known in the EFL teaching literature (Swan and Smith, 2001; Meng, 2008), research in GEC has mostly ignored this fact. We first analyze collocation errors in the NUS Corpus of Learner English (NUCLE), a fully annotated one-million-word corpus of learner English which we will make available to the community for research purposes (see Section 3 for details about the corpus). Our analysis confirms that many collocation errors can be traced to similar translations in the writer’s L1-language. Based on this result, we propose a novel approach for automatic collocation error correction. The key component in our approach generates L1-induced paraphrases which we automat</context>
<context position="8651" citStr="Meng, 2008" startWordPosition="1361" endWordPosition="1362">s which are not substitutable in their English context but appear to be substitutable in the L1-language of the writer, i.e., one forms a grammatical English sentence but the other does not. 108 Sentences 52,149 Words 1,149,100 Distinct words 27,593 Avg. sentence length (words) 22.04 Collocation errors 2,747 Avg. collocation error length (words) 1.17 Avg. correction length (words) 1.13 Table 1: Statistics of the NUS Corpus of Learner English (NUCLE) 3 Analysis of EFL collocation errors While the fact that collocation errors can be caused by L1-transfer has been ascertained by EFL researchers (Meng, 2008), we need to quantify how frequent collocation errors can be traced to these types of transfer errors in order to estimate how many errors in EFL writing we can potentially hope to correct with information about the writer’s L1- language. We base our analysis on the NUS Corpus of Learner English (NUCLE). The corpus consists of about 1,400 essays written by EFL university students on a wide range of topics, like environmental pollution or healthcare. Most of the students are native Chinese speakers. The corpus contains over one million words which are completely annotated with error tags and co</context>
</contexts>
<marker>Meng, 2008</marker>
<rawString>J. Meng. 2008. Erroneous collocations caused by language transfer in Chinese EFL writing. US-China Foreign Language, 6:57–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitton</author>
</authors>
<title>A description of a computer-usable dictionary file based on the Oxford Advanced Learner’s Dictionary of Current English.</title>
<date>1992</date>
<contexts>
<context position="11592" citStr="Mitton, 1992" startWordPosition="1840" endWordPosition="1841">s the possible corrections are not restricted to a closed set of choices and they are directly related to semantics rather than syntax. We analyzed the collocation errors and found that they can be attributed to the following sources of confusion: Spelling: We suspect that an error is caused by similar orthography if the edit distance between the erroneous phrase and its correction is less than a certain threshold. Homophones: We suspect that an error is caused by similar pronunciation if the erroneous word and its correction have the same pronunciation. We use the CuVPlus English dictionary (Mitton, 1992) to map words to their phonetic representations. Synonyms: We suspect that an error is caused by synonymy if the erroneous word and its correction are synonyms in WordNet (Fellbaum, 1998). We use WordNet 3.0. L1-transfer: We suspect that an error is caused by L1-transfer if the erroneous phrase and its correction share a common translation in a Chinese-English phrase table. The details of the phrase table construction are described in Section 4. We note that although we focus on Chinese-English translation, our method is applicable to any language pair where parallel corpora are available. As </context>
</contexts>
<marker>Mitton, 1992</marker>
<rawString>R. Mitton. 1992. A description of a computer-usable dictionary file based on the Oxford Advanced Learner’s Dictionary of Current English.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>Y S Chan</author>
</authors>
<title>Semeval-2007 task 11: English lexical sample task via english-chinese parallel text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval</booktitle>
<contexts>
<context position="4865" citStr="Ng and Chan, 2007" startWordPosition="773" endWordPosition="776">pproach generates L1-induced paraphrases which we automatically extract from an L1-English parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview </context>
</contexts>
<marker>Ng, Chan, 2007</marker>
<rawString>H. T. Ng and Y. S. Chan. 2007. Semeval-2007 task 11: English lexical sample task via english-chinese parallel text. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>J K Low</author>
</authors>
<title>Chinese part-of-speech tagging: One-at-a-time or all-at-once? word-based or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="15371" citStr="Ng and Low, 2004" startWordPosition="2457" endWordPosition="2460"> the popular technique of paraphrasing with parallel corpora (Bannard and Callison-Burch, 2005) to automatically find collocation candidates from a sentence-aligned L1-English parallel corpus. As most of the essays in our corpus are written by native Chinese speakers, we use the FBIS ChineseEnglish corpus, which consists of about 230,000 Chinese sentences (8.5 million words) from news articles, each with a single English translation. We tokenize and lowercase the English half of the corpus in the standard way. We segment the Chinese half of the corpus using the maximum entropy segmenter from (Ng and Low, 2004; Low et al., 2005). Subsequently, we automatically align the texts at the word level using the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We extract English-L1 and L1-English phrases of up to three words from the aligned texts using the widely used phrase extraction heuristic in (Koehn et al., 2003). The paraphrase probability of an English phrase e1 given an English phrase e2 is defined as �p(e1 e2) = p(e1 f)p(f e2) (1) f where f denotes a foreign phrase in the L1 language. The phrase translation probabilities p(e1 f) and p(f e2) are estimated by maximum likelihood estimat</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>H. T. Ng and J. K. Low. 2004. Chinese part-of-speech tagging: One-at-a-time or all-at-once? word-based or character-based? In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>B Wang</author>
<author>Y S Chan</author>
</authors>
<title>Exploiting parallel texts for word sense disambiguation: An empirical study.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4827" citStr="Ng et al., 2003" startWordPosition="765" endWordPosition="768">rrection. The key component in our approach generates L1-induced paraphrases which we automatically extract from an L1-English parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work</context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>H. T. Ng, B. Wang, and Y. S. Chan. 2003. Exploiting parallel texts for word sense disambiguation: An empirical study. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16910" citStr="Och, 2003" startWordPosition="2720" endWordPosition="2721">hn et al., 2003). Phrase-based SMT tries to find the highest scoring translation e given an input sentence f. The decoding process of finding the highest scoring translation is guided by a log-linear model which scores translation candidates using a set of feature functions hi, i = 1, ... , n �Aihi(e,f) . (2) Typical features include a phrase translation probability p(e|f), an inverse phrase translation probability p(f|e), a language model score p(e), and a constant phrase penalty. The optimization of the feature weights Ai, i = 1, ... , n can be done using minimum error rate training (MERT) (Och, 2003) on a development set of input sentences and their reference translations. Because of the great flexibility of the log-linear model, researchers have used the framework for other tasks outside SMT, including grammatical error correction (Brockett et al., 2006). We adopt a similar approach in this work. We modify the phrase table of the popular phrase-based SMT decoder MOSES (Koehn et al., 2007) to include collocation corrections with features derived from spelling, homophones, synonyms, and L1-induced paraphrases. • Spelling: For each English word, the phrase table contains entries consisting </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22106" citStr="Papineni et al., 2002" startWordPosition="3585" endWordPosition="3588">ntity. We remove phrase table entries where the phrase and the candidate correction are identical, thus practically forcing the system to change the identified phrase. We set the distortion limit of the decoder to zero to achieve monotone decoding. We previously observed that word order errors are virtually absent in our collocation errors. For the language model, we use a 5-gram language model trained on the English Gigaword corpus with modified Kneser-Ney smoothing. All experiments use the same language model to allow a fair comparison. We perform MERT training with the popular BLEU metric (Papineni et al., 2002) on the development set of erroneous sentences and their corrections. As the search space is restricted to changing a single phrase per sentence, training converges relatively quickly after two or three iterations. After convergence, the model can be used to automatically correct new collocation errors. 6 Results We evaluate the performance of the proposed method on our test set of 856 sentences, each with one collocation error. We conduct both an automatic and a human evaluation. In the automatic evaluation, the system’s performance is measured by computing the rank of the gold answer provide</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Generating confusion sets for context-sensitive error correction.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1952" citStr="Rozovskaya and Roth, 2010" startWordPosition="293" endWordPosition="296">ion (Golding and Roth, 1999) has traditionally focused on confusion sets with similar spelling (e.g., {dessert, desert}) or similar pronunciation (e.g., {there, their}). In other words, the words in a confusion set are deemed confusable because of orthographic or phonetic similarity. Other work in GEC has defined the confu1For simplicity, we will collectively refer to both terms as English as a foreign language (EFL) sion sets based on syntactic similarity, for example all English articles or the most frequent English prepositions form a confusion set (see for example (Tetreault et al., 2010; Rozovskaya and Roth, 2010; Gamon, 2010; Dahlmeier and Ng, 2011) among others). In contrast, we investigate in this paper a class of grammatical errors where the source of confusion is the similar semantics of the words, rather than orthography, phonetics, or syntax. In particular, we focus on collocation errors in EFL writing. The term collocation (Firth, 1957) describes a sequence of words that is conventionally used together in a particular way by native speakers and appears more often together than one would expect by chance. The correct use of collocations is a major difficulty for EFL students (Farghal and Obieda</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010. Generating confusion sets for context-sensitive error correction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Shei</author>
<author>H Pain</author>
</authors>
<title>An ESL writer’s collocational aid.</title>
<date>2000</date>
<journal>Computer Assisted Language Learning,</journal>
<volume>13</volume>
<contexts>
<context position="5867" citStr="Shei and Pain, 2000" startWordPosition="926" endWordPosition="929">rection. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight differences between collocation error correction and related NLP tasks like context-sensitive spelling error correction, synonym extraction, lexical substitution, and paraphrasing. Most work in collocation error correction has relied on dictionaries or manually created databases to generate collocation candidates (Shei and Pain, 2000; Wible et al., 2003; Futagi et al., 2008). Other work has focused on finding candidates that collocate with similar words, e.g., verbs that appear with the same noun objects form a confusion set (Liu et al., 2009; Wu et al., 2010). The work most similar to ours is probably the one presented by Chang et al. (2008), as they also use translation information to generate collocation candidates. However, they do not use automatically derived paraphrases from parallel corpora but bilingual dictionaries. Dictionaries usually have lower coverage, do not contain longer phrases or inflected forms, and d</context>
</contexts>
<marker>Shei, Pain, 2000</marker>
<rawString>C. C. Shei and H. Pain. 2000. An ESL writer’s collocational aid. Computer Assisted Language Learning, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proceedings of WMT.</booktitle>
<contexts>
<context position="5011" citStr="Snover et al., 2009" startWordPosition="795" endWordPosition="798">aditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight differences between collocation error correction and related NLP tasks like con</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swan</author>
<author>B Smith</author>
</authors>
<title>Learner English: A Teacher’s Guide to Interference and Other Problems.</title>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="3692" citStr="Swan and Smith, 2001" startWordPosition="585" endWordPosition="588">inks” in Chinese has to choose from all these possible translations when he wants to express a sentence like I like to watch movies and might instead produce a sentence like *I like to look movies. Although the meanings of watch and look are similar, the former is clearly the more fluent choice in this context. While these types of L1-transfer er107 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 107–117, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics rors have been known in the EFL teaching literature (Swan and Smith, 2001; Meng, 2008), research in GEC has mostly ignored this fact. We first analyze collocation errors in the NUS Corpus of Learner English (NUCLE), a fully annotated one-million-word corpus of learner English which we will make available to the community for research purposes (see Section 3 for details about the corpus). Our analysis confirms that many collocation errors can be traced to similar translations in the writer’s L1-language. Based on this result, we propose a novel approach for automatic collocation error correction. The key component in our approach generates L1-induced paraphrases whi</context>
</contexts>
<marker>Swan, Smith, 2001</marker>
<rawString>M. Swan and B. Smith. 2001. Learner English: A Teacher’s Guide to Interference and Other Problems. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>J Foster</author>
<author>M Chodorow</author>
</authors>
<title>Using parse features for preposition selection and error detection.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1925" citStr="Tetreault et al., 2010" startWordPosition="289" endWordPosition="292">e spelling error correction (Golding and Roth, 1999) has traditionally focused on confusion sets with similar spelling (e.g., {dessert, desert}) or similar pronunciation (e.g., {there, their}). In other words, the words in a confusion set are deemed confusable because of orthographic or phonetic similarity. Other work in GEC has defined the confu1For simplicity, we will collectively refer to both terms as English as a foreign language (EFL) sion sets based on syntactic similarity, for example all English articles or the most frequent English prepositions form a confusion set (see for example (Tetreault et al., 2010; Rozovskaya and Roth, 2010; Gamon, 2010; Dahlmeier and Ng, 2011) among others). In contrast, we investigate in this paper a class of grammatical errors where the source of confusion is the similar semantics of the words, rather than orthography, phonetics, or syntax. In particular, we focus on collocation errors in EFL writing. The term collocation (Firth, 1957) describes a sequence of words that is conventionally used together in a particular way by native speakers and appears more often together than one would expect by chance. The correct use of collocations is a major difficulty for EFL s</context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>J. Tetreault, J. Foster, and M. Chodorow. 2010. Using parse features for preposition selection and error detection. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wible</author>
<author>C H Kuo</author>
<author>N L Tsao</author>
<author>A Liu</author>
<author>H L Lin</author>
</authors>
<title>Bootstrapping in a language learning environment.</title>
<date>2003</date>
<journal>Journal of Computer-Assisted Learning,</journal>
<volume>19</volume>
<contexts>
<context position="5887" citStr="Wible et al., 2003" startWordPosition="930" endWordPosition="933">ntal setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on collocation error correction. We also highlight differences between collocation error correction and related NLP tasks like context-sensitive spelling error correction, synonym extraction, lexical substitution, and paraphrasing. Most work in collocation error correction has relied on dictionaries or manually created databases to generate collocation candidates (Shei and Pain, 2000; Wible et al., 2003; Futagi et al., 2008). Other work has focused on finding candidates that collocate with similar words, e.g., verbs that appear with the same noun objects form a confusion set (Liu et al., 2009; Wu et al., 2010). The work most similar to ours is probably the one presented by Chang et al. (2008), as they also use translation information to generate collocation candidates. However, they do not use automatically derived paraphrases from parallel corpora but bilingual dictionaries. Dictionaries usually have lower coverage, do not contain longer phrases or inflected forms, and do not provide any tr</context>
</contexts>
<marker>Wible, Kuo, Tsao, Liu, Lin, 2003</marker>
<rawString>D. Wible, C. H. Kuo, N. L. Tsao, A. Liu, and H. L. Lin. 2003. Bootstrapping in a language learning environment. Journal of Computer-Assisted Learning, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wu</author>
<author>M Zhou</author>
</authors>
<title>Synonymous collocation extraction using translation information.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7426" citStr="Wu and Zhou, 2003" startWordPosition="1171" endWordPosition="1174"> traditionally focused on a small number of pre-defined confusion sets, like homophones or frequent spelling errors. Even when the confusion sets are formed automatically, the similarity of words in a confusion set has been based on edit distance or phonetic similarity (Carlson et al., 2001). In contrast, we focus on words that are confusable due to their similar semantics instead of similar spelling or pronunciation. Also, we do not assume that the set of confusion sets is already given to us. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. However, there is a subtle but important difference between these tasks and collocation correction. In the former, the main criterion is whether the original phrase and the synonym/paraphrase candidate are substitutable, i.e., both form a grammatical sentence when substituted for each other in a particular context. In contrast, in collocation correction, we are primarily interested in fin</context>
</contexts>
<marker>Wu, Zhou, 2003</marker>
<rawString>H. Wu and M. Zhou. 2003. Synonymous collocation extraction using translation information. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Wu</author>
<author>Y C Chang</author>
<author>T Mitamura</author>
<author>J S Chang</author>
</authors>
<title>Automatic collocation suggestion in academic writing.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers.</booktitle>
<contexts>
<context position="6098" citStr="Wu et al., 2010" startWordPosition="968" endWordPosition="971">n collocation error correction. We also highlight differences between collocation error correction and related NLP tasks like context-sensitive spelling error correction, synonym extraction, lexical substitution, and paraphrasing. Most work in collocation error correction has relied on dictionaries or manually created databases to generate collocation candidates (Shei and Pain, 2000; Wible et al., 2003; Futagi et al., 2008). Other work has focused on finding candidates that collocate with similar words, e.g., verbs that appear with the same noun objects form a confusion set (Liu et al., 2009; Wu et al., 2010). The work most similar to ours is probably the one presented by Chang et al. (2008), as they also use translation information to generate collocation candidates. However, they do not use automatically derived paraphrases from parallel corpora but bilingual dictionaries. Dictionaries usually have lower coverage, do not contain longer phrases or inflected forms, and do not provide any translation probability estimates. Also, their work focuses solely on verb-noun collocations, while we target collocations of arbitrary syntactic type. Context-sensitive spelling error correction is the task of co</context>
<context position="21132" citStr="Wu et al., 2010" startWordPosition="3424" endWordPosition="3427">where A is the set of returned answers of rank k or less and score(·) is a real-valued scoring function between zero and one. 5.3 Collocation Error Experiments Automatic correction of collocation errors can conceptually be divided into two steps: i) identification of wrong collocations in the input, and ii) correction of the identified collocations. In this work, we focus on the second step and assume that the erroneous collocation has already been identified. While this might seem like a simplification, it has been the common evaluation setup in collocation error correction (see for example (Wu et al., 2010)). It also has a practical application where the user first selects a word or phrase and the system displays possible corrections. In our experiments, we use the start and end offset of the collocation error provided by the human annotator to identify the location of the collocation error. We fix the translation of the rest of the sentence to its identity. We remove phrase table entries where the phrase and the candidate correction are identical, thus practically forcing the system to change the identified phrase. We set the distortion limit of the decoder to zero to achieve monotone decoding.</context>
</contexts>
<marker>Wu, Chang, Mitamura, Chang, 2010</marker>
<rawString>J. C. Wu, Y. C. Chang, T. Mitamura, and J. S. Chang. 2010. Automatic collocation suggestion in academic writing. In Proceedings of the ACL 2010 Conference Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhong</author>
<author>H T Ng</author>
</authors>
<title>Word sense disambiguation for all words without hard labor.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="4886" citStr="Zhong and Ng, 2009" startWordPosition="777" endWordPosition="780">1-induced paraphrases which we automatically extract from an L1-English parallel corpus. Our proposed approach outperforms traditional approaches based on edit distance, homophones, and WordNet synonyms on a test set of real-world learner data in an automatic and a human evaluation. Finally, we present a detailed analysis of unsolved instances in our data set to highlight directions for future work. Our work adds to a growing body of research that leverages parallel corpora for semantic NLP tasks, for example in word sense disambiguation (Ng et al., 2003; Chan and Ng, 2005; Ng and Chan, 2007; Zhong and Ng, 2009), paraphrasing (Bannard and Callison-Burch, 2005; Liu et al., 2010a), and machine translation evaluation (Snover et al., 2009; Liu et al., 2010b). The remainder of this paper is organized as follows. The next section reviews related work. Section 3 presents our analysis of collocation errors. Section 4 describes our approach for automatic collocation error correction. The experimental setup and the results are described in Sections 5 and 6, respectively. Section 7 provides further analysis. Section 8 concludes the paper. 2 Related Work In this section, we give an overview of related work on co</context>
</contexts>
<marker>Zhong, Ng, 2009</marker>
<rawString>Z. Zhong and H. T. Ng. 2009. Word sense disambiguation for all words without hard labor. In Proceedings of IJCAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>