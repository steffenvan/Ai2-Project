<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.966323">
Variational Decoding for Statistical Machine Translation
</title>
<author confidence="0.998932">
Zhifei Li and Jason Eisner and Sanjeev Khudanpur
</author>
<affiliation confidence="0.9590005">
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
</affiliation>
<email confidence="0.988346">
zhifei.work@gmail.com, jason@cs.jhu.edu, khudanpur@jhu.edu
</email>
<sectionHeader confidence="0.994667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996747826087">
Statistical models in machine translation
exhibit spurious ambiguity. That is, the
probability of an output string is split
among many distinct derivations (e.g.,
trees or segmentations). In principle, the
goodness of a string is measured by the
total probability of its many derivations.
However, finding the best string (e.g., dur-
ing decoding) is then computationally in-
tractable. Therefore, most systems use
a simple Viterbi approximation that mea-
sures the goodness of a string using only
its most probable derivation. Instead,
we develop a variational approximation,
which considers all the derivations but still
allows tractable decoding. Our particular
variational distributions are parameterized
as n-gram models. We also analytically
show that interpolating these n-gram mod-
els for different n is similar to minimum-
risk decoding for BLEU (Tromble et al.,
2008). Experiments show that our ap-
proach improves the state of the art.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993474338709678">
Ambiguity is a central issue in natural language
processing. Many systems try to resolve ambigu-
ities in the input, for example by tagging words
with their senses or choosing a particular syntax
tree for a sentence. These systems are designed to
recover the values of interesting latent variables,
such as word senses, syntax trees, or translations,
given the observed input.
However, some systems resolve too many ambi-
guities. They recover additional latent variables—
so-called nuisance variables—that are not of in-
terest to the user.1 For example, though machine
translation (MT) seeks to output a string, typical
MT systems (Koehn et al., 2003; Chiang, 2007)
1These nuisance variables may be annotated in training
data, but it is more common for them to be latent even there,
i.e., there is no supervision as to their “correct” values.
will also recover a particular derivation of that out-
put string, which specifies a tree or segmentation
and its alignment to the input string. The compet-
ing derivations of a string are interchangeable for
a user who is only interested in the string itself, so
a system that unnecessarily tries to choose among
them is said to be resolving spurious ambiguity.
Of course, the nuisance variables are important
components of the system’s model. For example,
the translation process from one language to an-
other language may follow some hidden tree trans-
formation process, in a recursive fashion. Many
features of the model will crucially make reference
to such hidden structures or alignments.
However, collapsing the resulting spurious
ambiguity—i.e., marginalizing out the nuisance
variables—causes significant computational dif-
ficulties. The goodness of a possible MT out-
put string should be measured by summing up
the probabilities of all its derivations. Unfortu-
nately, finding the best string is then computation-
ally intractable (Sima’an, 1996; Casacuberta and
Higuera, 2000).2 Therefore, most systems merely
identify the single most probable derivation and
report the corresponding string. This corresponds
to a Viterbi approximation that measures the good-
ness of an output string using only its most proba-
ble derivation, ignoring all the others.
In this paper, we propose a variational method
that considers all the derivations but still allows
tractable decoding. Given an input string, the orig-
inal system produces a probability distribution p
over possible output strings and their derivations
(nuisance variables). Our method constructs a sec-
ond distribution q E 2 that approximates p as well
as possible, and then finds the best string accord-
ing to q. The last step is tractable because each
q E 2 is defined (unlike p) without reference to
nuisance variables. Notice that q here does not ap-
proximate the entire translation process, but only
2May and Knight (2006) have successfully used tree-
automaton determinization to exactly marginalize out some
of the nuisance variables, obtaining a distribution over parsed
translations. However, they do not marginalize over these
parse trees to obtain a distribution over translation strings.
</bodyText>
<page confidence="0.984162">
593
</page>
<note confidence="0.9996115">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593–601,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999770904761905">
the distribution over output strings for a particular
input. This is why it can be a fairly good approxi-
mation even without using the nuisance variables.
In practice, we approximate with several dif-
ferent variational families 2, corresponding to n-
gram (Markov) models of different orders. We
geometrically interpolate the resulting approxima-
tions q with one another (and with the original dis-
tribution p), justifying this interpolation as similar
to the minimum-risk decoding for BLEU proposed
by Tromble et al. (2008). Experiments show that
our approach improves the state of the art.
The methods presented in this paper should be
applicable to collapsing spurious ambiguity for
other tasks as well. Such tasks include data-
oriented parsing (DOP), applications of Hidden
Markov Models (HMMs) and mixture models, and
other models with latent variables. Indeed, our
methods were inspired by past work on varia-
tional decoding for DOP (Goodman, 1996) and for
latent-variable parsing (Matsuzaki et al., 2005).
</bodyText>
<sectionHeader confidence="0.992557" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.805897">
2.1 Terminology
</subsectionHeader>
<bodyText confidence="0.999980947368421">
In MT, spurious ambiguity occurs both in regular
phrase-based systems (e.g., Koehn et al. (2003)),
where different segmentations lead to the same
translation string (Figure 1), and in syntax-based
systems (e.g., Chiang (2007)), where different
derivation trees yield the same string (Figure 2).
In the Hiero system (Chiang, 2007) we are us-
ing, each string corresponds to about 115 distinct
derivations on average.
We use x to denote the input string, and D(x) to
consider the set of derivations then considered by
the system. Each derivation d E D(x) yields some
translation string y = Y(d) in the target language.
We write D(x, y) def = {d E D(x) : Y(d) = y} to
denote the set of all derivations that yield y. Thus,
the set of translations permitted by the model is
T(y) def= {y : D(x, y) =� 0} (or equivalently,
T(y) def = {Y(d) : d E D(x)}). We write y* for
the translation string that is actually output.
</bodyText>
<subsectionHeader confidence="0.999467">
2.2 Maximum A Posterior (MAP) Decoding
</subsectionHeader>
<bodyText confidence="0.999718333333333">
For a given input sentence x, a decoding method
identifies a particular “best” output string y*. The
maximum a posteriori (MAP) decision rule is
</bodyText>
<equation confidence="0.983069">
y* = argmax p(y  |x) (1)
yET(x)
</equation>
<figureCaption confidence="0.9953465">
Figure 1: Segmentation ambiguity in phrase-based MT: two
different segmentations lead to the same translation string.
</figureCaption>
<equation confidence="0.848556">
S-&gt;(! &amp;quot;, machine) S-&gt;(#$, translation) S-&gt;(%&amp;, software)
S-&gt;(S0 #$ S1, S0 translation S1)
S-&gt;(! &amp;quot;, machine) S-&gt;(%&amp;, software)
#$
</equation>
<figureCaption confidence="0.9959125">
Figure 2: Tree ambiguity in syntax-based MT: two derivation
trees yield the same translation string.
</figureCaption>
<bodyText confidence="0.9259202">
(An alternative decision rule, minimum Bayes
risk (MBR), will be discussed in Section 4.)
To obtain p(y  |x) above, we need to marginal-
ize over a nuisance variable, the derivation of y.
Therefore, the MAP decision rule becomes
</bodyText>
<equation confidence="0.997729666666667">
1:
y* = argmax p(y, d  |x) (2)
yET(x) dED(x,y)
</equation>
<bodyText confidence="0.9965935">
where p(y, d  |x) is typically derived from a log-
linear model as follows,
</bodyText>
<equation confidence="0.9975435">
e7&apos;3(x,y,d) e7&apos;3(x,y,d)
p(y, d  |x) = Z(x) =Ey,d e-Y&apos;3(x,y,d) (3)
</equation>
<bodyText confidence="0.989299833333333">
where -y is a scaling factor to adjust the sharp-
ness of the distribution, the score s(x, y, d) is a
learned linear combination of features of the triple
(x, y, d), and Z(x) is a normalization constant.
Note that p(y, d  |x) = 0 if y =� Y(d). Our deriva-
tion set D(x) is encoded in polynomial space, us-
ing a hypergraph or lattice.3 However, both |D(x)|
and |T(x) |may be exponential in |x|. Since the
marginalization needs to be carried out for each
member of T(x), the decoding problem of (2)
turns out to be NP-hard,4 as shown by Sima’an
(1996) for a similar problem.
</bodyText>
<footnote confidence="0.879741666666667">
3A hypergraph is analogous to a parse forest (Huang and
Chiang, 2007). (A finite-state lattice is a special case.) It can
be used to encode exponentially many hypotheses generated
by a phrase-based MT system (e.g., Koehn et al. (2003)) or a
syntax-based MT system (e.g., Chiang (2007)).
4Note that the marginalization for a particular y would be
tractable; it is used at training time in certain training objec-
tive functions, e.g., maximizing the conditional likelihood of
a reference translation (Blunsom et al., 2008).
</footnote>
<equation confidence="0.8669585">
! &amp;quot; # $ % &amp;
machine translation software
! &amp;quot; # $ % &amp;
machine translation software
S-&gt;(S0 S1, S0 S1)
S-&gt;(S0 S1, S0 S1)
</equation>
<page confidence="0.996328">
594
</page>
<subsectionHeader confidence="0.995778">
2.3 Viterbi Approximation
</subsectionHeader>
<bodyText confidence="0.998910333333333">
To approximate the intractable decoding problem
of (2), most MT systems (Koehn et al., 2003; Chi-
ang, 2007) use a simple Viterbi approximation,
</bodyText>
<equation confidence="0.99953725">
pViterbi(y  |x) (4)
max p(y, d  |x) (5)
dED(x,y)
!p(y, d  |x) (6)
</equation>
<bodyText confidence="0.997662166666667">
Clearly, (5) replaces the sum in (2) with a max.
In other words, it approximates the probability of
a translation string by the probability of its most-
probable derivation. (5) is found quickly via (6).
The Viterbi approximation is simple and tractable,
but it ignores most derivations.
</bodyText>
<subsectionHeader confidence="0.992622">
2.4 N-best Approximation (or Crunching)
</subsectionHeader>
<bodyText confidence="0.9993115">
Another popular approximation enumerates the N
best derivations in D(x), a set that we call ND(x).
Modifying (2) to sum over only these derivations
is called crunching by May and Knight (2006):
</bodyText>
<equation confidence="0.9975164">
y* = argmax pcrunch(y  |x) (7)
yET(x)
= argmax
yET(x) dED(x,y)nND(x)
X p(y, d  |x)
</equation>
<sectionHeader confidence="0.992723" genericHeader="method">
3 Variational Approximate Decoding
</sectionHeader>
<bodyText confidence="0.9998605">
The Viterbi and crunching methods above approx-
imate the intractable decoding of (2) by ignor-
ing most of the derivations. In this section, we
will present a novel variational approximation,
which considers all the derivations but still allows
tractable decoding.
</bodyText>
<subsectionHeader confidence="0.999221">
3.1 Approximate Inference
</subsectionHeader>
<bodyText confidence="0.999938210526316">
There are several popular approaches to approxi-
mate inference when exact inference is intractable
(Bishop, 2006). Stochastic techniques such as
Markov Chain Monte Carlo are exact in the limit
of infinite runtime, but tend to be too slow for large
problems. By contrast, deterministic variational
methods (Jordan et al., 1999), including message-
passing (Minka, 2005), are inexact but scale up
well. They approximate the original intractable
distribution with one that factorizes better or has
a specific parametric form (e.g., Gaussian).
In our work, we use a fast variational method.
Variational methods generally work as follows.
When exact inference under a complex model p
is intractable, one can approximate the posterior
p(y  |x) by a tractable model q(y), where q E 2 is
chosen to minimize some information loss such as
the KL divergence KL(p II q). The simpler model
q can then act as a surrogate for p during inference.
</bodyText>
<subsectionHeader confidence="0.999623">
3.2 Variational Decoding for MT
</subsectionHeader>
<bodyText confidence="0.9997505">
For each input sentence x, we assume that a base-
line MT system generates a hypergraph HG(x)
that compactly encodes the derivation set D(x)
along with a score for each d E D(x),5 which we
interpret as p(y, d  |x) (or proportional to it). For
any single y E T(x), it would be tractable using
HG(x) to compute p(y  |x) = Pd p(y, d  |x).
However, as mentioned, it is intractable to find
argmaxy p(y  |x) as required by the MAP de-
coding (2), so we seek an approximate distribution
q(y) ^ p(y  |x).6
For a fixed x, we seek a distribution q E 2 that
minimizes the KL divergence from p to q (both
regarded as distributions over y):7
</bodyText>
<equation confidence="0.9995105">
q* = argmin KL(p II q) (8)
qEQ
X=argmin (p log p − p log q) (9)
qEQ yET(x)
X=argmax
qEQ yET(x) p log q (10)
</equation>
<bodyText confidence="0.999598916666667">
So far, in order to approximate the intractable
optimization problem (2), we have defined an-
other optimization problem (10). If computing
p(y  |x) during decoding is computationally in-
tractable, one might wonder if the optimization
problem (10) is any simpler. We will show this is
the case. The trick is to parameterize q as a fac-
torized distribution such that the estimation of q*
and decoding using q* are both tractable through
efficient dynamic programs. In the next three sub-
sections, we will discuss the parameterization, es-
timation, and decoding, respectively.
</bodyText>
<subsectionHeader confidence="0.982611">
3.2.1 Parameterization of q
</subsectionHeader>
<bodyText confidence="0.99664675">
In (10), 2 is a family of distributions. If we se-
lect a large family 2, we can allow more com-
plex distributions, so that q* will better approxi-
mate p. If we select a smaller family 2, we can
</bodyText>
<footnote confidence="0.87286825">
5The baseline system may return a pruned hypergraph,
which has the effect of pruning D(x) and T(x) as well.
6Following the convention in describing variational infer-
ence, we write q(y) instead of q(y x), even though q(y)
always depends on x implicitly.
7To avoid clutter, we denote p(y x) by p, and q(y) by q.
We drop p log p from (9) because it is constant with respect
to q. We then flip the sign and change argmin to argmax.
</footnote>
<equation confidence="0.998934666666667">
y* = argmax
yET(x)
= argmax
yET(x)
= Y argmax
dED(x)
</equation>
<page confidence="0.975335">
595
</page>
<bodyText confidence="0.998665733333333">
guarantee that q* will have a simple form with
many conditional independencies, so that q*(y)
and y* = argmaxy q*(y) are easier to compute.
Since each q(y) is a distribution over output
strings, a natural choice for 2 is the family of
n-gram models. To obtain a small KL diver-
gence (8), we should make n as large as possible.
In fact, q* —* p as n —* oc. Of course, this last
point also means that our computation becomes
intractable as n —* oc.8 However, if p(y  |x) is de-
fined by a hypergraph HG(x) whose structure ex-
plicitly incorporates an m-gram language model,
both training and decoding will be efficient when
m &gt; n. We will give algorithms for this case that
are linear in the size of HG(x).9
</bodyText>
<equation confidence="0.883607666666667">
Formally, each q E 2 takes the form
q(y) = 11 q(r(w)  |h(w))&apos;(y) (11)
wEW
</equation>
<bodyText confidence="0.998837170212766">
where W is a set of n-gram types. Each w E W is
an n-gram, which occurs cw(y) times in the string
y, and w may be divided into an (n − 1)-gram
prefix h(w) (the history) and a 1-gram suffix r(w)
(the rightmost or current word).
8Blunsom et al. (2008) effectively do take n = oo, by
maintaining the whole translation string in the dynamic pro-
gramming state. They alleviate the computation cost some-
how by using aggressive beam pruning, which might be sen-
sible for their relatively small task (e.g., input sentences of
&lt; 10 words). But, we are interested in improving the perfor-
mance for a large-scale system, and thus their method is not
a viable solution. Moreover, we observe in our experiments
that using a larger n does not improve much over n = 2.
9A reviewer asks about the interaction with backed-off
language models. The issue is that the most compact finite-
state representations of these (Allauzen et al., 2003), which
exploit backoff structure, are not purely m-gram for any
m. They yield more compact hypergraphs (Li and Khudan-
pur, 2008), but unfortunately those hypergraphs might not be
treatable by Fig. 4—since where they back off to less than an
n-gram, a is not informative enough for line 8 to find w.
We sketch a method that works for any language model
given by a weighted FSA, L. The variational family Q can
be specified by any deterministic weighted FSA, Q, with
weights parameterized by 0. One seeks 0 to minimize (8).
Intersect HG(x) with an “unweighted” version of Q in
which all arcs have weight 1, so that Q does not prefer
any string to another. By lifting weights into an expectation
semiring (Eisner, 2002), it is then possible to obtain expected
transition counts in Q (where the expectation is taken under
p), or other sufficient statistics needed to estimate 0.
This takes only time O(|HG(x)|) when L is a left-to-right
refinement of Q (meaning that any two prefix strings that
reach the same state in L also reach the same state in Q),
for then intersecting L or HG(x) with Q does not split any
states. That is the case when L and Q are respectively pure
m-gram and n-gram models with m &gt; n, as assumed in (12)
and Figure 4. It is also the case when Q is a pure n-gram
model and L is constructed not to back off beyond n-grams;
or when the variational family Q is defined by deliberately
taking the FSA Q to have the same topology as L.
The parameters that specify a particular q E 2
are the (normalized) conditional probability distri-
butions q(r(w)  |h(w)). We will now see how to
estimate these parameters to approximate p(·  |x)
for a given x at test time.
</bodyText>
<subsubsectionHeader confidence="0.467275">
3.2.2 Estimation of q*
</subsubsectionHeader>
<bodyText confidence="0.9493525">
Note that the objective function (8)–(10) asks us to
approximate p as closely as possible, without any
further smoothing. (It is assumed that p is already
smoothed appropriately, having been constructed
from channel and language models that were esti-
mated with smoothing from finite training data.)
In fact, if p were the empirical distribution over
strings in a training corpus, then q* of (10) is just
the maximum-likelihood n-gram model—whose
parameters, trivially, are just unsmoothed ratios of
the n-gram and (n−1)-gram counts in the training
corpus. That is, q*(r(w)  |h(w)) = �(w)
�(h(w)).
Our actual job is exactly the same, except that p
is specified not by a corpus but by the hypergraph
HG(x). The only change is that the n-gram counts
c(w) are no longer integers from a corpus, but are
expected counts under p:10
</bodyText>
<equation confidence="0.92473625">
q*(r(w)  |h(w)) =
Ey cw(y)p(y  |x) Ey,d cw(y)p(y, d  |x)
E Ey,d ch(w)(y)p(y, d  |x)
y ch(w)(y)p(y  |x)
</equation>
<bodyText confidence="0.957990772727273">
Now, the question is how to efficiently compute
(12) from the hypergraph HG(x). To develop the
intuition, we first present a brute-force algorithm
in Figure 3. The algorithm is brute-force since
it first needs to unpack the hypergraph and enu-
merate each possible derivation in the hypergraph
(see line 1), which is computationally intractable.
The algorithm then enumerates each n-gram and
(n − 1)-gram in y and accumulates its soft count
into the expected count, and finally obtains the pa-
rameters of q* by taking count ratios via (12).
Figure 4 shows an efficient version that exploits
the packed-forest structure of HG(x) in com-
puting the expected counts. Specifically, it first
runs the inside-outside procedure, which annotates
each node (say v) with both an inside weight Q(v)
and an outside weight α(v). The inside-outside
also finds Z(x), the total weight of all derivations.
With these weights, the algorithm then explores
the hypergraph once more to collect the expected
10One can prove (12) via Lagrange multipliers, with q*(· |
h) constrained to be a normalized distribution for each h.
</bodyText>
<equation confidence="0.719808">
c(w) (12)
c(h(w))
</equation>
<page confidence="0.637971">
596
</page>
<figure confidence="0.705799">
Brute-Force-MLE(HG(x))
1 for y, d in HG(x) D each derivation
2 for w in y D each n-gram type
3 D accumulate soft count
4 c(w) + = cw(y) · p(y, d  |x)
5 c(h(w)) + = cw(y) · p(y, d  |x)
6 q∗ ← MLE using formula (12)
7 return q∗
</figure>
<figureCaption confidence="0.965423">
Figure 3: Brute-force estimation of q*.
</figureCaption>
<figure confidence="0.848760615384615">
Dynamic-Programming-MLE(HG(x))
1 run inside-outside on the hypergraph HG(x)
2 for v in HG(x) D each node
3 for e ∈ B(v) D each incoming hyperedge
4 ce ← pe · α(v)/Z(x)
5 for u ∈ T(e) D each antecedent node
6 ce ← ce · O(u)
7 D accumulate soft count
8 for w in e D each n-gram type
9 c(w) + = cw(e) · ce
10 c(h(w)) + = cw(e) · ce
11 q∗ ← MLE using formula (12)
12 return q∗
</figure>
<figureCaption confidence="0.9040504">
Figure 4: Dynamic programming estimation of q*. B(v) rep-
resents the set of incoming hyperedges of node v; pe repre-
sents the weight of the hyperedge e itself; T(e) represents
the set of antecedent nodes of hyperedge e. Please refer to
the text for the meanings of other notations.
</figureCaption>
<bodyText confidence="0.999920785714286">
counts. For each hyperedge (say e), it first gets the
posterior weight ce (see lines 4-6). Then, for each
n-gram type (say w), it increments the expected
count by cw(e) · ce, where cw(e) is the number of
copies of n-gram w that are added by hyperedge
e, i.e., that appear in the yield of e but not in the
yields of any of its antecedents u ∈ T(e).
While there may be exponentially many deriva-
tions, the hypergraph data structure represents
them in polynomial space by allowing multiple
derivations to share subderivations. The algorithm
of Figure 4 may be run over this packed forest
in time O(|HG(x)|) where |HG(x) |is the hyper-
graph’s size (number of hyperedges).
</bodyText>
<subsectionHeader confidence="0.703543">
3.2.3 Decoding with q∗
</subsectionHeader>
<bodyText confidence="0.999555666666667">
When translating x at runtime, the q∗ constructed
from HG(x) will be used as a surrogate for p dur-
ing decoding. We want its most probable string:
</bodyText>
<equation confidence="0.9238345">
y∗ = argmax q∗(y) (13)
y
</equation>
<bodyText confidence="0.998667631578947">
Since q∗ is an n-gram model, finding y∗ is equiv-
alent to a shortest-path problem in a certain graph
whose edges correspond to n-grams (weighted
with negative log-probabilities) and whose ver-
tices correspond to (n − 1)-grams.
However, because q∗ only approximates p, y∗ of
(13) may be locally appropriate but globally inade-
quate as a translation of x. Observe, e.g., that an n-
gram model q∗(y) will tend to favor short strings
y, regardless of the length of x. Suppose x = le
chat chasse la souris (“the cat chases the mouse”)
and q∗ is a bigram approximation to p(y  |x). Pre-
sumably q∗(the  |START), q∗(mouse  |the), and
q∗(END  |mouse) are all large in HG(x). So the
most probable string y∗ under q∗ may be simply
“the mouse,” which is short and has a high proba-
bility but fails to cover x.
Therefore, a better way of using q∗ is to restrict
the search space to the original hypergraph, i.e.:
</bodyText>
<equation confidence="0.9983915">
y∗ = argmax q∗(y) (14)
y∈T(x)
</equation>
<bodyText confidence="0.99981805882353">
This ensures that y∗ is a valid string in the origi-
nal hypergraph HG(x), which will tend to rule out
inadequate translations like “the mouse.”
If our sole objective is to get a good approxi-
mation to p(y  |x), we should just use a single
n-gram model q∗ whose order n is as large as pos-
sible, given computational constraints. This may
be regarded as favoring n-grams that are likely to
appear in the reference translation (because they
are likely in the derivation forest). However, in or-
der to score well on the BLEU metric for MT eval-
uation (Papineni et al., 2001), which gives partial
credit, we would also like to favor lower-order n-
grams that are likely to appear in the reference,
even if this means picking some less-likely high-
order n-grams. For this reason, it is useful to in-
terpolate different orders of variational models,
</bodyText>
<equation confidence="0.994848333333333">
1:
y∗ = argmax On · log q∗n(y) (15)
y∈T(x) n
</equation>
<bodyText confidence="0.999983833333333">
where n may include the value of zero, in which
case log q∗�(y) def = |y|, corresponding to a conven-
tional word penalty feature. In the geometric inter-
polation above, the weight On controls the relative
veto power of the n-gram approximation and can
be tuned using MERT (Och, 2003) or a minimum
risk procedure (Smith and Eisner, 2006).
Lastly, note that Viterbi and variational approx-
imation are different ways to approximate the ex-
act probability p(y  |x), and each of them has
pros and cons. Specifically, Viterbi approxima-
tion uses the correct probability of one complete
</bodyText>
<page confidence="0.994542">
597
</page>
<bodyText confidence="0.999977">
derivation, but ignores most of the derivations in
the hypergraph. In comparison, the variational ap-
proximation considers all the derivations in the hy-
pergraph, but uses only aggregate statistics of frag-
ments of derivations. Therefore, it is desirable to
interpolate further with the Viterbi approximation
when choosing the final translation output:11
</bodyText>
<equation confidence="0.998951">
X
y∗ = argmax θn · log q∗n(y)
y∈T(x) n
+ θv · log pViterbi(y  |x) (16)
</equation>
<bodyText confidence="0.9896334375">
where the first term corresponds to the interpolated
variational decoding of (15) and the second term
corresponds to the Viterbi decoding of (4).12 As-
suming θv &gt; 0, the second term penalizes transla-
tions with no good derivation in the hypergraph.13
For n ≤ m, any of these decoders (14)–
(16) may be implemented efficiently by using the
n-gram variational approximations q∗ to rescore
HG(x)—preserving its hypergraph topology, but
modifying the hyperedge weights.14 While the
original weights gave derivation d a score of
logp(d  |x), the weights as modified for (16)
will give d a score of Pn θn · log q∗n(Y(d)) + θv ·
log p(d  |x). We then find the best-scoring deriva-
tion and output its target yield; that is, we find
argmaxy∈T(x) via Y(argmaxd∈D(x)).
</bodyText>
<sectionHeader confidence="0.975944" genericHeader="method">
4 Variational vs. Min-Risk Decoding
</sectionHeader>
<bodyText confidence="0.999866166666667">
In place of the MAP decoding, another commonly
used decision rule is minimum Bayes risk (MBR):
where l(y, y0) represents the loss of y if the true
answer is y0, and the risk of y is its expected
loss.15 Statistical decision theory shows MBR is
optimal if p(y0  |x) is the true distribution, while
in practice p(y0  |x) is given by a model at hand.
We now observe that our variational decoding
resembles the MBR decoding of Tromble et al.
(2008). They use the following loss function, of
which a linear approximation to BLEU (Papineni
et al., 2001) is a special case,
</bodyText>
<equation confidence="0.9949785">
l(y, y0) = −(θ0|y |+ X θwcw(y)δw(y0)) (18)
w∈N
</equation>
<bodyText confidence="0.998819333333333">
where w is an n-gram type, N is a set of n-gram
types with n ∈ [1, 4], cw(y) is the number of oc-
currence of the n-gram w in y, and δw(y0) is an
indicator function to check if y0 contains at least
one occurrence of w. With the above loss func-
tion, Tromble et al. (2008) derive the MBR rule16
</bodyText>
<equation confidence="0.999174666666667">
y∗ = argmax (θ0|y |+ X θwcw(y)g(w  |x))
y w∈N
(19)
</equation>
<bodyText confidence="0.998766">
where g(w  |x) is a specialized “posterior” proba-
bility of the n-gram w, and is defined as
</bodyText>
<equation confidence="0.9733385">
g(w  |x) = X δw(y0)p(y0  |x) (20)
y&apos;
</equation>
<bodyText confidence="0.99949375">
Now, let us divide N, which contains n-gram
types of different n, into several subsets Wn, each
of which contains only the n-grams with a given
length n. We can now rewrite (19) as follows,
</bodyText>
<equation confidence="0.997119">
y∗ = argmin R(y) = argmin X l(y, y0)p(y0  |x) y∗ = argmax X
y y y&apos; y θn · gn(y  |x) (21)
n
(17)
</equation>
<bodyText confidence="0.992688368421053">
11It would also be possible to interpolate with the N-best
approximations (see Section 2.4), with some complications.
12Zens and Ney (2006) use a similar decision rule as here
and they also use posterior n-gram probabilities as feature
functions, but their model estimation and decoding are over
an N-best, which is trivial in terms of computation.
13Already at (14), we explicitly ruled out translations y
having no derivation at all in the hypergraph. However,
suppose the hypergraph were very large (thanks to a large
or smoothed translation model and weak pruning). Then
(14)’s heuristic would fail to eliminate bad translations (“the
mouse”), since nearly every string y E E* would be derived
as a translation with at least a tiny probability. The “soft” ver-
sion (16) solves this problem, since unlike the “hard” (14), it
penalizes translations that appear only weakly in the hyper-
graph. As an extreme case, translations not in the hypergraph
at all are infinitely penalized (log pViterbi(y) = log 0 =
−oo), making it natural for the decoder not to consider them,
i.e., to do only argmaxyET(a) rather than argmaxyEE*.
</bodyText>
<footnote confidence="0.805071333333333">
14One might also want to use the q* nor smoothed versions
of them to rescore additional hypotheses, e.g., hypotheses
proposed by other systems or by system combination.
</footnote>
<equation confidence="0.9141056">
by assuming θw = θ|w |and,
(
|y |if n = 0
P (22)
w∈Wn g(w  |x)cw(y) if n &gt; 0
</equation>
<bodyText confidence="0.9997615">
Clearly, their rule (21) has a quite similar form
to our rule (15), and we can relate (20) to (12) and
(22) to (11). This justifies the use of interpolation
in Section 3.2.3. However, there are several im-
portant differences. First, the n-gram “posterior”
of (20) is very expensive to compute. In fact, it re-
quires an intersection between each n-gram in the
lattice and the lattice itself, as is done by Tromble
</bodyText>
<footnote confidence="0.873466166666667">
15The MBR becomes the MAP decision rule of (1) if a so-
called zero-one loss function is used: l(y, y&apos;) = 0 if y = y&apos;;
otherwise l(y, y&apos;) = 1.
16Note that Tromble et al. (2008) only consider MBR for a
lattice without hidden structures, though their method can be
in principle applied in a hypergraph with spurious ambiguity.
</footnote>
<equation confidence="0.701757">
gn(y  |x)=
</equation>
<page confidence="0.988214">
598
</page>
<bodyText confidence="0.999975588235294">
et al. (2008). In comparison, the optimal n-gram
probabilities of (12) can be computed using the
inside-outside algorithm, once and for all. Also,
g(w  |x) of (20) is not normalized over the history
of w, while q*(r(w)  |h(w)) of (12) is. Lastly, the
definition of the n-gram model is different. While
the model (11) is a proper probabilistic model, the
function of (22) is simply an approximation of the
average n-gram precisions of y.
A connection between variational decoding and
minimum-risk decoding has been noted before
(e.g., Matsuzaki et al. (2005)), but the derivation
above makes the connection formal.
DeNero et al. (2009) concurrently developed
an alternate to MBR, called consensus decoding,
which is similar to ours in practice although moti-
vated quite differently.
</bodyText>
<sectionHeader confidence="0.992318" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999015666666667">
We report results using an open source MT toolkit,
called Joshua (Li et al., 2009), which implements
Hiero (Chiang, 2007).
</bodyText>
<subsectionHeader confidence="0.988349">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.942274076923077">
We work on a Chinese to English translation task.
Our translation model was trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora
distributed by LDC for the NIST MT evalua-
tion using a sampling method based on the n-
gram matches between training and test sets in
the foreign side. We also used a 5-gram lan-
guage model with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1998), trained on a data
set consisting of a 130M words in English Giga-
word (LDC2007T07) and the English side of the
parallel corpora. We use GIZA++ (Och and Ney,
2000), a suffix-array (Lopez, 2007), SRILM (Stol-
cke, 2002), and risk-based deterministic annealing
(Smith and Eisner, 2006)17 to obtain word align-
ments, translation models, language models, and
the optimal weights for combining these models,
respectively. We use standard beam-pruning and
cube-pruning parameter settings, following Chi-
ang (2007), when generating the hypergraphs.
The NIST MT’03 set is used to tune model
weights (e.g. those of (16)) and the scaling factor
17We have also experimented with MERT (Och, 2003), and
found that the deterministic annealing gave results that were
more consistent across runs and often better.
</bodyText>
<table confidence="0.999942">
Decoding scheme MT’04 MT’05
Viterbi 35.4 32.6
MBR (K=1000) 35.8 32.7
Crunching (N=10000) 35.7 32.8
Crunching+MBR (N=10000) 35.8 32.7
Variational (1to4gram+wp+vt) 36.6 33.5
</table>
<tableCaption confidence="0.866992727272727">
Table 1: BLEU scores for Viterbi, Crunching, MBR, and vari-
ational decoding. All the systems improve significantly over
the Viterbi baseline (paired permutation test, p &lt; 0.05). In
each column, we boldface the best result as well as all results
that are statistically indistinguishable from it. In MBR, K is
the number of unique strings. For Crunching and Crunch-
ing+MBR, N represents the number of derivations. On av-
erage, each string has about 115 distinct derivations. The
variational method “1to4gram+wp+vt” is our full interpola-
tion (16) of four variational n-gram models (“1to4gram”), the
Viterbi baseline (“vt”), and a word penalty feature (“wp”).
</tableCaption>
<bodyText confidence="0.999626">
-y of (3),18 and MT’04 and MT’05 are blind test-
sets. We will report results for lowercase BLEU-4,
using the shortest reference translation in comput-
ing brevity penalty.
</bodyText>
<subsectionHeader confidence="0.999534">
5.2 Main Results
</subsectionHeader>
<bodyText confidence="0.999971692307692">
Table 1 presents the BLEU scores under Viterbi,
crunching, MBR, and variational decoding. Both
crunching and MBR show slight significant im-
provements over the Viterbi baseline; variational
decoding gives a substantial improvement.
The difference between MBR and Crunch-
ing+MBR lies in how we approximate the distri-
bution p(y&apos;  |x) in (17).19 For MBR, we take
p(y&apos;  |x) to be proportional to pViterbi(y&apos;  |x) if y&apos;
is among the K best distinct strings on that mea-
sure, and 0 otherwise. For Crunching+MBR, we
take p(y&apos;  |x) to be proportional to pcrunch(y&apos;  |x),
which is based on the N best derivations.
</bodyText>
<subsectionHeader confidence="0.994276">
5.3 Results of Different Variational Decoding
</subsectionHeader>
<bodyText confidence="0.999973363636364">
Table 2 presents the BLEU results under different
ways in using the variational models, as discussed
in Section 3.2.3. As shown in Table 2a, decod-
ing with a single variational n-gram model (VM)
as per (14) improves the Viterbi baseline (except
the case with a unigram VM), though often not
statistically significant. Moreover, a bigram (i.e.,
“2gram”) achieves the best BLEU scores among
the four different orders of VMs.
The interpolation between a VM and a word
penalty feature (“wp”) improves over the unigram
</bodyText>
<footnote confidence="0.9675795">
18We found the BLEU scores are not very sensitive to &apos;y,
contrasting to the observations by Tromble et al. (2008).
19We also restrict T(x) to {y : p(y  |x) &gt; 01, using the
same approximation for p(y  |x) as we did for p(y&apos;  |x).
</footnote>
<page confidence="0.993566">
599
</page>
<figure confidence="0.458025">
(a) decoding with a single variational model
</figure>
<table confidence="0.862715782608696">
Decoding scheme MT’04 MT’05
Viterbi 35.4 32.6
1gram 25.9 24.5
2gram 36.1 33.4
3gram 36.0∗ 33.1
4gram 35.8∗ 32.9
(b) interpolation between a single variational
model and a word penalty feature
1gram+wp 29.7 27.7
2gram+wp 35.5 32.6
3gram+wp 36.1∗ 33.1
4gram+wp 35.7∗ 32.8∗
(c) interpolation of a single variational model, the
Viterbi model, and a word penalty feature
1gram+wp+vt 35.6∗ 32.8∗
2gram+wp+vt 36.5∗ 33.5∗
3gram+wp+vt 35.8∗ 32.9∗
4gram+wp+vt 35.6∗ 32.8∗
(d) interpolation of several n-gram VMs, the
Viterbi model, and a word penalty feature
1to2gram+wp+vt 36.6∗ 33.6∗
1to3gram+wp+vt 36.6∗ 33.5∗
1to4gram+wp+vt 36.6∗ 33.5∗
</table>
<tableCaption confidence="0.603902444444444">
Table 2: BLEU scores under different variational decoders
discussed in Section 3.2.3. A star * indicates a result that is
significantly better than Viterbi decoding (paired permutation
test, p &lt; 0.05). We boldface the best system and all systems
that are not significantly worse than it. The brevity penalty
BP in BLEU is always 1, meaning that on average y* is no
shorter than the reference translation, except for the “1gram”
systems in (a), which suffer from brevity penalties of 0.826
and 0.831.
</tableCaption>
<bodyText confidence="0.999751777777778">
VM dramatically, but does not improve higher-
order VMs (Table 2b). Adding the Viterbi fea-
ture (“vt”) into the interpolation further improves
the lower-order models (Table 2c), and all the im-
provements over the Viterbi baseline become sta-
tistically significant. At last, interpolation of sev-
eral variational models does not yield much fur-
ther improvement over the best previous model,
but makes the results more stable (Table 2d).
</bodyText>
<subsectionHeader confidence="0.952522">
5.4 KL Divergence of Approximate Models
</subsectionHeader>
<bodyText confidence="0.848929333333333">
While the BLEU scores reported show the prac-
tical utility of the variational models, it is also
interesting to measure how well each individual
variational model q(y) approximates the distribu-
tion p(y  |x). Ideally, the quality of approxima-
tion should be measured by the KL divergence
KL(p 11 q)def = H(p, q) − H(p), where the cross-
def
entropy H(p, q) = − Ey p(y  |x) log q(y), and
</bodyText>
<table confidence="0.998764">
Measure H(p, ·) Hd(p) H(p)
bits/word q∗1 q∗2 q∗3 q∗4 �
MT’04 2.33 1.68 1.57 1.53 1.36 1.03
MT’05 2.31 1.69 1.58 1.54 1.37 1.04
</table>
<tableCaption confidence="0.981531666666667">
Table 3: Cross-entropies H(p, q) achieved by various ap-
proximations q. The notation H denotes the sum of cross-
entropies of all test sentences, divided by the total number
</tableCaption>
<bodyText confidence="0.983178962962963">
of test words. A perfect approximation would achieve H(p),
which we estimate using the true Hd(p) and a 10000-best list.
the entropy H(p)def = − Ey p(y  |x) log p(y  |x).
Unfortunately H(p) (and hence KL = H(p, q) −
H(p)) is intractable to compute. But, since H(p)
is the same for all q, we can simply use H(p, q)
to compare different models q. Table 3 reports the
cross-entropies H(p, q) for various models q.
We also report the derivational entropy
Hd(p) def = − Edp(d  |x) logp(d  |x).20 From this,
we obtain an estimate of H(p) by observing that
the “gap” Hd(p) − H(p) equals Ep(y)[H(d  |y)],
which we estimate from our 10000-best list.
Table 3 confirms that higher-order variational
models (drawn from a larger family Q) approxi-
mate p better. This is necessarily true, but it is
interesting to see that most of the improvement is
obtained just by moving from a unigram to a bi-
gram model. Indeed, although Table 3 shows that
better approximations can be obtained by using
higher-order models, the best BLEU score in Ta-
bles 2a and 2c was obtained by the bigram model.
After all, p cannot perfectly predict the reference
translation anyway, hence may not be worth ap-
proximating closely; but p may do a good job
of predicting bigrams of the reference translation,
and the BLEU score rewards us for those.
</bodyText>
<sectionHeader confidence="0.998087" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.981581266666667">
We have successfully applied the general varia-
tional inference framework to a large-scale MT
task, to approximate the intractable problem of
MAP decoding in the presence of spurious am-
biguity. We also showed that interpolating vari-
ational models with the Viterbi approximation can
compensate for poor approximations, and that in-
terpolating them with one another can reduce the
Bayes risk and improve BLEU. Our empirical re-
sults improve the state of the art.
20Both H(p, q) and Hd(p) involve an expectation over ex-
ponentially many derivations, but they can be computed in
time only linear in the size of HG(x) using an expectation
semiring (Eisner, 2002). In particular, H(p, q) can be found
as − EdCD(x) p(d  |x) log q(Y(d)).
</bodyText>
<page confidence="0.989418">
600
</page>
<bodyText confidence="0.999992375">
Many interesting research directions remain
open. To approximate the intractable MAP de-
coding problem of (2), we can use different vari-
ational distributions other than the n-gram model
of (11). Interpolation with other models is also
interesting, e.g., the constituent model in Zhang
and Gildea (2008). We might also attempt to min-
imize KL(q 11 p) rather than KL(p 11 q), in order
to approximate the mode (which may be prefer-
able since we care most about the 1-best transla-
tion under p) rather than the mean of p (Minka,
2005). One could also augment our n-gram mod-
els with non-local string features (Rosenfeld et al.,
2001) provided that the expectations of these fea-
tures could be extracted from the hypergraph.
Variational inference can also be exploited to
solve many other intractable problems in MT (e.g.,
word/phrase alignment and system combination).
Finally, our method can be used for tasks beyond
MT. For example, it can be used to approximate
the intractable MAP decoding inherent in systems
using HMMs (e.g. speech recognition). It can also
be used to approximate a context-free grammar
with a finite state automaton (Nederhof, 2005).
</bodyText>
<sectionHeader confidence="0.99875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999220868131868">
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In ACL, pages 40–47.
Christopher M. Bishop. 2006. Pattern recognition and
machine learning. Springer.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200–208.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on proba-
bilistic grammars and transducers. In ICGI, pages
15–24.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
ACL-IJCNLP.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL, pages 1–8.
Joshua Goodman. 1996. Efficient algorithms for pars-
ing the DOP model. In EMNLP, pages 143–152.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL, pages 144–151.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational meth-
ods for graphical models. In Learning in Graphical
Models. MIT press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL, pages 48–54.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10–18.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09, pages 135–
139.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL, pages
976–985.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75–82.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351–358.
Tom Minka. 2005. Divergence measures and message
passing. In Microsoft Research Technical Report
(MSR-TR-2005-173). Microsoft Research.
Mark-Jan Nederhof. 2005. A general technique to
train language models on language models. Com-
put. Linguist., 31(2):173–186.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In ACL, pages 440–
447.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160–
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311–
318.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: A vehicle for linguistic-statistical integration.
Computer Speech and Language, 15(1).
Khalil Sima’an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In COLING, pages 1175–1180.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In ACL,
pages 787–794.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In ICSLP, pages 901–904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
EMNLP, pages 620–629.
Richard Zens and Hermann Ney. 2006. N-gram poste-
rior probabilities for statistical machine translation.
In WMT06, pages 72–77.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In ACL, pages 209–217.
</reference>
<page confidence="0.998114">
601
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936867">
<title confidence="0.999373">Variational Decoding for Statistical Machine Translation</title>
<author confidence="0.999514">Li Eisner Khudanpur</author>
<affiliation confidence="0.999576">Department of Computer Science and Center for Language and Speech Processing</affiliation>
<address confidence="0.9992">Johns Hopkins University, Baltimore, MD 21218, USA</address>
<email confidence="0.999853">zhifei.work@gmail.com,jason@cs.jhu.edu,khudanpur@jhu.edu</email>
<abstract confidence="0.997102458333333">Statistical models in machine translation That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations). In principle, the goodness of a string is measured by the total probability of its many derivations. However, finding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. Our particular variational distributions are parameterized as n-gram models. We also analytically show that interpolating these n-gram models for different n is similar to minimumdecoding for et al., 2008). Experiments show that our approach improves the state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="14427" citStr="Allauzen et al., 2003" startWordPosition="2407" endWordPosition="2410"> whole translation string in the dynamic programming state. They alleviate the computation cost somehow by using aggressive beam pruning, which might be sensible for their relatively small task (e.g., input sentences of &lt; 10 words). But, we are interested in improving the performance for a large-scale system, and thus their method is not a viable solution. Moreover, we observe in our experiments that using a larger n does not improve much over n = 2. 9A reviewer asks about the interaction with backed-off language models. The issue is that the most compact finitestate representations of these (Allauzen et al., 2003), which exploit backoff structure, are not purely m-gram for any m. They yield more compact hypergraphs (Li and Khudanpur, 2008), but unfortunately those hypergraphs might not be treatable by Fig. 4—since where they back off to less than an n-gram, a is not informative enough for line 8 to find w. We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by 0. One seeks 0 to minimize (8). Intersect HG(x) with an “unweighted” version of Q in which all arcs have weigh</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In ACL, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern recognition and machine learning.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="9889" citStr="Bishop, 2006" startWordPosition="1586" endWordPosition="1587">. Modifying (2) to sum over only these derivations is called crunching by May and Knight (2006): y* = argmax pcrunch(y |x) (7) yET(x) = argmax yET(x) dED(x,y)nND(x) X p(y, d |x) 3 Variational Approximate Decoding The Viterbi and crunching methods above approximate the intractable decoding of (2) by ignoring most of the derivations. In this section, we will present a novel variational approximation, which considers all the derivations but still allows tractable decoding. 3.1 Approximate Inference There are several popular approaches to approximate inference when exact inference is intractable (Bishop, 2006). Stochastic techniques such as Markov Chain Monte Carlo are exact in the limit of infinite runtime, but tend to be too slow for large problems. By contrast, deterministic variational methods (Jordan et al., 1999), including messagepassing (Minka, 2005), are inexact but scale up well. They approximate the original intractable distribution with one that factorizes better or has a specific parametric form (e.g., Gaussian). In our work, we use a fast variational method. Variational methods generally work as follows. When exact inference under a complex model p is intractable, one can approximate </context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern recognition and machine learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>200--208</pages>
<contexts>
<context position="8497" citStr="Blunsom et al., 2008" startWordPosition="1356" endWordPosition="1359">e decoding problem of (2) turns out to be NP-hard,4 as shown by Sima’an (1996) for a similar problem. 3A hypergraph is analogous to a parse forest (Huang and Chiang, 2007). (A finite-state lattice is a special case.) It can be used to encode exponentially many hypotheses generated by a phrase-based MT system (e.g., Koehn et al. (2003)) or a syntax-based MT system (e.g., Chiang (2007)). 4Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al., 2008). ! &amp;quot; # $ % &amp; machine translation software ! &amp;quot; # $ % &amp; machine translation software S-&gt;(S0 S1, S0 S1) S-&gt;(S0 S1, S0 S1) 594 2.3 Viterbi Approximation To approximate the intractable decoding problem of (2), most MT systems (Koehn et al., 2003; Chiang, 2007) use a simple Viterbi approximation, pViterbi(y |x) (4) max p(y, d |x) (5) dED(x,y) !p(y, d |x) (6) Clearly, (5) replaces the sum in (2) with a max. In other words, it approximates the probability of a translation string by the probability of its mostprobable derivation. (5) is found quickly via (6). The Viterbi approximation is simple and tr</context>
<context position="13758" citStr="Blunsom et al. (2008)" startWordPosition="2293" endWordPosition="2296"> computation becomes intractable as n —* oc.8 However, if p(y |x) is defined by a hypergraph HG(x) whose structure explicitly incorporates an m-gram language model, both training and decoding will be efficient when m &gt; n. We will give algorithms for this case that are linear in the size of HG(x).9 Formally, each q E 2 takes the form q(y) = 11 q(r(w) |h(w))&apos;(y) (11) wEW where W is a set of n-gram types. Each w E W is an n-gram, which occurs cw(y) times in the string y, and w may be divided into an (n − 1)-gram prefix h(w) (the history) and a 1-gram suffix r(w) (the rightmost or current word). 8Blunsom et al. (2008) effectively do take n = oo, by maintaining the whole translation string in the dynamic programming state. They alleviate the computation cost somehow by using aggressive beam pruning, which might be sensible for their relatively small task (e.g., input sentences of &lt; 10 words). But, we are interested in improving the performance for a large-scale system, and thus their method is not a viable solution. Moreover, we observe in our experiments that using a larger n does not improve much over n = 2. 9A reviewer asks about the interaction with backed-off language models. The issue is that the most</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In ACL, pages 200–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
<author>Colin De La Higuera</author>
</authors>
<title>Computational complexity of problems on probabilistic grammars and transducers.</title>
<date>2000</date>
<booktitle>In ICGI,</booktitle>
<pages>15--24</pages>
<contexts>
<context position="3169" citStr="Casacuberta and Higuera, 2000" startWordPosition="475" endWordPosition="478">ample, the translation process from one language to another language may follow some hidden tree transformation process, in a recursive fashion. Many features of the model will crucially make reference to such hidden structures or alignments. However, collapsing the resulting spurious ambiguity—i.e., marginalizing out the nuisance variables—causes significant computational difficulties. The goodness of a possible MT output string should be measured by summing up the probabilities of all its derivations. Unfortunately, finding the best string is then computationally intractable (Sima’an, 1996; Casacuberta and Higuera, 2000).2 Therefore, most systems merely identify the single most probable derivation and report the corresponding string. This corresponds to a Viterbi approximation that measures the goodness of an output string using only its most probable derivation, ignoring all the others. In this paper, we propose a variational method that considers all the derivations but still allows tractable decoding. Given an input string, the original system produces a probability distribution p over possible output strings and their derivations (nuisance variables). Our method constructs a second distribution q E 2 that</context>
</contexts>
<marker>Casacuberta, Higuera, 2000</marker>
<rawString>Francisco Casacuberta and Colin De La Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In ICGI, pages 15–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report.</tech>
<contexts>
<context position="28390" citStr="Chen and Goodman, 1998" startWordPosition="4884" endWordPosition="4887">uite differently. 5 Experimental Results We report results using an open source MT toolkit, called Joshua (Li et al., 2009), which implements Hiero (Chiang, 2007). 5.1 Experimental Setup We work on a Chinese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those o</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1914" citStr="Chiang, 2007" startWordPosition="281" endWordPosition="282"> processing. Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of interest to the user.1 For example, though machine translation (MT) seeks to output a string, typical MT systems (Koehn et al., 2003; Chiang, 2007) 1These nuisance variables may be annotated in training data, but it is more common for them to be latent even there, i.e., there is no supervision as to their “correct” values. will also recover a particular derivation of that output string, which specifies a tree or segmentation and its alignment to the input string. The competing derivations of a string are interchangeable for a user who is only interested in the string itself, so a system that unnecessarily tries to choose among them is said to be resolving spurious ambiguity. Of course, the nuisance variables are important components of t</context>
<context position="5754" citStr="Chiang (2007)" startWordPosition="880" endWordPosition="881">collapsing spurious ambiguity for other tasks as well. Such tasks include dataoriented parsing (DOP), applications of Hidden Markov Models (HMMs) and mixture models, and other models with latent variables. Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al., 2005). 2 Background 2.1 Terminology In MT, spurious ambiguity occurs both in regular phrase-based systems (e.g., Koehn et al. (2003)), where different segmentations lead to the same translation string (Figure 1), and in syntax-based systems (e.g., Chiang (2007)), where different derivation trees yield the same string (Figure 2). In the Hiero system (Chiang, 2007) we are using, each string corresponds to about 115 distinct derivations on average. We use x to denote the input string, and D(x) to consider the set of derivations then considered by the system. Each derivation d E D(x) yields some translation string y = Y(d) in the target language. We write D(x, y) def = {d E D(x) : Y(d) = y} to denote the set of all derivations that yield y. Thus, the set of translations permitted by the model is T(y) def= {y : D(x, y) =� 0} (or equivalently, T(y) def = </context>
<context position="8047" citStr="Chiang, 2007" startWordPosition="1287" endWordPosition="1288">actor to adjust the sharpness of the distribution, the score s(x, y, d) is a learned linear combination of features of the triple (x, y, d), and Z(x) is a normalization constant. Note that p(y, d |x) = 0 if y =� Y(d). Our derivation set D(x) is encoded in polynomial space, using a hypergraph or lattice.3 However, both |D(x)| and |T(x) |may be exponential in |x|. Since the marginalization needs to be carried out for each member of T(x), the decoding problem of (2) turns out to be NP-hard,4 as shown by Sima’an (1996) for a similar problem. 3A hypergraph is analogous to a parse forest (Huang and Chiang, 2007). (A finite-state lattice is a special case.) It can be used to encode exponentially many hypotheses generated by a phrase-based MT system (e.g., Koehn et al. (2003)) or a syntax-based MT system (e.g., Chiang (2007)). 4Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al., 2008). ! &amp;quot; # $ % &amp; machine translation software ! &amp;quot; # $ % &amp; machine translation software S-&gt;(S0 S1, S0 S1) S-&gt;(S0 S1, S0 S1) 594 2.3 Viterbi Approximation </context>
<context position="27929" citStr="Chiang, 2007" startWordPosition="4807" endWordPosition="4808">(11) is a proper probabilistic model, the function of (22) is simply an approximation of the average n-gram precisions of y. A connection between variational decoding and minimum-risk decoding has been noted before (e.g., Matsuzaki et al. (2005)), but the derivation above makes the connection formal. DeNero et al. (2009) concurrently developed an alternate to MBR, called consensus decoding, which is similar to ours in practice although motivated quite differently. 5 Experimental Results We report results using an open source MT toolkit, called Joshua (Li et al., 2009), which implements Hiero (Chiang, 2007). 5.1 Experimental Setup We work on a Chinese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GI</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Fast consensus decoding over translation forests.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="27638" citStr="DeNero et al. (2009)" startWordPosition="4761" endWordPosition="4764">). In comparison, the optimal n-gram probabilities of (12) can be computed using the inside-outside algorithm, once and for all. Also, g(w |x) of (20) is not normalized over the history of w, while q*(r(w) |h(w)) of (12) is. Lastly, the definition of the n-gram model is different. While the model (11) is a proper probabilistic model, the function of (22) is simply an approximation of the average n-gram precisions of y. A connection between variational decoding and minimum-risk decoding has been noted before (e.g., Matsuzaki et al. (2005)), but the derivation above makes the connection formal. DeNero et al. (2009) concurrently developed an alternate to MBR, called consensus decoding, which is similar to ours in practice although motivated quite differently. 5 Experimental Results We report results using an open source MT toolkit, called Joshua (Li et al., 2009), which implements Hiero (Chiang, 2007). 5.1 Experimental Setup We work on a Chinese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matc</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>John DeNero, David Chiang, and Kevin Knight. 2009. Fast consensus decoding over translation forests. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="15143" citStr="Eisner, 2002" startWordPosition="2535" endWordPosition="2536"> (Li and Khudanpur, 2008), but unfortunately those hypergraphs might not be treatable by Fig. 4—since where they back off to less than an n-gram, a is not informative enough for line 8 to find w. We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by 0. One seeks 0 to minimize (8). Intersect HG(x) with an “unweighted” version of Q in which all arcs have weight 1, so that Q does not prefer any string to another. By lifting weights into an expectation semiring (Eisner, 2002), it is then possible to obtain expected transition counts in Q (where the expectation is taken under p), or other sufficient statistics needed to estimate 0. This takes only time O(|HG(x)|) when L is a left-to-right refinement of Q (meaning that any two prefix strings that reach the same state in L also reach the same state in Q), for then intersecting L or HG(x) with Q does not split any states. That is the case when L and Q are respectively pure m-gram and n-gram models with m &gt; n, as assumed in (12) and Figure 4. It is also the case when Q is a pure n-gram model and L is constructed not to</context>
<context position="35887" citStr="Eisner, 2002" startWordPosition="6122" endWordPosition="6123"> variational inference framework to a large-scale MT task, to approximate the intractable problem of MAP decoding in the presence of spurious ambiguity. We also showed that interpolating variational models with the Viterbi approximation can compensate for poor approximations, and that interpolating them with one another can reduce the Bayes risk and improve BLEU. Our empirical results improve the state of the art. 20Both H(p, q) and Hd(p) involve an expectation over exponentially many derivations, but they can be computed in time only linear in the size of HG(x) using an expectation semiring (Eisner, 2002). In particular, H(p, q) can be found as − EdCD(x) p(d |x) log q(Y(d)). 600 Many interesting research directions remain open. To approximate the intractable MAP decoding problem of (2), we can use different variational distributions other than the n-gram model of (11). Interpolation with other models is also interesting, e.g., the constituent model in Zhang and Gildea (2008). We might also attempt to minimize KL(q 11 p) rather than KL(p 11 q), in order to approximate the mode (which may be preferable since we care most about the 1-best translation under p) rather than the mean of p (Minka, 200</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In ACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient algorithms for parsing the DOP model.</title>
<date>1996</date>
<booktitle>In EMNLP,</booktitle>
<pages>143--152</pages>
<contexts>
<context position="5441" citStr="Goodman, 1996" startWordPosition="835" endWordPosition="836">approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al. (2008). Experiments show that our approach improves the state of the art. The methods presented in this paper should be applicable to collapsing spurious ambiguity for other tasks as well. Such tasks include dataoriented parsing (DOP), applications of Hidden Markov Models (HMMs) and mixture models, and other models with latent variables. Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al., 2005). 2 Background 2.1 Terminology In MT, spurious ambiguity occurs both in regular phrase-based systems (e.g., Koehn et al. (2003)), where different segmentations lead to the same translation string (Figure 1), and in syntax-based systems (e.g., Chiang (2007)), where different derivation trees yield the same string (Figure 2). In the Hiero system (Chiang, 2007) we are using, each string corresponds to about 115 distinct derivations on average. We use x to denote the input string, and D(x) to consider the set of derivations then considered b</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Efficient algorithms for parsing the DOP model. In EMNLP, pages 143–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="8047" citStr="Huang and Chiang, 2007" startWordPosition="1285" endWordPosition="1288"> scaling factor to adjust the sharpness of the distribution, the score s(x, y, d) is a learned linear combination of features of the triple (x, y, d), and Z(x) is a normalization constant. Note that p(y, d |x) = 0 if y =� Y(d). Our derivation set D(x) is encoded in polynomial space, using a hypergraph or lattice.3 However, both |D(x)| and |T(x) |may be exponential in |x|. Since the marginalization needs to be carried out for each member of T(x), the decoding problem of (2) turns out to be NP-hard,4 as shown by Sima’an (1996) for a similar problem. 3A hypergraph is analogous to a parse forest (Huang and Chiang, 2007). (A finite-state lattice is a special case.) It can be used to encode exponentially many hypotheses generated by a phrase-based MT system (e.g., Koehn et al. (2003)) or a syntax-based MT system (e.g., Chiang (2007)). 4Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al., 2008). ! &amp;quot; # $ % &amp; machine translation software ! &amp;quot; # $ % &amp; machine translation software S-&gt;(S0 S1, S0 S1) S-&gt;(S0 S1, S0 S1) 594 2.3 Viterbi Approximation </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In ACL, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M I Jordan</author>
<author>Z Ghahramani</author>
<author>T S Jaakkola</author>
<author>L K Saul</author>
</authors>
<title>An introduction to variational methods for graphical models. In Learning in Graphical Models.</title>
<date>1999</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="10102" citStr="Jordan et al., 1999" startWordPosition="1618" endWordPosition="1621">coding The Viterbi and crunching methods above approximate the intractable decoding of (2) by ignoring most of the derivations. In this section, we will present a novel variational approximation, which considers all the derivations but still allows tractable decoding. 3.1 Approximate Inference There are several popular approaches to approximate inference when exact inference is intractable (Bishop, 2006). Stochastic techniques such as Markov Chain Monte Carlo are exact in the limit of infinite runtime, but tend to be too slow for large problems. By contrast, deterministic variational methods (Jordan et al., 1999), including messagepassing (Minka, 2005), are inexact but scale up well. They approximate the original intractable distribution with one that factorizes better or has a specific parametric form (e.g., Gaussian). In our work, we use a fast variational method. Variational methods generally work as follows. When exact inference under a complex model p is intractable, one can approximate the posterior p(y |x) by a tractable model q(y), where q E 2 is chosen to minimize some information loss such as the KL divergence KL(p II q). The simpler model q can then act as a surrogate for p during inference</context>
</contexts>
<marker>Jordan, Ghahramani, Jaakkola, Saul, 1999</marker>
<rawString>M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. 1999. An introduction to variational methods for graphical models. In Learning in Graphical Models. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1899" citStr="Koehn et al., 2003" startWordPosition="277" endWordPosition="280"> in natural language processing. Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of interest to the user.1 For example, though machine translation (MT) seeks to output a string, typical MT systems (Koehn et al., 2003; Chiang, 2007) 1These nuisance variables may be annotated in training data, but it is more common for them to be latent even there, i.e., there is no supervision as to their “correct” values. will also recover a particular derivation of that output string, which specifies a tree or segmentation and its alignment to the input string. The competing derivations of a string are interchangeable for a user who is only interested in the string itself, so a system that unnecessarily tries to choose among them is said to be resolving spurious ambiguity. Of course, the nuisance variables are important </context>
<context position="5625" citStr="Koehn et al. (2003)" startWordPosition="860" endWordPosition="863"> (2008). Experiments show that our approach improves the state of the art. The methods presented in this paper should be applicable to collapsing spurious ambiguity for other tasks as well. Such tasks include dataoriented parsing (DOP), applications of Hidden Markov Models (HMMs) and mixture models, and other models with latent variables. Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al., 2005). 2 Background 2.1 Terminology In MT, spurious ambiguity occurs both in regular phrase-based systems (e.g., Koehn et al. (2003)), where different segmentations lead to the same translation string (Figure 1), and in syntax-based systems (e.g., Chiang (2007)), where different derivation trees yield the same string (Figure 2). In the Hiero system (Chiang, 2007) we are using, each string corresponds to about 115 distinct derivations on average. We use x to denote the input string, and D(x) to consider the set of derivations then considered by the system. Each derivation d E D(x) yields some translation string y = Y(d) in the target language. We write D(x, y) def = {d E D(x) : Y(d) = y} to denote the set of all derivations</context>
<context position="8212" citStr="Koehn et al. (2003)" startWordPosition="1312" endWordPosition="1315">alization constant. Note that p(y, d |x) = 0 if y =� Y(d). Our derivation set D(x) is encoded in polynomial space, using a hypergraph or lattice.3 However, both |D(x)| and |T(x) |may be exponential in |x|. Since the marginalization needs to be carried out for each member of T(x), the decoding problem of (2) turns out to be NP-hard,4 as shown by Sima’an (1996) for a similar problem. 3A hypergraph is analogous to a parse forest (Huang and Chiang, 2007). (A finite-state lattice is a special case.) It can be used to encode exponentially many hypotheses generated by a phrase-based MT system (e.g., Koehn et al. (2003)) or a syntax-based MT system (e.g., Chiang (2007)). 4Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al., 2008). ! &amp;quot; # $ % &amp; machine translation software ! &amp;quot; # $ % &amp; machine translation software S-&gt;(S0 S1, S0 S1) S-&gt;(S0 S1, S0 S1) 594 2.3 Viterbi Approximation To approximate the intractable decoding problem of (2), most MT systems (Koehn et al., 2003; Chiang, 2007) use a simple Viterbi approximation, pViterbi(y |x) (4) max</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In ACL SSST,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="14555" citStr="Li and Khudanpur, 2008" startWordPosition="2427" endWordPosition="2431">m pruning, which might be sensible for their relatively small task (e.g., input sentences of &lt; 10 words). But, we are interested in improving the performance for a large-scale system, and thus their method is not a viable solution. Moreover, we observe in our experiments that using a larger n does not improve much over n = 2. 9A reviewer asks about the interaction with backed-off language models. The issue is that the most compact finitestate representations of these (Allauzen et al., 2003), which exploit backoff structure, are not purely m-gram for any m. They yield more compact hypergraphs (Li and Khudanpur, 2008), but unfortunately those hypergraphs might not be treatable by Fig. 4—since where they back off to less than an n-gram, a is not informative enough for line 8 to find w. We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by 0. One seeks 0 to minimize (8). Intersect HG(x) with an “unweighted” version of Q in which all arcs have weight 1, so that Q does not prefer any string to another. By lifting weights into an expectation semiring (Eisner, 2002), it is then</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In ACL SSST, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsingbased machine translation.</title>
<date>2009</date>
<booktitle>In WMT09,</booktitle>
<pages>135--139</pages>
<marker>Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar. Zaidan. 2009. Joshua: An open source toolkit for parsingbased machine translation. In WMT09, pages 135– 139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>976--985</pages>
<contexts>
<context position="28583" citStr="Lopez, 2007" startWordPosition="4920" endWordPosition="4921">ese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those of (16)) and the scaling factor 17We have also experimented with MERT (Och, 2003), and found that the deterministic annealing gave results that were more consistent across runs and often better.</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In EMNLP-CoNLL, pages 976–985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="5498" citStr="Matsuzaki et al., 2005" startWordPosition="841" endWordPosition="844">original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al. (2008). Experiments show that our approach improves the state of the art. The methods presented in this paper should be applicable to collapsing spurious ambiguity for other tasks as well. Such tasks include dataoriented parsing (DOP), applications of Hidden Markov Models (HMMs) and mixture models, and other models with latent variables. Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al., 2005). 2 Background 2.1 Terminology In MT, spurious ambiguity occurs both in regular phrase-based systems (e.g., Koehn et al. (2003)), where different segmentations lead to the same translation string (Figure 1), and in syntax-based systems (e.g., Chiang (2007)), where different derivation trees yield the same string (Figure 2). In the Hiero system (Chiang, 2007) we are using, each string corresponds to about 115 distinct derivations on average. We use x to denote the input string, and D(x) to consider the set of derivations then considered by the system. Each derivation d E D(x) yields some transl</context>
<context position="27561" citStr="Matsuzaki et al. (2005)" startWordPosition="4749" endWordPosition="4752">iple applied in a hypergraph with spurious ambiguity. gn(y |x)= 598 et al. (2008). In comparison, the optimal n-gram probabilities of (12) can be computed using the inside-outside algorithm, once and for all. Also, g(w |x) of (20) is not normalized over the history of w, while q*(r(w) |h(w)) of (12) is. Lastly, the definition of the n-gram model is different. While the model (11) is a proper probabilistic model, the function of (22) is simply an approximation of the average n-gram precisions of y. A connection between variational decoding and minimum-risk decoding has been noted before (e.g., Matsuzaki et al. (2005)), but the derivation above makes the connection formal. DeNero et al. (2009) concurrently developed an alternate to MBR, called consensus decoding, which is similar to ours in practice although motivated quite differently. 5 Experimental Results We report results using an open source MT toolkit, called Joshua (Li et al., 2009), which implements Hiero (Chiang, 2007). 5.1 Experimental Setup We work on a Chinese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by L</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>A better n-best list: practical determinization of weighted finite tree automata.</title>
<date>2006</date>
<booktitle>In NAACL,</booktitle>
<pages>351--358</pages>
<contexts>
<context position="4065" citStr="May and Knight (2006)" startWordPosition="620" endWordPosition="623"> this paper, we propose a variational method that considers all the derivations but still allows tractable decoding. Given an input string, the original system produces a probability distribution p over possible output strings and their derivations (nuisance variables). Our method constructs a second distribution q E 2 that approximates p as well as possible, and then finds the best string according to q. The last step is tractable because each q E 2 is defined (unlike p) without reference to nuisance variables. Notice that q here does not approximate the entire translation process, but only 2May and Knight (2006) have successfully used treeautomaton determinization to exactly marginalize out some of the nuisance variables, obtaining a distribution over parsed translations. However, they do not marginalize over these parse trees to obtain a distribution over translation strings. 593 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593–601, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP the distribution over output strings for a particular input. This is why it can be a fairly good approximation even without using the nuisance variables. In practice, we </context>
<context position="9371" citStr="May and Knight (2006)" startWordPosition="1506" endWordPosition="1509">07) use a simple Viterbi approximation, pViterbi(y |x) (4) max p(y, d |x) (5) dED(x,y) !p(y, d |x) (6) Clearly, (5) replaces the sum in (2) with a max. In other words, it approximates the probability of a translation string by the probability of its mostprobable derivation. (5) is found quickly via (6). The Viterbi approximation is simple and tractable, but it ignores most derivations. 2.4 N-best Approximation (or Crunching) Another popular approximation enumerates the N best derivations in D(x), a set that we call ND(x). Modifying (2) to sum over only these derivations is called crunching by May and Knight (2006): y* = argmax pcrunch(y |x) (7) yET(x) = argmax yET(x) dED(x,y)nND(x) X p(y, d |x) 3 Variational Approximate Decoding The Viterbi and crunching methods above approximate the intractable decoding of (2) by ignoring most of the derivations. In this section, we will present a novel variational approximation, which considers all the derivations but still allows tractable decoding. 3.1 Approximate Inference There are several popular approaches to approximate inference when exact inference is intractable (Bishop, 2006). Stochastic techniques such as Markov Chain Monte Carlo are exact in the limit of</context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>Jonathan May and Kevin Knight. 2006. A better n-best list: practical determinization of weighted finite tree automata. In NAACL, pages 351–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Minka</author>
</authors>
<title>Divergence measures and message passing.</title>
<date>2005</date>
<journal>In Microsoft Research</journal>
<tech>Technical Report (MSR-TR-2005-173). Microsoft Research.</tech>
<contexts>
<context position="10142" citStr="Minka, 2005" startWordPosition="1625" endWordPosition="1626">pproximate the intractable decoding of (2) by ignoring most of the derivations. In this section, we will present a novel variational approximation, which considers all the derivations but still allows tractable decoding. 3.1 Approximate Inference There are several popular approaches to approximate inference when exact inference is intractable (Bishop, 2006). Stochastic techniques such as Markov Chain Monte Carlo are exact in the limit of infinite runtime, but tend to be too slow for large problems. By contrast, deterministic variational methods (Jordan et al., 1999), including messagepassing (Minka, 2005), are inexact but scale up well. They approximate the original intractable distribution with one that factorizes better or has a specific parametric form (e.g., Gaussian). In our work, we use a fast variational method. Variational methods generally work as follows. When exact inference under a complex model p is intractable, one can approximate the posterior p(y |x) by a tractable model q(y), where q E 2 is chosen to minimize some information loss such as the KL divergence KL(p II q). The simpler model q can then act as a surrogate for p during inference. 3.2 Variational Decoding for MT For ea</context>
<context position="36489" citStr="Minka, 2005" startWordPosition="6228" endWordPosition="6229">ner, 2002). In particular, H(p, q) can be found as − EdCD(x) p(d |x) log q(Y(d)). 600 Many interesting research directions remain open. To approximate the intractable MAP decoding problem of (2), we can use different variational distributions other than the n-gram model of (11). Interpolation with other models is also interesting, e.g., the constituent model in Zhang and Gildea (2008). We might also attempt to minimize KL(q 11 p) rather than KL(p 11 q), in order to approximate the mode (which may be preferable since we care most about the 1-best translation under p) rather than the mean of p (Minka, 2005). One could also augment our n-gram models with non-local string features (Rosenfeld et al., 2001) provided that the expectations of these features could be extracted from the hypergraph. Variational inference can also be exploited to solve many other intractable problems in MT (e.g., word/phrase alignment and system combination). Finally, our method can be used for tasks beyond MT. For example, it can be used to approximate the intractable MAP decoding inherent in systems using HMMs (e.g. speech recognition). It can also be used to approximate a context-free grammar with a finite state automa</context>
</contexts>
<marker>Minka, 2005</marker>
<rawString>Tom Minka. 2005. Divergence measures and message passing. In Microsoft Research Technical Report (MSR-TR-2005-173). Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>A general technique to train language models on language models.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>2</issue>
<marker>Nederhof, 2005</marker>
<rawString>Mark-Jan Nederhof. 2005. A general technique to train language models on language models. Comput. Linguist., 31(2):173–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="28553" citStr="Och and Ney, 2000" startWordPosition="4914" endWordPosition="4917">Experimental Setup We work on a Chinese to English translation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those of (16)) and the scaling factor 17We have also experimented with MERT (Och, 2003), and found that the deterministic annealing gave results that were more consistent</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In ACL, pages 440– 447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="22020" citStr="Och, 2003" startWordPosition="3776" endWordPosition="3777">i et al., 2001), which gives partial credit, we would also like to favor lower-order ngrams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams. For this reason, it is useful to interpolate different orders of variational models, 1: y∗ = argmax On · log q∗n(y) (15) y∈T(x) n where n may include the value of zero, in which case log q∗�(y) def = |y|, corresponding to a conventional word penalty feature. In the geometric interpolation above, the weight On controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). Lastly, note that Viterbi and variational approximation are different ways to approximate the exact probability p(y |x), and each of them has pros and cons. Specifically, Viterbi approximation uses the correct probability of one complete 597 derivation, but ignores most of the derivations in the hypergraph. In comparison, the variational approximation considers all the derivations in the hypergraph, but uses only aggregate statistics of fragments of derivations. Therefore, it is desirable to interpolate further with the Viterbi approximati</context>
<context position="29070" citStr="Och, 2003" startWordPosition="4992" endWordPosition="4993"> (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those of (16)) and the scaling factor 17We have also experimented with MERT (Och, 2003), and found that the deterministic annealing gave results that were more consistent across runs and often better. Decoding scheme MT’04 MT’05 Viterbi 35.4 32.6 MBR (K=1000) 35.8 32.7 Crunching (N=10000) 35.7 32.8 Crunching+MBR (N=10000) 35.8 32.7 Variational (1to4gram+wp+vt) 36.6 33.5 Table 1: BLEU scores for Viterbi, Crunching, MBR, and variational decoding. All the systems improve significantly over the Viterbi baseline (paired permutation test, p &lt; 0.05). In each column, we boldface the best result as well as all results that are statistically indistinguishable from it. In MBR, K is the num</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL, pages 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="21425" citStr="Papineni et al., 2001" startWordPosition="3668" endWordPosition="3671"> original hypergraph, i.e.: y∗ = argmax q∗(y) (14) y∈T(x) This ensures that y∗ is a valid string in the original hypergraph HG(x), which will tend to rule out inadequate translations like “the mouse.” If our sole objective is to get a good approximation to p(y |x), we should just use a single n-gram model q∗ whose order n is as large as possible, given computational constraints. This may be regarded as favoring n-grams that are likely to appear in the reference translation (because they are likely in the derivation forest). However, in order to score well on the BLEU metric for MT evaluation (Papineni et al., 2001), which gives partial credit, we would also like to favor lower-order ngrams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams. For this reason, it is useful to interpolate different orders of variational models, 1: y∗ = argmax On · log q∗n(y) (15) y∈T(x) n where n may include the value of zero, in which case log q∗�(y) def = |y|, corresponding to a conventional word penalty feature. In the geometric interpolation above, the weight On controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a</context>
<context position="24073" citStr="Papineni et al., 2001" startWordPosition="4120" endWordPosition="4123">e find argmaxy∈T(x) via Y(argmaxd∈D(x)). 4 Variational vs. Min-Risk Decoding In place of the MAP decoding, another commonly used decision rule is minimum Bayes risk (MBR): where l(y, y0) represents the loss of y if the true answer is y0, and the risk of y is its expected loss.15 Statistical decision theory shows MBR is optimal if p(y0 |x) is the true distribution, while in practice p(y0 |x) is given by a model at hand. We now observe that our variational decoding resembles the MBR decoding of Tromble et al. (2008). They use the following loss function, of which a linear approximation to BLEU (Papineni et al., 2001) is a special case, l(y, y0) = −(θ0|y |+ X θwcw(y)δw(y0)) (18) w∈N where w is an n-gram type, N is a set of n-gram types with n ∈ [1, 4], cw(y) is the number of occurrence of the n-gram w in y, and δw(y0) is an indicator function to check if y0 contains at least one occurrence of w. With the above loss function, Tromble et al. (2008) derive the MBR rule16 y∗ = argmax (θ0|y |+ X θwcw(y)g(w |x)) y w∈N (19) where g(w |x) is a specialized “posterior” probability of the n-gram w, and is defined as g(w |x) = X δw(y0)p(y0 |x) (20) y&apos; Now, let us divide N, which contains n-gram types of different n, i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. In ACL, pages 311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roni Rosenfeld</author>
<author>Stanley F Chen</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Whole-sentence exponential language models: A vehicle for linguistic-statistical integration.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Rosenfeld, Chen, Zhu, 2001</marker>
<rawString>Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001. Whole-sentence exponential language models: A vehicle for linguistic-statistical integration. Computer Speech and Language, 15(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<title>Computational complexity of probabilistic disambiguation by means of treegrammars.</title>
<date>1996</date>
<booktitle>In COLING,</booktitle>
<pages>1175--1180</pages>
<marker>Sima’an, 1996</marker>
<rawString>Khalil Sima’an. 1996. Computational complexity of probabilistic disambiguation by means of treegrammars. In COLING, pages 1175–1180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>787--794</pages>
<contexts>
<context position="22073" citStr="Smith and Eisner, 2006" startWordPosition="3783" endWordPosition="3786">dit, we would also like to favor lower-order ngrams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams. For this reason, it is useful to interpolate different orders of variational models, 1: y∗ = argmax On · log q∗n(y) (15) y∈T(x) n where n may include the value of zero, in which case log q∗�(y) def = |y|, corresponding to a conventional word penalty feature. In the geometric interpolation above, the weight On controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). Lastly, note that Viterbi and variational approximation are different ways to approximate the exact probability p(y |x), and each of them has pros and cons. Specifically, Viterbi approximation uses the correct probability of one complete 597 derivation, but ignores most of the derivations in the hypergraph. In comparison, the variational approximation considers all the derivations in the hypergraph, but uses only aggregate statistics of fragments of derivations. Therefore, it is desirable to interpolate further with the Viterbi approximation when choosing the final translation output:11 X y∗</context>
<context position="28671" citStr="Smith and Eisner, 2006" startWordPosition="4930" endWordPosition="4933">M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those of (16)) and the scaling factor 17We have also experimented with MERT (Och, 2003), and found that the deterministic annealing gave results that were more consistent across runs and often better. Decoding scheme MT’04 MT’05 Viterbi 35.4 32.6 MBR (K=1000) 35.8 32.7 Crunching (N=10000</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In ACL, pages 787–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="28606" citStr="Stolcke, 2002" startWordPosition="4923" endWordPosition="4925">ation task. Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the ngram matches between training and test sets in the foreign side. We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the English side of the parallel corpora. We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006)17 to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs. The NIST MT’03 set is used to tune model weights (e.g. those of (16)) and the scaling factor 17We have also experimented with MERT (Och, 2003), and found that the deterministic annealing gave results that were more consistent across runs and often better. Decoding scheme MT’04 </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In ICSLP, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum BayesRisk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>620--629</pages>
<contexts>
<context position="1170" citStr="Tromble et al., 2008" startWordPosition="161" endWordPosition="164">otal probability of its many derivations. However, finding the best string (e.g., during decoding) is then computationally intractable. Therefore, most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation. Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding. Our particular variational distributions are parameterized as n-gram models. We also analytically show that interpolating these n-gram models for different n is similar to minimumrisk decoding for BLEU (Tromble et al., 2008). Experiments show that our approach improves the state of the art. 1 Introduction Ambiguity is a central issue in natural language processing. Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence. These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input. However, some systems resolve too many ambiguities. They recover additional latent variables— so-called nuisance variables—that are not of int</context>
<context position="5013" citStr="Tromble et al. (2008)" startWordPosition="766" endWordPosition="769">d the 4th IJCNLP of the AFNLP, pages 593–601, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP the distribution over output strings for a particular input. This is why it can be a fairly good approximation even without using the nuisance variables. In practice, we approximate with several different variational families 2, corresponding to ngram (Markov) models of different orders. We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al. (2008). Experiments show that our approach improves the state of the art. The methods presented in this paper should be applicable to collapsing spurious ambiguity for other tasks as well. Such tasks include dataoriented parsing (DOP), applications of Hidden Markov Models (HMMs) and mixture models, and other models with latent variables. Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al., 2005). 2 Background 2.1 Terminology In MT, spurious ambiguity occurs both in regular phrase-based systems (e.g., Koehn e</context>
<context position="23970" citStr="Tromble et al. (2008)" startWordPosition="4103" endWordPosition="4106">) + θv · log p(d |x). We then find the best-scoring derivation and output its target yield; that is, we find argmaxy∈T(x) via Y(argmaxd∈D(x)). 4 Variational vs. Min-Risk Decoding In place of the MAP decoding, another commonly used decision rule is minimum Bayes risk (MBR): where l(y, y0) represents the loss of y if the true answer is y0, and the risk of y is its expected loss.15 Statistical decision theory shows MBR is optimal if p(y0 |x) is the true distribution, while in practice p(y0 |x) is given by a model at hand. We now observe that our variational decoding resembles the MBR decoding of Tromble et al. (2008). They use the following loss function, of which a linear approximation to BLEU (Papineni et al., 2001) is a special case, l(y, y0) = −(θ0|y |+ X θwcw(y)δw(y0)) (18) w∈N where w is an n-gram type, N is a set of n-gram types with n ∈ [1, 4], cw(y) is the number of occurrence of the n-gram w in y, and δw(y0) is an indicator function to check if y0 contains at least one occurrence of w. With the above loss function, Tromble et al. (2008) derive the MBR rule16 y∗ = argmax (θ0|y |+ X θwcw(y)g(w |x)) y w∈N (19) where g(w |x) is a specialized “posterior” probability of the n-gram w, and is defined as</context>
<context position="26843" citStr="Tromble et al. (2008)" startWordPosition="4632" endWordPosition="4635">0 P (22) w∈Wn g(w |x)cw(y) if n &gt; 0 Clearly, their rule (21) has a quite similar form to our rule (15), and we can relate (20) to (12) and (22) to (11). This justifies the use of interpolation in Section 3.2.3. However, there are several important differences. First, the n-gram “posterior” of (20) is very expensive to compute. In fact, it requires an intersection between each n-gram in the lattice and the lattice itself, as is done by Tromble 15The MBR becomes the MAP decision rule of (1) if a socalled zero-one loss function is used: l(y, y&apos;) = 0 if y = y&apos;; otherwise l(y, y&apos;) = 1. 16Note that Tromble et al. (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hypergraph with spurious ambiguity. gn(y |x)= 598 et al. (2008). In comparison, the optimal n-gram probabilities of (12) can be computed using the inside-outside algorithm, once and for all. Also, g(w |x) of (20) is not normalized over the history of w, while q*(r(w) |h(w)) of (12) is. Lastly, the definition of the n-gram model is different. While the model (11) is a proper probabilistic model, the function of (22) is simply an approximation of the average n-gram precisions of y. A </context>
<context position="31464" citStr="Tromble et al. (2008)" startWordPosition="5372" endWordPosition="5375"> 2 presents the BLEU results under different ways in using the variational models, as discussed in Section 3.2.3. As shown in Table 2a, decoding with a single variational n-gram model (VM) as per (14) improves the Viterbi baseline (except the case with a unigram VM), though often not statistically significant. Moreover, a bigram (i.e., “2gram”) achieves the best BLEU scores among the four different orders of VMs. The interpolation between a VM and a word penalty feature (“wp”) improves over the unigram 18We found the BLEU scores are not very sensitive to &apos;y, contrasting to the observations by Tromble et al. (2008). 19We also restrict T(x) to {y : p(y |x) &gt; 01, using the same approximation for p(y |x) as we did for p(y&apos; |x). 599 (a) decoding with a single variational model Decoding scheme MT’04 MT’05 Viterbi 35.4 32.6 1gram 25.9 24.5 2gram 36.1 33.4 3gram 36.0∗ 33.1 4gram 35.8∗ 32.9 (b) interpolation between a single variational model and a word penalty feature 1gram+wp 29.7 27.7 2gram+wp 35.5 32.6 3gram+wp 36.1∗ 33.1 4gram+wp 35.7∗ 32.8∗ (c) interpolation of a single variational model, the Viterbi model, and a word penalty feature 1gram+wp+vt 35.6∗ 32.8∗ 2gram+wp+vt 36.5∗ 33.5∗ 3gram+wp+vt 35.8∗ 32.9∗ </context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum BayesRisk decoding for statistical machine translation. In EMNLP, pages 620–629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>N-gram posterior probabilities for statistical machine translation.</title>
<date>2006</date>
<booktitle>In WMT06,</booktitle>
<pages>72--77</pages>
<contexts>
<context position="25029" citStr="Zens and Ney (2006)" startWordPosition="4312" endWordPosition="4315">erive the MBR rule16 y∗ = argmax (θ0|y |+ X θwcw(y)g(w |x)) y w∈N (19) where g(w |x) is a specialized “posterior” probability of the n-gram w, and is defined as g(w |x) = X δw(y0)p(y0 |x) (20) y&apos; Now, let us divide N, which contains n-gram types of different n, into several subsets Wn, each of which contains only the n-grams with a given length n. We can now rewrite (19) as follows, y∗ = argmin R(y) = argmin X l(y, y0)p(y0 |x) y∗ = argmax X y y y&apos; y θn · gn(y |x) (21) n (17) 11It would also be possible to interpolate with the N-best approximations (see Section 2.4), with some complications. 12Zens and Ney (2006) use a similar decision rule as here and they also use posterior n-gram probabilities as feature functions, but their model estimation and decoding are over an N-best, which is trivial in terms of computation. 13Already at (14), we explicitly ruled out translations y having no derivation at all in the hypergraph. However, suppose the hypergraph were very large (thanks to a large or smoothed translation model and weak pruning). Then (14)’s heuristic would fail to eliminate bad translations (“the mouse”), since nearly every string y E E* would be derived as a translation with at least a tiny pro</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. N-gram posterior probabilities for statistical machine translation. In WMT06, pages 72–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Efficient multipass decoding for synchronous context free grammars.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>209--217</pages>
<contexts>
<context position="36264" citStr="Zhang and Gildea (2008)" startWordPosition="6181" endWordPosition="6184">ur empirical results improve the state of the art. 20Both H(p, q) and Hd(p) involve an expectation over exponentially many derivations, but they can be computed in time only linear in the size of HG(x) using an expectation semiring (Eisner, 2002). In particular, H(p, q) can be found as − EdCD(x) p(d |x) log q(Y(d)). 600 Many interesting research directions remain open. To approximate the intractable MAP decoding problem of (2), we can use different variational distributions other than the n-gram model of (11). Interpolation with other models is also interesting, e.g., the constituent model in Zhang and Gildea (2008). We might also attempt to minimize KL(q 11 p) rather than KL(p 11 q), in order to approximate the mode (which may be preferable since we care most about the 1-best translation under p) rather than the mean of p (Minka, 2005). One could also augment our n-gram models with non-local string features (Rosenfeld et al., 2001) provided that the expectations of these features could be extracted from the hypergraph. Variational inference can also be exploited to solve many other intractable problems in MT (e.g., word/phrase alignment and system combination). Finally, our method can be used for tasks </context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>Hao Zhang and Daniel Gildea. 2008. Efficient multipass decoding for synchronous context free grammars. In ACL, pages 209–217.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>