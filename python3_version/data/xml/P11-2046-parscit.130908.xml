<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025808">
<title confidence="0.997093">
Relation Guided Bootstrapping of Semantic Lexicons
</title>
<author confidence="0.999084">
Tara McIntosh&apos; Lars Yencken&apos; James R. Curran* Timothy Baldwin&apos;
</author>
<affiliation confidence="0.993771">
4 NICTA, Victoria Research Lab Q School of Information Technologies
Dept. of Computer Science and Software Engineering The University of Sydney
The University of Melbourne
</affiliation>
<email confidence="0.9355925">
nlp@taramcintosh.org james@it.usyd.edu.au
lars@yencken.org tb@ldwin.net
</email>
<sectionHeader confidence="0.994462" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999488916666667">
State-of-the-art bootstrapping systems rely on
expert-crafted semantic constraints such as
negative categories to reduce semantic drift.
Unfortunately, their use introduces a substan-
tial amount of supervised knowledge. We
present the Relation Guided Bootstrapping
(RGB) algorithm, which simultaneously ex-
tracts lexicons and open relationships to guide
lexicon growth and reduce semantic drift.
This removes the necessity for manually craft-
ing category and relationship constraints, and
manually generating negative categories.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996541509434">
Many approaches to extracting semantic lexicons
extend the unsupervised bootstrapping framework
(Riloff and Shepherd, 1997). These use a small set
of seed examples from the target lexicon to identify
contextual patterns which are then used to extract
new lexicon items (Riloff and Jones, 1999).
Bootstrappers are prone to semantic drift, caused
by selection of poor candidate terms or patterns
(Curran et al., 2007), which can be reduced by
semantically constraining the candidates. Multi-
category bootstrappers, such as NOMEN (Yangar-
ber et al., 2002) and WMEB (McIntosh and Curran,
2008), reduce semantic drift by extracting multiple
categories simultaneously in competition.
The inclusion of manually-crafted negative cate-
gories to multi-category bootstrappers achieves the
best results, by clarifying the boundaries between
categories (Yangarber et al., 2002). For exam-
ple, female names are often bootstrapped with
the negative categories flowers (e.g. Rose, Iris)
and gem stones (e.g. Ruby, Pearl) (Curran et al.,
2007). Unfortunately, negative categories are dif-
ficult to design, introducing a substantial amount
of human expertise into an otherwise unsupervised
framework. McIntosh (2010) made some progress
towards automatically learning useful negative cate-
gories during bootstrapping.
In this work we identify an unsupervised source
of semantic constraints inspired by the Coupled Pat-
tern Learner (CPL, Carlson et al. (2010)). In CPL,
relation bootstrapping is coupled with lexicon boot-
strapping in order to control semantic drift in the
target relation’s arguments. Semantic constraints
on categories and relations are manually crafted in
CPL. For example, a candidate of the relation IS-
CEOOF will only be extracted if its arguments can
be extracted into the ceo and company lexicons
and a ceo is constrained to not be a celebrity
or politician. Negative examples such as IS-
CEOOF(Sergey Brin, Google) are also introduced to
clarify boundary conditions. CPL employs a large
number of these manually-crafted constraints to im-
prove precision at the expense of recall (only 18 IS-
CEOOF instances were extracted). In our approach,
we exploit open relation bootstrapping to minimise
semantic drift, without any manual seeding of rela-
tions or pre-defined category lexicon combinations.
Orthogonal to these seeded and constraint-based
methods is the relation-independent Open Informa-
tion Extraction (OPENIE) paradigm. OPENIE sys-
tems, such as TEXTRUNNER (Banko et al., 2007),
define neither lexicon categories nor predefined re-
lationships. They extract relation tuples by exploit-
</bodyText>
<page confidence="0.97494">
266
</page>
<note confidence="0.582317">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 266–270,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999147">
ing broad syntactic patterns that are likely to indi-
cate relations. This enables the extraction of inter-
esting and unanticipated relations from text. How-
ever these patterns are often too broad, resulting in
the extraction of tuples that do not represent rela-
tions at all. As a result, heavy (supervised) post-
processing or use of supervised information is nec-
essary. For example, Christensen et al. (2010) im-
prove TEXTRUNNER precision by using deep pars-
ing information via semantic role labelling.
</bodyText>
<sectionHeader confidence="0.948755" genericHeader="method">
2 Relation Guided Bootstrapping
</sectionHeader>
<bodyText confidence="0.999937117647059">
Rather than relying on manually-crafted category
and relation constraints, Relation Guided Bootstrap-
ping (RGB) automatically detects, seeds and boot-
straps open relations between the target categories.
These relations anchor categories together, e.g. IS-
CEOOF and ISFOUNDEROF anchor person and
company, preventing them from drifting into other
categories. Relations can also identify new terms.
We demonstrate that this relation guidance effec-
tively reduces semantic drift, with performance ap-
proaching manually-crafted constraints.
RGB can be applied to any multi-category boot-
strapper, and in these experiments we use WMEB
(McIntosh and Curran, 2008), as shown in Figure 1.
RGB alternates between two phases of WMEB, one
for terms and the other for relations, with a one-off
relation discovery phase in between.
</bodyText>
<subsectionHeader confidence="0.930576">
Term Extraction
</subsectionHeader>
<bodyText confidence="0.999930266666667">
The first stage of RGB follows the term extraction
process of WMEB. Each category is initialised by a
set of hand-picked seed terms. In each iteration, a
category’s terms are used to identify candidate pat-
terns that can match the terms in the text. Seman-
tic drift is reduced by forcing the categories to be
mutually exclusive (i.e. patterns must be nominated
by only one category). The remaining patterns are
ranked according to reliability and relevance, and
the top-n patterns are then added to the pattern set.1
The reliability of a pattern for a given category is
the number of extracted terms in the category’s lex-
icon that match the pattern. A pattern’s relevance
weight is defined as the sum of the X2 values be-
tween the pattern (p) and each of the lexicon terms
</bodyText>
<footnote confidence="0.993137">
1In this work, n is set to 5.
</footnote>
<figureCaption confidence="0.999746">
Figure 1: Relation Guided Bootstrapping framework
</figureCaption>
<bodyText confidence="0.998623428571429">
(t): weight(p) = EtET X2(p, t). These metrics are
symmetrical for both candidate terms and pattern.
In WMEB’s term selection phase, a category’s pat-
tern set is used to identify candidate terms. Like the
candidate patterns, terms matching multiple cate-
gories are excluded. The remaining terms are ranked
and the top-n terms are added to the lexicon.
</bodyText>
<sectionHeader confidence="0.533997" genericHeader="method">
Relation Discovery
</sectionHeader>
<bodyText confidence="0.999205722222222">
In CPL (Carlson et al., 2010), a relation is instanti-
ated with manually-crafted seed tuples and patterns.
In RGB, the relations and their seeds are automati-
cally identified in relation discovery. Relation dis-
covery is only performed once after the first 20 iter-
ations of term extraction, which ensures the lexicons
have adequate coverage to form potential relations.
Each ordered pair of categories (C1, C2) = R1,2
is checked for open (not pre-defined) relations be-
tween C1 and C2. This check removes all pairs of
terms, tuples (t1, t2) E C1 x C2 with freq(t1, t2) &lt;
5 and a cooccurrence score X2(t1, t2) &lt; 0.2 If R1,2
has fewer than 10 remaining tuples, it is discarded.
The tuples for R1,2 are then used to find its ini-
tial set of relation patterns. Each pattern must match
more than one tuple and must be mutually exclusive
between the relations. If fewer than n relation pat-
terns are found for R1,2, it is discarded. At this stage
</bodyText>
<footnote confidence="0.9151885">
2This cut-off is used as the x2 statistic is sensitive to low
frequencies.
</footnote>
<figure confidence="0.939579545454545">
WMEB
lexicon
➀ Person
get patterns
get patterns
lexicon
Company
➁
Term extraction
get terms
get terms
relation
discovery
WMEB
arg ➀ arg ➁
➀ relation ➁
get patterns
get tuples
Lee Scott, Walmart
Sergey Brin, Google
Joe Bloggs, Walmart
Relation extraction
</figure>
<page confidence="0.639107">
267
</page>
<table confidence="0.9982164">
TYPE 5gm 5gm + 4gm 5gm + DC
Terms 1347 002
Patterns 4 090 412
Tuples 2114 243 3 470 206 14 369 673
Relation Patterns 5 523 473 10 317 703 31867 250
</table>
<tableCaption confidence="0.9999">
Table 1: Statistics of three filtered MEDLINE datasets
</tableCaption>
<bodyText confidence="0.999655909090909">
we have identified the open relations that link cate-
gories together and their initial extraction patterns.
Using the initial relation patterns, the top-n mu-
tually exclusive seed tuples are identified for the re-
lation R1,2. In CPL, these tuple seeds are manually
crafted. Note that R1,2 can represent multiple rela-
tions between C1 and C2, which may not apply to all
of the seeds, e.g. isCeoOf and isEmployedBy.
We discover two types of relations, inter-category
relations where C1 =� C2, and intra-category rela-
tions where C1 = C2.
</bodyText>
<sectionHeader confidence="0.639151" genericHeader="method">
Relation Extraction
</sectionHeader>
<bodyText confidence="0.99997125">
The relation extraction phase involves running
WMEB over tuples rather than terms. If multiple re-
lations are found, e.g. R1,2 and R2,3, these are boot-
strapped simultaneously, competing with each other
for tuples and relation patterns. Mutual exclusion
constraints between the relations are also forced.
In each iteration, a relation’s set of tuples is used
to identify candidate relation patterns, as for term
extraction. The top-n non-overlapping patterns are
extracted for each relation, and are used to identify
the top-n candidate tuples. The tuples are scored
similarly to the relation patterns, and any tuple iden-
tified by multiple relations is excluded.
For tuple extraction, a relation R1,2 is constrained
to only consider candidates where either t1 or t2
has previously been extracted into C1 or C2, respec-
tively. To extract a candidate tuple with an unknown
term, the term must also be a valid candidate of its
associated category. That is, the term must match
at least one pattern assigned to the category and not
match patterns assigned to another category.
This type-checking anchors relations to the cat-
egories they link together, limiting their drift into
other relations. It also provides guided term growth
in the categories they link. The growth is “guided”
because the relations define, semantically coher-
ent subregions of the category search spaces. For
example, ISCEOOF defines the subregion ceo
</bodyText>
<table confidence="0.919539533333333">
CAT DESCRIPTION
ANTI Antibodies: MAb IgG IgM rituximab infiiximab
CELL Cells: RBC HUVEC BAEC VSMC SMC
CLNE Cell lines: PC12 CHO HeLa Jurkat COS
DISE Diseases: asthma hepatitis tuberculosis HIV malaria
DRUG Drugs: acetylcholine carbachol heparin penicillin
tetracyclin
FUNC Molecular functions and processes:
kinase ligase acetyltransferase helicase binding
MUTN Mutations: Leiden C677T C282Y 35delG null
PROT Proteins and genes: p53 actin collagen albumin IL-6
SIGN Signs and symptoms: anemia cough fever
hypertension hyperglycemia
TUMR Tumors: lymphoma sarcoma melanoma
neuroblastoma osteosarcoma
</table>
<tableCaption confidence="0.999335">
Table 2: The MEDLINE semantic categories
</tableCaption>
<bodyText confidence="0.918947">
within person. This guidance reduces semantic
drift.
</bodyText>
<sectionHeader confidence="0.99921" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999696428571428">
To compare the effectiveness of RGB we consider
the task of extracting biomedical semantic lexi-
cons, building on the work of McIntosh and Curran
(2008). Note however the method is equally appli-
cable to any corpus and set of semantic categories.
The corpus consists of approximately 18.5 mil-
lion MEDLINE abstracts (up to Nov 2009). The text
was tokenised and POS-tagged using bio-specific
NLP tools (Grover et al., 2006), and parsed using
the biomedical C&amp;C CCG parser (Rimell and Clark,
2009; Clark and Curran, 2007).
The term extraction data is formed from the raw
5-grams (t1, t2, t3, t4, t5), where the set of candi-
date terms correspond to the middle tokens (t3) and
the patterns are formed from the surrounding tokens
(t1, t2, t4, t5). The relation extraction data is also
formed from the 5-grams. The candidate tuples cor-
respond to the tokens (t1, t5) and the patterns are
formed from the intervening tokens (t2, t3, t4).
The second relation dataset (5gm + 4gm), also in-
cludes length 2 patterns formed from 4-grams. The
final relation dataset (5gm + DC) includes depen-
dency chains up to length 5 as the patterns between
terms (Greenwood et al., 2005). These chains are
formed using the Stanford dependencies generated
by the Rimell and Clark (2009) parser. All candi-
dates occurring less than 10 times were filtered. The
sizes of the resulting datasets are shown in Table 1.
</bodyText>
<page confidence="0.994828">
268
</page>
<table confidence="0.999129444444444">
1-500 501-1000 1-1000
WMEB 76.1 56.4 66.3
+negative 86.9 68.7 77.8
intra-RGB 75.7 62.7 69.2
+negative 87.4 72.4 79.9
inter-RGB 80.5 69.9 75.1
+negative 87.7 76.4 82.0
mixed-RGB 74.7 69.9 72.3
+negative 87.9 73.5 80.7
</table>
<tableCaption confidence="0.999916">
Table 3: Performance comparison of WMEB and RGB
</tableCaption>
<bodyText confidence="0.9999631">
We follow McIntosh and Curran (2009) in us-
ing the 10 biomedical semantic categories and
their hand-picked seeds in Table 2, and manu-
ally crafted negative categories: amino acid,
animal, body part and organism. Our eval-
uation process involved manually judging each ex-
tracted term and we calculate the average precision
of the top-1000 terms over the 10 target categories.
We do not calculate recall, due to the open-ended
nature of the categories.
</bodyText>
<sectionHeader confidence="0.999591" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999475869565218">
Table 3 compares the performance of WMEB and
RGB, with and without the negative categories. For
RGB, we compare intra-, inter- and mixed relation
types, and use the 5gm format of tuples and relation
patterns. In WMEB, drift dominates in the later iter-
ations with -19% precision drop between the first
and last 500 terms. The manually-crafted negative
categories give a substantial boost in precision on
both the first and last 500 terms (+11.5% overall).
Over the top 1000 terms, RGB significantly out-
performs the corresponding WMEB with and with-
out negative categories (p &lt; 0.05).3 In particu-
lar, inter-RGB significantly improves upon WMEB
with no negative categories (501-1000:+13.5%,
1-1000:+8.8%). In similar experiments, NEG-
FINDER, used during bootstrapping, was shown to
increase precision by -5% (McIntosh, 2010). Inter-
RGB without negatives approaches the precision of
WMEB with the negatives, trailing only by 2.7%
overall. This demonstrates that RGB effectively re-
duces the reliance on manually-crafted negative cat-
egories for lexicon bootstrapping.
The use of intra-category relations was far less
</bodyText>
<footnote confidence="0.963776">
3Significance was tested using intensive randomisation tests.
</footnote>
<table confidence="0.999474857142857">
INTER-RGB 1-500 501-1000 1-1000
5gm 80.5 69.9 75.1
+negative 87.7 76.4 82.0
5gm + 4gm 79.6 71.5 75.5
+negative 87.7 76.1 81.9
5gm + DC 77.2 70.1 73.5
+negative 86.6 80.2 83.5
</table>
<tableCaption confidence="0.999959">
Table 4: Comparison of different relation pattern types
</tableCaption>
<bodyText confidence="0.99993475">
effective than inter-category relations, and the com-
bination of intra- and inter- was less effective than
just using inter-category relations. In intra-RGB the
categories are more susceptible to single-category
drift. The additional constraints provided by anchor-
ing two categories appear to make inter-RGB less
susceptible to drift. Many intra-category relations
represent listings commonly identified by conjunc-
tions. However, these patterns are identified by mul-
tiple intra-category relations and are excluded.
Through manual inspection of inter-RGB’s tuples
and patterns, we identified numerous meaningful re-
lations, such as isExpressedIn(prot, cell).
Relations like this helped to reduce semantic drift
within the CELL lexicon by up to 23%.
Table 4 compares the effect of different relation
pattern representations on the performance of inter-
RGB. The 5gm+4gm data, which doubles the num-
ber of possible candidate relation patterns, performs
similarly to the 5gm representation. Adding depen-
dency chains decreased and increased precision de-
pending on whether negative categories were used.
In Wu and Weld (2010), the performance of an
OPENIE system was significantly improved by us-
ing patterns formed from dependency parses. How-
ever in our DC experiments, the earlier bootstrap-
ping iterations were less precise than the simple
5gm+4gm and 5gm representations. Since the
chains can be as short as two dependencies, some
of these patterns may not be specific enough. These
results demonstrate that useful open relations can be
represented using only n-grams.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9992775">
In this paper, we have proposed Relation Guided
Bootstrapping (RGB), an unsupervised approach to
discovering and seeding open relations to constrain
semantic lexicon bootstrapping.
</bodyText>
<page confidence="0.993071">
269
</page>
<bodyText confidence="0.9996899">
Previous work used manually-crafted lexical and
relation constraints to improve relation extraction
(Carlson et al., 2010). We turn this idea on its head,
by using open relation extraction to provide con-
straints for lexicon bootstrapping, and automatically
discover the open relations and their seeds from the
expanding bootstrapped lexicons.
RGB effectively reduces semantic drift delivering
performance comparable to state-of-the-art systems
that rely on manually-crafted negative constraints.
</bodyText>
<sectionHeader confidence="0.994731" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997141">
We would like to thank Dr Cassie Thornley, our sec-
ond evaluator, and the reviewers for their helpful
feedback. NICTA is funded by the Australian Gov-
ernment as represented by the Department of Broad-
band, Communications and the Digital Economy
and the Australian Research Council through the
ICT Centre of Excellence program. This work has
been supported by the Australian Research Council
under Discovery Project DP1097291 and the Capital
Markets Cooperative Research Centre.
</bodyText>
<sectionHeader confidence="0.998107" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999757759493671">
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 2670–2676, Hyderabad, India.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining,
pages 101–110, New York, USA.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2010. Semantic role labeling for
open information extraction. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, pages 52–60, Los Angeles, California, USA, June.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493–
552.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 172–180, Melbourne, Australia.
Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk
Harkema, and Angus Roberts. 2005. Automatically
acquiring a linguistically motivated genic interaction
extraction system. In Proceedings of the 4th Learn-
ing Language in Logic Workshop, pages 46–52, Bonn,
Germany.
Claire Grover, Michael Matthews, and Richard Tobin.
2006. Tools to address the interdependence between
tokenisation and standoff annotation. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 19–26, Trento, Italy.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop, pages 97–105, Hobart, Australia.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional similar-
ity. In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, pages 396–404, Suntec, Singapore, Au-
gust.
Tara McIntosh. 2010. Unsupervised discovery of neg-
ative categories in lexicon bootstrapping. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 356–365,
Boston, USA.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In Proceedings of the 16th National Conference
on Artificial Intelligence and the 11th Innovative Ap-
plications of Artificial Intelligence Conference, pages
474–479, Orlando, USA.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117–124,
Providence, USA.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, pages 852–865.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the 48th
Annual Meeting of the Association of Computational
Linguistics, pages 118–127, Uppsala, Sweden.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names. In
Proceedings of the 19th International Conference on
Computational Linguistics, pages 1135–1141, Taipei,
Taiwan.
</reference>
<page confidence="0.996818">
270
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.386480">
<title confidence="0.998703">Relation Guided Bootstrapping of Semantic Lexicons</title>
<author confidence="0.53555">R</author>
<affiliation confidence="0.939513666666667">Victoria Research Lab of Information Technologies Dept. of Computer Science and Software Engineering The University of The University of Melbourne</affiliation>
<abstract confidence="0.982412666666667">nlp@taramcintosh.org james@it.usyd.edu.au lars@yencken.org tb@ldwin.net Abstract State-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2670--2676</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="3382" citStr="Banko et al., 2007" startWordPosition="476" endWordPosition="479"> Negative examples such as ISCEOOF(Sergey Brin, Google) are also introduced to clarify boundary conditions. CPL employs a large number of these manually-crafted constraints to improve precision at the expense of recall (only 18 ISCEOOF instances were extracted). In our approach, we exploit open relation bootstrapping to minimise semantic drift, without any manual seeding of relations or pre-defined category lexicon combinations. Orthogonal to these seeded and constraint-based methods is the relation-independent Open Information Extraction (OPENIE) paradigm. OPENIE systems, such as TEXTRUNNER (Banko et al., 2007), define neither lexicon categories nor predefined relationships. They extract relation tuples by exploit266 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 266–270, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ing broad syntactic patterns that are likely to indicate relations. This enables the extraction of interesting and unanticipated relations from text. However these patterns are often too broad, resulting in the extraction of tuples that do not represent relations at all. As a result, heavy</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 2670–2676, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Richard C Wang</author>
<author>Estevam R Hruschka</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>101--110</pages>
<location>New York, USA.</location>
<contexts>
<context position="2341" citStr="Carlson et al. (2010)" startWordPosition="317" endWordPosition="320">daries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of human expertise into an otherwise unsupervised framework. McIntosh (2010) made some progress towards automatically learning useful negative categories during bootstrapping. In this work we identify an unsupervised source of semantic constraints inspired by the Coupled Pattern Learner (CPL, Carlson et al. (2010)). In CPL, relation bootstrapping is coupled with lexicon bootstrapping in order to control semantic drift in the target relation’s arguments. Semantic constraints on categories and relations are manually crafted in CPL. For example, a candidate of the relation ISCEOOF will only be extracted if its arguments can be extracted into the ceo and company lexicons and a ceo is constrained to not be a celebrity or politician. Negative examples such as ISCEOOF(Sergey Brin, Google) are also introduced to clarify boundary conditions. CPL employs a large number of these manually-crafted constraints to im</context>
<context position="6296" citStr="Carlson et al., 2010" startWordPosition="933" endWordPosition="936">at match the pattern. A pattern’s relevance weight is defined as the sum of the X2 values between the pattern (p) and each of the lexicon terms 1In this work, n is set to 5. Figure 1: Relation Guided Bootstrapping framework (t): weight(p) = EtET X2(p, t). These metrics are symmetrical for both candidate terms and pattern. In WMEB’s term selection phase, a category’s pattern set is used to identify candidate terms. Like the candidate patterns, terms matching multiple categories are excluded. The remaining terms are ranked and the top-n terms are added to the lexicon. Relation Discovery In CPL (Carlson et al., 2010), a relation is instantiated with manually-crafted seed tuples and patterns. In RGB, the relations and their seeds are automatically identified in relation discovery. Relation discovery is only performed once after the first 20 iterations of term extraction, which ensures the lexicons have adequate coverage to form potential relations. Each ordered pair of categories (C1, C2) = R1,2 is checked for open (not pre-defined) relations between C1 and C2. This check removes all pairs of terms, tuples (t1, t2) E C1 x C2 with freq(t1, t2) &lt; 5 and a cooccurrence score X2(t1, t2) &lt; 0.2 If R1,2 has fewer </context>
<context position="15820" citStr="Carlson et al., 2010" startWordPosition="2445" endWordPosition="2448">the earlier bootstrapping iterations were less precise than the simple 5gm+4gm and 5gm representations. Since the chains can be as short as two dependencies, some of these patterns may not be specific enough. These results demonstrate that useful open relations can be represented using only n-grams. 5 Conclusion In this paper, we have proposed Relation Guided Bootstrapping (RGB), an unsupervised approach to discovering and seeding open relations to constrain semantic lexicon bootstrapping. 269 Previous work used manually-crafted lexical and relation constraints to improve relation extraction (Carlson et al., 2010). We turn this idea on its head, by using open relation extraction to provide constraints for lexicon bootstrapping, and automatically discover the open relations and their seeds from the expanding bootstrapped lexicons. RGB effectively reduces semantic drift delivering performance comparable to state-of-the-art systems that rely on manually-crafted negative constraints. Acknowledgements We would like to thank Dr Cassie Thornley, our second evaluator, and the reviewers for their helpful feedback. NICTA is funded by the Australian Government as represented by the Department of Broadband, Commun</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Hruschka, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka, Jr., and Tom M. Mitchell. 2010. Coupled semi-supervised learning for information extraction. In Proceedings of the Third ACM International Conference on Web Search and Data Mining, pages 101–110, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Semantic role labeling for open information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading,</booktitle>
<pages>52--60</pages>
<location>Los Angeles, California, USA,</location>
<contexts>
<context position="4096" citStr="Christensen et al. (2010)" startWordPosition="581" endWordPosition="584">uples by exploit266 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 266–270, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ing broad syntactic patterns that are likely to indicate relations. This enables the extraction of interesting and unanticipated relations from text. However these patterns are often too broad, resulting in the extraction of tuples that do not represent relations at all. As a result, heavy (supervised) postprocessing or use of supervised information is necessary. For example, Christensen et al. (2010) improve TEXTRUNNER precision by using deep parsing information via semantic role labelling. 2 Relation Guided Bootstrapping Rather than relying on manually-crafted category and relation constraints, Relation Guided Bootstrapping (RGB) automatically detects, seeds and bootstraps open relations between the target categories. These relations anchor categories together, e.g. ISCEOOF and ISFOUNDEROF anchor person and company, preventing them from drifting into other categories. Relations can also identify new terms. We demonstrate that this relation guidance effectively reduces semantic drift, wit</context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2010</marker>
<rawString>Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2010. Semantic role labeling for open information extraction. In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 52–60, Los Angeles, California, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with ccg and loglinear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<pages>552</pages>
<contexts>
<context position="10935" citStr="Clark and Curran, 2007" startWordPosition="1685" endWordPosition="1688"> The MEDLINE semantic categories within person. This guidance reduces semantic drift. 3 Experimental Setup To compare the effectiveness of RGB we consider the task of extracting biomedical semantic lexicons, building on the work of McIntosh and Curran (2008). Note however the method is equally applicable to any corpus and set of semantic categories. The corpus consists of approximately 18.5 million MEDLINE abstracts (up to Nov 2009). The text was tokenised and POS-tagged using bio-specific NLP tools (Grover et al., 2006), and parsed using the biomedical C&amp;C CCG parser (Rimell and Clark, 2009; Clark and Curran, 2007). The term extraction data is formed from the raw 5-grams (t1, t2, t3, t4, t5), where the set of candidate terms correspond to the middle tokens (t3) and the patterns are formed from the surrounding tokens (t1, t2, t4, t5). The relation extraction data is also formed from the 5-grams. The candidate tuples correspond to the tokens (t1, t5) and the patterns are formed from the intervening tokens (t2, t3, t4). The second relation dataset (5gm + 4gm), also includes length 2 patterns formed from 4-grams. The final relation dataset (5gm + DC) includes dependency chains up to length 5 as the patterns</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with ccg and loglinear models. Computational Linguistics, 33(4):493– 552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Tara Murphy</author>
<author>Bernhard Scholz</author>
</authors>
<title>Minimising semantic drift with mutual exclusion bootstrapping.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>172--180</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="1324" citStr="Curran et al., 2007" startWordPosition="174" endWordPosition="177">ships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories. 1 Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al.</context>
</contexts>
<marker>Curran, Murphy, Scholz, 2007</marker>
<rawString>James R. Curran, Tara Murphy, and Bernhard Scholz. 2007. Minimising semantic drift with mutual exclusion bootstrapping. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages 172–180, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Greenwood</author>
<author>Mark Stevenson</author>
<author>Yikun Guo</author>
<author>Henk Harkema</author>
<author>Angus Roberts</author>
</authors>
<title>Automatically acquiring a linguistically motivated genic interaction extraction system.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th Learning Language in Logic Workshop,</booktitle>
<pages>46--52</pages>
<location>Bonn, Germany.</location>
<contexts>
<context position="11574" citStr="Greenwood et al., 2005" startWordPosition="1798" endWordPosition="1801">ction data is formed from the raw 5-grams (t1, t2, t3, t4, t5), where the set of candidate terms correspond to the middle tokens (t3) and the patterns are formed from the surrounding tokens (t1, t2, t4, t5). The relation extraction data is also formed from the 5-grams. The candidate tuples correspond to the tokens (t1, t5) and the patterns are formed from the intervening tokens (t2, t3, t4). The second relation dataset (5gm + 4gm), also includes length 2 patterns formed from 4-grams. The final relation dataset (5gm + DC) includes dependency chains up to length 5 as the patterns between terms (Greenwood et al., 2005). These chains are formed using the Stanford dependencies generated by the Rimell and Clark (2009) parser. All candidates occurring less than 10 times were filtered. The sizes of the resulting datasets are shown in Table 1. 268 1-500 501-1000 1-1000 WMEB 76.1 56.4 66.3 +negative 86.9 68.7 77.8 intra-RGB 75.7 62.7 69.2 +negative 87.4 72.4 79.9 inter-RGB 80.5 69.9 75.1 +negative 87.7 76.4 82.0 mixed-RGB 74.7 69.9 72.3 +negative 87.9 73.5 80.7 Table 3: Performance comparison of WMEB and RGB We follow McIntosh and Curran (2009) in using the 10 biomedical semantic categories and their hand-picked s</context>
</contexts>
<marker>Greenwood, Stevenson, Guo, Harkema, Roberts, 2005</marker>
<rawString>Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk Harkema, and Angus Roberts. 2005. Automatically acquiring a linguistically motivated genic interaction extraction system. In Proceedings of the 4th Learning Language in Logic Workshop, pages 46–52, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Michael Matthews</author>
<author>Richard Tobin</author>
</authors>
<title>Tools to address the interdependence between tokenisation and standoff annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Workshop on NLP and XML: MultiDimensional Markup in Natural Language Processing,</booktitle>
<pages>pages</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="10838" citStr="Grover et al., 2006" startWordPosition="1669" endWordPosition="1672">nsion hyperglycemia TUMR Tumors: lymphoma sarcoma melanoma neuroblastoma osteosarcoma Table 2: The MEDLINE semantic categories within person. This guidance reduces semantic drift. 3 Experimental Setup To compare the effectiveness of RGB we consider the task of extracting biomedical semantic lexicons, building on the work of McIntosh and Curran (2008). Note however the method is equally applicable to any corpus and set of semantic categories. The corpus consists of approximately 18.5 million MEDLINE abstracts (up to Nov 2009). The text was tokenised and POS-tagged using bio-specific NLP tools (Grover et al., 2006), and parsed using the biomedical C&amp;C CCG parser (Rimell and Clark, 2009; Clark and Curran, 2007). The term extraction data is formed from the raw 5-grams (t1, t2, t3, t4, t5), where the set of candidate terms correspond to the middle tokens (t3) and the patterns are formed from the surrounding tokens (t1, t2, t4, t5). The relation extraction data is also formed from the 5-grams. The candidate tuples correspond to the tokens (t1, t5) and the patterns are formed from the intervening tokens (t2, t3, t4). The second relation dataset (5gm + 4gm), also includes length 2 patterns formed from 4-grams</context>
</contexts>
<marker>Grover, Matthews, Tobin, 2006</marker>
<rawString>Claire Grover, Michael Matthews, and Richard Tobin. 2006. Tools to address the interdependence between tokenisation and standoff annotation. In Proceedings of the 5th Workshop on NLP and XML: MultiDimensional Markup in Natural Language Processing, pages 19–26, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara McIntosh</author>
<author>James R Curran</author>
</authors>
<title>Weighted mutual exclusion bootstrapping for domain independent lexicon and template acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop,</booktitle>
<pages>97--105</pages>
<location>Hobart, Australia.</location>
<contexts>
<context position="1496" citStr="McIntosh and Curran, 2008" startWordPosition="200" endWordPosition="203">ng negative categories. 1 Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of human expertise into an otherwise unsupervised framework. McIntosh </context>
<context position="4871" citStr="McIntosh and Curran, 2008" startWordPosition="689" endWordPosition="692">anually-crafted category and relation constraints, Relation Guided Bootstrapping (RGB) automatically detects, seeds and bootstraps open relations between the target categories. These relations anchor categories together, e.g. ISCEOOF and ISFOUNDEROF anchor person and company, preventing them from drifting into other categories. Relations can also identify new terms. We demonstrate that this relation guidance effectively reduces semantic drift, with performance approaching manually-crafted constraints. RGB can be applied to any multi-category bootstrapper, and in these experiments we use WMEB (McIntosh and Curran, 2008), as shown in Figure 1. RGB alternates between two phases of WMEB, one for terms and the other for relations, with a one-off relation discovery phase in between. Term Extraction The first stage of RGB follows the term extraction process of WMEB. Each category is initialised by a set of hand-picked seed terms. In each iteration, a category’s terms are used to identify candidate patterns that can match the terms in the text. Semantic drift is reduced by forcing the categories to be mutually exclusive (i.e. patterns must be nominated by only one category). The remaining patterns are ranked accord</context>
<context position="10570" citStr="McIntosh and Curran (2008)" startWordPosition="1625" endWordPosition="1628">eparin penicillin tetracyclin FUNC Molecular functions and processes: kinase ligase acetyltransferase helicase binding MUTN Mutations: Leiden C677T C282Y 35delG null PROT Proteins and genes: p53 actin collagen albumin IL-6 SIGN Signs and symptoms: anemia cough fever hypertension hyperglycemia TUMR Tumors: lymphoma sarcoma melanoma neuroblastoma osteosarcoma Table 2: The MEDLINE semantic categories within person. This guidance reduces semantic drift. 3 Experimental Setup To compare the effectiveness of RGB we consider the task of extracting biomedical semantic lexicons, building on the work of McIntosh and Curran (2008). Note however the method is equally applicable to any corpus and set of semantic categories. The corpus consists of approximately 18.5 million MEDLINE abstracts (up to Nov 2009). The text was tokenised and POS-tagged using bio-specific NLP tools (Grover et al., 2006), and parsed using the biomedical C&amp;C CCG parser (Rimell and Clark, 2009; Clark and Curran, 2007). The term extraction data is formed from the raw 5-grams (t1, t2, t3, t4, t5), where the set of candidate terms correspond to the middle tokens (t3) and the patterns are formed from the surrounding tokens (t1, t2, t4, t5). The relatio</context>
</contexts>
<marker>McIntosh, Curran, 2008</marker>
<rawString>Tara McIntosh and James R. Curran. 2008. Weighted mutual exclusion bootstrapping for domain independent lexicon and template acquisition. In Proceedings of the Australasian Language Technology Association Workshop, pages 97–105, Hobart, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara McIntosh</author>
<author>James R Curran</author>
</authors>
<title>Reducing semantic drift with bagging and distributional similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,</booktitle>
<pages>396--404</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="12103" citStr="McIntosh and Curran (2009)" startWordPosition="1885" endWordPosition="1888"> includes dependency chains up to length 5 as the patterns between terms (Greenwood et al., 2005). These chains are formed using the Stanford dependencies generated by the Rimell and Clark (2009) parser. All candidates occurring less than 10 times were filtered. The sizes of the resulting datasets are shown in Table 1. 268 1-500 501-1000 1-1000 WMEB 76.1 56.4 66.3 +negative 86.9 68.7 77.8 intra-RGB 75.7 62.7 69.2 +negative 87.4 72.4 79.9 inter-RGB 80.5 69.9 75.1 +negative 87.7 76.4 82.0 mixed-RGB 74.7 69.9 72.3 +negative 87.9 73.5 80.7 Table 3: Performance comparison of WMEB and RGB We follow McIntosh and Curran (2009) in using the 10 biomedical semantic categories and their hand-picked seeds in Table 2, and manually crafted negative categories: amino acid, animal, body part and organism. Our evaluation process involved manually judging each extracted term and we calculate the average precision of the top-1000 terms over the 10 target categories. We do not calculate recall, due to the open-ended nature of the categories. 4 Results and Discussion Table 3 compares the performance of WMEB and RGB, with and without the negative categories. For RGB, we compare intra-, inter- and mixed relation types, and use the</context>
</contexts>
<marker>McIntosh, Curran, 2009</marker>
<rawString>Tara McIntosh and James R. Curran. 2009. Reducing semantic drift with bagging and distributional similarity. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, pages 396–404, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara McIntosh</author>
</authors>
<title>Unsupervised discovery of negative categories in lexicon bootstrapping.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>356--365</pages>
<location>Boston, USA.</location>
<contexts>
<context position="2102" citStr="McIntosh (2010)" startWordPosition="284" endWordPosition="285">an, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of human expertise into an otherwise unsupervised framework. McIntosh (2010) made some progress towards automatically learning useful negative categories during bootstrapping. In this work we identify an unsupervised source of semantic constraints inspired by the Coupled Pattern Learner (CPL, Carlson et al. (2010)). In CPL, relation bootstrapping is coupled with lexicon bootstrapping in order to control semantic drift in the target relation’s arguments. Semantic constraints on categories and relations are manually crafted in CPL. For example, a candidate of the relation ISCEOOF will only be extracted if its arguments can be extracted into the ceo and company lexicons </context>
<context position="13358" citStr="McIntosh, 2010" startWordPosition="2085" endWordPosition="2086">rns. In WMEB, drift dominates in the later iterations with -19% precision drop between the first and last 500 terms. The manually-crafted negative categories give a substantial boost in precision on both the first and last 500 terms (+11.5% overall). Over the top 1000 terms, RGB significantly outperforms the corresponding WMEB with and without negative categories (p &lt; 0.05).3 In particular, inter-RGB significantly improves upon WMEB with no negative categories (501-1000:+13.5%, 1-1000:+8.8%). In similar experiments, NEGFINDER, used during bootstrapping, was shown to increase precision by -5% (McIntosh, 2010). InterRGB without negatives approaches the precision of WMEB with the negatives, trailing only by 2.7% overall. This demonstrates that RGB effectively reduces the reliance on manually-crafted negative categories for lexicon bootstrapping. The use of intra-category relations was far less 3Significance was tested using intensive randomisation tests. INTER-RGB 1-500 501-1000 1-1000 5gm 80.5 69.9 75.1 +negative 87.7 76.4 82.0 5gm + 4gm 79.6 71.5 75.5 +negative 87.7 76.1 81.9 5gm + DC 77.2 70.1 73.5 +negative 86.6 80.2 83.5 Table 4: Comparison of different relation pattern types effective than int</context>
</contexts>
<marker>McIntosh, 2010</marker>
<rawString>Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 356–365, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications of Artificial Intelligence Conference,</booktitle>
<pages>474--479</pages>
<location>Orlando, USA.</location>
<contexts>
<context position="1202" citStr="Riloff and Jones, 1999" startWordPosition="155" endWordPosition="158">ledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories. 1 Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are </context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications of Artificial Intelligence Conference, pages 474–479, Orlando, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Jessica Shepherd</author>
</authors>
<title>A corpus-based approach for building semantic lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<location>Providence, USA.</location>
<contexts>
<context position="1032" citStr="Riloff and Shepherd, 1997" startWordPosition="126" endWordPosition="129">rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories. 1 Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative cate</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Ellen Riloff and Jessica Shepherd. 1997. A corpus-based approach for building semantic lexicons. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 117–124, Providence, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Porting a lexicalized-grammar parser to the biomedical domain.</title>
<date>2009</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>852--865</pages>
<contexts>
<context position="10910" citStr="Rimell and Clark, 2009" startWordPosition="1681" endWordPosition="1684">ma osteosarcoma Table 2: The MEDLINE semantic categories within person. This guidance reduces semantic drift. 3 Experimental Setup To compare the effectiveness of RGB we consider the task of extracting biomedical semantic lexicons, building on the work of McIntosh and Curran (2008). Note however the method is equally applicable to any corpus and set of semantic categories. The corpus consists of approximately 18.5 million MEDLINE abstracts (up to Nov 2009). The text was tokenised and POS-tagged using bio-specific NLP tools (Grover et al., 2006), and parsed using the biomedical C&amp;C CCG parser (Rimell and Clark, 2009; Clark and Curran, 2007). The term extraction data is formed from the raw 5-grams (t1, t2, t3, t4, t5), where the set of candidate terms correspond to the middle tokens (t3) and the patterns are formed from the surrounding tokens (t1, t2, t4, t5). The relation extraction data is also formed from the 5-grams. The candidate tuples correspond to the tokens (t1, t5) and the patterns are formed from the intervening tokens (t2, t3, t4). The second relation dataset (5gm + 4gm), also includes length 2 patterns formed from 4-grams. The final relation dataset (5gm + DC) includes dependency chains up to</context>
</contexts>
<marker>Rimell, Clark, 2009</marker>
<rawString>Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-grammar parser to the biomedical domain. Journal of Biomedical Informatics, pages 852–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>118--127</pages>
<location>Uppsala,</location>
<contexts>
<context position="15054" citStr="Wu and Weld (2010)" startWordPosition="2332" endWordPosition="2335">and are excluded. Through manual inspection of inter-RGB’s tuples and patterns, we identified numerous meaningful relations, such as isExpressedIn(prot, cell). Relations like this helped to reduce semantic drift within the CELL lexicon by up to 23%. Table 4 compares the effect of different relation pattern representations on the performance of interRGB. The 5gm+4gm data, which doubles the number of possible candidate relation patterns, performs similarly to the 5gm representation. Adding dependency chains decreased and increased precision depending on whether negative categories were used. In Wu and Weld (2010), the performance of an OPENIE system was significantly improved by using patterns formed from dependency parses. However in our DC experiments, the earlier bootstrapping iterations were less precise than the simple 5gm+4gm and 5gm representations. Since the chains can be as short as two dependencies, some of these patterns may not be specific enough. These results demonstrate that useful open relations can be represented using only n-grams. 5 Conclusion In this paper, we have proposed Relation Guided Bootstrapping (RGB), an unsupervised approach to discovering and seeding open relations to co</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics, pages 118–127, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Winston Lin</author>
<author>Ralph Grishman</author>
</authors>
<title>Unsupervised learning of generalized names.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1135--1141</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1459" citStr="Yangarber et al., 2002" startWordPosition="193" endWordPosition="197">constraints, and manually generating negative categories. 1 Introduction Many approaches to extracting semantic lexicons extend the unsupervised bootstrapping framework (Riloff and Shepherd, 1997). These use a small set of seed examples from the target lexicon to identify contextual patterns which are then used to extract new lexicon items (Riloff and Jones, 1999). Bootstrappers are prone to semantic drift, caused by selection of poor candidate terms or patterns (Curran et al., 2007), which can be reduced by semantically constraining the candidates. Multicategory bootstrappers, such as NOMEN (Yangarber et al., 2002) and WMEB (McIntosh and Curran, 2008), reduce semantic drift by extracting multiple categories simultaneously in competition. The inclusion of manually-crafted negative categories to multi-category bootstrappers achieves the best results, by clarifying the boundaries between categories (Yangarber et al., 2002). For example, female names are often bootstrapped with the negative categories flowers (e.g. Rose, Iris) and gem stones (e.g. Ruby, Pearl) (Curran et al., 2007). Unfortunately, negative categories are difficult to design, introducing a substantial amount of human expertise into an otherw</context>
</contexts>
<marker>Yangarber, Lin, Grishman, 2002</marker>
<rawString>Roman Yangarber, Winston Lin, and Ralph Grishman. 2002. Unsupervised learning of generalized names. In Proceedings of the 19th International Conference on Computational Linguistics, pages 1135–1141, Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>