<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9929065">
Case markers and Morphology: Addressing the crux of the fluency
problem in English-Hindi SMT
</title>
<author confidence="0.9387475">
Ananthakrishnan Ramanathan, Hansraj Choudhary
Avishek Ghosh, Pushpak Bhattacharyya
</author>
<affiliation confidence="0.996435">
Department of Computer Science and Engineering
Indian Institute of Technology Bombay
</affiliation>
<author confidence="0.448664">
Powai, Mumbai-400076
</author>
<affiliation confidence="0.581945">
India
</affiliation>
<email confidence="0.969616">
{anand, hansraj, avis, pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.994265" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976212121212">
We report in this paper our work on
accurately generating case markers and
suffixes in English-to-Hindi SMT. Hindi
is a relatively free word-order language,
and makes use of a comparatively richer
set of case markers and morphological
suffixes for correct meaning representa-
tion. From our experience of large-scale
English-Hindi MT, we are convinced that
fluency and fidelity in the Hindi output get
an order of magnitude facelift if accurate
case markers and suffixes are produced.
Now, the moot question is: what entity on
the English side encodes the information
contained in case markers and suffixes on
the Hindi side? Our studies of correspon-
dences in the two languages show that case
markers and suffixes in Hindi are predom-
inantly determined by the combination of
suffixes and semantic relations on the En-
glish side. We, therefore, augment the
aligned corpus of the two languages, with
the correspondence of English suffixes and
semantic relations with Hindi suffixes and
case markers. Our results on 400 test
sentences, translated using an SMT sys-
tem trained on around 13000 parallel sen-
tences, show that suffix + semantic rela-
tion —* case marker/suffix is a very useful
translation factor, in the sense of making a
significant difference to output quality as
indicated by subjective evaluation as well
as BLEU scores.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999832860465116">
Two fundamental problems in applying statistical
machine translation (SMT) techniques to English-
Hindi (and generally to Indian language) MT are:
i) the wide syntactic divergence between the lan-
guage pairs, and ii) the richer morphology and
case marking of Hindi compared to English. The
first problem manifests itself in poor word-order in
the output translations, while the second one leads
to incorrect inflections (word-endings) and case
marking. Being a free word-order language, Hindi
suffers badly when morphology and case markers
are incorrect.
To solve the former, word-order related, prob-
lem, we use a preprocessing technique, which we
have discussed in (Ananthakrishnan et al., 2008).
This procedure is similar to what is suggested in
(Collins et al., 2005) and (Wang, 2007), and re-
sults in the input sentence being reordered to fol-
low Hindi structure.
The focus of this paper, however, is on the
thorny problem of generating case markers and
morphology. It is recognized that translating from
poor to rich morphology is a challenge (Avramidis
and Koehn, 2008) that calls for deeper linguistic
analysis to be part of the translation process. Such
analysis is facilitated by factored models (Koehn
et al., 2007), which provide a framework for incor-
porating lemmas, suffixes, POS tags, and any other
linguistic factors in a log-linear model for phrase-
based SMT. In this paper, we motivate a factoriza-
tion well-suited to English-Hindi translation. The
factorization uses semantic relations and suffixes
to generate inflections and case markers. Our ex-
periments include two different kinds of semantic
relations, namely, dependency relations provided
by the Stanford parser, and the deeper semantic
roles (agent, patient, etc.) provided by the univer-
sal networking language (UNL). Our experiments
show that the use of semantic relations and syntac-
tic reordering leads to substantially better quality
translation. The use of even moderately accurate
semantic relations has an especially salubrious ef-
fect on fluency.
</bodyText>
<page confidence="0.948938">
800
</page>
<note confidence="0.996936">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 800–808,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.999002" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999937276595745">
There have been quite a few attempts at includ-
ing morphological information within statistical
MT. NieBen and Ney (2004) show that the use of
morpho-syntactic information drastically reduces
the need for bilingual training data. Popovic and
Ney (2006) report the use of morphological and
syntactic restructuring information for Spanish-
English and Serbian-English translation.
Koehn and Hoang (2007) propose factored
translation models that combine feature functions
to handle syntactic, morphological, and other lin-
guistic information in a log-linear model. This
work also describes experiments in translating
from English to German, Spanish, and Czech, in-
cluding the use of morphological factors.
Avramidis and Koehn (2008) report work on
translating from poor to rich morphology, namely,
English to Greek and Czech translation. They use
factored models with case and verb conjugation
related factors determined by heuristics on parse
trees. The factors are used only on the source side,
and not on the target side.
To handle syntactic differences,
Melamed (2004) proposes methods based on
tree-to-tree mappings. Imamura et al. (2005)
present a similar method that achieves significant
improvements over a phrase-based baseline model
for Japanese-English translation.
Another method for handling syntactic differ-
ences is preprocessing, which is especially perti-
nent when the target language does not have pars-
ing tools. These algorithms attempt to recon-
cile the word-order differences between the source
and target language sentences by reordering the
source language data prior to the SMT training
and decoding cycles. NieBen and Ney (2004) pro-
pose some restructuring steps for German-English
SMT. Popovic and Ney (2006) report the use
of simple local transformation rules for Spanish-
English and Serbian-English translation. Collins
et al. (2005) propose German clause restructur-
ing to improve German-English SMT, while Wang
et al. (2007) present similar work for Chinese-
English SMT. Our earlier work (Ananthakrishnan
et al., 2008) describes syntactic reordering and
morphological suffix separation for English-Hindi
SMT.
</bodyText>
<sectionHeader confidence="0.986873" genericHeader="method">
3 Motivation
</sectionHeader>
<bodyText confidence="0.986228">
The fundamental differences between English and
Hindi are:
</bodyText>
<listItem confidence="0.998108888888889">
• English follows SVO order, whereas Hindi
follows SOV order
• English uses post-modifiers, whereas Hindi
uses pre-modifiers
• Hindi allows greater freedom in word-order,
identifying constituents through case mark-
ing
• Hindi has a relatively richer system of mor-
phology
</listItem>
<bodyText confidence="0.99996675">
We resolve the first two syntactic differences
by reordering the English sentence to conform to
Hindi word-order in a preprocessing step as de-
scribed in (Ananthakrishnan et al., 2008).
The focus of this paper, however, is on the last
two of these differences, and here we dwell a bit
on why this focus on case markers and morphol-
ogy is crucial to the quality of translation.
</bodyText>
<subsectionHeader confidence="0.999867">
3.1 Case markers
</subsectionHeader>
<bodyText confidence="0.9977105">
While in English, the major constituents of a sen-
tence (subject, object, etc.) can usually be iden-
tified by their position in the sentence, Hindi is a
relatively free word-order language. Constituents
can be moved around in the sentence without im-
pacting the core meaning. For example, the fol-
lowing sentence pair conveys the same meaning
(John saw Mary), albeit with different emphases.
</bodyText>
<equation confidence="0.552315166666667">
qf;;[- 4- 4-Ift 4?r kw
John ne Mary ko dekhaa
John-nom Mary-acc saw
4-Ift 4?r qf;;[- 4- kw
Mary ko John ne dekhaa
Mary-acc John-nom saw
</equation>
<bodyText confidence="0.999815571428571">
The identity of John as the subject and Mary
as the object in both sentences comes from the
case markers 4- (ne – nominative) and 4?r (ko –
accusative). Therefore, even though Hindi is pre-
dominantly SOV in its word-order, correct case
marking is a crucial part of making translations
convey the right meaning.
</bodyText>
<page confidence="0.98693">
801
</page>
<subsectionHeader confidence="0.993826">
3.2 Morphology
</subsectionHeader>
<bodyText confidence="0.924504571428572">
The following examples illustrate the richer mor-
phology of Hindi compared to English:
Oblique case: The plural-marker in the word
“boys” in English is translated as e (e – plural di-
rect) or ao\ (on – plural oblique):
The boys went to school.
lXk� pAWfAlA gy�
ladake paathashaalaa gaye
The boys ate apples.
lXko\ n� s�b KAy-
ladokon ne seba khaaye
Future tense: Future tense in Hindi is marked
on the verb. In the following example, “will go” is
translated as jAy&apos;\g� (jaaenge), with e\g� (enge) as
the future tense marker:
The boys will go to school.
lXk� pAWfAlA jAy�\g�
ladake paathashaalaa jayenge
Causative constructions: The aAyA (aayaa)
suffix indicates causativity:
The boys made them cry.
</bodyText>
<equation confidence="0.402007">
lXko\ n~ u-* zlAyA
ladakon ne unhe rulaayaa
</equation>
<subsectionHeader confidence="0.990999">
3.3 Sparsity
</subsectionHeader>
<bodyText confidence="0.999962863636363">
Using a standard SMT system for English-Hindi
translation will cause severe data sparsity with re-
spect to case marking and morphology.
For example, the fact that the word boys in
oblique case (say, when followed by n� (ne))
should take the form lXko\ (ladakon) will be
learnt only if the correspondence between boys
and lXko\ n� (ladakon ne) exists in the training
corpus. The more general rule that n� (ne) should
be preceded by the oblique case ending ao\ (on)
cannot be learnt. Similarly, the plural form of boys
will be produced only if that form exists in the
training corpus.
Essentially, all morphological forms of a word
and its translations have to exist in the training cor-
pus, and every word has to appear with every pos-
sible case marker, which will require an impossi-
ble amount of training data. Therefore, it is im-
perative to make it possible for the system to learn
general rules for morphology and case marking.
The next section describes our approach to facili-
tating the learning of such rules.
</bodyText>
<sectionHeader confidence="0.990763" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999981416666667">
While translating from a language of moderate
case marking and morphology (English) to one
with relatively richer case marking and morphol-
ogy (Hindi), we are faced with the problem of ex-
tracting information from the source language sen-
tence, transferring the information onto the target
side, and translating this information into the ap-
propriate case markers and morphological affixes.
The key bits of information for us are suffixes
and semantic relations, and the vehicle that trans-
fers and translates the information is the factored
model for phrase based SMT (Koehn 2007).
</bodyText>
<subsectionHeader confidence="0.830363">
4.1 Factored Model
</subsectionHeader>
<bodyText confidence="0.998753333333333">
Factored models allow the translation to be broken
down into various components, which are com-
bined using a log-linear model:
</bodyText>
<equation confidence="0.99851875">
1
�
p(e|f) = Z exp
i=1
</equation>
<bodyText confidence="0.947891">
Each hi is a feature function for a component of
the translation (such as the language model), and
the A values are weights for the feature functions.
</bodyText>
<subsectionHeader confidence="0.993916">
4.2 Our Factorization
</subsectionHeader>
<bodyText confidence="0.9832855">
Our factorization, which is illustrated in figure 1,
consists of:
</bodyText>
<listItem confidence="0.943841">
1. a lemma to lemma translation factor (boy →
lXk^ (ladak))
2. a suffix + semantic relation to suffix/case
marker factor (-s + subj → e (e))
3. a lemma + suffix to surface form genera-
tion factor (lXk^ + e (ladak + e) → lXk�
(ladake))
The above factorization is motivated by the fol-
lowing:
• Case markers are decided by semantic re-
lations and tense-aspect information in suf-
fixes.
</listItem>
<bodyText confidence="0.853851833333333">
For example, if a clause has an object, and
has a perfective form, the subject usually re-
quires the case marker n� (ne).
John ate an apple.
John|empty|subj eat|ed|empty an|empty|det
apple|empty|obj
</bodyText>
<equation confidence="0.9825065">
n
Aihi(e,f) (1)
</equation>
<page confidence="0.990039">
802
</page>
<figureCaption confidence="0.9951085">
Figure 1: Semantic and Suffix Factors: the combination of English suffixes and semantic relations is
aligned with Hindi suffixes and case markers
</figureCaption>
<bodyText confidence="0.7564466">
jA�n n� s�b KAyA
john ne seba khaayaa
Thus, the combination of the suffix and
semantic relation generates the right case
marker (ed|empty + empty|obj —* n� (ne)).
</bodyText>
<listItem confidence="0.969955">
• Target language suffixes are largely deter-
mined by source language suffixes and case
markers (which in turn are determined by the
semantic relations)
</listItem>
<bodyText confidence="0.627496">
The boys ate apples.
</bodyText>
<equation confidence="0.777772666666667">
The|empty|det boy|s|subj eat|ed|empty
apple|s|obj
lXko\ n� s�b KAy�
</equation>
<bodyText confidence="0.9035868">
ladakon ne seba khaaye
Here, the plural suffix on boys leads to two
possibilities – lXk� (ladake – plural direct)
and lXko\ (ladakon – plural oblique). The
case marker n� (ne) requires the oblique case.
</bodyText>
<listItem confidence="0.559793333333333">
• Our factorization provides the system with
two sources to determine the case markers
and suffixes. While the translation steps dis-
cussed above are one source, the language
model over the suffix/case marker factor re-
inforces the decisions made.
</listItem>
<bodyText confidence="0.999939625">
For example, the combination lXkA n~
(ladakaa ne) is impossible, while lXko\ n~
(ladakon ne) is very likely. The separation of
the lemma and suffix helps in tiding over the
data sparsity problem by allowing the system
to reason about the suffix-case marker com-
bination rather than the combination of the
specific word and the case marker.
</bodyText>
<sectionHeader confidence="0.993323" genericHeader="method">
5 Semantic Relations
</sectionHeader>
<bodyText confidence="0.99983925">
The experiments have been conducted with two
kinds of semantic relations. One of them is the re-
lations from the Universal Networking Language
(UNL), and the other is the grammatical relations
produced by the Stanford parser.
The relations in both UNL and the Stanford de-
pendency parser are strictly binary and form a di-
rected graph. These relations express the semantic
dependencies among the various words in the sen-
tence.
Stanford: The Stanford dependency
parser (Marie-Catherine and Manning, 2008)
uses 55 relations to express the dependencies
among the various words in a sentence. These
relations form a hierarchical structure with the
most general relation at the root. There are
various argument relations like subject, object,
objects of prepositions, and clausal complements,
modifier relations like adjectival, adverbial,
participial, and infinitival modifiers, and other
relations like coordination, conjunct, expletive,
and punctuation.
UNL: The 44 UNL relations1 include relations
such as agent, object, co-agent, and partner, tem-
poral relations, locative relations, conjunctive and
disjunctive relations, comparative relations and
also hierarchical relationships like part-of and an-
instance-of.
Comparison: Unlike the Stanford parser which
expresses the semantic relationships through
grammatical relations, UNL uses attributes and
universal words, in addition to the semantic roles,
to express the same. Universal words are used to
disambiguate words, while attributes are used to
express the speaker’s point of view in the sentence.
UNL relations, compared to the relations in the
Stanford parser, are more semantic than grammat-
ical. For instance, in the Stanford parser, the agent
relation is the complement of a passive verb intro-
duced by the preposition by, whereas in UNL it
</bodyText>
<footnote confidence="0.988702">
1http://www.undl.org/unlsys/unl/unl2005/
</footnote>
<page confidence="0.998047">
803
</page>
<figureCaption confidence="0.803926">
Figure 2: UNL and Stanford semantic relation graphs for the sentence “John said that he was hit
by Jack”
</figureCaption>
<table confidence="0.99488075">
#sentences #words
Training 12868 316508
Tuning 600 15279
Test 400 8557
</table>
<tableCaption confidence="0.999487">
Table 1: Corpus Statistics
</tableCaption>
<bodyText confidence="0.996527954545455">
signifies the doer of an action. Consider the fol-
lowing sentence:
John said that he was hit by Jack.
In this sentence, the Stanford parser produces
the relation agent(hit, Jack) and nsubj(said, John)
as shown in figure 2. In UNL, however, both the
cases use the agent relation. The other distinguish-
ing aspect of UNL is the hyper-node that repre-
sents scope. In the example sentence, the whole
clause “that he was hit by Jack” forms the ob-
ject of the verb said, and hence is represented in
a scope. The Stanford dependency parser on the
other hand represents these dependencies with the
help of the clausal complement relation, which
links said with hit, and uses the complementizer
relation to introduce the subordinating conjunc-
tion.
The pre-dependency accuracy of the Stan-
ford dependency parser is around 80% (Marie-
Catherine et al., 2006), while the accuracy
achieved by the UNL generating system is
64.89%.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.94591">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.999881">
The corpus described in table 1 was used for the
experiments.
The SRILM toolkit 2 was used to create Hindi
language models using the target side of the train-
ing corpus.
Training, tuning, and decoding were performed
using the Moses toolkit 3. Tuning (learning the
A values discussed in section 4.1) was done using
minimum error rate training (Och, 2003).
The Stanford parser 4 was used for parsing the
English text for syntactic reordering and to gener-
ate “stanford” semantic relations.
The program for syntactic reordering used the
parse trees generated by the Stanford parser,
and was written in perl using the module
Parse::RecDescent.
English morphological analysis was performed
using morpha (Minnen et al., 2001), while Hindi
suffix separation was done using the stemmer de-
scribed in (Ananthakrishnan and Rao, 2003).
Syntactic and morphological transformations,
in the models where they were employed, were ap-
plied at every phase: training, tuning, and testing.
Evaluation Criteria: Automatic evaluation
was performed using BLEU and NIST on the en-
tire test set of 400 sentences. Subjective evaluation
was performed on 125 sentences from the test set.
</bodyText>
<listItem confidence="0.954753428571428">
• BLEU (Papineni et al., 2001): measures the
precision of n-grams with respect to the ref-
erence translations, with a brevity penalty. A
higher BLEU score indicates better transla-
tion.
• NIST 5: measures the precision of n-grams.
This metric is a variant of BLEU, which was
</listItem>
<footnote confidence="0.99996925">
2http://www.speech.sri.com/projects/srilm/
3http://www.statmt.org/moses/
4http://nlp.stanford.edu/software/lex-parser.shtml
5www.nist.gov/speech/tests/mt/doc/ngram-study.pdf
</footnote>
<page confidence="0.99837">
804
</page>
<bodyText confidence="0.98719">
shown to correlate better with human judg-
ments. Again, a higher score indicates better
translation.
</bodyText>
<listItem confidence="0.996068666666667">
• Subjective: Human evaluators judged the
fluency and adequacy, and counted the num-
ber of errors in case markers and morphology.
</listItem>
<subsectionHeader confidence="0.876849">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.99965832">
Table 2 shows the impact of suffix and semantic
factors. The models experimented with are de-
scribed below:
baseline: The default settings of Moses were
used for this model.
lemma + suffix: This uses the lemma and suf-
fix factors on the source side, and the lemma and
suffix/case marker on the target side. The trans-
lation steps are i) lemma to lemma and ii) suffix
to suffix/case marker, and the generation step is
lemma+suffix/case marker to surface form.
lemma + suffix + unl: This model uses, in ad-
dition to the factors in the lemma+suffix model,
a semantic relation factor (UNL relations). The
translation steps are i) lemma to lemma and ii)
suffix+semantic relation to suffix/case marker, and
the generation step again is lemma+suffix/case
marker to surface form.
lemma + suffix + stanford: This is identical
to the previous model, except that stanford depen-
dency relations are used instead of UNL relations.
We can see a substantial improvement in scores
when semantic relations are used.
Table 5 shows the impact of syntactic reorder-
ing. The surface form with distortion-based, lex-
icalized, and syntactic reordering were experi-
mented with. The model with the suffix and se-
mantic factors was used with syntactic reordering.
For subjective evaluation, sentences were
judged on fluency, adequacy and the number of er-
rors in case marking/morphology.
To judge fluency, the judges were asked to look
at how well-formed the output sentence is accord-
ing to Hindi grammar, without considering what
the translation is supposed to convey. The five-
point scale in table 3 was used for evaluation.
To judge adequacy, the judges were asked to
compare each output sentence to the reference
translation and judge how well the meaning con-
veyed by the reference was also conveyed by the
output sentence. The five-point scale in table 4
was used.
Table 6 shows the average fluency and adequacy
scores, and the average number of errors per sen-
tence.
All differences are significant at the 99%
level, except the difference in adequacy be-
tween the surface-syntactic model and the
lemma+suffix+stanford syntactic model, which is
significant at the 95% level.
</bodyText>
<sectionHeader confidence="0.996178" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.985688128205128">
We can see from the results that better fluency and
adequacy are achieved with the use of semantic re-
lations. The improvement in fluency is especially
noteworthy. Figure 3 shows the distribution of flu-
ency and adequacy scores. What is worth noting
is that the number of sentences at levels 4 and 5
in terms of fluency and adequacy are much higher
in case of the model that uses semantic relations.
That is, the use of semantic relations, in combi-
nation with syntactic reordering, produces many
more sentences that are reasonably or even per-
fectly fluent and convey most or all of the mean-
ing.
Table 7 shows the impact of sentence length on
translation quality. We can see that with smaller
sentences the improvements using syntactic re-
ordering and semantic relations are much more
pronounced. All models find long sentences dif-
ficult to handle, which contributes to bringing the
mean performances closer. However, it is clear
that many more useful translations are being pro-
duced due to syntactic reordering and semantic re-
lations.
The following is an example of the kind of im-
provements achieved:
Input: Inland waterway is one of the most pop-
ular picnic spots in Alappuzha.
Baseline: m�\ ek a;-t,-TlFy jlmAg k�
sbs� þEsˆ EpkEnk -Tl m~\ jlo\ m~\ dOXtF
h{
men eka antahsthaliiya jalamaarga ke sabase
prasiddha pikanika sthala men jalon men daudatii
hai
gloss: in a waterway of most popular picnic spot
in waters runs.
Reorder: a;:t,-TlFy jlmAg� aAlp� yA k�
sbs� þEsˆ EpkEnk -Tl m�\ s� ek h{
antahsthaliiya jalamaarga aalapuzaa ke sabase
prasiddha pikanika sthala men se eka hai
</bodyText>
<page confidence="0.99424">
805
</page>
<table confidence="0.998393">
Model BLEU NIST
Baseline (surface) 24.32 5.85
lemma + suffix 25.16 5.87
lemma + suffix + unl 27.79 6.05
lemma + suffix + stanford 28.21 5.99
</table>
<tableCaption confidence="0.97068">
Table 2: Results: The impact of suffix and semantic factors
</tableCaption>
<table confidence="0.994650833333333">
Level Interpretation
5 Flawless Hindi, with no grammatical errors whatsoever
4 Good Hindi, with a few minor errors in morphology
3 Non-native Hindi, with possibly a few minor grammatical errors
2 Disfluent Hindi, with most phrases correct, but ungrammatical overall
1 Incomprehensible
</table>
<tableCaption confidence="0.998664">
Table 3: Subjective Evaluation: Fluency Scale
</tableCaption>
<figure confidence="0.547030166666667">
Level Interpretation
5 All meaning is conveyed
4 Most of the meaning is conveyed
3 Much of the meaning is conveyed
2 Little meaning is conveyed
1 None of the meaning is conveyed
</figure>
<tableCaption confidence="0.996299">
Table 4: Subjective Evaluation: Adequacy Scale
</tableCaption>
<table confidence="0.9988372">
Model Reordering BLEU NIST
surface distortion 24.42 5.85
surface lexicalized 28.75 6.19
surface syntactic 31.57 6.40
lemma + suffix + stanford syntactic 31.49 6.34
</table>
<tableCaption confidence="0.994028">
Table 5: Results: The impact of reordering and semantic relations
</tableCaption>
<table confidence="0.9993725">
Model Reordering Fluency Adequacy #errors
surface lexicalized 2.14 2.26 2.16
surface syntactic 2.6 2.71 1.79
lemma + suffix + stanford syntactic 2.88 2.82 1.44
</table>
<tableCaption confidence="0.986233">
Table 6: Subjective Evaluation: The impact of reordering and semantic relations
</tableCaption>
<table confidence="0.9997192">
Baseline Reorder Stanford
F A E F A E F A E
Small (&lt;19 words) 2.63 2.84 1.30 3.30 3.52 0.74 3.66 3.75 0.62
Medium (20-34 words) 1.92 2.00 2.23 2.32 2.43 2.05 2.62 2.46 1.74
Large (&gt;34 words) 1.62 1.69 4.00 1.86 1.73 3.36 1.86 1.86 2.82
</table>
<tableCaption confidence="0.999285">
Table 7: Impact of sentence length (F: Fluency; A:Adequacy; E:# Errors)
</tableCaption>
<page confidence="0.995818">
806
</page>
<figureCaption confidence="0.717329333333333">
Figure 3: Subjective evaluation: analysis
gloss: waterway Alappuzha of most popular
picnic spot of one is
</figureCaption>
<bodyText confidence="0.95926566">
Semantic: a -t,-TlFy jlmAg aAlp yA k�
sbs� þEsˆ EpkEnk -Tlo\ m~\ s~ ek {h
antahsthaliiya jalamaarga aalapuzaa ke sabase
prasiddha pikanika sthalon men se eka hai
gloss: waterway Alappuzha of most popular
picnic spots of one is
We can see that poor word-order makes the
baseline output almost incomprehensible, while
syntactic reordering solves the problem correctly.
The morphology improvement using semantic
relations can be seen in the correct inflection
achieved in the word -Tlo\ (sthalon – plural
oblique – spots), whereas the output without using
semantic relations generates -Tl (sthala – singu-
lar – spot).
The next couple of examples illustrate how case
marking improves through the use of semantic re-
lations.
Input: Gandhi Darshan and Gandhi National
Museum is across Rajghat.
Reorder: gA\DF df�n v gA\DF rA£~Fy s\g}hAly
rAjGAV m�\ h{
gaandhii darshana va gaandhii raashtriiya san-
grahaalaya raajaghaata men hai
Semantic: gA\DF df�n v gA\DF rA£~Fy
s\g}hAly rAjGAV k� pAr h{
gaandhii darshana va gaandhii raashtriiya san-
grahaalaya raajaghaata ke paara hai
Here, the use of semantic relations produces the
correct meaning that the locations mentioned are
across (k� pAr (ke paara)) Rajghat, and not in (m�\
(men)) Rajghat as suggested by the translation pro-
duced without using semantic relations.
Another common error in case marking is that
two case markers are produced in successive po-
sitions in the translation, which is not possible in
Hindi. The following example (a fragment) shows
this error (kF (kii) repeated) being correctly han-
dled by using semantic relations:
Input: For varieties of migratory birds
Reorder: þvAsF pE&amp;quot;yo\ kF kF þkAr k~ Ely�
pravaasii pakshiyon kii kii prakaara ke liye
Semantic: þvAsF pE&amp;quot;yo\ kF þkAr k~ Ely�
pravaasii pakshiyon kii prakaara ke liye
It is important to note that the gains made us-
ing syntactic reordering and semantic relations are
limited by the accuracy of the parsers (see section
5). We observe that even the use of moderate qual-
ity semantic relations goes a long way in increas-
ing the quality of translation.
</bodyText>
<sectionHeader confidence="0.998325" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999939125">
We have reported in this paper the marked im-
provement in the output quality of Hindi transla-
tions – especially fluency – when the correspon-
dence of English semantic relations and suffixes
with Hindi case markers and inflections is used as
a translation factor in English-Hindi SMT. The im-
provement is statistically significant. Subjective
evaluation too lends ample credence to this claim.
Future work consists of investigations into (i) how
the internal structure of constituents can be strictly
preserved and (ii) how to glue together correctly
the syntactically well-formed bits and pieces of
the sentences. This course of future action is sug-
gested by the fact that smaller sentences are much
more fluent in translation compared to medium
length and long sentences.
</bodyText>
<page confidence="0.988036">
807
</page>
<bodyText confidence="0.9732492">
Papineni, K., Roukos, S., Ward, T., and Zhu,
W., BLEU: a Method for Automatic Evalu-
ation of Machine Translation, IBM Research
Report, Thomas J. Watson Research Center,
2001.
</bodyText>
<sectionHeader confidence="0.91614" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.97594026">
Ananthakrishnan, R., and Rao, D., A Lightweight
Stemmer for Hindi, Workshop on Com-
putational Linguistics for South-Asian Lan-
guages, EACL, 2003.
Ananthakrishnan, R., Bhattacharyya, P., Hegde, J.
J., Shah, R. M., and Sasikumar, M., Sim-
ple Syntactic and Morphological Processing
Can Help English-Hindi Statistical Machine
Translation, Proceedings of IJCNLP, 2008.
Avramidis, E., and Koehn, P., Enriching Morpho-
logically Poor Languages for Statistical Ma-
chine Translation, Proceedings of ACL-08:
HLT, 2008.
Collins, M., Koehn, P., and I. Kucerova, Clause
Restructuring for Statistical Machine Trans-
lation, Proceedings of ACL, 2005.
Imamura, K., Okuma, H., Sumita, E., Prac-
tical Approach to Syntax-based Statistical
Machine Translation, Proceedings of MT-
SUMMIT X, 2005.
Koehn, P., and Hoang, H., Factored Translation
Models, Proceedings of EMNLP, 2007.
Marie-Catherine de Marneffe, MacCartney, B.,
and Manning, C., Generating Typed Depen-
dency Parses from Phrase Structure Parses,
Proceedings of LREC, 2006.
Marie-Catherine de Marneffe and Manning, C.,
Stanford Typed Dependency Manual, 2008.
Melamed, D., Statistical Machine Translation by
Parsing, Proceedings of ACL, 2004.
Minnen, G., Carroll, J., and Pearce, D., Applied
Morphological Processing of English, Natu-
ral Language Engineering, 7(3), pages 207–
223, 2001.
Nießen, S., and Ney, H., Statistical Machine
Translation with Scarce Resources Using
Morpho-syntactic Information, Computa-
tional Linguistics, 30(2), pages 181–204,
2004.
Och, F., Minimum Error Rate Training in Sta-
tistical Machine Translation, Proceedings of
ACL, 2003.
Popovic, M., and Ney, H., Statistical Machine
Translation with a Small Amount of Bilin-
gual Training Data, 5th LREC SALTMIL
Workshop on Minority Languages, 2006.
Wang, C., Collins, M., and Koehn, P., Chinese
Syntactic Reordering for Statistical Machine
Translation, Proceedings of the EMNLP-
CoNLL, 2007.
</reference>
<page confidence="0.99749">
808
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.353558">
<title confidence="0.781898333333333">Case markers and Morphology: Addressing the crux of the fluency problem in English-Hindi SMT Ananthakrishnan Ramanathan, Hansraj Choudhary</title>
<author confidence="0.990062">Avishek Ghosh</author>
<author confidence="0.990062">Pushpak Bhattacharyya</author>
<affiliation confidence="0.9977765">Department of Computer Science and Engineering Indian Institute of Technology Bombay</affiliation>
<address confidence="0.9171285">Powai, Mumbai-400076 India</address>
<email confidence="0.904139">hansraj,avis,</email>
<abstract confidence="0.999084882352942">We report in this paper our work on accurately generating case markers and suffixes in English-to-Hindi SMT. Hindi is a relatively free word-order language, and makes use of a comparatively richer set of case markers and morphological suffixes for correct meaning representation. From our experience of large-scale English-Hindi MT, we are convinced that fluency and fidelity in the Hindi output get an order of magnitude facelift if accurate case markers and suffixes are produced. the moot question is: entity on the English side encodes the information contained in case markers and suffixes on Hindi Our studies of correspondences in the two languages show that case markers and suffixes in Hindi are predominantly determined by the combination of suffixes and semantic relations on the English side. We, therefore, augment the aligned corpus of the two languages, with the correspondence of English suffixes and semantic relations with Hindi suffixes and case markers. Our results on 400 test sentences, translated using an SMT system trained on around 13000 parallel senshow that + semantic relamarker/suffix a very useful translation factor, in the sense of making a significant difference to output quality as indicated by subjective evaluation as well as BLEU scores.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Ananthakrishnan</author>
<author>D Rao</author>
</authors>
<title>A Lightweight Stemmer for Hindi,</title>
<date>2003</date>
<booktitle>Workshop on Computational Linguistics for South-Asian Languages, EACL,</booktitle>
<contexts>
<context position="16102" citStr="Ananthakrishnan and Rao, 2003" startWordPosition="2549" endWordPosition="2552"> decoding were performed using the Moses toolkit 3. Tuning (learning the A values discussed in section 4.1) was done using minimum error rate training (Och, 2003). The Stanford parser 4 was used for parsing the English text for syntactic reordering and to generate “stanford” semantic relations. The program for syntactic reordering used the parse trees generated by the Stanford parser, and was written in perl using the module Parse::RecDescent. English morphological analysis was performed using morpha (Minnen et al., 2001), while Hindi suffix separation was done using the stemmer described in (Ananthakrishnan and Rao, 2003). Syntactic and morphological transformations, in the models where they were employed, were applied at every phase: training, tuning, and testing. Evaluation Criteria: Automatic evaluation was performed using BLEU and NIST on the entire test set of 400 sentences. Subjective evaluation was performed on 125 sentences from the test set. • BLEU (Papineni et al., 2001): measures the precision of n-grams with respect to the reference translations, with a brevity penalty. A higher BLEU score indicates better translation. • NIST 5: measures the precision of n-grams. This metric is a variant of BLEU, w</context>
</contexts>
<marker>Ananthakrishnan, Rao, 2003</marker>
<rawString>Ananthakrishnan, R., and Rao, D., A Lightweight Stemmer for Hindi, Workshop on Computational Linguistics for South-Asian Languages, EACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ananthakrishnan</author>
<author>P Bhattacharyya</author>
<author>J J Hegde</author>
<author>R M Shah</author>
<author>M Sasikumar</author>
</authors>
<date>2008</date>
<booktitle>Simple Syntactic and Morphological Processing Can Help English-Hindi Statistical Machine Translation, Proceedings of IJCNLP,</booktitle>
<contexts>
<context position="2375" citStr="Ananthakrishnan et al., 2008" startWordPosition="357" endWordPosition="360">tion (SMT) techniques to EnglishHindi (and generally to Indian language) MT are: i) the wide syntactic divergence between the language pairs, and ii) the richer morphology and case marking of Hindi compared to English. The first problem manifests itself in poor word-order in the output translations, while the second one leads to incorrect inflections (word-endings) and case marking. Being a free word-order language, Hindi suffers badly when morphology and case markers are incorrect. To solve the former, word-order related, problem, we use a preprocessing technique, which we have discussed in (Ananthakrishnan et al., 2008). This procedure is similar to what is suggested in (Collins et al., 2005) and (Wang, 2007), and results in the input sentence being reordered to follow Hindi structure. The focus of this paper, however, is on the thorny problem of generating case markers and morphology. It is recognized that translating from poor to rich morphology is a challenge (Avramidis and Koehn, 2008) that calls for deeper linguistic analysis to be part of the translation process. Such analysis is facilitated by factored models (Koehn et al., 2007), which provide a framework for incorporating lemmas, suffixes, POS tags,</context>
<context position="5911" citStr="Ananthakrishnan et al., 2008" startWordPosition="893" endWordPosition="896">e parsing tools. These algorithms attempt to reconcile the word-order differences between the source and target language sentences by reordering the source language data prior to the SMT training and decoding cycles. NieBen and Ney (2004) propose some restructuring steps for German-English SMT. Popovic and Ney (2006) report the use of simple local transformation rules for SpanishEnglish and Serbian-English translation. Collins et al. (2005) propose German clause restructuring to improve German-English SMT, while Wang et al. (2007) present similar work for ChineseEnglish SMT. Our earlier work (Ananthakrishnan et al., 2008) describes syntactic reordering and morphological suffix separation for English-Hindi SMT. 3 Motivation The fundamental differences between English and Hindi are: • English follows SVO order, whereas Hindi follows SOV order • English uses post-modifiers, whereas Hindi uses pre-modifiers • Hindi allows greater freedom in word-order, identifying constituents through case marking • Hindi has a relatively richer system of morphology We resolve the first two syntactic differences by reordering the English sentence to conform to Hindi word-order in a preprocessing step as described in (Ananthakrishn</context>
</contexts>
<marker>Ananthakrishnan, Bhattacharyya, Hegde, Shah, Sasikumar, 2008</marker>
<rawString>Ananthakrishnan, R., Bhattacharyya, P., Hegde, J. J., Shah, R. M., and Sasikumar, M., Simple Syntactic and Morphological Processing Can Help English-Hindi Statistical Machine Translation, Proceedings of IJCNLP, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Avramidis</author>
<author>P Koehn</author>
</authors>
<title>Enriching Morphologically Poor Languages for Statistical Machine Translation,</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<contexts>
<context position="2752" citStr="Avramidis and Koehn, 2008" startWordPosition="421" endWordPosition="424">king. Being a free word-order language, Hindi suffers badly when morphology and case markers are incorrect. To solve the former, word-order related, problem, we use a preprocessing technique, which we have discussed in (Ananthakrishnan et al., 2008). This procedure is similar to what is suggested in (Collins et al., 2005) and (Wang, 2007), and results in the input sentence being reordered to follow Hindi structure. The focus of this paper, however, is on the thorny problem of generating case markers and morphology. It is recognized that translating from poor to rich morphology is a challenge (Avramidis and Koehn, 2008) that calls for deeper linguistic analysis to be part of the translation process. Such analysis is facilitated by factored models (Koehn et al., 2007), which provide a framework for incorporating lemmas, suffixes, POS tags, and any other linguistic factors in a log-linear model for phrasebased SMT. In this paper, we motivate a factorization well-suited to English-Hindi translation. The factorization uses semantic relations and suffixes to generate inflections and case markers. Our experiments include two different kinds of semantic relations, namely, dependency relations provided by the Stanfo</context>
<context position="4602" citStr="Avramidis and Koehn (2008)" startWordPosition="697" endWordPosition="700"> NieBen and Ney (2004) show that the use of morpho-syntactic information drastically reduces the need for bilingual training data. Popovic and Ney (2006) report the use of morphological and syntactic restructuring information for SpanishEnglish and Serbian-English translation. Koehn and Hoang (2007) propose factored translation models that combine feature functions to handle syntactic, morphological, and other linguistic information in a log-linear model. This work also describes experiments in translating from English to German, Spanish, and Czech, including the use of morphological factors. Avramidis and Koehn (2008) report work on translating from poor to rich morphology, namely, English to Greek and Czech translation. They use factored models with case and verb conjugation related factors determined by heuristics on parse trees. The factors are used only on the source side, and not on the target side. To handle syntactic differences, Melamed (2004) proposes methods based on tree-to-tree mappings. Imamura et al. (2005) present a similar method that achieves significant improvements over a phrase-based baseline model for Japanese-English translation. Another method for handling syntactic differences is pr</context>
</contexts>
<marker>Avramidis, Koehn, 2008</marker>
<rawString>Avramidis, E., and Koehn, P., Enriching Morphologically Poor Languages for Statistical Machine Translation, Proceedings of ACL-08: HLT, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>P Koehn</author>
<author>I Kucerova</author>
</authors>
<title>Clause Restructuring for Statistical Machine Translation,</title>
<date>2005</date>
<booktitle>Proceedings of ACL,</booktitle>
<contexts>
<context position="2449" citStr="Collins et al., 2005" startWordPosition="370" endWordPosition="373">) the wide syntactic divergence between the language pairs, and ii) the richer morphology and case marking of Hindi compared to English. The first problem manifests itself in poor word-order in the output translations, while the second one leads to incorrect inflections (word-endings) and case marking. Being a free word-order language, Hindi suffers badly when morphology and case markers are incorrect. To solve the former, word-order related, problem, we use a preprocessing technique, which we have discussed in (Ananthakrishnan et al., 2008). This procedure is similar to what is suggested in (Collins et al., 2005) and (Wang, 2007), and results in the input sentence being reordered to follow Hindi structure. The focus of this paper, however, is on the thorny problem of generating case markers and morphology. It is recognized that translating from poor to rich morphology is a challenge (Avramidis and Koehn, 2008) that calls for deeper linguistic analysis to be part of the translation process. Such analysis is facilitated by factored models (Koehn et al., 2007), which provide a framework for incorporating lemmas, suffixes, POS tags, and any other linguistic factors in a log-linear model for phrasebased SM</context>
<context position="5726" citStr="Collins et al. (2005)" startWordPosition="865" endWordPosition="868">e model for Japanese-English translation. Another method for handling syntactic differences is preprocessing, which is especially pertinent when the target language does not have parsing tools. These algorithms attempt to reconcile the word-order differences between the source and target language sentences by reordering the source language data prior to the SMT training and decoding cycles. NieBen and Ney (2004) propose some restructuring steps for German-English SMT. Popovic and Ney (2006) report the use of simple local transformation rules for SpanishEnglish and Serbian-English translation. Collins et al. (2005) propose German clause restructuring to improve German-English SMT, while Wang et al. (2007) present similar work for ChineseEnglish SMT. Our earlier work (Ananthakrishnan et al., 2008) describes syntactic reordering and morphological suffix separation for English-Hindi SMT. 3 Motivation The fundamental differences between English and Hindi are: • English follows SVO order, whereas Hindi follows SOV order • English uses post-modifiers, whereas Hindi uses pre-modifiers • Hindi allows greater freedom in word-order, identifying constituents through case marking • Hindi has a relatively richer sys</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Collins, M., Koehn, P., and I. Kucerova, Clause Restructuring for Statistical Machine Translation, Proceedings of ACL, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Imamura</author>
<author>H Okuma</author>
<author>E Sumita</author>
</authors>
<title>Practical Approach to Syntax-based Statistical Machine Translation,</title>
<date>2005</date>
<booktitle>Proceedings of MTSUMMIT X,</booktitle>
<contexts>
<context position="5013" citStr="Imamura et al. (2005)" startWordPosition="761" endWordPosition="764">nguistic information in a log-linear model. This work also describes experiments in translating from English to German, Spanish, and Czech, including the use of morphological factors. Avramidis and Koehn (2008) report work on translating from poor to rich morphology, namely, English to Greek and Czech translation. They use factored models with case and verb conjugation related factors determined by heuristics on parse trees. The factors are used only on the source side, and not on the target side. To handle syntactic differences, Melamed (2004) proposes methods based on tree-to-tree mappings. Imamura et al. (2005) present a similar method that achieves significant improvements over a phrase-based baseline model for Japanese-English translation. Another method for handling syntactic differences is preprocessing, which is especially pertinent when the target language does not have parsing tools. These algorithms attempt to reconcile the word-order differences between the source and target language sentences by reordering the source language data prior to the SMT training and decoding cycles. NieBen and Ney (2004) propose some restructuring steps for German-English SMT. Popovic and Ney (2006) report the u</context>
</contexts>
<marker>Imamura, Okuma, Sumita, 2005</marker>
<rawString>Imamura, K., Okuma, H., Sumita, E., Practical Approach to Syntax-based Statistical Machine Translation, Proceedings of MTSUMMIT X, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
</authors>
<title>Factored Translation Models,</title>
<date>2007</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<contexts>
<context position="4276" citStr="Koehn and Hoang (2007)" startWordPosition="651" endWordPosition="654">s has an especially salubrious effect on fluency. 800 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 800–808, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Related Work There have been quite a few attempts at including morphological information within statistical MT. NieBen and Ney (2004) show that the use of morpho-syntactic information drastically reduces the need for bilingual training data. Popovic and Ney (2006) report the use of morphological and syntactic restructuring information for SpanishEnglish and Serbian-English translation. Koehn and Hoang (2007) propose factored translation models that combine feature functions to handle syntactic, morphological, and other linguistic information in a log-linear model. This work also describes experiments in translating from English to German, Spanish, and Czech, including the use of morphological factors. Avramidis and Koehn (2008) report work on translating from poor to rich morphology, namely, English to Greek and Czech translation. They use factored models with case and verb conjugation related factors determined by heuristics on parse trees. The factors are used only on the source side, and not o</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Koehn, P., and Hoang, H., Factored Translation Models, Proceedings of EMNLP, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>B MacCartney</author>
<author>C Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses,</title>
<date>2006</date>
<booktitle>Proceedings of LREC,</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, MacCartney, B., and Manning, C., Generating Typed Dependency Parses from Phrase Structure Parses, Proceedings of LREC, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>C Manning</author>
<author>Stanford</author>
</authors>
<title>Typed Dependency Manual,</title>
<date>2008</date>
<marker>de Marneffe, Manning, Stanford, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Manning, C., Stanford Typed Dependency Manual, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Melamed</author>
</authors>
<title>Statistical Machine Translation by Parsing,</title>
<date>2004</date>
<booktitle>Proceedings of ACL,</booktitle>
<contexts>
<context position="4942" citStr="Melamed (2004)" startWordPosition="753" endWordPosition="754">ature functions to handle syntactic, morphological, and other linguistic information in a log-linear model. This work also describes experiments in translating from English to German, Spanish, and Czech, including the use of morphological factors. Avramidis and Koehn (2008) report work on translating from poor to rich morphology, namely, English to Greek and Czech translation. They use factored models with case and verb conjugation related factors determined by heuristics on parse trees. The factors are used only on the source side, and not on the target side. To handle syntactic differences, Melamed (2004) proposes methods based on tree-to-tree mappings. Imamura et al. (2005) present a similar method that achieves significant improvements over a phrase-based baseline model for Japanese-English translation. Another method for handling syntactic differences is preprocessing, which is especially pertinent when the target language does not have parsing tools. These algorithms attempt to reconcile the word-order differences between the source and target language sentences by reordering the source language data prior to the SMT training and decoding cycles. NieBen and Ney (2004) propose some restruct</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>Melamed, D., Statistical Machine Translation by Parsing, Proceedings of ACL, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<date>2001</date>
<journal>Applied Morphological Processing of English, Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>207--223</pages>
<contexts>
<context position="15999" citStr="Minnen et al., 2001" startWordPosition="2533" endWordPosition="2536">ate Hindi language models using the target side of the training corpus. Training, tuning, and decoding were performed using the Moses toolkit 3. Tuning (learning the A values discussed in section 4.1) was done using minimum error rate training (Och, 2003). The Stanford parser 4 was used for parsing the English text for syntactic reordering and to generate “stanford” semantic relations. The program for syntactic reordering used the parse trees generated by the Stanford parser, and was written in perl using the module Parse::RecDescent. English morphological analysis was performed using morpha (Minnen et al., 2001), while Hindi suffix separation was done using the stemmer described in (Ananthakrishnan and Rao, 2003). Syntactic and morphological transformations, in the models where they were employed, were applied at every phase: training, tuning, and testing. Evaluation Criteria: Automatic evaluation was performed using BLEU and NIST on the entire test set of 400 sentences. Subjective evaluation was performed on 125 sentences from the test set. • BLEU (Papineni et al., 2001): measures the precision of n-grams with respect to the reference translations, with a brevity penalty. A higher BLEU score indicat</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Minnen, G., Carroll, J., and Pearce, D., Applied Morphological Processing of English, Natural Language Engineering, 7(3), pages 207– 223, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nießen</author>
<author>H Ney</author>
</authors>
<title>Statistical Machine Translation with Scarce Resources Using Morpho-syntactic Information,</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<pages>181--204</pages>
<marker>Nießen, Ney, 2004</marker>
<rawString>Nießen, S., and Ney, H., Statistical Machine Translation with Scarce Resources Using Morpho-syntactic Information, Computational Linguistics, 30(2), pages 181–204, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation,</title>
<date>2003</date>
<booktitle>Proceedings of ACL,</booktitle>
<contexts>
<context position="15634" citStr="Och, 2003" startWordPosition="2480" endWordPosition="2481">tizer relation to introduce the subordinating conjunction. The pre-dependency accuracy of the Stanford dependency parser is around 80% (MarieCatherine et al., 2006), while the accuracy achieved by the UNL generating system is 64.89%. 6 Experiments 6.1 Setup The corpus described in table 1 was used for the experiments. The SRILM toolkit 2 was used to create Hindi language models using the target side of the training corpus. Training, tuning, and decoding were performed using the Moses toolkit 3. Tuning (learning the A values discussed in section 4.1) was done using minimum error rate training (Och, 2003). The Stanford parser 4 was used for parsing the English text for syntactic reordering and to generate “stanford” semantic relations. The program for syntactic reordering used the parse trees generated by the Stanford parser, and was written in perl using the module Parse::RecDescent. English morphological analysis was performed using morpha (Minnen et al., 2001), while Hindi suffix separation was done using the stemmer described in (Ananthakrishnan and Rao, 2003). Syntactic and morphological transformations, in the models where they were employed, were applied at every phase: training, tuning</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F., Minimum Error Rate Training in Statistical Machine Translation, Proceedings of ACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Popovic</author>
<author>H Ney</author>
</authors>
<title>Statistical Machine Translation with a Small Amount of Bilingual Training</title>
<date>2006</date>
<booktitle>Data, 5th LREC SALTMIL Workshop on Minority Languages,</booktitle>
<contexts>
<context position="4129" citStr="Popovic and Ney (2006)" startWordPosition="632" endWordPosition="635">emantic relations and syntactic reordering leads to substantially better quality translation. The use of even moderately accurate semantic relations has an especially salubrious effect on fluency. 800 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 800–808, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Related Work There have been quite a few attempts at including morphological information within statistical MT. NieBen and Ney (2004) show that the use of morpho-syntactic information drastically reduces the need for bilingual training data. Popovic and Ney (2006) report the use of morphological and syntactic restructuring information for SpanishEnglish and Serbian-English translation. Koehn and Hoang (2007) propose factored translation models that combine feature functions to handle syntactic, morphological, and other linguistic information in a log-linear model. This work also describes experiments in translating from English to German, Spanish, and Czech, including the use of morphological factors. Avramidis and Koehn (2008) report work on translating from poor to rich morphology, namely, English to Greek and Czech translation. They use factored mod</context>
<context position="5600" citStr="Popovic and Ney (2006)" startWordPosition="847" endWordPosition="850">ree mappings. Imamura et al. (2005) present a similar method that achieves significant improvements over a phrase-based baseline model for Japanese-English translation. Another method for handling syntactic differences is preprocessing, which is especially pertinent when the target language does not have parsing tools. These algorithms attempt to reconcile the word-order differences between the source and target language sentences by reordering the source language data prior to the SMT training and decoding cycles. NieBen and Ney (2004) propose some restructuring steps for German-English SMT. Popovic and Ney (2006) report the use of simple local transformation rules for SpanishEnglish and Serbian-English translation. Collins et al. (2005) propose German clause restructuring to improve German-English SMT, while Wang et al. (2007) present similar work for ChineseEnglish SMT. Our earlier work (Ananthakrishnan et al., 2008) describes syntactic reordering and morphological suffix separation for English-Hindi SMT. 3 Motivation The fundamental differences between English and Hindi are: • English follows SVO order, whereas Hindi follows SOV order • English uses post-modifiers, whereas Hindi uses pre-modifiers •</context>
</contexts>
<marker>Popovic, Ney, 2006</marker>
<rawString>Popovic, M., and Ney, H., Statistical Machine Translation with a Small Amount of Bilingual Training Data, 5th LREC SALTMIL Workshop on Minority Languages, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>M Collins</author>
<author>P Koehn</author>
</authors>
<title>Chinese Syntactic Reordering for Statistical Machine Translation,</title>
<date>2007</date>
<booktitle>Proceedings of the EMNLPCoNLL,</booktitle>
<contexts>
<context position="5818" citStr="Wang et al. (2007)" startWordPosition="879" endWordPosition="882">preprocessing, which is especially pertinent when the target language does not have parsing tools. These algorithms attempt to reconcile the word-order differences between the source and target language sentences by reordering the source language data prior to the SMT training and decoding cycles. NieBen and Ney (2004) propose some restructuring steps for German-English SMT. Popovic and Ney (2006) report the use of simple local transformation rules for SpanishEnglish and Serbian-English translation. Collins et al. (2005) propose German clause restructuring to improve German-English SMT, while Wang et al. (2007) present similar work for ChineseEnglish SMT. Our earlier work (Ananthakrishnan et al., 2008) describes syntactic reordering and morphological suffix separation for English-Hindi SMT. 3 Motivation The fundamental differences between English and Hindi are: • English follows SVO order, whereas Hindi follows SOV order • English uses post-modifiers, whereas Hindi uses pre-modifiers • Hindi allows greater freedom in word-order, identifying constituents through case marking • Hindi has a relatively richer system of morphology We resolve the first two syntactic differences by reordering the English s</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Wang, C., Collins, M., and Koehn, P., Chinese Syntactic Reordering for Statistical Machine Translation, Proceedings of the EMNLPCoNLL, 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>