<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998407666666667">
Cross-lingual Sentiment Lexicon
Learning With Bilingual Word
Graph Label Propagation
</title>
<author confidence="0.745446">
Dehong Gao†,*
The Hong Kong Polytechnic University
Furu Wei**
</author>
<affiliation confidence="0.707713">
Microsoft Research
</affiliation>
<author confidence="0.679619">
Wenjie Li†
The Hong Kong Polytechnic University
Xiaohua Liu**
</author>
<affiliation confidence="0.508092">
Microsoft Research
</affiliation>
<author confidence="0.626858">
Ming Zhou**
</author>
<affiliation confidence="0.200313">
Microsoft Research
</affiliation>
<bodyText confidence="0.998812214285714">
In this article we address the task of cross-lingual sentiment lexicon learning, which aims
to automatically generate sentiment lexicons for the target languages with available English
sentiment lexicons. We formalize the task as a learning problem on a bilingual word graph,
in which the intra-language relations among the words in the same language and the inter-
language relations among the words between different languages are properly represented.
With the words in the English sentiment lexicon as seeds, we propose a bilingual word graph
label propagation approach to induce sentiment polarities of the unlabeled words in the target
language. Particularly, we show that both synonym and antonym word relations can be used
to build the intra-language relation, and that the word alignment information derived from
bilingual parallel sentences can be effectively leveraged to build the inter-language relation. The
evaluation of Chinese sentiment lexicon learning shows that the proposed approach outperforms
existing approaches in both precision and recall. Experiments conducted on the NTCIR data set
further demonstrate the effectiveness of the learned sentiment lexicon in sentence-level sentiment
classification.
</bodyText>
<note confidence="0.585508">
* Contribution during internship at Microsoft Research (Beijing).
** Furu Wei, Xiaohua Liu, and Ming Zhou are from Microsoft Research, Beijing, China.
</note>
<email confidence="0.450697">
E-mail: {fuwei, xiaoliu, mingzhou}®microsoft.com.
</email>
<note confidence="0.963458857142857">
† Dehong Gao and Wenjie Li are from the Department of Computing, the Hong Kong Polytechnic
University. E-mail: gaodehong polyu®163.com, cswjli®comp.polyu.edu.hk.
Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication:
12 May 2014.
doi:10.1162/COLI a 00207
© 2015 Association for Computational Linguistics
Computational Linguistics Volume 41, Number 1
</note>
<sectionHeader confidence="0.979958" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999259625">
A sentiment lexicon is regarded as the most valuable resource for sentiment analysis
(Pang and Lee 2008), and lays the groundwork of much sentiment analysis research,
for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy
2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating
sentiment words, an automatically learning sentiment lexicon has attracted consider-
able attention in the community of sentiment analysis. The existing work determines
word sentiment polarities either by the statistical information (e.g., the co-occurrence
of words with predefined sentiment seed words) derived from a large corpus (Riloff,
Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information
(e.g., synonym relations) found in existing human-created resources (e.g., WordNet)
(Takamura, Inui, and Okumura 2005; Rao and Ravichandran 2009). However, current
work mainly focuses on English sentiment lexicon generation or expansion, while
sentiment lexicon learning for other languages has not been well studied.
In this article, we address the issue of cross-lingual sentiment lexicon learning,
which aims to generate sentiment lexicons for a non-English language (hereafter re-
ferred to as “the target language”) with the help of the available English sentiment
lexicons. The underlying motivation of this task is to leverage the existing English
sentiment lexicons and substantial linguistic resources to label the sentiment polarities
of the words in the target language. To this end, we need an approach to transferring
the sentiment information from English words to the words in the target language.
The few existing approaches first build word relations between English and the target
language. Then, based on the word relation and English sentiment seed words, they
determine the sentiment polarities of the words in the target language. In these two
steps, relation-building plays a fundamental role because it is responsible for the trans-
fer of sentiment information between the two languages. Two approaches are often
used to connect the words in different languages in the literature. One is based on
translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on
a machine translation (MT) engine as a black box to translate the sentiment words in
English to the target language (Steinberger et al. 2011). The two approaches in Duh,
Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small
set of vocabularies to translate the natural language, which leads to a low coverage of
generated sentiment lexicons for the target language.
To solve this problem, we propose a generic approach to addressing the task of
cross-lingual sentiment lexicon learning. Specifically, we model this task with a bilin-
gual word graph, which is composed of two intra-language subgraphs and an inter-
language subgraph. The intra-language subgraphs are used to model the semantic
relations among the words in the same languages. When building them, we incorporate
both synonym and antonym word relations in a novel manner, represented by positive
and negative sign weights in the subgraphs, respectively. These two intra-language
subgraphs are then connected by the inter-language subgraph. We propose Bilingual
word graph Label Propagation (BLP), which simultaneously takes the inter-language
relations and the intra-language relations into account in an iterative way. Moreover,
we leverage the word alignment information derived from a parallel corpus to build
the inter-language relations. We connect two words from different languages that are
aligned to each other in a parallel sentence pair. Taking advantage of a large parallel
corpus, this approach significantly improves the coverage of the generated sentiment
lexicon. The experimental results on Chinese sentiment lexicon learning show the ef-
fectiveness of the proposed approach in terms of both precision and recall. We further
</bodyText>
<page confidence="0.995261">
22
</page>
<note confidence="0.929501">
Gao et al. Cross-lingual Sentiment Lexicon Learning
</note>
<bodyText confidence="0.723074">
evaluate the impact of the learned sentiment lexicon on sentence-level sentiment clas-
sification. When using words in the learned sentiment lexicon as features for sentiment
classification of the target language, the sentiment classification can achieve a high
performance.
We make the following contributions in this article.
1. We present a generic approach to automatically learning sentiment
lexicons for the target language with the available sentiment lexicon in
English, and we formalize the cross-lingual sentiment learning task on a
bilingual word graph.
2. We build a bilingual word graph by using synonym and antonym word
relations and propose a bilingual word graph label propagation approach,
which effectively leverages the inter-language relations and both types
(synonym and antonym) of the intra-language relations in sentiment
lexicon learning.
3. We leverage the word alignment information derived from a large
number of parallel sentences in sentiment lexicon learning. We build the
inter-language relation in the bilingual word graph upon word alignment,
and achieve significant results.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="related work">
2. Related Work
</sectionHeader>
<subsectionHeader confidence="0.884213">
2.1 English Sentiment Lexicon Learning
</subsectionHeader>
<bodyText confidence="0.99997808">
In general, the work on sentiment lexicon learning focuses mainly on English and can
be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown
1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches
(Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy
2004).
The co-occurrence-based approaches determine the sentiment polarity of a given
word according to the statistical information, like the co-occurrence of the word to pre-
defined sentiment seed words or the co-occurrence to product features. The statistical
information is mainly derived from certain corpora. One of the earliest work conducted
by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can
convey the polarity relation of the two words they connect. For example, the conjunction
word and tends to link two words with the same polarity, whereas the conjunction
word but is likely to link two words with opposite polarities. Their approach only
considers adjectives, not nouns or verbs, and it is unable to extract adjectives that are
not conjoined by conjunctions. Riloff et al. (2003) define several pattern templates and
extract sentiment words by two bootstrapping approaches. Turney and Littman (2003)
calculate the pointwise mutual information (PMI) of a given word with positive and
negative sets of sentiment words. The sentiment polarity of the word is determined
by average PMI values of the positive and negative sets. To obtain PMI, they provide
queries (consisting of the given word and the sentiment word) to the search engine.
The number of hits and the position (if the given word is near the sentiment word)
are used to estimate the association of the given word to the sentiment word. Hu and
Liu (2004) research sentiment word learning on customer reviews and they assume that
the sentiment words tend to be correlated with product features. The frequent nouns
and noun phrases are treated as product features. Then they extract the adjective words
</bodyText>
<page confidence="0.996115">
23
</page>
<note confidence="0.839598">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999961666666667">
as sentiment words from those sentences that contain one or more product features.
This approach may work on a product review corpus, where one product feature may
frequently appear. But for other corpora, like news articles, this approach may not
be effective. Qiu et al. (2011) combine sentiment lexicon learning and opinion target
extraction. A double propagation approach is proposed to learn sentiment words and
to extract opinion targets simultaneously, based on eight manually defined rules.
The semantic-based approaches determine the sentiment polarity of a given
word according to the word semantic relation, like the synonyms of sentiment seed
words. The word semantic relation is usually obtained from dictionaries, for example,
WordNet.1 Kim and Hovy (2004) assume that the synonyms of a positive (negative)
word are positive (negative) and its antonyms are negative (positive). Initializing with
a set of sentiment words, they expand sentiment lexicons based on these two kinds of
word relations. Kamps et al. (2004) build a synonym graph according to the synonym
relation (synset) derived from WordNet. The sentiment polarity of a word is calculated
by the shortest path to two sentiment words good and bad. However, the shortest path
cannot precisely describe the sentiment orientation, considering there are only five steps
between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura
et al. (2005) construct a word graph with the gloss of WordNet. Words are connected
if a word appears in the gloss of another. The word sentiment polarity is determined
by the weight of its connections on the word graph. Based on WordNet, Rao and
Ravichandran (2009) exploit several graph-based semi-supervised learning methods
like Mincuts and Label Propagation. The word polarity orientations are induced by ini-
tializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and
Baccianella et al. (2010) treat sentiment word learning as a machine learning problem,
that is, to classify the polarity orientations of the words in WordNet. They select seven
positive words and seven negative words and expand them through the see-also and
antonym relations in WordNet. These expanded words are then used for training. They
train a ternary classifier to predict the sentiment polarities of all the words in WordNet
and use the glosses (textual definitions of the words in WordNet) as the features of
classification. The sentiment lexicon generated is the well-known SentiWordNet.2
</bodyText>
<subsectionHeader confidence="0.99912">
2.2 Cross-Lingual Sentiment Lexicon Learning
</subsectionHeader>
<bodyText confidence="0.999837615384615">
The work on cross-lingual sentiment lexicon learning is still at an early stage and can be
categorized into two types, according to how they bridge the words in two languages.
Mihalcea et al. (2007) generate sentiment lexicon for Romanian by directly translat-
ing the English sentiment words into Romanian through bilingual English–Romanian
dictionaries. When confronting multiword translations, they translate the multiwords
word by word. Then the validated translations must occur at least three times on the
Web. The approach proposed by Hassan et al. (2011) learns sentiment words based on
English WordNet and WordNets in the target languages (e.g., Hindi and Arabic). Cross-
lingual dictionaries are used to connect the words in two languages and the polarity of
a given word is determined by the average hitting time from the word to the English
sentiment word set. These approaches connect words in two languages based on cross-
lingual dictionaries. The main concern of these approaches is the effect of morphological
inflection (i.e., a word may be mapped to multiple words in cross-lingual dictionaries).
</bodyText>
<footnote confidence="0.998217">
1 http://wordnet.princeton.edu/.
2 http://sentiwordnet.isti.cnr.it/.
</footnote>
<page confidence="0.998821">
24
</page>
<note confidence="0.627736">
Gao et al. Cross-lingual Sentiment Lexicon Learning
</note>
<bodyText confidence="0.99994085">
For example, one single English word typically has four Spanish or Italian word forms
(two each for gender and for number) and many Russian word forms (due to gender,
number, and case distinctions) (Steinberger et al. 2011). Usually, this approach requires
an additional process to disambiguate the sentiment polarities of all the morphological
variants.
To improve the sentiment classification for the target language, Banea, Mihalcea,
and Wiebe (2010) translate the English sentiment lexicon into the target language using
Google Translator.3 Similarly, Google Translator is used by Steinberger et al. (2011). They
manually produce two high-level gold-standard sentiment lexicons for two languages
(e.g., English and Spanish) and then translate them into the third language (e.g., Italian)
via Google Translator. They believe that those words in the third language that appear
in both translation lists are likely to be sentiment words. These approaches connect the
words in two languages based on MT engines. The main concern of these approaches is
the low overlapping between the vocabularies of natural documents and the vocabular-
ies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng
et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low
coverage.
Our task resembles the task of cross-lingual sentiment classification, like Wan
(2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities
of product reviews. Generally, these studies use semi-supervised learning approaches
and regard translations from labeled English sentiment reviews as the training data.
The terms in each review are leveraged as the features for training, which has proven
to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of
sentiment lexicon learning as word-level sentiment classification. However, for word-
level sentiment classification, it is not straightforward to extract features for a single
word. Without sufficient features, it is difficult for these approaches to perform well
in learning. Another line of cross-lingual sentiment classification uses Latent Dirichlet
Allocation (LDA) (Blei, Ng, and Jordan 2003) or its variants, like Boyd-Graber and
Resnik (2010) or He, Alani, and Zhou (2010). These studies assume that each review is a
mixture of sentiments and each sentiment is a probability over words. Then they apply
the LDA-like approach to model the sentiment polarity of each review. Nonetheless,
this assumption may not be applicable in sentiment lexicon learning because a single
word can be regarded as the minimal semantic unit, and it is difficult, if not impossible,
to infer the latent topics from a single word. Recall that different from the sentiment
classification of product reviews where the instances are normally independent, words
in sentiment lexicon learning are highly related with each other, like synonyms and
antonyms. Through these relations, the words can naturally form a word graph. Thus
we use the graph-based learning approach to leverage the word distributions in senti-
ment lexicon learning. In the next section, we will introduce our proposed graph-based
cross-lingual sentiment lexicon learning.
</bodyText>
<sectionHeader confidence="0.96054" genericHeader="method">
3. Cross-Lingual Sentiment Lexicon Learning
</sectionHeader>
<bodyText confidence="0.9995655">
In this work, we model the task of cross-lingual sentiment lexicon learning with a
bilingual word graph, where (1) the words in the two languages are represented by
the nodes in two intra-language subgraphs, respectively; (2) the synonym and antonym
word relations within each language are represented by the positive and negative sign
</bodyText>
<footnote confidence="0.769314">
3 http://translate.google.com/.
</footnote>
<page confidence="0.994859">
25
</page>
<note confidence="0.356315">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999591375">
weights in the corresponding intra-language subgraphs; and (3) the two intra-language
subgraphs are connected by an inter-language subgraph. Mathematically, we build a
graph 9 = (XE U XT,WE U WE U WT U WT U WA) that consists of two intra-language
subgraphs 9E = (XE, WE UWE) and 9T = (XT, WT U WT) as shown in Figure 1. These
two subgraphs are connected by the inter-language graph 9R = (XE U XT, WA). The
elements of WE, WT, and WA are positive real numbers, that is, WE, WT, and WA
E R+, and WE and WT E R−. Because 9 incorporates the words in two languages,
we call it a Bilingual Word Graph. Specifically, the positive weights, WE and WT,
represent the synonym intra-language relations, and the negative weights, WE and
WT, represent the antonym intra-language relations. The inter-language relations, WA,
represent the connections between the words in the two languages. For cross-lingual
sentiment lexicon learning, XE = {XLE, XUE } denotes the labeled and unlabeled words in
English and XT denotes the unlabeled words in the target language. Given the labels
YLE = {yE1, ..., yEt} of the seeds XLE, we aim to predict the sentiment polarities of the
words XT. In the remainder of this section, we will present the bilingual word graph
construction and the algorithm of bilingual word graph label propagation.
</bodyText>
<subsectionHeader confidence="0.999512">
3.1 Bilingual Word Graph Building With Parallel Corpus and Word Alignment
</subsectionHeader>
<bodyText confidence="0.999984266666667">
We represent the words in English and in the target language as the nodes of the
bilingual word graph. We use the synonym and antonym relations of the words in the
same language to build W and W in the intra-language graph, respectively. In the rest
of this section, we will focus on how to build the inter-language relation.
Intuitively, there are two ways to connect the words in two languages. One is to
insert links to the words if there exist entry mappings between the words in bilingual
dictionaries (e.g., the English–Chinese dictionary). This method is simple and straight-
forward, but it suffers from two limitations. (1) Dictionaries are static during a certain
period, whereas the sentiment lexicon evolves over time. (2) The entries in dictionaries
tend to be the expressions of formal and written languages, but people prefer using
colloquial language in expressing their sentiments or opinions on-line. These limitations
lead to the low coverage of the links from English to the target language. An alternative
way is to use an MT engine as a black box to build the inter-language relation. One can
send each word in English to a publicly available MT engine and get the translations
in the target language. Edges can then be inserted into the graph between the English
</bodyText>
<figureCaption confidence="0.631228">
Figure 1
</figureCaption>
<bodyText confidence="0.708795">
Bilingual word graph for cross-lingual sentiment lexicon learning.
</bodyText>
<page confidence="0.99391">
26
</page>
<note confidence="0.814924">
Gao et al. Cross-lingual Sentiment Lexicon Learning
</note>
<figureCaption confidence="0.917067">
Figure 2
</figureCaption>
<subsectionHeader confidence="0.960597">
Parallel sentences with word alignments.
</subsectionHeader>
<bodyText confidence="0.999962103448276">
words and their corresponding translations. This approach suffers from the problem of
low coverage as well because MT engines tend to use a small set of vocabularies (Duh,
Fujino, and Nagata 2011).
In this article, we propose to leverage a large bilingual parallel corpus, which is
readily available in the MT research community, to build the bilingual word graph. The
parallel corpus consists of a large number of parallel sentence pairs from two different
languages that have been used as the foundation of the state-of-the-art statistical MT
engines. Like the example shown in Figure 2, the two sentences in English and Chinese
are parallel sentences, which express the same meaning in different languages. We can
easily derive the word alignment from the sentence pairs, automatically using a state-
of-the-art toolkit, like GIZA++4 or BerkeleyAligner.5 In this example, the Chinese word
(happy) is linked to the English word happy and we say that these two words are
aligned. Similarly the English words best and wishes are both aligned to (wish).
The word alignment information encodes the rich association information between
the words from the two languages. We are therefore motivated to leverage the par-
allel corpus and word alignment to build the bilingual word graph for cross-lingual
sentiment lexicon learning. We take the words from both languages in the bilingual
parallel corpus as the nodes in the bilingual word graph, and build the inter-language
relations by connecting the two words that are aligned together in a sentence pair from
a parallel corpus. There are several advantages of using a parallel corpus to build the
inter-language subgraph. First, large parallel corpora are extensively used for training
statistical MT engines and can be easily reused in our task. The parallel sentence pairs
are usually automatically collected and mined from the Web. As a result, they contain
the different and practical variations of words and phrases embedded in sentiment
expressions. Second, the parallel corpus can be dynamically changed when necessary
because it is relatively easy to collect from the Web. Consequentially, the novel senti-
ment information inferred from the parallel corpus can easily update the existing sen-
timent lexicons. These advantages can greatly improve the coverage of the generated
sentiment lexicon, as demonstrated later in our experiments.
</bodyText>
<subsectionHeader confidence="0.999659">
3.2 Bilingual Word Graph Label Propagation
</subsectionHeader>
<bodyText confidence="0.999928">
As commonly used semi-supervised approaches, label propagation (Zhu and
Ghahramani 2002) and its variants (Zhu, Ghahramani, and Lafferty 2003; Zhou et al.
2004) have been applied to many applications, such as part-of-speech tagging (Das and
Petrov 2011; Li, Graca, and Taskar 2012), image annotation (Wang, Huang, and Ding
2011), protein function prediction (Jiang 2011; Jiang and McQuay 2012), and so forth.
</bodyText>
<footnote confidence="0.9938895">
4 http://www.statmt.org/moses/giza/GIZA++.html.
5 http://nlp.cs.berkeley.edu.
</footnote>
<page confidence="0.994185">
27
</page>
<note confidence="0.343163">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.99968475">
The underlying idea of label propagation is that the connected nodes in the graph tend
to share the same sentiment labels. In bilingual word graph label propagation, the
words tend to share same sentiment labels if they are connected by synonym relations
or word alignment and tend to belong to different sentiment labels if connected by
antonym relations.
In this article we propose bilingual word graph label propagation for cross-lingual
sentiment lexicon learning. Let F = {FE, FT} denote the predicted labels of the unlabeled
words X. The loss function can be defined as
</bodyText>
<equation confidence="0.999545">
El(F) = µ Xn kfEi − yEik2 + µ Xm kfTi − yTik2 (1)
i=1 i=1
</equation>
<bodyText confidence="0.999982714285714">
where n and m denote the numbers of English words and words in the target language.
Let Y = {YE, YT} denote the initial sentiment labels of all the words; the loss function
means that the prediction could not change too much from the initial label assignment.
Similar to Zhou et al. (2004), we define the smoothness function to indicate that if two
words are connected by synonym relation or by word alignment, then they tend to share
the same sentiment label. The smoothness function can be further represented by two
parts, that is, the inter-language smoothness Einter
</bodyText>
<figure confidence="0.9527787">
s (F) and the synonym intra-language
smoothness Eintra
s (F)
Einter Xn m wAijk fEi fTj k2
s(F) = 12 i=1 X pdALii − q (2) dARjj
j=1
Eintra Xn Ei −fEj k2 + J Tf2
s(F) = P112 i,j=1 wEij k P2 X wTij II i − k (3)
pdEii qdEjj 2 ij=1 v/dTii
qdTjj
</figure>
<bodyText confidence="0.9608322">
DAL and DAR are defined as DAL= diag(Pj wA(1, j), ..., Pj wA(n, j))′ and DAR=
diag(Pi wA(i, 1), ..., Pi wA(i, m))′. DE and DT are the degree matrices of the synonym
intra-language relations WE and WT, respectively. We then define the distance function
to indicate that if two words are connected by the antonym relation they tend to belong
to different sentiment labels. The distance function can be defined as
Eintra Xn fEi A&apos; 2 1 m fTi A 2
d(F) = P31 2 i,j=1 wEij |k 1 / - II + P4 2 X  |wTij  |k - k (4)
V dEii qdEjj ,j
=
1 A, q dTjj
where eDE and eDT are the degree matrices of the absolute value of the antonym intra-
language relations eWE and eWT, respectively. Intuitively, for the inter-language smooth-
ness and the synonym intra-language smoothness, the nearer the words connect with
each other, the better performance will be achieved, whereas for the antonym intra-
language distance, the farther the better. The objective functions can be defined as
</bodyText>
<equation confidence="0.99646725">
argmin(E(F)) = argmin(Eintra
s (F) + Einter
s (F) + El(F))
argmax(E(F)) = argmax(Eintra
</equation>
<bodyText confidence="0.540919">
d (F))
</bodyText>
<page confidence="0.964944">
28
</page>
<note confidence="0.519359">
Gao et al. Cross-lingual Sentiment Lexicon Learning
</note>
<bodyText confidence="0.999912">
Thus, we define the whole objective function for cross-lingual sentiment lexicon
learning as
</bodyText>
<equation confidence="0.99301975">
argmin(E(F)) = argmin(Eintra
s (F) + Einter
s (F) + El(F) − Eintra
d (F)) (5)
</equation>
<bodyText confidence="0.999315">
To obtain the solution to Equation (5), we differentiate the objective function according
to FE and FT, and we have
</bodyText>
<equation confidence="0.9740925">
∂E(F)
∂FE |FE=F⋆E= ρ1SEFE + 12SAFT − ρ39EFE + µFE − µYE = 0 (6)
∂E(F) (7)
∂FT |FT=F⋆T = ρ2STFT + 1 2SA′FE − ρ4�STFT + µFT − µYT = 0
</equation>
<bodyText confidence="0.936191">
where P′ is the transpose of the matrix P. The graph Laplacians SE and ST of the
synonym intra-language relations are (I − D−1
</bodyText>
<equation confidence="0.955202">
E WED− 1
2 E ) and (I − D− 1
2 T WTD− 1
2 T ), where
2
</equation>
<bodyText confidence="0.690599">
I is the identity matrix. The graph Laplacians SE and ST of the antonym intra-language
relations are (I − �D− 12
</bodyText>
<equation confidence="0.883199">
E �WE �D− 1
E ) and (I − �D− 12
2 T �WT �D− 12
</equation>
<bodyText confidence="0.886041">
T ), which has been proven to
be positive semi-definite (Kunegis et al. 2010). The graph Laplacian SA of the inter-
language relation is (I − D−1
</bodyText>
<figure confidence="0.4333522">
AL WAD−1
2 AR ). From Equations (6) and (7), we can obtain the
2
optimal solutions
(ME − SAM−1
T S′ A)FE = 2µ(YE − SAM−1
T YT) (8)
(MT − S′ AM−1
E SA)FT = 2µ(YT − S′ AM−1
E YE) (9)
</figure>
<bodyText confidence="0.955403363636364">
where ME = 2ρ1SE − 2ρ3�SE + 2µI and MT = 2ρ2ST − 2ρ4�ST + 2µI. To avoid computing
the inverse matrix in Equations (8) and (9), we apply the Jacobi algorithm (Saad 2003)
to calculate the solutions as described in Algorithm 1. In line 1, we set the label of the
positive seed xi as yLE+i = (1, 0) and the label of the negative seed xj as yLE−j = (0, 1). We
set the label YUE of the unlabeled words as zero, and then generate YE with YLE and YUE .
Line 2 sets YT as zero matrix. In line 3, we compute the matrixes SE, SE, ST, ST, SA, and
then compute the matrixes ME and MT. The sentiment information is simultaneously
Algorithm 1. Bilingual word graph label propagation
Input: Given G = (XE ∪ XT, WE ∪ WE ∪ WT ∪ WT ∪ WA),XE,
label YLE for XLE, initialize µ and ρ1∼4
Output: FT for XT and FE for XE
</bodyText>
<listItem confidence="0.9546005">
1. Initialize YE with the English sentiment seeds
2. Set YT as zero
3. Calculate SE, SE, ST, ST, and SA, then calculate ME and MT
4. Loop
5. r(t+1) = 1 1 ′ (2µ(YE − SAMT1YT)i − Ej6=i(ME − SAM−%)ijf(0)
�Ei(ME−SAMT SA)ii �.(
6. f( (MT−S′AME 1SA)ii (2µ(YT − S′AME 1YE)i − Ej6=i(MT − S′AME 1SA)ijJTt))
7. Until FE and FT convergence
</listItem>
<page confidence="0.925028">
29
</page>
<bodyText confidence="0.8693758">
Computational Linguistics Volume 41, Number 1
propagated through lines 4–7 until the predicted labels FE and FT are converged. For an
unlabeled word xi, if |f (i, 0) − f (i, 1) |&lt; &amp; (&amp; is set as 1.0E−4), xi is regarded as neutral; if
( f (i, 0) − f (i, 1)) &gt; &amp;, xi is regarded as positive; and if (f (i, 1) − f (i, 0)) &gt; &amp;, xi is regarded
as negative.
</bodyText>
<sectionHeader confidence="0.991874" genericHeader="method">
4. Experiment
</sectionHeader>
<subsectionHeader confidence="0.771542">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.99949125">
We conduct experiments on Chinese sentiment lexicon learning. As in previous
work (Baccianella, Esuli, and Sebastiani 2010), the sentiment words in General In-
quirer lexicon are selected as the English seeds (Stone 1997). From the GI lexicon we
collect 2,005 positive words and 1,635 negative words. To build the bilingual word
graph, we adopt the Chinese–English parallel corpus, which is obtained from the
news articles published by Xinhua News Agency in Chinese and English collections,
using the automatic parallel sentence identification approach (Munteanu and Marcu
2005). Altogether, we collect more than 25M parallel sentence pairs in English and
Chinese. We remove all the stopwords in Chinese and English (e.g., (of) and am)
together with the low-frequency words that occur fewer than 5 times. After prepro-
cessing, we finally have more than 174,000 English words, among which 3,519 words
have sentiment labels and more than 146,000 Chinese words for which we need to
predict the sentiment labels. To transfer sentiment information to Chinese unlabeled
words more efficiently, we remove the unlabeled English words in the word graph
(i.e., XUE = Φ). The unsupervised method, namely, BerkeleyAligner, is used to align the
parallel sentences in this article (Liang, Taskar, and Klein 2006). As an unsupervised
method, it does not require us to manually collect training data and does not need
the complex training processing, and its performance is competitive with supervised
methods. With these two advantages, we can focus more on our task of cross-lingual
sentiment lexicon learning. Based on the word alignment derived by BerkeleyAligner, the
inter-language WA is initialized with the normalized alignment frequency. The English
and Chinese v_ersions6 of WordNet are used to build the intra-language relations WE,
WE, WT, and WT, respectively. WordNet (Miller 1995) groups words into synonym sets,
called synsets. We collect about 117,000 synsets from the English WordNet and about
80,000 synsets from the Chinese WordNet. In total, we obtain 8,406 and 6,312 antonym
synset pairs.
We first generate both positive and negative scores for each unlabeled word
and then determine the word sentiment polarities based on its scores. We rank the
two sets of newly labeled sentiment words according to their polarity scores. The
top-ranked Chinese words are shown in Table 1. We manually label the top-ranked
1K sentiment words. For P@10K, we sequentially divide the top 10K ranked list
into ten equal parts. One hundred sentiment words are randomly selected from
each part for labeling. Similar to the evaluation of TREC Blog Distillation (Ounis,
Macdonald, and Soboroff 2008), all the labeled words from each approach are used in
the evaluation. We then evaluate the ranked lists with two metrics, Precision@K and
Recall.
</bodyText>
<footnote confidence="0.960026">
6 http://www.globalwordnet.org/gwa/wordnet table.html.
</footnote>
<page confidence="0.985494">
30
</page>
<note confidence="0.847797">
Gao et al. Cross-lingual Sentiment Lexicon Learning
</note>
<tableCaption confidence="0.992581">
Table 1
</tableCaption>
<table confidence="0.98814375">
The top learned Chinese sentiment words.
Word Meaning Polarity Word Meaning Polarity
good positive disaster negative
correct positive tragedy negative
useful positive dangerous negative
smart positive harm negative
happy positive fault negative
reliable positive rage negative
accurate positive fail negative
happy positive damage negative
optimistic positive sore negative
loyal positive clash negative
</table>
<subsectionHeader confidence="0.921992">
4.2 Evaluation of the Bilingual Word Graph
</subsectionHeader>
<bodyText confidence="0.928100538461538">
In this set of experiments, we examine the influence of graph topologies on sentiment
lexicon learning.
Mono: This approach learns the Chinese sentiment lexicon based only on the
Chinese monolingual word graph GT = (XT, WT ∪ WT). Because it needs labeled sen-
timent words, we incorporate the English labeled sentiment words XE and the inter-
language relation WA in the first iteration. Then we set XE and WA to be zero in later
iterations.
BLP-WOA (bilingual word graph without antonym): This approach is based on the
bilingual word graph. It only involves the inter-language relation WA and the synonym
intra-language relations WE and WT. WE and WT are set to be zero.
BLP: This approach is based on the bilingual word graph. It incorporates the inter-
language relation WA, the synonym intra-language relations WE and WT, and the
negative precision at k
</bodyText>
<figureCaption confidence="0.925939333333333">
Figure 3
Precision evaluation of the experiments based on monolingual (Mono), bilingual without
antonym (BLP-WOA), and bilingual (BLP) word graphs.
</figureCaption>
<figure confidence="0.9825141875">
antonym intra-language relations WE and WT.
0.8
0.6
0.4
0.2
P@100P@200 P@1K P@10K
positive precision at k
Mono
BLP−WOA BLP
P@100P@200 P@1K P@10K
0.8
0.6
0.4
0.2
Mono
BLP−WOA BLP
</figure>
<page confidence="0.991236">
31
</page>
<table confidence="0.48424">
Computational Linguistics Volume 41, Number 1
</table>
<tableCaption confidence="0.667047666666667">
Table 2
Recall evaluation of the experiments based on monolingual (Mono), bilingual without antonym
(BLP-WOA), and bilingual (BLP) word graphs.
</tableCaption>
<table confidence="0.995271333333333">
Chinese Mono BLP-WOA BLP
Positive 0.623 0.702 0.708
Negative 0.631 0.706 0.709
</table>
<bodyText confidence="0.999087466666667">
In these approaches, µ is set to 0.1 as in Zhou et al. (2004). The precision of these
approaches are shown in Figure 3. The figure shows that the approaches based on
the bilingual word graph significantly outperform the one based on the monolingual
word graph. The bilingual word graph can bring in more word relations and accelerate
the sentiment propagation. Besides, in the bilingual word graph, the English sentiment
seed words can continually provide accurate sentiment information. Thus we observe
the increase in the approaches based on the bilingual word graph in term of both
precision and recall (Table 2). Meanwhile, we find that adding the antonym relation in
the bilingual word graph slightly enhances precision in top-ranked words and similar
findings are observed in our later experiments. It appears that the antonym relations
depict word relations in a more accurate way and can refine the word sentiment scores
more precisely. However, the synonym relation and word alignment relation dominate,
whereas the antonym relation accounts for only a small percentage of the graph. It is
hard for the antonym relation to introduce new relations into the graph and thus it
cannot help to further improve recall.
</bodyText>
<subsectionHeader confidence="0.999716">
4.3 Comparison with Baseline and Existing Approaches
</subsectionHeader>
<bodyText confidence="0.945665777777778">
In this set of experiments, we compare our approach with the baseline and existing
approaches.
Rule: For the intra-language relation, this approach assumes that the synonyms
of a positive (negative) word are always positive (negative), and the antonyms of a
positive (negative) word are always negative (positive). For the inter-language relation,
we regard the Chinese word aligned to positive (negative) English words as positive
(negative). If a word connects to both positive and negative English words, we regard it
as objective. Based on this heuristic, we generate two sets of sentiment words.
SOP: Hassan et al. (2011) present a method to predict the semantic orientation of
unlabeled words based on the mean hitting time to the two sets of sentiment seed words.
Given the graph 9 = (XE U XT, WE U WE U WT U WT U WA), it defines the transition
probability from node i to node j as
p(j|i) = wi,j
Ek wi,k
The mean hitting time h(i|j) is the average number of the weighted steps from word i to
word j. Starting with the word i and ending with the sentiment word k E M, the mean
hitting time h(i|M) can be formally defined as
� 0, i E M
</bodyText>
<equation confidence="0.983969">
h(i|M) =
Ej∈V p(j|i) x h(j|M) + 1, otherwise
</equation>
<page confidence="0.975328">
32
</page>
<note confidence="0.763303">
Gao et al. Cross-lingual Sentiment Lexicon Learning
</note>
<bodyText confidence="0.999891461538462">
Let M+ and M_ denote the GI positive and negative seeds. If h(ilM+) is greater than
h(iJM_), the word xi is classified as negative; otherwise it is classified as positive.
The generated positive words and negative words are then ranked according to their
polarity scores, respectively.
MAD: Talukdar and Crammer (2009) propose a MAD algorithm to modify the
adsorption algorithm (Baluja et al. 2008) by adding a new regularization term. In
particular, besides the positive and negative labels, a dummy label is assigned to each
word in the MAD approach. Two additional columns, representing the scores of the
dummy label, are added into Y and F, respectively. We denote these two matrices with
the dummy labels as Y&amp;quot; and &amp;quot;F. Meanwhile, R&amp;quot; is used to represent the initial dummy
scores of all the words. For a word xi, the newly added columns in &amp;quot;Yi and &amp;quot;Fi are set to
zero (i.e., &amp;quot;y(i, 3) = f&amp;quot;(i, 3) = 0). rr&amp;quot;(i, 0) and r&amp;quot;(i, 1) are set to zero, and r&amp;quot;(i, 3) is assigned to one.
Then, the predicted label &amp;quot;Fi of the word xi is iteratively obtained by
</bodyText>
<equation confidence="0.9811134">
0+1) = 1 (λ1 �&amp;quot;Yi + λ2 Wij &amp;quot;F(t)
t j j + λ3γ&amp;quot;Ri)
Mii
&amp;quot;Mii = (λ1 + λ2 � Wji + λ3)
j#i
</equation>
<bodyText confidence="0.999311380952381">
λ1-3 and γ are used to tune the importance of each iteration term. We set λ1-2 to one,
λ3 to 10, and γ to 0.1, which produces reasonably good results. After propagation, f&amp;quot;(i, 0)
and f&amp;quot;(i, 1) are used to determine the sentiment polarity of the word xi.
We show recall of the learned Chinese sentiment words in Table 3. Compared with
BLP and SOP, the Rule approach learns fewer sentiment words. The coverage of the
Rule approach is inevitably low because many words in the corpus are aligned to both
positive and negative words. For example, in most cases the positive Chinese word
(helpful) is aligned to the positive English word helpful. But sometimes it is aligned
(or misaligned) to the negative English words, like freak. Under this situation, the word
tends to be predicted as objective. In SOP, the positive and negative scores are related
to the distances of the word to the positive and negative seed words, and the distance is
usually coarse-grained to depict the sentiment polarity. For example, the shortest path
between the word good and the word bad in WordNet is only 5 (Kamps et al. 2004). The
Rule and SOP approaches find different sentiment words. We then evaluate the learned
Chinese polarity word lists by precision at k. As illustrated in Figure 4, the significance
test indicates that our approach significantly outperforms the Rule and SOP approaches.
The major difference of our approach is that the polarity information can be transferred
between English and Chinese and within each language at the same time, whereas in
the other two approaches the polarity information mainly transfers from English to
Chinese and once a word gets a polarity score, it is difficult to change or refine. The
idea of the MAD approach is similar to bilingual graph label propagation, but the MAD
</bodyText>
<tableCaption confidence="0.798392">
Table 3
Recall evaluation of the Rule, SOP, MAD, and BLP approaches.
</tableCaption>
<table confidence="0.875807">
Chinese Rule SOP MAD BLP
Positive 0.382 0.604 0.662 0.708
Negative 0.371 0.582 0.681 0.709
</table>
<page confidence="0.994018">
33
</page>
<figure confidence="0.577143">
negative precision at k
</figure>
<figureCaption confidence="0.877154">
Figure 4
Precision evaluation of the Rule, SOP, MAD, and BLP approaches.
</figureCaption>
<bodyText confidence="0.999138875">
approach fails to leverage the antonym intra-language relation. We observe that the
MAD approach can achieve a comparable result to the BLP approach. MAD can obtain a
smoother label score by adding a dummy label. But the dummy label does not influence
the sentiment labels too much because it is not used in the determination of the word
sentiment polarity. Besides, MAD cannot deal with the antonym relation. As a result,
these experiments demonstrate the overall superiority of our approach in cross-lingual
sentiment lexicon learning. This also indicates the effectiveness of the BLP approach in
Chinese sentiment lexicon learning.
</bodyText>
<subsectionHeader confidence="0.997013">
4.4 Evaluation of the Inter-Language Relation
</subsectionHeader>
<bodyText confidence="0.998713941176471">
This set of experiments is to examine the ways to build the inter-language relation.
BLP-dict: The inter-language relation is built upon the translation entries from LDC7
and Universal Dictionary (UD).8 From these dictionaries (both English–Chinese and
Chinese–English dictionaries), we collect 41,034 translation entries between the English
and Chinese words. If the English word xi can be translated to the Chinese word xj in
UD dictionary, wA(i, j) and wA(j, i) are set to 1.
BLP-MT: All the Chinese (English) words are translated into English (Chinese) by
Google Translator. If the Chinese word xi can be translated to the English word xj, the
wA(i, j) and wA(j, i) are set to 1. If a Chinese word is translated to an English phrase,
we assume that the Chinese word is projected to each word in the English phrase. To
improve the coverage, we translate the English sentiment seed words with three other
methods; they are word collocation, coordinated phrase, and punctuation, as mentioned
in Meng et al. (2012b).
The learned Chinese sentiment word lists are also evaluated with precision at
k. As shown in Figure 5, we find that the alignment-based approach outperforms
the dictionary-based and MT-based approaches. The reason that contributes to this is
that we can build more inter-language relations based on word alignment, compared
</bodyText>
<footnote confidence="0.6322465">
7 http://projects.ldc.upenn.edu/Chinese/LDC ch.htm.
8 http://www.dicts.info/uddl.php.
</footnote>
<figure confidence="0.982113441176471">
Computational Linguistics Volume 41, Number 1
BLP Rule SOP MAD
P@100P@200 P@1K P@10K
positive precision at k
BLP Rule SOP MAD
P@100P@200 P@1K P@10K
0.8
0.6
0.4
0.2
0.8
0.6
0.4
0.2
34
Gao et al. Cross-lingual Sentiment Lexicon Learning
BLP BLP−dict BLP−MT
P@100P@200 P@1K P@10K
P@100P@200 P@1K P@10K
positive precision at k
BLP BLP−dict BLP−MT
0.8
0.7
0.6
0.5
0.4
0.3
0.8
0.7
0.6
0.5
0.4
0.3
negative precision at k
</figure>
<figureCaption confidence="0.9207635">
Figure 5
Influence on Precision of the inter-language relation.
</figureCaption>
<bodyText confidence="0.996193352941177">
with the translation entries from the dictionary and the translation pairs from Google
Translator. For example, the English word move is often translated to (shift) and
(affect, touch) by dictionaries or MT engines. From the parallel sentences, besides
these word translation pairs, the word move can be also aligned to (plain
sailing bon voyage) that is commonly used in Chinese greeting texts. This translation
entry is hard to find in dictionaries or by MT engines. The words are aligned between
the two parallel sentences. Sometimes the word move may be forced to be aligned
to in the parallel sentences good luck and best wishes on your career move and
. Thus, when building the inter-language relations with word
alignment, our approach is likely to learn more sentiment word candidates. It is also
the reason why the dictionary-based and MT-based approaches learn fewer sentiment
words than our approach, as indicated in Table 4. According to our statistic, on average
a Chinese word is connected to 2.3 and 2.1 English words if we build the inter-language
relations with the dictionary and Google Translator, respectively. By building the
inter-language relation with word alignment, our approach connects a Chinese word
to 16.21 English words an average, which greatly increases the coverage of the learned
sentiment lexicon.
</bodyText>
<subsectionHeader confidence="0.976575">
4.5 Evaluation of the Intra-Language Relation
</subsectionHeader>
<bodyText confidence="0.999197">
The following set of experiments reveals the influence of the intra-language relation.
BLP-A: As the baseline of this set of experiments, it does not build the intra-
language relations with either English or Chinese WordNet synsets. Only the
</bodyText>
<tableCaption confidence="0.9334725">
Table 4
Influence on Recall of the inter-language relation.
</tableCaption>
<subsectionHeader confidence="0.593062">
Chinese BLP-dict BLP-MT
</subsectionHeader>
<bodyText confidence="0.711071">
Positive 0.649 0.654
Negative 0.660 0.679
</bodyText>
<page confidence="0.991063">
35
</page>
<figure confidence="0.543705">
negative precision at k
</figure>
<figureCaption confidence="0.8713705">
Figure 6
Influence on Precision of the intra-language relation.
</figureCaption>
<bodyText confidence="0.999040533333333">
inter-language relation with word alignment is used to build the graph. That means
WE, WE, WT, and WT are defined as zero matrixes.
BLP-AE: Word alignment and the English WordNet synsets are used to build the
intra-English relation WE, but the intra-Chinese relation WT and WT are set to zero
matrixes.
BLP-AC: Word alignment and the Chinese WordNet synsets are used to build the
intra-Chinese relation WT, but the intra-English relation WE and WE are set to zero
matrixes.
As Figure 6 shows, when combining both English and Chinese intra-language
relations, the precision curves of both positive and negative predictions increase. This
indicates that adding the intra-language relations has a positive influence. The im-
provement can be explained by the ability of the intra-language relations to refine
the polarity scores. For example, the English word sophisticated can be aligned to the
positive Chinese word (delicate) as well as the negative Chinese word
(wily, wicked). In the GI lexicon, the English word sophisticated is labeled as posi-
tive. In the bilingual word graph that contains only the inter-language relations, the
negative Chinese word is likely to be labeled as positive. However, with the
intra-language relation, the negative Chinese word may connect to the other
negative Chinese words, like (foxy); and the Chinese positive word
may connect to the other positive Chinese words, like (elaborate). Thus the
polarity score of the word can be refined by the intra-language relation in each iteration
of propagation. Another advantage of the intra-language relation is that it helps to
reduce the noise introduced by the inter-language relation. For example, sometimes
the Chinese positive word (help) is misaligned to the negative English word freak
by the inter-language relation, but it is also connected to the synonyms (help)
and (salutary) (which are positive) by the intra-language relations. The polarity
score of the word can be adjusted by the intra-language relation. Thus, though
the inter-language relation brings in certain noisy alignments, the intra-language re-
lation can help to refine the polarity score of the word using its intra-language
relation.
</bodyText>
<figure confidence="0.991300777777778">
Computational Linguistics Volume 41, Number 1
P@100P@200 P@1K P@10K
BLP BLP−A BLP−AE BLP−AC
positive precision at k
P@100P@200 P@1K P@10K
BLP BLP−A BLP−AE BLP−AC
0.8
0.7
0.6
0.5
0.4
0.3
0.8
0.7
0.6
0.5
0.4
0.3
</figure>
<page confidence="0.990651">
36
</page>
<note confidence="0.918168">
Gao et al. Cross-lingual Sentiment Lexicon Learning
</note>
<tableCaption confidence="0.996483">
Table 5
</tableCaption>
<table confidence="0.791763">
Numbers of labeled sentiment sentences.
Positive Negative Neutral
sentence number 1,218 944 528
</table>
<subsectionHeader confidence="0.997676">
4.6 Sensitivity of Parameter p1∼4
</subsectionHeader>
<bodyText confidence="0.996367333333333">
p1 and p2 in Equation (3) tune the English and Chinese synonym intra-language prop-
agation, while p3 and p4 in Equation (4) adjust the English and Chinese antonym intra-
language propagation. For simplicity, let p1 equal p2 and let p3 equal p4. Then we tune
p1,2 and p3,4 together. When p1,2 and p3,4 range from {1e − 2, 1e − 1, 1, 10, 100, 1000},
Precision@1K ranges from 0.631 to 0.689 and Recall ranges from 0.651 to 0.729 on
average. In general, we find that when 1 &lt; p3,4 &lt; p1,2 &lt; 10, we can obtain better results.
</bodyText>
<subsectionHeader confidence="0.991117">
4.7 Evaluation on Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999274052631579">
Sentiment classification is one of the most extensively studied tasks in the community of
sentiment analysis (Pang and Lee 2008). To see whether the performance improvement
in lexicon learning also improves the results of sentiment classification, we apply the
generated Chinese sentiment lexicons to sentence-level sentiment classification.
Data set: The NTCIR sentiment-labeled corpus is used for sentiment classification
(Seki et al. 2008, 2009). We extract the Chinese sentences that have positive, negative,
or neutral labels. The numbers of extracted sentences are shown in Table 5. The learned
sentiment words in the Mono and BLP approaches are used as classification features.
We implement the following baselines for comparison.
BSL DF: The Chinese word unigrams and bigrams are extracted from the NTCIR
data set as features. We rank the features according to their frequencies and gradually
increase the value of N for the Top-N classification features.
BSL LF: The words in existing Chinese sentiment lexicons are used as features. A
total of 836 positive words and 1,254 negative words are collected from HowNet.9
We use LibSVM10 and perform 10-fold cross-validation on the NTCIR polarity sen-
tences. The accuracies over N number of features are plotted in Figure 7. Our approach
achieves a very promising improvement, although the features and the sentences that
need to be classified are selected from different corpora. This suggests that the generated
sentiment lexicon is adaptive and qualitative enough for sentiment classification.
</bodyText>
<sectionHeader confidence="0.964549" genericHeader="conclusions">
5. Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998009">
In this article, we studied the task of cross-lingual sentiment lexicon learning. We built
a bilingual word graph with the words in two languages and connected them with the
inter-language and intra-language relations. We proposed a bilingual word graph label
propagation approach to transduce the sentiment information from English sentiment
words to the words in the target language. The synonym and antonym relations among
</bodyText>
<footnote confidence="0.9078055">
9 http://www.keenage.com/.
10 http://www.csie.ntu.edu.tw/∼cjlin/libsvm/.
</footnote>
<page confidence="0.995765">
37
</page>
<figure confidence="0.844189333333333">
Computational Linguistics Volume 41, Number 1
Evaluation of sentence−level sentiment classification
Top−N frequency or ranked features
</figure>
<figureCaption confidence="0.994565">
Figure 7
</figureCaption>
<subsectionHeader confidence="0.798215">
Accuracy evaluation on sentiment classification.
</subsectionHeader>
<bodyText confidence="0.999920090909091">
the words in the same languages are leveraged to build the intra-language relations.
Word alignment derived from a large parallel corpus is used to build the inter-language
relations. Experiments on Chinese sentiment lexicon learning demonstrate the effec-
tiveness of the proposed approach. There are three main conclusions from this work.
First, the bilingual word graph is suitable for sentiment information transfer and the
proposed approach can iteratively improve the precision of the generated sentiment
lexicon. Second, building the inter-language relations with the large parallel corpus can
significantly improve the coverage. Third, by incorporating the antonym relations into
the bilingual word graph, the BLP approach can achieve an improvement in precision.
In the future, we will explore the opportunity of expanding or generating the sentiment
lexicons for multiple languages by bootstrapping.
</bodyText>
<figure confidence="0.994658357142857">
0.8
0.79
0.78
Accuracy
0.77
0.76
BSL_DF
BLP
BSL_LF
Mono
0.73
1K 2K 5K 8K 10K
0.75
0.74
</figure>
<sectionHeader confidence="0.923855" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9199824">
The work described in this article was
supported by a Hong Kong RGC project
(PolyU no. 5202/12E) and a National
Nature Science Foundation of China
(NSFC no. 61272291).
</bodyText>
<sectionHeader confidence="0.992217" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98829056097561">
Baccianella, Stefano, Andrea Esuli, and
Fabrizio Sebastiani. 2010. Sentiwordnet
3.0: An enhanced lexical resource for
sentiment analysis and opinion mining.
In Proceedings of the 7th International
Conference on Language Resources and
Evaluation, pages 2,200–2,204, Malta.
Baluja, Shumeet, Rohan Seth, D. Sivakumar,
Yushi Jing, Jay Yagnik, Shankar Kumar,
Deepak Ravichandran, and Mohamed Aly.
2008. Video suggestion and discovery for
Youtube: Taking random walks through
the view graph. In Proceedings of the
17th International Conference on the World
Wide Web, pages 895–904, Beijing.
Banea, Carmen, Rada Mihalcea, and Janyce
Wiebe. 2010. Multilingual subjectivity:
Are more languages better? In Proceedings
of the 23rd International Conference on
Computational Linguistics, pages 28–36,
Beijing.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
3:993–1022.
Boyd-Graber, Jordan and Philip Resnik.
2010. Holistic sentiment analysis across
languages: Multilingual supervised latent
Dirichlet allocation. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, pages 45–55,
Boston, MA.
Das, Dipanjan and Slav Petrov. 2011.
Unsupervised part-of-speech tagging
with bilingual graph-based projections.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 600–609, Portland, OR.
Duh, Kevin, Akinori Fujino, and Masaaki
Nagata. 2011. Is machine translation ripe
</reference>
<page confidence="0.997924">
38
</page>
<note confidence="0.938962">
Gao et al. Cross-lingual Sentiment Lexicon Learning
</note>
<reference confidence="0.993211347457628">
for cross-lingual sentiment classification?
In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics,
pages 429–433, Portland, OR.
Esuli, Andrea and Fabrizio Sebastiani. 2006.
Sentiwordnet: A publicly available lexical
resource for opinion mining. In Proceedings
of the 3rd International Conference on
Language Resources and Evaluation,
pages 417–422, Las Palmas.
Esuli, Andrea and Fabrizio Sebastiani.
2007. Random-walk models of term
semantics: An application to opinion-
related properties. In Proceedings of the
3rd Language and Technology Conference,
pages 221–225, Poznan.
Hassan, Ahmed, Amjad Abu-Jbara, Rahul
Jha, and Dragomir Radev. 2011. Identifying
the semantic orientation of foreign words.
In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics,
pages 592–597, Portland, OR.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics and 8th Conference
of the European Chapter of the Association for
Computational Linguistics, pages 174–181,
Madrid.
He, Yulan, Harith Alani, and Deyu Zhou.
2010. Exploring English lexicon
knowledge for Chinese sentiment analysis.
In Proceedings of the 2010 CIPS-SIGHAN
Joint Conference on Chinese Language
Processing, Beijing.
Hu, Minqing and Bing Liu. 2004. Mining
and summarizing customer reviews.
In Proceedings of the 10th ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 168–177,
Seattle, WA.
Hu, Minqing and Bing Liu. 2006. Opinion
extraction and summarization on the Web.
In Proceedings of the 21st National Conference
on Artificial Intelligence, pages 1,621–1,624,
Boston, MA.
Jiang, Jonathan Q. 2011. Learning protein
functions from bi-relational graph of
proteins and function annotations.
Algorithms in Bioinformatics, 6833:128–138,
Springer Berlin Heidelberg.
Jiang, Jonathan Q. and Lisa J. McQuay. 2012.
Predicting protein function by multi-label
correlated semi-supervised learning.
IEEE/ACM Transactions on Computational
Biology and Bioinformatics, 9(4):1059–1069.
Kamps, Jaap, Maarten Marx, Robert J.
Mokken, and Maarten de Rijke. 2004.
Using Wordnet to measure semantic
orientations of adjectives. In Proceedings of
the 4th International Conference on Language
Resources and Evaluation, pages 1,115–1,118,
Lisbon.
Kim, Soo-Min and Eduard Hovy. 2004.
Determining the sentiment of opinions.
In Proceedings of the 20th International
Conference on Computational Linguistics,
pages 355–363, Geneva.
Kunegis, Jerome, Stephan Schmidt,
Andreas Lommatzsch, J¨urgen Lerner,
Ernesto W. De, and Luca Sahin Albayrak.
2010. Spectral analysis of signed graphs for
clustering, prediction and visualization. In
Proceedings of the 2010 SIAM International
Conference on Data Mining, pages 559–570,
Columbus, OH.
Li, Shen, Joao V. Graca, and Ben Taskar. 2012.
Wiki-ly supervised part-of-speech tagging.
In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 1,389–1,398,
Jeju Island.
Liang, Percy, Ben Taskar, and Dan Klein.
2006. Alignment by agreement. In
Proceedings of the Main Conference on Human
Language Technology Conference of the North
American Chapter of the Association of
Computational Linguistics, pages 104–111,
New York, NY.
Lu, Bin, Chenhao Tan, Claire Cardie, and
Benjamin K. Tsou. 2011. Joint bilingual
sentiment classification with unlabeled
parallel corpora. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 320–330, Portland, OR.
Meng, Xinfan, Furu Wei, Xiaohua Liu, Ming
Zhou, Ge Xu, and Houfeng Wang.
2012a. Cross-lingual mixture model for
sentiment classification. In Proceedings of
the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers,
pages 572–581, Jeju Island.
Meng, Xinfan, Furu Wei, Ge Xu, Longkai
Zhang, Xiaohua Liu, Ming Zhou,
and Houfeng Wang. 2012b. Lost in
translations? Building sentiment lexicons
using context-based machine translation.
In the 24th International Conference on
Computational Linguistics, pages 829–838,
Bombay.
Mihalcea, Rada, Carmen Banea, and
Janyce Wiebe. 2007. Learning multilingual
subjective language via cross-lingual
projections. In Proceedings of the 45th
Annual Meeting of the Association for
</reference>
<page confidence="0.972649">
39
</page>
<reference confidence="0.994781613445379">
Computational Linguistics Volume 41, Number 1
Computational Linguistics, pages 976–983,
Prague.
Miller, George A. 1995. Wordnet: A lexical
database for English. Communications of
the ACM, 38(11):39–41.
Munteanu, Dragos Stefan and Daniel Marcu.
2005. Improving machine translation
performance by exploiting non-parallel
corpora. Journal of Computational
Linguistics, 31(4):477–504.
Ounis, Iadh, Craig Macdonald, and
Ian Soboroff. 2008. Overview of the
TREC-2010 blog track. In Proceedings of the
NTCIR07 workshop, pages 104–111, Tokyo.
Pang, Bo and Lillian Lee. 2008. Opinion
mining and sentiment analysis, volume 2.
Foundations and Trends in Information
Retrieval. Now Publishers, Inc.
Qiu, Guang, Bing Liu, Jiajun Bu, and Chun
Chen. 2011. Opinion word expansion
and target extraction through double
propagation. Computational Linguistics,
37(1):9–27.
Rao, Delip and Deepak Ravichandran.
2009. Semi-supervised polarity lexicon
induction. In Proceedings of the 12th
Conference of the European Chapter of the
ACL, pages 675–682, Athens.
Riloff, Ellen, Janyce Wiebe, and Theresa
Wilson. 2003. Learning subjective nouns
using extraction pattern bootstrapping.
In Proceedings of the 7th Conference on
Natural Language Learning, pages 25–32,
Edmonton.
Saad, Yousef. 2003. Iterative Methods for Sparse
Linear Systems. Society for Industrial and
Applied Mathematics, Philadelphia,
PA, USA, 2nd edition.
Seki, Yohei, David Kirk Evans, Lun-Wei Ku,
Le Sun, Hsin-Hsi Chen, and Noriko
Kando. 2008. Overview of multilingual
opinion analysis task at NTCIR-7.
Proceedings of the NTCIR07 Workshop,
pages 104–111, Tokyo.
Seki, Yohei, Lun-Wei Ku, Le Sun, Hsin-Hsi
Chen, and Noriko Kando. 2009. Overview
of multilingual opinion analysis task at
NTCIR-8: A step toward cross lingual
opinion analysis. In Proceedings of the
NTCIR08 Workshop, pages 209–220, Tokyo.
Steinberger, Josef, Polina Lenkova,
Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Kabadjov, Ralf
Steinberger, Hristo Tanev, Vanni Zavarella,
and Silvia Vazquez. 2011. Creating
sentiment dictionaries via triangulation.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics, pages 28–36, Beijing.
Stone, Philip J. 1997. Thematic text analysis:
New agendas for analyzing text content.
In Text Analysis for the Social Sciences,
chapter 2. Lawerence Erlbaum,
Mahwah, NJ.
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2005. Extracting
semantic orientations of words using spin
model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 130–140, Ann Arbor, MI.
Talukdar, Partha Pratim and Koby Crammer.
2009. New regularized algorithms for
transductive learning. In Proceedings of the
European Conference on Machine Learning
and Knowledge Discovery in Databases,
pages 442–457, Bled.
Turney, Peter D. and Michael L. Littman.
2003. Measuring praise and criticism:
Inference of semantic orientation from
association. ACM Transactions on
Information Systems, 21(4):315–346.
Wan, Xiaojun. 2009. Co-training for
cross-lingual sentiment classification.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural
Language, pages 235–243, Singapore.
Wang, Hua, Heng Huang, and Chris Ding.
2011. Image annotation using bi-relational
graph of images and semantic labels. In the
24th IEEE Conference on Computer Vision
and Pattern Recognition, pages 793–800,
Colorado Springs, CO.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing, pages 129–136,
Seattle, WA.
Zhou, Dengyong, Olivier Bousquet,
Thomas Navin Lal, Jason Weston, and
Bernhard Scholkopf. 2004. Learning with
local and global consistency. In Proceedings
of Advances in Neural Information Processing
Systems, pages 321–328, Vancouver.
Zhu, Xiaojin and Zoubin Ghahramani. 2002.
Learning from labeled and unlabeled
data with label propagation. Technical
report CMU-CALD-02-107, Carnegie
Mellon University.
Zhu, Xiaojin, Zoubin Ghahramani, and
John Lafferty. 2003. Semi-supervised
learning using Gaussian fields and
harmonic functions. In International
Conference Machine Learning,
pages 912–919, Washington, DC.
</reference>
<page confidence="0.998627">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.038740">
<title confidence="0.902092666666667">Cross-lingual Sentiment Lexicon Learning With Bilingual Word Graph Label Propagation</title>
<affiliation confidence="0.9055164">The Hong Kong Polytechnic University Microsoft Research The Hong Kong Polytechnic University Microsoft Research Microsoft Research</affiliation>
<abstract confidence="0.999356714285714">In this article we address the task of cross-lingual sentiment lexicon learning, which aims to automatically generate sentiment lexicons for the target languages with available English sentiment lexicons. We formalize the task as a learning problem on a bilingual word graph, in which the intra-language relations among the words in the same language and the interlanguage relations among the words between different languages are properly represented. With the words in the English sentiment lexicon as seeds, we propose a bilingual word graph label propagation approach to induce sentiment polarities of the unlabeled words in the target language. Particularly, we show that both synonym and antonym word relations can be used to build the intra-language relation, and that the word alignment information derived from bilingual parallel sentences can be effectively leveraged to build the inter-language relation. The evaluation of Chinese sentiment lexicon learning shows that the proposed approach outperforms existing approaches in both precision and recall. Experiments conducted on the NTCIR data set further demonstrate the effectiveness of the learned sentiment lexicon in sentence-level sentiment classification.</abstract>
<affiliation confidence="0.41247">during internship at Microsoft Research (Beijing).</affiliation>
<address confidence="0.830604">Wei, Xiaohua Liu, and Ming Zhou are from Microsoft Research, Beijing, China.</address>
<email confidence="0.953481">xiaoliu,</email>
<author confidence="0.360886">Gao</author>
<author confidence="0.360886">Wenjie Li are from the Department of Computing</author>
<author confidence="0.360886">the Hong Kong Polytechnic</author>
<email confidence="0.527199">E-mail:polyu®163.com,</email>
<note confidence="0.9844618">Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation,</booktitle>
<pages>2--200</pages>
<contexts>
<context position="11356" citStr="Baccianella et al. (2010)" startWordPosition="1693" endWordPosition="1696">e only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the polarity orientations of the words in WordNet. They select seven positive words and seven negative words and expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classifier to predict the sentiment polarities of all the words in WordNet and use the glosses (textual definitions of the words in WordNet) as the features of classification. The sentiment lexicon generated is the well-known SentiWordNet.2 2.2 Cross-Lingual Sentimen</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the 7th International Conference on Language Resources and Evaluation, pages 2,200–2,204, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shumeet Baluja</author>
<author>Rohan Seth</author>
<author>D Sivakumar</author>
<author>Yushi Jing</author>
<author>Jay Yagnik</author>
<author>Shankar Kumar</author>
<author>Deepak Ravichandran</author>
<author>Mohamed Aly</author>
</authors>
<title>Video suggestion and discovery for Youtube: Taking random walks through the view graph.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on the World Wide Web,</booktitle>
<pages>895--904</pages>
<location>Beijing.</location>
<contexts>
<context position="35645" citStr="Baluja et al. 2008" startWordPosition="5696" endWordPosition="5699"> the word i and ending with the sentiment word k E M, the mean hitting time h(i|M) can be formally defined as � 0, i E M h(i|M) = Ej∈V p(j|i) x h(j|M) + 1, otherwise 32 Gao et al. Cross-lingual Sentiment Lexicon Learning Let M+ and M_ denote the GI positive and negative seeds. If h(ilM+) is greater than h(iJM_), the word xi is classified as negative; otherwise it is classified as positive. The generated positive words and negative words are then ranked according to their polarity scores, respectively. MAD: Talukdar and Crammer (2009) propose a MAD algorithm to modify the adsorption algorithm (Baluja et al. 2008) by adding a new regularization term. In particular, besides the positive and negative labels, a dummy label is assigned to each word in the MAD approach. Two additional columns, representing the scores of the dummy label, are added into Y and F, respectively. We denote these two matrices with the dummy labels as Y&amp;quot; and &amp;quot;F. Meanwhile, R&amp;quot; is used to represent the initial dummy scores of all the words. For a word xi, the newly added columns in &amp;quot;Yi and &amp;quot;Fi are set to zero (i.e., &amp;quot;y(i, 3) = f&amp;quot;(i, 3) = 0). rr&amp;quot;(i, 0) and r&amp;quot;(i, 1) are set to zero, and r&amp;quot;(i, 3) is assigned to one. Then, the predicted </context>
</contexts>
<marker>Baluja, Seth, Sivakumar, Jing, Yagnik, Kumar, Ravichandran, Aly, 2008</marker>
<rawString>Baluja, Shumeet, Rohan Seth, D. Sivakumar, Yushi Jing, Jay Yagnik, Shankar Kumar, Deepak Ravichandran, and Mohamed Aly. 2008. Video suggestion and discovery for Youtube: Taking random walks through the view graph. In Proceedings of the 17th International Conference on the World Wide Web, pages 895–904, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multilingual subjectivity: Are more languages better?</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>28--36</pages>
<location>Beijing.</location>
<marker>Banea, Mihalcea, Wiebe, 2010</marker>
<rawString>Banea, Carmen, Rada Mihalcea, and Janyce Wiebe. 2010. Multilingual subjectivity: Are more languages better? In Proceedings of the 23rd International Conference on Computational Linguistics, pages 28–36, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>45--55</pages>
<location>Boston, MA.</location>
<contexts>
<context position="15489" citStr="Boyd-Graber and Resnik (2010)" startWordPosition="2311" endWordPosition="2314">a. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for a single word. Without sufficient features, it is difficult for these approaches to perform well in learning. Another line of cross-lingual sentiment classification uses Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) or its variants, like Boyd-Graber and Resnik (2010) or He, Alani, and Zhou (2010). These studies assume that each review is a mixture of sentiments and each sentiment is a probability over words. Then they apply the LDA-like approach to model the sentiment polarity of each review. Nonetheless, this assumption may not be applicable in sentiment lexicon learning because a single word can be regarded as the minimal semantic unit, and it is difficult, if not impossible, to infer the latent topics from a single word. Recall that different from the sentiment classification of product reviews where the instances are normally independent, words in sen</context>
</contexts>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>Boyd-Graber, Jordan and Philip Resnik. 2010. Holistic sentiment analysis across languages: Multilingual supervised latent Dirichlet allocation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 45–55, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>600--609</pages>
<location>Portland, OR.</location>
<contexts>
<context position="22460" citStr="Das and Petrov 2011" startWordPosition="3423" endWordPosition="3426">essary because it is relatively easy to collect from the Web. Consequentially, the novel sentiment information inferred from the parallel corpus can easily update the existing sentiment lexicons. These advantages can greatly improve the coverage of the generated sentiment lexicon, as demonstrated later in our experiments. 3.2 Bilingual Word Graph Label Propagation As commonly used semi-supervised approaches, label propagation (Zhu and Ghahramani 2002) and its variants (Zhu, Ghahramani, and Lafferty 2003; Zhou et al. 2004) have been applied to many applications, such as part-of-speech tagging (Das and Petrov 2011; Li, Graca, and Taskar 2012), image annotation (Wang, Huang, and Ding 2011), protein function prediction (Jiang 2011; Jiang and McQuay 2012), and so forth. 4 http://www.statmt.org/moses/giza/GIZA++.html. 5 http://nlp.cs.berkeley.edu. 27 Computational Linguistics Volume 41, Number 1 The underlying idea of label propagation is that the connected nodes in the graph tend to share the same sentiment labels. In bilingual word graph label propagation, the words tend to share same sentiment labels if they are connected by synonym relations or word alignment and tend to belong to different sentiment l</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Das, Dipanjan and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 600–609, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Is machine translation ripe for cross-lingual sentiment classification?</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>429--433</pages>
<location>Portland, OR.</location>
<marker>Duh, Fujino, Nagata, 2011</marker>
<rawString>Duh, Kevin, Akinori Fujino, and Masaaki Nagata. 2011. Is machine translation ripe for cross-lingual sentiment classification? In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 429–433, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>417--422</pages>
<location>Las Palmas.</location>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Esuli, Andrea and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 417–422, Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Random-walk models of term semantics: An application to opinionrelated properties.</title>
<date>2007</date>
<booktitle>In Proceedings of the 3rd Language and Technology Conference,</booktitle>
<pages>221--225</pages>
<location>Poznan.</location>
<marker>Esuli, Sebastiani, 2007</marker>
<rawString>Esuli, Andrea and Fabrizio Sebastiani. 2007. Random-walk models of term semantics: An application to opinionrelated properties. In Proceedings of the 3rd Language and Technology Conference, pages 221–225, Poznan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Amjad Abu-Jbara</author>
<author>Rahul Jha</author>
<author>Dragomir Radev</author>
</authors>
<title>Identifying the semantic orientation of foreign words.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>592--597</pages>
<location>Portland, OR.</location>
<contexts>
<context position="4356" citStr="Hassan et al. 2011" startWordPosition="621" endWordPosition="624">ords to the words in the target language. The few existing approaches first build word relations between English and the target language. Then, based on the word relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language. In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the literature. One is based on translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). The two approaches in Duh, Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language. To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. Specifically, we model this task with a bilingual word graph,</context>
<context position="10819" citStr="Hassan et al. 2011" startWordPosition="1607" endWordPosition="1610">yms of a positive (negative) word are positive (negative) and its antonyms are negative (positive). Initializing with a set of sentiment words, they expand sentiment lexicons based on these two kinds of word relations. Kamps et al. (2004) build a synonym graph according to the synonym relation (synset) derived from WordNet. The sentiment polarity of a word is calculated by the shortest path to two sentiment words good and bad. However, the shortest path cannot precisely describe the sentiment orientation, considering there are only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, t</context>
<context position="12535" citStr="Hassan et al. (2011)" startWordPosition="1872" endWordPosition="1875">SentiWordNet.2 2.2 Cross-Lingual Sentiment Lexicon Learning The work on cross-lingual sentiment lexicon learning is still at an early stage and can be categorized into two types, according to how they bridge the words in two languages. Mihalcea et al. (2007) generate sentiment lexicon for Romanian by directly translating the English sentiment words into Romanian through bilingual English–Romanian dictionaries. When confronting multiword translations, they translate the multiwords word by word. Then the validated translations must occur at least three times on the Web. The approach proposed by Hassan et al. (2011) learns sentiment words based on English WordNet and WordNets in the target languages (e.g., Hindi and Arabic). Crosslingual dictionaries are used to connect the words in two languages and the polarity of a given word is determined by the average hitting time from the word to the English sentiment word set. These approaches connect words in two languages based on crosslingual dictionaries. The main concern of these approaches is the effect of morphological inflection (i.e., a word may be mapped to multiple words in cross-lingual dictionaries). 1 http://wordnet.princeton.edu/. 2 http://sentiwor</context>
<context position="34632" citStr="Hassan et al. (2011)" startWordPosition="5508" endWordPosition="5511">s set of experiments, we compare our approach with the baseline and existing approaches. Rule: For the intra-language relation, this approach assumes that the synonyms of a positive (negative) word are always positive (negative), and the antonyms of a positive (negative) word are always negative (positive). For the inter-language relation, we regard the Chinese word aligned to positive (negative) English words as positive (negative). If a word connects to both positive and negative English words, we regard it as objective. Based on this heuristic, we generate two sets of sentiment words. SOP: Hassan et al. (2011) present a method to predict the semantic orientation of unlabeled words based on the mean hitting time to the two sets of sentiment seed words. Given the graph 9 = (XE U XT, WE U WE U WT U WT U WA), it defines the transition probability from node i to node j as p(j|i) = wi,j Ek wi,k The mean hitting time h(i|j) is the average number of the weighted steps from word i to word j. Starting with the word i and ending with the sentiment word k E M, the mean hitting time h(i|M) can be formally defined as � 0, i E M h(i|M) = Ej∈V p(j|i) x h(j|M) + 1, otherwise 32 Gao et al. Cross-lingual Sentiment Le</context>
</contexts>
<marker>Hassan, Abu-Jbara, Jha, Radev, 2011</marker>
<rawString>Hassan, Ahmed, Amjad Abu-Jbara, Rahul Jha, and Dragomir Radev. 2011. Identifying the semantic orientation of foreign words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 592–597, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<location>Madrid.</location>
<contexts>
<context position="7496" citStr="Hatzivassiloglou and McKeown 1997" startWordPosition="1087" endWordPosition="1090">roach, which effectively leverages the inter-language relations and both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning. 3. We leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the p</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Hatzivassiloglou, Vasileios and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 174–181, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Harith Alani</author>
<author>Deyu Zhou</author>
</authors>
<title>Exploring English lexicon knowledge for Chinese sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 CIPS-SIGHAN Joint Conference on Chinese Language Processing,</booktitle>
<location>Beijing.</location>
<marker>He, Alani, Zhou, 2010</marker>
<rawString>He, Yulan, Harith Alani, and Deyu Zhou. 2010. Exploring English lexicon knowledge for Chinese sentiment analysis. In Proceedings of the 2010 CIPS-SIGHAN Joint Conference on Chinese Language Processing, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="2420" citStr="Hu and Liu 2004" startWordPosition="334" endWordPosition="337"> E-mail: gaodehong polyu®163.com, cswjli®comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Okumura 2005; Rao and Ravichandran 2009). Howev</context>
<context position="9112" citStr="Hu and Liu (2004)" startWordPosition="1343" endWordPosition="1346">attern templates and extract sentiment words by two bootstrapping approaches. Turney and Littman (2003) calculate the pointwise mutual information (PMI) of a given word with positive and negative sets of sentiment words. The sentiment polarity of the word is determined by average PMI values of the positive and negative sets. To obtain PMI, they provide queries (consisting of the given word and the sentiment word) to the search engine. The number of hits and the position (if the given word is near the sentiment word) are used to estimate the association of the given word to the sentiment word. Hu and Liu (2004) research sentiment word learning on customer reviews and they assume that the sentiment words tend to be correlated with product features. The frequent nouns and noun phrases are treated as product features. Then they extract the adjective words 23 Computational Linguistics Volume 41, Number 1 as sentiment words from those sentences that contain one or more product features. This approach may work on a product review corpus, where one product feature may frequently appear. But for other corpora, like news articles, this approach may not be effective. Qiu et al. (2011) combine sentiment lexico</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu, Minqing and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168–177, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Opinion extraction and summarization on the Web.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<pages>1--621</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2831" citStr="Hu and Liu 2006" startWordPosition="392" endWordPosition="395">d Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Okumura 2005; Rao and Ravichandran 2009). However, current work mainly focuses on English sentiment lexicon generation or expansion, while sentiment lexicon learning for other languages has not been well studied. In this article, we address the issue of cross-lingual sentiment lexicon learning, which aims to generate sentiment lexicons for a non-English language (hereafter referred to as “the target language”) with the help of the available English senti</context>
</contexts>
<marker>Hu, Liu, 2006</marker>
<rawString>Hu, Minqing and Bing Liu. 2006. Opinion extraction and summarization on the Web. In Proceedings of the 21st National Conference on Artificial Intelligence, pages 1,621–1,624, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Q Jiang</author>
</authors>
<title>Learning protein functions from bi-relational graph of proteins and function annotations. Algorithms in Bioinformatics, 6833:128–138,</title>
<date>2011</date>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="22577" citStr="Jiang 2011" startWordPosition="3442" endWordPosition="3443">he parallel corpus can easily update the existing sentiment lexicons. These advantages can greatly improve the coverage of the generated sentiment lexicon, as demonstrated later in our experiments. 3.2 Bilingual Word Graph Label Propagation As commonly used semi-supervised approaches, label propagation (Zhu and Ghahramani 2002) and its variants (Zhu, Ghahramani, and Lafferty 2003; Zhou et al. 2004) have been applied to many applications, such as part-of-speech tagging (Das and Petrov 2011; Li, Graca, and Taskar 2012), image annotation (Wang, Huang, and Ding 2011), protein function prediction (Jiang 2011; Jiang and McQuay 2012), and so forth. 4 http://www.statmt.org/moses/giza/GIZA++.html. 5 http://nlp.cs.berkeley.edu. 27 Computational Linguistics Volume 41, Number 1 The underlying idea of label propagation is that the connected nodes in the graph tend to share the same sentiment labels. In bilingual word graph label propagation, the words tend to share same sentiment labels if they are connected by synonym relations or word alignment and tend to belong to different sentiment labels if connected by antonym relations. In this article we propose bilingual word graph label propagation for cross-</context>
</contexts>
<marker>Jiang, 2011</marker>
<rawString>Jiang, Jonathan Q. 2011. Learning protein functions from bi-relational graph of proteins and function annotations. Algorithms in Bioinformatics, 6833:128–138, Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Q Jiang</author>
<author>Lisa J McQuay</author>
</authors>
<title>Predicting protein function by multi-label correlated semi-supervised learning.</title>
<date>2012</date>
<journal>IEEE/ACM Transactions on Computational Biology and Bioinformatics,</journal>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="22601" citStr="Jiang and McQuay 2012" startWordPosition="3444" endWordPosition="3447">corpus can easily update the existing sentiment lexicons. These advantages can greatly improve the coverage of the generated sentiment lexicon, as demonstrated later in our experiments. 3.2 Bilingual Word Graph Label Propagation As commonly used semi-supervised approaches, label propagation (Zhu and Ghahramani 2002) and its variants (Zhu, Ghahramani, and Lafferty 2003; Zhou et al. 2004) have been applied to many applications, such as part-of-speech tagging (Das and Petrov 2011; Li, Graca, and Taskar 2012), image annotation (Wang, Huang, and Ding 2011), protein function prediction (Jiang 2011; Jiang and McQuay 2012), and so forth. 4 http://www.statmt.org/moses/giza/GIZA++.html. 5 http://nlp.cs.berkeley.edu. 27 Computational Linguistics Volume 41, Number 1 The underlying idea of label propagation is that the connected nodes in the graph tend to share the same sentiment labels. In bilingual word graph label propagation, the words tend to share same sentiment labels if they are connected by synonym relations or word alignment and tend to belong to different sentiment labels if connected by antonym relations. In this article we propose bilingual word graph label propagation for cross-lingual sentiment lexico</context>
</contexts>
<marker>Jiang, McQuay, 2012</marker>
<rawString>Jiang, Jonathan Q. and Lisa J. McQuay. 2012. Predicting protein function by multi-label correlated semi-supervised learning. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 9(4):1059–1069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>Robert J Mokken</author>
<author>Maarten de Rijke</author>
</authors>
<title>Using Wordnet to measure semantic orientations of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1--115</pages>
<location>Lisbon.</location>
<marker>Kamps, Marx, Mokken, de Rijke, 2004</marker>
<rawString>Kamps, Jaap, Maarten Marx, Robert J. Mokken, and Maarten de Rijke. 2004. Using Wordnet to measure semantic orientations of adjectives. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pages 1,115–1,118, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>355--363</pages>
<location>Geneva.</location>
<contexts>
<context position="2376" citStr="Kim and Hovy 2004" startWordPosition="327" endWordPosition="330">mputing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu®163.com, cswjli®comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Oku</context>
<context position="7663" citStr="Kim and Hovy 2004" startWordPosition="1113" endWordPosition="1116"> word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. For example, the conjunction word and tends to link two words with the same polarity, whereas the conjunction word but </context>
<context position="10178" citStr="Kim and Hovy (2004)" startWordPosition="1504" endWordPosition="1507"> feature may frequently appear. But for other corpora, like news articles, this approach may not be effective. Qiu et al. (2011) combine sentiment lexicon learning and opinion target extraction. A double propagation approach is proposed to learn sentiment words and to extract opinion targets simultaneously, based on eight manually defined rules. The semantic-based approaches determine the sentiment polarity of a given word according to the word semantic relation, like the synonyms of sentiment seed words. The word semantic relation is usually obtained from dictionaries, for example, WordNet.1 Kim and Hovy (2004) assume that the synonyms of a positive (negative) word are positive (negative) and its antonyms are negative (positive). Initializing with a set of sentiment words, they expand sentiment lexicons based on these two kinds of word relations. Kamps et al. (2004) build a synonym graph according to the synonym relation (synset) derived from WordNet. The sentiment polarity of a word is calculated by the shortest path to two sentiment words good and bad. However, the shortest path cannot precisely describe the sentiment orientation, considering there are only five steps between the word good and the</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Kim, Soo-Min and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th International Conference on Computational Linguistics, pages 355–363, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Kunegis</author>
<author>Stephan Schmidt</author>
<author>Andreas Lommatzsch</author>
<author>J¨urgen Lerner</author>
<author>Ernesto W De</author>
<author>Luca Sahin Albayrak</author>
</authors>
<title>Spectral analysis of signed graphs for clustering, prediction and visualization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 SIAM International Conference on Data Mining,</booktitle>
<pages>559--570</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="26133" citStr="Kunegis et al. 2010" startWordPosition="4098" endWordPosition="4101">erentiate the objective function according to FE and FT, and we have ∂E(F) ∂FE |FE=F⋆E= ρ1SEFE + 12SAFT − ρ39EFE + µFE − µYE = 0 (6) ∂E(F) (7) ∂FT |FT=F⋆T = ρ2STFT + 1 2SA′FE − ρ4�STFT + µFT − µYT = 0 where P′ is the transpose of the matrix P. The graph Laplacians SE and ST of the synonym intra-language relations are (I − D−1 E WED− 1 2 E ) and (I − D− 1 2 T WTD− 1 2 T ), where 2 I is the identity matrix. The graph Laplacians SE and ST of the antonym intra-language relations are (I − �D− 12 E �WE �D− 1 E ) and (I − �D− 12 2 T �WT �D− 12 T ), which has been proven to be positive semi-definite (Kunegis et al. 2010). The graph Laplacian SA of the interlanguage relation is (I − D−1 AL WAD−1 2 AR ). From Equations (6) and (7), we can obtain the 2 optimal solutions (ME − SAM−1 T S′ A)FE = 2µ(YE − SAM−1 T YT) (8) (MT − S′ AM−1 E SA)FT = 2µ(YT − S′ AM−1 E YE) (9) where ME = 2ρ1SE − 2ρ3�SE + 2µI and MT = 2ρ2ST − 2ρ4�ST + 2µI. To avoid computing the inverse matrix in Equations (8) and (9), we apply the Jacobi algorithm (Saad 2003) to calculate the solutions as described in Algorithm 1. In line 1, we set the label of the positive seed xi as yLE+i = (1, 0) and the label of the negative seed xj as yLE−j = (0, 1). </context>
</contexts>
<marker>Kunegis, Schmidt, Lommatzsch, Lerner, De, Albayrak, 2010</marker>
<rawString>Kunegis, Jerome, Stephan Schmidt, Andreas Lommatzsch, J¨urgen Lerner, Ernesto W. De, and Luca Sahin Albayrak. 2010. Spectral analysis of signed graphs for clustering, prediction and visualization. In Proceedings of the 2010 SIAM International Conference on Data Mining, pages 559–570, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Joao V Graca</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1--389</pages>
<location>Jeju Island.</location>
<marker>Li, Graca, Taskar, 2012</marker>
<rawString>Li, Shen, Joao V. Graca, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1,389–1,398, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>New York, NY.</location>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Liang, Percy, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 104–111, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Lu</author>
<author>Chenhao Tan</author>
<author>Claire Cardie</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Joint bilingual sentiment classification with unlabeled parallel corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>320--330</pages>
<location>Portland, OR.</location>
<contexts>
<context position="14624" citStr="Lu et al. (2011)" startWordPosition="2186" endWordPosition="2189">gle Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for a single word. Wit</context>
</contexts>
<marker>Lu, Tan, Cardie, Tsou, 2011</marker>
<rawString>Lu, Bin, Chenhao Tan, Claire Cardie, and Benjamin K. Tsou. 2011. Joint bilingual sentiment classification with unlabeled parallel corpora. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 320–330, Portland, OR.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xinfan Meng</author>
<author>Furu Wei</author>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Ge Xu</author>
<author>Houfeng Wang</author>
</authors>
<marker>Meng, Wei, Liu, Zhou, Xu, Wang, </marker>
<rawString>Meng, Xinfan, Furu Wei, Xiaohua Liu, Ming Zhou, Ge Xu, and Houfeng Wang.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2012a</author>
</authors>
<title>Cross-lingual mixture model for sentiment classification.</title>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers,</booktitle>
<pages>572--581</pages>
<location>Jeju Island.</location>
<marker>2012a, </marker>
<rawString>2012a. Cross-lingual mixture model for sentiment classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers, pages 572–581, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinfan Meng</author>
<author>Furu Wei</author>
<author>Ge Xu</author>
<author>Longkai Zhang</author>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Houfeng Wang</author>
</authors>
<title>Lost in translations? Building sentiment lexicons using context-based machine translation.</title>
<date>2012</date>
<booktitle>In the 24th International Conference on Computational Linguistics,</booktitle>
<pages>829--838</pages>
<location>Bombay.</location>
<contexts>
<context position="14437" citStr="Meng et al. 2012" startWordPosition="2159" endWordPosition="2162">They manually produce two high-level gold-standard sentiment lexicons for two languages (e.g., English and Spanish) and then translate them into the third language (e.g., Italian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task </context>
<context position="40139" citStr="Meng et al. (2012" startWordPosition="6465" endWordPosition="6468">lated to the Chinese word xj in UD dictionary, wA(i, j) and wA(j, i) are set to 1. BLP-MT: All the Chinese (English) words are translated into English (Chinese) by Google Translator. If the Chinese word xi can be translated to the English word xj, the wA(i, j) and wA(j, i) are set to 1. If a Chinese word is translated to an English phrase, we assume that the Chinese word is projected to each word in the English phrase. To improve the coverage, we translate the English sentiment seed words with three other methods; they are word collocation, coordinated phrase, and punctuation, as mentioned in Meng et al. (2012b). The learned Chinese sentiment word lists are also evaluated with precision at k. As shown in Figure 5, we find that the alignment-based approach outperforms the dictionary-based and MT-based approaches. The reason that contributes to this is that we can build more inter-language relations based on word alignment, compared 7 http://projects.ldc.upenn.edu/Chinese/LDC ch.htm. 8 http://www.dicts.info/uddl.php. Computational Linguistics Volume 41, Number 1 BLP Rule SOP MAD P@100P@200 P@1K P@10K positive precision at k BLP Rule SOP MAD P@100P@200 P@1K P@10K 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 34 Gao</context>
</contexts>
<marker>Meng, Wei, Xu, Zhang, Liu, Zhou, Wang, 2012</marker>
<rawString>Meng, Xinfan, Furu Wei, Ge Xu, Longkai Zhang, Xiaohua Liu, Ming Zhou, and Houfeng Wang. 2012b. Lost in translations? Building sentiment lexicons using context-based machine translation. In the 24th International Conference on Computational Linguistics, pages 829–838, Bombay.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning multilingual subjective language via cross-lingual projections.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Volume 41, Number</booktitle>
<volume>1</volume>
<contexts>
<context position="12173" citStr="Mihalcea et al. (2007)" startWordPosition="1821" endWordPosition="1824"> expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classifier to predict the sentiment polarities of all the words in WordNet and use the glosses (textual definitions of the words in WordNet) as the features of classification. The sentiment lexicon generated is the well-known SentiWordNet.2 2.2 Cross-Lingual Sentiment Lexicon Learning The work on cross-lingual sentiment lexicon learning is still at an early stage and can be categorized into two types, according to how they bridge the words in two languages. Mihalcea et al. (2007) generate sentiment lexicon for Romanian by directly translating the English sentiment words into Romanian through bilingual English–Romanian dictionaries. When confronting multiword translations, they translate the multiwords word by word. Then the validated translations must occur at least three times on the Web. The approach proposed by Hassan et al. (2011) learns sentiment words based on English WordNet and WordNets in the target languages (e.g., Hindi and Arabic). Crosslingual dictionaries are used to connect the words in two languages and the polarity of a given word is determined by the</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>Mihalcea, Rada, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Volume 41, Number 1</rawString>
</citation>
<citation valid="false">
<authors>
<author>Computational Linguistics</author>
</authors>
<pages>976--983</pages>
<location>Prague.</location>
<marker>Linguistics, </marker>
<rawString>Computational Linguistics, pages 976–983, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="29765" citStr="Miller 1995" startWordPosition="4753" endWordPosition="4754">(Liang, Taskar, and Klein 2006). As an unsupervised method, it does not require us to manually collect training data and does not need the complex training processing, and its performance is competitive with supervised methods. With these two advantages, we can focus more on our task of cross-lingual sentiment lexicon learning. Based on the word alignment derived by BerkeleyAligner, the inter-language WA is initialized with the normalized alignment frequency. The English and Chinese v_ersions6 of WordNet are used to build the intra-language relations WE, WE, WT, and WT, respectively. WordNet (Miller 1995) groups words into synonym sets, called synsets. We collect about 117,000 synsets from the English WordNet and about 80,000 synsets from the Chinese WordNet. In total, we obtain 8,406 and 6,312 antonym synset pairs. We first generate both positive and negative scores for each unlabeled word and then determine the word sentiment polarities based on its scores. We rank the two sets of newly labeled sentiment words according to their polarity scores. The top-ranked Chinese words are shown in Table 1. We manually label the top-ranked 1K sentiment words. For P@10K, we sequentially divide the top 10</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>Miller, George A. 1995. Wordnet: A lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Journal of Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="28465" citStr="Munteanu and Marcu 2005" startWordPosition="4550" endWordPosition="4553">garded as negative. 4. Experiment 4.1 Data Sets We conduct experiments on Chinese sentiment lexicon learning. As in previous work (Baccianella, Esuli, and Sebastiani 2010), the sentiment words in General Inquirer lexicon are selected as the English seeds (Stone 1997). From the GI lexicon we collect 2,005 positive words and 1,635 negative words. To build the bilingual word graph, we adopt the Chinese–English parallel corpus, which is obtained from the news articles published by Xinhua News Agency in Chinese and English collections, using the automatic parallel sentence identification approach (Munteanu and Marcu 2005). Altogether, we collect more than 25M parallel sentence pairs in English and Chinese. We remove all the stopwords in Chinese and English (e.g., (of) and am) together with the low-frequency words that occur fewer than 5 times. After preprocessing, we finally have more than 174,000 English words, among which 3,519 words have sentiment labels and more than 146,000 Chinese words for which we need to predict the sentiment labels. To transfer sentiment information to Chinese unlabeled words more efficiently, we remove the unlabeled English words in the word graph (i.e., XUE = Φ). The unsupervised m</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Munteanu, Dragos Stefan and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Journal of Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iadh Ounis</author>
<author>Craig Macdonald</author>
<author>Ian Soboroff</author>
</authors>
<title>Overview of the TREC-2010 blog track.</title>
<date>2008</date>
<booktitle>In Proceedings of the NTCIR07 workshop,</booktitle>
<pages>104--111</pages>
<location>Tokyo.</location>
<marker>Ounis, Macdonald, Soboroff, 2008</marker>
<rawString>Ounis, Iadh, Craig Macdonald, and Ian Soboroff. 2008. Overview of the TREC-2010 blog track. In Proceedings of the NTCIR07 workshop, pages 104–111, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis, volume 2. Foundations and Trends in Information Retrieval.</title>
<date>2008</date>
<publisher>Now Publishers, Inc.</publisher>
<contexts>
<context position="2226" citStr="Pang and Lee 2008" startWordPosition="306" endWordPosition="309">re from Microsoft Research, Beijing, China. E-mail: {fuwei, xiaoliu, mingzhou}®microsoft.com. † Dehong Gao and Wenjie Li are from the Department of Computing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu®163.com, cswjli®comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu </context>
<context position="15012" citStr="Pang and Lee 2008" startWordPosition="2243" endWordPosition="2246">, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for a single word. Without sufficient features, it is difficult for these approaches to perform well in learning. Another line of cross-lingual sentiment classification uses Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) or its variants, like Boyd-Graber and Resnik (2010) or He, Alani, and Zhou (2010). These studies assume that each review is a mixture of sentiments and each sentiment is a pr</context>
<context position="46149" citStr="Pang and Lee 2008" startWordPosition="7415" endWordPosition="7418">anguage propagation, while p3 and p4 in Equation (4) adjust the English and Chinese antonym intralanguage propagation. For simplicity, let p1 equal p2 and let p3 equal p4. Then we tune p1,2 and p3,4 together. When p1,2 and p3,4 range from {1e − 2, 1e − 1, 1, 10, 100, 1000}, Precision@1K ranges from 0.631 to 0.689 and Recall ranges from 0.651 to 0.729 on average. In general, we find that when 1 &lt; p3,4 &lt; p1,2 &lt; 10, we can obtain better results. 4.7 Evaluation on Sentiment Classification Sentiment classification is one of the most extensively studied tasks in the community of sentiment analysis (Pang and Lee 2008). To see whether the performance improvement in lexicon learning also improves the results of sentiment classification, we apply the generated Chinese sentiment lexicons to sentence-level sentiment classification. Data set: The NTCIR sentiment-labeled corpus is used for sentiment classification (Seki et al. 2008, 2009). We extract the Chinese sentences that have positive, negative, or neutral labels. The numbers of extracted sentences are shown in Table 5. The learned sentiment words in the Mono and BLP approaches are used as classification features. We implement the following baselines for co</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Pang, Bo and Lillian Lee. 2008. Opinion mining and sentiment analysis, volume 2. Foundations and Trends in Information Retrieval. Now Publishers, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Opinion word expansion and target extraction through double propagation.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="7546" citStr="Qiu et al. 2011" startWordPosition="1096" endWordPosition="1099"> both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning. 3. We leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. Fo</context>
<context position="9687" citStr="Qiu et al. (2011)" startWordPosition="1434" endWordPosition="1437">d to the sentiment word. Hu and Liu (2004) research sentiment word learning on customer reviews and they assume that the sentiment words tend to be correlated with product features. The frequent nouns and noun phrases are treated as product features. Then they extract the adjective words 23 Computational Linguistics Volume 41, Number 1 as sentiment words from those sentences that contain one or more product features. This approach may work on a product review corpus, where one product feature may frequently appear. But for other corpora, like news articles, this approach may not be effective. Qiu et al. (2011) combine sentiment lexicon learning and opinion target extraction. A double propagation approach is proposed to learn sentiment words and to extract opinion targets simultaneously, based on eight manually defined rules. The semantic-based approaches determine the sentiment polarity of a given word according to the word semantic relation, like the synonyms of sentiment seed words. The word semantic relation is usually obtained from dictionaries, for example, WordNet.1 Kim and Hovy (2004) assume that the synonyms of a positive (negative) word are positive (negative) and its antonyms are negative</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2011</marker>
<rawString>Qiu, Guang, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through double propagation. Computational Linguistics, 37(1):9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semi-supervised polarity lexicon induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL,</booktitle>
<pages>675--682</pages>
<location>Athens.</location>
<contexts>
<context position="3013" citStr="Rao and Ravichandran 2009" startWordPosition="417" endWordPosition="420">n summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Okumura 2005; Rao and Ravichandran 2009). However, current work mainly focuses on English sentiment lexicon generation or expansion, while sentiment lexicon learning for other languages has not been well studied. In this article, we address the issue of cross-lingual sentiment lexicon learning, which aims to generate sentiment lexicons for a non-English language (hereafter referred to as “the target language”) with the help of the available English sentiment lexicons. The underlying motivation of this task is to leverage the existing English sentiment lexicons and substantial linguistic resources to label the sentiment polarities of</context>
<context position="11096" citStr="Rao and Ravichandran (2009)" startWordPosition="1655" endWordPosition="1658">g to the synonym relation (synset) derived from WordNet. The sentiment polarity of a word is calculated by the shortest path to two sentiment words good and bad. However, the shortest path cannot precisely describe the sentiment orientation, considering there are only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the polarity orientations of the words in WordNet. They select seven positive words and seven negative words and expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classif</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Rao, Delip and Deepak Ravichandran. 2009. Semi-supervised polarity lexicon induction. In Proceedings of the 12th Conference of the European Chapter of the ACL, pages 675–682, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Natural Language Learning,</booktitle>
<pages>25--32</pages>
<location>Edmonton.</location>
<contexts>
<context position="8478" citStr="Riloff et al. (2003)" startWordPosition="1238" endWordPosition="1241">ds or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. For example, the conjunction word and tends to link two words with the same polarity, whereas the conjunction word but is likely to link two words with opposite polarities. Their approach only considers adjectives, not nouns or verbs, and it is unable to extract adjectives that are not conjoined by conjunctions. Riloff et al. (2003) define several pattern templates and extract sentiment words by two bootstrapping approaches. Turney and Littman (2003) calculate the pointwise mutual information (PMI) of a given word with positive and negative sets of sentiment words. The sentiment polarity of the word is determined by average PMI values of the positive and negative sets. To obtain PMI, they provide queries (consisting of the given word and the sentiment word) to the search engine. The number of hits and the position (if the given word is near the sentiment word) are used to estimate the association of the given word to the</context>
</contexts>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Riloff, Ellen, Janyce Wiebe, and Theresa Wilson. 2003. Learning subjective nouns using extraction pattern bootstrapping. In Proceedings of the 7th Conference on Natural Language Learning, pages 25–32, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yousef Saad</author>
</authors>
<title>Iterative Methods for Sparse Linear Systems. Society for Industrial and Applied Mathematics,</title>
<date>2003</date>
<location>Philadelphia, PA, USA,</location>
<note>2nd edition.</note>
<contexts>
<context position="26549" citStr="Saad 2003" startWordPosition="4191" endWordPosition="4192">ans SE and ST of the antonym intra-language relations are (I − �D− 12 E �WE �D− 1 E ) and (I − �D− 12 2 T �WT �D− 12 T ), which has been proven to be positive semi-definite (Kunegis et al. 2010). The graph Laplacian SA of the interlanguage relation is (I − D−1 AL WAD−1 2 AR ). From Equations (6) and (7), we can obtain the 2 optimal solutions (ME − SAM−1 T S′ A)FE = 2µ(YE − SAM−1 T YT) (8) (MT − S′ AM−1 E SA)FT = 2µ(YT − S′ AM−1 E YE) (9) where ME = 2ρ1SE − 2ρ3�SE + 2µI and MT = 2ρ2ST − 2ρ4�ST + 2µI. To avoid computing the inverse matrix in Equations (8) and (9), we apply the Jacobi algorithm (Saad 2003) to calculate the solutions as described in Algorithm 1. In line 1, we set the label of the positive seed xi as yLE+i = (1, 0) and the label of the negative seed xj as yLE−j = (0, 1). We set the label YUE of the unlabeled words as zero, and then generate YE with YLE and YUE . Line 2 sets YT as zero matrix. In line 3, we compute the matrixes SE, SE, ST, ST, SA, and then compute the matrixes ME and MT. The sentiment information is simultaneously Algorithm 1. Bilingual word graph label propagation Input: Given G = (XE ∪ XT, WE ∪ WE ∪ WT ∪ WT ∪ WA),XE, label YLE for XLE, initialize µ and ρ1∼4 Outp</context>
</contexts>
<marker>Saad, 2003</marker>
<rawString>Saad, Yousef. 2003. Iterative Methods for Sparse Linear Systems. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen Le Sun</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of multilingual opinion analysis task at</title>
<date>2008</date>
<booktitle>NTCIR-7. Proceedings of the NTCIR07 Workshop,</booktitle>
<pages>104--111</pages>
<location>Tokyo.</location>
<marker>Seki, Evans, Ku, Le Sun, Kando, 2008</marker>
<rawString>Seki, Yohei, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and Noriko Kando. 2008. Overview of multilingual opinion analysis task at NTCIR-7. Proceedings of the NTCIR07 Workshop, pages 104–111, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen Le Sun</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of multilingual opinion analysis task at NTCIR-8: A step toward cross lingual opinion analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the NTCIR08 Workshop,</booktitle>
<pages>209--220</pages>
<location>Tokyo.</location>
<marker>Seki, Ku, Le Sun, Kando, 2009</marker>
<rawString>Seki, Yohei, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and Noriko Kando. 2009. Overview of multilingual opinion analysis task at NTCIR-8: A step toward cross lingual opinion analysis. In Proceedings of the NTCIR08 Workshop, pages 209–220, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Steinberger</author>
<author>Polina Lenkova</author>
<author>Mohamed Ebrahim</author>
<author>Maud Ehrmann</author>
<author>Ali Hurriyetoglu</author>
<author>Mijail Kabadjov</author>
<author>Ralf Steinberger</author>
<author>Hristo Tanev</author>
<author>Vanni Zavarella</author>
<author>Silvia Vazquez</author>
</authors>
<title>Creating sentiment dictionaries via triangulation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>28--36</pages>
<location>Beijing.</location>
<contexts>
<context position="4519" citStr="Steinberger et al. 2011" startWordPosition="649" endWordPosition="652">ord relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language. In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the literature. One is based on translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). The two approaches in Duh, Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language. To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. Specifically, we model this task with a bilingual word graph, which is composed of two intra-language subgraphs and an interlanguage subgraph. The intra-language subgraphs are used to model the semantic relations among the w</context>
<context position="13432" citStr="Steinberger et al. 2011" startWordPosition="2009" endWordPosition="2012"> to the English sentiment word set. These approaches connect words in two languages based on crosslingual dictionaries. The main concern of these approaches is the effect of morphological inflection (i.e., a word may be mapped to multiple words in cross-lingual dictionaries). 1 http://wordnet.princeton.edu/. 2 http://sentiwordnet.isti.cnr.it/. 24 Gao et al. Cross-lingual Sentiment Lexicon Learning For example, one single English word typically has four Spanish or Italian word forms (two each for gender and for number) and many Russian word forms (due to gender, number, and case distinctions) (Steinberger et al. 2011). Usually, this approach requires an additional process to disambiguate the sentiment polarities of all the morphological variants. To improve the sentiment classification for the target language, Banea, Mihalcea, and Wiebe (2010) translate the English sentiment lexicon into the target language using Google Translator.3 Similarly, Google Translator is used by Steinberger et al. (2011). They manually produce two high-level gold-standard sentiment lexicons for two languages (e.g., English and Spanish) and then translate them into the third language (e.g., Italian) via Google Translator. They bel</context>
</contexts>
<marker>Steinberger, Lenkova, Ebrahim, Ehrmann, Hurriyetoglu, Kabadjov, Steinberger, Tanev, Zavarella, Vazquez, 2011</marker>
<rawString>Steinberger, Josef, Polina Lenkova, Mohamed Ebrahim, Maud Ehrmann, Ali Hurriyetoglu, Mijail Kabadjov, Ralf Steinberger, Hristo Tanev, Vanni Zavarella, and Silvia Vazquez. 2011. Creating sentiment dictionaries via triangulation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 28–36, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
</authors>
<title>Thematic text analysis: New agendas for analyzing text content. In Text Analysis for the Social Sciences, chapter 2. Lawerence Erlbaum,</title>
<date>1997</date>
<location>Mahwah, NJ.</location>
<contexts>
<context position="28108" citStr="Stone 1997" startWordPosition="4499" endWordPosition="4500">vergence 29 Computational Linguistics Volume 41, Number 1 propagated through lines 4–7 until the predicted labels FE and FT are converged. For an unlabeled word xi, if |f (i, 0) − f (i, 1) |&lt; &amp; (&amp; is set as 1.0E−4), xi is regarded as neutral; if ( f (i, 0) − f (i, 1)) &gt; &amp;, xi is regarded as positive; and if (f (i, 1) − f (i, 0)) &gt; &amp;, xi is regarded as negative. 4. Experiment 4.1 Data Sets We conduct experiments on Chinese sentiment lexicon learning. As in previous work (Baccianella, Esuli, and Sebastiani 2010), the sentiment words in General Inquirer lexicon are selected as the English seeds (Stone 1997). From the GI lexicon we collect 2,005 positive words and 1,635 negative words. To build the bilingual word graph, we adopt the Chinese–English parallel corpus, which is obtained from the news articles published by Xinhua News Agency in Chinese and English collections, using the automatic parallel sentence identification approach (Munteanu and Marcu 2005). Altogether, we collect more than 25M parallel sentence pairs in English and Chinese. We remove all the stopwords in Chinese and English (e.g., (of) and am) together with the low-frequency words that occur fewer than 5 times. After preprocess</context>
</contexts>
<marker>Stone, 1997</marker>
<rawString>Stone, Philip J. 1997. Thematic text analysis: New agendas for analyzing text content. In Text Analysis for the Social Sciences, chapter 2. Lawerence Erlbaum, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>130--140</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="10843" citStr="Takamura et al. (2005)" startWordPosition="1611" endWordPosition="1614">gative) word are positive (negative) and its antonyms are negative (positive). Initializing with a set of sentiment words, they expand sentiment lexicons based on these two kinds of word relations. Kamps et al. (2004) build a synonym graph according to the synonym relation (synset) derived from WordNet. The sentiment polarity of a word is calculated by the shortest path to two sentiment words good and bad. However, the shortest path cannot precisely describe the sentiment orientation, considering there are only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the </context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Takamura, Hiroya, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 130–140, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Koby Crammer</author>
</authors>
<title>New regularized algorithms for transductive learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>442--457</pages>
<location>Bled.</location>
<contexts>
<context position="35565" citStr="Talukdar and Crammer (2009)" startWordPosition="5683" endWordPosition="5686"> h(i|j) is the average number of the weighted steps from word i to word j. Starting with the word i and ending with the sentiment word k E M, the mean hitting time h(i|M) can be formally defined as � 0, i E M h(i|M) = Ej∈V p(j|i) x h(j|M) + 1, otherwise 32 Gao et al. Cross-lingual Sentiment Lexicon Learning Let M+ and M_ denote the GI positive and negative seeds. If h(ilM+) is greater than h(iJM_), the word xi is classified as negative; otherwise it is classified as positive. The generated positive words and negative words are then ranked according to their polarity scores, respectively. MAD: Talukdar and Crammer (2009) propose a MAD algorithm to modify the adsorption algorithm (Baluja et al. 2008) by adding a new regularization term. In particular, besides the positive and negative labels, a dummy label is assigned to each word in the MAD approach. Two additional columns, representing the scores of the dummy label, are added into Y and F, respectively. We denote these two matrices with the dummy labels as Y&amp;quot; and &amp;quot;F. Meanwhile, R&amp;quot; is used to represent the initial dummy scores of all the words. For a word xi, the newly added columns in &amp;quot;Yi and &amp;quot;Fi are set to zero (i.e., &amp;quot;y(i, 3) = f&amp;quot;(i, 3) = 0). rr&amp;quot;(i, 0) and</context>
</contexts>
<marker>Talukdar, Crammer, 2009</marker>
<rawString>Talukdar, Partha Pratim and Koby Crammer. 2009. New regularized algorithms for transductive learning. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, pages 442–457, Bled.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="8598" citStr="Turney and Littman (2003)" startWordPosition="1254" endWordPosition="1257">e of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. For example, the conjunction word and tends to link two words with the same polarity, whereas the conjunction word but is likely to link two words with opposite polarities. Their approach only considers adjectives, not nouns or verbs, and it is unable to extract adjectives that are not conjoined by conjunctions. Riloff et al. (2003) define several pattern templates and extract sentiment words by two bootstrapping approaches. Turney and Littman (2003) calculate the pointwise mutual information (PMI) of a given word with positive and negative sets of sentiment words. The sentiment polarity of the word is determined by average PMI values of the positive and negative sets. To obtain PMI, they provide queries (consisting of the given word and the sentiment word) to the search engine. The number of hits and the position (if the given word is near the sentiment word) are used to estimate the association of the given word to the sentiment word. Hu and Liu (2004) research sentiment word learning on customer reviews and they assume that the sentime</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Turney, Peter D. and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language,</booktitle>
<pages>235--243</pages>
<contexts>
<context position="14606" citStr="Wan (2009)" startWordPosition="2184" endWordPosition="2185">ian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for </context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Wan, Xiaojun. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language, pages 235–243, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wang</author>
<author>Heng Huang</author>
<author>Chris Ding</author>
</authors>
<title>Image annotation using bi-relational graph of images and semantic labels.</title>
<date>2011</date>
<booktitle>In the 24th IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>793--800</pages>
<location>Colorado Springs, CO.</location>
<marker>Wang, Huang, Ding, 2011</marker>
<rawString>Wang, Hua, Heng Huang, and Chris Ding. 2011. Image annotation using bi-relational graph of images and semantic labels. In the 24th IEEE Conference on Computer Vision and Pattern Recognition, pages 793–800, Colorado Springs, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--136</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="2356" citStr="Yu and Hatzivassiloglou 2003" startWordPosition="323" endWordPosition="326"> are from the Department of Computing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu®163.com, cswjli®comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Tak</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 129–136, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>Olivier Bousquet</author>
<author>Thomas Navin Lal</author>
<author>Jason Weston</author>
<author>Bernhard Scholkopf</author>
</authors>
<title>Learning with local and global consistency.</title>
<date>2004</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems,</booktitle>
<pages>321--328</pages>
<location>Vancouver.</location>
<contexts>
<context position="22368" citStr="Zhou et al. 2004" startWordPosition="3409" endWordPosition="3412"> in sentiment expressions. Second, the parallel corpus can be dynamically changed when necessary because it is relatively easy to collect from the Web. Consequentially, the novel sentiment information inferred from the parallel corpus can easily update the existing sentiment lexicons. These advantages can greatly improve the coverage of the generated sentiment lexicon, as demonstrated later in our experiments. 3.2 Bilingual Word Graph Label Propagation As commonly used semi-supervised approaches, label propagation (Zhu and Ghahramani 2002) and its variants (Zhu, Ghahramani, and Lafferty 2003; Zhou et al. 2004) have been applied to many applications, such as part-of-speech tagging (Das and Petrov 2011; Li, Graca, and Taskar 2012), image annotation (Wang, Huang, and Ding 2011), protein function prediction (Jiang 2011; Jiang and McQuay 2012), and so forth. 4 http://www.statmt.org/moses/giza/GIZA++.html. 5 http://nlp.cs.berkeley.edu. 27 Computational Linguistics Volume 41, Number 1 The underlying idea of label propagation is that the connected nodes in the graph tend to share the same sentiment labels. In bilingual word graph label propagation, the words tend to share same sentiment labels if they are </context>
<context position="23667" citStr="Zhou et al. (2004)" startWordPosition="3621" endWordPosition="3624">sentiment labels if connected by antonym relations. In this article we propose bilingual word graph label propagation for cross-lingual sentiment lexicon learning. Let F = {FE, FT} denote the predicted labels of the unlabeled words X. The loss function can be defined as El(F) = µ Xn kfEi − yEik2 + µ Xm kfTi − yTik2 (1) i=1 i=1 where n and m denote the numbers of English words and words in the target language. Let Y = {YE, YT} denote the initial sentiment labels of all the words; the loss function means that the prediction could not change too much from the initial label assignment. Similar to Zhou et al. (2004), we define the smoothness function to indicate that if two words are connected by synonym relation or by word alignment, then they tend to share the same sentiment label. The smoothness function can be further represented by two parts, that is, the inter-language smoothness Einter s (F) and the synonym intra-language smoothness Eintra s (F) Einter Xn m wAijk fEi fTj k2 s(F) = 12 i=1 X pdALii − q (2) dARjj j=1 Eintra Xn Ei −fEj k2 + J Tf2 s(F) = P112 i,j=1 wEij k P2 X wTij II i − k (3) pdEii qdEjj 2 ij=1 v/dTii qdTjj DAL and DAR are defined as DAL= diag(Pj wA(1, j), ..., Pj wA(n, j))′ and DAR=</context>
<context position="32792" citStr="Zhou et al. (2004)" startWordPosition="5221" endWordPosition="5224">riments based on monolingual (Mono), bilingual without antonym (BLP-WOA), and bilingual (BLP) word graphs. antonym intra-language relations WE and WT. 0.8 0.6 0.4 0.2 P@100P@200 P@1K P@10K positive precision at k Mono BLP−WOA BLP P@100P@200 P@1K P@10K 0.8 0.6 0.4 0.2 Mono BLP−WOA BLP 31 Computational Linguistics Volume 41, Number 1 Table 2 Recall evaluation of the experiments based on monolingual (Mono), bilingual without antonym (BLP-WOA), and bilingual (BLP) word graphs. Chinese Mono BLP-WOA BLP Positive 0.623 0.702 0.708 Negative 0.631 0.706 0.709 In these approaches, µ is set to 0.1 as in Zhou et al. (2004). The precision of these approaches are shown in Figure 3. The figure shows that the approaches based on the bilingual word graph significantly outperform the one based on the monolingual word graph. The bilingual word graph can bring in more word relations and accelerate the sentiment propagation. Besides, in the bilingual word graph, the English sentiment seed words can continually provide accurate sentiment information. Thus we observe the increase in the approaches based on the bilingual word graph in term of both precision and recall (Table 2). Meanwhile, we find that adding the antonym r</context>
</contexts>
<marker>Zhou, Bousquet, Lal, Weston, Scholkopf, 2004</marker>
<rawString>Zhou, Dengyong, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scholkopf. 2004. Learning with local and global consistency. In Proceedings of Advances in Neural Information Processing Systems, pages 321–328, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report CMU-CALD-02-107,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="22296" citStr="Zhu and Ghahramani 2002" startWordPosition="3397" endWordPosition="3400">ey contain the different and practical variations of words and phrases embedded in sentiment expressions. Second, the parallel corpus can be dynamically changed when necessary because it is relatively easy to collect from the Web. Consequentially, the novel sentiment information inferred from the parallel corpus can easily update the existing sentiment lexicons. These advantages can greatly improve the coverage of the generated sentiment lexicon, as demonstrated later in our experiments. 3.2 Bilingual Word Graph Label Propagation As commonly used semi-supervised approaches, label propagation (Zhu and Ghahramani 2002) and its variants (Zhu, Ghahramani, and Lafferty 2003; Zhou et al. 2004) have been applied to many applications, such as part-of-speech tagging (Das and Petrov 2011; Li, Graca, and Taskar 2012), image annotation (Wang, Huang, and Ding 2011), protein function prediction (Jiang 2011; Jiang and McQuay 2012), and so forth. 4 http://www.statmt.org/moses/giza/GIZA++.html. 5 http://nlp.cs.berkeley.edu. 27 Computational Linguistics Volume 41, Number 1 The underlying idea of label propagation is that the connected nodes in the graph tend to share the same sentiment labels. In bilingual word graph label</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Zhu, Xiaojin and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical report CMU-CALD-02-107, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In International Conference Machine Learning,</booktitle>
<pages>912--919</pages>
<location>Washington, DC.</location>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Zhu, Xiaojin, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using Gaussian fields and harmonic functions. In International Conference Machine Learning, pages 912–919, Washington, DC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>