<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.288665">
Book Reviews The Formal Complexity of Natural Language
</note>
<bodyText confidence="0.999985897959184">
To provide some details, Cleopatra&apos;s parsing is &amp;quot;lex-
ically driven&amp;quot; in that &amp;quot;each word in the input sentence
invokes procedures that direct the parsing process&amp;quot;.
Samad observes that &amp;quot;we can invoke any arbitrary
function when evaluating a constraint&amp;quot;. Similarly, the
integration procedures that build meaning structures
may also involve &amp;quot;arbitrary Lisp functions&amp;quot;.
Cleopatra attacks ambiguity by use of &amp;quot;confidence
levels&amp;quot; related to such phenomena as &amp;quot;the relative
frequencies of occurrence of different senses of a word,
the likelihood of particular structures, and the corre-
spondence of conjuncts&amp;quot;. Parsing is viewed as a parallel
process, but the author clearly and honestly states that
the existing program is breadth-first.
While the author claims to have achieved a &amp;quot;sharp
contrast to the limited domains of previous natural
language interfaces&amp;quot;, I cannot concur. Although he
does handle certain issues in greater detail than have
most previous systems (for example, time references
and some types of conjunction), there are also, as he so
often and honestly admits, many typical distinctions
that have been largely ignored (apparently, without
suffering a great loss). For example, the system &amp;quot;is not
very sophisticated about auxiliaries or adverbs&amp;quot; and
does not yet &amp;quot;know about&amp;quot; person or number.
Samad suggests that the &amp;quot;final determinant&amp;quot; of a
system with the practical aspirations of his is the
evaluation it receives from its intended users. But his
claim that &amp;quot;Cleopatra is more than a vehicle to demon-
strate the feasibility of our ultimate goal. It is a useful
CAD tool&amp;quot; is simply not substantiated. Although &amp;quot;we
are confident that experimental studies will confirm the
utility of Cleopatra&amp;quot;, there&apos;s no indication that such
investigations have been conducted.
In conclusion, the author is to be commended for his
interest in building a complete system that can be used
for some meaningful purpose. His frank and honest
discussion of the details and limitations of his work are
also to be praised, and the presence of some inherently
interesting example sentences in the domain under
study should be mentioned. However. I find little in the
book that helps clarify any problems of language proc-
essing, nor do I suspect the techniques presented can
provide any &amp;quot;value added&amp;quot; over what&apos;s available from
existing literature. I am also disappointed by the fact
that most of the book concerns implementational issues
discussed at the level of data structures. Although I
cannot recommend the book as a text, it could certainly
be found useful as a case study.
</bodyText>
<sectionHeader confidence="0.737169" genericHeader="abstract">
REFERENCE
</sectionHeader>
<reference confidence="0.451676">
Brown, J. S. and Burton, R. R. 1975 Multiple Representations of
Knowledge for Tutorial Reasoning. In Bobrow, Daniel G. and
Collins, Allan (eds.). Representation and Understanding. Aca-
demic Press, New York, NY: 311-349.
</reference>
<footnote confidence="0.727406666666667">
Bruce Ballard is a Member of Technical Staff at AT&amp;T Bell
Laboratories in Murray Hill, NJ. He holds a Ph.D. from Duke
University and has been working in the field of computational
</footnote>
<note confidence="0.7150416">
linguistics since 1977. He has been on the faculty at Ohio State
University arid Duke University and currently teaches natural
language processing at Rutgers University. Ballard&apos;s address
is 3C-440A, AT&amp;T Bell Labs, 600 Mountain Avenue, Murray
Hill, N.J. 07974 USA. E-mail: allegralbwb.att.com
</note>
<title confidence="0.463356">
THE FORMAL COMPLEXITY OF NATURAL LANGUAGE
</title>
<author confidence="0.7901405">
Walter J. Savitch; Emmon Bach; William Marsh; and
Gila Safran-Naveh (eds.)
</author>
<affiliation confidence="0.993860333333333">
(University of California, San Diego; University of
Massachusetts, Amherst; Xerox Palo Alto
Research Center; and University of Cincinnati)
</affiliation>
<figure confidence="0.5394105">
(Studies in linguistics and philosophy 33)
Dordrecht: D. Reidel, 1987, xviii+451 pp.
ISBN 1-55608-046-8, $69.00, Dfl 145.00, £ 44.95 (hb)
Reviewed by
Alexis Manaster-Ramer
IBM T.J. Watson Research Center
</figure>
<bodyText confidence="0.5911025">
This book anthologizes a number of papers dealing with
mathematical models of, and mathematical claims
about, human languages. The collection begins with a
stage-setting paper by Stanley Peters, &amp;quot;What is mathe-
matical linguistics?&amp;quot; and gives the last word to Gerald
Gazdar and Geoffrey K. Pullum in their &amp;quot;Computa-
tionally relevant properties of natural languages and
their grammars&amp;quot;. The papers in between are grouped
into three sets:
Early non transformational grammar:
Janet Dean Fodor, &amp;quot;Formal linguistics and formal logic&amp;quot;.
Emmon Bach and William Marsh, &amp;quot;An elementary proof of
the Peters-Ritchie theorem&amp;quot;.
Thomas Wasow, &amp;quot;On constraining the class of transforma-
tional languages&amp;quot;.
Gilbert H. Harman, &amp;quot;Generative grammars without trans-
formation rules: A defense of phrase structure&amp;quot;.
P. T. Geach, &amp;quot;A program for syntax&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.228043">
Modern context-free-like models:
</subsectionHeader>
<bodyText confidence="0.712603857142857">
Geoffrey K. Pullum and Gerald Gazdar, &amp;quot;Natural lan-
guages and context-free languages&amp;quot;.
Gerald Gazdar, &amp;quot;Unbounded dependency and coordinate
structure&amp;quot;.
Hans Uszkoreit and Stanley Peters, &amp;quot;On some formal
properties of metarules&amp;quot;.
Emmon Bach, &amp;quot;Some generalizations of categorial gram-
mars&amp;quot;.
More than context-free and less than transformational
grammar:
Joan Bresnan et al., &amp;quot;Cross-serial dependencies in
Dutch&amp;quot;.
Stuart M. Shieber, &amp;quot;Evidence against the context-freeness
of natural language&amp;quot;.
</bodyText>
<footnote confidence="0.80864625">
James Higginbotham, &amp;quot;English is not a context-free
language&amp;quot;.
Christopher Culy, &amp;quot;The complexity of the vocabulary of
Bambara&amp;quot;.
</footnote>
<page confidence="0.822109">
98 Computational Linguistics, Volume 14, Number 4, December 1988
</page>
<subsectionHeader confidence="0.361804">
Book Reviews The Formal Complexity of Natural Language
</subsectionHeader>
<bodyText confidence="0.99682570940171">
Walter J. Savitch, &amp;quot;Context-sensitive grammars and natu-
ral language syntax&amp;quot;.
William Marsh and Barbara H. Partee, &amp;quot;How non-context
free is variable binding&amp;quot;.
In addition the editors penned a general introduction, as
well as brief prefatory notes to each of the three groups
of papers. In this review, I Will comment primarily on
the editorial part of the work, and on the two articles
(Peters&apos;s and Savitch&apos;s) that have been published here
for the first time, with only scattered remarks about
points raised by the other papers.
The title of this collection may mean different things
to different people, but the editors make it clear that by
formal complexity we should understand generative
capacity, and for the most part weak generative capac-
ity. Specifically, the book is organized around the still
unsettled issue of the place where human languages as a
class fall in the Chomsky hierarchy, with particular
attention to the question of whether they are context-
free or, if not, then by how much. The reader should
beware of confusion with computational complexity (in-
volving bounds on resources such as time and work-
space) or static complexity (which deals with grammar or
program size). In short, this volume is an invitation to
the mathematical side of what is commonly, if some-
what inaccurately, known as generative syntax, the
tradition of linguistic analysis presided over by
Chomsky and occasionally haunted by the spirit of
Montague.
The book begins with the editors&apos; brief history of
work on &amp;quot;a mathematically formal theory of syntax&amp;quot;
(pp.vii–viii). This story begins with the rejection of
finite-state models as too weak. &amp;quot;In response&amp;quot;, trans-
formations were introduced, but these were subse-
quently seen as being too powerful, in light of the
Peters–Ritchie proof (for which see the Bach–Marsh
paper in the volume). This result showed that Aspects-
style transformational grammar generated all the recur-
sively enumerable languages (despite, one might add,
the constraints on &amp;quot;recoverability of deletion&amp;quot;). This in
turn led to the introduction of context-free models,
which are intermediate in formal complexity between
finite state and transformational grammar. The focus
here is on generalized phrase structure grammar, with
distinctly less attention being devoted to Harman&apos;s
&amp;quot;discontinuous-constituent phrase-structure grammar
with subscripts and deletes&amp;quot; (derived in part from
earlier work of Yngve&apos;s) and to categorial grammar. The
last section of the book is supposed to &amp;quot;discuss alter-
natives to the context-free model&amp;quot; (p.viii), but that is
not quite true. Actually, it focuses on a sample of the
recent arguments, directed explicitly or implicitly at
GPSG, that human languages are not context-free.
While these articles imply the need for non-CF models,
they do not necessarily present these. Savitch&apos;s excel-
lent paper in this section shows why context-sensitive
grammars are not a plausible basis for a linguistic
theory, but it also offers no alternatives. To be sure,
Bresnan et al. give a brief introduction to, and an
example of, lexical-functional grammar, and Bach&apos;s
paper in the previous section discusses—but seems to
shy away from—extensions of categorial grammar that
allow (certain) non-CF languages (p.273).
But such real alternatives to GPSG as tree-adjoining
and head grammars, both only slightly non-CF, and the
more powerful but linguistically less attractive indexed
grammars are only adumbrated in Gazdar and Pullum&apos;s
epilogical paper. However, the principal current model
of syntax, which also appears to be intermediate in
power between CFG and TG, is the government-binding
framework. This is referred to in passing by Gazdar and
Pullum (p.401), but only to have scorn heaped on its
lack of formal pretensions. The same criticism is pre-
sumably implicit in the editorial decision not to discuss
GB, but while correct up to a point, it strikes me as
ultimately sterile. Ask not whether &amp;quot;N. Chomsky and
his students&amp;quot; have developed &amp;quot;a mathematical under-
pinning&amp;quot; for their theories &amp;quot;in terms of a class of
admissible grammars&amp;quot; (ibid). ask rather how to do it
yourself!
And, if you do, you will almost certainly find a formal
complexity class that is incomparable to any of the
Chomsky hierarchy classes, something that is also
implied by Wasow&apos;s argument (p.65) that finite lan-
guages should be excluded from the class of possible
human languages. There is, I believe, more mathemat-
ical interest to GB than meets the eye, even if this has
not been brought out well in the literature. It is thus
impossible to fault the editors for not reprinting any
existing GB readings, but why not commission a special
contribution on the formalization of GB, or at least raise
the issue editorially?
The heavy emphasis on GPSG, its antecedents, and
its consequences should not, however, discourage those
readers whose allegiances lie elsewhere. While this
volume has its limitations, they are more or less the
natural frontiers of the discipline. First, they embody
the consensus of many of those most active in formal
syntax about what is important and what is not. Second,
they reflect the neglect of formal methods and issues by
the vast majority of syntacticians. Finally, they have to
do with the need to tell a simple story to a reader who
is likely to be a relative novice to the field. One has to
begin somewhere, and Savitch et al. do begin at just
about the right place for somebody interested in the
current state of mathematical linguistics of the genera-
tive persuasion.
Where I do have a substantive quarrel with Savitch et
al. is that their history of the field is so simple as to be
misleading. Chomsky did not ever consider finite state
models as a basis for linguistic theory, and transforma-
tions were not introduced in order to handle the (con-
text-free) center-embedded constructions that seem to
make English a non-regular (non-finite-state) language.
Transformations were intended to do justice to those
features of human language which immediate constitu-
Computational Linguistics, Volume 14, Number 4, December 1988 99
Book Reviews The Formal Complexity of Natural Language
ent and morpheme-to-utterance models, as Chomsky
understood them, seemed to handle by contrived and
epicyclical means, such as discontinuous constituents,
unbounded branching, cross-classifying (complex) cat-
egories, zero constituents, separation of categorial and
linear information, and a small amount of context
sensitivity. The formal complexity argumentation was
no more than a mathematical fig leaf on a soft linguistic
underbelly. This is, of course, why Chomsky advanced
TG, rather than context-free, context-sensitive, or even
unrestricted (type-0) grammar, as the correct model of
human language.
In this context, the argument that &amp;quot;logically&amp;quot;, if not
chronologically, Harman&apos;s proposal of an extended
kind of CFG for English syntax was either a &amp;quot;com-
plement&amp;quot; or a &amp;quot;follow-up&amp;quot; to the Peters and Ritchie
result (pp.22-23) strikes me as completely at odds with
the facts, for Harman&apos;s motivation was avowedly to
show that Chomsky had been wrong in denying the
linguistic sufficiency of CFG, rather than to rein in the
unbridled generative capacity of TG.
In the same way, the result that TGs are equivalent in
formal complexity to unrestricted grammars and to
Turing machines (i.e., generate all recursively enumer-
able languages) was not the reason that linguists started
giving up on them. Rather, TG itself had been found
unable to handle a variety of linguistic phenomena
insightfully. Need I remind readers of global, deep
structure, surface structure, or transderivational con-
straints? Who can forget the dark ages when English
became a verb-first language? When verb-first lan-
guages could not have VPs? When free word order
required scrambling? Without extra devices, transfor-
mations were seen to be inadequate. With them, they
became unnecessary as well. The fact that TG was &amp;quot;too
adequate&amp;quot; (p.vii) certainly bothered some people: this
may have been a small part of the motivation for the
early constraints on deletion developed by Chomsky
and Matthews. And when these efforts failed to achieve
recursiveness, as shown by Peters and Ritchie, it may
be that this result had some psychological effect on the
field, though hardly anyone understood it. But the
literature attests that the renewed attempts in this
direction undertaken by Peters (1973), Lapointe (1977),
Wasow (in the volume), and others, were received with
apathy. Constraining TG, never a burning issue, had
become moot as post-Aspects models began taking
over.
Of these, the approach which culminated in GB
appears to have been motivated not at all by generative
capacity considerations, and the various functional
models little more. Only in the case of the self-con-
sciously CF models of recent years has formal complex-
ity been a significant issue, but even here the main
motivation was linguistic (as noted by Gazdar and
Pullum, (pp.405-407)), not formalistic, and usually had
to do with the inadequacy of TG. For example, Bach
(p.263) discusses a number of things that can be done by
his wrapping alias infixing operations within a weakly
CF categorial model, but which Aspects-style TG is
incapable of. Likewise, Gazdar argues for the GPSG
treatment of coordination by focusing (in part) on the
inability of TG to handle the conjunction of active and
passive VPs (p.185). All this does not contradict the
Peters–Ritchie theorem or Church&apos;s Thesis (which
holds that there is no &amp;quot;machine&amp;quot; more powerful than a
Turing machine or, equivalently, no &amp;quot;grammar&amp;quot; more
powerful than a TG). Rather it serves to highlight the
fact that these results are strictly about weak generative
capacity. So, if TG is still linguistically inadequate, that
simply shows that weak generative capacity is not the
measure of complexity that syntacticians have been
applying to human languages all these years.
It should also be striking that, having rejected
Chomsky&apos;s arguments against phrase structure, the
creators of GPSG chose to develop specifically a con-
text-free alternative. If the picture painted by Savitch et
al. were the correct one, then surely they would have
reached to finite-state models instead, for it is obvious
that Chomsky&apos;s &amp;quot;devastating&amp;quot; (p.vii) arguments against
the possibility that human languages are regular (finite
state) suffered from much the same defects that Pullum
and Gazdar&apos;s paper discusses in connection with the
arguments against context-freeness. This was shown by
Daly (1974) and Levelt (1974), and even more telling is
the fact that there have been several attempts to fix
these arguments up, notably by Hugo Brandt Corstius
(as reported in Levelt, 1974), Langendoen (1977), and—
with particular elegance—by Gazdar and Pullum (p.394)
themselves. The obvious conclusion is that CFG, with
suitable extensions, seemed like a linguistically and
computationally reasonable model, whereas regular
grammars did not. The case against context-freeness
had to be wrong, whereas the very similar case against
finite-state models had to be correct.
Furthermore, Chomsky&apos;s attack on phrase structure
was explicitly directed at context-sensitive grammar,
which he took to be a formalization of structuralist
descriptive practices, yet no one has risen to defend
CSG as a model for human language. Nor have
Chomsky&apos;s arguments been disputed by the defenders
of phrase structure. Instead, from Harman on, the term
phrase structure has been liberated from its formal
definition and allowed to cover devices which get
around Chomsky&apos;s objections to CSG, and thereby
implicitly attest to the validity of these objections. To be
sure, Chomsky had done the opposite to the structural-
ists, by leaving out of his purported formalization of
immediate constituent analysis, almost all the devices
which made it workable. So perhaps it is not surprising
that the editors of the book apparently tried to ignore
the sordid details of the history of syntactic theory by
sketching a separate history for the formal complexity
field. There is nothing wrong in this per se, so long as
readers are made aware that this is an extreme ideali-
zation of what actually happened.
</bodyText>
<page confidence="0.65481">
100 Computational Linguistics, Volume 14, Number 4, December 1988
</page>
<note confidence="0.321475">
Book Reviews The Formal Complexity of Natural Language
</note>
<bodyText confidence="0.996317068376069">
This aside, the introductory passages are on the
whole quite informative and accurate. For example, we
are treated to a nice, straightforward account of the
basic concepts and results of the theory of computation
(pp.ix–xiii), followed by a good summary of the simplest
techniques that can be used to show that a language is—
or is not—context-free (p.xiii–xiv). (The reader should
note, however, that there are other such techniques.)
Another noteworthy feature is the real effort made to
correct a number of misconceptions about formal mod-
els that are widespread in linguistics (e.g., pp.21, 136,
284). However, there are a few inaccuracies, which I
would like to warn the reader about.
The statement that CFGs are a fully adequate model
of programming language syntax (pp. vii–viii) contra-
dicts the observation that most programming languages
are not CF (p.284). The latter is correct, although it is
also true that computer scientists have continued to use
CFGs for (incomplete) specifications of programming
language syntax and as the basis (but not the whole) of
compiler design. In this context, I was also surprised to
read that human languages &amp;quot;appear to be more context-
free&amp;quot; than programming languages, and that the only
proofs of non-CFness for human languages, especially
English, have involved &amp;quot;marginal constructs&amp;quot;, such as
the respectively and such that constructions. Actually,
the notion of formal complexity that is at issue is
essentially that of weak generative capacity, and in
these terms it is impossible to distinguish construc-
t(ion)s.
More serious is the fact that this generalization does
not do justice to the literature at the time the collection
was being put together. The Pullum–Gazdar and Gaz-
dar–Pullum papers cite—though sometimes with all too
little faith—many other non-CF constructions in various
languages. Notably, they refer to the fact that redupli-
cation is widespread in the languages of the world
(p.393) including English (p.399). Indeed, many lin-
guists would probably agree that reduplication—and
related phenomena, such as haplology—are universal in
human language. However, they are not characteristic
of programming languages. On the other hand, I believe
that the features which make many programming lan-
guages non-CF have close parallels in human language.
The declaration of identifiers in languages such as
Pascal may be compared with various rules for topic,
co-reference, and definiteness. The use of definite arti-
cles seems to include (though is not limited to) the
ability to determine which &amp;quot;identifiers&amp;quot; (or, NPs) have
been previously &amp;quot;declared&amp;quot; by the use of indefinite
articles. Likewise, the uniqueness of statement labels in
Pascal is related to various devices (never as successful
as in programming languages but common nonetheless)
for combatting ambiguity, such as the constraint against
using, say, the same name to refer indiscriminately in
the same discourse to two different individuals of that
name.
Likewise, I question the statement that GPSG was
designed to be strongly equivalent to CFG &amp;quot;under many
views of the meaning of strong equivalence&amp;quot; (p.136). I
do not know of any well-defined sense of strong equiv-
alence under which this is true, although, to be sure, the
object grammar induced by a GPSG is, trivially,
strongly equivalent to a CFG, on the assumption that
we treat the complex nonterminal symbols of the former
as equivalent to the unanalyzed symbols of the latter.
Nor can I see why anybody would wish to come up with
such a definition of strong equivalence, since it would
only serve to deny the whole genius of GPSG. The point
of creating GPSG, instead of just using CFG, was
precisely to capture linguistic generalizations which
simple CFG cannot express, in other words to increase
the strong generative capacity of the latter without
sacrificing its restricted weak generative capacity.
Turning now to the individual papers, I will, as I said,
not attempt a detailed review, but only offer a few
plaudits, comments, and emendations. One of the
highly commendable things that Savitch et al. did in
their prefatory remarks was to amplify or correct (e.g.,
pp.21, 135) incomplete or plain wrong statements in the
individual papers. I begin with a few errata that they
missed.
First, Pullum and Gazdar assume that it is possible to
show that a language is not CF by the use of the
pumping lemma (p.147). This is incorrect, inasmuch as
there are non-CF languages which satisfy all the condi-
tions of the pumping lemma, e.g., fanbPcqdr, where
either n = 0 or p =-- q =
Likewise, Shieber makes a formal mistake in his
argument that Swiss German is not CF. In the sentences
he is considering the number of verbs that govern the
dative must equal the number of actual NPs in the
dative case in the sentence, and likewise for the accu-
sative. This is crucial to the proof, which relies on the
one-to-one correspondence of NPs and verbs in each
case. However, he also notes that the NPs themselves
are optional, and yet dismisses this as not &amp;quot;affect[ing]
the proof&amp;quot; (p.332). This amounts to intersecting Swiss
German with a made-up language in which there are
always as many object NPs as transitive verbs. This
latter language is not regular, and the intersection of a
CFL with a non-regular language need not be CF, so the
argument is vitiated. To be sure, it can be fixed,
assuming Shieber&apos;s data to be correct. The number of
dative NPs must still be no greater than that of dative-
governing verbs, and likewise for the accusative, and
this is enough to imply non-context-freeness. The de-
tails are left to the reader.
Finally, the Culy paper discusses a reduplicative
construction in Bambara of the form &amp;quot;Noun o Noun&amp;quot;,
and insists that this is a part of the lexicon rather than
the syntax of the language. The argument given for this
is that this construction shows a tonal behavior (not
described) that is not characteristic of comparable se-
quences of adjacent words. This does not follow, for at
least two reasons. One, we have not been shown that
Computational Linguistics, Volume 14, Number 4, December 1988 101
Book Reviews The Formal Complexity of Natural Language
the behavior in question is characteristic of word-
internal sandhi (in fact, the implication is that it is not).
Two, unique (or, irregular) behavior, whether tonal or
segmental, is quite common in external sandhi the world
over, and hence cannot be a criterion for wordhood.
Finally, I turn to four papers (or at least to certain
points raised in them) that I would like to call the
reader&apos;s special attention to. The first is the introduc-
tory paper by Peters, and here I must first clear away a
possible terminological problem. In talking about
phrase structure grammar (pp.13, 15), Peters is referring
neither to normal context-sensitive grammar (which
was Chomsky&apos;s notion of phrase structure) nor to
context-free grammar (which tends to be identified with
phrase structure nowadays) nor yet to unrestricted
(type-0) grammars (which would be the modern com-
puter scientist&apos;s usage). Rather he is concerned with
context-sensitive node admissibility systems (p.12). Pe-
ters refers to this as parsing by a phrase structure (i.e.,
context-sensitive) grammar, but this is a special techni-
cal sense of the term, as he notes, and quite distinct
from parsing for normal CSGs. On this special interpre-
tation, CSGs &amp;quot;parse&amp;quot; only the CFLs. However, when
the reader is told (p.15) that TG can handle certain
languages that PSG cannot, e.g., copying languages, he
should not conclude that copying languages cannot be
generated or parsed (in the usual sense of the term) by
non-transformational grammars (e.g., CSGs, indexed
grammars, TAGs, and head grammars).
Second, readers should be forewarned that this pa-
per, although published here for the first time, was
apparently written a number of years ago (in the early
1970s, I would say). As a result, it is quite dated in
certain respects, which should not be taken as indica-
tions of a peculiar regression in the field of mathemati-
cal linguistics. Mathematical linguists do not spend their
time these days on the learnability problem for TG or
the &amp;quot;factual&amp;quot; question of whether the base component
is innate (pp.15-16). However, in emphasizing the age
of Peters&apos;s paper, I do not mean to discourage people
from reading it. Quite the reverse, in fact, since I firmly
believe in the value of continually re-examining &amp;quot;old&amp;quot;
work, for this—more than anything--seems to lead to
the kind of permanent revolution that characterizes
science at its best.
A case in point: Peters&apos;s paper is the only place in the
volume where Postal&apos;s much-maligned argument about
Mohawk not being context-free is presented sympathet-
ically. Presumably, this is because at the time the Postal
work had not yet been maligned. Yet I think Postal was
linguistically quite right, even if he mangled the mathe-
matics of the argument a little more than is—or was—
customary. For it does seem to be a fact that Mohawk
reduplicates an intransitive subject or an object noun
and incorporates one copy in the verb. Pullum and
Gazdar in their celebrated attack on Postal (in their
paper in the volume) point out that it is also possible to
incorporate a noun stem without reduplicating, and to
leave a possessor noun outside the verb. Thus, in
addition, to N-V N, where N is subject or object, we
also get N1- V N2, where N1 is subject or object and N2
is the possessor of NI. Surely, while this may make
Mohawk weakly CF, no one would ever write a gram-
mar of this language that failed to distinguish these two
constructions, and to do that we would need a grammar
more powerful than CFG in just the way suggested by
Postal (and Peters). Thus Postal deserves credit for first
calling attention to the fact that human languages make
use of reduplication in their syntax, something which
has turned out to be true of perhaps all known lan-
guages.
Another &amp;quot;old&amp;quot; paper I would call attention to is
Wasow&apos;s, not so much for its specific proposals about
TG, but because it is perhaps the most lucid discussion
of the whole question of formal complexity available. I
wish the editors had put this paper up front, together
with the Peters one. Savitch et al. note that &amp;quot;it can
serve as another introductory article to the whole
volume&amp;quot; (p.22), but the reader may miss this pointer,
and judge the paper solely by its title, which would be a
great pity. And while I am on the topic, I would like to
call attention to a point made by Wasow, which has only
very recently resurfaced in the literature. Many formal
languages are obviously too simple to serve as possible
human languages. Wasow cites finite languages and
infinite mirror-image languages as examples, and there
are many others, e.g., all languages over one letter.
However, linguistic models have always allowed such
degenerate cases. Chomsky was no sooner done arguing
that human languages were neither finite nor regular
than he introduced TGs, which clearly generate all such
languages. Moreover, as noted, Wasow&apos;s position im-
plies the all-important conclusion that the formal com-
plexity of human languages cannot be measured in
terms of the Chomsky hierarchy.
The third paper I would like to single out for praise is
that of Pullum and Gazdar. Again, I would like to urge
the importance of a point made here which is not
directly reflected in the title, and which is too easily
missed on a first reading. In their discussion of certain
constructions in Dutch, Pullum and Gazdar allow sen-
tences to be generated whose English equivalents would
be things like &amp;quot;John will let Mary see Arabic write
Peter&amp;quot; and comment that &amp;quot;[it is not for the syntax to
rule out examples of this sort, for the above example is
perfect on the assumption that there is a language or
writing system called &apos;Peter&apos; and a person named &apos;Ar-
abic&apos; has learned to write it&amp;quot; (p.158).
As far as I know this is the first clear statement in the
linguistic literature of what some know as Ziff&apos;s Law,
the proposition that essentially any string can be the
name of someone or something. A corollary of this is, of
course, that perhaps every string over the alphabet is a
well-formed sentence of every language, since any part
that offends thee can always be taken to be a sufficiently
strange proper name. I
</bodyText>
<page confidence="0.935838">
102 Computational Linguistics, Volume 14, Number 4, December 1988
</page>
<note confidence="0.530933">
Book Reviews Briefly Noted
</note>
<bodyText confidence="0.999780970149254">
And from this it follows that weak generative capac-
ity, as usually conceived, cannot be the right notion of
complexity to study in connection with human language
(even if it were to be measured by a yardstick different
from the Chomsky hierarchy).
Important as the three papers I have mentioned are,
my favorite is the previously unpublished—and, this
time, brand new—paper by Savitch on context-sensitive
grammars. Most of the mathematics underlying the
theory of computation and mathematical linguistics is
familiar, but the implications of these results for empir-
ical science are rarely discussed explicitly in the litera-
ture, and are not widely known. Savitch&apos;s contribution
is one of the few places where a humdrum formal result,
which has been known for a number of years, comes to
life.
The point is essentially this: we know that the
recursively enumerable (r.e.) languages are accepted by
Turing machines, which may use an arbitrary amount of
tape before accepting a given string, whereas the CSLs
are accepted by (non-deterministic) TMs which have a
certain (linear) bound on the amount of tape that may be
used. Now, given any r.e. language, we can construct a
rather similar CSL, in which the extra tape that was
required for the computation is built into the input. To
do this, a string of special marker symbols is appended
to the end of the real input. A computation proceeds as
it would in the original (unbounded) TM except that the
marked cells are used instead of blank ones. If the TM
runs out of the marked cells to hold intermediate
results, then the computation is aborted without accept-
ing. Intuitively, then, the CSL accepted is not very
different from the underlying r.e. language, and we see
that each r.e. language can be encoded as a CSL.
Moreover, the process of converting this CSL to the
underlying r.e. language is trivial: delete the marker
symbols (though there is no algorithm for going from the
r.e. language to the CSL).
Savitch argues, convincingly to my mind, that all this
indicates that CSLs have all the structural complexity of
r.e. languages, and hence are not suitable as a model of
human language—or anything else. We have all been
taught to think of the r.e. languages as including &amp;quot;every-
thing&amp;quot;, but in a real sense, so do the CSLs. For years,
there was a feeling that recursiveness was something to
strive for, but now we see that this was much too
modest a goal. If human languages are characterized by
certain structural universals (e.g., they all use redupli-
cation but not prime length of a string as a grammatical
device), then CSLs are already much too inclusive, for
they contain all languages that can be characterized in
such structural terms. Savitch&apos;s , contribution should
help open up the heavily fortified border between the
mathematical theory of computation and empirical sci-
ence. Perhaps it will be the beginning of a beautiful
friendship.
In sum, this book is well worth careful study. It is by
no means the last word on the subject, but for many it
may well be the first word, and Savitch et al. have done
an excellent job, both in their selections and in their
commentaries, of giving a solid introduction to a
sparsely cultivated but already complex field. They
have also done much to foster the dissemination and the
comprehension of formal complexity results in linguis-
tics and to encourage accuracy and lucidity in the
formulation, presentation, and interpretation of such
results.
</bodyText>
<sectionHeader confidence="0.998148" genericHeader="keywords">
REFERENCES
</sectionHeader>
<reference confidence="0.968435444444445">
Daly, R. T. 1974 Applications of the Mathematical Theory of Linguis-
tics. Mouton, The Hague, Netherlands.
Langendoen, D. T. 1977 On the inadequacy of type-3 and type-2
grammars for human languages, In Studies in Descriptive and
Historical Linguistics: Festschrift for Winfred Lehmann Hopper,
P. J. (Ed.). John Benjamins, Amsterdam, Netherlands, 159-172.
Lapointe, S. 1977 Recursiveness and deletion, Linguistic Analysis
3:227-266.
Levelt, W. J. M. 1974 Formal Grammars in Linguistics and Psycho-
linguistics. Mouton, The Hague, Netherlands.
Peters, S. 1973 On restricting deletion transformations, In The Formal
Analysis of Natural Languages Gross, Maurice; Halle, Morris;
and Schutzenberger, Marcel-Paul (eds.). Mouton, The Hague,
Netherlands and Paris, France.
Ziff, P. 1960 Semantic Analysis. Cornell University Press, Ithaca,
NY.
NOTE
1. Ziff (1960, pp.85-86) drew a different conclusion, namely, that
</reference>
<bodyText confidence="0.698519692307692">
proper names are not words, but this would still spell the end of
any real raison d&apos;etre for weak generative capacity studies in
linguistics, since now when we look at sentences of a human
language we must somehow distinguish those containing names
from those that do not.
Alexis Manaster-Ramer received his Ph.D. in linguistics from
the University of Chicago in 1981, and has taught linguistics at
the University of Michigan, and computer science at Wayne
State University. He is editor of the book Mathematics of
Language (John Benjamins 1987), and is presently organizing
the new Association for Mathematics of Language. Manaster-
Ramer&apos;s address is IBM T.J. Watson Research Center, PO
Box 704, Yorktown Heights, NY 10598.
</bodyText>
<sectionHeader confidence="0.831224" genericHeader="introduction">
BRIEFLY NOTED
</sectionHeader>
<reference confidence="0.436189333333333">
THE CAMBRIDGE ENCYCLOPEDIA OF LANGUAGE
David Crystal
(University College of North Wales)
Cambridge University Press, Cambridge, England, 1987,
vii +472 pp.
ISBN 0-521-26438-3, $39.50 (hb)
David Crystal is well known as a linguist who has worked in
such assorted areas as stylistics and language pathology. In
addition to his research writings, he is also a popularizer of
linguistics, being the author of an introductory book and a
radio broadcaster in the U.K.
The present volume is also aimed at the general public, and
is a particularly ambitious attempt to present the fascination
(and breadth) of the sciences of language. Topics covered
Computational Linguistics, Volume 14, Number 4, December 1988 103
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001984">
<title confidence="0.968841">Reviews The Complexity of Natural Language</title>
<abstract confidence="0.994937306122449">To provide some details, Cleopatra&apos;s parsing is &amp;quot;lexically driven&amp;quot; in that &amp;quot;each word in the input sentence invokes procedures that direct the parsing process&amp;quot;. Samad observes that &amp;quot;we can invoke any arbitrary function when evaluating a constraint&amp;quot;. Similarly, the integration procedures that build meaning structures may also involve &amp;quot;arbitrary Lisp functions&amp;quot;. Cleopatra attacks ambiguity by use of &amp;quot;confidence levels&amp;quot; related to such phenomena as &amp;quot;the relative frequencies of occurrence of different senses of a word, the likelihood of particular structures, and the correspondence of conjuncts&amp;quot;. Parsing is viewed as a parallel process, but the author clearly and honestly states that the existing program is breadth-first. While the author claims to have achieved a &amp;quot;sharp contrast to the limited domains of previous natural language interfaces&amp;quot;, I cannot concur. Although he does handle certain issues in greater detail than have most previous systems (for example, time references and some types of conjunction), there are also, as he so often and honestly admits, many typical distinctions that have been largely ignored (apparently, without suffering a great loss). For example, the system &amp;quot;is not very sophisticated about auxiliaries or adverbs&amp;quot; and does not yet &amp;quot;know about&amp;quot; person or number. Samad suggests that the &amp;quot;final determinant&amp;quot; of a system with the practical aspirations of his is the evaluation it receives from its intended users. But his claim that &amp;quot;Cleopatra is more than a vehicle to demonstrate the feasibility of our ultimate goal. It is a useful CAD tool&amp;quot; is simply not substantiated. Although &amp;quot;we are confident that experimental studies will confirm the utility of Cleopatra&amp;quot;, there&apos;s no indication that such investigations have been conducted. In conclusion, the author is to be commended for his interest in building a complete system that can be used for some meaningful purpose. His frank and honest discussion of the details and limitations of his work are also to be praised, and the presence of some inherently interesting example sentences in the domain under study should be mentioned. However. I find little in the book that helps clarify any problems of language processing, nor do I suspect the techniques presented can provide any &amp;quot;value added&amp;quot; over what&apos;s available from existing literature. I am also disappointed by the fact that most of the book concerns implementational issues discussed at the level of data structures. Although I cannot recommend the book as a text, it could certainly be found useful as a case study.</abstract>
<note confidence="0.723778583333333">REFERENCE S. Burton, R. R. Representations of Knowledge for Tutorial Reasoning. In Bobrow, Daniel G. and Allan (eds.). and Understanding. Aca- New York, NY: Ballard is Member of Technical Staff at AT&amp;T Bell Laboratories in Murray Hill, NJ. He holds a Ph.D. from Duke University and has been working in the field of computational linguistics since 1977. He has been on the faculty at Ohio State University arid Duke University and currently teaches natural language processing at Rutgers University. Ballard&apos;s address is 3C-440A, AT&amp;T Bell Labs, 600 Mountain Avenue, Murray</note>
<email confidence="0.437311">Hill,N.J.07974USA.E-mail:allegralbwb.att.com</email>
<title confidence="0.705701">THE FORMAL COMPLEXITY OF NATURAL LANGUAGE</title>
<author confidence="0.7862635">Walter J Savitch</author>
<author confidence="0.7862635">Emmon Bach</author>
<author confidence="0.7862635">William Marsh</author>
<author confidence="0.7862635">Gila Safran-Naveh</author>
<affiliation confidence="0.821586">University of California, San Diego; University of</affiliation>
<address confidence="0.532471">Massachusetts, Amherst; Xerox Palo Alto</address>
<note confidence="0.9697644">Research Center; and University of Cincinnati) (Studies in linguistics and philosophy 33) Dordrecht: D. Reidel, 1987, xviii+451 pp. ISBN 1-55608-046-8, $69.00, Dfl 145.00, £ 44.95 (hb) Reviewed by</note>
<author confidence="0.966437">Alexis Manaster-Ramer</author>
<affiliation confidence="0.904447">T.J. Research Center</affiliation>
<abstract confidence="0.980436628571429">This book anthologizes a number of papers dealing with mathematical models of, and mathematical claims about, human languages. The collection begins with a stage-setting paper by Stanley Peters, &amp;quot;What is mathematical linguistics?&amp;quot; and gives the last word to Gerald Gazdar and Geoffrey K. Pullum in their &amp;quot;Computationally relevant properties of natural languages and their grammars&amp;quot;. The papers in between are grouped three Early non transformational grammar: Janet Dean Fodor, &amp;quot;Formal linguistics and formal logic&amp;quot;. Emmon Bach and William Marsh, &amp;quot;An elementary proof of the Peters-Ritchie theorem&amp;quot;. Thomas Wasow, &amp;quot;On constraining the class of transformational languages&amp;quot;. H. Harman, &amp;quot;Generative grammars without transformation rules: A defense of phrase structure&amp;quot;. P. T. Geach, &amp;quot;A program for syntax&amp;quot;. Modern context-free-like models: Geoffrey K. Pullum and Gerald Gazdar, &amp;quot;Natural languages and context-free languages&amp;quot;. Gerald Gazdar, &amp;quot;Unbounded dependency and coordinate structure&amp;quot;. Hans Uszkoreit and Stanley Peters, &amp;quot;On some formal properties of metarules&amp;quot;. Emmon Bach, &amp;quot;Some generalizations of categorial grammars&amp;quot;. More than context-free and less than transformational grammar: Joan Bresnan et al., &amp;quot;Cross-serial dependencies in Dutch&amp;quot;. Stuart M. Shieber, &amp;quot;Evidence against the context-freeness of natural language&amp;quot;. James Higginbotham, &amp;quot;English is not a context-free language&amp;quot;.</abstract>
<note confidence="0.709504">Christopher Culy, &amp;quot;The complexity of the vocabulary of Bambara&amp;quot;. Computational Linguistics, Volume 14, Number 4, 1988</note>
<title confidence="0.766131">Book Reviews The Formal Complexity of Natural Language</title>
<author confidence="0.586616">Walter J Savitch</author>
<author confidence="0.586616">Context-sensitive grammars</author>
<author confidence="0.586616">natu-</author>
<abstract confidence="0.57532">ral language syntax&amp;quot;. William Marsh and Barbara H. Partee, &amp;quot;How non-context free is variable binding&amp;quot;.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J S Brown</author>
<author>R R Burton</author>
</authors>
<title>Multiple Representations of Knowledge for Tutorial Reasoning.</title>
<date>1975</date>
<pages>311--349</pages>
<editor>In Bobrow, Daniel G. and Collins, Allan (eds.). Representation and Understanding.</editor>
<publisher>Academic Press,</publisher>
<location>New York, NY:</location>
<marker>Brown, Burton, 1975</marker>
<rawString>Brown, J. S. and Burton, R. R. 1975 Multiple Representations of Knowledge for Tutorial Reasoning. In Bobrow, Daniel G. and Collins, Allan (eds.). Representation and Understanding. Academic Press, New York, NY: 311-349.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T Daly</author>
</authors>
<title>Applications of the Mathematical Theory of Linguistics.</title>
<date>1974</date>
<location>Mouton, The Hague, Netherlands.</location>
<marker>Daly, 1974</marker>
<rawString>Daly, R. T. 1974 Applications of the Mathematical Theory of Linguistics. Mouton, The Hague, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D T Langendoen</author>
</authors>
<title>On the inadequacy of type-3 and type-2 grammars for human languages,</title>
<date>1977</date>
<booktitle>In Studies in Descriptive and Historical Linguistics: Festschrift for</booktitle>
<pages>159--172</pages>
<location>Amsterdam, Netherlands,</location>
<marker>Langendoen, 1977</marker>
<rawString>Langendoen, D. T. 1977 On the inadequacy of type-3 and type-2 grammars for human languages, In Studies in Descriptive and Historical Linguistics: Festschrift for Winfred Lehmann Hopper, P. J. (Ed.). John Benjamins, Amsterdam, Netherlands, 159-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lapointe</author>
</authors>
<title>Recursiveness and deletion,</title>
<date>1977</date>
<journal>Linguistic Analysis</journal>
<pages>3--227</pages>
<marker>Lapointe, 1977</marker>
<rawString>Lapointe, S. 1977 Recursiveness and deletion, Linguistic Analysis 3:227-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
</authors>
<title>Formal Grammars in Linguistics and Psycholinguistics.</title>
<date>1974</date>
<location>Mouton, The Hague, Netherlands.</location>
<marker>Levelt, 1974</marker>
<rawString>Levelt, W. J. M. 1974 Formal Grammars in Linguistics and Psycholinguistics. Mouton, The Hague, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Peters</author>
</authors>
<title>On restricting deletion transformations,</title>
<date>1973</date>
<booktitle>In The Formal Analysis of Natural Languages</booktitle>
<editor>Gross, Maurice; Halle, Morris; and Schutzenberger, Marcel-Paul (eds.). Mouton, The Hague, Netherlands and Paris,</editor>
<marker>Peters, 1973</marker>
<rawString>Peters, S. 1973 On restricting deletion transformations, In The Formal Analysis of Natural Languages Gross, Maurice; Halle, Morris; and Schutzenberger, Marcel-Paul (eds.). Mouton, The Hague, Netherlands and Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ziff</author>
</authors>
<title>Semantic Analysis.</title>
<date>1960</date>
<publisher>Cornell University Press,</publisher>
<location>Ithaca, NY.</location>
<marker>Ziff, 1960</marker>
<rawString>Ziff, P. 1960 Semantic Analysis. Cornell University Press, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NOTE</author>
</authors>
<title>pp.85-86) drew a different conclusion, namely,</title>
<date>1960</date>
<journal>that THE CAMBRIDGE ENCYCLOPEDIA OF LANGUAGE</journal>
<volume>472</volume>
<pages>pp.</pages>
<publisher>Cambridge University Press,</publisher>
<institution>David Crystal (University College of North Wales)</institution>
<location>Cambridge, England,</location>
<marker>NOTE, 1960</marker>
<rawString>NOTE 1. Ziff (1960, pp.85-86) drew a different conclusion, namely, that THE CAMBRIDGE ENCYCLOPEDIA OF LANGUAGE David Crystal (University College of North Wales) Cambridge University Press, Cambridge, England, 1987, vii +472 pp.</rawString>
</citation>
<citation valid="false">
<authors>
<author>ISBN</author>
</authors>
<title>39.50 (hb) David Crystal is well known as a linguist who has worked in such assorted areas as stylistics and language pathology. In addition to his research writings, he is also a popularizer of linguistics, being the author of an introductory book and a radio broadcaster in the U.K.</title>
<marker>ISBN, </marker>
<rawString>ISBN 0-521-26438-3, $39.50 (hb) David Crystal is well known as a linguist who has worked in such assorted areas as stylistics and language pathology. In addition to his research writings, he is also a popularizer of linguistics, being the author of an introductory book and a radio broadcaster in the U.K.</rawString>
</citation>
<citation valid="true">
<title>The present volume is also aimed at the general public, and is a particularly ambitious attempt to present the fascination (and breadth) of the sciences of language.</title>
<date>1988</date>
<journal>Topics covered Computational Linguistics, Volume</journal>
<volume>14</volume>
<pages>103</pages>
<marker>1988</marker>
<rawString>The present volume is also aimed at the general public, and is a particularly ambitious attempt to present the fascination (and breadth) of the sciences of language. Topics covered Computational Linguistics, Volume 14, Number 4, December 1988 103</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>