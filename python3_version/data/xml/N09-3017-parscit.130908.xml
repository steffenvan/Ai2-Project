<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000149">
<title confidence="0.9102185">
Disambiguation of Preposition Sense Using Linguistically Motivated
Features
</title>
<author confidence="0.996764">
Stephen Tratz and Dirk Hovy
</author>
<affiliation confidence="0.934982333333333">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
</affiliation>
<email confidence="0.999058">
{stratz,dirkh}@isi.edu
</email>
<sectionHeader confidence="0.996662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999605923076923">
In this paper, we present a supervised classifi-
cation approach for disambiguation of prepo-
sition senses. We use the SemEval 2007
Preposition Sense Disambiguation datasets to
evaluate our system and compare its results to
those of the systems participating in the work-
shop. We derived linguistically motivated fea-
tures from both sides of the preposition. In-
stead of restricting these to a fixed window
size, we utilized the phrase structure. Testing
with five different classifiers, we can report an
increased accuracy that outperforms the best
system in the SemEval task.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999787111111111">
Classifying instances of polysemous words into
their proper sense classes (aka sense disambigua-
tion) is potentially useful to any NLP application
that needs to extract information from text or build
a semantic representation of the textual information.
However, to date, disambiguation between preposi-
tion senses has not been an object of great study. In-
stead, most word sense disambiguation work has fo-
cused upon classifying noun and verb instances into
their appropriate WordNet (Fellbaum, 1998) senses.
Prepositions have mostly been studied in the con-
text of verb complements (Litkowski and Hargraves,
2007). Like instances of other word classes, many
prepositions are ambiguous, carrying different se-
mantic meanings (including notions of instrumental,
accompaniment, location, etc.) as in “He ran with
determination”, “He ran with a broken leg”, or “He
ran with Jane”. As NLP systems take more and more
</bodyText>
<page confidence="0.875006">
96
</page>
<bodyText confidence="0.999700823529412">
semantic content into account, disambiguating be-
tween preposition senses becomes increasingly im-
portant for text processing tasks.
In order to disambiguate different senses, most
systems to date use a fixed window size to derive
classification features. These may or may not be
syntactically related to the preposition in question,
resulting–in the worst case–in an arbitrary bag of
words. In our approach, we make use of the phrase
structure to extract words that have a certain syn-
tactic relation with the preposition. From the words
collected that way, we derive higher level features.
In 2007, the SemEval workshop presented par-
ticipants with a formal preposition sense dis-
ambiguation task to encourage the development
of systems for the disambiguation of preposition
senses (Litkowski and Hargraves, 2007). The train-
ing and test data sets used for SemEval have been re-
leased to the general public, and we used these data
to train and test our system. The SemEval work-
shop data consists of instances of 34 prepositions
in natural text that have been tagged with the ap-
propriate sense from the list of the common Eng-
lish preposition senses compiled by The Preposition
Project, cf. Litkowski (2005). The SemEval data
provides a natural method for comparing the per-
formance of preposition sense disambiguation sys-
tems. In our paper, we follow the task requirements
and can thus directly compare our results to the ones
from the study. For evaluation, we compared our re-
sults to those of the three systems that participated
in the task (MELB: Ye and Baldwin (2007); KU:
Yuret (2007); IRST: Popescu et al. (2007)). We also
used the “first sense” and the “most frequent sense”
</bodyText>
<note confidence="0.6705315">
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96–100,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999859">
baselines (see section 3 and table 1). These baselines
are determined by the TPP listing and the frequency
in the training data, respectively. Our system beat
the baselines and outperformed the three participat-
ing systems.
</bodyText>
<sectionHeader confidence="0.997857" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999312">
2.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.969372533333333">
We downloaded the test and training data provided
by the SemEval-2007 website for the preposition
sense disambiguation task. These are 34 separate
XML files–one for each preposition–, comprising
16557 training and 8096 test example sentences,
each sentence containing one example of the respec-
tive preposition.
What are your beliefs
&lt;head&gt;about&lt;/head&gt; these emotions ?
The preposition is annotated by a head tag, and the
meaning of the preposition in question is given as
defined by TPP.
Each preposition had between 2 and 25 different
senses (on average 9.76). For the case of “about”
these would be
</bodyText>
<listItem confidence="0.999701428571429">
1. on the subject of; concerning
2. so as to affect
3. used to indicate movement within a particular
area
4. around
5. used to express location in a particular place
6. used to describe a quality apparent in a person
</listItem>
<bodyText confidence="0.995634">
We parsed the sentences using the Charniak
parser (Charniak, 2000). Note that the Charniak
parser–even though among the best availbale Eng-
lish parsers–occasionally fails to parse a sentence
correctly. This might result in an erroneous extrac-
tion, such as an incorrect or no word. However,
these cases are fairly rare, and we did not manually
correct this, but rather relied on the size of the data
to compensate for such an error.
After this preprocessing step, we were able to ex-
tract the features.
</bodyText>
<subsectionHeader confidence="0.999562">
2.2 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999966277777778">
Following O’Hara and Wiebe (2003) and
Alam (2004), we assumed that there is a meaningful
connection between syntactically related words on
both sides of the preposition. We thus focused on
specific words that are syntactically related to the
preposition via the phrase structure. This has the
advantage that it is not limited to a certain window
size; phrases might stretch over dozens of words,
so the extracted word may occur far away from the
actual preposition. These words were chosen based
on a manual analysis of training data. Using Tregex
(Levy and Andrew, 2006), a utility for expressing
“regular expressions over trees”, we created a set
of rules to extract the words in question. Each rule
matched words that exhibited a specific relationship
with the preposition or were within a two word
window to cover collocations. An example rule is
given below.
</bodyText>
<equation confidence="0.901479">
IN &gt; (PP &lt; (V P &lt; # = x&amp; &lt;
#!AUX))
</equation>
<bodyText confidence="0.999737">
This particular rule finds the head (denoted by x) of
a verb phrase that governs the prepositional phrase
containing the preposition, unless x is an auxiliary
verb. Tregex rules were used to identify the follow-
ing words for feature generation:
</bodyText>
<listItem confidence="0.980381">
• the head verb/noun that immediately dominates
the preposition along with all of its modifying
determiners, quantifiers, numbers, and adjec-
tives
• the head verb/noun immediately dominated by
the preposition along with all of its modifying
determiners, quantifiers, numbers, and adjec-
tives
• the subject, negator, and object(s) of the imme-
diately dominating verb
• neighboring prepositional phrases dominated
by the same verb/noun (“sister” prepositional
phrases)
• words within 2 positions to the left or right of
the preposition
</listItem>
<bodyText confidence="0.9977565">
For each word extracted using these rules, we col-
lected the following items:
</bodyText>
<page confidence="0.993932">
97
</page>
<listItem confidence="0.996044923076923">
• the word itself
� the head v
• lemma
its
• part-of-speech (both exact and conflated, e.g.
� the subject negaor, and object(s) of the
both ’VBD’ and ’verb’ for ’VBD’)
immediaty doinatig verb
� hb ii hr
• all synonymsof the first WordNet sense
d by he ame vb/nou (
rsiil h
• all hypernyms of the first WordNet sense
</listItem>
<bodyText confidence="0.856418666666667">
wd w 2 psions th
f th prii
Fr words extracted using these rul
</bodyText>
<listItem confidence="0.657958">
• boolean indicator for capitalization
</listItem>
<bodyText confidence="0.949255764705882">
d h flli f
Each feature is a combination of the extraction
he word isef
lem
rule and the extracted item. The values the feature
� tfph (bth t d fltd
can take on are binary: present or absent. For some
(eg boh &apos;VBD and &apos;verb for &apos;VBD&apos;))
prepositions, this resulted in several thousand fea-
tures. In order to reduceecomputation time, we used
the following steps: For each preposition classifier,
� boolean indicator for capitalization
werankd th features using information gain (For-
This resulted in several thousand featues for the
prepositions We used information gain (Foreman,
man, 2003). From the resulting lists,we included at
most 4000 features. Thus not all classifiers used the
</bodyText>
<figure confidence="0.658171166666667">
3) in ordr nd ghe kig ure
f h l d liid lifi th
4000 f
same features.
t i
23 lassifie Training
</figure>
<subsectionHeader confidence="0.990047">
2.3 Classifier Training
</subsectionHeader>
<bodyText confidence="0.991012464285714">
We chose maximum entropy (Berger et al., 1996) as
W ho xium trpy (Brge 1996) our
our primary classifier, since it had been successfully
imay lasifi ba th hight fi
systems in both the SemEval2007 preposition
applied by the highest performing systems in both
sense disambiguation task (Ye and Baldwin 2007)
the SemEval-2007 preposition sense disambiguation
and the general word sense dismbiguatio tsk
task (Ye and Baldwin, 2007) and the general word
(Tratz et l., 2007) used it. We used the implemn-
sense disambiguation task (Tratz et al., 2007). We
tation provided by he Mallet machine larning
used the implementation provided by the Mallet ma-
toolki (McCalum, 2002). Then, fo the sae of
chine learning toolkit (McCallum, 2002). For the
sakeof comparison, we also built several other clas-
mpri, it val cssrs
inclding multiial nïe By SVM kNN,
and decsion trees (J48) using he WEKA toolkit
sifiers, including multinomial na¨ıve Bayes, SVMs,
(Witten 1999) We chose the radal bass fuction
kNN, and decision trees (J48) using the WEKA
(RBF) kernel fo the SVMs and left all other pa-
toolkit (Witten, 1999). We chose the radial basis
rameers at their defaut values.
function (RBF) kernel for the SVMs and left all
other parameters at their default values.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.98553845">
the test et provided by SemEval2007 and pro-
We measured the accuracy of the classifiers over
vided these results in Table 1. It is notable that our
the test set provided by SemEval-2007 and provided
system produced good results with all classifiers:
these results in Table 1. It is notable that our system
the f h lssfers, th auray is high
produced good results with all classifiers: For three
th MELB, th wii f th tk A
of the classifiers, the accuracy is higher than MELB,
the winning system of the task. As expected, the
highest accuracy was achieved using the maximum
entropy classifier. Overall, our system outperformed
the winning system by 0.058, an 8 percent improve-
Overall, our system outperformed the winning
ment. A simple proportion test shows this to be sta-
ym y 00, 8 pret provmen A
impl pprtion test show thi t b ttitill
sgnificant at 0001
tistically significant at 0.001.
</bodyText>
<figure confidence="0.2398106">
ystem
S�stem Acc�rac�
Accuracy
*++ ,-,./0
N
</figure>
<table confidence="0.987736117647059">
!67 89:; *#&lt;=#&gt;? ,-,.12
VM (RBF Ke) ,-,3%2
@0/ A#BCDCE= $&lt;##D ,-,34%
48 decision trees ,-,35%
7F&gt;$C=EGC&apos;&gt; +&apos;HI# :&apos;J#D ,-,.14
Multinomial Naïve Bay ,-,503
7&apos;KCGFG #=$&lt;ELJ ,-,01.
,-,41.
7MN:
aximum Entropy
8O# &apos;=A :&apos;&gt;APC=Q 2,,3?
RS 8OF&lt;#$Q 2,,3?
ost Frequn Se
T9!U 8&amp;EL#DBF #$ &apos;&gt;-Q 2,,3?
RS (Popu t , 7
7ED$ V&lt;#WF#=$ D#=D#
MELB (Ye and Baldwin, 2007
</table>
<tableCaption confidence="0.888789333333333">
Table 1: Accuracy results on SemEval data (with 4000
693
Table 1 A
</tableCaption>
<equation confidence="0.592534">
features)
nce cu o e ue
</equation>
<bodyText confidence="0.986502454545455">
Since our initial cutoff of 4000 features was ar-
Mi Et it
multiple times with different cutoffs Accuracy
bitrary, we reran our Maximum Entropy experiment
consistenly increased as the feature limit was re
multiple times with different cutoffs. Accuracy con-
axed, resulting in 0.764 accuracy at the 10k fea-
sistently increased as the feature limit was relaxed,
resuting in 0.764 accurcy at the 10k feature limit.
ure limit These results are displayed in Figure 1.
These results are displayed in Figure 1.
</bodyText>
<figureCaption confidence="0.831631">
and accurcy for the Maximum Entropy lassifiers
Figure 1: Maximum feature limit vs. accuracy for maxi-
mum entropy classifier
</figureCaption>
<bodyText confidence="0.8652">
us s copious and di
</bodyText>
<sectionHeader confidence="0.999864" genericHeader="related work">
4 Related Work
</sectionHeader>
<subsectionHeader confidence="0.545508">
biguation n computational linguiscs
</subsectionHeader>
<bodyText confidence="0.948489615384615">
The linguistic literature on prepositions and their use
iscopious an divrse. We restrict ourlves to the
O&apos;Hara and Wiebe 2003) make use of Penn
Treebank (Marcus t al, 1993) and FrameNet
systems that competed in the SemEval 2007 Prepo-
(Bake et al. 1998) to classify prpositions. They
sition Sense Disambiguation task. All three of the
show that using high level features from the con-
systems within the framework of the SemEval task
, mai rl, ignifily aid dis
used supervised learning algorithms, yet they dif-
fered widely in the data collection and model prepa-
ration.
</bodyText>
<page confidence="0.99035">
98
</page>
<bodyText confidence="0.999886">
Ye and Baldwin (2007) participated in the Sem-
Eval task using a maximum entropy classifier and
achieved the highest accuracy of the participating
systems. The features they extracted were similar
to the ones we used, including POS and WordNet
features, but they used a substantially larger word
window, taking seven words from each side of the
preposition. While they included many higher level
features, they state that the direct lexical context
(i.e., bag-of-words) features were the most effective
and account for the majority of features, while syn-
tactic and semantic features had relatively little im-
pact.
Yuret (2007) used a n-gram model based on word
substitution by synonyms or antonyms. While this
proved to be quite successful with content words, it
had considerable problems with prepositions, since
the number of synonyms and/or antonyms is fairly
limited.
Popescu et al. (2007) take an interesting approach
which they call Chain Clarifying Relationship. They
are using a supervised algorithm to learn a regu-
lar language. They used the Charniak parser and
FrameNet information on the head, yet the features
they extract are generally not linguistically moti-
vated.
</bodyText>
<sectionHeader confidence="0.998065" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999961918918919">
Using the phrase structure allows for more freedom
in the choice of words for feature selection, yet still
guarantees to find words for which some syntactic
relation with the preposition holds. Extracting se-
mantic features from these words (hypernyms, syn-
onyms, etc.) allows for a certain degree of abstrac-
tion, and thus a high level comparison. O’Hara and
Wiebe (2003) also make use of high level features,
in their case the Penn Treebank (Marcus et al., 1993)
and FrameNet (Baker et al., 1998) to classify prepo-
sitions. They show that using high level features–
such as semantic roles–of words in the context sub-
stantially aids disambiguation efforts. They cau-
tion, however, that indiscriminately using colloca-
tions and neighboring words may yield high accu-
racy, but has the risk of overfitting. In order to mit-
igate this, they classify the features by their part of
speech. While we made use of collocation features,
we also took into account higher order aspects of the
context, such as the governing phrase, part of speech
type, and semantic class according to WordNet. All
other things being equal, this seems to increase per-
formance substantially.
As for the classifiers used, our results seem to
confirm that Maximum Entropy classifiers are very
well suited for disambiguation tasks. Other than
naive Bayes, they do not presuppose a conditional
independence between the features, which clearly
not always holds (quite contrary, the underlying syn-
tactic structure creates strong interdependencies be-
tween words and features). This, however, does not
satisfactory explain the ranking of the other classi-
fiers. One possible explanation could be the sensi-
tivity of for example decision trees to random noise.
Though we made use of information gain before
classification, there still seems to be a certain ten-
dency to split on features that are not optimal.
</bodyText>
<sectionHeader confidence="0.999016" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999981740740741">
We showed that using a number of simple linguis-
tically motivated features can improve the accu-
racy of preposition sense disambiguation. Utilizing
widely used and freely available standard tools for
language processing and a set of simple rules, we
were able to extract these features easily and with
very limited preprocessing. Instead of taking a “bag
of words” approach that focuses primarily upon the
words within a fixed window size, we focused on el-
ements that are related via the phrase structure. We
also included semantic information gathered from
WordNet about the extracted words. We compared
five different classifiers and demonstrated that they
all perform very well, using our selected feature set.
Several of them even outperformed the top system
at SemEval. Our best result was obtained using a
maximum entropy classifier, just as the best partici-
pating system, leading us to believe that our primary
advantage was our feature set. While the contribu-
tion of the direct context (+/-7 words) might have
a stronger effect than higher level features (Ye and
Baldwin, 2007), we conclude from our findings that
higher level features do make an important contribu-
tion. These results are very encouraging on several
levels, and demonstrate the close interaction of syn-
tax and semantics. Leveraging these types of fea-
tures effectively is a promising prospect for future
</bodyText>
<page confidence="0.995488">
99
</page>
<bodyText confidence="0.997776">
machine learning research in preposition sense dis-
ambiguation.
</bodyText>
<sectionHeader confidence="0.990987" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999771666666667">
The authors would like to thank Eduard Hovy and
Gully Burns for invaluable comments and helpful
discussions.
</bodyText>
<sectionHeader confidence="0.997877" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999895453125">
Y.S. Alam. 2004. Decision Trees for Sense Disambigua-
tion of Prepositions: Case of Over. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 52–59.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86–90. Association for
Computational Linguistics Morristown, NJ, USA.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–71.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In ACM International Conference Proceeding Series,
volume 4, pages 132–139.
C. Fellbaum. 1998. WordNet: an electronic lexical
database. MIT Press USA.
G. Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. The Jour-
nal of Machine Learning Research, 3:1289–1305.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data struc-
tures. In LREC 2006.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Ken Litkowski. 2005. The preposition project.
http://www.clres.com/prepositions.html.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of Eng-
lish: the Penn TreeBank. Computational Linguistics,
19(2):313–330.
A.K. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit. 2002. http://mallet. cs. umass.
edu.
T. O’Hara and J. Wiebe. 2003. Preposition semantic
classification via Penn Treebank and FrameNet. In
Proceedings of CoNLL, pages 79–86.
Octavian Popescu, Sara Tonelli, and Emanuele Pianta.
2007. IRST-BP: Preposition Disambiguation based on
Chain Clarifying Relationships Contexts. In MELB-
YB: Preposition Sense Disambiguation Using Rich Se-
mantic Features, Prague, Czech Republic.
S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell,
C. Posse, and P. Whitney. 2007. PNNL: A Supervised
Maximum Entropy Approach to Word Sense Disam-
biguation. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007).
I.H. Witten. 1999. Weka: Practical Machine Learn-
ing Tools and Techniques with Java Implementations.
Dept. of Computer Science, University of Waikato,
University of Waikato, Dept. of Computer Science.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Seman-
tic Features. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Deniz Yuret. 2007. Ku: Word sense disambiguation by
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
</reference>
<page confidence="0.990727">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943779">
<title confidence="0.998556">Disambiguation of Preposition Sense Using Linguistically Motivated Features</title>
<author confidence="0.991874">Stephen Tratz</author>
<author confidence="0.991874">Dirk</author>
<affiliation confidence="0.9995715">Information Sciences University of Southern</affiliation>
<address confidence="0.997918">4676 Admiralty Way, Marina del Rey, CA</address>
<abstract confidence="0.996817571428571">In this paper, we present a supervised classification approach for disambiguation of preposition senses. We use the SemEval 2007 Preposition Sense Disambiguation datasets to evaluate our system and compare its results to those of the systems participating in the workshop. We derived linguistically motivated features from both sides of the preposition. Instead of restricting these to a fixed window size, we utilized the phrase structure. Testing with five different classifiers, we can report an increased accuracy that outperforms the best system in the SemEval task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y S Alam</author>
</authors>
<title>Decision Trees for Sense Disambiguation of Prepositions: Case of Over.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Workshop on Computational Lexical Semantics,</booktitle>
<pages>52--59</pages>
<contexts>
<context position="5239" citStr="Alam (2004)" startWordPosition="827" endWordPosition="828">escribe a quality apparent in a person We parsed the sentences using the Charniak parser (Charniak, 2000). Note that the Charniak parser–even though among the best availbale English parsers–occasionally fails to parse a sentence correctly. This might result in an erroneous extraction, such as an incorrect or no word. However, these cases are fairly rare, and we did not manually correct this, but rather relied on the size of the data to compensate for such an error. After this preprocessing step, we were able to extract the features. 2.2 Feature Extraction Following O’Hara and Wiebe (2003) and Alam (2004), we assumed that there is a meaningful connection between syntactically related words on both sides of the preposition. We thus focused on specific words that are syntactically related to the preposition via the phrase structure. This has the advantage that it is not limited to a certain window size; phrases might stretch over dozens of words, so the extracted word may occur far away from the actual preposition. These words were chosen based on a manual analysis of training data. Using Tregex (Levy and Andrew, 2006), a utility for expressing “regular expressions over trees”, we created a set </context>
</contexts>
<marker>Alam, 2004</marker>
<rawString>Y.S. Alam. 2004. Decision Trees for Sense Disambiguation of Prepositions: Case of Over. In HLT-NAACL 2004: Workshop on Computational Lexical Semantics, pages 52–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>86--90</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="13730" citStr="Baker et al., 1998" startWordPosition="2250" endWordPosition="2253">nd FrameNet information on the head, yet the features they extract are generally not linguistically motivated. 5 Discussion Using the phrase structure allows for more freedom in the choice of words for feature selection, yet still guarantees to find words for which some syntactic relation with the preposition holds. Extracting semantic features from these words (hypernyms, synonyms, etc.) allows for a certain degree of abstraction, and thus a high level comparison. O’Hara and Wiebe (2003) also make use of high level features, in their case the Penn Treebank (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepositions. They show that using high level features– such as semantic roles–of words in the context substantially aids disambiguation efforts. They caution, however, that indiscriminately using collocations and neighboring words may yield high accuracy, but has the risk of overfitting. In order to mitigate this, they classify the features by their part of speech. While we made use of collocation features, we also took into account higher order aspects of the context, such as the governing phrase, part of speech type, and semantic class according to WordNet. All other things bei</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th international conference on Computational linguistics-Volume 1, pages 86–90. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="8208" citStr="Berger et al., 1996" startWordPosition="1332" endWordPosition="1335"> &apos;VBD&apos;)) prepositions, this resulted in several thousand features. In order to reduceecomputation time, we used the following steps: For each preposition classifier, � boolean indicator for capitalization werankd th features using information gain (ForThis resulted in several thousand featues for the prepositions We used information gain (Foreman, man, 2003). From the resulting lists,we included at most 4000 features. Thus not all classifiers used the 3) in ordr nd ghe kig ure f h l d liid lifi th 4000 f same features. t i 23 lassifie Training 2.3 Classifier Training We chose maximum entropy (Berger et al., 1996) as W ho xium trpy (Brge 1996) our our primary classifier, since it had been successfully imay lasifi ba th hight fi systems in both the SemEval2007 preposition applied by the highest performing systems in both sense disambiguation task (Ye and Baldwin 2007) the SemEval-2007 preposition sense disambiguation and the general word sense dismbiguatio tsk task (Ye and Baldwin, 2007) and the general word (Tratz et l., 2007) used it. We used the implemnsense disambiguation task (Tratz et al., 2007). We tation provided by he Mallet machine larning used the implementation provided by the Mallet matoolk</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In ACM International Conference Proceeding Series,</booktitle>
<volume>4</volume>
<pages>132--139</pages>
<contexts>
<context position="4733" citStr="Charniak, 2000" startWordPosition="742" endWordPosition="743">ne example of the respective preposition. What are your beliefs &lt;head&gt;about&lt;/head&gt; these emotions ? The preposition is annotated by a head tag, and the meaning of the preposition in question is given as defined by TPP. Each preposition had between 2 and 25 different senses (on average 9.76). For the case of “about” these would be 1. on the subject of; concerning 2. so as to affect 3. used to indicate movement within a particular area 4. around 5. used to express location in a particular place 6. used to describe a quality apparent in a person We parsed the sentences using the Charniak parser (Charniak, 2000). Note that the Charniak parser–even though among the best availbale English parsers–occasionally fails to parse a sentence correctly. This might result in an erroneous extraction, such as an incorrect or no word. However, these cases are fairly rare, and we did not manually correct this, but rather relied on the size of the data to compensate for such an error. After this preprocessing step, we were able to extract the features. 2.2 Feature Extraction Following O’Hara and Wiebe (2003) and Alam (2004), we assumed that there is a meaningful connection between syntactically related words on both</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In ACM International Conference Proceeding Series, volume 4, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press USA.</publisher>
<contexts>
<context position="1330" citStr="Fellbaum, 1998" startWordPosition="195" endWordPosition="196">classifiers, we can report an increased accuracy that outperforms the best system in the SemEval task. 1 Introduction Classifying instances of polysemous words into their proper sense classes (aka sense disambiguation) is potentially useful to any NLP application that needs to extract information from text or build a semantic representation of the textual information. However, to date, disambiguation between preposition senses has not been an object of great study. Instead, most word sense disambiguation work has focused upon classifying noun and verb instances into their appropriate WordNet (Fellbaum, 1998) senses. Prepositions have mostly been studied in the context of verb complements (Litkowski and Hargraves, 2007). Like instances of other word classes, many prepositions are ambiguous, carrying different semantic meanings (including notions of instrumental, accompaniment, location, etc.) as in “He ran with determination”, “He ran with a broken leg”, or “He ran with Jane”. As NLP systems take more and more 96 semantic content into account, disambiguating between preposition senses becomes increasingly important for text processing tasks. In order to disambiguate different senses, most systems </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1289</pages>
<marker>Forman, 2003</marker>
<rawString>G. Forman. 2003. An extensive empirical study of feature selection metrics for text classification. The Journal of Machine Learning Research, 3:1289–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>G Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="5761" citStr="Levy and Andrew, 2006" startWordPosition="911" endWordPosition="914">le to extract the features. 2.2 Feature Extraction Following O’Hara and Wiebe (2003) and Alam (2004), we assumed that there is a meaningful connection between syntactically related words on both sides of the preposition. We thus focused on specific words that are syntactically related to the preposition via the phrase structure. This has the advantage that it is not limited to a certain window size; phrases might stretch over dozens of words, so the extracted word may occur far away from the actual preposition. These words were chosen based on a manual analysis of training data. Using Tregex (Levy and Andrew, 2006), a utility for expressing “regular expressions over trees”, we created a set of rules to extract the words in question. Each rule matched words that exhibited a specific relationship with the preposition or were within a two word window to cover collocations. An example rule is given below. IN &gt; (PP &lt; (V P &lt; # = x&amp; &lt; #!AUX)) This particular rule finds the head (denoted by x) of a verb phrase that governs the prepositional phrase containing the preposition, unless x is an auxiliary verb. Tregex rules were used to identify the following words for feature generation: • the head verb/noun that im</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>R. Levy and G. Andrew. 2006. Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>SemEval2007 Task 06: Word-Sense Disambiguation of Prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1443" citStr="Litkowski and Hargraves, 2007" startWordPosition="210" endWordPosition="213">ask. 1 Introduction Classifying instances of polysemous words into their proper sense classes (aka sense disambiguation) is potentially useful to any NLP application that needs to extract information from text or build a semantic representation of the textual information. However, to date, disambiguation between preposition senses has not been an object of great study. Instead, most word sense disambiguation work has focused upon classifying noun and verb instances into their appropriate WordNet (Fellbaum, 1998) senses. Prepositions have mostly been studied in the context of verb complements (Litkowski and Hargraves, 2007). Like instances of other word classes, many prepositions are ambiguous, carrying different semantic meanings (including notions of instrumental, accompaniment, location, etc.) as in “He ran with determination”, “He ran with a broken leg”, or “He ran with Jane”. As NLP systems take more and more 96 semantic content into account, disambiguating between preposition senses becomes increasingly important for text processing tasks. In order to disambiguate different senses, most systems to date use a fixed window size to derive classification features. These may or may not be syntactically related </context>
</contexts>
<marker>Litkowski, Hargraves, 2007</marker>
<rawString>Ken Litkowski and Orin Hargraves. 2007. SemEval2007 Task 06: Word-Sense Disambiguation of Prepositions. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
</authors>
<title>The preposition project.</title>
<date>2005</date>
<note>http://www.clres.com/prepositions.html.</note>
<contexts>
<context position="2943" citStr="Litkowski (2005)" startWordPosition="453" endWordPosition="454">res. In 2007, the SemEval workshop presented participants with a formal preposition sense disambiguation task to encourage the development of systems for the disambiguation of preposition senses (Litkowski and Hargraves, 2007). The training and test data sets used for SemEval have been released to the general public, and we used these data to train and test our system. The SemEval workshop data consists of instances of 34 prepositions in natural text that have been tagged with the appropriate sense from the list of the common English preposition senses compiled by The Preposition Project, cf. Litkowski (2005). The SemEval data provides a natural method for comparing the performance of preposition sense disambiguation systems. In our paper, we follow the task requirements and can thus directly compare our results to the ones from the study. For evaluation, we compared our results to those of the three systems that participated in the task (MELB: Ye and Baldwin (2007); KU: Yuret (2007); IRST: Popescu et al. (2007)). We also used the “first sense” and the “most frequent sense” Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96–100, Boulder, Colorado, June 2009. c</context>
</contexts>
<marker>Litkowski, 2005</marker>
<rawString>Ken Litkowski. 2005. The preposition project. http://www.clres.com/prepositions.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn TreeBank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="13696" citStr="Marcus et al., 1993" startWordPosition="2244" endWordPosition="2247">ge. They used the Charniak parser and FrameNet information on the head, yet the features they extract are generally not linguistically motivated. 5 Discussion Using the phrase structure allows for more freedom in the choice of words for feature selection, yet still guarantees to find words for which some syntactic relation with the preposition holds. Extracting semantic features from these words (hypernyms, synonyms, etc.) allows for a certain degree of abstraction, and thus a high level comparison. O’Hara and Wiebe (2003) also make use of high level features, in their case the Penn Treebank (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepositions. They show that using high level features– such as semantic roles–of words in the context substantially aids disambiguation efforts. They caution, however, that indiscriminately using collocations and neighboring words may yield high accuracy, but has the risk of overfitting. In order to mitigate this, they classify the features by their part of speech. While we made use of collocation features, we also took into account higher order aspects of the context, such as the governing phrase, part of speech type, and semantic class accordin</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: the Penn TreeBank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<note>http://mallet. cs. umass. edu.</note>
<contexts>
<context position="8886" citStr="McCallum, 2002" startWordPosition="1444" endWordPosition="1445">e it had been successfully imay lasifi ba th hight fi systems in both the SemEval2007 preposition applied by the highest performing systems in both sense disambiguation task (Ye and Baldwin 2007) the SemEval-2007 preposition sense disambiguation and the general word sense dismbiguatio tsk task (Ye and Baldwin, 2007) and the general word (Tratz et l., 2007) used it. We used the implemnsense disambiguation task (Tratz et al., 2007). We tation provided by he Mallet machine larning used the implementation provided by the Mallet matoolki (McCalum, 2002). Then, fo the sae of chine learning toolkit (McCallum, 2002). For the sakeof comparison, we also built several other clasmpri, it val cssrs inclding multiial nïe By SVM kNN, and decsion trees (J48) using he WEKA toolkit sifiers, including multinomial na¨ıve Bayes, SVMs, (Witten 1999) We chose the radal bass fuction kNN, and decision trees (J48) using the WEKA (RBF) kernel fo the SVMs and left all other patoolkit (Witten, 1999). We chose the radial basis rameers at their defaut values. function (RBF) kernel for the SVMs and left all other parameters at their default values. 3 Results the test et provided by SemEval2007 and proWe measured the accuracy of</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A.K. McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. 2002. http://mallet. cs. umass. edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T O’Hara</author>
<author>J Wiebe</author>
</authors>
<title>Preposition semantic classification via Penn Treebank and FrameNet.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>79--86</pages>
<marker>O’Hara, Wiebe, 2003</marker>
<rawString>T. O’Hara and J. Wiebe. 2003. Preposition semantic classification via Penn Treebank and FrameNet. In Proceedings of CoNLL, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Octavian Popescu</author>
<author>Sara Tonelli</author>
<author>Emanuele Pianta</author>
</authors>
<title>IRST-BP: Preposition Disambiguation based on Chain Clarifying Relationships Contexts. In MELBYB: Preposition Sense Disambiguation Using Rich Semantic Features,</title>
<date>2007</date>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3354" citStr="Popescu et al. (2007)" startWordPosition="521" endWordPosition="524"> of instances of 34 prepositions in natural text that have been tagged with the appropriate sense from the list of the common English preposition senses compiled by The Preposition Project, cf. Litkowski (2005). The SemEval data provides a natural method for comparing the performance of preposition sense disambiguation systems. In our paper, we follow the task requirements and can thus directly compare our results to the ones from the study. For evaluation, we compared our results to those of the three systems that participated in the task (MELB: Ye and Baldwin (2007); KU: Yuret (2007); IRST: Popescu et al. (2007)). We also used the “first sense” and the “most frequent sense” Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96–100, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics baselines (see section 3 and table 1). These baselines are determined by the TPP listing and the frequency in the training data, respectively. Our system beat the baselines and outperformed the three participating systems. 2 Methodology 2.1 Data Preparation We downloaded the test and training data provided by the SemEval-2007 website for the preposition sense d</context>
<context position="12936" citStr="Popescu et al. (2007)" startWordPosition="2121" endWordPosition="2124">ubstantially larger word window, taking seven words from each side of the preposition. While they included many higher level features, they state that the direct lexical context (i.e., bag-of-words) features were the most effective and account for the majority of features, while syntactic and semantic features had relatively little impact. Yuret (2007) used a n-gram model based on word substitution by synonyms or antonyms. While this proved to be quite successful with content words, it had considerable problems with prepositions, since the number of synonyms and/or antonyms is fairly limited. Popescu et al. (2007) take an interesting approach which they call Chain Clarifying Relationship. They are using a supervised algorithm to learn a regular language. They used the Charniak parser and FrameNet information on the head, yet the features they extract are generally not linguistically motivated. 5 Discussion Using the phrase structure allows for more freedom in the choice of words for feature selection, yet still guarantees to find words for which some syntactic relation with the preposition holds. Extracting semantic features from these words (hypernyms, synonyms, etc.) allows for a certain degree of ab</context>
</contexts>
<marker>Popescu, Tonelli, Pianta, 2007</marker>
<rawString>Octavian Popescu, Sara Tonelli, and Emanuele Pianta. 2007. IRST-BP: Preposition Disambiguation based on Chain Clarifying Relationships Contexts. In MELBYB: Preposition Sense Disambiguation Using Rich Semantic Features, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tratz</author>
<author>A Sanfilippo</author>
<author>M Gregory</author>
<author>A Chappell</author>
<author>C Posse</author>
<author>P Whitney</author>
</authors>
<title>PNNL: A Supervised Maximum Entropy Approach to Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).</booktitle>
<contexts>
<context position="8704" citStr="Tratz et al., 2007" startWordPosition="1413" endWordPosition="1416">fi th 4000 f same features. t i 23 lassifie Training 2.3 Classifier Training We chose maximum entropy (Berger et al., 1996) as W ho xium trpy (Brge 1996) our our primary classifier, since it had been successfully imay lasifi ba th hight fi systems in both the SemEval2007 preposition applied by the highest performing systems in both sense disambiguation task (Ye and Baldwin 2007) the SemEval-2007 preposition sense disambiguation and the general word sense dismbiguatio tsk task (Ye and Baldwin, 2007) and the general word (Tratz et l., 2007) used it. We used the implemnsense disambiguation task (Tratz et al., 2007). We tation provided by he Mallet machine larning used the implementation provided by the Mallet matoolki (McCalum, 2002). Then, fo the sae of chine learning toolkit (McCallum, 2002). For the sakeof comparison, we also built several other clasmpri, it val cssrs inclding multiial nïe By SVM kNN, and decsion trees (J48) using he WEKA toolkit sifiers, including multinomial na¨ıve Bayes, SVMs, (Witten 1999) We chose the radal bass fuction kNN, and decision trees (J48) using the WEKA (RBF) kernel fo the SVMs and left all other patoolkit (Witten, 1999). We chose the radial basis rameers at their def</context>
</contexts>
<marker>Tratz, Sanfilippo, Gregory, Chappell, Posse, Whitney, 2007</marker>
<rawString>S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell, C. Posse, and P. Whitney. 2007. PNNL: A Supervised Maximum Entropy Approach to Word Sense Disambiguation. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
</authors>
<title>Weka: Practical Machine Learning Tools and Techniques with Java Implementations.</title>
<date>1999</date>
<institution>Dept. of Computer Science, University of Waikato, University of Waikato, Dept. of Computer Science.</institution>
<contexts>
<context position="9110" citStr="Witten 1999" startWordPosition="1480" endWordPosition="1481">nse disambiguation and the general word sense dismbiguatio tsk task (Ye and Baldwin, 2007) and the general word (Tratz et l., 2007) used it. We used the implemnsense disambiguation task (Tratz et al., 2007). We tation provided by he Mallet machine larning used the implementation provided by the Mallet matoolki (McCalum, 2002). Then, fo the sae of chine learning toolkit (McCallum, 2002). For the sakeof comparison, we also built several other clasmpri, it val cssrs inclding multiial nïe By SVM kNN, and decsion trees (J48) using he WEKA toolkit sifiers, including multinomial na¨ıve Bayes, SVMs, (Witten 1999) We chose the radal bass fuction kNN, and decision trees (J48) using the WEKA (RBF) kernel fo the SVMs and left all other patoolkit (Witten, 1999). We chose the radial basis rameers at their defaut values. function (RBF) kernel for the SVMs and left all other parameters at their default values. 3 Results the test et provided by SemEval2007 and proWe measured the accuracy of the classifiers over vided these results in Table 1. It is notable that our the test set provided by SemEval-2007 and provided system produced good results with all classifiers: these results in Table 1. It is notable that </context>
</contexts>
<marker>Witten, 1999</marker>
<rawString>I.H. Witten. 1999. Weka: Practical Machine Learning Tools and Techniques with Java Implementations. Dept. of Computer Science, University of Waikato, University of Waikato, Dept. of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3307" citStr="Ye and Baldwin (2007)" startWordPosition="513" endWordPosition="516"> our system. The SemEval workshop data consists of instances of 34 prepositions in natural text that have been tagged with the appropriate sense from the list of the common English preposition senses compiled by The Preposition Project, cf. Litkowski (2005). The SemEval data provides a natural method for comparing the performance of preposition sense disambiguation systems. In our paper, we follow the task requirements and can thus directly compare our results to the ones from the study. For evaluation, we compared our results to those of the three systems that participated in the task (MELB: Ye and Baldwin (2007); KU: Yuret (2007); IRST: Popescu et al. (2007)). We also used the “first sense” and the “most frequent sense” Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96–100, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics baselines (see section 3 and table 1). These baselines are determined by the TPP listing and the frequency in the training data, respectively. Our system beat the baselines and outperformed the three participating systems. 2 Methodology 2.1 Data Preparation We downloaded the test and training data provided by the S</context>
<context position="8466" citStr="Ye and Baldwin 2007" startWordPosition="1375" endWordPosition="1378">esulted in several thousand featues for the prepositions We used information gain (Foreman, man, 2003). From the resulting lists,we included at most 4000 features. Thus not all classifiers used the 3) in ordr nd ghe kig ure f h l d liid lifi th 4000 f same features. t i 23 lassifie Training 2.3 Classifier Training We chose maximum entropy (Berger et al., 1996) as W ho xium trpy (Brge 1996) our our primary classifier, since it had been successfully imay lasifi ba th hight fi systems in both the SemEval2007 preposition applied by the highest performing systems in both sense disambiguation task (Ye and Baldwin 2007) the SemEval-2007 preposition sense disambiguation and the general word sense dismbiguatio tsk task (Ye and Baldwin, 2007) and the general word (Tratz et l., 2007) used it. We used the implemnsense disambiguation task (Tratz et al., 2007). We tation provided by he Mallet machine larning used the implementation provided by the Mallet matoolki (McCalum, 2002). Then, fo the sae of chine learning toolkit (McCallum, 2002). For the sakeof comparison, we also built several other clasmpri, it val cssrs inclding multiial nïe By SVM kNN, and decsion trees (J48) using he WEKA toolkit sifiers, including m</context>
<context position="10692" citStr="Ye and Baldwin, 2007" startWordPosition="1753" endWordPosition="1756"> percent improveOverall, our system outperformed the winning ment. A simple proportion test shows this to be staym y 00, 8 pret provmen A impl pprtion test show thi t b ttitill sgnificant at 0001 tistically significant at 0.001. ystem S�stem Acc�rac� Accuracy *++ ,-,./0 N !67 89:; *#&lt;=#&gt;? ,-,.12 VM (RBF Ke) ,-,3%2 @0/ A#BCDCE= $&lt;##D ,-,34% 48 decision trees ,-,35% 7F&gt;$C=EGC&apos;&gt; +&apos;HI# :&apos;J#D ,-,.14 Multinomial Naïve Bay ,-,503 7&apos;KCGFG #=$&lt;ELJ ,-,01. ,-,41. 7MN: aximum Entropy 8O# &apos;=A :&apos;&gt;APC=Q 2,,3? RS 8OF&lt;#$Q 2,,3? ost Frequn Se T9!U 8&amp;EL#DBF #$ &apos;&gt;-Q 2,,3? RS (Popu t , 7 7ED$ V&lt;#WF#=$ D#=D# MELB (Ye and Baldwin, 2007 Table 1: Accuracy results on SemEval data (with 4000 693 Table 1 A features) nce cu o e ue Since our initial cutoff of 4000 features was arMi Et it multiple times with different cutoffs Accuracy bitrary, we reran our Maximum Entropy experiment consistenly increased as the feature limit was re multiple times with different cutoffs. Accuracy conaxed, resulting in 0.764 accuracy at the 10k feasistently increased as the feature limit was relaxed, resuting in 0.764 accurcy at the 10k feature limit. ure limit These results are displayed in Figure 1. These results are displayed in Figure 1. and accu</context>
<context position="12067" citStr="Ye and Baldwin (2007)" startWordPosition="1985" endWordPosition="1988">ation n computational linguiscs The linguistic literature on prepositions and their use iscopious an divrse. We restrict ourlves to the O&apos;Hara and Wiebe 2003) make use of Penn Treebank (Marcus t al, 1993) and FrameNet systems that competed in the SemEval 2007 Prepo(Bake et al. 1998) to classify prpositions. They sition Sense Disambiguation task. All three of the show that using high level features from the consystems within the framework of the SemEval task , mai rl, ignifily aid dis used supervised learning algorithms, yet they differed widely in the data collection and model preparation. 98 Ye and Baldwin (2007) participated in the SemEval task using a maximum entropy classifier and achieved the highest accuracy of the participating systems. The features they extracted were similar to the ones we used, including POS and WordNet features, but they used a substantially larger word window, taking seven words from each side of the preposition. While they included many higher level features, they state that the direct lexical context (i.e., bag-of-words) features were the most effective and account for the majority of features, while syntactic and semantic features had relatively little impact. Yuret (200</context>
</contexts>
<marker>Ye, Baldwin, 2007</marker>
<rawString>Patrick Ye and Timothy Baldwin. 2007. MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Ku: Word sense disambiguation by substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3325" citStr="Yuret (2007)" startWordPosition="518" endWordPosition="519">rkshop data consists of instances of 34 prepositions in natural text that have been tagged with the appropriate sense from the list of the common English preposition senses compiled by The Preposition Project, cf. Litkowski (2005). The SemEval data provides a natural method for comparing the performance of preposition sense disambiguation systems. In our paper, we follow the task requirements and can thus directly compare our results to the ones from the study. For evaluation, we compared our results to those of the three systems that participated in the task (MELB: Ye and Baldwin (2007); KU: Yuret (2007); IRST: Popescu et al. (2007)). We also used the “first sense” and the “most frequent sense” Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 96–100, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics baselines (see section 3 and table 1). These baselines are determined by the TPP listing and the frequency in the training data, respectively. Our system beat the baselines and outperformed the three participating systems. 2 Methodology 2.1 Data Preparation We downloaded the test and training data provided by the SemEval-2007 websit</context>
<context position="12669" citStr="Yuret (2007)" startWordPosition="2081" endWordPosition="2082">win (2007) participated in the SemEval task using a maximum entropy classifier and achieved the highest accuracy of the participating systems. The features they extracted were similar to the ones we used, including POS and WordNet features, but they used a substantially larger word window, taking seven words from each side of the preposition. While they included many higher level features, they state that the direct lexical context (i.e., bag-of-words) features were the most effective and account for the majority of features, while syntactic and semantic features had relatively little impact. Yuret (2007) used a n-gram model based on word substitution by synonyms or antonyms. While this proved to be quite successful with content words, it had considerable problems with prepositions, since the number of synonyms and/or antonyms is fairly limited. Popescu et al. (2007) take an interesting approach which they call Chain Clarifying Relationship. They are using a supervised algorithm to learn a regular language. They used the Charniak parser and FrameNet information on the head, yet the features they extract are generally not linguistically motivated. 5 Discussion Using the phrase structure allows </context>
</contexts>
<marker>Yuret, 2007</marker>
<rawString>Deniz Yuret. 2007. Ku: Word sense disambiguation by substitution. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>