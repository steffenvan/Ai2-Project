<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001167">
<title confidence="0.985648">
Learning the Information Status of Noun Phrases in Spoken Dialogues
</title>
<author confidence="0.972205">
Altaf Rahman and Vincent Ng
</author>
<affiliation confidence="0.9872875">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.917877">
Richardson, TX 75083-0688
</address>
<email confidence="0.999724">
{altaf,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999860761904762">
An entity in a dialogue may be old, new,
or mediated/inferrable with respect to the
hearer’s beliefs. Knowing the information
status of the entities participating in a dia-
logue can therefore facilitate its interpreta-
tion. We address the under-investigated prob-
lem of automatically determining the informa-
tion status of discourse entities. Specifically,
we extend Nissim’s (2006) machine learning
approach to information-status determination
with lexical and structured features, and ex-
ploit learned knowledge of the information
status of each discourse entity for coreference
resolution. Experimental results on a set of
Switchboard dialogues reveal that (1) incor-
porating our proposed features into Nissim’s
feature set enables our system to achieve state-
of-the-art performance on information-status
classification, and (2) the resulting informa-
tion can be used to improve the performance
of learning-based coreference resolvers.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999862130434782">
Information status is not a term unfamiliar to re-
searchers working on discourse processing prob-
lems. It describes the extent to which a discourse en-
tity, which is typically a noun phrase (NP), is avail-
able to the hearer given the speaker’s assumptions
about the hearer’s beliefs. According to Nissim et
al. (2004), a discourse entity can be new, old, or me-
diated. Informally, a discourse entity is (1) old to
the hearer if it is known to the hearer and has pre-
viously been referred to in the dialogue, (2) new if
it is unknown to her and has not been previously re-
ferred to; and (3) mediated if it is newly mentioned
in the dialogue but she can infer its identity from
a previously-mentioned entity. Information status
is a subject that has received a lot of attention in
theoretical linguistics (Halliday, 1976; Prince, 1981;
Hajiˇcov´a, 1984; Vallduvi, 1992; Steedman, 2000).
Knowing the information status of discourse enti-
ties can potentially benefit many NLP applications.
One such task is anaphora resolution. While there is
general belief that definite descriptions are mostly
anaphoric, Vieira and Poesio (2000) empirically
show that only 30% of these NPs are anaphoric.
Without being able to determine whether an NP is
anaphoric, an anaphora resolver will attempt to re-
solve every NP, potentially damaging its precision.
Since new entities are by definition new to the hearer
and therefore cannot refer to a previously-introduced
NP, knowledge of information status could be used
to improve anaphora resolution.
Despite the potential usefulness of information
status in NLP tasks, there has been little work on
learning the information status of discourse entities.
To investigate the plausibility of learning informa-
tion status, Nissim et al. (2004) annotate a set of
Switchboard dialogues with such information1, and
subsequently present a rule-based approach and a
learning-based approach to acquiring such knowl-
edge from the manual annotations (Nissim, 2006).
Our goals in this paper are two-fold. First, we
describe a learning approach to the under-studied
problem of determining the information status of
discourse entities that extends Nissim’s (2006) fea-
ture set with two novel types of features: lexical
features and structured features based on syntactic
parse trees. Second, we employ the automatically
</bodyText>
<footnote confidence="0.977648333333333">
1These and other linguistic annotations on the Switchboard
dialogues were later released by the LDC as part of the NXT
corpus, which is described in detail in Calhoun et al. (2010).
</footnote>
<page confidence="0.87489">
1069
</page>
<note confidence="0.958196">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1069–1080,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998384375">
acquired knowledge of information status for coref-
erence resolution. Experimental results on Nissim et
al.’s (2004) corpus of Switchboard dialogues show
that (1) adding our linguistic features to Nissim’s
feature set enables our system to outperform her sys-
tem by 8.1% in F-measure, and (2) learned knowl-
edge of information status can be used to improve
coreference resolvers by 1.1–2.6% in F-measure.
The rest of this paper is organized as follows. We
first illustrate with examples the concepts of new,
old, and mediated entities. Then, we describe the
dataset and the feature set that Nissim (2006) used
in her approach. After that, we introduce our lexi-
cal and structured features. Finally, we evaluate the
determination of information status as a standalone
task and in the context of coreference resolution.
</bodyText>
<sectionHeader confidence="0.925285" genericHeader="method">
2 Old, New, and Mediated Entities
</sectionHeader>
<bodyText confidence="0.999542333333334">
Since the concepts of old, new, and mediated entities
are not widely known to researchers working outside
the area of discourse processing, in this section we
will explain them in more detail.
The terms old and new information have meant
a variety of things over the years (Allerton, 1978;
Prince, 1981; Horn, 1986). Since we use Nissim
et al.’s (2004) corpus for training and evaluation,
the definitions of these concepts we present here are
those that Nissim et al. used to annotate their cor-
pus. According to Nissim et al., their definitions are
built upon Prince’s (1981), and the categorization
into old, new, and mediated entities resemble those
of Strube (1998) and Eckert and Strube (2001).
Old. As mentioned before, an entity is old if it is
both known to the hearer and has been mentioned in
the conversation. More precisely, an entity is old if
(1) it is coreferential with an entity introduced ear-
lier, (2) it is a generic pronoun, or (3) it is a personal
pronoun referring to the dialogue participants. To
exemplify, consider the following sentences.
</bodyText>
<listItem confidence="0.9976585">
(1) I was angry that he destroyed my tent.
(2) You cannot leave until the test is over.
</listItem>
<bodyText confidence="0.972609846153846">
In Example 1, my is an old entity because it is
coreferent with I. In Example 2, You is an old entity
because it is a generic pronoun.
Mediated. An entity is mediated if it has not been
previously introduced in the conversation, but can be
inferred from already-mentioned entities or is gener-
ally known to the hearer. More specifically, an entity
is mediated if (1) it is a generally known entity (e.g.,
the Earth, China, and most proper names), (2) it is
a bound pronoun, or (3) it is an instance of bridging
(i.e., an entity that is inferrable from a related entity
mentioned earlier in the dialogue). As an example,
consider the following sentences.
</bodyText>
<listItem confidence="0.96533575">
(3a) He passed by the door of Mary’s house and
saw that the door was painted purple.
(3b) He passed by Mary’s house and saw that
the door was painted purple.
</listItem>
<bodyText confidence="0.999928391304348">
In Example 3a, by the time the hearer processes
the second occurrence of the door, she has already
had a mental entity corresponding to the door (af-
ter processing the first occurrence). As a result, the
second occurrence of the door is an old entity. In
Example 3b, on the other hand, the hearer is not as-
sumed to have any mental representation of the door
in question, but she can infer that the door she saw
was part of Mary’s house. Hence, this occurrence of
the door is a mediated entity. In general, an entity
that is related to an earlier entity via a part-whole
relation or a set-subset relation is mediated.
New. An entity is new if it has not been introduced
in the dialogue and the hearer cannot infer it from
previously mentioned entities.
In case more than one class is appropriate for
a given entity, Nissim et al. employ additional tie-
breaking rules. Suppose, for instance, that we have
two occurrences of China in a dialogue. The second
occurrence can be labeled as old (because it is coref-
erential with an earlier entity) or mediated (because
it is a generally known entity). According to Nissim
et al.’s rules, the entity will be labeled as old.
</bodyText>
<sectionHeader confidence="0.998577" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.999954333333333">
We employ Nissim et al.’s (2004) dataset, which
comprises 147 Switchboard dialogues. A total of
68,992 NPs are annotated with information status:
51.2% of them are labeled as old, 34.5% as mediated
(henceforth med), and 14.3% as new. Nissim (2006)
randomly split the instances created from these NPs
into a training set (for classifier training), a develop-
ment set (for feature development), and an evalua-
tion set (for testing). Hence, the NPs from the same
</bodyText>
<page confidence="0.983411">
1070
</page>
<table confidence="0.996690769230769">
Feature Values
full prev mention numeric
mention time {first,second,more}
partial prev mention {yes,no,NA}
determiner {bare,def,dem,indef,poss,NA}
NP type {pronoun,common,proper,other}
NP length numeric
grammatical role {subject,subjpass,pp,other}
Training Test
old 31358 (51.7%) 3931 (47.4%)
med 20778 (34.2%) 3036 (36.6%)
new 8567 (14.1%) 1322 (16.0%)
total 60703 (100%) 8289 (100%)
</table>
<tableCaption confidence="0.998432">
Table 1: Information status distribution of NPs.
</tableCaption>
<bodyText confidence="0.9809402">
document may be split across different sets.
Unlike Nissim (2006), we partition the 147 dia-
logues (rather than the instances) into a training set
(117 dialogues) and a test set (30 dialogues). In
other words, we do not randomize the instances, as
we believe that it represents an unrealistic evalua-
tion setting, for the following reasons. First, in prac-
tice, the test dialogues may not be available until test
time. Second, we may want to examine how a sys-
tem performs on a given dialogue. Finally, random-
izing the instances does not allow us to apply learned
knowledge of information status to coreference res-
olution, which needs to be performed for each dia-
logue. The information status distribution of the NPs
in the training and test sets are shown in Table 1.
</bodyText>
<sectionHeader confidence="0.986935" genericHeader="method">
4 Baseline System
</sectionHeader>
<bodyText confidence="0.996538058823529">
In this section, we describe our baseline system,
which adopts a machine learning approach to deter-
mining the information status of a discourse entity.
Building SVM classifiers for information-status
determination. We employ the support vector
machine (SVM) learner as implemented in the
SVMlight package (Joachims, 1999) to train three
binary classifiers, one for predicting each of the
three possible classes (i.e., new, old, and med), us-
ing a linear kernel in combination with the one-
versus-all training scheme.2 Each training instance
represents a single NP and consists of the seven
morpho-syntactic features that Nissim (2006) used
in her learning-based approach (see Table 2 for an
overview). Following Nissim, we extract the NPs
directly from the gold-standard annotations, but the
features are computed entirely automatically.
</bodyText>
<footnote confidence="0.9014532">
2SVM was chosen because it provides the option to employ
kernels. The reason why we train three binary classifiers rather
than just one multi-class classifier (using SVMmulticlass) is that
SVMmulticlass does not permit the use of a non-linear kernel,
which we will need to incorporate structured features later on.
</footnote>
<tableCaption confidence="0.991246">
Table 2: Nissim’s feature set.
</tableCaption>
<bodyText confidence="0.999864714285714">
The seven features are all intuitively useful for
determining information status. For instance, if an
NP, NPk, and a discourse entity that appears before
it have the same string (full prev mention), then NPk
is likely to be an old entity. Mention time is the cat-
egorical version of full prev mention and therefore
serves to detect old entities. Partial prev mention
is useful for detecting mediated entities, especially
those that have a set-subset relation with a preceding
entity. For instance, your dogs would be considered
a partial previous mention of my dogs or my three
dogs. The value “NA” stands for “not applicable”,
and is used for pronouns. Determiners and NP type
are likely to be helpful for all three categories. For
instance, indefinite NPs and pronouns are likely to
be new and old, respectively. The “NP length” fea-
ture is motivated by the observation that old entities
tend to contain less lexical materials than new enti-
ties. For instance, subsequent references to Barack
Obama may simply be Obama.
Applying the classifiers. To determine the infor-
mation status of an NP in a test dialogue, we create
an instance for it as during training and present it
independently to the three binary SVM classifiers,
each of which returns a real value representing the
signed distance of the instance from the hyperplane.
We assign the instance to the class that is associated
with the most positive classification value.
</bodyText>
<sectionHeader confidence="0.991132" genericHeader="method">
5 Our Features
</sectionHeader>
<bodyText confidence="0.9999905">
We propose to extend Nissim’s (2006) feature set
with two types of features.
</bodyText>
<subsectionHeader confidence="0.991756">
5.1 Lexical Features
</subsectionHeader>
<bodyText confidence="0.9999705">
As discussed, an entity should be labeled as med if it
has not been introduced in the dialogue but is gener-
</bodyText>
<page confidence="0.97387">
1071
</page>
<bodyText confidence="0.99988814">
ally known to a human. Whether an entity is “gener-
ally known” may be easily determined by a human
but not by a machine, since world knowledge is in-
volved in the decision process. In particular, Nis-
sim’s feature set does not contain any features that
encode the notion of a “generally known” entity.
Hence, it would be desirable to augment Nissim’s
feature set with features that indicate whether an en-
tity is generally known or not. One way to do this is
to (1) create a list of generally known entities, and
then (2) create a binary feature that has the value
True if and only if the entity under consideration ap-
pears in this list. The question, then, is: how can
we obtain the list of generally known entities? We
may manually assemble this list, but this could be
a labor-intensive task. As a result, we propose to
acquire this kind of world knowledge automatically
from annotated data.
Specifically, we augment Nissim’s feature set
with the set of unigrams that appear in the training
data. Given a training/test instance (i.e., discourse
entity), we compute the values of its unigram fea-
tures (henceforth lexical features) as follows. For
each unigram, we check if it appears in the string
representing the discourse entity. If so, its feature
value is 1; otherwise, its value is 0. For instance, if
the entity is the red hat, then all of its lexical features
except the, red, and hat will have a value of 0.
It should perhaps not be too difficult to see why
these lexical features are useful for the information-
status classifier: these features enable the SVM
learner to determine the extent to which a unigram
correlates with each class. For instance, from the an-
notated data, the learner will learn that any instance
of China cannot be labeled as new, and the deci-
sion of whether it should be an old entity or a med
entity depends on whether it is coreferential with a
previously-mentioned entity. Hence, the use of lex-
ical features allows the learner to implicitly acquire
some world knowledge.
We believe that lexicalization is an important step
towards building high-performance text-processing
systems. In fact, lexicalized models have demon-
strated their effectiveness in other areas of language
processing, such as syntactic and semantic parsing.
While lexicalized models may be less portable to
new genres and domains than their unlexicalized
counterparts, we believe that this issue can be han-
dled via domain adaptation techniques and should
not be a reason against lexicalization.
</bodyText>
<subsectionHeader confidence="0.998815">
5.2 Structured Features
</subsectionHeader>
<bodyText confidence="0.999976431818182">
In Nissim’s (2006) feature set, there are a couple of
features that capture NP-internal information, such
as determiner, NP length, and NP type. However,
there is only one feature that captures the syntactic
context of an NP, grammatical role, which is com-
puted based on the parse tree in which the NP re-
sides. This is arguably a very shallow representation
of its syntactic context. We hypothesize that we can
train more accurate information-status classifiers if
we have access to a richer representation of syntac-
tic context. This motivates us to employ syntactic
parse trees directly as features.
Before describing how this can be done, recall
that in a traditional learning setting, the feature set
employed by an off-the-shelf learning algorithm typ-
ically consists of flat features (i.e., features whose
values are discrete- or real-valued, as the ones de-
scribed in the previous section). Advanced machine
learning algorithms such as SVMs, on the other
hand, have enabled the use of structured features
(i.e., features whose values are structures such as
parse trees), owing to their ability to employ ker-
nels to efficiently compute the similarity between
two potentially complex structures.
Perhaps the main advantage of employing struc-
tured features is simplicity. To understand this ad-
vantage, consider learning in a setting where we can
only employ flat features. If we want to use informa-
tion from a parse tree as features in this setting, we
will need to design heuristics to extract the desired
parse-based features from parse trees. For certain
tasks, designing a good set of heuristics can be time-
consuming and sometimes difficult. On the other
hand, SVMs enable a parse tree to be employed di-
rectly as a structured feature, obviating the need to
design such heuristics.
Given two parse trees (as features), we com-
pute their similarity using a convolution tree ker-
nel (Collins and Duffy, 2001), which efficiently enu-
merates the number of common substructures in the
two trees via dynamic programming. Note, however,
that while we want to use a parse tree directly as a
feature, we do not want to use the entire parse tree as
a feature. Specifically, while using the entire parse
</bodyText>
<page confidence="0.840753">
1072
</page>
<bodyText confidence="0.777211319148936">
tree enables a richer representation of the syntactic we need to employ a composite kernel to combine
context than using a partial parse tree, the increased them. Specifically, we define and employ the fol-
complexity of the tree also makes it more difficult lowing composite kernel:
for the SVM learner to make generalizations. K,(F1,F2) = K1(F1,F2) + K2(F1,F2),
To strike a better balance between having a rich where F1 and F2 are the full set of features that rep-
representation of the context and improving the resent the two entities under consideration, and K1
learner’s ability to generalize, we extract a substruc- and K2 are the kernels we are combining. To ensure
ture from a parse tree and use it as the value of the that both kernels contribute equally to the compos-
structured feature of an instance. Specifically, given ite kernel, we normalize the values they return to the
an instance corresponding to discourse entity e, we range [0,1].
extract the substructure from the parse tree contain- 6 Evaluation
ing e as follows. Let n(e) be the root of the sub- Next, we evaluate the effectiveness of our features
tree that spans all and only the words in e, and let in improving information-status classification.
Parent(n(e)) be its immediate parent node. We (1) 6.1 Results and Discussion
take the subtree rooted at Parent(n(e)), (2) replace Results of four information-status classification sys-
each leaf node in this subtree with a node labeled tems are shown in Table 3. Under Original Nis-
X, (3) replace the subtree rooted at n(e) with a leaf sim, we have the results copied verbatim from Nis-
node labeled Y, and (4) use the subtree rooted at sim’s (2006) paper. Baseline is the aforementioned
Parent(n(e)) as the structured feature for the in- baseline system, which is trained using Nissim’s fea-
stance corresponding to e. Intuitively, the first three ture set. Baseline+Lexical is the system trained us-
steps aim to provide generalizations by simplifying ing Nissim’s feature set augmented with lexical fea-
the tree. For instance, step (1) allows us to focus on tures. Finally, Baseline+Both is the system trained
using a small window as the context. Steps (2) and using Nissim’s feature set augmented with both lex-
(3) help generalization by ignoring the words within ical and structured features. For each system, we
e and its context. Note that using two labels, X and show the recall (R), precision (P), and F-measure (F)
Y, enables the kernel to distinguish the discourse en- of each of the three classes: old, new, and med. Be-
tity under consideration from its context within this fore we describe the results, two points deserve men-
substructure. In addition, we simply use a single tion. First, as noted earlier, Nissim partitioned the
node (Y) to represent the discourse entity, since any dialogues into training and test folds in a different
NP-internal information has presumably been cap- way than us. In particular, Original Nissim and the
tured by the flat features. We compute these struc- remaining three systems were not evaluated on the
tured features using hand-annotated parse trees. same set of test instances. Hence, the Original Nis-
While structured features have been employed for sim results are not directly comparable with those of
a multitude of tasks in syntax, semantics, and in- the other systems. We show them here just to pro-
formation extraction such as syntactic parsing (e.g., vide another point of reference. Second, the results
Collins (2002)), semantic parsing (e.g., Moschitti of the remaining three systems were obtained by ag-
(2004)), named entity recognition (e.g., Cumby and gregating the results of three binary SVM classifiers,
Roth (2003), and relation extraction (e.g., Zelenko as described earlier.
et al. (2003)), the same is not true for discourse Comparing Baseline and Baseline+Lexical, we
processing tasks. We hope that our use of struc- see that augmenting Nissim’s feature set with lexical
tured features for information-status classification features improves the F-measure scores on all three
can promote their use in discourse processing. classes. In particular, the F-measure and recall for
5.3 Combining Kernels med rise considerably by 3.0 and 7.8, respectively.
Recall that the flat features are computed using a This provides indirect empirical support for our ear-
linear kernel, while the structured features are com- lier hypothesis that the med class can benefit from
puted using a tree kernel. If we want our learner to
make use of more than one of these types of features,
</bodyText>
<table confidence="0.996871857142857">
1073
Original Nissim R Baseline F Baseline+Lexical Baseline+Both
R P F P R P F R P F
old 91.5 94.1 92.8 91.2 85.8 88.5 88.7 91.7 90.2 93.0 95.2 94.1
med 87.6 68.1 76.6 84.7 62.7 72.1 92.5 63.2 75.1 89.1 70.9 79.0
new 22.3 56.3 32.0 30.2 66.4 41.5 32.1 68.3 43.7 34.4 71.5 46.5
Accuracy 79.5 74.1 76.3 82.2
</table>
<tableCaption confidence="0.999945">
Table 3: Per-class performance of four information-status classifiers.
</tableCaption>
<bodyText confidence="0.999493263157895">
the shallow world knowledge that these lexical fea-
tures help to “extract” from annotated data.
Comparing Baseline+Lexical and Baseline+Both,
we see that the addition of structured features en-
ables a further boost to performance: F-measure in-
creases by 2.8–3.9 for the three classes. These re-
sults substantiate our hypothesis that employing a
richer representation of syntactic context is benefi-
cial to information-status classification.
Comparing Baseline and Baseline+Both, we see
that F-measure improves considerably by 5–6.9 for
the three classes. Overall, these results provide sug-
gestive evidence that both types of features are ef-
fective at improving an information-status classifier
that employs Nissim’s features.
For further comparison, we show the classifica-
tion accuracies of the four systems in the last row
of Table 3. As we can see, adding lexical features
to the baseline features improves accuracy by 2.2%,
and adding structured features further improves ac-
curacy by 5.9%. Our two types of features, when
used in combination with Nissim’s features, improve
the baseline substantially by an accuracy of 8.1%.
Note that while our results and Original Nissim’s
are not directly comparable, the two systems are
consistent in terms of the relative performance for
the three classes: best for old and worst for new. The
poor performance for new is largely a consequence
of its low recall, which can in turn be attributed to its
lower representation in the dataset. Since many new
instances are misclassified, a natural question is: are
these instances misclassified as old or med? Simi-
lar questions can be raised for old and med, despite
their substantially higher recall values than new.
To answer these questions, we need to better
understand the kind of errors made by our ap-
proach. Consequently, we show in Table 4 the con-
fusion matrix generated from the test set for our
</bodyText>
<table confidence="0.9996252">
C→ old med new
G ↓
old 3656 257 18
med 167 2706 163
new 17 850 455
</table>
<tableCaption confidence="0.9905885">
Table 4: Confusion matrix for the Baseline+Both
classifier. C=Classifier tag; G=Gold tag
</tableCaption>
<bodyText confidence="0.999365590909091">
best-performing information-status classifier, Base-
line+Both. The rows and the columns correspond
to the gold tags and the classifier tags, respectively.
As we can see, these numbers seem to suggest the
“in-between” nature of mediated entities: when an
old or new entity is misclassified, it is typically mis-
classified as med (rows 1 and 3); however, when a
med entity is misclassified, it is equally likely to be
misclassified as old and new (row 2).
These results are perhaps not surprisingly, since
intuitively med entities bear some resemblance to
both old and new entities. For instance, the simi-
larity between med and old stems from the fact that
different instances of the same entity (e.g., China)
can receive one of these two labels, with the deci-
sion dependent on whether the entity was previously
mentioned in the dialogue. On the other hand, med
and new are similar in that it may sometimes be dif-
ficult even for a human to determine whether certain
entities should be labeled as med or new, since the
decision depends on whether she believes these en-
tities are generally known or not.
</bodyText>
<subsectionHeader confidence="0.996881">
6.2 Relation to Anaphoricity Determination
</subsectionHeader>
<bodyText confidence="0.9999424">
Anaphoricity determination refers to the task of de-
termining whether an NP is anaphoric or not, where
an NP is considered anaphoric if it is part of a (non-
singleton) coreference chain but is not the head of
the chain (Ng and Cardie, 2002). In other words, an
</bodyText>
<page confidence="0.984287">
1074
</page>
<table confidence="0.999889666666667">
Anaphoricity Baseline+Ana Baseline+Lexical+Ana Baseline+Both+Ana
R P F R P F R P F R P F
old 91.4 86.6 88.9 91.3 87.3 89.3 90.8 91.7 91.3 92.8 94.9 93.9
med 84.3 63.1 72.2 84.9 64.1 73.1 92.3 64.7 76.1 88.7 71.1 78.9
new 30.8 66.4 42.1 31.1 66.9 42.5 32.9 68.7 44.5 34.1 71.7 46.2
Accuracy 74.7 75.1 77.6 82.0
</table>
<tableCaption confidence="0.999523">
Table 5: Impact of knowledge of anaphoricity on the information-status classifiers.
</tableCaption>
<bodyText confidence="0.992189771428572">
NP is anaphoric if and only if it has an antecedent.
Given this definition, anaphoricity determination
bears resemblance to information-status classifica-
tion. For instance, an old entity is anaphoric, since it
has been introduced earlier in the conversation and
therefore have an antecedent. Similarly, a new or
med entity is non-anaphoric, since the entity has not
been previously introduced in the conversation and
therefore cannot have an antecedent.
There has been a lot of recent work on anaphoric-
ity determination (e.g., Bean and Riloff (1999),
Uryupina (2003), Ng (2004), Denis and Baldridge
(2007), Versley et al. (2008), Ng (2009), Zhou and
Kong (2009)). Given the similarity between this task
and information-status classification, a natural ques-
tion is: will the anaphoricity features previously de-
veloped by coreference researchers be helpful for
information-status classification? To answer this
question, we (1) assemble a feature set composed
of the 26 anaphoricity features previously used by
Rahman and Ng (2009),3 and then (2) repeat the ex-
periments in Table 3, except that we augment the
feature set used in each of these experiments with
the anaphoricity features we assembled in step (1).
Results with the anaphoricity features are shown
in Table 5. Under Anaphoricity, we have the results
obtained using only the 29 anaphoricity features. As
we can see, these results are comparable to those
obtained using the Baseline features. Comparing
each of Baseline+Ana and Baseline+Lexical+Ana
with the corresponding experiments in Table 3, we
see that the addition of anaphoricity features yields
a mild performance improvement, which is consis-
tent over all three classes. However, comparing the
last column of the two tables, we can see that in the
</bodyText>
<footnote confidence="0.951767">
3These 26 features are derived from those employed by Ng
and Cardie’s (2002) anaphoricity determination system. See
Footnote 2 of Rahman and Ng (2009) for details.
</footnote>
<bodyText confidence="0.9939566">
presence of the structured features, the anaphoricity
features do not contribute positively to overall per-
formance. Hence, in the coreference experiments in
the next section, we will not employ anaphoricity
features for information-status classification.
</bodyText>
<sectionHeader confidence="0.973006" genericHeader="method">
7 Application to Coreference Resolution
</sectionHeader>
<bodyText confidence="0.9999832">
Since the significance of information-status classi-
fication stems in part from the potential benefits it
brings to higher-level NLP applications, we deter-
mine whether our information-status classification
systems can offer benefits to learning-based coref-
erence resolution. Since the 147 information-status
annotated dialogues are also coreference annotated,
we use them in our coreference evaluation. To our
knowledge, our work represents the first attempt to
report coreference results on this dataset.
</bodyText>
<subsectionHeader confidence="0.984503">
7.1 Coreference Models
</subsectionHeader>
<bodyText confidence="0.99999">
While the so-called mention-pair coreference model
has dominated coreference research for more than
a decade since its appearance in the mid-1990s, a
number of new coreference models have been pro-
posed in recent years. To investigate whether these
newer, presumably more sophisticated, coreference
models can better exploit the automatically acquired
information-status information, we will evaluate the
usefulness of information-status information when
used in combination with two different coreference
models, the aforementioned mention-pair model and
the recently-developed cluster-ranking model.
</bodyText>
<sectionHeader confidence="0.798036" genericHeader="method">
7.1.1 Mention-Pair Model
</sectionHeader>
<bodyText confidence="0.9999675">
The mention-pair (MP) model, proposed by Aone
and Bennett (1995) and McCarthy and Lehnert
(1995), is a classifier that determines whether two
NPs are co-referring or not. Each instance i(NPj,
NP,,) corresponds to two NPs, NPj and NP,,, and is
represented by 39 features. Table 1 of Rahman and
</bodyText>
<page confidence="0.988588">
1075
</page>
<bodyText confidence="0.999987459459459">
Ng (2009) contains a detailed description of these
features. Linguistically, they can be divided into
four categories: string-matching, grammatical, se-
mantic, and positional. They can also be categorized
based on whether they are relational or not. Specifi-
cally, relational features capture the relationship be-
tween NPj and NPk, whereas non-relational features
capture the linguistic property of one of these NPs.
We follow Soon et al.’s (2001) method for cre-
ating training instances. Specifically, we create (1)
a positive instance for each anaphoric NP NPk and
its closest antecedent NPj; and (2) a negative in-
stance for NPk paired with each of the intervening
NPs, NPj+1, NPj+2, ..., NPk−1. The classification
associated with a training instance is either positive
or negative, depending on whether the two NPs are
coreferent. To train the MP model, we use the SVM
learner from SVMlight (Joachims, 1999).4
After training, the classifier is used to identify an
antecedent for an NP in a test text. Specifically,
each NP, NPk, is compared in turn to each preced-
ing NP, NPj, from right to left, and selects NPj as its
antecedent if the pair is classified as coreferent. The
process terminates as soon as an antecedent is found
for NPk or the beginning of the text is reached.
Despite its popularity, the MP model has two
major weaknesses. First, since each candidate an-
tecedent for an NP to be resolved (henceforth an ac-
tive NP) is considered independently of the others,
this model only determines how good a candidate
antecedent is relative to the active NP, but not how
good a candidate antecedent is relative to other can-
didates. So, it fails to answer the critical question of
which candidate antecedent is most probable. Sec-
ond, it has limitations in its expressiveness: the in-
formation extracted from the two NPs alone may not
be sufficient for making a coreference decision.
</bodyText>
<subsectionHeader confidence="0.576327">
7.1.2 Cluster-Ranking Model
</subsectionHeader>
<bodyText confidence="0.999628571428571">
The cluster-ranking (CR) model, proposed by
Rahman and Ng (2009), addresses the two weak-
nesses of the MP model by combining the strengths
of the entity-mention model (e.g., Luo et al. (2004),
Yang et al. (2008)) and the mention-ranking model
(e.g., Denis and Baldridge (2008)). Specifically,
the CR model ranks the preceding clusters for an
</bodyText>
<footnote confidence="0.701512">
4For this and subsequent uses of the SVM learner in our
experiments, we set all parameters to their default values.
</footnote>
<bodyText confidence="0.999412645833333">
active NP so that the highest-ranked cluster is the
one to which the active NP should be linked. Em-
ploying a ranker addresses the first weakness, as
a ranker allows all candidates to be compared si-
multaneously. Considering preceding clusters rather
than antecedents as candidates addresses the second
weakness, as cluster-level features (i.e., features that
are defined over any subset of NPs in a preceding
cluster) can be employed.
Since the CR model ranks preceding clusters, a
training instance i(cj, NPk) represents a preceding
cluster cj and an anaphoric NP NPk. Each instance
consists of features that are computed based solely
on NPk as well as cluster-level features, which de-
scribe the relationship between cj and NPk. Mo-
tivated in part by Culotta et al. (2007), we create
cluster-level features from the relational features in
our feature set using four predicates: NONE, MOST-
FALSE, MOST-TRUE, and ALL. Specifically, for each
relational feature X, we first convert X into an equiv-
alent set of binary-valued features if it is multi-
valued. Then, for each resulting binary-valued fea-
ture Xb, we create four binary-valued cluster-level
features: (1) NONE-Xb is true when Xb is false be-
tween NPk and each NP in cj; (2) MOST-FALSE-Xb
is true when Xb is true between NPk and less than half
(but at least one) of the NPs in cj; (3) MOST-TRUE-
Xb is true when Xb is true between NPk and at least
half (but not all) of the NPs in cj; and (4) ALL-Xb is
true when Xb is true between NPk and each NP in cj.
We train a cluster ranker to jointly learn
anaphoricity determination and coreference reso-
lution using SVMlight’s ranker-learning algorithm.
Specifically, for each NP, NPk, we create a train-
ing instance between NPk and each preceding clus-
ter cj using the features described above. Since we
are learning a joint model, we need to provide the
ranker with the option to start a new cluster by creat-
ing an additional training instance that contains fea-
tures that solely describes NPk. The rank value of
a training instance i(cj, NPk) created for NPk is the
rank of cj among the competing clusters. If NPk is
anaphoric, its rank is HIGH if NPk belongs to cj, and
LOW otherwise. If NPk is non-anaphoric, its rank is
LOW unless it is the additional training instance de-
scribed above, which has rank HIGH.
After training, the cluster ranker processes the
NPs in a test text in a left-to-right manner. For each
</bodyText>
<page confidence="0.990993">
1076
</page>
<bodyText confidence="0.999891272727273">
active NP, NPk, we create test instances for it by pair-
ing it with each of its preceding clusters. To allow
for the possibility that NPk is non-anaphoric, we cre-
ate an additional test instance that contains features
that solely describe the active NP (similar to what
we did in the training step above). All these test in-
stances are then presented to the ranker. If the addi-
tional test instance is assigned the highest rank value
by the ranker, then NPk is classified as non-anaphoric
and will not be resolved. Otherwise, NPk is linked to
the cluster that has the highest rank.
</bodyText>
<subsectionHeader confidence="0.994078">
7.2 Coreference Experiments
7.2.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999995444444444">
The training/test split we use in the coreference
experiments is the same as that in the information-
status experiments. Specifically, we use the train-
ing set to train both the information-status classifier
and our coreference models, apply the information-
status classifier to each discourse entity in the test
set, and have the coreference models resolve all
and only those NPs that are labeled as old by the
information-status classifier. Our decision to allow
the coreference models to resolve only the old enti-
ties is motivated by the fact that med and new entities
have not been previously introduced in the conversa-
tion and therefore do not have antecedents. The NPs
used by the coreference models are the same as those
accessible to the information-status classifier.
We employ two scoring programs, B3 (Bagga and
Baldwin, 1998) and 03-CEAF (Luo, 2005), to score
the output of a coreference model. Given a gold-
standard (i.e., key) partition, KP, and a system-
generated (i.e., response) partition, RP, B3 com-
putes the recall and precision of each NP and av-
erages these values at the end. Specifically, for each
NP, NPj, B3 first computes the number of NPs that
appear in both KPj and RPj, the clusters containing
NPj in KP and RP, respectively, and then divides
this number by |KPj |and |RPj |to obtain the re-
call and precision of NPj, respectively. On the other
hand, CEAF finds the best one-to-one alignment
between the key clusters and the response clusters
using the Kuhn-Munkres algorithm (Kuhn, 1955),
where the weight of an edge connecting two clusters
is equal to the number of NPs that appear in both
clusters. Precision and recall are equal to the sum of
the weights of the edges in the alignment divided by
the total number of NPs in the response and the key,
respectively.
</bodyText>
<subsubsectionHeader confidence="0.500546">
7.2.2 Results and Discussion
</subsubsectionHeader>
<bodyText confidence="0.999989925">
As our baseline, we employ our coreference mod-
els to generate NP partitions on the test documents
without using any knowledge of information status.
Results, reported in terms of recall (R), precision
(P), and F-measure (F) using B3 and 03-CEAF, are
shown in row 1 of Table 6.5 As we can see, the
baseline achieves B3 F-measures of 69.2 (MP) and
74.5 (CR) and CEAF F-measures of 61.6 (MP) and
68.5 (CR). These results suggest that the CR model
is stronger than the MP model, corroborating previ-
ous empirical findings (Rahman and Ng, 2009).
Next, we examine the impact of learned knowl-
edge of information status on the performance of a
coreference model. Since knowledge of information
status enables a coreference model to focus on re-
solving only the old entities, we hypothesize that the
resulting model will have a higher precision than one
that does not employ such knowledge. An equally
important question is: will the F-measure of the re-
sulting model improve? Since we are employing
knowledge of information status in a pipeline coref-
erence architecture where information-status classi-
fication is performed prior to coreference resolution,
errors made by the (upstream) information-status
classifier may propagate to the (downstream) coref-
erence system. Given this observation, we hypoth-
esize that the answer to the aforementioned ques-
tion depends in part on the accuracy of information-
status classification. In particular, the higher the
accuracy of information-status classification is, the
more likely the F-measure of the downstream coref-
erence model will improve. To test this hypothe-
sis, we conduct experiments where we employ the
knowledge provided by the three information-status
classifiers which, as discussed earlier, perform at
varying levels of accuracy — the first one using only
Nissim’s features, the second one using both lexical
and Nissim’s features, and the last one using Nis-
sim’s features in combination with lexical and parse-
based features — for our coreference models.
</bodyText>
<footnote confidence="0.997021">
5Since gold-standard NPs are used in our experiments,
CEAF precision is always equal to CEAF recall. For brevity,
we only report F-measure scores for CEAF in the table.
</footnote>
<page confidence="0.95451">
1077
</page>
<table confidence="0.999902125">
Mention-Pair Model CEAF Cluster-Ranking Model CEAF
B3 B3
System R P F F R P F F
No knowledge of information status 78.6 61.8 69.2 61.6 78.2 71.1 74.5 68.5
Nissim features only 73.4 67.3 70.2 62.1 73.6 77.4 75.4 69.7
Nissim+Lexical features 71.0 69.5 70.2 61.9 73.7 77.3 75.4 69.9
Nissim+Lexical+Parse features 74.1 66.8 70.3 62.3 77.3 74.0 75.6 71.1
Perfect information status 76.7 68.1 72.1 66.4 77.1 79.5 78.3 74.2
</table>
<tableCaption confidence="0.998398">
Table 6: B3 and CEAF coreference results.
</tableCaption>
<bodyText confidence="0.99978512962963">
Results of the coreference models employing
knowledge provided by the three information-status
classifiers are shown in rows 2–4 of Table 6. As ex-
pected, B3 precision increases in comparison to the
baseline, regardless of the coreference model and the
scoring program. In addition, employing knowledge
of information status always improves coreference
performance: F-measure scores increase by 1.0–
1.1% (B3) and 0.3–0.7% (CEAF) for the MP model,
and by 0.9–1.1% (B3) and 1.2–2.6% (CEAF) for
the CR model. These results suggest that the three
information-status classifiers have achieved the level
of accuracy needed for the coreference models to
improve. On the other hand, it is somewhat surpris-
ing that the three information-status classifiers have
yielded coreference systems that perform at essen-
tially the same level of performance.
To understand why better information-status clas-
sification results do not necessarily yield better
coreference performance, we take a closer look at
the results of the coreference resolver employing
Nissim’s features (henceforth NISSIM) and the re-
solver employing our Nissim+Lexical+Parse fea-
tures (henceforth FULL-FEATURE). Among the old
entities that were correctly classified using our fea-
tures and incorrectly classified by Nissim’s features,
we found that the precision of the FULL-FEATURE
system suffered (since in many cases the corefer-
ence models identified wrong antecedents for these
old entities) whereas the NISSIM system remained
unaffected (since the entities were misclassified and
would not be resolved by the models). In addition,
although many med and new entities were correctly
classified using our features and incorrectly classi-
fied (as old) using Nissim’s features, we found that
in many cases no antecedents were identified for
these misclassified entities and hence the precision
of the NISSIM system was not adversely affected.
Finally, we investigate whether our coreference
system could be improved if it had access to per-
fect knowledge of information status (taken directly
from the gold-standard annotations). This experi-
ment will allow us to determine whether the useful-
ness of knowledge of information status for coref-
erence resolution is limited by the accuracy in com-
puting such knowledge. Results are shown in the
last row of Table 6. As we can see, using per-
fect information-status knowledge yields a corefer-
ence system that improves those that employs auto-
matically acquired information-status knowledge by
1.8–4.1% (MP) and 2.7–3.1% (CR) in F-measure.
This indicates that the accuracy in computing such
knowledge does play a role in determining its use-
fulness for coreference resolution.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999948166666667">
We examined the problem of automatically deter-
mining the information status of discourse entities in
spoken dialogues. In particular, we augmented Nis-
sim’s feature set with two types of features: lexical
features, which capture in a shallow manner world
knowledge implicitly encoded in the annotated data;
and syntactic parse trees, which provide a richer rep-
resentation of the syntactic context in which a dis-
course entity appears than grammatical roles. Re-
sults on 147 Switchboard dialogues demonstrated
the effectiveness of these features: we obtained a
significant improvement of 8.1% in accuracy over
a information-status classifier trained on Nissim’s
feature set. In addition, we evaluated information-
status classification in the context of coreference
resolution, and showed that automatically acquired
knowledge of information status can be profitably
used to improve coreference systems.
</bodyText>
<page confidence="0.958118">
1078
</page>
<sectionHeader confidence="0.614926" genericHeader="references">
Acknowledgments North American Chapter of the Association for Com-
</sectionHeader>
<bodyText confidence="0.9788277">
We thank the three reviewers for their invaluable putational Linguistics; Proceedings of the Main Con-
comments on an earlier draft of the paper. This work ference, pages 236–243.
was supported in part by NSF Grant IIS-0812261. Pascal Denis and Jason Baldridge. 2008. Specialized
References models and ranking for coreference resolution. In Pro-
David J. Allerton. 1978. The notion of ’givenness’ and ceedings of the 2008 Conference on Empirical Meth-
its relations to presupposition and to theme. Lingua, ods in Natural Language Processing, pages 660–669.
44:133–168. Miriam Eckert and Michael Strube. 2001. Dialogue acts,
Chinatsu Aone and Scott William Bennett. 1995. Eval- synchronising units and anaphora resolution. Journal
uating automated and manual acquisition of anaphora of Semantics, 17(1):51–89.
resolution strategies. In Proceedings of the 33rd An- Eva Hajiˇcov´a. 1984. Topic and focus. In Contributions
nual Meeting of the Association for Computational to Functional Syntax, Semantics, and Language Com-
Linguistics, pages 122–129. prehension (LLSEE 16), pages 189–202. John Ben-
Amit Bagga and Breck Baldwin. 1998. Algorithms jamins, Amsterdam.
for scoring coreference chains. In Proceedings of the Michael A. K. Halliday. 1976. Notes on transitivity and
Linguistic Coreference Workshop at the First Interna- theme in English. Journal of Linguistics, 3(2):199–
tional Conference on Language Resources and Evalu- 244.
ation, pages 563–566. Laurence R. Horn. 1986. Presupposition, theme, and
David Bean and Ellen Riloff. 1999. Corpus-based iden- variations. In A. Farley et al., editor, Papers from the
tification of non-anaphoric noun phrases. In Proceed- Parasession on Pragmatics and Grammatical Theory
ings of the 37th Annual Meeting of the Association for at the 22nd Regional Meeting, pages 168–192. CLS.
</bodyText>
<reference confidence="0.960645464646465">
Computational Linguistics, pages 373–380. Thorsten Joachims. 1999. Making large-scale SVM
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo, learning practical. In Bernhard Scholkopf and Alexan-
Dan Jurafsky, Mark Steedman, and David Beaver. der Smola, editors, Advances in Kernel Methods - Sup-
2010. The NXT-format Switchboard corpus: A rich port Vector Learning, pages 44–56. MIT Press.
resource for investigating the syntax, semantics, prag- Harold W. Kuhn. 1955. The Hungarian method for the
matics and prosody of dialogue. Language Resources assignment problem. Naval Research Logistics Quar-
and Evaluation, 44(4):387–419. terly, 2(83).
Michael Collins and Nigel Duffy. 2001. Convolution Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
kernels for natural language. In Advances in Neural Kambhatla, and Salim Roukos. 2004. A mention-
Information Processing Systems 14, pages 625–632. synchronous coreference resolution algorithm based
Michael Collins. 2002. Ranking algorithms for named- on the Bell tree. In Proceedings of the 42nd Annual
entity extraction: Boosting and the voted perceptron. Meeting ofthe Association for Computational Linguis-
In Proceedings of the 40th Annual Meeting of the As- tics, pages 135–142.
sociation for Computational Linguistics, pages 489– Xiaoqiang Luo. 2005. On coreference resolution perfor-
496. mance metrics. In Proceedings of Human Language
Aron Culotta, Michael Wick, and Andrew McCallum. Technology Conference and Conference on Empirical
2007. First-order probabilistic models for coreference Methods in Natural Language Processing, pages 25–
resolution. In Human Language Technologies 2007: 32.
The Conference of the North American Chapter of the Joseph McCarthy and Wendy Lehnert. 1995. Using de-
Association for Computational Linguistics; Proceed- cision trees for coreference resolution. In Proceedings
ings of the Main Conference, pages 81–88. of the Fourteenth International Conference on Artifi-
Chad Cumby and Dan Roth. 2003. On kernel methods cialIntelligence, pages 1050–1055.
for relational learning. In Proceedings of the 20th In- Alessandro Moschitti. 2004. A study on convolution ker-
ternational Conference on Machine Learning, pages nels for shallow statistic parsing. In Proceedings of the
107–114. 42nd Annual Meeting of the Association for Computa-
Pascal Denis and Jason Baldridge. 2007. Global, joint tional Linguistics, pages 335–342.
determination of anaphoricity and coreference reso- Vincent Ng and Claire Cardie. 2002. Identifying
lution using integer programming. In Human Lan- anaphoric and non-anaphoric noun phrases to improve
guage Technologies 2007: The Conference of the coreference resolution. In Proceedings of the 19th In-
1079 ternational Conference on Computational Linguistics,
pages 730–736.
Vincent Ng. 2004. Learning noun phrase anaphoricity
to improve conference resolution: Issues in represen-
tation and optimization. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, pages 151–158.
Vincent Ng. 2009. Graph-cut-based anaphoricity deter-
mination for coreference resolution. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 575–
583.
Malvina Nissim, Shipra Dingare, Jean Carletta, and Mark
Steedman. 2004. An annotation scheme for infor-
mation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 94–102.
Ellen Prince. 1981. Toward a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223–255. New York, N.Y.: Academic Press.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968–977.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521–544.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Michael Strube. 1998. Never look back: An alternative
to centering. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and the 17th International Conference on Computa-
tional Linguistics, pages 1251–1257.
Olga Uryupina. 2003. High-precision identification of
discourse new and unique noun phrases. In Proceed-
ings ofthe ACL Student Research Workshop, pages 80–
86.
Enric Vallduvi. 1992. The Informational Component.
Garland, New York.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008a. Coreference systems
based on kernels methods. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 961–968.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539–
593.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, and
Sheng Li. 2008. An entity-mention model for coref-
erence resolution with inductive logic programming.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 843–851.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
3:1083–1106.
GuoDong Zhou and Fang Kong. 2009. Global learn-
ing of noun phrase anaphoricity in coreference reso-
lution via label propagation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 978–986.
</reference>
<page confidence="0.993721">
1080
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.523564">
<title confidence="0.999989">Learning the Information Status of Noun Phrases in Spoken Dialogues</title>
<author confidence="0.972138">Rahman</author>
<affiliation confidence="0.987379">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.548877">Richardson, TX</address>
<abstract confidence="0.999756863636364">An entity in a dialogue may be old, new, or mediated/inferrable with respect to the hearer’s beliefs. Knowing the information status of the entities participating in a dialogue can therefore facilitate its interpretation. We address the under-investigated problem of automatically determining the information status of discourse entities. Specifically, we extend Nissim’s (2006) machine learning approach to information-status determination with lexical and structured features, and exploit learned knowledge of the information status of each discourse entity for coreference resolution. Experimental results on a set of Switchboard dialogues reveal that (1) incorporating our proposed features into Nissim’s feature set enables our system to achieve stateof-the-art performance on information-status classification, and (2) the resulting information can be used to improve the performance of learning-based coreference resolvers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>