<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001181">
<title confidence="0.995924">
Extended Spelling Correction for German
</title>
<author confidence="0.976578">
Ralf Kese, Friedrich Dudda, Gerhard Heyer, Marianne Kugler
</author>
<affiliation confidence="0.209769">
TA Triumph-Adler AG
</affiliation>
<address confidence="0.467711333333333">
TA Research EF
Farther Str. 212
D-8500 Nuremberg 80
</address>
<email confidence="0.963044">
e-mail: ralf@triumph-adler.de
</email>
<sectionHeader confidence="0.997929" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902285714286">
A conservative extension of standard
spelling correction systems for German is
discussed which goes beyond normal
checking of isolated single words by
taking multi - words, linguistically
motivated non-words, as well as contexts
into account.
</bodyText>
<sectionHeader confidence="0.991465" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.99992080952381">
As indicated by Maurice Gross in his COLING
86 lecture (Gross, 1986), European languages
contain thousands of what he calls &amp;quot;frozen&amp;quot; or
&amp;quot;compound words&amp;quot;. In contrast to &amp;quot;free forms&amp;quot;,
frozen words - though being separable into
several words and suffixes - lack syntactic and/or
semantic compositionality. This &amp;quot;lack of
compositionality is apparent from lexical
restrictions&amp;quot; (at night, but: *at day, *at evening,
etc.) as well as &amp;quot;by the impossibility of inserting
material that is a priori plausible&amp;quot; (*at {coming,
present, cold, dark} night) (Gross, 1986).
Since the degree of frozenness can vary, the
procedure for recognizing compound words
within a text can be more or less complicated.
Yet, at least for the entirely and nearly entirely
frozen forms, simple string matching operations
will do (Gross, 1986).
However, although this clearly indicates that
at least those compound words whose parts have a
high degree of frozenness are accessible to the
methods of standard spelling correction systems,
the problem is that these systems at best try to
cope with (some) compound nouns (Frisch and
Zamora, 1988) while they are still ignorant of the
bulk of other compound forms and of violations
of lexical and/or cooccurrence (Harris, 1970)
restrictions in general.
As Zimmermann points out (Zimmermann,
1987) with respect to German forms like &amp;quot;in
bezug auf&amp;quot; (= frozen) versus &amp;quot;mit Bezug auf&amp;quot; (--
free), compounds clearly are out of the scope of
standard spelling correction systems due to the
fact that these systems check for isolated words
only and disregard the respective contexts.
Following Gross and Zimmermann (Gross,
1986; Zimmermann, 1987), we propose to further
extend standard spelling correction systems onto
the level of compound words by making them
context-sensitive as well as capable of treating
more than a single word at a time.
Yet even on the level of single words many
more errors could be detected by a spelling
corrector if it possessed at least some rudimentary
linguistic knowledge. In the case of a word that
takes irregular forms (like the German verb
&amp;quot;laufen&amp;quot; or the English noun &amp;quot;mouse&amp;quot;, for
example), a standard system seems to &amp;quot;know&amp;quot; the
word and its forms for it is able to verify them,
e.g., by simple lexicon lookup. Yet when
confronted with a regular though false form of
the very same word (e.g. with &amp;quot;laufte&amp;quot; as the
lst/3rd pers. sg. simple past ind. act., or with the
plural &amp;quot;mouses&amp;quot;), such a system normally fails to
propose the corresponding irregular form (&amp;quot;lief&amp;quot;
or &amp;quot;mice&amp;quot;) as a correction alternative.
Following a hint in (Zimmermann, 1987), we
propose to enhance standard spelling correction
systems on the level of isolated words by
introducing an additional sort of lexicon entries
that explicitly records those cognitive errors that
are intuitively likely to occur (at least in the
writings of non-native speakers) but which a
</bodyText>
<page confidence="0.997767">
126
</page>
<bodyText confidence="0.999552615384615">
standard system fails to treat in an adequate way
for system intrinsic reasons.
Considering that most of these errors give a
clear indication of a writer&apos;s knowledge of the
orthography of a language, and that therefore
their correction may be of particular importance
to him, the system also is explicitly intended to
exemplify how more complex linguistic
phenomena that are of importance to a user may
yet be treated by simpler means, thus achieving
in the sense of an engineering approach (Heyer,
1990) a satisfactory trade-off between theoretical
costs and practical benefits.
</bodyText>
<sectionHeader confidence="0.919706" genericHeader="introduction">
2 Overview of new Phenomena for
</sectionHeader>
<subsectionHeader confidence="0.985807">
Spelling Correction
</subsectionHeader>
<bodyText confidence="0.999098205882352">
As there are irregular forms which nevertheless
are well-formed, i.e.: words, there are also
regular forms which are ill-formed, i.e.: non-
words. Whereas words usually are known to a
spelling correction system, we have to add the
non-words to its vocabulary in order to improve
the quality of its corrections.
On the level of single words in German, non-
words come from various sources and comprise,
among others, false feminine derivations of
certain masculine nouns (*Wandererin, *Abtin),
false plurals of nouns (*Thematas, *Tertias), non-
licensed inflections (*beigem, *lila(n)es) or
comparisons (*lokaler, *minimalst) of certain
adjectives, false comparisons (*nahste,
*rentabelerer), wrong names for the citizens of
towns (*Steinhagener, *Stadthager), etc. Some
out-dated forms (e.g.: PreiBelbeere, verkaufst,
aberglaubig) can likewise be treated as non-
words.
It&apos;s on the level of compounds that words
rather than non-words come into consideration
again when we look for contextual constraints or
cooccurrence restrictions that determine
orthography beyond the scope of what can be
accepted or rejected on the basis of isolated
words alone.
For words in German, these restrictions
determine, among other things, whether or not
certain forms (1) begin with an upper or lower
case letter; (2) have to be separated by (2.1)
blank, (2.2) hyphen, (2.3) or not at all; (3)
combine with certain other forms; or even (4)
influence punctuation. Examples are:
</bodyText>
<figure confidence="0.934893">
(1) Ich laufe cis.
versus
Ich laufe auf dem Eis.
Er diirfte Bankroll machen.
versus
Er diirfte bankroll sem.
(2.1) Sic kann nicht Fahrrad fahren.
versus
(2.3) Sic kann nicht radfahren.
Es war bitter kalt.
versus
Es war em n bitterkalter Tag.
Er liebt Ich-Romane.
versus
Er liebt Romane in Ichform.
Betonblocke vs. *Betonblocks
versus
Hauserblocks vs. *Ilduserblocke
(4) Er rauchteL ohne dafi sic davon wuBte.
versus
*Er rauchte ohne,. dafi sic davon wuBte.
</figure>
<sectionHeader confidence="0.958343" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999814619047619">
According to a distinction made in the literature
(Pollock and Zamora, 1984; Salton, 1989), there
are two main approaches in automatic spelling
correction: While the &apos;absolute&apos; approach &amp;quot;consists
of using a dictionary of commonly misspelled
words, and replacing any misspelling detected in
a text by the corrected version listed in the
dictionary&amp;quot; (Salton, 1989), the &apos;relative&apos; approach
consists of locating in a conventional dictionary
with correct spellings words &amp;quot;that are most similar
to a misspelling and selecting a correction from
these. Generally, the selection method is based on
maximizing similarity or minimizing the string-
to-string edit distance&amp;quot; (Pollock and Zamora,
1984).
Although there is some use of &apos;absolute&apos;
methods in some systems (Pollock and Zamora,
1984), &amp;quot;referencing a dictionary of correctly
spelled words&amp;quot; (Frisch and Zamora, 1988) is
standard. On that basis, most of the purely
motoric single word errors, or &amp;quot;typographical
</bodyText>
<equation confidence="0.521231">
(3)
</equation>
<page confidence="0.97291">
127
</page>
<bodyText confidence="0.984654744897959">
errors&amp;quot; (Berkel and Smedt, 1990), can be
corrected. Some conventional systems additionally
try to cope with a certain subset of cognitive, or
&amp;quot;orthographical&amp;quot; (Berkel and Smedt, 1990), errors
which &amp;quot;result in homophonous strings&amp;quot; and
involve &amp;quot;some kind of phonemic transcription&amp;quot;
(Berkel and Smedt, 1990) for their correction.
Since the cognitive errors outlined in 1 and 2
above are non-standard, in the sense that they are
neither motoric (by definition) nor phonologically
motivated, a straightforward method to correct
them is the &apos;absolute&apos; one of directly encoding
error patterns in a lexicon and replacing each
matching occurrence in a text by the correction
listed in the system lexicon.
Now, in order to treat single (non-) words
and compounds in a uniform way, each entry in
the system lexicon is modelled as a quintuple
&lt;W,L,R,C,E&gt; specifying a pattern of a (multi-)
word W for which a correction C will be
proposed accompanied by an explanation E if f a
given match of W against some passage in the
text under scrutiny differs significantly from C
and the - possibly empty - left and right contexts
L and R of W also match the environment of W&apos;s
counterpart in the text.
Disregarding E for a moment, this is
tantamount to saying that each such record is
interpreted as a string rewriting rule
W - -&gt; C / L R
replacing W (e.g.: Bezug) by C (e.g.: bezug) in the
environment L R (e.g.: in auf).
The form of these productions can best be
characterized with an eye to the Chomsky
hierarchy as unrestricted, since we can have any
non-null number of symbols on the LHS replaced
by any number of symbols on the RHS, possibly
by null (Partec et al., 1990).
With an eye to semi-Thue or extended
axiomatic systems one could say that a linearly
ordered sequence of strings W, Cl, C2, ..., Cm is
a derivation of Cm if f (1) W is a (faulty) string
(in the text to be corrected) and (2) each Ci
follows from the immediately preceding string by
one of the productions listed in the lexicon
(Partee et al., 1990).
Thus, theoretically, a single mistake can be
corrected by applying a whole sequence of
productions, though in practice the default is
clearly that a correction be done in a single
derivational step, at least as long as the system is
just operating on strings and not on additional
non-terminal symbols.
Occurrences of W, L, and R in a text are
recognized by pattern matching techniques. An
error pattern W ignores the particularly error-
prone aspects upper/lower case and word
separator (see the examples in 2 above). It thus
matches both the correct and incorrect spellings
with respect to these features.
Beside wildcards for characters, like &amp;quot;*&amp;quot;, a
pattern for W, L, or R may contain also
wildcards for words allowing, for example, the
specification of a maximal distance of L or R
with respect to W. Since the types of errors
discussed here only occur within sentences, such
a distant match has to be restricted by the
sentence boundaries. Thus, by having the system
operate sentencewise, any left or right context is
naturally restricted to be some string within the
same sentence as W or to be a boundary of that
sentence (e.g.: a punctuation mark).
Any left or right context is either a positive
or a negative one, i.e., its components are
homogeneously either required or forbidden in
order for the corresponding rule to fire. So far it
has not been necessary to allow for mixed modes
within a left or right context.
In case a correction C is proposed to the user,
additionally a message will be displayed to him
identifying the reason why C is correct rather
than W. Depending on the user&apos;s knowledge of
the language under investigation, he can take this
either as an opportunity to learn or rather as a
help for deciding whether to finally accept or
reject the proposal.
There are two kinds of explanations, absolute
and conditional ones. Whereas absolute rules
indicate that the system has necessary and
sufficient evidence for W&apos;s deviance, there clearly
are cases where either W or C could be correct
and this question cannot be decided on the basis
of the system&apos;s lexical information alone. In these
cases, a conditional or if-then explanation is
given to the user offering a higher-level decision
criterion which the system itself is unable to
apply.
Take, as an example, the sentence
</bodyText>
<page confidence="0.991411">
128
</page>
<bodyText confidence="0.997093945945946">
Dieser Film betrif ft Alt und Jung.
which clearly allows for two readings, one which
renders &amp;quot;Alt und Jung&amp;quot; as the false spelling of the
idiomatic expression &amp;quot;alt und jung&amp;quot; meaning
&amp;quot;everybody&amp;quot;, and another one which takes &amp;quot;Alt
und Jung&amp;quot; as the correct form that literally
designates the old and the young while excluding
the middle-aged. Thus, substitutability by
&amp;quot;jedermann&amp;quot; (i.e.: &amp;quot;everybody&amp;quot;) would be an
adequate decision criterion to convey to the user.
Although the method described above
introduces a new kind of lexical data, its (higher-
level) error correction still operates on nothing
but strings. No deep and time-consuming
analysis, like parsing, is involved.
Restricting the system that way makes our
approach to context-sensitiveness different from
the one considered in (Rimon and Herz, 1991),
where context sensitive spelling verification is
proposed to be done with the help of &amp;quot;local
constraints automata (LCAs)&amp;quot; which process
contextual constraints on the level of lexical or
syntactic categories rather than on the basic level
of strings. In fact, proof-reading with LCAs
rather amounts to genuine grammar checking and
as such belongs to a different and higher level of
language checking.
Context sensitive spelling checking, as
proposed here, can be regarded as a checking
level in its own right, lying in between any
checking on word level and grammar checking. It
thus could complement the two-level checker
discussed in (Vosse, 1992) by correcting especially
those errors in idiomatic expressions, like &amp;quot;te alle
tijden&amp;quot; -&gt; &amp;quot;te alien tijde&amp;quot;, which cannot be
detected on word or sentence level; compare
(Vosse, 1992).
</bodyText>
<sectionHeader confidence="0.999322" genericHeader="method">
4 A Processing Model
</sectionHeader>
<bodyText confidence="0.980670470588235">
A good model of the system is given by a
deterministic multitape Turing machine (Hoperoft
and Ullman, 1979) consisting of a finite control
with, in effect, three tapes and tape heads. The
following description relates to sentence level:
Initially, the input appears on the first tape
with each of the tape&apos;s cells containing either a
word, a blank (symbolized below by a single &amp;quot;B&amp;quot;),
or a left or right sentence boundary symbol.
Thus, any input sentence can be stored by a
finite sequence of cells.
The second tape holds a read-only copy of
the initial text. While the first tape will be
rewritten, the second serves just as a reference
tape. The third tape is also read-only, it holds the
finite sequence of lexicon entries.
Consider the following snapshot of the system
Ti: B in B Bezug B auf B
T2: B in B Bezug B auf B
T3: /b/ezug (lin )1auf bezug
where &amp;quot;Bezug&amp;quot; has been scanned on the reference
tape T2, and a pattern /b/ezug has been found in
the lexicon T3 that ignores upper/lower case in
the match but requires a lower case &amp;quot;bezug&amp;quot; just
in case &amp;quot;in&amp;quot; can be found as 1 word to the left (as
is expressed by &amp;quot;(lin&amp;quot;) and &amp;quot;auf&amp;quot; can be found 1
non-blank cell to the right on T2.
Since the corresponding contexts of /b/ezug
can be verified on T2 (by simply moving T2&apos;s
head ^ to the respective cells, scanning their
contents, and comparing these with the relevant
information on T3), finally the error &amp;quot;Bezug&amp;quot; is
corrected on Ti and a new checking cycle is
started with the next word:
</bodyText>
<equation confidence="0.537001666666667">
Ti: B in B bezug B auf B
T2: B in B Bezug B auf B
A
</equation>
<bodyText confidence="0.99986275">
Note, as should be clear from the outset, that
a previous correction on Ti is not available as a
context for any next word under scrutiny, but
only the uncorrected words on T2 are.
Thus, if it were counterfactually the case that
&amp;quot;auf&amp;quot; had to be corrected somehow whenever it
appeared to the right of &amp;quot;bezug&amp;quot; as opposed to
&amp;quot;Bezug&amp;quot;, and given the input of the above
example, our system - though producing &amp;quot;bezug&amp;quot;
as the left context of &amp;quot;auf&amp;quot; on Ti - would clearly
fail to correct &amp;quot;auf&amp;quot; since it would still be taking
any context from T2.
</bodyText>
<page confidence="0.997137">
129
</page>
<bodyText confidence="0.999850454545454">
Although one can think of other, more
realistic, cases (like, e.g., &amp;quot;daB ich Eis laufte&amp;quot; -&gt;
&amp;quot;daB ich eislief&amp;quot;) which require two or more
correction steps such that at least one of these
steps (&amp;quot;Eis lief&amp;quot; -&gt; &amp;quot;eislief&amp;quot;) depends on another
one (&amp;quot;laufte&amp;quot; -&gt; &amp;quot;lief&amp;quot;), there clearly are other
alternatives (like writing clever lexical entries)
beside giving up reading from T2.
Giving up 12 would mean to give up the
simple working hypothesis that all the higher-
level errors within a given input sentence can be
corrected independently. As a consequence, the
system would become much more complex and,
probably, less efficient.
For German, we have not yet faced any
(significant amout of) data that would justify a
more complex redesign of the system. However,
since the data captured in the system&apos;s lexicon
covers at present some 50 % of the relevant
phenomena compared to the Duden (Berger
1985), the ultimate complexity of the system has
to be regarded as an open and empirical question.
</bodyText>
<sectionHeader confidence="0.65357" genericHeader="method">
5 Status of Implementation
</sectionHeader>
<bodyText confidence="0.999925866666667">
A first prototype of the system described above
has been developed in C under UNIX within the
ESPRIT II project 2315 &amp;quot;Translator&apos;s Workbench&amp;quot;
(TWB) as one of several separate modules
checking basic as well as higher levels of various
languages [like grammar and style; see (Thurmair,
1990) and (Winkelmann, 1990)].
A derived and extended B-release version -
covering 3.000 rewriting rules - has been
integrated into both a proprietary text processing
software under DOS and Microsoft&apos;s WINWORD
1.1 under MS WINDOWS 3.0. In each case it runs
independently from the built-in standard spelling
verifier, although this is not transparent to the
user who perceives just one proofreader checking
each sentence of a text twice, i.e., on two
different levels.
On both these implementations, some
problems have received practical solutions to an
acceptable degree.
For example, the problem of mistaking an
abbreviation for the end of a sentence (because
both end with a dot), which could prevent a
context from being recognized, is &apos;solved&apos; by
having the sentence segmentation routine always
read beyond a known abbreviation. This might
result eventually in taking two sentences to be
one, but would, of course, not disturb intra-
sentential error correction. Nothing, however,
prevents the system from stopping at an unknown
abbreviation and thereby falling short of a
context it otherwise would have recognized. From
this it is clear that the system should at least
know the most frequent abbreviations of a given
language.
Likewise, the formatting information of a text
is preserved to a very high degree during
correction, as it should be. Nevertheless, there
naturally are cases where some such information
will get lost as is clear from the simple fact that
there can be shrinking productions reducing n
differently formatted elements on the LHS to m
elements on the RHS, with m &lt; n. But these are
borderline cases.
What is less acceptable, for each of the
implementations mentioned above, is the lack of
integration of the checking on the various levels.
Thus, it may happen that the checkers -
running one after the other over the same text -
disturb each other&apos;s results by proposing
antagonistic corrections with respect to one and
the same expression: Within the correct passage
&amp;quot;in bezug auf&amp;quot;, for example, &amp;quot;bezug&amp;quot; will first be
regarded as an error by the standard checker
which then will propose to rewrite it as &amp;quot;Bezug&amp;quot;.
If the user accepts this proposal, he will receive
the exactly opposite advice by the context
sensitive checker.
On the other hand, checking on different
levels could nicely go hand in hand and produce
synergetic effects: For, clearly, any context
sensitive checking requires that the contexts
themselves be correct and thus possibly have been
corrected in a previous, possibly context free,
step. The checking of a single word could in turn
profit from contextual knowledge in narrowing
down the number of correction alternatives to be
proposed for a given error: While there may be
some eight or nine plausible candidates as
corrections of &amp;quot;Bezjg&amp;quot; when regarded in isolation.
only one candidate, i.e. &amp;quot;bezug&amp;quot;, is left when the
context &amp;quot;in auf&amp;quot; is taken into account.
Thus, there is a strong demand for arriving at
a holistic solution for multi-level language
checking rather than for just having various level
</bodyText>
<page confidence="0.989489">
130
</page>
<bodyText confidence="0.7388625">
experts particularistically hooked together in
series. This will be a task for the future.
Rudolf Frisch and Antonio Zamora. Spelling
assistance for compound words. IBM Journal of
Research and Development, 32(2): 195-200, March
1988.
</bodyText>
<sectionHeader confidence="0.998932" genericHeader="conclusions">
6 Ongoing Work
</sectionHeader>
<bodyText confidence="0.999978642857143">
As an inhouse test revealed, it is very important
for users to have the possibility to add data to the
system. As a consequence of that we are currently
developing a higher-level user dictionary that will
accept and support entering context-sensitive
(multi-) words of the kind discussed in this
paper.
At the same time, data acquisition is still
going on. Since, unfortunately, there is (to our
knowledge) no ready-made corpus of higher-level
errors available, the collection of data is a time
consuming process. The best reference book for
German seems to be the Duden (Berger 1985), yet
it consists of an unstructured mixture of all
possible kinds of errors and often presents a
paradigmatic example rather than all the members
of a given error class.
As concerns languages other than German, we
take it that a similar approach is feasible for
them as well. Although in comparison with, e.g.,
English, French, Italian, and Spanish, German
seems to be unique as concerns the relevance of
the context for upper/lower case spellings in a
large number of cases, there are at least, as
indicated in (Gross, 1986), the thousands of
compounds or frozen words in each of these
languages which are clearly within reach for the
methods discussed.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999640482142857">
Dieter Berger, editor. Richtiges und gutes
Deutsch. WOrterbuch der sprach lichen
Zweifelsfiille. Der Duden in 10 Banden. Das
Standardwerk zur deutschen Sprache, volume 9.
Bibliographisches Institut, Mannheim, Germany,
3rd ed. 1985.
Brigitte van Berkel and Koenraad De Smedt 1990.
Triphone Analysis: A combined method for the
correction of orthographical and typographical
errors. In Proceedings of the Second Conference
on Applied Natural Language Processing, pages
77-83, Austin, Texas, February 1988. Association
for Computational Linguistics.
Maurice Gross. Lexicon Grammar. The
Representation of Compound Words. In
Proceedings of the Eleventh International
Conference on Computational Linguistics, pages
1-6, Bonn, Germany, August 1986. International
Committee on Computational Linguistics.
Zellig S. Harris. Papers in Structural and
Transformational Linguistics. Formal Linguistics
Series, volume 1. D. Reidel Publishing Company,
Dordrecht, The Netherlands, 1970.
Gerhard Heyer. Probleme und Au fgaben einer
angewandten Computerlinguistik. KI-Kiinstliche
Intelligenz 3(1): 38-42, 1990.
John E. Hoperoft and Jeffrey D. Ullman.
Introduction to Automata Theory, Languages, and
Computation. Addison -Wesley Publishing
Company, Reading, Massachusetts, 1979.
Barbara H. Partee, Alice ter Meulen, and Robert
E. Wall. Mathematical Methods in Linguistics.
Studies in Linguistics and Philosophy, volume 30.
Kluwer Academic Publishers, Dordrecht, The
Netherlands 1990.
Joseph J. Pollock and Antonio Zamora. Automatic
spelling correction in scientific and scholarly text.
Communications of the ACM, 27(4): 358-368,
April 1984.
Mori Rimon and Jacky Herz. The Recognition
Capacity of Local Syntactic Constraints. In
Proceedings of the Fifth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 155-160, Berlin,
Germany, April 1991. Association for
Computational Linguistics.
Gerard Salton. Automatic Text Processing. The
Transformation, Analysis, and Retrieval of
Information by Computer. Addison - Wesley
Publishing Company, Reading, Massachusetts,
1989.
Gregor Thurmair. Parsing for Grammar and Style
Checking. In Papers presented to the Thirteenth
International Conference on Computational
Linguistics, volume 2, pages 365-370, Helsinki,
Finland, August 1990. Hans Karlgren, editor.
</reference>
<page confidence="0.998895">
131
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.026117">
<title confidence="0.832538">Extended Spelling Correction for German</title>
<author confidence="0.998271">Ralf Kese</author>
<author confidence="0.998271">Friedrich Dudda</author>
<author confidence="0.998271">Gerhard Heyer</author>
<author confidence="0.998271">Marianne Kugler</author>
<affiliation confidence="0.969513">TA Triumph-Adler AG TA Research EF</affiliation>
<address confidence="0.991038">Farther Str. 212 D-8500 Nuremberg 80</address>
<email confidence="0.998255">e-mail:ralf@triumph-adler.de</email>
<abstract confidence="0.99437763449692">A conservative extension of standard spelling correction systems for German is discussed which goes beyond normal checking of isolated single words by taking multi words, linguistically motivated non-words, as well as contexts into account. 1 Motivation As indicated by Maurice Gross in his COLING 86 lecture (Gross, 1986), European languages contain thousands of what he calls &amp;quot;frozen&amp;quot; or &amp;quot;compound words&amp;quot;. In contrast to &amp;quot;free forms&amp;quot;, frozen words though being separable into several words and suffixes lack syntactic and/or semantic compositionality. This &amp;quot;lack of compositionality is apparent from lexical restrictions&amp;quot; (at night, but: *at day, *at evening, etc.) as well as &amp;quot;by the impossibility of inserting material that is a priori plausible&amp;quot; (*at {coming, present, cold, dark} night) (Gross, 1986). Since the degree of frozenness can vary, the procedure for recognizing compound words within a text can be more or less complicated. Yet, at least for the entirely and nearly entirely frozen forms, simple string matching operations will do (Gross, 1986). However, although this clearly indicates that at least those compound words whose parts have a high degree of frozenness are accessible to the methods of standard spelling correction systems, the problem is that these systems at best try to cope with (some) compound nouns (Frisch and 1988) while they are still ignorant of bulk of other compound forms and of violations and/or cooccurrence (Harris, 1970) restrictions in general. Zimmermann points 1987) with respect to German forms like &amp;quot;in bezug auf&amp;quot; (= frozen) versus &amp;quot;mit Bezug auf&amp;quot; (-free), compounds clearly are out of the scope of standard spelling correction systems due to the fact that these systems check for isolated words only and disregard the respective contexts. Following Gross and Zimmermann (Gross, 1986; Zimmermann, 1987), we propose to further extend standard spelling correction systems onto the level of compound words by making them context-sensitive as well as capable of treating more than a single word at a time. Yet even on the level of single words many more errors could be detected by a spelling corrector if it possessed at least some rudimentary linguistic knowledge. In the case of a word that irregular forms (like the verb &amp;quot;laufen&amp;quot; or the English noun &amp;quot;mouse&amp;quot;, for example), a standard system seems to &amp;quot;know&amp;quot; the word and its forms for it is able to verify them, e.g., by simple lexicon lookup. Yet when a regular though false form of very same word (e.g. with &amp;quot;laufte&amp;quot; as pers. sg. simple past act., or with the plural &amp;quot;mouses&amp;quot;), such a system normally fails to propose the corresponding irregular form (&amp;quot;lief&amp;quot; or &amp;quot;mice&amp;quot;) as a correction alternative. Following a hint in (Zimmermann, 1987), we propose to enhance standard spelling correction systems on the level of isolated words by introducing an additional sort of lexicon entries that explicitly records those cognitive errors that are intuitively likely to occur (at least in the writings of non-native speakers) but which a 126 standard system fails to treat in an adequate way for system intrinsic reasons. Considering that most of these errors give a clear indication of a writer&apos;s knowledge of the orthography of a language, and that therefore their correction may be of particular importance to him, the system also is explicitly intended to exemplify how more complex linguistic phenomena that are of importance to a user may yet be treated by simpler means, thus achieving in the sense of an engineering approach (Heyer, 1990) a satisfactory trade-off between theoretical costs and practical benefits. 2 Overview of new Phenomena for Spelling Correction As there are irregular forms which nevertheless are well-formed, i.e.: words, there are also regular forms which are ill-formed, i.e.: nonwords. Whereas words usually are known to a spelling correction system, we have to add the non-words to its vocabulary in order to improve the quality of its corrections. On the level of single words in German, nonwords come from various sources and comprise, among others, false feminine derivations of certain masculine nouns (*Wandererin, *Abtin), false plurals of nouns (*Thematas, *Tertias), nonlicensed inflections (*beigem, *lila(n)es) or comparisons (*lokaler, *minimalst) of certain false (*nahste, *rentabelerer), wrong names for the citizens of towns (*Steinhagener, *Stadthager), etc. Some forms verkaufst, aberglaubig) can likewise be treated as nonwords. It&apos;s on the level of compounds that words rather than non-words come into consideration again when we look for contextual constraints or cooccurrence restrictions that determine orthography beyond the scope of what can be accepted or rejected on the basis of isolated words alone. For words in German, these restrictions determine, among other things, whether or not certain forms (1) begin with an upper or lower case letter; (2) have to be separated by (2.1) blank, (2.2) hyphen, (2.3) or not at all; (3) combine with certain other forms; or even (4) influence punctuation. Examples are: Ich cis. versus auf dem Eis. diirfte Bankrollmachen. versus diirfte bankrollsem. Sic kann nicht fahren. versus Sic kann nicht war kalt. versus war em n liebt versus liebt Romane in vs. *Betonblocks versus *Ilduserblocke Er ohne dafi davon wuBte. versus rauchte dafi davon wuBte. 3 Method According to a distinction made in the literature (Pollock and Zamora, 1984; Salton, 1989), there are two main approaches in automatic spelling correction: While the &apos;absolute&apos; approach &amp;quot;consists of using a dictionary of commonly misspelled words, and replacing any misspelling detected in a text by the corrected version listed in the (Salton, 1989), the &apos;relative&apos; consists of locating in a conventional dictionary with correct spellings words &amp;quot;that are most similar to a misspelling and selecting a correction from these. Generally, the selection method is based on maximizing similarity or minimizing the stringto-string edit distance&amp;quot; (Pollock and Zamora, 1984). Although there is some use of &apos;absolute&apos; methods in some systems (Pollock and Zamora, 1984), &amp;quot;referencing a dictionary of correctly spelled words&amp;quot; (Frisch and Zamora, 1988) is standard. On that basis, most of the purely motoric single word errors, or &amp;quot;typographical 127 errors&amp;quot; (Berkel and Smedt, 1990), can be corrected. Some conventional systems additionally try to cope with a certain subset of cognitive, or &amp;quot;orthographical&amp;quot; (Berkel and Smedt, 1990), errors which &amp;quot;result in homophonous strings&amp;quot; and involve &amp;quot;some kind of phonemic transcription&amp;quot; (Berkel and Smedt, 1990) for their correction. Since the cognitive errors outlined in 1 and 2 above are non-standard, in the sense that they are neither motoric (by definition) nor phonologically motivated, a straightforward method to correct them is the &apos;absolute&apos; one of directly encoding error patterns in a lexicon and replacing each matching occurrence in a text by the correction listed in the system lexicon. Now, in order to treat single (non-) words and compounds in a uniform way, each entry in the system lexicon is modelled as a quintuple &lt;W,L,R,C,E&gt; specifying a pattern of a (multi-) word W for which a correction C will be proposed accompanied by an explanation E if f a given match of W against some passage in the text under scrutiny differs significantly from C and the possibly empty left and right contexts L and R of W also match the environment of W&apos;s counterpart in the text. Disregarding E for a moment, this is tantamount to saying that each such record is interpreted as a string rewriting rule W - -&gt; C / L R replacing W (e.g.: Bezug) by C (e.g.: bezug) in the environment L R (e.g.: in auf). The form of these productions can best be characterized with an eye to the Chomsky hierarchy as unrestricted, since we can have any non-null number of symbols on the LHS replaced by any number of symbols on the RHS, possibly by null (Partec et al., 1990). With an eye to semi-Thue or extended axiomatic systems one could say that a linearly ordered sequence of strings W, Cl, C2, ..., Cm is a derivation of Cm if f (1) W is a (faulty) string (in the text to be corrected) and (2) each Ci follows from the immediately preceding string by one of the productions listed in the lexicon (Partee et al., 1990). a single mistake can be corrected by applying a whole sequence of productions, though in practice the default is clearly that a correction be done in a single derivational step, at least as long as the system is just operating on strings and not on additional non-terminal symbols. Occurrences of W, L, and R in a text are recognized by pattern matching techniques. An error pattern W ignores the particularly errorprone aspects upper/lower case and word separator (see the examples in 2 above). It thus matches both the correct and incorrect spellings with respect to these features. Beside wildcards for characters, like &amp;quot;*&amp;quot;, a pattern for W, L, or R may contain also wildcards for words allowing, for example, the specification of a maximal distance of L or R with respect to W. Since the types of errors discussed here only occur within sentences, such a distant match has to be restricted by the sentence boundaries. Thus, by having the system operate sentencewise, any left or right context is naturally restricted to be some string within the same sentence as W or to be a boundary of that sentence (e.g.: a punctuation mark). Any left or right context is either a positive or a negative one, i.e., its components are homogeneously either required or forbidden in order for the corresponding rule to fire. So far it has not been necessary to allow for mixed modes within a left or right context. In case a correction C is proposed to the user, a message will be displayed to identifying the reason why C is correct rather than W. Depending on the user&apos;s knowledge of the language under investigation, he can take this either as an opportunity to learn or rather as a help for deciding whether to finally accept or reject the proposal. There are two kinds of explanations, absolute and conditional ones. Whereas absolute rules indicate that the system has necessary and sufficient evidence for W&apos;s deviance, there clearly are cases where either W or C could be correct and this question cannot be decided on the basis of the system&apos;s lexical information alone. In these cases, a conditional or if-then explanation is given to the user offering a higher-level decision criterion which the system itself is unable to apply. Take, as an example, the sentence 128 Dieser Film betrif ft Alt und Jung. which clearly allows for two readings, one which renders &amp;quot;Alt und Jung&amp;quot; as the false spelling of the idiomatic expression &amp;quot;alt und jung&amp;quot; meaning &amp;quot;everybody&amp;quot;, and another one which takes &amp;quot;Alt und Jung&amp;quot; as the correct form that literally designates the old and the young while excluding the middle-aged. Thus, substitutability by &amp;quot;jedermann&amp;quot; (i.e.: &amp;quot;everybody&amp;quot;) would be an adequate decision criterion to convey to the user. Although the method described above introduces a new kind of lexical data, its (higherlevel) error correction still operates on nothing but strings. No deep and time-consuming analysis, like parsing, is involved. Restricting the system that way makes our approach to context-sensitiveness different from the one considered in (Rimon and Herz, 1991), where context sensitive spelling verification is proposed to be done with the help of &amp;quot;local constraints automata (LCAs)&amp;quot; which process contextual constraints on the level of lexical or syntactic categories rather than on the basic level of strings. In fact, proof-reading with LCAs rather amounts to genuine grammar checking and as such belongs to a different and higher level of language checking. Context sensitive spelling checking, as proposed here, can be regarded as a checking level in its own right, lying in between any checking on word level and grammar checking. It thus could complement the two-level checker discussed in (Vosse, 1992) by correcting especially those errors in idiomatic expressions, like &amp;quot;te alle tijden&amp;quot; -&gt; &amp;quot;te alien tijde&amp;quot;, which cannot be detected on word or sentence level; compare (Vosse, 1992). 4 A Processing Model A good model of the system is given by a deterministic multitape Turing machine (Hoperoft and Ullman, 1979) consisting of a finite control with, in effect, three tapes and tape heads. The following description relates to sentence level: Initially, the input appears on the first tape with each of the tape&apos;s cells containing either a word, a blank (symbolized below by a single &amp;quot;B&amp;quot;), or a left or right sentence boundary symbol. Thus, any input sentence can be stored by a finite sequence of cells. The second tape holds a read-only copy of the initial text. While the first tape will be rewritten, the second serves just as a reference tape. The third tape is also read-only, it holds the finite sequence of lexicon entries. Consider the following snapshot of the system Ti: B in B Bezug B auf B T2: B in B Bezug B auf B T3: /b/ezug (lin )1auf bezug where &amp;quot;Bezug&amp;quot; has been scanned on the reference tape T2, and a pattern /b/ezug has been found in the lexicon T3 that ignores upper/lower case in the match but requires a lower case &amp;quot;bezug&amp;quot; just in case &amp;quot;in&amp;quot; can be found as 1 word to the left (as expressed by &amp;quot;(lin&amp;quot;) and &amp;quot;auf&amp;quot; can be found non-blank cell to the right on T2. Since the corresponding contexts of /b/ezug can be verified on T2 (by simply moving T2&apos;s head ^ to the respective cells, scanning their contents, and comparing these with the relevant information on T3), finally the error &amp;quot;Bezug&amp;quot; is corrected on Ti and a new checking cycle is started with the next word: Ti: B in B bezug B auf B T2: B in B Bezug B auf B A Note, as should be clear from the outset, that a previous correction on Ti is not available as a context for any next word under scrutiny, but only the uncorrected words on T2 are. Thus, if it were counterfactually the case that &amp;quot;auf&amp;quot; had to be corrected somehow whenever it appeared to the right of &amp;quot;bezug&amp;quot; as opposed to &amp;quot;Bezug&amp;quot;, and given the input of the above example, our system though producing &amp;quot;bezug&amp;quot; as the left context of &amp;quot;auf&amp;quot; on Ti would clearly fail to correct &amp;quot;auf&amp;quot; since it would still be taking any context from T2. 129 Although one can think of other, more realistic, cases (like, e.g., &amp;quot;daB ich Eis laufte&amp;quot; -&gt; &amp;quot;daB ich eislief&amp;quot;) which require two or more correction steps such that at least one of these steps (&amp;quot;Eis lief&amp;quot; -&gt; &amp;quot;eislief&amp;quot;) depends on another one (&amp;quot;laufte&amp;quot; -&gt; &amp;quot;lief&amp;quot;), there clearly are other alternatives (like writing clever lexical entries) beside giving up reading from T2. Giving up 12 would mean to give up the simple working hypothesis that all the higherlevel errors within a given input sentence can be corrected independently. As a consequence, the system would become much more complex and, probably, less efficient. For German, we have not yet faced any (significant amout of) data that would justify a more complex redesign of the system. However, since the data captured in the system&apos;s lexicon at present some 50 the relevant phenomena compared to the Duden (Berger 1985), the ultimate complexity of the system has to be regarded as an open and empirical question. 5 Status of Implementation A first prototype of the system described above has been developed in C under UNIX within the ESPRIT II project 2315 &amp;quot;Translator&apos;s Workbench&amp;quot; (TWB) as one of several separate modules checking basic as well as higher levels of various languages [like grammar and style; see (Thurmair, 1990) and (Winkelmann, 1990)]. A derived and extended B-release version covering 3.000 rewriting rules has been integrated into both a proprietary text processing software under DOS and Microsoft&apos;s WINWORD 1.1 under MS WINDOWS 3.0. In each case it runs independently from the built-in standard spelling verifier, although this is not transparent to the user who perceives just one proofreader checking each sentence of a text twice, i.e., on two different levels. On both these implementations, some problems have received practical solutions to an acceptable degree. For example, the problem of mistaking an abbreviation for the end of a sentence (because both end with a dot), which could prevent a context from being recognized, is &apos;solved&apos; by having the sentence segmentation routine always read beyond a known abbreviation. This might result eventually in taking two sentences to be one, but would, of course, not disturb intrasentential error correction. Nothing, however, prevents the system from stopping at an unknown abbreviation and thereby falling short of a context it otherwise would have recognized. From this it is clear that the system should at least know the most frequent abbreviations of a given language. Likewise, the formatting information of a text is preserved to a very high degree during correction, as it should be. Nevertheless, there naturally are cases where some such information will get lost as is clear from the simple fact that there can be shrinking productions reducing n differently formatted elements on the LHS to m elements on the RHS, with m &lt; n. But these are borderline cases. What is less acceptable, for each of the implementations mentioned above, is the lack of integration of the checking on the various levels. Thus, it may happen that the checkers running one after the other over the same text disturb each other&apos;s results by proposing antagonistic corrections with respect to one and the same expression: Within the correct passage &amp;quot;in bezug auf&amp;quot;, for example, &amp;quot;bezug&amp;quot; will first be regarded as an error by the standard checker which then will propose to rewrite it as &amp;quot;Bezug&amp;quot;. If the user accepts this proposal, he will receive the exactly opposite advice by the context sensitive checker. On the other hand, checking on different levels could nicely go hand in hand and produce synergetic effects: For, clearly, any context sensitive checking requires that the contexts themselves be correct and thus possibly have been corrected in a previous, possibly context free, step. The checking of a single word could in turn profit from contextual knowledge in narrowing down the number of correction alternatives to be proposed for a given error: While there may be some eight or nine plausible candidates as corrections of &amp;quot;Bezjg&amp;quot; when regarded in isolation. only one candidate, i.e. &amp;quot;bezug&amp;quot;, is left when the context &amp;quot;in auf&amp;quot; is taken into account. Thus, there is a strong demand for arriving at a holistic solution for multi-level language checking rather than for just having various level 130 experts particularistically hooked together in series. This will be a task for the future. Rudolf Frisch and Antonio Zamora. Spelling for compound words. Journal of and Development, 195-200, March 1988. 6 Ongoing Work As an inhouse test revealed, it is very important for users to have the possibility to add data to the system. As a consequence of that we are currently developing a higher-level user dictionary that will accept and support entering context-sensitive (multi-) words of the kind discussed in this paper. At the same time, data acquisition is still going on. Since, unfortunately, there is (to our knowledge) no ready-made corpus of higher-level errors available, the collection of data is a time consuming process. The best reference book for German seems to be the Duden (Berger 1985), yet it consists of an unstructured mixture of all possible kinds of errors and often presents a paradigmatic example rather than all the members of a given error class. As concerns languages other than German, we take it that a similar approach is feasible for them as well. Although in comparison with, e.g., English, French, Italian, and Spanish, German seems to be unique as concerns the relevance of the context for upper/lower case spellings in a large number of cases, there are at least, as indicated in (Gross, 1986), the thousands of compounds or frozen words in each of these languages which are clearly within reach for the methods discussed.</abstract>
<note confidence="0.920860214285714">References Berger, editor. und gutes Deutsch. WOrterbuch der sprach lichen Duden in 10 Standardwerk zur deutschen Sprache, volume 9. Bibliographisches Institut, Mannheim, Germany, 3rd ed. 1985. Brigitte van Berkel and Koenraad De Smedt 1990. Triphone Analysis: A combined method for the correction of orthographical and typographical In of the Second Conference Applied Natural Language Processing, 77-83, Austin, Texas, February 1988. Association for Computational Linguistics.</note>
<author confidence="0.946597">Lexicon Grammar The</author>
<affiliation confidence="0.916442666666667">Representation of Compound Words. In Proceedings of the Eleventh International on Computational Linguistics,</affiliation>
<address confidence="0.671386">1-6, Bonn, Germany, August 1986. International</address>
<note confidence="0.980072">Committee on Computational Linguistics. S. Harris. in Structural and Linguistics. Linguistics Series, volume 1. D. Reidel Publishing Company, Dordrecht, The Netherlands, 1970. Gerhard Heyer. Probleme und Au fgaben einer Computerlinguistik. 38-42, 1990. John E. Hoperoft and Jeffrey D. Ullman.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Richtiges und gutes Deutsch. WOrterbuch der sprach lichen Zweifelsfiille. Der Duden</title>
<booktitle>in 10 Banden. Das Standardwerk zur deutschen Sprache, volume 9. Bibliographisches Institut,</booktitle>
<editor>Dieter Berger, editor.</editor>
<location>Mannheim, Germany, 3rd</location>
<marker></marker>
<rawString>Dieter Berger, editor. Richtiges und gutes Deutsch. WOrterbuch der sprach lichen Zweifelsfiille. Der Duden in 10 Banden. Das Standardwerk zur deutschen Sprache, volume 9. Bibliographisches Institut, Mannheim, Germany, 3rd ed. 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brigitte van Berkel</author>
<author>Koenraad De Smedt</author>
</authors>
<title>Triphone Analysis: A combined method for the correction of orthographical and typographical errors.</title>
<date>1990</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>77--83</pages>
<location>Austin, Texas,</location>
<marker>van Berkel, De Smedt, 1990</marker>
<rawString>Brigitte van Berkel and Koenraad De Smedt 1990. Triphone Analysis: A combined method for the correction of orthographical and typographical errors. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 77-83, Austin, Texas, February 1988. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Maurice Gross</author>
</authors>
<title>Lexicon Grammar. The Representation of Compound Words.</title>
<date>1986</date>
<booktitle>In Proceedings of the Eleventh International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>1--6</pages>
<publisher>Reidel Publishing Company,</publisher>
<location>Bonn, Germany,</location>
<contexts>
<context position="1022" citStr="Gross, 1986" startWordPosition="148" endWordPosition="149">n-words, as well as contexts into account. 1 Motivation As indicated by Maurice Gross in his COLING 86 lecture (Gross, 1986), European languages contain thousands of what he calls &amp;quot;frozen&amp;quot; or &amp;quot;compound words&amp;quot;. In contrast to &amp;quot;free forms&amp;quot;, frozen words - though being separable into several words and suffixes - lack syntactic and/or semantic compositionality. This &amp;quot;lack of compositionality is apparent from lexical restrictions&amp;quot; (at night, but: *at day, *at evening, etc.) as well as &amp;quot;by the impossibility of inserting material that is a priori plausible&amp;quot; (*at {coming, present, cold, dark} night) (Gross, 1986). Since the degree of frozenness can vary, the procedure for recognizing compound words within a text can be more or less complicated. Yet, at least for the entirely and nearly entirely frozen forms, simple string matching operations will do (Gross, 1986). However, although this clearly indicates that at least those compound words whose parts have a high degree of frozenness are accessible to the methods of standard spelling correction systems, the problem is that these systems at best try to cope with (some) compound nouns (Frisch and Zamora, 1988) while they are still ignorant of the bulk of</context>
</contexts>
<marker>Gross, 1986</marker>
<rawString>Maurice Gross. Lexicon Grammar. The Representation of Compound Words. In Proceedings of the Eleventh International Conference on Computational Linguistics, pages 1-6, Bonn, Germany, August 1986. International Committee on Computational Linguistics. Zellig S. Harris. Papers in Structural and Transformational Linguistics. Formal Linguistics Series, volume 1. D. Reidel Publishing Company, Dordrecht, The Netherlands, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Heyer</author>
</authors>
<title>Probleme und Au fgaben einer angewandten Computerlinguistik.</title>
<date>1990</date>
<journal>KI-Kiinstliche Intelligenz</journal>
<volume>3</volume>
<issue>1</issue>
<pages>38--42</pages>
<contexts>
<context position="3851" citStr="Heyer, 1990" startWordPosition="610" endWordPosition="611">rors that are intuitively likely to occur (at least in the writings of non-native speakers) but which a 126 standard system fails to treat in an adequate way for system intrinsic reasons. Considering that most of these errors give a clear indication of a writer&apos;s knowledge of the orthography of a language, and that therefore their correction may be of particular importance to him, the system also is explicitly intended to exemplify how more complex linguistic phenomena that are of importance to a user may yet be treated by simpler means, thus achieving in the sense of an engineering approach (Heyer, 1990) a satisfactory trade-off between theoretical costs and practical benefits. 2 Overview of new Phenomena for Spelling Correction As there are irregular forms which nevertheless are well-formed, i.e.: words, there are also regular forms which are ill-formed, i.e.: nonwords. Whereas words usually are known to a spelling correction system, we have to add the non-words to its vocabulary in order to improve the quality of its corrections. On the level of single words in German, nonwords come from various sources and comprise, among others, false feminine derivations of certain masculine nouns (*Wand</context>
</contexts>
<marker>Heyer, 1990</marker>
<rawString>Gerhard Heyer. Probleme und Au fgaben einer angewandten Computerlinguistik. KI-Kiinstliche Intelligenz 3(1): 38-42, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hoperoft</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison -Wesley Publishing Company,</publisher>
<location>Reading, Massachusetts,</location>
<contexts>
<context position="12941" citStr="Hoperoft and Ullman, 1979" startWordPosition="2086" endWordPosition="2089">such belongs to a different and higher level of language checking. Context sensitive spelling checking, as proposed here, can be regarded as a checking level in its own right, lying in between any checking on word level and grammar checking. It thus could complement the two-level checker discussed in (Vosse, 1992) by correcting especially those errors in idiomatic expressions, like &amp;quot;te alle tijden&amp;quot; -&gt; &amp;quot;te alien tijde&amp;quot;, which cannot be detected on word or sentence level; compare (Vosse, 1992). 4 A Processing Model A good model of the system is given by a deterministic multitape Turing machine (Hoperoft and Ullman, 1979) consisting of a finite control with, in effect, three tapes and tape heads. The following description relates to sentence level: Initially, the input appears on the first tape with each of the tape&apos;s cells containing either a word, a blank (symbolized below by a single &amp;quot;B&amp;quot;), or a left or right sentence boundary symbol. Thus, any input sentence can be stored by a finite sequence of cells. The second tape holds a read-only copy of the initial text. While the first tape will be rewritten, the second serves just as a reference tape. The third tape is also read-only, it holds the finite sequence o</context>
</contexts>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>John E. Hoperoft and Jeffrey D. Ullman. Introduction to Automata Theory, Languages, and Computation. Addison -Wesley Publishing Company, Reading, Massachusetts, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara H Partee</author>
<author>Alice ter Meulen</author>
<author>Robert E Wall</author>
</authors>
<date>1990</date>
<booktitle>Mathematical Methods in Linguistics. Studies in Linguistics and Philosophy,</booktitle>
<volume>30</volume>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, The</location>
<contexts>
<context position="8867" citStr="Partee et al., 1990" startWordPosition="1417" endWordPosition="1420"> (e.g.: in auf). The form of these productions can best be characterized with an eye to the Chomsky hierarchy as unrestricted, since we can have any non-null number of symbols on the LHS replaced by any number of symbols on the RHS, possibly by null (Partec et al., 1990). With an eye to semi-Thue or extended axiomatic systems one could say that a linearly ordered sequence of strings W, Cl, C2, ..., Cm is a derivation of Cm if f (1) W is a (faulty) string (in the text to be corrected) and (2) each Ci follows from the immediately preceding string by one of the productions listed in the lexicon (Partee et al., 1990). Thus, theoretically, a single mistake can be corrected by applying a whole sequence of productions, though in practice the default is clearly that a correction be done in a single derivational step, at least as long as the system is just operating on strings and not on additional non-terminal symbols. Occurrences of W, L, and R in a text are recognized by pattern matching techniques. An error pattern W ignores the particularly errorprone aspects upper/lower case and word separator (see the examples in 2 above). It thus matches both the correct and incorrect spellings with respect to these fe</context>
</contexts>
<marker>Partee, Meulen, Wall, 1990</marker>
<rawString>Barbara H. Partee, Alice ter Meulen, and Robert E. Wall. Mathematical Methods in Linguistics. Studies in Linguistics and Philosophy, volume 30. Kluwer Academic Publishers, Dordrecht, The Netherlands 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph J Pollock</author>
<author>Antonio Zamora</author>
</authors>
<title>Automatic spelling correction in scientific and scholarly text.</title>
<date>1984</date>
<journal>Communications of the ACM,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>358--368</pages>
<contexts>
<context position="5972" citStr="Pollock and Zamora, 1984" startWordPosition="932" endWordPosition="935">combine with certain other forms; or even (4) influence punctuation. Examples are: (1) Ich laufe cis. versus Ich laufe auf dem Eis. Er diirfte Bankroll machen. versus Er diirfte bankroll sem. (2.1) Sic kann nicht Fahrrad fahren. versus (2.3) Sic kann nicht radfahren. Es war bitter kalt. versus Es war em n bitterkalter Tag. Er liebt Ich-Romane. versus Er liebt Romane in Ichform. Betonblocke vs. *Betonblocks versus Hauserblocks vs. *Ilduserblocke (4) Er rauchteL ohne dafi sic davon wuBte. versus *Er rauchte ohne,. dafi sic davon wuBte. 3 Method According to a distinction made in the literature (Pollock and Zamora, 1984; Salton, 1989), there are two main approaches in automatic spelling correction: While the &apos;absolute&apos; approach &amp;quot;consists of using a dictionary of commonly misspelled words, and replacing any misspelling detected in a text by the corrected version listed in the dictionary&amp;quot; (Salton, 1989), the &apos;relative&apos; approach consists of locating in a conventional dictionary with correct spellings words &amp;quot;that are most similar to a misspelling and selecting a correction from these. Generally, the selection method is based on maximizing similarity or minimizing the stringto-string edit distance&amp;quot; (Pollock and Z</context>
</contexts>
<marker>Pollock, Zamora, 1984</marker>
<rawString>Joseph J. Pollock and Antonio Zamora. Automatic spelling correction in scientific and scholarly text. Communications of the ACM, 27(4): 358-368, April 1984.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mori Rimon</author>
<author>Jacky Herz</author>
</authors>
<title>The Recognition Capacity of Local Syntactic Constraints.</title>
<booktitle>In Proceedings of the Fifth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>155--160</pages>
<location>Berlin,</location>
<marker>Rimon, Herz, </marker>
<rawString>Mori Rimon and Jacky Herz. The Recognition Capacity of Local Syntactic Constraints. In Proceedings of the Fifth Conference of the European Chapter of the Association for Computational Linguistics, pages 155-160, Berlin,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Germany</author>
</authors>
<title>Association for Computational Linguistics.</title>
<date>1991</date>
<marker>Germany, 1991</marker>
<rawString>Germany, April 1991. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing. The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison - Wesley Publishing Company,</publisher>
<location>Reading, Massachusetts,</location>
<contexts>
<context position="5987" citStr="Salton, 1989" startWordPosition="936" endWordPosition="937"> forms; or even (4) influence punctuation. Examples are: (1) Ich laufe cis. versus Ich laufe auf dem Eis. Er diirfte Bankroll machen. versus Er diirfte bankroll sem. (2.1) Sic kann nicht Fahrrad fahren. versus (2.3) Sic kann nicht radfahren. Es war bitter kalt. versus Es war em n bitterkalter Tag. Er liebt Ich-Romane. versus Er liebt Romane in Ichform. Betonblocke vs. *Betonblocks versus Hauserblocks vs. *Ilduserblocke (4) Er rauchteL ohne dafi sic davon wuBte. versus *Er rauchte ohne,. dafi sic davon wuBte. 3 Method According to a distinction made in the literature (Pollock and Zamora, 1984; Salton, 1989), there are two main approaches in automatic spelling correction: While the &apos;absolute&apos; approach &amp;quot;consists of using a dictionary of commonly misspelled words, and replacing any misspelling detected in a text by the corrected version listed in the dictionary&amp;quot; (Salton, 1989), the &apos;relative&apos; approach consists of locating in a conventional dictionary with correct spellings words &amp;quot;that are most similar to a misspelling and selecting a correction from these. Generally, the selection method is based on maximizing similarity or minimizing the stringto-string edit distance&amp;quot; (Pollock and Zamora, 1984). A</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Gerard Salton. Automatic Text Processing. The Transformation, Analysis, and Retrieval of Information by Computer. Addison - Wesley Publishing Company, Reading, Massachusetts, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Thurmair</author>
</authors>
<title>Parsing for Grammar and Style Checking.</title>
<date>1990</date>
<booktitle>In Papers presented to the Thirteenth International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>365--370</pages>
<editor>Hans Karlgren, editor.</editor>
<location>Helsinki, Finland,</location>
<contexts>
<context position="16214" citStr="Thurmair, 1990" startWordPosition="2673" endWordPosition="2674"> that would justify a more complex redesign of the system. However, since the data captured in the system&apos;s lexicon covers at present some 50 % of the relevant phenomena compared to the Duden (Berger 1985), the ultimate complexity of the system has to be regarded as an open and empirical question. 5 Status of Implementation A first prototype of the system described above has been developed in C under UNIX within the ESPRIT II project 2315 &amp;quot;Translator&apos;s Workbench&amp;quot; (TWB) as one of several separate modules checking basic as well as higher levels of various languages [like grammar and style; see (Thurmair, 1990) and (Winkelmann, 1990)]. A derived and extended B-release version - covering 3.000 rewriting rules - has been integrated into both a proprietary text processing software under DOS and Microsoft&apos;s WINWORD 1.1 under MS WINDOWS 3.0. In each case it runs independently from the built-in standard spelling verifier, although this is not transparent to the user who perceives just one proofreader checking each sentence of a text twice, i.e., on two different levels. On both these implementations, some problems have received practical solutions to an acceptable degree. For example, the problem of mista</context>
</contexts>
<marker>Thurmair, 1990</marker>
<rawString>Gregor Thurmair. Parsing for Grammar and Style Checking. In Papers presented to the Thirteenth International Conference on Computational Linguistics, volume 2, pages 365-370, Helsinki, Finland, August 1990. Hans Karlgren, editor.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>