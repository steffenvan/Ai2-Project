<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000398">
<title confidence="0.998652">
Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora
</title>
<author confidence="0.999578">
Shiqi Zhao&apos;, Haifeng Wang2, Ting Liu&apos;, Sheng Li&apos;
</author>
<affiliation confidence="0.999102">
&apos;Harbin Institute of Technology, Harbin, China
</affiliation>
<email confidence="0.978468">
{zhaosq,tliu,lisheng}@ir.hit.edu.cn
</email>
<affiliation confidence="0.723213">
2Toshiba (China) Research and Development Center, Beijing, China
</affiliation>
<email confidence="0.99087">
wanghaifeng@rdc.toshiba.com.cn
</email>
<sectionHeader confidence="0.998558" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989452">
Paraphrase patterns are useful in paraphrase
recognition and generation. In this paper, we
present a pivot approach for extracting para-
phrase patterns from bilingual parallel cor-
pora, whereby the English paraphrase patterns
are extracted using the sentences in a for-
eign language as pivots. We propose a log-
linear model to compute the paraphrase likeli-
hood of two patterns and exploit feature func-
tions based on maximum likelihood estima-
tion (MLE) and lexical weighting (LW). Us-
ing the presented method, we extract over
1,000,000 pairs of paraphrase patterns from
2M bilingual sentence pairs, the precision
of which exceeds 67%. The evaluation re-
sults show that: (1) The pivot approach is
effective in extracting paraphrase patterns,
which significantly outperforms the conven-
tional method DIRT. Especially, the log-linear
model with the proposed feature functions
achieves high performance. (2) The coverage
of the extracted paraphrase patterns is high,
which is above 84%. (3) The extracted para-
phrase patterns can be classified into 5 types,
which are useful in various applications.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924853658537">
Paraphrases are different expressions that convey
the same meaning. Paraphrases are important in
plenty of natural language processing (NLP) ap-
plications, such as question answering (QA) (Lin
and Pantel, 2001; Ravichandran and Hovy, 2002),
machine translation (MT) (Kauchak and Barzilay,
2006; Callison-Burch et al., 2006), multi-document
summarization (McKeown et al., 2002), and natural
language generation (Iordanskaja et al., 1991).
Paraphrase patterns are sets of semantically
equivalent patterns, in which a pattern generally
contains two parts, i.e., the pattern words and slots.
For example, in the pattern “X solves Y”, “solves” is
the pattern word, while “X” and “Y” are slots. One
can generate a text unit (phrase or sentence) by fill-
ing the pattern slots with specific words. Paraphrase
patterns are useful in both paraphrase recognition
and generation. In paraphrase recognition, if two
text units match a pair of paraphrase patterns and the
corresponding slot-fillers are identical, they can be
identified as paraphrases. In paraphrase generation,
a text unit that matches a pattern P can be rewritten
using the paraphrase patterns of P.
A variety of methods have been proposed on para-
phrase patterns extraction (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Shinyama et al.,
2002; Barzilay and Lee, 2003; Ibrahim et al., 2003;
Pang et al., 2003; Szpektor et al., 2004). However,
these methods have some shortcomings. Especially,
the precisions of the paraphrase patterns extracted
with these methods are relatively low.
In this paper, we extract paraphrase patterns from
bilingual parallel corpora based on a pivot approach.
We assume that if two English patterns are aligned
with the same pattern in another language, they are
likely to be paraphrase patterns. This assumption
is an extension of the one presented in (Bannard
and Callison-Burch, 2005), which was used for de-
riving phrasal paraphrases from bilingual corpora.
Our method involves three steps: (1) corpus prepro-
cessing, including English monolingual dependency
</bodyText>
<page confidence="0.954641">
780
</page>
<note confidence="0.697976">
Proceedings ofACL-08: HLT, pages 780–788,
</note>
<page confidence="0.495798">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999960138888889">
parsing and English-foreign language word align-
ment, (2) aligned patterns induction, which produces
English patterns along with the aligned pivot pat-
terns in the foreign language, (3) paraphrase pat-
terns extraction, in which paraphrase patterns are ex-
tracted based on a log-linear model.
Our contributions are as follows. Firstly, we are
the first to use a pivot approach to extract paraphrase
patterns from bilingual corpora, though similar
methods have been used for learning phrasal para-
phrases. Our experiments show that the pivot ap-
proach significantly outperforms conventional meth-
ods. Secondly, we propose a log-linear model for
computing the paraphrase likelihood. Besides, we
use feature functions based on maximum likeli-
hood estimation (MLE) and lexical weighting (LW),
which are effective in extracting paraphrase patterns.
Using the proposed approach, we extract over
1,000,000 pairs of paraphrase patterns from 2M
bilingual sentence pairs, the precision of which is
above 67%. Experimental results show that the pivot
approach evidently outperforms DIRT, a well known
method that extracts paraphrase patterns from mono-
lingual corpora (Lin and Pantel, 2001). Besides, the
log-linear model is more effective than the conven-
tional model presented in (Bannard and Callison-
Burch, 2005). In addition, the coverage of the ex-
tracted paraphrase patterns is high, which is above
84%. Further analysis shows that 5 types of para-
phrase patterns can be extracted with our method,
which can by used in multiple NLP applications.
The rest of this paper is structured as follows.
Section 2 reviews related work on paraphrase pat-
terns extraction. Section 3 presents our method in
detail. We evaluate the proposed method in Section
4, and finally conclude this paper in Section 5.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999584111111111">
Paraphrase patterns have been learned and used in
information extraction (IE) and answer extraction of
QA. For example, Lin and Pantel (2001) proposed a
method (DIRT), in which they obtained paraphrase
patterns from a parsed monolingual corpus based on
an extended distributional hypothesis, where if two
paths in dependency trees tend to occur in similar
contexts it is hypothesized that the meanings of the
paths are similar. The examples of obtained para-
</bodyText>
<listItem confidence="0.995524875">
(1) X solves Y
Y is solved by X
X finds a solution to Y
(2) born in &lt;ANSWER&gt; , &lt;NAME&gt;
&lt;NAME&gt; was born on &lt;ANSWER&gt; ,
&lt;NAME&gt; (&lt;ANSWER&gt; -
(3) ORGANIZATION decides 0
ORGANIZATION confirms 0
</listItem>
<tableCaption confidence="0.971411666666667">
Table 1: Examples of paraphrase patterns extracted with
the methods of Lin and Pantel (2001), Ravichandran and
Hovy (2002), and Shinyama et al. (2002).
</tableCaption>
<bodyText confidence="0.999086774193548">
phrase patterns are shown in Table 1 (1).
Based on the same hypothesis as above, some
methods extracted paraphrase patterns from the web.
For instance, Ravichandran and Hovy (2002) de-
fined a question taxonomy for their QA system.
They then used hand-crafted examples of each ques-
tion type as queries to retrieve paraphrase patterns
from the web. For instance, for the question type
“BIRTHDAY”, The paraphrase patterns produced by
their method can be seen in Table 1 (2).
Similar methods have also been used by Ibrahim
et al. (2003) and Szpektor et al. (2004). The main
disadvantage of the above methods is that the pre-
cisions of the learned paraphrase patterns are rela-
tively low. For instance, the precisions of the para-
phrase patterns reported in (Lin and Pantel, 2001),
(Ibrahim et al., 2003), and (Szpektor et al., 2004)
are lower than 50%. Ravichandran and Hovy (2002)
did not directly evaluate the precision of the para-
phrase patterns extracted using their method. How-
ever, the performance of their method is dependent
on the hand-crafted queries for web mining.
Shinyama et al. (2002) presented a method that
extracted paraphrase patterns from multiple news ar-
ticles about the same event. Their method was based
on the assumption that NEs are preserved across
paraphrases. Thus the method acquired paraphrase
patterns from sentence pairs that share comparable
NEs. Some examples can be seen in Table 1 (3).
The disadvantage of this method is that it greatly
relies on the number of NEs in sentences. The preci-
</bodyText>
<page confidence="0.988733">
781
</page>
<figure confidence="0.9995696">
start
killing SLOT3 other people and
Palestinian
suicide bomber blew himself up in SLOT1 on SLOT2
SLOT4 end
wounding
injuring
flattened
We first take
should
TE
take
demand
market
into
consideration
STE(take)
to
levelled
detroit
‘s
*e*
building
blasted
leveled
to
*e*
the
ground
demand
into
take
PSTE(take)
*e*
a
*e*
into ashes
*e*
was
reduced
razed
leveled
to
down
rubble
market
consideration
into
consideration
building in detroit
</figure>
<figureCaption confidence="0.794952">
Figure 1: Examples of paraphrase patterns extracted by
Barzilay and Lee (2003) and Pang et al. (2003). Figure 2: Examples of a subtree and a partial subtree.
</figureCaption>
<bodyText confidence="0.99994395">
sion of the extracted patterns may sharply decrease
if the sentences do not contain enough NEs.
Barzilay and Lee (2003) applied multi-sequence
alignment (MSA) to parallel news sentences and in-
duced paraphrase patterns for generating new sen-
tences (Figure 1 (1)). Pang et al. (2003) built finite
state automata (FSA) from semantically equivalent
translation sets based on syntactic alignment. The
learned FSAs could be used in paraphrase represen-
tation and generation (Figure 1 (2)). Obviously, it
is difficult for a sentence to match such complicated
patterns, especially if the sentence is not from the
same domain in which the patterns are extracted.
Bannard and Callison-Burch (2005) first ex-
ploited bilingual corpora for phrasal paraphrase ex-
traction. They assumed that if two English phrases
e1 and e2 are aligned with the same phrase c in
another language, these two phrases may be para-
phrases. Specifically, they computed the paraphrase
probability in terms of the translation probabilities:
</bodyText>
<equation confidence="0.9857545">
�p(e2|e1) = pMLE(c|e1)pMLE(e2|c) (1)
c
</equation>
<bodyText confidence="0.999679333333333">
In Equation (1), pMLE(c|e1) and pMLE(e2|c) are
the probabilities of translating e1 to c and c to e2,
which are computed based on MLE:
</bodyText>
<equation confidence="0.994738">
count(c, e1)
pMLE(c|e1) = (2)
Ec, count(c�, e1)
</equation>
<bodyText confidence="0.999954166666667">
where count(c, e1) is the frequency count that
phrases c and e1 are aligned in the corpus.
pMLE(e2|c) is computed in the same way.
This method proved effective in extracting high
quality phrasal paraphrases. As a result, we extend
it to paraphrase pattern extraction in this paper.
</bodyText>
<sectionHeader confidence="0.99917" genericHeader="method">
3 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.99993">
3.1 Corpus Preprocessing
</subsectionHeader>
<bodyText confidence="0.999985652173913">
In this paper, we use English paraphrase patterns ex-
traction as a case study. An English-Chinese (E-
C) bilingual parallel corpus is employed for train-
ing. The Chinese part of the corpus is used as pivots
to extract English paraphrase patterns. We conduct
word alignment with Giza++ (Och and Ney, 2000) in
both directions and then apply the grow-diag heuris-
tic (Koehn et al., 2005) for symmetrization.
Since the paraphrase patterns are extracted from
dependency trees, we parse the English sentences
in the corpus with MaltParser (Nivre et al., 2007).
Let SE be an English sentence, TE the parse tree
of SE, e a word of SE, we define the subtree and
partial subtree following the definitions in (Ouan-
graoua et al., 2007). In detail, a subtree STE(e)
is a particular connected subgraph of the tree TE,
which is rooted at e and includes all the descendants
of e. A partial subtree PSTE(e) is a connected sub-
graph of the subtree STE(e), which is rooted at e but
does not necessarily include all the descendants of e.
For instance, for the sentence “We should first take
market demand into consideration”, STE(take) and
PSTE(take) are shown in Figure 21.
</bodyText>
<subsectionHeader confidence="0.999658">
3.2 Aligned Patterns Induction
</subsectionHeader>
<bodyText confidence="0.9999555">
To induce the aligned patterns, we first induce the
English patterns using the subtrees and partial sub-
trees. Then, we extract the pivot Chinese patterns
aligning to the English patterns.
</bodyText>
<footnote confidence="0.998161">
1Note that, a subtree may contain several partial subtrees. In
this paper, all the possible partial subtrees are considered when
extracting paraphrase patterns.
</footnote>
<page confidence="0.986366">
782
</page>
<listItem confidence="0.97336">
Algorithm 1: Inducing an English pattern
1: Input: words in STE(e) : wiwi+1...wj
2: Input: PE(e) = 0
3: For each wk (i &lt; k &lt; j)
4: If wk is in PSTE(e)
5: Append wk to the end of PE(e)
6: Else
7: Append POS(wk) to the end of PE(e)
8: End For
Algorithm 2: Inducing an aligned pivot pattern
1: Input: SC = t1t2...tn
2: Input: PC = 0
3: For each tl (1 &lt; l &lt; n)
4: If tl is aligned with wk in SE
5: If wk is a word in PE(e)
6: Append tl to the end of PC
7: If POS(wk) is a slot in PE(e)
8: Append POS(wk) to the end of PC
</listItem>
<sectionHeader confidence="0.511987" genericHeader="method">
9: End For
</sectionHeader>
<bodyText confidence="0.996878541666667">
Step-1 Inducing English patterns. In this paper, an
English pattern PE(e) is a string comprising words
and part-of-speech (POS) tags. Our intuition for
inducing an English pattern is that a partial sub-
tree PSTE(e) can be viewed as a unit that conveys
a definite meaning, though the words in PSTE(e)
may not be continuous. For example, PSTE(take)
in Figure 2 contains words “take ... into consid-
eration”. Therefore, we may extract “take X into
consideration” as a pattern. In addition, the words
that are in STE(e) but not in PSTE(e) (denoted as
STE(e)/PSTE(e)) are also useful for inducing pat-
terns, since they can constrain the pattern slots. In
the example in Figure 2, the word “demand” indi-
cates that a noun can be filled in the slot X and the
pattern may have the form “take NN into considera-
tion”. Based on this intuition, we induce an English
pattern PE(e) as in Algorithm 12.
For the example in Figure 2, the generated pat-
tern PE(take) is “take NN NN into considera-
tion”. Note that the patterns induced in this way
are quite specific, since the POS of each word in
STE(e)/PSTE(e) forms a slot. Such patterns are
difficult to be matched in applications. We there-
</bodyText>
<equation confidence="0.770623333333333">
2POS(wk) in Algorithm 1 denotes the POS tag of wk.
NN_2 is considered by NN_1 NN_1 consider NN_2
NN_1 考虑 NN_2 NN_1 考虑 NN_2
</equation>
<figureCaption confidence="0.995986">
Figure 3: Aligned patterns with numbered slots.
</figureCaption>
<bodyText confidence="0.99890056">
fore take an additional step to simplify the patterns.
Let ei and ej be two words in STE(e)/PSTE(e),
whose POS posi and posj are slots in PE(e). If ei
is a descendant of ej in the parse tree, we remove
posi from PE(e). For the example above, the POS
of “market” is removed, since it is the descendant of
“demand”, whose POS also forms a slot. The sim-
plified pattern is “take NN into consideration”.
Step-2 Extracting pivot patterns. For each En-
glish pattern PE(e), we extract an aligned Chinese
pivot pattern PC. Let a Chinese sentence SC be the
translation of the English sentence SE, PE(e) a pat-
tern induced from SE, we extract the pivot pattern
PC aligning to PE(e) as in Algorithm 2. Note that
the Chinese patterns are not extracted from parse
trees. They are only sequences of Chinese words
and POSes that are aligned with English patterns.
A pattern may contain two or more slots shar-
ing the same POS. To distinguish them, we assign
a number to each slot in the aligned E-C patterns. In
detail, the slots having identical POS in PC are num-
bered incrementally (i.e., 1,2,3...), while each slot in
PE(e) is assigned the same number as its aligned
slot in PC. The examples of the aligned patterns
with numbered slots are illustrated in Figure 3.
</bodyText>
<subsectionHeader confidence="0.998899">
3.3 Paraphrase Patterns Extraction
</subsectionHeader>
<bodyText confidence="0.999965875">
As mentioned above, if patterns e1 and e2 are
aligned with the same pivot pattern c, e1 and e2 may
be paraphrase patterns. The paraphrase likelihood
can be computed using Equation (1). However, we
find that using only the MLE based probabilities can
suffer from data sparseness. In order to exploit more
and richer information to estimate the paraphrase
likelihood, we propose a log-linear model:
</bodyText>
<equation confidence="0.993839666666667">
N
�score(e2je1) = exp[ Aihi(e1, e2, c)1 (3)
C i=1
</equation>
<bodyText confidence="0.994676">
where hi(e1, e2, c) is a feature function and Ai is the
</bodyText>
<page confidence="0.993116">
783
</page>
<bodyText confidence="0.8806225">
weight. In this paper, 4 feature functions are used in
our log-linear model, which include:
</bodyText>
<equation confidence="0.99998525">
h1(e1, e2, c) = scoreMLE(c|e1)
h2(e1, e2, c) = scoreMLE(e2|c)
h3(e1, e2, c) = scoreLW(c|e1)
h4(e1, e2, c) = scoreLW (e2|c)
</equation>
<bodyText confidence="0.8380645">
Feature functions h1(e1, e2, c) and h2(e1, e2, c)
are based on MLE. scoreMLE(c|e) is computed as:
</bodyText>
<equation confidence="0.978234">
scoreMLE(c|e) = log pMLE(c|e) (4)
scoreMLE(e|c) is computed in the same way.
</equation>
<bodyText confidence="0.947485">
h3(e1, e2, c) and h4(e1, e2, c) are based on LW.
LW was originally used to validate the quality of a
phrase translation pair in MT (Koehn et al., 2003). It
checks how well the words of the phrases translate
to each other. This paper uses LW to measure the
quality of aligned patterns. We define scoreLW (c|e)
as the logarithm of the lexical weight3:
</bodyText>
<equation confidence="0.977395">
scoreLW(c|e) =
</equation>
<bodyText confidence="0.999924333333333">
where a denotes the word alignment between c and
e. n is the number of words in c. ci and ej are words
of c and e. w(ci|ej) is computed as follows:
</bodyText>
<equation confidence="0.999892">
w(ci|ej) = �ecount
cou(t(c.,)j) (6)
</equation>
<bodyText confidence="0.999965">
where count(ci, ej) is the frequency count of
the aligned word pair (ci, ej) in the corpus.
scoreLW(e|c) is computed in the same manner.
In our experiments, we set a threshold T. If the
score between e1 and e2 based on Equation (3) ex-
ceeds T, e2 is extracted as the paraphrase of e1.
</bodyText>
<subsectionHeader confidence="0.975883">
3.4 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.992303">
Five parameters need to be estimated, i.e., λ1, λ2,
λ3, λ4 in Equation (3), and the threshold T. To
estimate the parameters, we first construct a devel-
opment set. In detail, we randomly sample 7,086
</bodyText>
<footnote confidence="0.9581005">
3The logarithm of the lexical weight is divided by n so as
not to penalize long patterns.
</footnote>
<bodyText confidence="0.999494647058824">
groups of aligned E-C patterns that are obtained as
described in Section 3.2. The English patterns in
each group are all aligned with the same Chinese
pivot pattern. We then extract paraphrase patterns
from the aligned patterns as described in Section 3.3.
In this process, we set λi = 1 (i = 1, ..., 4) and as-
sign T a minimum value, so as to obtain all possible
paraphrase patterns.
A total of 4,162 pairs of paraphrase patterns have
been extracted and manually labeled as “1” (correct
paraphrase patterns) or “0” (incorrect). Here, two
patterns are regarded as paraphrase patterns if they
can generate paraphrase fragments by filling the cor-
responding slots with identical words. We use gra-
dient descent algorithm (Press et al., 1992) to esti-
mate the parameters. For each set of parameters, we
compute the precision P, recall R, and f-measure
</bodyText>
<equation confidence="0.9978075">
F as: P = |set1nset2 |R =  |set1nset2 |F = 2PR
|set1 |Iset2 |, P+R,
</equation>
<bodyText confidence="0.999928">
where set1 denotes the set of paraphrase patterns ex-
tracted under the current parameters. set2 denotes
the set of manually labeled correct paraphrase pat-
terns. We select the parameters that can maximize
the F-measure on the development set4.
</bodyText>
<sectionHeader confidence="0.999732" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999575">
The E-C parallel corpus in our experiments was con-
structed using several LDC bilingual corpora5. After
filtering sentences that are too long (&gt; 40 words) or
too short (&lt; 5 words), 2,048,009 pairs of parallel
sentences were retained.
We used two constraints in the experiments to im-
prove the efficiency of computation. First, only sub-
trees containing no more than 10 words were used to
induce English patterns. Second, although any POS
tag can form a slot in the induced patterns, we only
focused on three kinds of POSes in the experiments,
i.e., nouns (tags include NN, NNS, NNP, NNPS),
verbs (VB, VBD, VBG, VBN, VBP, VBZ), and ad-
jectives (JJ, JJS, JJR). In addition, we constrained
that a pattern must contain at least one content word
</bodyText>
<footnote confidence="0.496071125">
4The parameters are: A1 = 0.0594137, A2 = 0.995936,
A3 = −0.0048954, A4 = 1.47816, T = −10.002.
5The corpora include LDC2000T46, LDC2000T47,
LDC2002E18, LDC2002T01, LDC2003E07, LDC2003E14,
LDC2003T17, LDC2004E12, LDC2004T07, LDC2004T08,
LDC2005E83, LDC2005T06, LDC2005T10, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006T04,
LDC2007T02, LDC2007T09.
</footnote>
<equation confidence="0.9866022">
log (|{j|(i�) E a}I 1:w(cilej)) (5)
,
1 n
n i=1
V(i,j)Ea
</equation>
<page confidence="0.994438">
784
</page>
<table confidence="0.9998504">
Method #PP (pairs) Precision
LL-Model 1,058,624 67.03%
MLE-Model 1,015,533 60.60%
DIRT top-1 1,179 19.67%
DIRT top-5 5,528 18.73%
</table>
<tableCaption confidence="0.999916">
Table 2: Comparison of paraphrasing methods.
</tableCaption>
<bodyText confidence="0.978411">
so as to filter patterns like “the [NN 1]”.
</bodyText>
<subsectionHeader confidence="0.987498">
4.1 Evaluation of the Log-linear Model
</subsectionHeader>
<bodyText confidence="0.9998885625">
As previously mentioned, in the log-linear model of
this paper, we use both MLE based and LW based
feature functions. In this section, we evaluate the
log-linear model (LL-Model) and compare it with
the MLE based model (MLE-Model) presented by
Bannard and Callison-Burch (2005)6.
We extracted paraphrase patterns using two mod-
els, respectively. From the results of each model,
we randomly picked 3,000 pairs of paraphrase pat-
terns to evaluate the precision. The 6,000 pairs of
paraphrase patterns were mixed and presented to the
human judges, so that the judges cannot know by
which model each pair was produced. The sampled
patterns were then manually labeled and the preci-
sion was computed as described in Section 3.4.
The number of the extracted paraphrase patterns
(#PP) and the precision are depicted in the first two
lines of Table 2. We can see that the numbers of
paraphrase patterns extracted using the two mod-
els are comparable. However, the precision of LL-
Model is significantly higher than MLE-Model.
Actually, MLE-Model is a special case of LL-
Model and the enhancement of the precision is
mainly due to the use of LW based features.
It is not surprising, since Bannard and Callison-
Burch (2005) have pointed out that word alignment
error is the major factor that influences the perfor-
mance of the methods learning paraphrases from
bilingual corpora. The LW based features validate
the quality of word alignment and assign low scores
to those aligned E-C pattern pairs with incorrect
alignment. Hence the precision can be enhanced.
</bodyText>
<footnote confidence="0.9960585">
6In this experiment, we also estimated a threshold T&apos; for
MLE-Model using the development set (T&apos; _ −5.1). The pat-
tern pairs whose score based on Equation (1) exceed T&apos; were
extracted as paraphrase patterns.
</footnote>
<subsectionHeader confidence="0.99736">
4.2 Comparison with DIRT
</subsectionHeader>
<bodyText confidence="0.999906695652174">
It is necessary to compare our method with another
paraphrase patterns extraction method. However, it
is difficult to find methods that are suitable for com-
parison. Some methods only extract paraphrase pat-
terns using news articles on certain topics (Shinyama
et al., 2002; Barzilay and Lee, 2003), while some
others need seeds as initial input (Ravichandran and
Hovy, 2002). In this paper, we compare our method
with DIRT (Lin and Pantel, 2001), which does not
need to specify topics or input seeds.
As mentioned in Section 2, DIRT learns para-
phrase patterns from a parsed monolingual corpus
based on an extended distributional hypothesis. In
our experiment, we implemented DIRT and ex-
tracted paraphrase patterns from the English part of
our bilingual parallel corpus. Our corpus is smaller
than that reported in (Lin and Pantel, 2001). To alle-
viate the data sparseness problem, we only kept pat-
terns appearing more than 10 times in the corpus for
extracting paraphrase patterns. Different from our
method, no threshold was set in DIRT. Instead, the
extracted paraphrase patterns were ranked accord-
ing to their scores. In our experiment, we kept top-5
paraphrase patterns for each target pattern.
From the extracted paraphrase patterns, we sam-
pled 600 groups for evaluation. Each group com-
prises a target pattern and its top-5 paraphrase pat-
terns. The sampled data were manually labeled and
the top-n precision was calculated as ENzn �, where
N is the number of groups and ni is the number of
correct paraphrase patterns in the top-n paraphrase
patterns of the i-th group. The top-1 and top-5 re-
sults are shown in the last two lines of Table 2. Al-
though there are more correct patterns in the top-5
results, the precision drops sequentially from top-1
to top-5 since the denominator of top-5 is 4 times
larger than that of top-1.
Obviously, the number of the extracted para-
phrase patterns is much smaller than that extracted
using our method. Besides, the precision is also
much lower. We believe that there are two reasons.
First, the extended distributional hypothesis is not
strict enough. Patterns sharing similar slot-fillers do
not necessarily have the same meaning. They may
even have the opposite meanings. For example, “X
worsens P” and “X solves P” were extracted as para-
</bodyText>
<page confidence="0.994779">
785
</page>
<table confidence="0.993731333333333">
Type Count Example
trivial change 79 (e1) all the members of [NNPS 1] (e2) all members of [NNPS 1]
phrase replacement 267 (e1) [JJ 1] economic losses (e2) [JJ 1] financial losses
phrase reordering 56 (e1) [NN 1] definition (e2) the definition of [NN 1]
structural paraphrase 71 (e1) the admission of [NNP 1] to the wto (e2) the [NNP 1] ’s wto accession
information + or - 27 (e1) [NNS 1] are in fact women (e2) [NNS 1] are women
</table>
<tableCaption confidence="0.999967">
Table 3: The statistics and examples of each type of paraphrase patterns.
</tableCaption>
<bodyText confidence="0.9478608">
phrase patterns by DIRT. The other reason is that
DIRT can only be effective for patterns appearing
plenty of times in the corpus. In other words, it seri-
ously suffers from data sparseness. We believe that
DIRT can perform better on a larger corpus.
</bodyText>
<subsectionHeader confidence="0.999136">
4.3 Pivot Pattern Constraints
</subsectionHeader>
<bodyText confidence="0.999997043478261">
As described in Section 3.2, we constrain that the
pattern words of an English pattern e must be ex-
tracted from a partial subtree. However, we do not
have such constraint on the Chinese pivot patterns.
Hence, it is interesting to investigate whether the
performance can be improved if we constrain that
the pattern words of a pivot pattern c must also be
extracted from a partial subtree.
To conduct the evaluation, we parsed the Chinese
sentences of the corpus with a Chinese dependency
parser (Liu et al., 2006). We then induced English
patterns and extracted aligned pivot patterns. For the
aligned patterns (e, c), if c’s pattern words were not
extracted from a partial subtree, the pair was filtered.
After that, we extracted paraphrase patterns, from
which we sampled 3,000 pairs for evaluation.
The results show that 736,161 pairs of paraphrase
patterns were extracted and the precision is 65.77%.
Compared with Table 2, the number of the extracted
paraphrase patterns gets smaller and the precision
also gets lower. The results suggest that the perfor-
mance of the method cannot be improved by con-
straining the extraction of pivot patterns.
</bodyText>
<subsectionHeader confidence="0.999858">
4.4 Analysis of the Paraphrase Patterns
</subsectionHeader>
<bodyText confidence="0.999977704545455">
We sampled 500 pairs of correct paraphrase pat-
terns extracted using our method and analyzed the
types. We found that there are 5 types of para-
phrase patterns, which include: (1) trivial change,
such as changes of prepositions and articles, etc; (2)
phrase replacement; (3) phrase reordering; (4) struc-
tural paraphrase, which contain both phrase replace-
ments and phrase reordering; (5) adding or reducing
information that does not change the meaning. Some
statistics and examples are shown in Table 3.
The paraphrase patterns are useful in NLP appli-
cations. Firstly, over 50% of the paraphrase patterns
are in the type of phrase replacement, which can
be used in IE pattern reformulation and sentence-
level paraphrase generation. Compared with phrasal
paraphrases, the phrase replacements in patterns are
more accurate due to the constraints of the slots.
The paraphrase patterns in the type of phrase re-
ordering can also be used in IE pattern reformula-
tion and sentence paraphrase generation. Especially,
in sentence paraphrase generation, this type of para-
phrase patterns can reorder the phrases in a sentence,
which can hardly be achieved by the conventional
MT-based generation method (Quirk et al., 2004).
The structural paraphrase patterns have the advan-
tages of both phrase replacement and phrase reorder-
ing. More paraphrase sentences can be generated
using these patterns.
The paraphrase patterns in the type of “informa-
tion + and -” are useful in sentence compression and
expansion. A sentence matching a long pattern can
be compressed by paraphrasing it using shorter pat-
terns. Similarly, a short sentence can be expanded
by paraphrasing it using longer patterns.
For the 3,000 pairs of test paraphrase patterns, we
also investigate the number and type of the pattern
slots. The results are summarized in Table 4 and 5.
From Table 4, we can see that more than 92%
of the paraphrase patterns contain only one slot,
just like the examples shown in Table 3. In addi-
tion, about 7% of the paraphrase patterns contain
two slots, such as “give [NN 1] [NN 2]” vs. “give
[NN 2] to [NN 1]”. This result suggests that our
method tends to extract short paraphrase patterns,
</bodyText>
<page confidence="0.993419">
786
</page>
<table confidence="0.99940275">
Slot No. #PP Percentage Precision
1-slot 2,780 92.67% 66.51%
2-slots 218 7.27% 73.85%
&gt;3-slots 2 &lt;1% 50.00%
</table>
<tableCaption confidence="0.998689">
Table 4: The statistics of the numbers of pattern slots.
</tableCaption>
<table confidence="0.999726">
Slot Type #PP Percentage Precision
N-slots 2,376 79.20% 66.71%
V-slots 273 9.10% 70.33%
J-slots 438 14.60% 70.32%
</table>
<tableCaption confidence="0.999934">
Table 5: The statistics of the type of pattern slots.
</tableCaption>
<bodyText confidence="0.999826571428571">
which is mainly because the data sparseness prob-
lem is more serious when extracting long patterns.
From Table 5, we can find that near 80% of the
paraphrase patterns contain noun slots, while about
9% and 15% contain verb slots and adjective slots7.
This result implies that nouns are the most typical
variables in paraphrase patterns.
</bodyText>
<subsectionHeader confidence="0.97202">
4.5 Evaluation within Context Sentences
</subsectionHeader>
<bodyText confidence="0.999921055555556">
In Section 4.1, we have evaluated the precision of
the paraphrase patterns without considering context
information. In this section, we evaluate the para-
phrase patterns within specific context sentences.
The open test set includes 119 English sentences.
We parsed the sentences with MaltParser and in-
duced patterns as described in Section 3.2. For each
pattern e in sentence 5E, we searched e’s paraphrase
patterns from the database of the extracted para-
phrase patterns. The result shows that 101 of the
119 sentences contain at least one pattern that can
be paraphrased using the extracted paraphrase pat-
terns, the coverage of which is 84.87%.
Furthermore, since a pattern may have several
paraphrase patterns, we exploited a method to au-
tomatically select the best one in the given context
sentence. In detail, a paraphrase pattern e0 of e was
reranked based on a language model (LM):
</bodyText>
<equation confidence="0.988215">
SCore(e0Je, 5E) =
ASCoreLL(e0Je) + (1 − A)SCoreLM(e0J5E) (7)
</equation>
<bodyText confidence="0.956854952380953">
7Notice that, a pattern may contain more than one type of
slots, thus the sum of the percentages is larger than 1.
Here, SCoreLL(e0Je) denotes the score based on
Equation (3). SCoreLM(e0J5E) is the LM based
score: SCoreLM(e0J5E) = nlogPLM(50E), where
50E is the sentence generated by replacing e in 5E
with e0. The language model in the experiment was
a tri-gram model trained using the English sentences
in the bilingual corpus. We empirically set A = 0.7.
The selected best paraphrase patterns in context
sentences were manually labeled. The context infor-
mation was also considered by our judges. The re-
sult shows that the precision of the best paraphrase
patterns is 59.39%. To investigate the contribution
of the LM based score, we ran the experiment again
with A = 1(ignoring the LM based score) and found
that the precision is 57.09%. It indicates that the LM
based reranking can improve the precision. How-
ever, the improvement is small. Further analysis
shows that about 70% of the correct paraphrase sub-
stitutes are in the type of phrase replacement.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999984555555556">
This paper proposes a pivot approach for extracting
paraphrase patterns from bilingual corpora. We use
a log-linear model to compute the paraphrase like-
lihood and exploit feature functions based on MLE
and LW. Experimental results show that the pivot ap-
proach is effective, which extracts over 1,000,000
pairs of paraphrase patterns from 2M bilingual sen-
tence pairs. The precision and coverage of the ex-
tracted paraphrase patterns exceed 67% and 84%,
respectively. In addition, the log-linear model with
the proposed feature functions significantly outper-
forms the conventional models. Analysis shows that
5 types of paraphrase patterns are extracted with our
method, which are useful in various applications.
In the future we wish to exploit more feature func-
tions in the log-linear model. In addition, we will try
to make better use of the context information when
replacing paraphrase patterns in context sentences.
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<footnote confidence="0.4245915">
This research was supported by National Nat-
ural Science Foundation of China (60503072,
60575042). We thank Lin Zhao, Xiaohang Qu, and
Zhenghua Li for their help in the experiments.
</footnote>
<page confidence="0.996533">
787
</page>
<sectionHeader confidence="0.998329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948769230769">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of ACL, pages 597-604.
Regina Barzilay and Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT-NAACL,
pages 16-23.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolingual
Corpora. In Proceedings of IWP, pages 57-64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgu`ere. 1991. Lexical Selection and Paraphrase in a
Meaning-Text Generation Model. In C´ecile L. Paris,
William R. Swartout, and William C. Mann (Eds.):
Natural Language Generation in Artificial Intelligence
and Computational Linguistics, pages 293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL, pages 455-462.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Ting Liu, Jin-Shan Ma, Hui-Jia Zhu, and Sheng Li. 2006.
Dependency Parsing Based on Dynamic Local Opti-
mization. In Proceedings of CoNLL-X, pages 211-215.
Kathleen R. Mckeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and Summarizing News on
a Daily Basis with Columbia’s Newsblaster. In Pro-
ceedings of HLT, pages 280-285.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A Language-
Independent System for Data-Driven Dependency
Parsing. In Natural Language Engineering 13(2): 95-
135.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
Aida Ouangraoua, Pascal Ferraro, Laurent Tichit, and
Serge Dulucq. 2007. Local Similarity between Quo-
tiented Ordered Trees. In Journal of Discrete Algo-
rithms 5(1): 23-35.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In Proceedings of HLT-NAACL, pages 102-109.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press, Cambridge, U.K., 1992, 412-420.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic Paraphrase Acquisition from News
Articles. In Proceedings of HLT, pages 40-46.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaven-
tura Coppola. 2004. Scaling Web-based Acquisition
of Entailment Relations. In Proceedings of EMNLP,
pages 41-48.
</reference>
<page confidence="0.99695">
788
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.438270">
<title confidence="0.995674">Pivot Approach for Extracting Paraphrase Patterns from Bilingual Corpora</title>
<author confidence="0.951243">Haifeng Ting Sheng</author>
<affiliation confidence="0.987108">Institute of Technology, Harbin, China</affiliation>
<email confidence="0.839446">zhaosq@ir.hit.edu.cn</email>
<email confidence="0.839446">tliu@ir.hit.edu.cn</email>
<email confidence="0.839446">lisheng@ir.hit.edu.cn</email>
<affiliation confidence="0.560307">(China) Research and Development Center, Beijing, China</affiliation>
<email confidence="0.975037">wanghaifeng@rdc.toshiba.com.cn</email>
<abstract confidence="0.999647884615385">Paraphrase patterns are useful in paraphrase recognition and generation. In this paper, we present a pivot approach for extracting paraphrase patterns from bilingual parallel corpora, whereby the English paraphrase patterns are extracted using the sentences in a foreign language as pivots. We propose a loglinear model to compute the paraphrase likelihood of two patterns and exploit feature functions based on maximum likelihood estimation (MLE) and lexical weighting (LW). Using the presented method, we extract over 1,000,000 pairs of paraphrase patterns from 2M bilingual sentence pairs, the precision of which exceeds 67%. The evaluation results show that: (1) The pivot approach is effective in extracting paraphrase patterns, which significantly outperforms the conventional method DIRT. Especially, the log-linear model with the proposed feature functions achieves high performance. (2) The coverage of the extracted paraphrase patterns is high, which is above 84%. (3) The extracted paraphrase patterns can be classified into 5 types, which are useful in various applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="3284" citStr="Bannard and Callison-Burch, 2005" startWordPosition="485" endWordPosition="488">el, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora. Our method involves three steps: (1) corpus preprocessing, including English monolingual dependency 780 Proceedings ofACL-08: HLT, pages 780–788, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics parsing and English-foreign language word alignment, (2) aligned patterns induction, which produces English patterns along with the aligned pivot patterns in the foreign language, (3) paraphrase patterns extraction, in which paraphrase patterns are extracted based on a log-linear model. Our contrib</context>
<context position="8967" citStr="Bannard and Callison-Burch (2005)" startWordPosition="1389" endWordPosition="1392">f the sentences do not contain enough NEs. Barzilay and Lee (2003) applied multi-sequence alignment (MSA) to parallel news sentences and induced paraphrase patterns for generating new sentences (Figure 1 (1)). Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment. The learned FSAs could be used in paraphrase representation and generation (Figure 1 (2)). Obviously, it is difficult for a sentence to match such complicated patterns, especially if the sentence is not from the same domain in which the patterns are extracted. Bannard and Callison-Burch (2005) first exploited bilingual corpora for phrasal paraphrase extraction. They assumed that if two English phrases e1 and e2 are aligned with the same phrase c in another language, these two phrases may be paraphrases. Specifically, they computed the paraphrase probability in terms of the translation probabilities: �p(e2|e1) = pMLE(c|e1)pMLE(e2|c) (1) c In Equation (1), pMLE(c|e1) and pMLE(e2|c) are the probabilities of translating e1 to c and c to e2, which are computed based on MLE: count(c, e1) pMLE(c|e1) = (2) Ec, count(c�, e1) where count(c, e1) is the frequency count that phrases c and e1 ar</context>
<context position="19413" citStr="Bannard and Callison-Burch (2005)" startWordPosition="3202" endWordPosition="3205"> LDC2006T04, LDC2007T02, LDC2007T09. log (|{j|(i�) E a}I 1:w(cilej)) (5) , 1 n n i=1 V(i,j)Ea 784 Method #PP (pairs) Precision LL-Model 1,058,624 67.03% MLE-Model 1,015,533 60.60% DIRT top-1 1,179 19.67% DIRT top-5 5,528 18.73% Table 2: Comparison of paraphrasing methods. so as to filter patterns like “the [NN 1]”. 4.1 Evaluation of the Log-linear Model As previously mentioned, in the log-linear model of this paper, we use both MLE based and LW based feature functions. In this section, we evaluate the log-linear model (LL-Model) and compare it with the MLE based model (MLE-Model) presented by Bannard and Callison-Burch (2005)6. We extracted paraphrase patterns using two models, respectively. From the results of each model, we randomly picked 3,000 pairs of paraphrase patterns to evaluate the precision. The 6,000 pairs of paraphrase patterns were mixed and presented to the human judges, so that the judges cannot know by which model each pair was produced. The sampled patterns were then manually labeled and the precision was computed as described in Section 3.4. The number of the extracted paraphrase patterns (#PP) and the precision are depicted in the first two lines of Table 2. We can see that the numbers of parap</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of ACL, pages 597-604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to Paraphrase: An Unsupervised Approach Using MultipleSequence Alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2735" citStr="Barzilay and Lee, 2003" startWordPosition="400" endWordPosition="403">ate a text unit (phrase or sentence) by filling the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases f</context>
<context position="8201" citStr="Barzilay and Lee (2003)" startWordPosition="1268" endWordPosition="1271">1 (3). The disadvantage of this method is that it greatly relies on the number of NEs in sentences. The preci781 start killing SLOT3 other people and Palestinian suicide bomber blew himself up in SLOT1 on SLOT2 SLOT4 end wounding injuring flattened We first take should TE take demand market into consideration STE(take) to levelled detroit ‘s *e* building blasted leveled to *e* the ground demand into take PSTE(take) *e* a *e* into ashes *e* was reduced razed leveled to down rubble market consideration into consideration building in detroit Figure 1: Examples of paraphrase patterns extracted by Barzilay and Lee (2003) and Pang et al. (2003). Figure 2: Examples of a subtree and a partial subtree. sion of the extracted patterns may sharply decrease if the sentences do not contain enough NEs. Barzilay and Lee (2003) applied multi-sequence alignment (MSA) to parallel news sentences and induced paraphrase patterns for generating new sentences (Figure 1 (1)). Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment. The learned FSAs could be used in paraphrase representation and generation (Figure 1 (2)). Obviously, it is difficult for a sent</context>
<context position="21208" citStr="Barzilay and Lee, 2003" startWordPosition="3498" endWordPosition="3501">hose aligned E-C pattern pairs with incorrect alignment. Hence the precision can be enhanced. 6In this experiment, we also estimated a threshold T&apos; for MLE-Model using the development set (T&apos; _ −5.1). The pattern pairs whose score based on Equation (1) exceed T&apos; were extracted as paraphrase patterns. 4.2 Comparison with DIRT It is necessary to compare our method with another paraphrase patterns extraction method. However, it is difficult to find methods that are suitable for comparison. Some methods only extract paraphrase patterns using news articles on certain topics (Shinyama et al., 2002; Barzilay and Lee, 2003), while some others need seeds as initial input (Ravichandran and Hovy, 2002). In this paper, we compare our method with DIRT (Lin and Pantel, 2001), which does not need to specify topics or input seeds. As mentioned in Section 2, DIRT learns paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis. In our experiment, we implemented DIRT and extracted paraphrase patterns from the English part of our bilingual parallel corpus. Our corpus is smaller than that reported in (Lin and Pantel, 2001). To alleviate the data sparseness problem, we only kept patt</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to Paraphrase: An Unsupervised Approach Using MultipleSequence Alignment. In Proceedings of HLT-NAACL, pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved Statistical Machine Translation Using Paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1734" citStr="Callison-Burch et al., 2006" startWordPosition="244" endWordPosition="247">Especially, the log-linear model with the proposed feature functions achieves high performance. (2) The coverage of the extracted paraphrase patterns is high, which is above 84%. (3) The extracted paraphrase patterns can be classified into 5 types, which are useful in various applications. 1 Introduction Paraphrases are different expressions that convey the same meaning. Paraphrases are important in plenty of natural language processing (NLP) applications, such as question answering (QA) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002), machine translation (MT) (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), multi-document summarization (McKeown et al., 2002), and natural language generation (Iordanskaja et al., 1991). Paraphrase patterns are sets of semantically equivalent patterns, in which a pattern generally contains two parts, i.e., the pattern words and slots. For example, in the pattern “X solves Y”, “solves” is the pattern word, while “X” and “Y” are slots. One can generate a text unit (phrase or sentence) by filling the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pai</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved Statistical Machine Translation Using Paraphrases. In Proceedings of HLTNAACL, pages 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Ibrahim</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
</authors>
<title>Extracting Structural Paraphrases from Aligned Monolingual Corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of IWP,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="2757" citStr="Ibrahim et al., 2003" startWordPosition="404" endWordPosition="407">or sentence) by filling the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora.</context>
<context position="6702" citStr="Ibrahim et al. (2003)" startWordPosition="1024" endWordPosition="1027">thods of Lin and Pantel (2001), Ravichandran and Hovy (2002), and Shinyama et al. (2002). phrase patterns are shown in Table 1 (1). Based on the same hypothesis as above, some methods extracted paraphrase patterns from the web. For instance, Ravichandran and Hovy (2002) defined a question taxonomy for their QA system. They then used hand-crafted examples of each question type as queries to retrieve paraphrase patterns from the web. For instance, for the question type “BIRTHDAY”, The paraphrase patterns produced by their method can be seen in Table 1 (2). Similar methods have also been used by Ibrahim et al. (2003) and Szpektor et al. (2004). The main disadvantage of the above methods is that the precisions of the learned paraphrase patterns are relatively low. For instance, the precisions of the paraphrase patterns reported in (Lin and Pantel, 2001), (Ibrahim et al., 2003), and (Szpektor et al., 2004) are lower than 50%. Ravichandran and Hovy (2002) did not directly evaluate the precision of the paraphrase patterns extracted using their method. However, the performance of their method is dependent on the hand-crafted queries for web mining. Shinyama et al. (2002) presented a method that extracted parap</context>
</contexts>
<marker>Ibrahim, Katz, Lin, 2003</marker>
<rawString>Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extracting Structural Paraphrases from Aligned Monolingual Corpora. In Proceedings of IWP, pages 57-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidija Iordanskaja</author>
<author>Richard Kittredge</author>
<author>Alain Polgu`ere</author>
</authors>
<title>Lexical Selection and Paraphrase in a Meaning-Text Generation Model. In C´ecile</title>
<date>1991</date>
<booktitle>Mann (Eds.): Natural Language Generation in Artificial Intelligence and Computational Linguistics,</booktitle>
<pages>293--312</pages>
<marker>Iordanskaja, Kittredge, Polgu`ere, 1991</marker>
<rawString>Lidija Iordanskaja, Richard Kittredge, and Alain Polgu`ere. 1991. Lexical Selection and Paraphrase in a Meaning-Text Generation Model. In C´ecile L. Paris, William R. Swartout, and William C. Mann (Eds.): Natural Language Generation in Artificial Intelligence and Computational Linguistics, pages 293-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for Automatic Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="1704" citStr="Kauchak and Barzilay, 2006" startWordPosition="240" endWordPosition="243">e conventional method DIRT. Especially, the log-linear model with the proposed feature functions achieves high performance. (2) The coverage of the extracted paraphrase patterns is high, which is above 84%. (3) The extracted paraphrase patterns can be classified into 5 types, which are useful in various applications. 1 Introduction Paraphrases are different expressions that convey the same meaning. Paraphrases are important in plenty of natural language processing (NLP) applications, such as question answering (QA) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002), machine translation (MT) (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), multi-document summarization (McKeown et al., 2002), and natural language generation (Iordanskaja et al., 1991). Paraphrase patterns are sets of semantically equivalent patterns, in which a pattern generally contains two parts, i.e., the pattern words and slots. For example, in the pattern “X solves Y”, “solves” is the pattern word, while “X” and “Y” are slots. One can generate a text unit (phrase or sentence) by filling the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition,</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation. In Proceedings of HLTNAACL, pages 455-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>Edinburgh System Description for the</booktitle>
<contexts>
<context position="10205" citStr="Koehn et al., 2005" startWordPosition="1593" endWordPosition="1596">rpus. pMLE(e2|c) is computed in the same way. This method proved effective in extracting high quality phrasal paraphrases. As a result, we extend it to paraphrase pattern extraction in this paper. 3 Proposed Method 3.1 Corpus Preprocessing In this paper, we use English paraphrase patterns extraction as a case study. An English-Chinese (EC) bilingual parallel corpus is employed for training. The Chinese part of the corpus is used as pivots to extract English paraphrase patterns. We conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag heuristic (Koehn et al., 2005) for symmetrization. Since the paraphrase patterns are extracted from dependency trees, we parse the English sentences in the corpus with MaltParser (Nivre et al., 2007). Let SE be an English sentence, TE the parse tree of SE, e a word of SE, we define the subtree and partial subtree following the definitions in (Ouangraoua et al., 2007). In detail, a subtree STE(e) is a particular connected subgraph of the tree TE, which is rooted at e and includes all the descendants of e. A partial subtree PSTE(e) is a connected subgraph of the subtree STE(e), which is rooted at e but does not necessarily i</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="15559" citStr="Koehn et al., 2003" startWordPosition="2556" endWordPosition="2559">e2, c)1 (3) C i=1 where hi(e1, e2, c) is a feature function and Ai is the 783 weight. In this paper, 4 feature functions are used in our log-linear model, which include: h1(e1, e2, c) = scoreMLE(c|e1) h2(e1, e2, c) = scoreMLE(e2|c) h3(e1, e2, c) = scoreLW(c|e1) h4(e1, e2, c) = scoreLW (e2|c) Feature functions h1(e1, e2, c) and h2(e1, e2, c) are based on MLE. scoreMLE(c|e) is computed as: scoreMLE(c|e) = log pMLE(c|e) (4) scoreMLE(e|c) is computed in the same way. h3(e1, e2, c) and h4(e1, e2, c) are based on LW. LW was originally used to validate the quality of a phrase translation pair in MT (Koehn et al., 2003). It checks how well the words of the phrases translate to each other. This paper uses LW to measure the quality of aligned patterns. We define scoreLW (c|e) as the logarithm of the lexical weight3: scoreLW(c|e) = where a denotes the word alignment between c and e. n is the number of words in c. ci and ej are words of c and e. w(ci|ej) is computed as follows: w(ci|ej) = �ecount cou(t(c.,)j) (6) where count(ci, ej) is the frequency count of the aligned word pair (ci, ej) in the corpus. scoreLW(e|c) is computed in the same manner. In our experiments, we set a threshold T. If the score between e1</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of HLT-NAACL, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>De-Kang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of Inference Rules for Question Answering.</title>
<date>2001</date>
<journal>In Natural Language Engineering</journal>
<volume>7</volume>
<issue>4</issue>
<pages>343--360</pages>
<contexts>
<context position="1620" citStr="Lin and Pantel, 2001" startWordPosition="229" endWordPosition="232">ffective in extracting paraphrase patterns, which significantly outperforms the conventional method DIRT. Especially, the log-linear model with the proposed feature functions achieves high performance. (2) The coverage of the extracted paraphrase patterns is high, which is above 84%. (3) The extracted paraphrase patterns can be classified into 5 types, which are useful in various applications. 1 Introduction Paraphrases are different expressions that convey the same meaning. Paraphrases are important in plenty of natural language processing (NLP) applications, such as question answering (QA) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002), machine translation (MT) (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), multi-document summarization (McKeown et al., 2002), and natural language generation (Iordanskaja et al., 1991). Paraphrase patterns are sets of semantically equivalent patterns, in which a pattern generally contains two parts, i.e., the pattern words and slots. For example, in the pattern “X solves Y”, “solves” is the pattern word, while “X” and “Y” are slots. One can generate a text unit (phrase or sentence) by filling the pattern slots with specific words. Paraphrase patterns </context>
<context position="4754" citStr="Lin and Pantel, 2001" startWordPosition="698" endWordPosition="701">ificantly outperforms conventional methods. Secondly, we propose a log-linear model for computing the paraphrase likelihood. Besides, we use feature functions based on maximum likelihood estimation (MLE) and lexical weighting (LW), which are effective in extracting paraphrase patterns. Using the proposed approach, we extract over 1,000,000 pairs of paraphrase patterns from 2M bilingual sentence pairs, the precision of which is above 67%. Experimental results show that the pivot approach evidently outperforms DIRT, a well known method that extracts paraphrase patterns from monolingual corpora (Lin and Pantel, 2001). Besides, the log-linear model is more effective than the conventional model presented in (Bannard and CallisonBurch, 2005). In addition, the coverage of the extracted paraphrase patterns is high, which is above 84%. Further analysis shows that 5 types of paraphrase patterns can be extracted with our method, which can by used in multiple NLP applications. The rest of this paper is structured as follows. Section 2 reviews related work on paraphrase patterns extraction. Section 3 presents our method in detail. We evaluate the proposed method in Section 4, and finally conclude this paper in Sect</context>
<context position="6111" citStr="Lin and Pantel (2001)" startWordPosition="926" endWordPosition="929">mple, Lin and Pantel (2001) proposed a method (DIRT), in which they obtained paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The examples of obtained para(1) X solves Y Y is solved by X X finds a solution to Y (2) born in &lt;ANSWER&gt; , &lt;NAME&gt; &lt;NAME&gt; was born on &lt;ANSWER&gt; , &lt;NAME&gt; (&lt;ANSWER&gt; - (3) ORGANIZATION decides 0 ORGANIZATION confirms 0 Table 1: Examples of paraphrase patterns extracted with the methods of Lin and Pantel (2001), Ravichandran and Hovy (2002), and Shinyama et al. (2002). phrase patterns are shown in Table 1 (1). Based on the same hypothesis as above, some methods extracted paraphrase patterns from the web. For instance, Ravichandran and Hovy (2002) defined a question taxonomy for their QA system. They then used hand-crafted examples of each question type as queries to retrieve paraphrase patterns from the web. For instance, for the question type “BIRTHDAY”, The paraphrase patterns produced by their method can be seen in Table 1 (2). Similar methods have also been used by Ibrahim et al. (2003) and Szpe</context>
<context position="21356" citStr="Lin and Pantel, 2001" startWordPosition="3523" endWordPosition="3526">r MLE-Model using the development set (T&apos; _ −5.1). The pattern pairs whose score based on Equation (1) exceed T&apos; were extracted as paraphrase patterns. 4.2 Comparison with DIRT It is necessary to compare our method with another paraphrase patterns extraction method. However, it is difficult to find methods that are suitable for comparison. Some methods only extract paraphrase patterns using news articles on certain topics (Shinyama et al., 2002; Barzilay and Lee, 2003), while some others need seeds as initial input (Ravichandran and Hovy, 2002). In this paper, we compare our method with DIRT (Lin and Pantel, 2001), which does not need to specify topics or input seeds. As mentioned in Section 2, DIRT learns paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis. In our experiment, we implemented DIRT and extracted paraphrase patterns from the English part of our bilingual parallel corpus. Our corpus is smaller than that reported in (Lin and Pantel, 2001). To alleviate the data sparseness problem, we only kept patterns appearing more than 10 times in the corpus for extracting paraphrase patterns. Different from our method, no threshold was set in DIRT. Instead</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>De-Kang Lin and Patrick Pantel. 2001. Discovery of Inference Rules for Question Answering. In Natural Language Engineering 7(4): 343-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Jin-Shan Ma</author>
<author>Hui-Jia Zhu</author>
<author>Sheng Li</author>
</authors>
<title>Dependency Parsing Based on Dynamic Local Optimization. In</title>
<date>2006</date>
<booktitle>Proceedings of CoNLL-X,</booktitle>
<pages>211--215</pages>
<contexts>
<context position="24493" citStr="Liu et al., 2006" startWordPosition="4060" endWordPosition="4063">a sparseness. We believe that DIRT can perform better on a larger corpus. 4.3 Pivot Pattern Constraints As described in Section 3.2, we constrain that the pattern words of an English pattern e must be extracted from a partial subtree. However, we do not have such constraint on the Chinese pivot patterns. Hence, it is interesting to investigate whether the performance can be improved if we constrain that the pattern words of a pivot pattern c must also be extracted from a partial subtree. To conduct the evaluation, we parsed the Chinese sentences of the corpus with a Chinese dependency parser (Liu et al., 2006). We then induced English patterns and extracted aligned pivot patterns. For the aligned patterns (e, c), if c’s pattern words were not extracted from a partial subtree, the pair was filtered. After that, we extracted paraphrase patterns, from which we sampled 3,000 pairs for evaluation. The results show that 736,161 pairs of paraphrase patterns were extracted and the precision is 65.77%. Compared with Table 2, the number of the extracted paraphrase patterns gets smaller and the precision also gets lower. The results suggest that the performance of the method cannot be improved by constraining</context>
</contexts>
<marker>Liu, Ma, Zhu, Li, 2006</marker>
<rawString>Ting Liu, Jin-Shan Ma, Hui-Jia Zhu, and Sheng Li. 2006. Dependency Parsing Based on Dynamic Local Optimization. In Proceedings of CoNLL-X, pages 211-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R Mckeown</author>
<author>Regina Barzilay</author>
<author>David Evans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Ani Nenkova</author>
<author>Carl Sable</author>
<author>Barry Schiffman</author>
<author>Sergey Sigelman</author>
</authors>
<title>Tracking and Summarizing News on a Daily Basis with Columbia’s Newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>280--285</pages>
<marker>Mckeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>Kathleen R. Mckeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and Summarizing News on a Daily Basis with Columbia’s Newsblaster. In Proceedings of HLT, pages 280-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>In Natural Language Engineering</journal>
<volume>13</volume>
<issue>2</issue>
<pages>95--135</pages>
<contexts>
<context position="10374" citStr="Nivre et al., 2007" startWordPosition="1618" endWordPosition="1621">n extraction in this paper. 3 Proposed Method 3.1 Corpus Preprocessing In this paper, we use English paraphrase patterns extraction as a case study. An English-Chinese (EC) bilingual parallel corpus is employed for training. The Chinese part of the corpus is used as pivots to extract English paraphrase patterns. We conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag heuristic (Koehn et al., 2005) for symmetrization. Since the paraphrase patterns are extracted from dependency trees, we parse the English sentences in the corpus with MaltParser (Nivre et al., 2007). Let SE be an English sentence, TE the parse tree of SE, e a word of SE, we define the subtree and partial subtree following the definitions in (Ouangraoua et al., 2007). In detail, a subtree STE(e) is a particular connected subgraph of the tree TE, which is rooted at e and includes all the descendants of e. A partial subtree PSTE(e) is a connected subgraph of the subtree STE(e), which is rooted at e but does not necessarily include all the descendants of e. For instance, for the sentence “We should first take market demand into consideration”, STE(take) and PSTE(take) are shown in Figure 21.</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A LanguageIndependent System for Data-Driven Dependency Parsing. In Natural Language Engineering 13(2): 95-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="10126" citStr="Och and Ney, 2000" startWordPosition="1579" endWordPosition="1582">ount(c, e1) is the frequency count that phrases c and e1 are aligned in the corpus. pMLE(e2|c) is computed in the same way. This method proved effective in extracting high quality phrasal paraphrases. As a result, we extend it to paraphrase pattern extraction in this paper. 3 Proposed Method 3.1 Corpus Preprocessing In this paper, we use English paraphrase patterns extraction as a case study. An English-Chinese (EC) bilingual parallel corpus is employed for training. The Chinese part of the corpus is used as pivots to extract English paraphrase patterns. We conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag heuristic (Koehn et al., 2005) for symmetrization. Since the paraphrase patterns are extracted from dependency trees, we parse the English sentences in the corpus with MaltParser (Nivre et al., 2007). Let SE be an English sentence, TE the parse tree of SE, e a word of SE, we define the subtree and partial subtree following the definitions in (Ouangraoua et al., 2007). In detail, a subtree STE(e) is a particular connected subgraph of the tree TE, which is rooted at e and includes all the descendants of e. A partial subtree PSTE(e) is a connected </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL, pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aida Ouangraoua</author>
<author>Pascal Ferraro</author>
<author>Laurent Tichit</author>
<author>Serge Dulucq</author>
</authors>
<title>Local Similarity between Quotiented Ordered Trees.</title>
<date>2007</date>
<journal>In Journal of Discrete Algorithms</journal>
<volume>5</volume>
<issue>1</issue>
<pages>23--35</pages>
<contexts>
<context position="10544" citStr="Ouangraoua et al., 2007" startWordPosition="1650" endWordPosition="1654">se (EC) bilingual parallel corpus is employed for training. The Chinese part of the corpus is used as pivots to extract English paraphrase patterns. We conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag heuristic (Koehn et al., 2005) for symmetrization. Since the paraphrase patterns are extracted from dependency trees, we parse the English sentences in the corpus with MaltParser (Nivre et al., 2007). Let SE be an English sentence, TE the parse tree of SE, e a word of SE, we define the subtree and partial subtree following the definitions in (Ouangraoua et al., 2007). In detail, a subtree STE(e) is a particular connected subgraph of the tree TE, which is rooted at e and includes all the descendants of e. A partial subtree PSTE(e) is a connected subgraph of the subtree STE(e), which is rooted at e but does not necessarily include all the descendants of e. For instance, for the sentence “We should first take market demand into consideration”, STE(take) and PSTE(take) are shown in Figure 21. 3.2 Aligned Patterns Induction To induce the aligned patterns, we first induce the English patterns using the subtrees and partial subtrees. Then, we extract the pivot C</context>
</contexts>
<marker>Ouangraoua, Ferraro, Tichit, Dulucq, 2007</marker>
<rawString>Aida Ouangraoua, Pascal Ferraro, Laurent Tichit, and Serge Dulucq. 2007. Local Similarity between Quotiented Ordered Trees. In Journal of Discrete Algorithms 5(1): 23-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>102--109</pages>
<contexts>
<context position="2776" citStr="Pang et al., 2003" startWordPosition="408" endWordPosition="411">g the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora. Our method involve</context>
<context position="8224" citStr="Pang et al. (2003)" startWordPosition="1273" endWordPosition="1276">his method is that it greatly relies on the number of NEs in sentences. The preci781 start killing SLOT3 other people and Palestinian suicide bomber blew himself up in SLOT1 on SLOT2 SLOT4 end wounding injuring flattened We first take should TE take demand market into consideration STE(take) to levelled detroit ‘s *e* building blasted leveled to *e* the ground demand into take PSTE(take) *e* a *e* into ashes *e* was reduced razed leveled to down rubble market consideration into consideration building in detroit Figure 1: Examples of paraphrase patterns extracted by Barzilay and Lee (2003) and Pang et al. (2003). Figure 2: Examples of a subtree and a partial subtree. sion of the extracted patterns may sharply decrease if the sentences do not contain enough NEs. Barzilay and Lee (2003) applied multi-sequence alignment (MSA) to parallel news sentences and induced paraphrase patterns for generating new sentences (Figure 1 (1)). Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment. The learned FSAs could be used in paraphrase representation and generation (Figure 1 (2)). Obviously, it is difficult for a sentence to match such comp</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences. In Proceedings of HLT-NAACL, pages 102-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C: The Art of Scientific Computing.</title>
<date>1992</date>
<pages>412--420</pages>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, U.K.,</location>
<contexts>
<context position="17291" citStr="Press et al., 1992" startWordPosition="2864" endWordPosition="2867">all aligned with the same Chinese pivot pattern. We then extract paraphrase patterns from the aligned patterns as described in Section 3.3. In this process, we set λi = 1 (i = 1, ..., 4) and assign T a minimum value, so as to obtain all possible paraphrase patterns. A total of 4,162 pairs of paraphrase patterns have been extracted and manually labeled as “1” (correct paraphrase patterns) or “0” (incorrect). Here, two patterns are regarded as paraphrase patterns if they can generate paraphrase fragments by filling the corresponding slots with identical words. We use gradient descent algorithm (Press et al., 1992) to estimate the parameters. For each set of parameters, we compute the precision P, recall R, and f-measure F as: P = |set1nset2 |R = |set1nset2 |F = 2PR |set1 |Iset2 |, P+R, where set1 denotes the set of paraphrase patterns extracted under the current parameters. set2 denotes the set of manually labeled correct paraphrase patterns. We select the parameters that can maximize the F-measure on the development set4. 4 Experiments The E-C parallel corpus in our experiments was constructed using several LDC bilingual corpora5. After filtering sentences that are too long (&gt; 40 words) or too short (</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, Cambridge, U.K., 1992, 412-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual Machine Translation for Paraphrase Generation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>142--149</pages>
<contexts>
<context position="26375" citStr="Quirk et al., 2004" startWordPosition="4358" endWordPosition="4361">rase patterns are in the type of phrase replacement, which can be used in IE pattern reformulation and sentencelevel paraphrase generation. Compared with phrasal paraphrases, the phrase replacements in patterns are more accurate due to the constraints of the slots. The paraphrase patterns in the type of phrase reordering can also be used in IE pattern reformulation and sentence paraphrase generation. Especially, in sentence paraphrase generation, this type of paraphrase patterns can reorder the phrases in a sentence, which can hardly be achieved by the conventional MT-based generation method (Quirk et al., 2004). The structural paraphrase patterns have the advantages of both phrase replacement and phrase reordering. More paraphrase sentences can be generated using these patterns. The paraphrase patterns in the type of “information + and -” are useful in sentence compression and expansion. A sentence matching a long pattern can be compressed by paraphrasing it using shorter patterns. Similarly, a short sentence can be expanded by paraphrasing it using longer patterns. For the 3,000 pairs of test paraphrase patterns, we also investigate the number and type of the pattern slots. The results are summariz</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual Machine Translation for Paraphrase Generation. In Proceedings of EMNLP, pages 142-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning Surface Text Patterns for a Question Answering System. In</title>
<date>2002</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="1650" citStr="Ravichandran and Hovy, 2002" startWordPosition="233" endWordPosition="236"> paraphrase patterns, which significantly outperforms the conventional method DIRT. Especially, the log-linear model with the proposed feature functions achieves high performance. (2) The coverage of the extracted paraphrase patterns is high, which is above 84%. (3) The extracted paraphrase patterns can be classified into 5 types, which are useful in various applications. 1 Introduction Paraphrases are different expressions that convey the same meaning. Paraphrases are important in plenty of natural language processing (NLP) applications, such as question answering (QA) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002), machine translation (MT) (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), multi-document summarization (McKeown et al., 2002), and natural language generation (Iordanskaja et al., 1991). Paraphrase patterns are sets of semantically equivalent patterns, in which a pattern generally contains two parts, i.e., the pattern words and slots. For example, in the pattern “X solves Y”, “solves” is the pattern word, while “X” and “Y” are slots. One can generate a text unit (phrase or sentence) by filling the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase </context>
<context position="6141" citStr="Ravichandran and Hovy (2002)" startWordPosition="930" endWordPosition="933">001) proposed a method (DIRT), in which they obtained paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The examples of obtained para(1) X solves Y Y is solved by X X finds a solution to Y (2) born in &lt;ANSWER&gt; , &lt;NAME&gt; &lt;NAME&gt; was born on &lt;ANSWER&gt; , &lt;NAME&gt; (&lt;ANSWER&gt; - (3) ORGANIZATION decides 0 ORGANIZATION confirms 0 Table 1: Examples of paraphrase patterns extracted with the methods of Lin and Pantel (2001), Ravichandran and Hovy (2002), and Shinyama et al. (2002). phrase patterns are shown in Table 1 (1). Based on the same hypothesis as above, some methods extracted paraphrase patterns from the web. For instance, Ravichandran and Hovy (2002) defined a question taxonomy for their QA system. They then used hand-crafted examples of each question type as queries to retrieve paraphrase patterns from the web. For instance, for the question type “BIRTHDAY”, The paraphrase patterns produced by their method can be seen in Table 1 (2). Similar methods have also been used by Ibrahim et al. (2003) and Szpektor et al. (2004). The main d</context>
<context position="21285" citStr="Ravichandran and Hovy, 2002" startWordPosition="3510" endWordPosition="3513">sion can be enhanced. 6In this experiment, we also estimated a threshold T&apos; for MLE-Model using the development set (T&apos; _ −5.1). The pattern pairs whose score based on Equation (1) exceed T&apos; were extracted as paraphrase patterns. 4.2 Comparison with DIRT It is necessary to compare our method with another paraphrase patterns extraction method. However, it is difficult to find methods that are suitable for comparison. Some methods only extract paraphrase patterns using news articles on certain topics (Shinyama et al., 2002; Barzilay and Lee, 2003), while some others need seeds as initial input (Ravichandran and Hovy, 2002). In this paper, we compare our method with DIRT (Lin and Pantel, 2001), which does not need to specify topics or input seeds. As mentioned in Section 2, DIRT learns paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis. In our experiment, we implemented DIRT and extracted paraphrase patterns from the English part of our bilingual parallel corpus. Our corpus is smaller than that reported in (Lin and Pantel, 2001). To alleviate the data sparseness problem, we only kept patterns appearing more than 10 times in the corpus for extracting paraphrase pat</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning Surface Text Patterns for a Question Answering System. In Proceedings of ACL, pages 41-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
</authors>
<title>Automatic Paraphrase Acquisition from News Articles.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>40--46</pages>
<contexts>
<context position="2711" citStr="Shinyama et al., 2002" startWordPosition="396" endWordPosition="399">re slots. One can generate a text unit (phrase or sentence) by filling the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for derivi</context>
<context position="6169" citStr="Shinyama et al. (2002)" startWordPosition="935" endWordPosition="938">which they obtained paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The examples of obtained para(1) X solves Y Y is solved by X X finds a solution to Y (2) born in &lt;ANSWER&gt; , &lt;NAME&gt; &lt;NAME&gt; was born on &lt;ANSWER&gt; , &lt;NAME&gt; (&lt;ANSWER&gt; - (3) ORGANIZATION decides 0 ORGANIZATION confirms 0 Table 1: Examples of paraphrase patterns extracted with the methods of Lin and Pantel (2001), Ravichandran and Hovy (2002), and Shinyama et al. (2002). phrase patterns are shown in Table 1 (1). Based on the same hypothesis as above, some methods extracted paraphrase patterns from the web. For instance, Ravichandran and Hovy (2002) defined a question taxonomy for their QA system. They then used hand-crafted examples of each question type as queries to retrieve paraphrase patterns from the web. For instance, for the question type “BIRTHDAY”, The paraphrase patterns produced by their method can be seen in Table 1 (2). Similar methods have also been used by Ibrahim et al. (2003) and Szpektor et al. (2004). The main disadvantage of the above met</context>
<context position="21183" citStr="Shinyama et al., 2002" startWordPosition="3494" endWordPosition="3497"> assign low scores to those aligned E-C pattern pairs with incorrect alignment. Hence the precision can be enhanced. 6In this experiment, we also estimated a threshold T&apos; for MLE-Model using the development set (T&apos; _ −5.1). The pattern pairs whose score based on Equation (1) exceed T&apos; were extracted as paraphrase patterns. 4.2 Comparison with DIRT It is necessary to compare our method with another paraphrase patterns extraction method. However, it is difficult to find methods that are suitable for comparison. Some methods only extract paraphrase patterns using news articles on certain topics (Shinyama et al., 2002; Barzilay and Lee, 2003), while some others need seeds as initial input (Ravichandran and Hovy, 2002). In this paper, we compare our method with DIRT (Lin and Pantel, 2001), which does not need to specify topics or input seeds. As mentioned in Section 2, DIRT learns paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis. In our experiment, we implemented DIRT and extracted paraphrase patterns from the English part of our bilingual parallel corpus. Our corpus is smaller than that reported in (Lin and Pantel, 2001). To alleviate the data sparseness p</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo. 2002. Automatic Paraphrase Acquisition from News Articles. In Proceedings of HLT, pages 40-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
</authors>
<title>Hristo Tanev, Ido Dagan and Bonaventura Coppola.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>41--48</pages>
<marker>Szpektor, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaventura Coppola. 2004. Scaling Web-based Acquisition of Entailment Relations. In Proceedings of EMNLP, pages 41-48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>