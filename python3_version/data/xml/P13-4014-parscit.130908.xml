<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010788">
<title confidence="0.969847">
QuEst - A translation quality estimation framework
</title>
<author confidence="0.999247">
Lucia Specia§, Kashif Shah§, Jose G. C. de Souza† and Trevor Cohn§
</author>
<affiliation confidence="0.9868895">
§Department of Computer Science
University of Sheffield, UK
</affiliation>
<email confidence="0.989186">
{l.specia,kashif.shah,t.cohn}@sheffield.ac.uk
</email>
<author confidence="0.686661">
†Fondazione Bruno Kessler
</author>
<affiliation confidence="0.997927">
University of Trento, Italy
</affiliation>
<email confidence="0.991873">
desouza@fbk.eu
</email>
<sectionHeader confidence="0.997324" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999790846153846">
We describe QUEST, an open source
framework for machine translation quality
estimation. The framework allows the ex-
traction of several quality indicators from
source segments, their translations, exter-
nal resources (corpora, language models,
topic models, etc.), as well as language
tools (parsers, part-of-speech tags, etc.). It
also provides machine learning algorithms
to build quality estimation models. We
benchmark the framework on a number of
datasets and discuss the efficacy of fea-
tures and algorithms.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.871164">
As Machine Translation (MT) systems become
widely adopted both for gisting purposes and to
produce professional quality translations, auto-
matic methods are needed for predicting the qual-
ity of a translated segment. This is referred to as
Quality Estimation (QE). Different from standard
MT evaluation metrics, QE metrics do not have
access to reference (human) translations; they are
aimed at MT systems in use. QE has a number of
applications, including:
</bodyText>
<listItem confidence="0.9859851">
• Deciding which segments need revision by a
translator (quality assurance);
• Deciding whether a reader gets a reliable gist
of the text;
• Estimating how much effort it will be needed
to post-edit a segment;
• Selecting among alternative translations pro-
duced by different MT systems;
• Deciding whether the translation can be used
for self-training of MT systems.
</listItem>
<bodyText confidence="0.999452444444444">
Work in QE for MT started in the early 2000’s,
inspired by the confidence scores used in Speech
Recognition: mostly the estimation of word pos-
terior probabilities. Back then it was called confi-
dence estimation, which we believe is a narrower
term. A 6-week workshop on the topic at John
Hopkins University in 2003 (Blatz et al., 2004)
had as goal to estimate automatic metrics such as
BLEU (Papineni et al., 2002) and WER. These
metrics are difficult to interpret, particularly at the
sentence-level, and results of their very many trials
proved unsuccessful. The overall quality of MT
was considerably lower at the time, and therefore
pinpointing the very few good quality segments
was a hard problem. No software nor datasets
were made available after the workshop.
A new surge of interest in the field started re-
cently, motivated by the widespread used of MT
systems in the translation industry, as a conse-
quence of better translation quality, more user-
friendly tools, and higher demand for translation.
In order to make MT maximally useful in this
scenario, a quantification of the quality of trans-
lated segments similar to “fuzzy match scores”
from translation memory systems is needed. QE
work addresses this problem by using more com-
plex metrics that go beyond matching the source
segment with previously translated data. QE can
also be useful for end-users reading translations
for gisting, particularly those who cannot read the
source language.
QE nowadays focuses on estimating more inter-
pretable metrics. “Quality” is defined according to
the application: post-editing, gisting, etc. A num-
ber of positive results have been reported. Exam-
ples include improving post-editing efficiency by
filtering out low quality segments which would re-
quire more effort or time to correct than translating
from scratch (Specia et al., 2009; Specia, 2011),
selecting high quality segments to be published as
they are, without post-editing (Soricut and Echi-
habi, 2010), selecting a translation from either
an MT system or a translation memory for post-
editing (He et al., 2010), selecting the best trans-
lation from multiple MT systems (Specia et al.,
</bodyText>
<page confidence="0.984004">
79
</page>
<bodyText confidence="0.972801557692308">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
2010), and highlighting sub-segments that need re-
vision (Bach et al., 2011).
QE is generally addressed as a supervised ma-
chine learning task using a variety of algorithms to
induce models from examples of translations de-
scribed through a number of features and anno-
tated for quality. For an overview of various al-
gorithms and features we refer the reader to the
WMT12 shared task on QE (Callison-Burch et
al., 2012). Most of the research work lies on
deciding which aspects of quality are more rel-
evant for a given task and designing feature ex-
tractors for them. While simple features such as
counts of tokens and language model scores can be
easily extracted, feature engineering for more ad-
vanced and useful information can be quite labour-
intensive. Different language pairs or optimisation
against specific quality scores (e.g., post-editing
time vs translation adequacy) can benefit from
very different feature sets.
QUEST, our framework for quality estimation,
provides a wide range of feature extractors from
source and translation texts and external resources
and tools (Section 2). These go from simple,
language-independent features, to advanced, lin-
guistically motivated features. They include fea-
tures that rely on information from the MT sys-
tem that generated the translations, and features
that are oblivious to the way translations were
produced (Section 2.1). In addition, by inte-
grating a well-known machine learning toolkit,
scikit-learn,1 and algorithms that are known
to perform well on this task, QUEST provides a
simple and effective way of experimenting with
techniques for feature selection and model build-
ing, as well as parameter optimisation through grid
search (Section 2.2). In Section 3 we present
experiments using the framework with nine QE
datasets.
In addition to providing a practical platform
for quality estimation, by freeing researchers from
feature engineering, QUEST will facilitate work
on the learning aspect of the problem. Quality
estimation poses several machine learning chal-
lenges, such as the fact that it can exploit a large,
diverse, but often noisy set of information sources,
with a relatively small number of annotated data
points, and it relies on human annotations that are
often inconsistent due to the subjectivity of the
task (quality judgements). Moreover, QE is highly
</bodyText>
<footnote confidence="0.943935">
1http://scikit-learn.org/
</footnote>
<bodyText confidence="0.999591833333333">
non-linear: unlike many other problems in lan-
guage processing, considerable improvements can
be achieved using non-linear kernel techniques.
Also, different applications for the quality predic-
tions may benefit from different machine learn-
ing techniques, an aspect that has been mostly ne-
glected so far. Finally, the framework will also
facilitate research on ways of using quality predic-
tions in novel extrinsic tasks, such as self-training
of statistical machine translation systems, and for
estimating quality in other text output applications
such as text summarisation.
</bodyText>
<sectionHeader confidence="0.97573" genericHeader="method">
2 The QUEST framework
</sectionHeader>
<bodyText confidence="0.99998508">
QUEST consists of two main modules: a feature
extraction module and a machine learning mod-
ule. The first module provides a number of feature
extractors, including the most commonly used fea-
tures in the literature and by systems submitted to
the WMT12 shared task on QE (Callison-Burch et
al., 2012). More than 15 researchers from 10 in-
stitutions contributed to it as part of the QUEST
project.2 It is implemented in Java and provides
abstract classes for features, resources and pre-
processing steps so that extractors for new features
can be easily added.
The basic functioning of the feature extraction
module requires raw text files with the source and
translation texts, and a few resources (where avail-
able) such as the source MT training corpus and
language models of source and target. Configura-
tion files are used to indicate the resources avail-
able and a list of features that should be extracted.
The machine learning module provides
scripts connecting the feature files with the
scikit-learn toolkit. It also uses GPy, a
Python toolkit for Gaussian Processes regression,
which outperformed algorithms commonly used
for the task such as SVM regressors.
</bodyText>
<subsectionHeader confidence="0.996654">
2.1 Feature sets
</subsectionHeader>
<bodyText confidence="0.999322375">
In Figure 1 we show the types of features that
can be extracted in QUEST. Although the text
unit for which features are extracted can be of any
length, most features are more suitable for sen-
tences. Therefore, a “segment” here denotes a sen-
tence.
From the source segments QUEST can extract
features that attempt to quantify the complexity
</bodyText>
<footnote confidence="0.954705">
2http://www.dcs.shef.ac.uk/˜lucia/
projects/quest.html
</footnote>
<page confidence="0.995104">
80
</page>
<figureCaption confidence="0.999882">
Figure 1: Families of features in QUEST.
</figureCaption>
<bodyText confidence="0.979469333333333">
of translating those segments, or how unexpected
they are given what is known to the MT system.
Examples of features include:
</bodyText>
<listItem confidence="0.9918664">
• number of tokens in the source segment;
• language model (LM) probability of source
segment using the source side of the parallel
corpus used to train the MT system as LM;
• percentage of source 1–3-grams observed in
different frequency quartiles of the source
side of the MT training corpus;
• average number of translations per source
word in the segment as given by IBM 1
model with probabilities thresholded in dif-
ferent ways.
From the translated segments QUEST can ex-
tract features that attempt to measure the fluency
of such translations. Examples of features include:
• number of tokens in the target segment;
• average number of occurrences of the target
word within the target segment;
• LM probability of target segment using a
large corpus of the target language to build
the LM.
</listItem>
<bodyText confidence="0.998196285714286">
From the comparison between the source and
target segments, QUEST can extract adequacy
features, which attempt to measure whether the
structure and meaning of the source are pre-
served in the translation. Some of these are based
on word-alignment information as provided by
GIZA++. Features include:
</bodyText>
<listItem confidence="0.9988094">
• ratio of number of tokens in source and target
segments;
• ratio of brackets and punctuation symbols in
source and target segments;
• ratio of percentages of numbers, content- /
non-content words in the source &amp; target seg-
ments;
• ratio of percentage of nouns/verbs/etc in the
source and target segments;
• proportion of dependency relations between
(aligned) constituents in source and target
segments;
• difference between the depth of the syntactic
trees of the source and target segments;
• difference between the number of
PP/NP/VP/ADJP/ADVP/CONJP phrases in
the source and target;
• difference between the number of per-
son/location/organization entities in source
and target sentences;
• proportion of person/location/organization
entities in source aligned to the same type of
entities in target segment;
• percentage of direct object personal or pos-
sessive pronouns incorrectly translated.
</listItem>
<bodyText confidence="0.9990934375">
When available, information from the MT sys-
tem used to produce the translations can be very
useful, particularly for statistical machine transla-
tion (SMT). These features can provide an indi-
cation of the confidence of the MT system in the
translations. They are called “glass-box” features,
to distinguish them from MT system-independent,
“black-box” features. To extract these features,
QUEST assumes the output of Moses-like SMT
systems, taking into account word- and phrase-
alignment information, a dump of the decoder’s
standard output (search graph information), global
model score and feature values, n-best lists, etc.
For other SMT systems, it can also take an XML
file with relevant information. Examples of glass-
box features include:
</bodyText>
<listItem confidence="0.875323266666667">
• features and global score of the SMT system;
• number of distinct hypotheses in the n-best
list;
• 1–3-gram LM probabilities using translations
in the n-best to train the LM;
• average size of the target phrases;
• proportion of pruned search graph nodes;
• proportion of recombined graph nodes.
We note that some of these features are
language-independent by definition (such as the
confidence features), while others can be depen-
dent on linguistic resources (such as POS taggers),
or very language-specific, such as the incorrect
translation of pronouns, which was designed for
Arabic-English QE.
</listItem>
<bodyText confidence="0.952954333333333">
Some word-level features have also been im-
plemented: they include standard word posterior
probabilities and n-gram probabilities for each tar-
</bodyText>
<figure confidence="0.997447909090909">
Source text
Complexity
indicators
Confidence
indicators
MT system
Adequacy
indicators
Fluency
indicators
Translation
</figure>
<page confidence="0.990137">
81
</page>
<bodyText confidence="0.999812375">
get word. These can also be averaged across the
whole sentence to provide sentence-level value.
The complete list of features available is given
as part of QUEST’s documentation. At the current
stage, the number of BB features varies from 80
to 123 depending on the language pair, while GB
features go from 39 to 48 depending on the SMT
system used (see Section 3).
</bodyText>
<subsectionHeader confidence="0.99604">
2.2 Machine learning
</subsectionHeader>
<bodyText confidence="0.99990075">
QUEST provides a command-line interface mod-
ule for the scikit-learn library implemented
in Python. This module is completely indepen-
dent from the feature extraction code and it uses
the extracted feature sets to build QE models.
The dependencies are the scikit-learn li-
brary and all its dependencies (such as NumPy3
and SciPy4). The module can be configured to
run different regression and classification algo-
rithms, feature selection methods and grid search
for hyper-parameter optimisation.
The pipeline with feature selection and hyper-
parameter optimisation can be set using a con-
figuration file. Currently, the module has an
interface for Support Vector Regression (SVR),
Support Vector Classification, and Lasso learn-
ing algorithms. They can be used in conjunction
with the feature selection algorithms (Randomised
Lasso and Randomised decision trees) and the grid
search implementation of scikit-learn to fit
an optimal model of a given dataset.
Additionally, QUEST includes Gaussian Pro-
cess (GP) regression (Rasmussen and Williams,
2006) using the GPy toolkit.5 GPs are an ad-
vanced machine learning framework incorporating
Bayesian non-parametrics and kernel machines,
and are widely regarded as state of the art for
regression. Empirically we found the perfor-
mance to be similar to SVR on most datasets,
with slightly worse MAE and better RMSE.6 In
contrast to SVR, inference in GP regression can
be expressed analytically and the model hyper-
parameters optimised directly using gradient as-
cent, thus avoiding the need for costly grid search.
This also makes the method very suitable for fea-
ture selection.
</bodyText>
<footnote confidence="0.979975833333333">
3http://www.numpy.org/
4http://www.scipy.org/
5https://github.com/SheffieldML/GPy
6This follows from the optimisation objective: GPs use a
quadratic loss (the log-likelihood of a Gaussian) compared to
SVR which penalises absolute margin violations.
</footnote>
<table confidence="0.999706666666667">
Data Training Test
WMT12 (en-es) 1,832 422
EAMT11 (en-es) 900 64
EAMT11 (fr-en) 2,300 225
EAMT09-s1-s4 (en-es) 3,095 906
GALE11-s1-s2 (ar-en) 2,198 387
</table>
<tableCaption confidence="0.99765">
Table 1: Number of sentences used for training
</tableCaption>
<bodyText confidence="0.60673">
and testing in our datasets.
</bodyText>
<sectionHeader confidence="0.988989" genericHeader="method">
3 Benchmarking
</sectionHeader>
<bodyText confidence="0.999356">
In this section we benchmark QUEST on nine ex-
isting datasets using feature selection and learning
algorithms known to perform well in the task.
</bodyText>
<subsectionHeader confidence="0.960674">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9998166">
The statistics of the datasets used in the experi-
ments are shown in Table 1.7
WMT12 English-Spanish sentence translations
produced by an SMT system and judged for
post-editing effort in 1-5 (worst-best), taking a
weighted average of three annotators.
EAMT11 English-Spanish (EAMT11-en-es)
and French-English (EAMT11-fr-en) sentence
translations judged for post-editing effort in 1-4.
EAMT09 English sentences translated by four
SMT systems into Spanish and scored for post-
editing effort in 1-4. Systems are denoted by s1-s4.
GALE11 Arabic sentences translated by two
SMT systems into English and scored for ade-
quacy in 1-4. Systems are denoted by s1-s2.
</bodyText>
<subsectionHeader confidence="0.997957">
3.2 Settings
</subsectionHeader>
<bodyText confidence="0.999890714285714">
Amongst the various learning algorithms available
in QUEST, to make our results comparable we se-
lected SVR with radial basis function (RBF) ker-
nel, which has been shown to perform very well
in this task (Callison-Burch et al., 2012). The op-
timisation of parameters is done with grid search
using the following ranges of values:
</bodyText>
<listItem confidence="0.999727">
• penalty parameter C: [1, 10, 10]
• γ: [0.0001, 0.1, 10]
• c: [0.1, 0.2, 10]
</listItem>
<bodyText confidence="0.99561125">
where elements in list denote beginning, end and
number of samples to generate, respectively.
For feature selection, we have experimented
with two techniques: Randomised Lasso and
</bodyText>
<footnote confidence="0.992682">
7The datasets can be downloaded from http://www.
dcs.shef.ac.uk/˜lucia/resources.html
</footnote>
<page confidence="0.998905">
82
</page>
<bodyText confidence="0.996642481481481">
Gaussian Processes. Randomised Lasso (Mein-
shausen and B¨uhlmann, 2010) repeatedly resam-
ples the training data and fits a Lasso regression
model on each sample. A feature is said to be se-
lected if it was selected (i.e., assigned a non-zero
weight) in at least 25% of the samples (we do this
1000 times). This strategy improves the robust-
ness of Lasso in the presence of high dimensional
and correlated inputs.
Feature selection with Gaussian Processes is
done by fitting per-feature RBF widths (also
known as the automatic relevance determination
kernel). The RBF width denotes the importance
of a feature, the narrower the RBF the more impor-
tant a change in the feature value is to the model
prediction. To make the results comparable with
our baseline systems we select the 17 top ranked
features and then train a SVR on these features.8
As feature sets, we select all features available
in QUEST for each of our datasets. We differen-
tiate between black-box (BB) and glass-box (GB)
features, as only BB are available for all datasets
(we did not have access to the MT systems that
produced the other datasets). For the WMT12 and
GALE11 datasets, we experimented with both BB
and GB features. For each dataset we build four
systems:
</bodyText>
<listItem confidence="0.997831125">
• BL: 17 baseline features that performed well
across languages in previous work and were
used as baseline in the WMT12 QE task.
• AF: All features available for dataset.
• FS: Feature selection for automatic ranking
and selection of top features with:
– RL: Randomised Lasso.
– GP: Gaussian Process.
</listItem>
<bodyText confidence="0.820884333333333">
Mean Absolute Error (MAE) and Root Mean
Squared Error (RMSE) are used to evaluate the
models.
</bodyText>
<subsectionHeader confidence="0.932376">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.967204666666667">
The error scores for all datasets with BB features
are reported in Table 2, while Table 3 shows the re-
sults with GB features, and Table 4 the results with
BB and GB features together. For each table and
dataset, bold-faced figures are significantly better
than all others (paired t-test with p ≤ 0.05).
It can be seen from the results that adding more
BB features (systems AF) improves the results in
most cases as compared to the baseline systems
</bodyText>
<footnote confidence="0.9883165">
8More features resulted in further performance gains on
most tasks, with 25–35 features giving the best results.
</footnote>
<table confidence="0.999970054054054">
Dataset System #feats. MAE RMSE
WMT12 BL 17 0.6802 0.8192
AF 80 0.6703 0.8373
FS(RL) 69 0.6628 0.8107
FS(GP) 17 0.6537 0.8014
EAMT11(en-es) BL 17 0.4867 0.6288
AF 80 0.4696 0.5438
FS(RL) 29 0.4657 0.5424
FS(GP) 17 0.4640 0.5420
EAMT11(fr-en) BL 17 0.4387 0.6357
AF 80 0.4275 0.6211
FS(RL) 65 0.4266 0.6196
FS(GP) 17 0.4240 0.6189
EAMT09-s1 BL 17 0.5294 0.6643
AF 80 0.5235 0.6558
FS(RL) 73 0.5190 0.6516
FS(GP) 17 0.5195 0.6511
EAMT09-s2 BL 17 0.4604 0.5856
AF 80 0.4734 0.5973
FS(RL) 59 0.4601 0.5837
FS(GP) 17 0.4610 0.5825
EAMT09-s3 BL 17 0.5321 0.6643
AF 80 0.5437 0.6827
FS(RL) 67 0.5338 0.6627
FS(GP) 17 0.5320 0.6630
EAMT09-s4 BL 17 0.3583 0.4953
AF 80 0.3569 0.5000
FS(RL) 40 0.3554 0.4995
FS(GP) 17 0.3560 0.4949
GALE11-s1 BL 17 0.5456 0.6905
AF 123 0.5359 0.6665
FS(RL) 56 0.5358 0.6649
FS(GP) 17 0.5410 0.6721
GALE11-s2 BL 17 0.5532 0.7177
AF 123 0.5381 0.6933
FS(RL) 54 0.5369 0.6955
FS(GP) 17 0.5424 0.6999
</table>
<tableCaption confidence="0.997012">
Table 2: Results with BB features.
</tableCaption>
<table confidence="0.9999006">
Dataset System #feats. MAE RMSE
WMT12 AF 47 0.7036 0.8476
FS(RL) 26 0.6821 0.8388
FS(GP) 17 0.6771 0.8308
GALE11-s1 AF 39 0.5720 0.7392
FS(RL) 46 0.5691 0.7388
FS(GP) 17 0.5711 0.7378
GALE11-s2 AF 48 0.5510 0.6977
FS(RL) 46 0.5512 0.6970
FS(GP) 17 0.5501 0.6978
</table>
<tableCaption confidence="0.99918">
Table 3: Results with GB features.
</tableCaption>
<table confidence="0.9998785">
Dataset System #feats. MAE RMSE
WMT12 AF 127 0.7165 0.8476
FS(RL) 26 0.6601 0.8098
FS(GP) 17 0.6501 0.7989
GALE11-s1 AF 162 0.5437 0.6741
FS(RL) 69 0.5310 0.6681
FS(GP) 17 0.5370 0.6701
GALE11-s2 AF 171 0.5222 0.6499
FS(RL) 82 0.5152 0.6421
FS(GP) 17 0.5121 0.6384
</table>
<tableCaption confidence="0.999959">
Table 4: Results with BB and GB features.
</tableCaption>
<page confidence="0.998314">
83
</page>
<bodyText confidence="0.999836636363636">
BL, however, in some cases the improvements are
not significant. This behaviour is to be expected
as adding more features may bring more relevant
information, but at the same time it makes the rep-
resentation more sparse and the learning prone to
overfitting. In most cases, feature selection with
both or either RL and GP improves over all fea-
tures (AF). It should be noted that RL automati-
cally selects the number of features used for train-
ing while FS(GP) was limited to selecting the top
17 features in order to make the results compara-
ble with our baseline feature set. It is interesting
to note that system FS(GP) outperformed the other
systems in spite of using fewer features. This tech-
nique is promising as it reduces the time require-
ments and overall computational complexity for
training the model, while achieving similar results
compared to systems with many more features.
Another interesting question is whether these
feature selection techniques identify a common
subset of features from the various datasets. The
overall top ranked features are:
</bodyText>
<listItem confidence="0.995647888888889">
• LM perplexities and log probabilities for
source and target;
• size of source and target sentences;
• average number of possible translations of
source words (IBM 1 with thresholds);
• ratio of target by source lengths in words;
• percentage of numbers in the target sentence;
• percentage of distinct unigrams seen in the
MT source training corpus.
</listItem>
<bodyText confidence="0.999897666666667">
Interestingly, not all top ranked features are
among the baseline 17 features which are report-
edly best in literature.
GB features on their own perform worse than
BB features, but in all three datasets, the combi-
nation of GB and BB followed by feature selec-
tion resulted in significantly lower errors than us-
ing only BB features with feature selection, show-
ing that the two features sets are complementary.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="conclusions">
4 Remarks
</sectionHeader>
<bodyText confidence="0.999980214285714">
The source code for the framework, the datasets
and extra resources can be downloaded from
http://www.quest.dcs.shef.ac.uk/.
The project is also set to receive contribution from
interested researchers using a GitHub repository:
https://github.com/lspecia/quest.
The license for the Java code, Python and shell
scripts is BSD, a permissive license with no re-
strictions on the use or extensions of the software
for any purposes, including commercial. For pre-
existing code and resources, e.g., scikit-learn, GPy
and Berkeley parser, their licenses apply, but fea-
tures relying on these resources can be easily dis-
carded if necessary.
</bodyText>
<sectionHeader confidence="0.998647" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994701">
This work was supported by the QuEst (EU
FP7 PASCAL2 NoE, Harvest program) and QT-
LaunchPad (EU FP7 CSA No. 296347) projects.
</bodyText>
<sectionHeader confidence="0.999293" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999850891891892">
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-
ness: a method for measuring machine translation
confidence. In ACL11, pages 211–219, Portland.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence Estimation for Machine Transla-
tion. In Coling04, pages 315–321, Geneva.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 workshop on statistical machine translation. In
WMT12, pages 10–51, Montr´eal.
Y. He, Y. Ma, J. van Genabith, and A. Way. 2010.
Bridging SMT and TM with Translation Recom-
mendation. In ACL10, pages 622–630, Uppsala.
N. Meinshausen and P. B¨uhlmann. 2010. Stability se-
lection. Journal of the Royal Statistical Society: Se-
ries B (Statistical Methodology), 72:417–473.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL02, pages 311–318,
Philadelphia.
C.E. Rasmussen and C.K.I. Williams. 2006. Gaus-
sian processes for machine learning, volume 1. MIT
Press, Cambridge.
R. Soricut and A. Echihabi. 2010. Trustrank: Induc-
ing trust in automatic translations via ranking. In
ACL11, pages 612–621, Uppsala.
L. Specia, M. Turchi, N. Cancedda, M. Dymetman,
and N. Cristianini. 2009. Estimating the Sentence-
Level Quality of Machine Translation Systems. In
EAMT09, pages 28–37, Barcelona.
L. Specia, D. Raj, and M. Turchi. 2010. Ma-
chine translation evaluation versus quality estima-
tion. Machine Translation, 24(1):39–50.
L. Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In
EAMT11, pages 73–80, Leuven.
</reference>
<page confidence="0.999243">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.497960">
<title confidence="0.996144">QuEst - A translation quality estimation framework</title>
<author confidence="0.999935">Kashif Jose G C de</author>
<affiliation confidence="0.885609">of Computer University of Sheffield, Bruno University of Trento,</affiliation>
<email confidence="0.998016">desouza@fbk.eu</email>
<abstract confidence="0.994163714285714">describe an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efficacy of features and algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bach</author>
<author>F Huang</author>
<author>Y Al-Onaizan</author>
</authors>
<title>Goodness: a method for measuring machine translation confidence.</title>
<date>2011</date>
<booktitle>In ACL11,</booktitle>
<pages>211--219</pages>
<location>Portland.</location>
<contexts>
<context position="4059" citStr="Bach et al., 2011" startWordPosition="623" endWordPosition="626"> translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 79 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engine</context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Goodness: a method for measuring machine translation confidence. In ACL11, pages 211–219, Portland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blatz</author>
<author>E Fitzgerald</author>
<author>G Foster</author>
<author>S Gandrabur</author>
<author>C Goutte</author>
<author>A Kulesza</author>
<author>A Sanchis</author>
<author>N Ueffing</author>
</authors>
<title>Confidence Estimation for Machine Translation. In</title>
<date>2004</date>
<booktitle>Coling04,</booktitle>
<pages>315--321</pages>
<location>Geneva.</location>
<contexts>
<context position="1986" citStr="Blatz et al., 2004" startWordPosition="298" endWordPosition="301">iding whether a reader gets a reliable gist of the text; • Estimating how much effort it will be needed to post-edit a segment; • Selecting among alternative translations produced by different MT systems; • Deciding whether the translation can be used for self-training of MT systems. Work in QE for MT started in the early 2000’s, inspired by the confidence scores used in Speech Recognition: mostly the estimation of word posterior probabilities. Back then it was called confidence estimation, which we believe is a narrower term. A 6-week workshop on the topic at John Hopkins University in 2003 (Blatz et al., 2004) had as goal to estimate automatic metrics such as BLEU (Papineni et al., 2002) and WER. These metrics are difficult to interpret, particularly at the sentence-level, and results of their very many trials proved unsuccessful. The overall quality of MT was considerably lower at the time, and therefore pinpointing the very few good quality segments was a hard problem. No software nor datasets were made available after the workshop. A new surge of interest in the field started recently, motivated by the widespread used of MT systems in the translation industry, as a consequence of better translat</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confidence Estimation for Machine Translation. In Coling04, pages 315–321, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In WMT12,</booktitle>
<pages>10--51</pages>
<location>Montr´eal.</location>
<contexts>
<context position="4399" citStr="Callison-Burch et al., 2012" startWordPosition="682" endWordPosition="685"> systems (Specia et al., 79 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labourintensive. Different language pairs or optimisation against specific quality scores (e.g., post-editing time vs translation adequacy) can benefit from very different feature sets. QUEST, our framework for quality estimation, provides a wide range of feature extractors from </context>
<context position="7233" citStr="Callison-Burch et al., 2012" startWordPosition="1114" endWordPosition="1117">at has been mostly neglected so far. Finally, the framework will also facilitate research on ways of using quality predictions in novel extrinsic tasks, such as self-training of statistical machine translation systems, and for estimating quality in other text output applications such as text summarisation. 2 The QUEST framework QUEST consists of two main modules: a feature extraction module and a machine learning module. The first module provides a number of feature extractors, including the most commonly used features in the literature and by systems submitted to the WMT12 shared task on QE (Callison-Burch et al., 2012). More than 15 researchers from 10 institutions contributed to it as part of the QUEST project.2 It is implemented in Java and provides abstract classes for features, resources and preprocessing steps so that extractors for new features can be easily added. The basic functioning of the feature extraction module requires raw text files with the source and translation texts, and a few resources (where available) such as the source MT training corpus and language models of source and target. Configuration files are used to indicate the resources available and a list of features that should be ext</context>
<context position="15824" citStr="Callison-Burch et al., 2012" startWordPosition="2451" endWordPosition="2454">sh (EAMT11-en-es) and French-English (EAMT11-fr-en) sentence translations judged for post-editing effort in 1-4. EAMT09 English sentences translated by four SMT systems into Spanish and scored for postediting effort in 1-4. Systems are denoted by s1-s4. GALE11 Arabic sentences translated by two SMT systems into English and scored for adequacy in 1-4. Systems are denoted by s1-s2. 3.2 Settings Amongst the various learning algorithms available in QUEST, to make our results comparable we selected SVR with radial basis function (RBF) kernel, which has been shown to perform very well in this task (Callison-Burch et al., 2012). The optimisation of parameters is done with grid search using the following ranges of values: • penalty parameter C: [1, 10, 10] • γ: [0.0001, 0.1, 10] • c: [0.1, 0.2, 10] where elements in list denote beginning, end and number of samples to generate, respectively. For feature selection, we have experimented with two techniques: Randomised Lasso and 7The datasets can be downloaded from http://www. dcs.shef.ac.uk/˜lucia/resources.html 82 Gaussian Processes. Randomised Lasso (Meinshausen and B¨uhlmann, 2010) repeatedly resamples the training data and fits a Lasso regression model on each sampl</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In WMT12, pages 10–51, Montr´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>Y Ma</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Bridging SMT and TM with Translation Recommendation. In</title>
<date>2010</date>
<booktitle>ACL10,</booktitle>
<pages>622--630</pages>
<location>Uppsala.</location>
<marker>He, Ma, van Genabith, Way, 2010</marker>
<rawString>Y. He, Y. Ma, J. van Genabith, and A. Way. 2010. Bridging SMT and TM with Translation Recommendation. In ACL10, pages 622–630, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Meinshausen</author>
<author>P B¨uhlmann</author>
</authors>
<title>Stability selection.</title>
<date>2010</date>
<journal>Journal of the Royal Statistical Society: Series B (Statistical Methodology),</journal>
<pages>72--417</pages>
<marker>Meinshausen, B¨uhlmann, 2010</marker>
<rawString>N. Meinshausen and P. B¨uhlmann. 2010. Stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72:417–473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia.</location>
<contexts>
<context position="2065" citStr="Papineni et al., 2002" startWordPosition="312" endWordPosition="315">ch effort it will be needed to post-edit a segment; • Selecting among alternative translations produced by different MT systems; • Deciding whether the translation can be used for self-training of MT systems. Work in QE for MT started in the early 2000’s, inspired by the confidence scores used in Speech Recognition: mostly the estimation of word posterior probabilities. Back then it was called confidence estimation, which we believe is a narrower term. A 6-week workshop on the topic at John Hopkins University in 2003 (Blatz et al., 2004) had as goal to estimate automatic metrics such as BLEU (Papineni et al., 2002) and WER. These metrics are difficult to interpret, particularly at the sentence-level, and results of their very many trials proved unsuccessful. The overall quality of MT was considerably lower at the time, and therefore pinpointing the very few good quality segments was a hard problem. No software nor datasets were made available after the workshop. A new surge of interest in the field started recently, motivated by the widespread used of MT systems in the translation industry, as a consequence of better translation quality, more userfriendly tools, and higher demand for translation. In ord</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL02, pages 311–318, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Rasmussen</author>
<author>C K I Williams</author>
</authors>
<title>Gaussian processes for machine learning,</title>
<date>2006</date>
<volume>1</volume>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="13706" citStr="Rasmussen and Williams, 2006" startWordPosition="2129" endWordPosition="2132">, feature selection methods and grid search for hyper-parameter optimisation. The pipeline with feature selection and hyperparameter optimisation can be set using a configuration file. Currently, the module has an interface for Support Vector Regression (SVR), Support Vector Classification, and Lasso learning algorithms. They can be used in conjunction with the feature selection algorithms (Randomised Lasso and Randomised decision trees) and the grid search implementation of scikit-learn to fit an optimal model of a given dataset. Additionally, QUEST includes Gaussian Process (GP) regression (Rasmussen and Williams, 2006) using the GPy toolkit.5 GPs are an advanced machine learning framework incorporating Bayesian non-parametrics and kernel machines, and are widely regarded as state of the art for regression. Empirically we found the performance to be similar to SVR on most datasets, with slightly worse MAE and better RMSE.6 In contrast to SVR, inference in GP regression can be expressed analytically and the model hyperparameters optimised directly using gradient ascent, thus avoiding the need for costly grid search. This also makes the method very suitable for feature selection. 3http://www.numpy.org/ 4http:/</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>C.E. Rasmussen and C.K.I. Williams. 2006. Gaussian processes for machine learning, volume 1. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>A Echihabi</author>
</authors>
<title>Trustrank: Inducing trust in automatic translations via ranking. In</title>
<date>2010</date>
<booktitle>ACL11,</booktitle>
<pages>612--621</pages>
<location>Uppsala.</location>
<contexts>
<context position="3614" citStr="Soricut and Echihabi, 2010" startWordPosition="554" endWordPosition="558"> also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 79 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of translations described th</context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>R. Soricut and A. Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In ACL11, pages 612–621, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>M Turchi</author>
<author>N Cancedda</author>
<author>M Dymetman</author>
<author>N Cristianini</author>
</authors>
<title>Estimating the SentenceLevel Quality of Machine Translation Systems. In</title>
<date>2009</date>
<booktitle>EAMT09,</booktitle>
<pages>28--37</pages>
<location>Barcelona.</location>
<contexts>
<context position="3487" citStr="Specia et al., 2009" startWordPosition="536" endWordPosition="539">roblem by using more complex metrics that go beyond matching the source segment with previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 79 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed </context>
</contexts>
<marker>Specia, Turchi, Cancedda, Dymetman, Cristianini, 2009</marker>
<rawString>L. Specia, M. Turchi, N. Cancedda, M. Dymetman, and N. Cristianini. 2009. Estimating the SentenceLevel Quality of Machine Translation Systems. In EAMT09, pages 28–37, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>D Raj</author>
<author>M Turchi</author>
</authors>
<title>Machine translation evaluation versus quality estimation.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Specia, Raj, Turchi, 2010</marker>
<rawString>L. Specia, D. Raj, and M. Turchi. 2010. Machine translation evaluation versus quality estimation. Machine Translation, 24(1):39–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
</authors>
<title>Exploiting objective annotations for measuring translation post-editing effort.</title>
<date>2011</date>
<booktitle>In EAMT11,</booktitle>
<pages>73--80</pages>
<location>Leuven.</location>
<contexts>
<context position="3502" citStr="Specia, 2011" startWordPosition="540" endWordPosition="541">complex metrics that go beyond matching the source segment with previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language. QE nowadays focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for postediting (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 79 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2010), and highlighting sub-segments that need revision (Bach et al., 2011). QE is generally addressed as a supervised</context>
</contexts>
<marker>Specia, 2011</marker>
<rawString>L. Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In EAMT11, pages 73–80, Leuven.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>