<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.8317705">
Book Reviews
Head-driven Phrase Structure Grammar
</title>
<author confidence="0.988373">
Carl Pollard and Ivan A. Sag
</author>
<affiliation confidence="0.968344">
(The Ohio State University and Stanford University)
</affiliation>
<bodyText confidence="0.9660415">
Stanford, CA: Center for the Study of
Learning and Information and Chicago:
The University of Chicago Press
(Studies in contemporary linguistics,
edited by John Goldsmith, James D.
McCawley, and Jerrold M. Sadock),
1994, xi+440 pp; hardbound, ISBN
0-226-67446-0, $80.00, £63.95;
paperbound, ISBN 0-226-67447-9,
$34.95, £27.95
</bodyText>
<title confidence="0.60676">
German in Head-driven Phrase-structure Grammar
</title>
<author confidence="0.959698">
John Nerbonne, Klaus Netter, and Carl Pollard (editors)
</author>
<bodyText confidence="0.913902375">
(Rijksuniversiteit Gronigen, DFKI Saarbriicken, and The Ohio State University)
Stanford, CA: Center for the Study of
Learning and Information (CSLI lecture
notes 46), 1994, xi+404 pp; distributed
by the University of Chicago Press;
hardbound, ISBN 1-881526-30-5, $49.95,
£39.95; paperbound, ISBN
1-881526-29-1, $21.95, £17.50
</bodyText>
<figure confidence="0.369156">
Reviewed by
Jean-Pierre Koenig
</figure>
<affiliation confidence="0.614502">
State University of New York at Buffalo
</affiliation>
<bodyText confidence="0.999942785714286">
It has been almost ten years since the classic Generalized Phrase Structure Grammar
(Gazdar, Klein, Pullum, and Sag 1985) appeared. And like its predecessor, Head-driven
Phrase Structure Grammar will probably become a classic too. Head-driven phrase struc-
ture grammar (HPSG) is the state of the art in what Pullum and Zwicky (1991) have
called category-based syntax, and this book makes available to a wide audience recent
developments in a grammatical framework used extensively in syntactically oriented
research in natural language processing. Moreover, of the grammatical theories using
inheritance-based grammars, a widespread tradition in the NLP community HPSG
achieves the widest coverage (vide the special issues of Computational Linguistics de-
voted to this topic in 1992). The book thus gives the computational linguist a good idea
of how to apply the basic knowledge-representation ideas of inheritance and typing to
state-of-the-art linguistic analyses. It also complements the more theoretically oriented
works of Carpenter (1992) and Keller (1993) on typed-feature structures and their logic.
So, although its intended audience is clearly primarily linguists, this book is essential
</bodyText>
<note confidence="0.503747">
Computational Linguistics Volume 22, Number 1
</note>
<bodyText confidence="0.999904647058824">
reading for anybody interested in building an NLP system with a nontrivial syntactic
component. All the more so, since Pollard and Sag, in order to challenge the domi-
nant Principles and Parameters syntactic framework of Chomsky and his associates
(Chomsky 1981, 1986), are meticulous in comparing their theory to the alternatives of
Principles and Parameters: the book provides a welcome cross-theoretical discussion
of all major syntactic issues.
For readers interested in either HPSG or German syntax, German in Head-driven
Phrase-structure Grammar is also highly recommended: it presents more current, cutting-
edge research and gives an idea of the kinds of questions an HPSG approach raises.
It also includes several technical and theoretical innovations that have become in-
creasingly popular within the HPSG community—function composition or generalized
raising and domain union, for example. For reasons of space, this review will focus
only on Pollard and Sag&apos;s book. It should be noted, though, that, unlike Pollard and
Sag&apos;s book, German in Head-driven Phrase-structure Grammar is editorially poor: typos
and incorrect referencing of examples abound.
The topics covered by Polland and Sag (henceforth P&amp;S) include all the usual major
syntactic phenomena that grammatical theories must account for: agreement, subcat-
egorization, unbounded dependencies (including parasitic gaps and relative clauses),
control, binding theory, and, to a small extent, quantification. As in the tradition of
Gazdar et al. (1985), the analyses are detailed and made precise, so that a fair evalua-
tion is possible. Moreover, the formalism used is similar enough to by-now traditional
NLP grammar formalisms (in particular feature-based grammars) that readers with-
out prior knowledge of HPSG implementations can have a reasonable idea of how to
implement the analyses. As mentioned, the book carefully compares P&amp;S&apos;s proposals
to standard analyses within Principles and Parameters. Unfortunately, there is much
less explicit comparison with work done within frameworks intellectually closer to
HPSG, such as lexical-functional grammar (LFG) (Bresnan 1982). Finally, although the
book focuses mainly on English syntax, P&amp;S attempt to provide a cross-linguistic per-
spective in several chapters, in particular when dealing with agreement and relative
clauses.
To the novice reader, HPSG is a constraint-based grammatical formalism that be-
longs to the growing family of frameworks using feature structures as their basic data
structure. In contrast to other feature-based frameworks, such as PAIR-IT (Shieber 1986,
Shieber et al. 1983), LFG, or GPSG, HPSG does not rely on a context-free backbone;
constituency is only one of the attributes of the linguistic object par excellence, the lin-
guistic sign. It is on a par with other syntactic and semantic attributes. Moreover, HPSG
is characterized by a systematic use of typing of feature structures (not unlike the use
of templates in PATR-II). It is similar to DATR (Evans and Gazdar 1989a, 1989b) in this
respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify
linguistic objects. Although they leave the question open, P&amp;S de facto use a strict in-
heritance scheme in their analyses, rather than the default inheritance sometimes used
in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the
formalism that HPSG uses, which owes a lot to King (1989), is very close to that dis-
cussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar
to that developed by Kasper and Rounds (1986) or Keller (1993). Typing in HPSG is
used to factor out shared properties of linguistic objects, be they words, phrases, or
anything else, into appropriate classes. Typing is also used to restrict the application
of general principles to the right class of linguistic structures. The Head-Feature Prin-
ciple, for example, which identifies the relevant syntactic properties of a phrasal head
with that of its mother node, is restricted to headed structures (i.e., objects of the type
headed structure). Nonheaded structures are not subject to the principle.
</bodyText>
<page confidence="0.993534">
130
</page>
<subsectionHeader confidence="0.948575">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.9982835">
As in most modern syntactic frameworks, in particular lexically oriented theories,
HPSG uses a minimal number of stipulated syntactic rules:
</bodyText>
<listItem confidence="0.9960825">
1. a few phrase-structural schemata (six in the book);1
2. a few principles governing feature percolation (head-feature principle,
non-local feature principle);
3. principles governing the major classes of syntactic phenomena (binding
principles, various principles relating to unbounded dependencies,
control theory).
</listItem>
<bodyText confidence="0.999309416666667">
Most of the information is located—and most of the action takes place—in a richly
structured lexicon.
Having described the basic framework used by P&amp;S, let me give a brief sum-
mary of some of their analyses. Chapter 2 presents a general theory of agreement as
token-identity between indices rather than copying of features. To anybody versed
in the unification-grammars literature, this is hardly news. A detailed discussion of
the variety of kinds of agreement found cross-linguistically makes this chapter more
interesting. Two important linguistic claims are made in the chapter. First, agreement
is partly semantic. It consists in identifying indices at a level of discourse representa-
tion (in fact, in the theory they propose, number, gender, and person are properties
of these indices). Obvious correlations between properties of indices (such as gender
or number) and properties of their denotata (male/female or singularity/aggregation)
are a matter of pragmatic constraints on the anchoring of indices, on which languages
differ. Second, because of the nature of indices and the inclusion of background or
pragmatic information in the description of linguistic signs, HPSG can account for
honorifics agreement as well as partially semantic agreement.
Overall, the theory that P&amp;S propose is compelling and illustrates perfectly the
theoretical value of integrating various kinds of information in each constituent or
sign, a hallmark of sign-based grammatical theories. Similarities and differences be-
tween various kinds of agreements attested cross-linguistically can be easily modeled.
Despite its success, one important issue is not addressed. Agreement, P&amp;S claim, in-
volves unification (more precisely token-identity of indices). There are instances of
agreement in several languages, though, which do not seem to involve token-identity
and unification, but rather feature-checking, as shown by Ingria (1990) or Pullum and
Zwicky (1986). A discussion of these challenges would have been welcome.
Chapter 6 proposes a theory of anaphoric binding that does not rely on configura-
tional notions such as c-command, but rather on the notion of relative obliqueness of
the antecedent and anaphor. Relative obliqueness is defined (roughly) in terms of the
relative order of the antecedent and anaphor in the subcategorization lists of predica-
tors. Furthermore, many classes of sentences involving anaphors generally assumed to
be subject to grammatical constraints are in fact subject, P&amp;S claim, to pragmatic con-
straints on anaphors (be it NPs such as pictures of each other in English or long-distance
anaphors such as Japanese zibun). As in the case of their theory of agreement, one
interesting aspect of P&amp;S&apos;s binding theory is its willingness to appeal to both syntac-
tic and pragmatic constraints on grammatical phenomena. Part of binding, according
to P&amp;S, is syntactic in nature, but part of it has to do with notions such as point of
</bodyText>
<footnote confidence="0.874993">
1 Again, these are simply a partial description of feature-structures that are subtypes of the general type
phrase, rather than separate kinds of objects.
</footnote>
<page confidence="0.984928">
131
</page>
<note confidence="0.406772">
Computational Linguistics Volume 22, Number 1
</note>
<bodyText confidence="0.999813772727273">
view or processing considerations (e.g., the relative distance of potential antecedents
to anaphors).
The same interaction between syntactic and semantic constraints is at play, accord-
ing to P&amp;S, in control phenomena, discussed in chapter 7. As is traditional in recent
linguistic work, they consider the unexpressed subject of eat in John tries to eat to be an
anaphor. What&apos;s more interesting is their claim that the choice of controller (i.e., the
NP whose index is identified with that of the unexpressed subject) is determined by
purely (lexical) semantic considerations.
To be fair, though, several aspects of P&amp;S&apos;s theory of control are unclear. First, is
the principle determining the controller of an unexpressed (reflexive) subject on a par
with the binding theory (i.e., a general constraint on feature structures)? If yes, is it a
universal principle? Or is it, rather, a lexical constraint, like the Raising Principle (i.e., a
constraint on lexical entries rather than on entry-tokens instantiating this entry)? P&amp;S
mention several phenomena that militate against a purely lexical treatment of control.
But there is one aspect of control that still assimilates the phenomenon to constraints
on lexical types. There are several exceptions to the &amp;quot;principle&amp;quot;, such as deserve, claim,
afford, defy, none of which denote a semantic relation of type commitment, orientation, or
influence. Moreover, although control verbs tend cross-linguistically to denote relations
that belong to these three semantic classes, some languages contain control verbs that
denote other kinds of semantic relations. French, for example, contains at least two
other classes of control verbs: verbs of saying and verbs of mental representation, as
illustrated in examples 1 and 2:
</bodyText>
<listItem confidence="0.781630333333333">
(1) Marc Oil pretend} etre heureux.
Marc {say.PRES I pretend.PRES} be.INFIN happy
(2) Marc croit avoir resolu le probleme.
</listItem>
<bodyText confidence="0.980497047619048">
Marc believe.PREs have.E.THN resolve.PPT the problem
Such cross-linguistic lexical variation is the hallmark of lexical constraints, not of gen-
eral grammatical principles such as the Head-Feature Principle, or even the binding
principle. How to reconcile both aspects of control needs further study.
Second, the treatment of control as involving a reflexive anaphor forces the addi-
tion of a separate clause in the definition of local o-command that, to my knowledge,
is not independently motivated. Although such disjunctive definition of the relation
crucial to anaphoric binding is not unique to HPSG, it still requires an explanation.
A final point worth mentioning about P&amp;S&apos;s binding theory: despite some mention
of cross-linguistic data, the theory presented covers mostly English anaphors. Recent
work by Xue, Pollard, and Sag (1994) has extended HPSG binding theory, parame-
terizing it somewhat to cover the Chinese long-distance anaphor ziji. But the theory
does not as yet cover the full range of attested grammatically-governed anaphors, dis-
cussed for example in Manzini and Wexler (1987) or Dalrymple (1993). Comparison
with Dalrymple&apos;s theory in particular would have been particularly useful, given the
overall formal similarities between LFG and HPSG.
Chapters 4 and 5 and half of chapter 9 are devoted to unbounded dependen-
cies. Technically, the modeling of unbounded dependencies in HPSG (leaving aside
the reformulation in chapter 9, for now) is similar to the classic SLASH percolation
method used by Gazdar et al. (1985). There are three major differences between the
two treatments.
</bodyText>
<footnote confidence="0.367137">
1. The SLASH feature is set-valued, thus allowing for multiple gaps.
</footnote>
<page confidence="0.919159">
132
</page>
<figure confidence="0.525546">
Book Reviews
</figure>
<listItem confidence="0.971289857142857">
2. Coordinate structures are not headed, thus allowing a crucial separation
of the Coordinate Structure Constraint from the theory of parasitic gaps
(a dissociation necessary to account for languages, such as Swedish, that
obey the Coordinate Structure Constraint, but allow extraction out of
subjects without the presence of other gaps in the sentence).
3. Traces are no longer a necessary part of an adequate treatment of
unbounded dependencies (see below for more on this).
</listItem>
<bodyText confidence="0.996269076923077">
The detail dnd empirical coverage of the analyses make these chapters stand out. An
added bonus is the treatment of relative clauses of a kind different from those present
in English, namely correlative relative clauses and internally headed relative clauses.
An important revision in the theory of unbounded dependencies is suggested in
the final chapter of the book, where a theory of unbounded dependencies that does
not rely on traces is proposed (see Sag and Fodor (1994) for an extension of the same
idea). The gist of the modification is simple: rather than introduce the bottom of un-
bounded dependencies by means of a particular empty sign with a non-null SLASH
value, the bottom of unbounded dependencies is introduced by a lexical rule that
passes the (nonlocally satisfied) valence requirement from the SUBCAT list to the SLASH
set of the relevant predicator. The newly created predicator entry then serves as the
bottom of the dependency; the rest of the theory remains unchanged. Such a move
illustrates the increasingly important role of lexical rules in HPSG (see below for more
on this topic): most, if not all, of the variation in the environments in which lexical
entries occur (actives vs. passives, of course, but also here local vs. nonlocal realiza-
tion of subcategorized complements) is accounted for by assuming that noncanonical
environments are the result of the presence of a variant of the &amp;quot;base&amp;quot; entry derived
by lexical rules.
The treatment of English relative clauses proposed in chapter 5 is particularly
unintuitive (despite its similarities to widely accepted accounts within the Principles
and Parameters approach). Since other accounts are possible within a typed-feature-
structure approach, I dwell on it some more. P&amp;S&apos;s basic idea is simple enough: they
posit a null relativizer (eR in the example below) which subcategorizes for both an
S-slashed complement and a specifier containing the relative marker. Example (3a)
is thus given the structure in (3b) (irrelevant information is omitted; RP stands for
Relative Phrase):
</bodyText>
<listItem confidence="0.9208655">
(3) a. person to whom Kim gave a book
b. [N, person [RP [pp to whom] [R, eR LS Kim gave a book]]]]
</listItem>
<bodyText confidence="0.999835307692308">
Although positing the empty relativizer eR in (3b) allows for a relatively simple account
of the English facts, there are several drawbacks to P&amp;S&apos;s analysis. First, there is
no independent motivation for this specific kind of empty category. Note that this
is the only empty category with independent subcategorization requirements; that
is, its subcategorization properties do not follow from that of a corresponding non-
empty category somewhere else in the string. Moreover, its semantic content is entirely
parasitic. Its index is identified with that of both the modified noun and the relative
marker (whom in (3b)). The restrictions on its index are the union of the semantic
content of its sentential (or VP) complement and of the modified noun. The absence of
any independent semantic contribution suggests that the role of this empty relativizer
is merely theory-internal.
Finally, there is a technical difficulty with the solution proposed by P&amp;S. To under-
stand the nature of this difficulty, consider example (3b) again. To terminate the relative
</bodyText>
<page confidence="0.99688">
133
</page>
<note confidence="0.624139">
Computational Linguistics Volume 22, Number 1
</note>
<bodyText confidence="0.999847764705882">
(unbounded) dependency introduced by the relative marker (whom), the noun modi-
fied by the relative phrase must bear a specification &amp;quot;[TO-BIND IREL {#1}1&amp;quot; where #1
is the index of the relative marker. So, the N&apos; person in (3b) must bear this specifica-
tion. The question is: where does this specification come from? Certainly, it does not
result from a lexical specification on the lexical entry person: it is not a lexical prop-
erty of person to be (sometimes) modified by a relative clause. Nor can it originate on
other nodes and percolate down (or up) to person. The Non-Local Feature Principle
regulates only the upward percolation of the values of inherited long-distance features,
not of the TO-BIND features. Finally, it does not come from any constraint on the
head-modifier-structure schema. The (necessary) presence of &amp;quot;[TO-BINDIREL {#1}1&amp;quot;
on person as used in (3a) therefore remains mysterious. One possible solution is to
introduce a special phrase-structural schema (of the form N&apos; N&apos; Re1C1) that would
directly introduce this feature specification.
Introducing a schema specific to relative clauses suggests another kind of solu-
tion within HPSG that would not rely on empty relativizers. Indeed, within a typed
grammar using multiple inheritance, another kind of analysis is possible, using a cross-
classification of phrases (see unpublished work by Fillmore and Kay (1993) and Sag
(1994)). The basic idea is to distinguish between the kind of phrase and the kind of
clause instantiated by a given constituent structure (say head-complement/head-filler
structures vs. interrogative/declarative/relative structures). One can then define a par-
ticular kind of relative clause as inheriting from both the head-filler and relative-clause
types. The relevant properties of relative clauses are ascribed directly to this particular
subtype of phrase, rather than being projected from an empty relativizer.
The reason that such a solution was not adopted by P&amp;S, I suspect, comes from
linguists&apos; bias against multiplying phrase-structural schemata in favor of multiply-
ing lexical entries. The linguist&apos;s intuition is that phrase-structural schemata represent
the universal structure of sentences whereas lexical entries always contain a certain
amount of idiosyncrasy. When in doubt, then, one ought to add to the idiosyncrasy of
lexical entries rather than make phrase-structural schemata less universal. In a non—
inheritance-based theory, this reasoning is sound: one can indeed expect a phrase-
structural solution to increase the number of stipulations that must be made. But in
an inheritance-based grammar, the phrase-structural solution is not necessarily less
economical. The similarities between relative clauses and other unbounded depen-
dencies can be preserved by making relative clauses (or a subclass of relative clauses)
a subtype of the head-filler schema, for example. In fact, the phrase-structural solution
has the added advantage of capturing the obvious similarity between relative clauses
and nonsubject questions by making them both subcases of the head-filler schema.
Aside from these quibbles on the detail of the analyses, the book has two small,
general shortcomings. First, the explanation of the formal underpinnings of the theory
is too short. In particular, the notions of typed-feature structures, sorts, or type hier-
archy are only briefly described. True, some of these ideas were already broached in
P&amp;S&apos;s 1987 book, but to novice readers, this will not provide much help. This omission
is all the more unfortunate, as most books and articles on the subject are very tech-
nical (for example, Carpenter&apos;s otherwise excellent 1992 book) and unlikely to help
linguistically oriented readers not familiar with typed feature structures.
A more significant omission is the absence of any substantive discussion of the
theory of lexical rules assumed by P&amp;S, although, again, their 1987 book contains more
details. Given that more and more of the &amp;quot;grammatical action&amp;quot; in a lexically oriented
theory such as HPSG takes place in the lexicon, this omission is more serious. A
discussion of the theory of lexical organization assumed in the theory would certainly
have been welcome. In fact, some of the few statements about this theory made in the
</bodyText>
<page confidence="0.996227">
134
</page>
<subsectionHeader confidence="0.927636">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999861">
book are misleading: P&amp;S refer to Bresnan&apos;s (1982) notion of lexical rule (p. 37) (itself
borrowed from jackendoff (1975)). But Bresnan&apos;s original view of lexical rules is that
they function as redundancy rules over separately listed lexical entries. This view is
implausible, given the use of lexical rules in HPSG to model productive inflectional
morphology (already suggested in P&amp;S (1987)) and the postulation of two entries for
eat in What did you want to eat? and Joe wanted to eat pasta (see Krieger and Nerbonne
(1993) or Koenig and jurafsky (1994) for some of the difficulties associated with lexical
rules in HPSG, and Godard and Sag (1995) for a response).
This somewhat critical summary of P&amp;S&apos;s major analyses does not give a good
idea of the book&apos;s richness and scholarship. The empirical coverage and the savvy of
the analyses is truly remarkable. Moreover, the structure of the type hierarchy being
assumed, the relevant type declarations, and the principles being proposed are all
laid out and summarized in the appendix, so that the reader can easily assess the
proposals: a welcome relief to anybody accustomed to recent syntactic work! Overall,
this is a linguistic book that ought to be on every computational linguist&apos;s shelf and
is likely to have a profound impact on computational linguistics.
</bodyText>
<subsectionHeader confidence="0.607819">
Acknowledgment
</subsectionHeader>
<bodyText confidence="0.999478">
I am grateful to Carl Pollard and Ivan Sag
for comments and clarifications on a
previous version of this review.
</bodyText>
<sectionHeader confidence="0.941447" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.998173545454545">
Bresnan, Joan, ed. (1982). The Mental
Representation of Grammatical Relations.
Cambridge, MA: The MIT Press.
Briscoe, Ted; de Paiva, Valeria; and
Copestake, Ann, eds. (1993). Inheritance,
Defaults, and the Lexicon. Cambridge
University Press.
Carpenter, Bob. (1992). The Logic of Typed
Feature Structures. Cambridge University
Press.
Chomsky, Noam. (1981). Lectures on
Government and Binding. Dordrecht: Foris.
Chomsky, Noam. (1986). Barriers. Cambridge,
MA: The MIT Press.
Dalrymple, Mary. (1993). The Syntax of
Anaphoric Binding. Stanford, CA: Center for
the Study of Language and Information.
Evans, Roger and Gazdar, Gerald. (1989a).
Inference in DATR. Proceedings, 4th
Conference of the European Chapter of the
Association for Computational Linguistics,
Manchester, 66-71.
Evans, Roger and Gazdar, Gerald. (1989b).
The semantics of DATR. Proceedings, 7th
Conference of the Society for the Study of
Artificial Intelligence and the Simulation of
Behaviour, 79-87.
Fillmore, Charles and Kay, Paul. (1993). On
construction grammar. Unpublished ms.,
University of California, Berkeley.
Gazdar, Gerald; Klein, Ewan; Pullum,
Geoffrey K.; and Sag, Ivan. (1985).
Generalized Phrase Structure Grammar.
Oxford: Basil Blackwell.
Godard, Daniele and Sag, Ivan. (1995).
&amp;quot;Reflexivization and intransitivity: The
case of French.&amp;quot; Paper presented at the
annual meeting of the Linguistic Society
of America.
Ingria, Robert. (1990). The limits of
unification. Proceedings, 28th Annual
Meeting of the Association for Computational
Linguistics, Pittsburgh, 194-204.
Jackendoff, Ray. (1975). &amp;quot;Morphological and
Semantic Regularities in the Lexicon.&amp;quot;
Language, 51(3): 639-671.
Kasper, Robert and Rounds, William. (1986).
A Logical Semantics for Feature
Structures. Proceedings, 24th Annual
Meeting of the Association for Computational
Linguistics, New York, 257-266.
Keller, Bill. (1993). Feature Logics, Infinitary
Descriptions, and Grammars. Stanford, CA:
Center for the Study of Language and
Information.
King, Paul. (1989). A Logical Formalism for
Head-driven Phrase Structure Grammar.
Ph.D. dissertation, University of
Manchester.
Koenig, Jean-Pierre and Jurafsky, Daniel.
(1994). Type Underspecification and
On-line Type Construction in the Lexicon.
Proceedings, 13th West Coast Conference on
Formal Linguistics, Stanford, CA: Center
for the Study of Language and
Information, 270-285.
</reference>
<page confidence="0.994414">
135
</page>
<note confidence="0.674277">
Computational Linguistics Volume 22, Number 1
</note>
<reference confidence="0.984494086206897">
Krieger, Hans-Ulrich and Nerbonne, John.
(1993). &amp;quot;Feature-based inheritance
networks for computational lexicons.&amp;quot; In
Inheritance, Defaults, and the Lexicon, edited
by Ted Briscoe, Valeria de Paiva, and Ann
Copestake. Cambridge Univeristy Press,
90-136.
Manzini, Maria Rita and Wexler, Kenneth.
(1987). Parameters, Binding, and
Learnability. Linguistic Inquiry, 18(3):
413-444.
Pollard, Carl and Sag, Ivan. (1987).
Information-based Syntax and Semantics,
Volume 1: Fundamentals. Stanford, CA:
Center for the Study of Language and
Information.
Pullum, Geoffrey K. and Zwicky, Arnold.
(1986). Phonological Resolution of
Syntactic Feature Conflict. Language, 62(4):
751-774.
Pullum, Geoffrey K. and Zwicky, Arnold.
(1991). A Misconceived Approach to
Morphology. Proceedings, 10th West Coast
Conference on Formal Linguistics, Stanford,
CA: Center for the Study of Language and
Information, 387-398.
Sag, Ivan. (1994). &amp;quot;Relative Clauses—A
Multiple Inheritance Analysis.&amp;quot; Paper
presented at the HPSG 1994 Conference,
Copenhagen.
Sag, Ivan and Fodor, Janet D. (1994).
Extractions Without Traces. Proceedings,
13th West Coast Conference on Formal
Linguistics, Stanford, CA: Center for the
Study of Language and Information,
365-384.
Shieber, Stuart. (1986). An Introduction to
Unification-based Approaches to Grammar.
Stanford, CA: Center for the Study of
Language and Information.
Shieber, Stuart; Uszkoreit, Hans; Robinson,
Jane; and Tyson, Mabry. (1983). &amp;quot;The
Formalism and Implementation of
PATR-II.&amp;quot; In Research on Interactive
Acquisition and Use of Knowledge, Menlo
Park, CA: Artificial Intelligence Center,
SRI International.
Xue, Ping; Pollard, Carl; and Sag, Ivan.
(1994). A New Perspective on Chinese
&apos;ziji&apos;. Proceedings, 13th West Coast
Conference on Formal Linguistics, Stanford,
CA: Center for the Study of Language
and Information, 432-447.
Jean-Pierre Koenig is Assistant Professor in the Linguistics Department at the State University of
New York at Buffalo. His main research interests are in lexical knowledge representation. He
has worked recently on developing a notion of lexical relatedness that does not rely on lexical
rules, but rather on underspecified types. Koenig&apos;s address is 684 Baldy Hall, SUNY at Buffalo,
Buffalo, NY 14052; e-mail: jpkoenig@acsu.buffalo.edu.
</reference>
<page confidence="0.99868">
136
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9839805">Book Reviews Head-driven Phrase Structure Grammar</title>
<author confidence="0.998992">Carl Pollard</author>
<author confidence="0.998992">Ivan A Sag</author>
<affiliation confidence="0.655377">(The Ohio State University and Stanford University)</affiliation>
<address confidence="0.387777">Stanford, CA: Center for the Study of</address>
<title confidence="0.817348">Learning and Information and Chicago: The University of Chicago Press</title>
<note confidence="0.432207944444444">(Studies in contemporary linguistics, edited by John Goldsmith, James D. McCawley, and Jerrold M. Sadock), 1994, xi+440 pp; hardbound, ISBN 0-226-67446-0, $80.00, £63.95; paperbound, ISBN 0-226-67447-9, $34.95, £27.95 German in Head-driven Phrase-structure Grammar John Nerbonne, Klaus Netter, and Carl Pollard (editors) (Rijksuniversiteit Gronigen, DFKI Saarbriicken, and The Ohio State University) Stanford, CA: Center for the Study of Learning and Information (CSLI lecture notes 46), 1994, xi+404 pp; distributed by the University of Chicago Press; hardbound, ISBN 1-881526-30-5, $49.95, £39.95; paperbound, ISBN 1-881526-29-1, $21.95, £17.50 Reviewed by</note>
<author confidence="0.995274">Jean-Pierre Koenig</author>
<affiliation confidence="0.996299">State University of New York at Buffalo</affiliation>
<abstract confidence="0.991370225806451">has been almost ten years since the classic Phrase Structure Grammar Klein, Pullum, and Sag 1985) appeared. And like its predecessor, Structure Grammar probably become a classic too. Head-driven phrase structure grammar (HPSG) is the state of the art in what Pullum and Zwicky (1991) have called category-based syntax, and this book makes available to a wide audience recent developments in a grammatical framework used extensively in syntactically oriented research in natural language processing. Moreover, of the grammatical theories using inheritance-based grammars, a widespread tradition in the NLP community HPSG the widest coverage special issues of Linguistics devoted to this topic in 1992). The book thus gives the computational linguist a good idea of how to apply the basic knowledge-representation ideas of inheritance and typing to state-of-the-art linguistic analyses. It also complements the more theoretically oriented works of Carpenter (1992) and Keller (1993) on typed-feature structures and their logic. So, although its intended audience is clearly primarily linguists, this book is essential Computational Linguistics Volume 22, Number 1 reading for anybody interested in building an NLP system with a nontrivial syntactic component. All the more so, since Pollard and Sag, in order to challenge the dominant Principles and Parameters syntactic framework of Chomsky and his associates (Chomsky 1981, 1986), are meticulous in comparing their theory to the alternatives of Principles and Parameters: the book provides a welcome cross-theoretical discussion of all major syntactic issues. readers interested in either HPSG or German syntax, in Head-driven Grammar also highly recommended: it presents more current, cuttingedge research and gives an idea of the kinds of questions an HPSG approach raises. It also includes several technical and theoretical innovations that have become increasingly popular within the HPSG community—function composition or generalized raising and domain union, for example. For reasons of space, this review will focus only on Pollard and Sag&apos;s book. It should be noted, though, that, unlike Pollard and book, in Head-driven Phrase-structure Grammar editorially poor: typos and incorrect referencing of examples abound. The topics covered by Polland and Sag (henceforth P&amp;S) include all the usual major syntactic phenomena that grammatical theories must account for: agreement, subcategorization, unbounded dependencies (including parasitic gaps and relative clauses), control, binding theory, and, to a small extent, quantification. As in the tradition of Gazdar et al. (1985), the analyses are detailed and made precise, so that a fair evaluation is possible. Moreover, the formalism used is similar enough to by-now traditional NLP grammar formalisms (in particular feature-based grammars) that readers without prior knowledge of HPSG implementations can have a reasonable idea of how to implement the analyses. As mentioned, the book carefully compares P&amp;S&apos;s proposals to standard analyses within Principles and Parameters. Unfortunately, there is much less explicit comparison with work done within frameworks intellectually closer to HPSG, such as lexical-functional grammar (LFG) (Bresnan 1982). Finally, although the book focuses mainly on English syntax, P&amp;S attempt to provide a cross-linguistic perspective in several chapters, in particular when dealing with agreement and relative clauses. To the novice reader, HPSG is a constraint-based grammatical formalism that belongs to the growing family of frameworks using feature structures as their basic data structure. In contrast to other feature-based frameworks, such as PAIR-IT (Shieber 1986, Shieber et al. 1983), LFG, or GPSG, HPSG does not rely on a context-free backbone; is only one of the attributes of the linguistic object excellence, linguistic sign. It is on a par with other syntactic and semantic attributes. Moreover, HPSG is characterized by a systematic use of typing of feature structures (not unlike the use of templates in PATR-II). It is similar to DATR (Evans and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify objects. Although they leave the question open, P&amp;S facto a strict inheritance scheme in their analyses, rather than the default inheritance sometimes used in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the formalism that HPSG uses, which owes a lot to King (1989), is very close to that discussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar to that developed by Kasper and Rounds (1986) or Keller (1993). Typing in HPSG is used to factor out shared properties of linguistic objects, be they words, phrases, or anything else, into appropriate classes. Typing is also used to restrict the application general principles to the right class of linguistic structures. The Prinexample, which identifies the relevant syntactic properties of a phrasal head with that of its mother node, is restricted to headed structures (i.e., objects of the type headed structure). Nonheaded structures are not subject to the principle. 130 Book Reviews in most modern frameworks, in particular lexically oriented theories, HPSG uses a minimal number of stipulated syntactic rules: a few phrase-structural schemata (six in the 2. a few principles governing feature percolation (head-feature principle, non-local feature principle); 3. principles governing the major classes of syntactic phenomena (binding principles, various principles relating to unbounded dependencies, control theory). Most of the information is located—and most of the action takes place—in a richly structured lexicon. Having described the basic framework used by P&amp;S, let me give a brief summary of some of their analyses. Chapter 2 presents a general theory of agreement as token-identity between indices rather than copying of features. To anybody versed in the unification-grammars literature, this is hardly news. A detailed discussion of the variety of kinds of agreement found cross-linguistically makes this chapter more interesting. Two important linguistic claims are made in the chapter. First, agreement is partly semantic. It consists in identifying indices at a level of discourse representation (in fact, in the theory they propose, number, gender, and person are properties of these indices). Obvious correlations between properties of indices (such as gender or number) and properties of their denotata (male/female or singularity/aggregation) are a matter of pragmatic constraints on the anchoring of indices, on which languages differ. Second, because of the nature of indices and the inclusion of background or pragmatic information in the description of linguistic signs, HPSG can account for honorifics agreement as well as partially semantic agreement. Overall, the theory that P&amp;S propose is compelling and illustrates perfectly the theoretical value of integrating various kinds of information in each constituent or sign, a hallmark of sign-based grammatical theories. Similarities and differences between various kinds of agreements attested cross-linguistically can be easily modeled. Despite its success, one important issue is not addressed. Agreement, P&amp;S claim, involves unification (more precisely token-identity of indices). There are instances of agreement in several languages, though, which do not seem to involve token-identity and unification, but rather feature-checking, as shown by Ingria (1990) or Pullum and Zwicky (1986). A discussion of these challenges would have been welcome. Chapter 6 proposes a theory of anaphoric binding that does not rely on configurational notions such as c-command, but rather on the notion of relative obliqueness of the antecedent and anaphor. Relative obliqueness is defined (roughly) in terms of the relative order of the antecedent and anaphor in the subcategorization lists of predicators. Furthermore, many classes of sentences involving anaphors generally assumed to be subject to grammatical constraints are in fact subject, P&amp;S claim, to pragmatic conon anaphors (be it NPs such as of each other English or long-distance such as Japanese in the case of their theory of agreement, one interesting aspect of P&amp;S&apos;s binding theory is its willingness to appeal to both syntactic and pragmatic constraints on grammatical phenomena. Part of binding, according to P&amp;S, is syntactic in nature, but part of it has to do with notions such as point of 1 Again, these are simply a partial description of feature-structures that are subtypes of the general type than separate kinds of objects. 131 Computational Linguistics Volume 22, Number 1 or considerations (e.g., the relative distance of potential antecedents to anaphors). The same interaction between syntactic and semantic constraints is at play, according to P&amp;S, in control phenomena, discussed in chapter 7. As is traditional in recent work, they consider the unexpressed subject of tries to eat be an anaphor. What&apos;s more interesting is their claim that the choice of controller (i.e., the NP whose index is identified with that of the unexpressed subject) is determined by purely (lexical) semantic considerations.</abstract>
<note confidence="0.580862">To be fair, though, several aspects of P&amp;S&apos;s theory of control are unclear. First, is</note>
<abstract confidence="0.995918355828221">the principle determining the controller of an unexpressed (reflexive) subject on a par with the binding theory (i.e., a general constraint on feature structures)? If yes, is it a universal principle? Or is it, rather, a lexical constraint, like the Raising Principle (i.e., a constraint on lexical entries rather than on entry-tokens instantiating this entry)? P&amp;S mention several phenomena that militate against a purely lexical treatment of control. But there is one aspect of control that still assimilates the phenomenon to constraints lexical types. There are several exceptions to the &amp;quot;principle&amp;quot;, such as claim, defy, of which denote a semantic relation of type orientation, although control verbs tend cross-linguistically to denote relations that belong to these three semantic classes, some languages contain control verbs that denote other kinds of semantic relations. French, for example, contains at least two other classes of control verbs: verbs of saying and verbs of mental representation, as illustrated in examples 1 and 2: (1) Marc Oil pretend} etre heureux. Ipretend.PRES} be.INFIN (2) Marc croit avoir resolu le probleme. believe.PREs have.E.THN problem Such cross-linguistic lexical variation is the hallmark of lexical constraints, not of general grammatical principles such as the Head-Feature Principle, or even the binding principle. How to reconcile both aspects of control needs further study. Second, the treatment of control as involving a reflexive anaphor forces the addition of a separate clause in the definition of local o-command that, to my knowledge, is not independently motivated. Although such disjunctive definition of the relation crucial to anaphoric binding is not unique to HPSG, it still requires an explanation. A final point worth mentioning about P&amp;S&apos;s binding theory: despite some mention of cross-linguistic data, the theory presented covers mostly English anaphors. Recent work by Xue, Pollard, and Sag (1994) has extended HPSG binding theory, parameit somewhat to cover the Chinese long-distance anaphor the theory does not as yet cover the full range of attested grammatically-governed anaphors, discussed for example in Manzini and Wexler (1987) or Dalrymple (1993). Comparison with Dalrymple&apos;s theory in particular would have been particularly useful, given the overall formal similarities between LFG and HPSG. Chapters 4 and 5 and half of chapter 9 are devoted to unbounded dependencies. Technically, the modeling of unbounded dependencies in HPSG (leaving aside reformulation in chapter 9, for now) is similar to the classic method used by Gazdar et al. (1985). There are three major differences between the two treatments. The is set-valued, thus allowing for multiple gaps. 132 Book Reviews 2. Coordinate structures are not headed, thus allowing a crucial separation of the Coordinate Structure Constraint from the theory of parasitic gaps (a dissociation necessary to account for languages, such as Swedish, that obey the Coordinate Structure Constraint, but allow extraction out of subjects without the presence of other gaps in the sentence). 3. Traces are no longer a necessary part of an adequate treatment of unbounded dependencies (see below for more on this). detail coverage of the analyses make these chapters stand out. An added bonus is the treatment of relative clauses of a kind different from those present in English, namely correlative relative clauses and internally headed relative clauses. An important revision in the theory of unbounded dependencies is suggested in the final chapter of the book, where a theory of unbounded dependencies that does not rely on traces is proposed (see Sag and Fodor (1994) for an extension of the same idea). The gist of the modification is simple: rather than introduce the bottom of undependencies by means of a particular empty sign with a non-null value, the bottom of unbounded dependencies is introduced by a lexical rule that the (nonlocally satisfied) valence requirement from the list the set of the relevant predicator. The newly created predicator entry then serves as the bottom of the dependency; the rest of the theory remains unchanged. Such a move illustrates the increasingly important role of lexical rules in HPSG (see below for more on this topic): most, if not all, of the variation in the environments in which lexical entries occur (actives vs. passives, of course, but also here local vs. nonlocal realization of subcategorized complements) is accounted for by assuming that noncanonical environments are the result of the presence of a variant of the &amp;quot;base&amp;quot; entry derived by lexical rules. The treatment of English relative clauses proposed in chapter 5 is particularly unintuitive (despite its similarities to widely accepted accounts within the Principles and Parameters approach). Since other accounts are possible within a typed-featurestructure approach, I dwell on it some more. P&amp;S&apos;s basic idea is simple enough: they a null relativizer the example below) which subcategorizes for both an S-slashed complement and a specifier containing the relative marker. Example (3a) is thus given the structure in (3b) (irrelevant information is omitted; RP stands for Relative Phrase): to whom Kim gave a book [pp to eR gave a book]]]] positing the empty relativizer (3b) allows for a relatively simple account of the English facts, there are several drawbacks to P&amp;S&apos;s analysis. First, there is no independent motivation for this specific kind of empty category. Note that this is the only empty category with independent subcategorization requirements; that is, its subcategorization properties do not follow from that of a corresponding nonempty category somewhere else in the string. Moreover, its semantic content is entirely parasitic. Its index is identified with that of both the modified noun and the relative (3b)). The restrictions on its index are the union of the semantic content of its sentential (or VP) complement and of the modified noun. The absence of any independent semantic contribution suggests that the role of this empty relativizer is merely theory-internal. Finally, there is a technical difficulty with the solution proposed by P&amp;S. To understand the nature of this difficulty, consider example (3b) again. To terminate the relative 133 Computational Linguistics Volume 22, Number 1 dependency introduced by the relative marker noun modified by the relative phrase must bear a specification &amp;quot;[TO-BIND IREL {#1}1&amp;quot; where #1 the index of the relative marker. So, the N&apos; (3b) must bear this specification. The question is: where does this specification come from? Certainly, it does not from a lexical specification on the lexical entry is a lexical propof be (sometimes) modified by a relative clause. Nor can it originate on nodes and percolate down (or up) to Non-Local Feature Principle only the upward percolation of the values of features, not of the TO-BIND features. Finally, it does not come from any constraint on the head-modifier-structure schema. The (necessary) presence of &amp;quot;[TO-BINDIREL {#1}1&amp;quot; used in (3a) therefore remains mysterious. One possible solution is to introduce a special phrase-structural schema (of the form N&apos; N&apos; Re1C1) that directly introduce this feature specification. Introducing a schema specific to relative clauses suggests another kind of solution within HPSG that would not rely on empty relativizers. Indeed, within a typed grammar using multiple inheritance, another kind of analysis is possible, using a crossclassification of phrases (see unpublished work by Fillmore and Kay (1993) and Sag (1994)). The basic idea is to distinguish between the kind of phrase and the kind of clause instantiated by a given constituent structure (say head-complement/head-filler structures vs. interrogative/declarative/relative structures). One can then define a particular kind of relative clause as inheriting from both the head-filler and relative-clause types. The relevant properties of relative clauses are ascribed directly to this particular subtype of phrase, rather than being projected from an empty relativizer. The reason that such a solution was not adopted by P&amp;S, I suspect, comes from linguists&apos; bias against multiplying phrase-structural schemata in favor of multiplying lexical entries. The linguist&apos;s intuition is that phrase-structural schemata represent the universal structure of sentences whereas lexical entries always contain a certain amount of idiosyncrasy. When in doubt, then, one ought to add to the idiosyncrasy of lexical entries rather than make phrase-structural schemata less universal. In a non— inheritance-based theory, this reasoning is sound: one can indeed expect a phrasestructural solution to increase the number of stipulations that must be made. But in an inheritance-based grammar, the phrase-structural solution is not necessarily less economical. The similarities between relative clauses and other unbounded dependencies can be preserved by making relative clauses (or a subclass of relative clauses) a subtype of the head-filler schema, for example. In fact, the phrase-structural solution has the added advantage of capturing the obvious similarity between relative clauses and nonsubject questions by making them both subcases of the head-filler schema. Aside from these quibbles on the detail of the analyses, the book has two small, general shortcomings. First, the explanation of the formal underpinnings of the theory is too short. In particular, the notions of typed-feature structures, sorts, or type hierare only briefly described. True, some of these ideas were already broached P&amp;S&apos;s 1987 book, but to novice readers, this will not provide much help. This omission is all the more unfortunate, as most books and articles on the subject are very technical (for example, Carpenter&apos;s otherwise excellent 1992 book) and unlikely to help linguistically oriented readers not familiar with typed feature structures. A more significant omission is the absence of any substantive discussion of the theory of lexical rules assumed by P&amp;S, although, again, their 1987 book contains more details. Given that more and more of the &amp;quot;grammatical action&amp;quot; in a lexically oriented theory such as HPSG takes place in the lexicon, this omission is more serious. A discussion of the theory of lexical organization assumed in the theory would certainly have been welcome. In fact, some of the few statements about this theory made in the 134 Book Reviews book are misleading: P&amp;S refer to Bresnan&apos;s (1982) notion of lexical rule (p. 37) (itself borrowed from jackendoff (1975)). But Bresnan&apos;s original view of lexical rules is that they function as redundancy rules over separately listed lexical entries. This view is implausible, given the use of lexical rules in HPSG to model productive inflectional morphology (already suggested in P&amp;S (1987)) and the postulation of two entries for in did you want to eat? wanted to eat pasta Krieger and Nerbonne (1993) or Koenig and jurafsky (1994) for some of the difficulties associated with lexical rules in HPSG, and Godard and Sag (1995) for a response). This somewhat critical summary of P&amp;S&apos;s major analyses does not give a good idea of the book&apos;s richness and scholarship. The empirical coverage and the savvy of the analyses is truly remarkable. Moreover, the structure of the type hierarchy being assumed, the relevant type declarations, and the principles being proposed are all laid out and summarized in the appendix, so that the reader can easily assess the proposals: a welcome relief to anybody accustomed to recent syntactic work! Overall, this is a linguistic book that ought to be on every computational linguist&apos;s shelf and is likely to have a profound impact on computational linguistics. Acknowledgment I am grateful to Carl Pollard and Ivan Sag for comments and clarifications on a previous version of this review.</abstract>
<note confidence="0.848120585714286">References Joan, ed. (1982). Mental Representation of Grammatical Relations. Cambridge, MA: The MIT Press. Briscoe, Ted; de Paiva, Valeria; and Ann, eds. (1993). and the Lexicon. University Press. Bob. (1992). Logic of Typed Structures. University Press. Noam. (1981). on and Binding. Foris. Noam. (1986). MA: The MIT Press. Mary. (1993). Syntax of Binding. CA: Center for the Study of Language and Information. Evans, Roger and Gazdar, Gerald. (1989a). in DATR. 4th Conference of the European Chapter of the Association for Computational Linguistics, Manchester, 66-71. Evans, Roger and Gazdar, Gerald. (1989b). semantics of DATR. 7th Conference of the Society for the Study of Artificial Intelligence and the Simulation of Fillmore, Charles and Kay, Paul. (1993). On construction grammar. Unpublished ms., University of California, Berkeley. Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan. (1985). Generalized Phrase Structure Grammar. Oxford: Basil Blackwell. Godard, Daniele and Sag, Ivan. (1995). &amp;quot;Reflexivization and intransitivity: The case of French.&amp;quot; Paper presented at the annual meeting of the Linguistic Society of America. Ingria, Robert. (1990). The limits of 28th Annual Meeting of the Association for Computational 194-204. Jackendoff, Ray. (1975). &amp;quot;Morphological and Semantic Regularities in the Lexicon.&amp;quot; Kasper, Robert and Rounds, William. (1986). A Logical Semantics for Feature 24th Annual Meeting of the Association for Computational York, 257-266. Bill. (1993). Logics, Infinitary and Grammars. CA: Center for the Study of Language and Information. Paul. (1989). Logical Formalism for Head-driven Phrase Structure Grammar. Ph.D. dissertation, University of Manchester. Koenig, Jean-Pierre and Jurafsky, Daniel. (1994). Type Underspecification and On-line Type Construction in the Lexicon. Proceedings, 13th West Coast Conference on Linguistics, CA: Center for the Study of Language and Information, 270-285. 135 Computational Linguistics Volume 22, Number 1 Krieger, Hans-Ulrich and Nerbonne, John. (1993). &amp;quot;Feature-based inheritance networks for computational lexicons.&amp;quot; In</note>
<title confidence="0.593069">Defaults, and the Lexicon,</title>
<author confidence="0.706274">by Ted Briscoe</author>
<author confidence="0.706274">Valeria de_Paiva</author>
<author confidence="0.706274">Ann</author>
<affiliation confidence="0.754876">Copestake. Cambridge Univeristy Press,</affiliation>
<address confidence="0.7932075">90-136. Manzini, Maria Rita and Wexler, Kenneth.</address>
<note confidence="0.927448107142857">(1987). Parameters, Binding, and Inquiry, 413-444. Pollard, Carl and Sag, Ivan. (1987). Information-based Syntax and Semantics, 1: Fundamentals. CA: Center for the Study of Language and Information. Pullum, Geoffrey K. and Zwicky, Arnold. (1986). Phonological Resolution of Feature Conflict. 751-774. Pullum, Geoffrey K. and Zwicky, Arnold. (1991). A Misconceived Approach to 10th West Coast on Formal Linguistics, CA: Center for the Study of Language and Information, 387-398. Sag, Ivan. (1994). &amp;quot;Relative Clauses—A Multiple Inheritance Analysis.&amp;quot; Paper presented at the HPSG 1994 Conference, Copenhagen. Sag, Ivan and Fodor, Janet D. (1994). Without Traces. 13th West Coast Conference on Formal CA: Center for the Study of Language and Information, 365-384.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>ed</author>
</authors>
<title>The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Bresnan, ed, 1982</marker>
<rawString>Bresnan, Joan, ed. (1982). The Mental Representation of Grammatical Relations. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<booktitle>Inheritance, Defaults, and the Lexicon.</booktitle>
<editor>Briscoe, Ted; de Paiva, Valeria; and Copestake, Ann, eds.</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2018" citStr="(1993)" startWordPosition="285" endWordPosition="285">amework used extensively in syntactically oriented research in natural language processing. Moreover, of the grammatical theories using inheritance-based grammars, a widespread tradition in the NLP community HPSG achieves the widest coverage (vide the special issues of Computational Linguistics devoted to this topic in 1992). The book thus gives the computational linguist a good idea of how to apply the basic knowledge-representation ideas of inheritance and typing to state-of-the-art linguistic analyses. It also complements the more theoretically oriented works of Carpenter (1992) and Keller (1993) on typed-feature structures and their logic. So, although its intended audience is clearly primarily linguists, this book is essential Computational Linguistics Volume 22, Number 1 reading for anybody interested in building an NLP system with a nontrivial syntactic component. All the more so, since Pollard and Sag, in order to challenge the dominant Principles and Parameters syntactic framework of Chomsky and his associates (Chomsky 1981, 1986), are meticulous in comparing their theory to the alternatives of Principles and Parameters: the book provides a welcome cross-theoretical discussion o</context>
<context position="5817" citStr="(1993)" startWordPosition="861" endWordPosition="861">dar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&amp;S de facto use a strict inheritance scheme in their analyses, rather than the default inheritance sometimes used in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the formalism that HPSG uses, which owes a lot to King (1989), is very close to that discussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar to that developed by Kasper and Rounds (1986) or Keller (1993). Typing in HPSG is used to factor out shared properties of linguistic objects, be they words, phrases, or anything else, into appropriate classes. Typing is also used to restrict the application of general principles to the right class of linguistic structures. The Head-Feature Principle, for example, which identifies the relevant syntactic properties of a phrasal head with that of its mother node, is restricted to headed structures (i.e., objects of the type headed structure). Nonheaded structures are not subject to the principle. 130 Book Reviews As in most modern syntactic frameworks, in p</context>
<context position="12994" citStr="(1993)" startWordPosition="1945" endWordPosition="1945"> disjunctive definition of the relation crucial to anaphoric binding is not unique to HPSG, it still requires an explanation. A final point worth mentioning about P&amp;S&apos;s binding theory: despite some mention of cross-linguistic data, the theory presented covers mostly English anaphors. Recent work by Xue, Pollard, and Sag (1994) has extended HPSG binding theory, parameterizing it somewhat to cover the Chinese long-distance anaphor ziji. But the theory does not as yet cover the full range of attested grammatically-governed anaphors, discussed for example in Manzini and Wexler (1987) or Dalrymple (1993). Comparison with Dalrymple&apos;s theory in particular would have been particularly useful, given the overall formal similarities between LFG and HPSG. Chapters 4 and 5 and half of chapter 9 are devoted to unbounded dependencies. Technically, the modeling of unbounded dependencies in HPSG (leaving aside the reformulation in chapter 9, for now) is similar to the classic SLASH percolation method used by Gazdar et al. (1985). There are three major differences between the two treatments. 1. The SLASH feature is set-valued, thus allowing for multiple gaps. 132 Book Reviews 2. Coordinate structures are </context>
<context position="18693" citStr="(1993)" startWordPosition="2849" endWordPosition="2849">er-structure schema. The (necessary) presence of &amp;quot;[TO-BINDIREL {#1}1&amp;quot; on person as used in (3a) therefore remains mysterious. One possible solution is to introduce a special phrase-structural schema (of the form N&apos; N&apos; Re1C1) that would directly introduce this feature specification. Introducing a schema specific to relative clauses suggests another kind of solution within HPSG that would not rely on empty relativizers. Indeed, within a typed grammar using multiple inheritance, another kind of analysis is possible, using a crossclassification of phrases (see unpublished work by Fillmore and Kay (1993) and Sag (1994)). The basic idea is to distinguish between the kind of phrase and the kind of clause instantiated by a given constituent structure (say head-complement/head-filler structures vs. interrogative/declarative/relative structures). One can then define a particular kind of relative clause as inheriting from both the head-filler and relative-clause types. The relevant properties of relative clauses are ascribed directly to this particular subtype of phrase, rather than being projected from an empty relativizer. The reason that such a solution was not adopted by P&amp;S, I suspect, comes f</context>
<context position="22124" citStr="(1993)" startWordPosition="3376" endWordPosition="3376">fact, some of the few statements about this theory made in the 134 Book Reviews book are misleading: P&amp;S refer to Bresnan&apos;s (1982) notion of lexical rule (p. 37) (itself borrowed from jackendoff (1975)). But Bresnan&apos;s original view of lexical rules is that they function as redundancy rules over separately listed lexical entries. This view is implausible, given the use of lexical rules in HPSG to model productive inflectional morphology (already suggested in P&amp;S (1987)) and the postulation of two entries for eat in What did you want to eat? and Joe wanted to eat pasta (see Krieger and Nerbonne (1993) or Koenig and jurafsky (1994) for some of the difficulties associated with lexical rules in HPSG, and Godard and Sag (1995) for a response). This somewhat critical summary of P&amp;S&apos;s major analyses does not give a good idea of the book&apos;s richness and scholarship. The empirical coverage and the savvy of the analyses is truly remarkable. Moreover, the structure of the type hierarchy being assumed, the relevant type declarations, and the principles being proposed are all laid out and summarized in the appendix, so that the reader can easily assess the proposals: a welcome relief to anybody accusto</context>
</contexts>
<marker>1993</marker>
<rawString>Briscoe, Ted; de Paiva, Valeria; and Copestake, Ann, eds. (1993). Inheritance, Defaults, and the Lexicon. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures.</title>
<date>1992</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2000" citStr="Carpenter (1992)" startWordPosition="281" endWordPosition="282">lopments in a grammatical framework used extensively in syntactically oriented research in natural language processing. Moreover, of the grammatical theories using inheritance-based grammars, a widespread tradition in the NLP community HPSG achieves the widest coverage (vide the special issues of Computational Linguistics devoted to this topic in 1992). The book thus gives the computational linguist a good idea of how to apply the basic knowledge-representation ideas of inheritance and typing to state-of-the-art linguistic analyses. It also complements the more theoretically oriented works of Carpenter (1992) and Keller (1993) on typed-feature structures and their logic. So, although its intended audience is clearly primarily linguists, this book is essential Computational Linguistics Volume 22, Number 1 reading for anybody interested in building an NLP system with a nontrivial syntactic component. All the more so, since Pollard and Sag, in order to challenge the dominant Principles and Parameters syntactic framework of Chomsky and his associates (Chomsky 1981, 1986), are meticulous in comparing their theory to the alternatives of Principles and Parameters: the book provides a welcome cross-theore</context>
<context position="5701" citStr="Carpenter (1992)" startWordPosition="839" endWordPosition="840">tematic use of typing of feature structures (not unlike the use of templates in PATR-II). It is similar to DATR (Evans and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&amp;S de facto use a strict inheritance scheme in their analyses, rather than the default inheritance sometimes used in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the formalism that HPSG uses, which owes a lot to King (1989), is very close to that discussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar to that developed by Kasper and Rounds (1986) or Keller (1993). Typing in HPSG is used to factor out shared properties of linguistic objects, be they words, phrases, or anything else, into appropriate classes. Typing is also used to restrict the application of general principles to the right class of linguistic structures. The Head-Feature Principle, for example, which identifies the relevant syntactic properties of a phrasal head with that of its mother node, is restricted to headed structures (i.e., objects of the type headed structure). </context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Carpenter, Bob. (1992). The Logic of Typed Feature Structures. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding.</booktitle>
<location>Dordrecht: Foris.</location>
<contexts>
<context position="2460" citStr="Chomsky 1981" startWordPosition="350" endWordPosition="351">n ideas of inheritance and typing to state-of-the-art linguistic analyses. It also complements the more theoretically oriented works of Carpenter (1992) and Keller (1993) on typed-feature structures and their logic. So, although its intended audience is clearly primarily linguists, this book is essential Computational Linguistics Volume 22, Number 1 reading for anybody interested in building an NLP system with a nontrivial syntactic component. All the more so, since Pollard and Sag, in order to challenge the dominant Principles and Parameters syntactic framework of Chomsky and his associates (Chomsky 1981, 1986), are meticulous in comparing their theory to the alternatives of Principles and Parameters: the book provides a welcome cross-theoretical discussion of all major syntactic issues. For readers interested in either HPSG or German syntax, German in Head-driven Phrase-structure Grammar is also highly recommended: it presents more current, cuttingedge research and gives an idea of the kinds of questions an HPSG approach raises. It also includes several technical and theoretical innovations that have become increasingly popular within the HPSG community—function composition or generalized ra</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, Noam. (1981). Lectures on Government and Binding. Dordrecht: Foris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1986</date>
<publisher>The MIT Press.</publisher>
<location>Barriers. Cambridge, MA:</location>
<marker>Chomsky, 1986</marker>
<rawString>Chomsky, Noam. (1986). Barriers. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
</authors>
<title>The Syntax of Anaphoric Binding. Stanford, CA: Center for the Study of Language and Information.</title>
<date>1993</date>
<contexts>
<context position="12994" citStr="Dalrymple (1993)" startWordPosition="1944" endWordPosition="1945">hough such disjunctive definition of the relation crucial to anaphoric binding is not unique to HPSG, it still requires an explanation. A final point worth mentioning about P&amp;S&apos;s binding theory: despite some mention of cross-linguistic data, the theory presented covers mostly English anaphors. Recent work by Xue, Pollard, and Sag (1994) has extended HPSG binding theory, parameterizing it somewhat to cover the Chinese long-distance anaphor ziji. But the theory does not as yet cover the full range of attested grammatically-governed anaphors, discussed for example in Manzini and Wexler (1987) or Dalrymple (1993). Comparison with Dalrymple&apos;s theory in particular would have been particularly useful, given the overall formal similarities between LFG and HPSG. Chapters 4 and 5 and half of chapter 9 are devoted to unbounded dependencies. Technically, the modeling of unbounded dependencies in HPSG (leaving aside the reformulation in chapter 9, for now) is similar to the classic SLASH percolation method used by Gazdar et al. (1985). There are three major differences between the two treatments. 1. The SLASH feature is set-valued, thus allowing for multiple gaps. 132 Book Reviews 2. Coordinate structures are </context>
</contexts>
<marker>Dalrymple, 1993</marker>
<rawString>Dalrymple, Mary. (1993). The Syntax of Anaphoric Binding. Stanford, CA: Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<date>1989</date>
<booktitle>Inference in DATR. Proceedings, 4th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>66--71</pages>
<location>Manchester,</location>
<contexts>
<context position="5219" citStr="Evans and Gazdar 1989" startWordPosition="760" endWordPosition="763">ical formalism that belongs to the growing family of frameworks using feature structures as their basic data structure. In contrast to other feature-based frameworks, such as PAIR-IT (Shieber 1986, Shieber et al. 1983), LFG, or GPSG, HPSG does not rely on a context-free backbone; constituency is only one of the attributes of the linguistic object par excellence, the linguistic sign. It is on a par with other syntactic and semantic attributes. Moreover, HPSG is characterized by a systematic use of typing of feature structures (not unlike the use of templates in PATR-II). It is similar to DATR (Evans and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&amp;S de facto use a strict inheritance scheme in their analyses, rather than the default inheritance sometimes used in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the formalism that HPSG uses, which owes a lot to King (1989), is very close to that discussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar to that developed by Kasper and Rounds (1986) or Keller (1993). </context>
</contexts>
<marker>Evans, Gazdar, 1989</marker>
<rawString>Evans, Roger and Gazdar, Gerald. (1989a). Inference in DATR. Proceedings, 4th Conference of the European Chapter of the Association for Computational Linguistics, Manchester, 66-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<title>The semantics of DATR.</title>
<date>1989</date>
<booktitle>Proceedings, 7th Conference of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour,</booktitle>
<pages>79--87</pages>
<contexts>
<context position="5219" citStr="Evans and Gazdar 1989" startWordPosition="760" endWordPosition="763">ical formalism that belongs to the growing family of frameworks using feature structures as their basic data structure. In contrast to other feature-based frameworks, such as PAIR-IT (Shieber 1986, Shieber et al. 1983), LFG, or GPSG, HPSG does not rely on a context-free backbone; constituency is only one of the attributes of the linguistic object par excellence, the linguistic sign. It is on a par with other syntactic and semantic attributes. Moreover, HPSG is characterized by a systematic use of typing of feature structures (not unlike the use of templates in PATR-II). It is similar to DATR (Evans and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&amp;S de facto use a strict inheritance scheme in their analyses, rather than the default inheritance sometimes used in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the formalism that HPSG uses, which owes a lot to King (1989), is very close to that discussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar to that developed by Kasper and Rounds (1986) or Keller (1993). </context>
</contexts>
<marker>Evans, Gazdar, 1989</marker>
<rawString>Evans, Roger and Gazdar, Gerald. (1989b). The semantics of DATR. Proceedings, 7th Conference of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour, 79-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
<author>Paul Kay</author>
</authors>
<title>On construction grammar. Unpublished ms.,</title>
<date>1993</date>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="18693" citStr="Fillmore and Kay (1993)" startWordPosition="2846" endWordPosition="2849">n the head-modifier-structure schema. The (necessary) presence of &amp;quot;[TO-BINDIREL {#1}1&amp;quot; on person as used in (3a) therefore remains mysterious. One possible solution is to introduce a special phrase-structural schema (of the form N&apos; N&apos; Re1C1) that would directly introduce this feature specification. Introducing a schema specific to relative clauses suggests another kind of solution within HPSG that would not rely on empty relativizers. Indeed, within a typed grammar using multiple inheritance, another kind of analysis is possible, using a crossclassification of phrases (see unpublished work by Fillmore and Kay (1993) and Sag (1994)). The basic idea is to distinguish between the kind of phrase and the kind of clause instantiated by a given constituent structure (say head-complement/head-filler structures vs. interrogative/declarative/relative structures). One can then define a particular kind of relative clause as inheriting from both the head-filler and relative-clause types. The relevant properties of relative clauses are ascribed directly to this particular subtype of phrase, rather than being projected from an empty relativizer. The reason that such a solution was not adopted by P&amp;S, I suspect, comes f</context>
</contexts>
<marker>Fillmore, Kay, 1993</marker>
<rawString>Fillmore, Charles and Kay, Paul. (1993). On construction grammar. Unpublished ms., University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey K Pullum</author>
<author>Ivan Sag</author>
</authors>
<date>1985</date>
<booktitle>Generalized Phrase Structure Grammar.</booktitle>
<publisher>Basil Blackwell.</publisher>
<location>Oxford:</location>
<contexts>
<context position="3716" citStr="Gazdar et al. (1985)" startWordPosition="530" endWordPosition="533"> For reasons of space, this review will focus only on Pollard and Sag&apos;s book. It should be noted, though, that, unlike Pollard and Sag&apos;s book, German in Head-driven Phrase-structure Grammar is editorially poor: typos and incorrect referencing of examples abound. The topics covered by Polland and Sag (henceforth P&amp;S) include all the usual major syntactic phenomena that grammatical theories must account for: agreement, subcategorization, unbounded dependencies (including parasitic gaps and relative clauses), control, binding theory, and, to a small extent, quantification. As in the tradition of Gazdar et al. (1985), the analyses are detailed and made precise, so that a fair evaluation is possible. Moreover, the formalism used is similar enough to by-now traditional NLP grammar formalisms (in particular feature-based grammars) that readers without prior knowledge of HPSG implementations can have a reasonable idea of how to implement the analyses. As mentioned, the book carefully compares P&amp;S&apos;s proposals to standard analyses within Principles and Parameters. Unfortunately, there is much less explicit comparison with work done within frameworks intellectually closer to HPSG, such as lexical-functional gram</context>
<context position="13415" citStr="Gazdar et al. (1985)" startWordPosition="2008" endWordPosition="2011">ese long-distance anaphor ziji. But the theory does not as yet cover the full range of attested grammatically-governed anaphors, discussed for example in Manzini and Wexler (1987) or Dalrymple (1993). Comparison with Dalrymple&apos;s theory in particular would have been particularly useful, given the overall formal similarities between LFG and HPSG. Chapters 4 and 5 and half of chapter 9 are devoted to unbounded dependencies. Technically, the modeling of unbounded dependencies in HPSG (leaving aside the reformulation in chapter 9, for now) is similar to the classic SLASH percolation method used by Gazdar et al. (1985). There are three major differences between the two treatments. 1. The SLASH feature is set-valued, thus allowing for multiple gaps. 132 Book Reviews 2. Coordinate structures are not headed, thus allowing a crucial separation of the Coordinate Structure Constraint from the theory of parasitic gaps (a dissociation necessary to account for languages, such as Swedish, that obey the Coordinate Structure Constraint, but allow extraction out of subjects without the presence of other gaps in the sentence). 3. Traces are no longer a necessary part of an adequate treatment of unbounded dependencies (se</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan. (1985). Generalized Phrase Structure Grammar. Oxford: Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Godard</author>
<author>Ivan Sag</author>
</authors>
<title>Reflexivization and intransitivity: The case of French.&amp;quot; Paper presented at the annual meeting of the Linguistic Society of America.</title>
<date>1995</date>
<contexts>
<context position="22248" citStr="Godard and Sag (1995)" startWordPosition="3394" endWordPosition="3397">r to Bresnan&apos;s (1982) notion of lexical rule (p. 37) (itself borrowed from jackendoff (1975)). But Bresnan&apos;s original view of lexical rules is that they function as redundancy rules over separately listed lexical entries. This view is implausible, given the use of lexical rules in HPSG to model productive inflectional morphology (already suggested in P&amp;S (1987)) and the postulation of two entries for eat in What did you want to eat? and Joe wanted to eat pasta (see Krieger and Nerbonne (1993) or Koenig and jurafsky (1994) for some of the difficulties associated with lexical rules in HPSG, and Godard and Sag (1995) for a response). This somewhat critical summary of P&amp;S&apos;s major analyses does not give a good idea of the book&apos;s richness and scholarship. The empirical coverage and the savvy of the analyses is truly remarkable. Moreover, the structure of the type hierarchy being assumed, the relevant type declarations, and the principles being proposed are all laid out and summarized in the appendix, so that the reader can easily assess the proposals: a welcome relief to anybody accustomed to recent syntactic work! Overall, this is a linguistic book that ought to be on every computational linguist&apos;s shelf an</context>
</contexts>
<marker>Godard, Sag, 1995</marker>
<rawString>Godard, Daniele and Sag, Ivan. (1995). &amp;quot;Reflexivization and intransitivity: The case of French.&amp;quot; Paper presented at the annual meeting of the Linguistic Society of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Ingria</author>
</authors>
<title>The limits of unification.</title>
<date>1990</date>
<booktitle>Proceedings, 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Pittsburgh,</location>
<contexts>
<context position="8753" citStr="Ingria (1990)" startWordPosition="1288" endWordPosition="1289">ng and illustrates perfectly the theoretical value of integrating various kinds of information in each constituent or sign, a hallmark of sign-based grammatical theories. Similarities and differences between various kinds of agreements attested cross-linguistically can be easily modeled. Despite its success, one important issue is not addressed. Agreement, P&amp;S claim, involves unification (more precisely token-identity of indices). There are instances of agreement in several languages, though, which do not seem to involve token-identity and unification, but rather feature-checking, as shown by Ingria (1990) or Pullum and Zwicky (1986). A discussion of these challenges would have been welcome. Chapter 6 proposes a theory of anaphoric binding that does not rely on configurational notions such as c-command, but rather on the notion of relative obliqueness of the antecedent and anaphor. Relative obliqueness is defined (roughly) in terms of the relative order of the antecedent and anaphor in the subcategorization lists of predicators. Furthermore, many classes of sentences involving anaphors generally assumed to be subject to grammatical constraints are in fact subject, P&amp;S claim, to pragmatic constr</context>
</contexts>
<marker>Ingria, 1990</marker>
<rawString>Ingria, Robert. (1990). The limits of unification. Proceedings, 28th Annual Meeting of the Association for Computational Linguistics, Pittsburgh, 194-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Morphological and Semantic Regularities in the Lexicon.&amp;quot;</title>
<date>1975</date>
<journal>Language,</journal>
<volume>51</volume>
<issue>3</issue>
<pages>639--671</pages>
<marker>Jackendoff, 1975</marker>
<rawString>Jackendoff, Ray. (1975). &amp;quot;Morphological and Semantic Regularities in the Lexicon.&amp;quot; Language, 51(3): 639-671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Kasper</author>
<author>William Rounds</author>
</authors>
<title>A Logical Semantics for Feature Structures.</title>
<date>1986</date>
<booktitle>Proceedings, 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>257--266</pages>
<location>New York,</location>
<contexts>
<context position="5800" citStr="Kasper and Rounds (1986)" startWordPosition="855" endWordPosition="858">t is similar to DATR (Evans and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&amp;S de facto use a strict inheritance scheme in their analyses, rather than the default inheritance sometimes used in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the formalism that HPSG uses, which owes a lot to King (1989), is very close to that discussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar to that developed by Kasper and Rounds (1986) or Keller (1993). Typing in HPSG is used to factor out shared properties of linguistic objects, be they words, phrases, or anything else, into appropriate classes. Typing is also used to restrict the application of general principles to the right class of linguistic structures. The Head-Feature Principle, for example, which identifies the relevant syntactic properties of a phrasal head with that of its mother node, is restricted to headed structures (i.e., objects of the type headed structure). Nonheaded structures are not subject to the principle. 130 Book Reviews As in most modern syntactic</context>
</contexts>
<marker>Kasper, Rounds, 1986</marker>
<rawString>Kasper, Robert and Rounds, William. (1986). A Logical Semantics for Feature Structures. Proceedings, 24th Annual Meeting of the Association for Computational Linguistics, New York, 257-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Keller</author>
</authors>
<title>Feature Logics, Infinitary Descriptions, and Grammars. Stanford, CA: Center for the Study of Language and Information.</title>
<date>1993</date>
<contexts>
<context position="2018" citStr="Keller (1993)" startWordPosition="284" endWordPosition="285">ical framework used extensively in syntactically oriented research in natural language processing. Moreover, of the grammatical theories using inheritance-based grammars, a widespread tradition in the NLP community HPSG achieves the widest coverage (vide the special issues of Computational Linguistics devoted to this topic in 1992). The book thus gives the computational linguist a good idea of how to apply the basic knowledge-representation ideas of inheritance and typing to state-of-the-art linguistic analyses. It also complements the more theoretically oriented works of Carpenter (1992) and Keller (1993) on typed-feature structures and their logic. So, although its intended audience is clearly primarily linguists, this book is essential Computational Linguistics Volume 22, Number 1 reading for anybody interested in building an NLP system with a nontrivial syntactic component. All the more so, since Pollard and Sag, in order to challenge the dominant Principles and Parameters syntactic framework of Chomsky and his associates (Chomsky 1981, 1986), are meticulous in comparing their theory to the alternatives of Principles and Parameters: the book provides a welcome cross-theoretical discussion o</context>
<context position="5817" citStr="Keller (1993)" startWordPosition="860" endWordPosition="861">and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&amp;S de facto use a strict inheritance scheme in their analyses, rather than the default inheritance sometimes used in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the formalism that HPSG uses, which owes a lot to King (1989), is very close to that discussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar to that developed by Kasper and Rounds (1986) or Keller (1993). Typing in HPSG is used to factor out shared properties of linguistic objects, be they words, phrases, or anything else, into appropriate classes. Typing is also used to restrict the application of general principles to the right class of linguistic structures. The Head-Feature Principle, for example, which identifies the relevant syntactic properties of a phrasal head with that of its mother node, is restricted to headed structures (i.e., objects of the type headed structure). Nonheaded structures are not subject to the principle. 130 Book Reviews As in most modern syntactic frameworks, in p</context>
</contexts>
<marker>Keller, 1993</marker>
<rawString>Keller, Bill. (1993). Feature Logics, Infinitary Descriptions, and Grammars. Stanford, CA: Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul King</author>
</authors>
<title>A Logical Formalism for Head-driven Phrase Structure Grammar.</title>
<date>1989</date>
<institution>University of Manchester.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="5638" citStr="King (1989)" startWordPosition="827" endWordPosition="828">antic attributes. Moreover, HPSG is characterized by a systematic use of typing of feature structures (not unlike the use of templates in PATR-II). It is similar to DATR (Evans and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&amp;S de facto use a strict inheritance scheme in their analyses, rather than the default inheritance sometimes used in similar approaches (see Briscoe, de Paiva, and Copestake 1993). Overall, then, the formalism that HPSG uses, which owes a lot to King (1989), is very close to that discussed in detail in Carpenter (1992), for example. But for typing it uses a logic similar to that developed by Kasper and Rounds (1986) or Keller (1993). Typing in HPSG is used to factor out shared properties of linguistic objects, be they words, phrases, or anything else, into appropriate classes. Typing is also used to restrict the application of general principles to the right class of linguistic structures. The Head-Feature Principle, for example, which identifies the relevant syntactic properties of a phrasal head with that of its mother node, is restricted to h</context>
</contexts>
<marker>King, 1989</marker>
<rawString>King, Paul. (1989). A Logical Formalism for Head-driven Phrase Structure Grammar. Ph.D. dissertation, University of Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Pierre Koenig</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Type Underspecification and On-line Type Construction</title>
<date>1994</date>
<booktitle>in the Lexicon. Proceedings, 13th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information,</booktitle>
<pages>270--285</pages>
<marker>Koenig, Jurafsky, 1994</marker>
<rawString>Koenig, Jean-Pierre and Jurafsky, Daniel. (1994). Type Underspecification and On-line Type Construction in the Lexicon. Proceedings, 13th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information, 270-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans-Ulrich Krieger</author>
<author>John Nerbonne</author>
</authors>
<title>Feature-based inheritance networks for computational lexicons.&amp;quot; In Inheritance, Defaults, and the Lexicon, edited by Ted Briscoe, Valeria de Paiva, and Ann Copestake.</title>
<date>1993</date>
<pages>90--136</pages>
<publisher>Cambridge Univeristy Press,</publisher>
<contexts>
<context position="22124" citStr="Krieger and Nerbonne (1993)" startWordPosition="3373" endWordPosition="3376">ave been welcome. In fact, some of the few statements about this theory made in the 134 Book Reviews book are misleading: P&amp;S refer to Bresnan&apos;s (1982) notion of lexical rule (p. 37) (itself borrowed from jackendoff (1975)). But Bresnan&apos;s original view of lexical rules is that they function as redundancy rules over separately listed lexical entries. This view is implausible, given the use of lexical rules in HPSG to model productive inflectional morphology (already suggested in P&amp;S (1987)) and the postulation of two entries for eat in What did you want to eat? and Joe wanted to eat pasta (see Krieger and Nerbonne (1993) or Koenig and jurafsky (1994) for some of the difficulties associated with lexical rules in HPSG, and Godard and Sag (1995) for a response). This somewhat critical summary of P&amp;S&apos;s major analyses does not give a good idea of the book&apos;s richness and scholarship. The empirical coverage and the savvy of the analyses is truly remarkable. Moreover, the structure of the type hierarchy being assumed, the relevant type declarations, and the principles being proposed are all laid out and summarized in the appendix, so that the reader can easily assess the proposals: a welcome relief to anybody accusto</context>
</contexts>
<marker>Krieger, Nerbonne, 1993</marker>
<rawString>Krieger, Hans-Ulrich and Nerbonne, John. (1993). &amp;quot;Feature-based inheritance networks for computational lexicons.&amp;quot; In Inheritance, Defaults, and the Lexicon, edited by Ted Briscoe, Valeria de Paiva, and Ann Copestake. Cambridge Univeristy Press, 90-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Rita Manzini</author>
<author>Kenneth Wexler</author>
</authors>
<date>1987</date>
<journal>Parameters, Binding, and Learnability. Linguistic Inquiry,</journal>
<volume>18</volume>
<issue>3</issue>
<pages>413--444</pages>
<contexts>
<context position="12974" citStr="Manzini and Wexler (1987)" startWordPosition="1939" endWordPosition="1942"> independently motivated. Although such disjunctive definition of the relation crucial to anaphoric binding is not unique to HPSG, it still requires an explanation. A final point worth mentioning about P&amp;S&apos;s binding theory: despite some mention of cross-linguistic data, the theory presented covers mostly English anaphors. Recent work by Xue, Pollard, and Sag (1994) has extended HPSG binding theory, parameterizing it somewhat to cover the Chinese long-distance anaphor ziji. But the theory does not as yet cover the full range of attested grammatically-governed anaphors, discussed for example in Manzini and Wexler (1987) or Dalrymple (1993). Comparison with Dalrymple&apos;s theory in particular would have been particularly useful, given the overall formal similarities between LFG and HPSG. Chapters 4 and 5 and half of chapter 9 are devoted to unbounded dependencies. Technically, the modeling of unbounded dependencies in HPSG (leaving aside the reformulation in chapter 9, for now) is similar to the classic SLASH percolation method used by Gazdar et al. (1985). There are three major differences between the two treatments. 1. The SLASH feature is set-valued, thus allowing for multiple gaps. 132 Book Reviews 2. Coordi</context>
</contexts>
<marker>Manzini, Wexler, 1987</marker>
<rawString>Manzini, Maria Rita and Wexler, Kenneth. (1987). Parameters, Binding, and Learnability. Linguistic Inquiry, 18(3): 413-444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Information-based Syntax and Semantics, Volume 1: Fundamentals. Stanford, CA: Center for the Study of Language and Information.</title>
<date>1987</date>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, Carl and Sag, Ivan. (1987). Information-based Syntax and Semantics, Volume 1: Fundamentals. Stanford, CA: Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
<author>Arnold Zwicky</author>
</authors>
<date>1986</date>
<journal>Phonological Resolution of Syntactic Feature Conflict. Language,</journal>
<volume>62</volume>
<issue>4</issue>
<pages>751--774</pages>
<contexts>
<context position="8781" citStr="Pullum and Zwicky (1986)" startWordPosition="1291" endWordPosition="1294">s perfectly the theoretical value of integrating various kinds of information in each constituent or sign, a hallmark of sign-based grammatical theories. Similarities and differences between various kinds of agreements attested cross-linguistically can be easily modeled. Despite its success, one important issue is not addressed. Agreement, P&amp;S claim, involves unification (more precisely token-identity of indices). There are instances of agreement in several languages, though, which do not seem to involve token-identity and unification, but rather feature-checking, as shown by Ingria (1990) or Pullum and Zwicky (1986). A discussion of these challenges would have been welcome. Chapter 6 proposes a theory of anaphoric binding that does not rely on configurational notions such as c-command, but rather on the notion of relative obliqueness of the antecedent and anaphor. Relative obliqueness is defined (roughly) in terms of the relative order of the antecedent and anaphor in the subcategorization lists of predicators. Furthermore, many classes of sentences involving anaphors generally assumed to be subject to grammatical constraints are in fact subject, P&amp;S claim, to pragmatic constraints on anaphors (be it NPs</context>
</contexts>
<marker>Pullum, Zwicky, 1986</marker>
<rawString>Pullum, Geoffrey K. and Zwicky, Arnold. (1986). Phonological Resolution of Syntactic Feature Conflict. Language, 62(4): 751-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
<author>Arnold Zwicky</author>
</authors>
<title>A Misconceived Approach to Morphology.</title>
<date>1991</date>
<booktitle>Proceedings, 10th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information,</booktitle>
<pages>387--398</pages>
<contexts>
<context position="1288" citStr="Pullum and Zwicky (1991)" startWordPosition="179" endWordPosition="182">the Study of Learning and Information (CSLI lecture notes 46), 1994, xi+404 pp; distributed by the University of Chicago Press; hardbound, ISBN 1-881526-30-5, $49.95, £39.95; paperbound, ISBN 1-881526-29-1, $21.95, £17.50 Reviewed by Jean-Pierre Koenig State University of New York at Buffalo It has been almost ten years since the classic Generalized Phrase Structure Grammar (Gazdar, Klein, Pullum, and Sag 1985) appeared. And like its predecessor, Head-driven Phrase Structure Grammar will probably become a classic too. Head-driven phrase structure grammar (HPSG) is the state of the art in what Pullum and Zwicky (1991) have called category-based syntax, and this book makes available to a wide audience recent developments in a grammatical framework used extensively in syntactically oriented research in natural language processing. Moreover, of the grammatical theories using inheritance-based grammars, a widespread tradition in the NLP community HPSG achieves the widest coverage (vide the special issues of Computational Linguistics devoted to this topic in 1992). The book thus gives the computational linguist a good idea of how to apply the basic knowledge-representation ideas of inheritance and typing to sta</context>
</contexts>
<marker>Pullum, Zwicky, 1991</marker>
<rawString>Pullum, Geoffrey K. and Zwicky, Arnold. (1991). A Misconceived Approach to Morphology. Proceedings, 10th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information, 387-398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Sag</author>
</authors>
<title>Relative Clauses—A Multiple Inheritance Analysis.&amp;quot;</title>
<date>1994</date>
<booktitle>Paper presented at the HPSG 1994 Conference,</booktitle>
<location>Copenhagen.</location>
<contexts>
<context position="12716" citStr="Sag (1994)" startWordPosition="1901" endWordPosition="1902">nciple. How to reconcile both aspects of control needs further study. Second, the treatment of control as involving a reflexive anaphor forces the addition of a separate clause in the definition of local o-command that, to my knowledge, is not independently motivated. Although such disjunctive definition of the relation crucial to anaphoric binding is not unique to HPSG, it still requires an explanation. A final point worth mentioning about P&amp;S&apos;s binding theory: despite some mention of cross-linguistic data, the theory presented covers mostly English anaphors. Recent work by Xue, Pollard, and Sag (1994) has extended HPSG binding theory, parameterizing it somewhat to cover the Chinese long-distance anaphor ziji. But the theory does not as yet cover the full range of attested grammatically-governed anaphors, discussed for example in Manzini and Wexler (1987) or Dalrymple (1993). Comparison with Dalrymple&apos;s theory in particular would have been particularly useful, given the overall formal similarities between LFG and HPSG. Chapters 4 and 5 and half of chapter 9 are devoted to unbounded dependencies. Technically, the modeling of unbounded dependencies in HPSG (leaving aside the reformulation in </context>
<context position="18708" citStr="Sag (1994)" startWordPosition="2851" endWordPosition="2852">e schema. The (necessary) presence of &amp;quot;[TO-BINDIREL {#1}1&amp;quot; on person as used in (3a) therefore remains mysterious. One possible solution is to introduce a special phrase-structural schema (of the form N&apos; N&apos; Re1C1) that would directly introduce this feature specification. Introducing a schema specific to relative clauses suggests another kind of solution within HPSG that would not rely on empty relativizers. Indeed, within a typed grammar using multiple inheritance, another kind of analysis is possible, using a crossclassification of phrases (see unpublished work by Fillmore and Kay (1993) and Sag (1994)). The basic idea is to distinguish between the kind of phrase and the kind of clause instantiated by a given constituent structure (say head-complement/head-filler structures vs. interrogative/declarative/relative structures). One can then define a particular kind of relative clause as inheriting from both the head-filler and relative-clause types. The relevant properties of relative clauses are ascribed directly to this particular subtype of phrase, rather than being projected from an empty relativizer. The reason that such a solution was not adopted by P&amp;S, I suspect, comes from linguists&apos; </context>
</contexts>
<marker>Sag, 1994</marker>
<rawString>Sag, Ivan. (1994). &amp;quot;Relative Clauses—A Multiple Inheritance Analysis.&amp;quot; Paper presented at the HPSG 1994 Conference, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Sag</author>
<author>Janet D Fodor</author>
</authors>
<title>Extractions Without Traces.</title>
<date>1994</date>
<booktitle>Proceedings, 13th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information,</booktitle>
<pages>365--384</pages>
<contexts>
<context position="14518" citStr="Sag and Fodor (1994)" startWordPosition="2182" endWordPosition="2185">aps in the sentence). 3. Traces are no longer a necessary part of an adequate treatment of unbounded dependencies (see below for more on this). The detail dnd empirical coverage of the analyses make these chapters stand out. An added bonus is the treatment of relative clauses of a kind different from those present in English, namely correlative relative clauses and internally headed relative clauses. An important revision in the theory of unbounded dependencies is suggested in the final chapter of the book, where a theory of unbounded dependencies that does not rely on traces is proposed (see Sag and Fodor (1994) for an extension of the same idea). The gist of the modification is simple: rather than introduce the bottom of unbounded dependencies by means of a particular empty sign with a non-null SLASH value, the bottom of unbounded dependencies is introduced by a lexical rule that passes the (nonlocally satisfied) valence requirement from the SUBCAT list to the SLASH set of the relevant predicator. The newly created predicator entry then serves as the bottom of the dependency; the rest of the theory remains unchanged. Such a move illustrates the increasingly important role of lexical rules in HPSG (s</context>
</contexts>
<marker>Sag, Fodor, 1994</marker>
<rawString>Sag, Ivan and Fodor, Janet D. (1994). Extractions Without Traces. Proceedings, 13th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information, 365-384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>An Introduction to Unification-based Approaches to Grammar. Stanford, CA: Center for the Study of Language and Information.</title>
<date>1986</date>
<contexts>
<context position="4794" citStr="Shieber 1986" startWordPosition="689" endWordPosition="690">there is much less explicit comparison with work done within frameworks intellectually closer to HPSG, such as lexical-functional grammar (LFG) (Bresnan 1982). Finally, although the book focuses mainly on English syntax, P&amp;S attempt to provide a cross-linguistic perspective in several chapters, in particular when dealing with agreement and relative clauses. To the novice reader, HPSG is a constraint-based grammatical formalism that belongs to the growing family of frameworks using feature structures as their basic data structure. In contrast to other feature-based frameworks, such as PAIR-IT (Shieber 1986, Shieber et al. 1983), LFG, or GPSG, HPSG does not rely on a context-free backbone; constituency is only one of the attributes of the linguistic object par excellence, the linguistic sign. It is on a par with other syntactic and semantic attributes. Moreover, HPSG is characterized by a systematic use of typing of feature structures (not unlike the use of templates in PATR-II). It is similar to DATR (Evans and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&amp;S de facto u</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart. (1986). An Introduction to Unification-based Approaches to Grammar. Stanford, CA: Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Hans Uszkoreit</author>
<author>Jane Robinson</author>
<author>Mabry Tyson</author>
</authors>
<title>The Formalism and Implementation of PATR-II.&amp;quot;</title>
<date>1983</date>
<booktitle>In Research on Interactive Acquisition and Use of Knowledge, Menlo Park, CA: Artificial Intelligence Center, SRI International.</booktitle>
<contexts>
<context position="4816" citStr="Shieber et al. 1983" startWordPosition="691" endWordPosition="694">less explicit comparison with work done within frameworks intellectually closer to HPSG, such as lexical-functional grammar (LFG) (Bresnan 1982). Finally, although the book focuses mainly on English syntax, P&amp;S attempt to provide a cross-linguistic perspective in several chapters, in particular when dealing with agreement and relative clauses. To the novice reader, HPSG is a constraint-based grammatical formalism that belongs to the growing family of frameworks using feature structures as their basic data structure. In contrast to other feature-based frameworks, such as PAIR-IT (Shieber 1986, Shieber et al. 1983), LFG, or GPSG, HPSG does not rely on a context-free backbone; constituency is only one of the attributes of the linguistic object par excellence, the linguistic sign. It is on a par with other syntactic and semantic attributes. Moreover, HPSG is characterized by a systematic use of typing of feature structures (not unlike the use of templates in PATR-II). It is similar to DATR (Evans and Gazdar 1989a, 1989b) in this respect. HPSG uses a multiple-inheritance scheme over those types to cross-classify linguistic objects. Although they leave the question open, P&amp;S de facto use a strict inheritanc</context>
</contexts>
<marker>Shieber, Uszkoreit, Robinson, Tyson, 1983</marker>
<rawString>Shieber, Stuart; Uszkoreit, Hans; Robinson, Jane; and Tyson, Mabry. (1983). &amp;quot;The Formalism and Implementation of PATR-II.&amp;quot; In Research on Interactive Acquisition and Use of Knowledge, Menlo Park, CA: Artificial Intelligence Center, SRI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Xue</author>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>A New Perspective on</title>
<date>1994</date>
<booktitle>Chinese &apos;ziji&apos;. Proceedings, 13th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information,</booktitle>
<pages>432--447</pages>
<marker>Xue, Pollard, Sag, 1994</marker>
<rawString>Xue, Ping; Pollard, Carl; and Sag, Ivan. (1994). A New Perspective on Chinese &apos;ziji&apos;. Proceedings, 13th West Coast Conference on Formal Linguistics, Stanford, CA: Center for the Study of Language and Information, 432-447.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jean-Pierre</author>
</authors>
<title>Koenig is Assistant Professor in the Linguistics Department at the State University of New York at Buffalo. His main research interests are in lexical knowledge representation. He has worked recently on developing a notion of lexical relatedness that does not rely on lexical rules, but rather on underspecified types. Koenig&apos;s address is 684 Baldy Hall, SUNY at Buffalo,</title>
<location>Buffalo, NY</location>
<note>14052; e-mail: jpkoenig@acsu.buffalo.edu.</note>
<marker>Jean-Pierre, </marker>
<rawString>Jean-Pierre Koenig is Assistant Professor in the Linguistics Department at the State University of New York at Buffalo. His main research interests are in lexical knowledge representation. He has worked recently on developing a notion of lexical relatedness that does not rely on lexical rules, but rather on underspecified types. Koenig&apos;s address is 684 Baldy Hall, SUNY at Buffalo, Buffalo, NY 14052; e-mail: jpkoenig@acsu.buffalo.edu.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>