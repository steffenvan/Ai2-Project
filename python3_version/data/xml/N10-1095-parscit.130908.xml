<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008059">
<title confidence="0.94624">
Reranking the Berkeley and Brown Parsers*
</title>
<author confidence="0.998352">
Mark Johnson
</author>
<affiliation confidence="0.996544">
Department of Computing
Macquarie University
</affiliation>
<address confidence="0.5665">
Sydney, Australia
</address>
<email confidence="0.997246">
mjohnson@science.mq.edu.au
</email>
<author confidence="0.726377">
Ahmet Engin Ural
</author>
<affiliation confidence="0.7895895">
Cognitive and Linguistic Sciences
Brown University
</affiliation>
<address confidence="0.942414">
Providence, RI, USA
</address>
<email confidence="0.999065">
aeural@gmail.com
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999830636363636">
The Brown and the Berkeley parsers are two
state-of-the-art generative parsers. Since both
parsers produce n-best lists, it is possible to
apply reranking techniques to the output of
both of these parsers, and to their union. We
note that the standard reranker feature set dis-
tributed with the Brown parser does not do
well with the Berkeley parser, and propose an
extended set that does better. An ablation ex-
periment shows that different parsers benefit
from different reranker features.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979894150943396">
Syntactic parsing is the task of identifying the
phrases and clauses in natural language sentences.
It has been intensively studied primarily because it
is generally believed that identifying syntactic struc-
ture is a first step towards semantic interpretation.
This paper focuses on parsing the Wall Street Jour-
nal (WSJ) section of the University of Pennsylva-
nia treebank corpus (Marcus et al., 1993). There
are a large number of different approaches to this
task. For simplicity we focus on two popular gener-
ative statistical parsing models: Charniak’s “Maxi-
mum Entropy Inspired” parser (Charniak and John-
son, 2005) and Petrov’s “split-merge” parser (Petrov
et al., 2006). We follow conventional informal usage
and refer to these as the “Brown” and the “Berkeley”
parsers respectively.
* We would like to thank Eugene Charniak and the other
members of BLLIP for their helpful advice on this work. Natu-
rally all errors remain our own.
Briefly, the Berkeley parser is a smoothed PCFG
whose non-terminals are refinements of the original
treebank grammar obtained by an automatic split-
merge procedure, while the Brown parser is effec-
tively a smoothed PCFG whose non-terminals en-
code a wide variety of manually chosen condition-
ing information, such as heads, governors, etc. The
Berkeley parser is usually viewed as unlexicalized
(although the preterminals may be split so finely
that they may be viewed as identifying lexical clus-
ters), while essentially every distribution used in
the Brown parser conditions on lexical information.
Even from this cursory description it is clear that
these parsers parsers extract generalizations from the
training data in different ways.
This paper applies reranking (Collins and Koo,
2005) to the n-best output of both parsers individ-
ually, as well as to an n-best list consisting of the
union of the outputs of both parsers. We are inter-
ested to see whether the same kinds of features im-
prove the performance of both the Berkeley and the
Brown parsers, or whether successful reranking re-
quires features that are specially tuned to the parser
it is applied to. Finally, we are interested in the
performance of the reranker trained on the union n-
best lists. Combining the output of multiple parsers
in other more complex ways has been previously
demonstrated to improve overall accuracy, so it is in-
teresting to see if the relatively simple method used
here improves parsing accuracy as well.
The approach of Zhang et al. (2009) is closest to
the work described here. They combine n-best lists
produced by the same parsers as we do, but use only
a relatively small set of features (the log probabil-
</bodyText>
<page confidence="0.971996">
665
</page>
<subsubsectionHeader confidence="0.48891">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 665–668,
</subsubsectionHeader>
<page confidence="0.475708">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</page>
<figure confidence="0.8997522">
Trees Reranker features
standard extended
91.6 91.7
91.8 91.6
91.8 91.9
</figure>
<tableCaption confidence="0.8484668">
Table 1: The f-scores on section 22 of rerankers trained
on folds 1–18 by minimizing a regularized “MaxEnt” ob-
jective (negative log likelihood with a Gaussian regular-
izer) using L-BFGS. The weight of the regularizer was
tuned to optimize f-score on folds 19–20.
</tableCaption>
<bodyText confidence="0.999957111111111">
ity of the parses plus a constituent overlap feature),
while we investigate models with millions of fea-
tures here. They report a higher f-score than we do
when they replace the generative Brown parser with
the the self-trained discriminatively-reranked parser
of McClosky et al. (2006), but with inputs provided
by the generative Berkeley and Brown n-best parsers
they report an f-score of 91.43 on section 23, which
is consistent with the results reported here.
</bodyText>
<sectionHeader confidence="0.985665" genericHeader="introduction">
2 Experimental setup
</sectionHeader>
<bodyText confidence="0.999079823529412">
We ran both parsers in 50-best mode, and con-
structed 20-fold cross-validated training data as de-
scribed in Collins and Koo (2005) and Charniak
and Johnson (2005), i.e., the trees in sections 2–21
of the WSJ treebank were divided into 20 equal-
sized folds, and the parses for each fold were gen-
erated by a parser trained on the trees in the other
folds. Then sections 22, 23 and 24 were parsed us-
ing the standard “out-of-the-box” parser. Follow-
ing the suggestion in Collins and Koo (2005), in or-
der to avoid over-training on section 23 all rerank-
ing experiments reported here (except the final one)
used folds 1–18 as training data, used folds 19–20
as development data and used section 22 as test data.
(The averaged perceptron algorithm does not require
development data, so the experiments using that al-
gorithm report averages over folds 19–20 and sec-
tion 22).
The Berkeley parser can be run in many modes;
in order to produce the 20-fold training data we ran
the Berkeley trainer with 6 splits, and ran the re-
sulting parsers in “accurate” mode. It failed to pro-
duce any parses for 12 sentences in sections 2–21
and one sentence in section 24. The Brown parser
was trained using the “out-of-the-box” settings, and
produced parses for all sentences.
Using the reranker features distributed with the
Brown reranker (Charniak and Johnson, 2005),
which we call the “standard” set below, we ob-
tained no overall improvement in f-score when ei-
ther reranking the Berkeley parser n-best lists alone,
or when the Berkeley parses were combined with the
Brown parses.
However, it is possible that these results reflect
the fact that the features used by the reranker were
chosen because they improve the Brown parser, i.e.,
they are the result of feature selection based on
reranking the Brown parser’s n-best lists. In order
to determine if this is the case, we developed an “ex-
tended” feature set that incorporates a wider set of
features, specifically including features that capture
global properties of the tree that might be harder for
the Berkeley parser to learn.
Our extended feature set consists of
4,256,553 features, which are instances of
162 feature classes, which in turn are grouped
into 20 feature “super-classes”. By contrast, the
standard feature set contains 1,333,950 features in
90 feature classes, grouped into 14 super-classes.
A brief description of the extended feature set
super-classes follows:
</bodyText>
<tableCaption confidence="0.53885185">
Parser: an indicator feature indicating which parsers
generated this parse,
RelLogP: the log probability of this parse according to
each parser,
InterpLogCondP: an indicator feature based on the
binned log conditional probability according to
each parser,
RightBranch: an indicator function of each node that
lies on the right-most branch of the parse tree,
Heavy: an indicator function based on the size and lo-
cation of each nonterminal (designed to identify the
locations of “heavy” phrases),
LeftBranchLength: an indicator function of the binned
length of each left-branching chain,
RightBranchLength: an indicator function of the
binned length of each right-branching chain,
Rule: an indicator function of parent and children cate-
gories, optionally with head POS annotations,
NNGram: and indicator function of parent and n-gram
sequences of children categories, optionally head
</tableCaption>
<figure confidence="0.98080240625">
Berkeley
Brown
Combined
666
Parser
RelLogP
InterpLogCondP
RightBranch
Heavy
LeftBranchLength
RightBranchLength
Rule
NNGram
Heads
SynSemHeads
RBContext
SubjVerbAgr
CoPar
CoLenPar
Word
WProj
WEdges
NGramTree
HeadTree
Berkeley
Brown
Combined
0.1
0.0
-0.1
-0.2
-0.3
</figure>
<figureCaption confidence="0.9904864">
Figure 1: The average change in f-score on folds 19–20 and section 22 caused by removing a feature superclass
from the extended feature set and retraining. Difference in scores less that 0.1% are probably not significant. In this
experiment all rerankers were trained using the averaged perceptron algorithm. With the full extended feature set,
rerankers trained with the averaged perceptron algorithm achieve f-scores of 91.2% on both the Berkeley and Brown
parses, and 91.6% on the combined parses.
</figureCaption>
<bodyText confidence="0.983686107142857">
annotated, inspired by the n-gram rule features de-
scribed in Collins and Koo (2005),
Heads: an indicator function of “head-to-head” depen-
dencies,
SynSemHeads: an indicator function of the pair of syn-
tactic (i.e., functional) and semantic (i.e., lexical)
heads of each non-terminal,
RBContext: an indicator function of how much each
subtree deviates from from right-branching,
SubjVerbAgr: an indicator function of whether subject-
verb agreement is violated,
CoPar: an indicator function that fires when conjoined
phrases in a coordinate structure have approxi-
mately parallel syntactic structure,
CoLenPar: an indicator function that fires when con-
joined phrases in a coordinate structure have ap-
proximately the same length,
Word: an indicator function that identifies words and
their preterminals,
WProj: an indicator function that identifies words and
their phrasal projections up to their maximal pro-
jection,
WEdges: an indicator function that identifies the words
and POS tags appearing at the edges of each nonter-
minal,
NGramTree: an indicator function of the subtree con-
sisting of nodes connecting each pair of adjacent
words in the parse tree, and
HeadTree: a tree fragment consisting of a head word
and its projection up to its maximal projection, plus
all of the siblings of each node in this sequence (this
is like an auxiliary tree in a TAG).
The InterpLogCondP features were designed to
capture non-linearities in the way that the Berke-
ley and Brown parsers assign probabilities to trees.
We deliberately added features that incorporated lin-
guistic notions such as head, governor and maximal
projection, as the Berkeley parser does not explic-
itly condition on such information (in contrast to the
Brown parser, which does).
In fact, as the reader can verify the differences in
f-scores between rerankers containing the extended
features and the standard features is minimal. In
order to better study the importance of the various
features we conducted an ablation study, in which
we trained rerankers which were missing one feature
superclass from the 20 superclasses of the extended
feature set. In order to speed training time we used
the averaged perceptron algorithm (Collins, 2002)
(it converges an order of magnitude faster than the L-
BFGS algorithm we used in the other experiments,
but the f-score of the model estimated with the av-
eraged perceptron is approximately 0.1% lower than
when using L-BFGS). The results from this experi-
ment are shown in Figure 1. The averaged percep-
tron algorithm does not rely on the development data
</bodyText>
<page confidence="0.992876">
667
</page>
<bodyText confidence="0.999964380952381">
(folds 19–20), so the results we report are average f-
scores on the development data and on section 22
(we did this because the differences are small, so
a larger evaluation set may be able to detect differ-
ences more reliably).
It is interesting that linguistically-informed fea-
tures (such as Heads, SynSemHeads and HeadTree)
seem to be much more important when reranking
the combined n-best lists than when reranking the
output of either parser alone. This suggests that the
log probability scores from both parsers are inter-
nally consistent, but need to be recalibrated when
the parses are combined. The log probability scores
from the parsers themselves (in the form of the In-
terpLogCondP feature) are also supplying useful in-
formation that the reranker features on their own are
not providing. Finally, the WEdges feature, which
identifies the words and POS at the left and right
boundaries of each nonterminal, also provides ex-
tremely useful information, especially for reranking
the Berkeley parser.
</bodyText>
<sectionHeader confidence="0.999293" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999934111111111">
Reranking is a straight-forward method for improv-
ing the accuracy of n-best parsers. While one
might have hoped that reranking the n-best output
of the Berkeley parser, or the union of the outputs
of the Berkeley and Brown parsers, would dramat-
ically improve overall f-score, this seems not to be
the case. It’s possible that the features of current
rerankers have been implicitly designed to work well
with parsers like the Brown parser, but a reranker
with a dramatically enlarged feature set performs
only marginally better. This result was confirmed
by training a reranker with the extended features on
the union of the output of the Berkeley and Brown
parsers on sections 2–21 and testing on section 23
(i.e., the standard WSJ parsing evaluation), which
achieved an f-score of 91.49%; approximately 0.1%
higher than a reranker with the standard feature set
trained on the output of the Brown parser alone.
</bodyText>
<sectionHeader confidence="0.998848" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9926075">
This research was funded by NSF awards 0530118,
0544127 and 0631667.
</bodyText>
<sectionHeader confidence="0.995539" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999794210526316">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173–180, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25–70.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1–8. Association for
Computational Linguistics.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152–159, New
York City, USA, June. Association for Computational
Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433–440, Sydney, Aus-
tralia. Association for Computational Linguistics.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552–1560, Singapore. Association for Computational
Linguistics.
</reference>
<page confidence="0.996504">
668
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.048208">
<title confidence="0.989898">the Berkeley and Brown</title>
<author confidence="0.996657">Mark</author>
<affiliation confidence="0.986308">Department of</affiliation>
<title confidence="0.415854">Macquarie</title>
<author confidence="0.49392">Sydney</author>
<email confidence="0.996479">mjohnson@science.mq.edu.au</email>
<title confidence="0.498791">Ahmet Engin Cognitive and Linguistic</title>
<author confidence="0.354316">Brown</author>
<affiliation confidence="0.31017">Providence, RI,</affiliation>
<email confidence="0.998982">aeural@gmail.com</email>
<abstract confidence="0.9987775">The Brown and the Berkeley parsers are two state-of-the-art generative parsers. Since both parsers produce n-best lists, it is possible to apply reranking techniques to the output of both of these parsers, and to their union. We note that the standard reranker feature set distributed with the Brown parser does not do well with the Berkeley parser, and propose an extended set that does better. An ablation experiment shows that different parsers benefit from different reranker features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1380" citStr="Charniak and Johnson, 2005" startWordPosition="202" endWordPosition="206">troduction Syntactic parsing is the task of identifying the phrases and clauses in natural language sentences. It has been intensively studied primarily because it is generally believed that identifying syntactic structure is a first step towards semantic interpretation. This paper focuses on parsing the Wall Street Journal (WSJ) section of the University of Pennsylvania treebank corpus (Marcus et al., 1993). There are a large number of different approaches to this task. For simplicity we focus on two popular generative statistical parsing models: Charniak’s “Maximum Entropy Inspired” parser (Charniak and Johnson, 2005) and Petrov’s “split-merge” parser (Petrov et al., 2006). We follow conventional informal usage and refer to these as the “Brown” and the “Berkeley” parsers respectively. * We would like to thank Eugene Charniak and the other members of BLLIP for their helpful advice on this work. Naturally all errors remain our own. Briefly, the Berkeley parser is a smoothed PCFG whose non-terminals are refinements of the original treebank grammar obtained by an automatic splitmerge procedure, while the Brown parser is effectively a smoothed PCFG whose non-terminals encode a wide variety of manually chosen co</context>
<context position="4576" citStr="Charniak and Johnson (2005)" startWordPosition="723" endWordPosition="726">plus a constituent overlap feature), while we investigate models with millions of features here. They report a higher f-score than we do when they replace the generative Brown parser with the the self-trained discriminatively-reranked parser of McClosky et al. (2006), but with inputs provided by the generative Berkeley and Brown n-best parsers they report an f-score of 91.43 on section 23, which is consistent with the results reported here. 2 Experimental setup We ran both parsers in 50-best mode, and constructed 20-fold cross-validated training data as described in Collins and Koo (2005) and Charniak and Johnson (2005), i.e., the trees in sections 2–21 of the WSJ treebank were divided into 20 equalsized folds, and the parses for each fold were generated by a parser trained on the trees in the other folds. Then sections 22, 23 and 24 were parsed using the standard “out-of-the-box” parser. Following the suggestion in Collins and Koo (2005), in order to avoid over-training on section 23 all reranking experiments reported here (except the final one) used folds 1–18 as training data, used folds 19–20 as development data and used section 22 as test data. (The averaged perceptron algorithm does not require develop</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173–180, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2487" citStr="Collins and Koo, 2005" startWordPosition="376" endWordPosition="379">ile the Brown parser is effectively a smoothed PCFG whose non-terminals encode a wide variety of manually chosen conditioning information, such as heads, governors, etc. The Berkeley parser is usually viewed as unlexicalized (although the preterminals may be split so finely that they may be viewed as identifying lexical clusters), while essentially every distribution used in the Brown parser conditions on lexical information. Even from this cursory description it is clear that these parsers parsers extract generalizations from the training data in different ways. This paper applies reranking (Collins and Koo, 2005) to the n-best output of both parsers individually, as well as to an n-best list consisting of the union of the outputs of both parsers. We are interested to see whether the same kinds of features improve the performance of both the Berkeley and the Brown parsers, or whether successful reranking requires features that are specially tuned to the parser it is applied to. Finally, we are interested in the performance of the reranker trained on the union nbest lists. Combining the output of multiple parsers in other more complex ways has been previously demonstrated to improve overall accuracy, so</context>
<context position="4544" citStr="Collins and Koo (2005)" startWordPosition="718" endWordPosition="721">s 19–20. ity of the parses plus a constituent overlap feature), while we investigate models with millions of features here. They report a higher f-score than we do when they replace the generative Brown parser with the the self-trained discriminatively-reranked parser of McClosky et al. (2006), but with inputs provided by the generative Berkeley and Brown n-best parsers they report an f-score of 91.43 on section 23, which is consistent with the results reported here. 2 Experimental setup We ran both parsers in 50-best mode, and constructed 20-fold cross-validated training data as described in Collins and Koo (2005) and Charniak and Johnson (2005), i.e., the trees in sections 2–21 of the WSJ treebank were divided into 20 equalsized folds, and the parses for each fold were generated by a parser trained on the trees in the other folds. Then sections 22, 23 and 24 were parsed using the standard “out-of-the-box” parser. Following the suggestion in Collins and Koo (2005), in order to avoid over-training on section 23 all reranking experiments reported here (except the final one) used folds 1–18 as training data, used folds 19–20 as development data and used section 22 as test data. (The averaged perceptron al</context>
<context position="8555" citStr="Collins and Koo (2005)" startWordPosition="1350" endWordPosition="1353">Brown Combined 0.1 0.0 -0.1 -0.2 -0.3 Figure 1: The average change in f-score on folds 19–20 and section 22 caused by removing a feature superclass from the extended feature set and retraining. Difference in scores less that 0.1% are probably not significant. In this experiment all rerankers were trained using the averaged perceptron algorithm. With the full extended feature set, rerankers trained with the averaged perceptron algorithm achieve f-scores of 91.2% on both the Berkeley and Brown parses, and 91.6% on the combined parses. annotated, inspired by the n-gram rule features described in Collins and Koo (2005), Heads: an indicator function of “head-to-head” dependencies, SynSemHeads: an indicator function of the pair of syntactic (i.e., functional) and semantic (i.e., lexical) heads of each non-terminal, RBContext: an indicator function of how much each subtree deviates from from right-branching, SubjVerbAgr: an indicator function of whether subjectverb agreement is violated, CoPar: an indicator function that fires when conjoined phrases in a coordinate structure have approximately parallel syntactic structure, CoLenPar: an indicator function that fires when conjoined phrases in a coordinate struct</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10668" citStr="Collins, 2002" startWordPosition="1679" endWordPosition="1680">overnor and maximal projection, as the Berkeley parser does not explicitly condition on such information (in contrast to the Brown parser, which does). In fact, as the reader can verify the differences in f-scores between rerankers containing the extended features and the standard features is minimal. In order to better study the importance of the various features we conducted an ablation study, in which we trained rerankers which were missing one feature superclass from the 20 superclasses of the extended feature set. In order to speed training time we used the averaged perceptron algorithm (Collins, 2002) (it converges an order of magnitude faster than the LBFGS algorithm we used in the other experiments, but the f-score of the model estimated with the averaged perceptron is approximately 0.1% lower than when using L-BFGS). The results from this experiment are shown in Figure 1. The averaged perceptron algorithm does not rely on the development data 667 (folds 19–20), so the results we report are average fscores on the development data and on section 22 (we did this because the differences are small, so a larger evaluation set may be able to detect differences more reliably). It is interesting</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1164" citStr="Marcus et al., 1993" startWordPosition="169" endWordPosition="172">with the Brown parser does not do well with the Berkeley parser, and propose an extended set that does better. An ablation experiment shows that different parsers benefit from different reranker features. 1 Introduction Syntactic parsing is the task of identifying the phrases and clauses in natural language sentences. It has been intensively studied primarily because it is generally believed that identifying syntactic structure is a first step towards semantic interpretation. This paper focuses on parsing the Wall Street Journal (WSJ) section of the University of Pennsylvania treebank corpus (Marcus et al., 1993). There are a large number of different approaches to this task. For simplicity we focus on two popular generative statistical parsing models: Charniak’s “Maximum Entropy Inspired” parser (Charniak and Johnson, 2005) and Petrov’s “split-merge” parser (Petrov et al., 2006). We follow conventional informal usage and refer to these as the “Brown” and the “Berkeley” parsers respectively. * We would like to thank Eugene Charniak and the other members of BLLIP for their helpful advice on this work. Naturally all errors remain our own. Briefly, the Berkeley parser is a smoothed PCFG whose non-termina</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Michell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="4216" citStr="McClosky et al. (2006)" startWordPosition="664" endWordPosition="667">cs Trees Reranker features standard extended 91.6 91.7 91.8 91.6 91.8 91.9 Table 1: The f-scores on section 22 of rerankers trained on folds 1–18 by minimizing a regularized “MaxEnt” objective (negative log likelihood with a Gaussian regularizer) using L-BFGS. The weight of the regularizer was tuned to optimize f-score on folds 19–20. ity of the parses plus a constituent overlap feature), while we investigate models with millions of features here. They report a higher f-score than we do when they replace the generative Brown parser with the the self-trained discriminatively-reranked parser of McClosky et al. (2006), but with inputs provided by the generative Berkeley and Brown n-best parsers they report an f-score of 91.43 on section 23, which is consistent with the results reported here. 2 Experimental setup We ran both parsers in 50-best mode, and constructed 20-fold cross-validated training data as described in Collins and Koo (2005) and Charniak and Johnson (2005), i.e., the trees in sections 2–21 of the WSJ treebank were divided into 20 equalsized folds, and the parses for each fold were generated by a parser trained on the trees in the other folds. Then sections 22, 23 and 24 were parsed using the</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="1436" citStr="Petrov et al., 2006" startWordPosition="211" endWordPosition="214">ases and clauses in natural language sentences. It has been intensively studied primarily because it is generally believed that identifying syntactic structure is a first step towards semantic interpretation. This paper focuses on parsing the Wall Street Journal (WSJ) section of the University of Pennsylvania treebank corpus (Marcus et al., 1993). There are a large number of different approaches to this task. For simplicity we focus on two popular generative statistical parsing models: Charniak’s “Maximum Entropy Inspired” parser (Charniak and Johnson, 2005) and Petrov’s “split-merge” parser (Petrov et al., 2006). We follow conventional informal usage and refer to these as the “Brown” and the “Berkeley” parsers respectively. * We would like to thank Eugene Charniak and the other members of BLLIP for their helpful advice on this work. Naturally all errors remain our own. Briefly, the Berkeley parser is a smoothed PCFG whose non-terminals are refinements of the original treebank grammar obtained by an automatic splitmerge procedure, while the Brown parser is effectively a smoothed PCFG whose non-terminals encode a wide variety of manually chosen conditioning information, such as heads, governors, etc. T</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
<author>Haizhou Li</author>
</authors>
<title>K-best combination of syntactic parsers.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1552--1560</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="3225" citStr="Zhang et al. (2009)" startWordPosition="506" endWordPosition="509"> of both parsers. We are interested to see whether the same kinds of features improve the performance of both the Berkeley and the Brown parsers, or whether successful reranking requires features that are specially tuned to the parser it is applied to. Finally, we are interested in the performance of the reranker trained on the union nbest lists. Combining the output of multiple parsers in other more complex ways has been previously demonstrated to improve overall accuracy, so it is interesting to see if the relatively simple method used here improves parsing accuracy as well. The approach of Zhang et al. (2009) is closest to the work described here. They combine n-best lists produced by the same parsers as we do, but use only a relatively small set of features (the log probabil665 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 665–668, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Trees Reranker features standard extended 91.6 91.7 91.8 91.6 91.8 91.9 Table 1: The f-scores on section 22 of rerankers trained on folds 1–18 by minimizing a regularized “MaxEnt” objective (negative log likelihood with a Gaussi</context>
</contexts>
<marker>Zhang, Zhang, Tan, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li. 2009. K-best combination of syntactic parsers. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1552–1560, Singapore. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>