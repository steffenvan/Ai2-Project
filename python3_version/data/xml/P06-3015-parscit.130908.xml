<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001593">
<title confidence="0.997375">
Clavius: Bi-Directional Parsing for Generic Multimodal Interaction
</title>
<author confidence="0.998192">
Frank Rudzicz
</author>
<affiliation confidence="0.845774333333333">
Centre for Intelligent Machines
McGill University
Montr´eal, Canada
</affiliation>
<email confidence="0.995257">
frudzi@cim.mcgill.ca
</email>
<sectionHeader confidence="0.99383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999695307692308">
We introduce a new multi-threaded
parsing algorithm on unification grammars
designed specifically for multimodal
interaction and noisy environments.
By lifting some traditional constraints,
namely those related to the ordering
of constituents, we overcome several
difficulties of other systems in this
domain. We also present several criteria
used in this model to constrain the search
process using dynamically loadable
scoring functions. Some early analyses of
our implementation are discussed.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999826153846154">
Since the seminal work of Bolt (Bolt, 1980), the
methods applied to multimodal interaction (MMI)
have diverged towards unreconcilable approaches
retrofitted to models not specifically amenable to
the problem. For example, the representational
differences between neural networks, decision
trees, and finite-state machines (Johnston and
Bangalore, 2000) have limited the adoption of
the results using these models, and the typical
reliance on the use of whole unimodal sentences
defeats one of the main advantages of MMI - the
ability to constrain the search using cross-modal
information as early as possible.
CLAVIUS is the result of an effort to combine
sensing technologies for several modality types,
speech and video-tracked gestures chief among
them, within the immersive virtual environment
(Boussemart, 2004) shown in Figure 1. Its purpose
is to comprehend multimodal phrases such as
“put this \ here \ .”, for pointing gestures \,
in either command-based or dialogue interaction.
CLAVIUS provides a flexible, and trainable
new bi-directional parsing algorithm on multi-
dimensional input spaces, and produces modality-
independent semantic interpretation with a low
computational cost.
</bodyText>
<figureCaption confidence="0.999195">
Figure 1: The target immersive environment.
</figureCaption>
<subsectionHeader confidence="0.984489">
1.1 Graphical Models and Unification
</subsectionHeader>
<bodyText confidence="0.999918882352941">
Unification grammars on typed directed acyclic
graphs have been explored previously in MMI,
but typically extend existing mechanisms not
designed for multi-dimensional input. For
example, both (Holzapfel et al., 2004) and
(Johnston, 1998) essentially adapt Earley’s chart
parser by representing edges as sets of references
to terminal input elements - unifying these as new
edges are added to the agenda. In practice this
has led to systems that analyze every possible
subset of the input resulting in a combinatorial
explosion that balloons further when considering
the complexities of cross-sentential phenomena
such as anaphora, and the effects of noise and
uncertainty on speech and gesture tracking. We
will later show the extent to which CLAVIUS
reduces the size of the search space.
</bodyText>
<page confidence="0.996208">
85
</page>
<bodyText confidence="0.865848466666667">
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 85–90,
Sydney, July 2006. c�2006 Association for Computational Linguistics
Directed graphs conveniently represent
both syntactic and semantic structure, and all
partial parses in CLAVIUS , including terminal-
level input, are represented graphically. Few
restrictions apply, except that arcs labelled
CAT and TIME must exist to represent the
grammar category and time spanned by the
parse, respectively1. Similarly, all grammar rules,
Γi : LHS −→ RHS1 RHS2 ... RHS, are
graphical structures, as exemplified in Figure 2.
Figure 2: Γ1 : OBJECT REFERENCE −→
NP click {where(NP :: f1) = (click:: f1)}, with
NP expanded by Γ2 : NP −→ DT NN.
</bodyText>
<subsectionHeader confidence="0.998447">
1.2 Multimodal Bi-Directional Parsing
</subsectionHeader>
<bodyText confidence="0.999829">
Our parsing strategy combines bottom-up and
top-down approaches, but differs from other
approaches to bi-directional chart parsing (Rocio,
1998) in several key respects, discussed below.
</bodyText>
<subsectionHeader confidence="0.923706">
1.2.1 Asynchronous Collaborating Threads
</subsectionHeader>
<bodyText confidence="0.999968666666667">
A defining characteristic of our approach is
that edges are selected asynchronously by two
concurrent processing threads, rather than serially
in a two-stage process. In this way, we can
distribute processing across multiple machines,
or dynamically alter the priorities given to each
thread. Generally, this allows for a more dynamic
process where no thread can dominate the other. In
typical bi-directional chart parsing the top-down
component is only activated when the bottom-up
component has no more legal expansions (Ageno,
2000).
</bodyText>
<subsectionHeader confidence="0.866821">
1.2.2 Unordered Constituents
</subsectionHeader>
<bodyText confidence="0.990674333333333">
Alhough evidence suggests that deictic
gestures overlap or follow corresponding spoken
pronomials 85-93% of the time (Kettebekov et al,
</bodyText>
<footnote confidence="0.979162666666667">
1Usually this timespan corresponds to the real-time
occurrence of a speech or gestural event, but the actual
semantics are left to the application designer
</footnote>
<bodyText confidence="0.99990075">
2002), we must allow for all possible permutations
of multi-dimensional input - as in “put &amp; this &amp;
here.” vs. “put this &amp; here &amp; .”, for example.
We therefore take the unconvential approach
of placing no mandatory ordering constraints on
constituents, hence the rule Γabc : A −→ B C
parses the input “ C B”. We show how we can
easily maintain regular temporal ordering in §3.5.
</bodyText>
<subsectionHeader confidence="0.919216">
1.2.3 Partial Qualification
</subsectionHeader>
<bodyText confidence="0.999663142857143">
Whereas existing bi-directional chart parsers
maintain fully-qualified edges by incrementally
adding adjacent input words to the agenda,
CLAVIUS has the ability to construct parses that
instantiate only a subset of their constituents,
so Γabc also parses the input “B”, for example.
Repercussions are discussed in §3.4 and §4.
</bodyText>
<sectionHeader confidence="0.97108" genericHeader="method">
2 The Algorithm
</sectionHeader>
<bodyText confidence="0.9976245">
CLAVIUS expands parses according to a best-first
process where newly expanded edges are ordered
according to trainable criteria of multimodal
language, as discussed in §3. Figure 3 shows a
component breakdown of CLAVIUS ’s software
architecture. The sections that follow explain
the flow of information through this system from
sensory input to semantic interpretation.
</bodyText>
<figureCaption confidence="0.873392">
Figure 3: Simplified information flow between
fundamental software components.
</figureCaption>
<subsectionHeader confidence="0.995907">
2.1 Lexica and Preprocessing
</subsectionHeader>
<bodyText confidence="0.999968625">
Each unique input modality is asynchronously
monitored by one of T TRACKERS, each sending
an n-best list of lexical hypotheses to CLAVIUS for
any activity as soon as it is detected. For example,
a gesture tracker (see Figure 4a) parametrizes the
gestures preparation, stroke/point, and retraction
(McNeill, 1992), with values reflecting spatial
positions and velocities of arm motion, whereas
</bodyText>
<page confidence="0.979964">
86
</page>
<bodyText confidence="0.9998115">
our speech tracker parametrises words with part-
of-speech tags, and prior probabilities (see Figure
4b). Although preprocessing is reduced to the
identification of lexical tokens, this is more
involved than simple lexicon lookup due to the
modelling of complex signals.
</bodyText>
<figureCaption confidence="0.986626">
Figure 4: Gestural (a) and spoken (b) ‘words’.
</figureCaption>
<subsectionHeader confidence="0.998501">
2.2 Data Structures
</subsectionHeader>
<bodyText confidence="0.999996785714286">
All TRACKERS write their hypotheses directly
to the first of three SUBSPACES that partition
all partial parses in the search space. The first
is the GENERALISER’s subspace, °[G], which
is monitored by the GENERALISER thread -
the first part of the parser. All new parses
are first written to °[G] before being moved to
the SPECIFIER’s active and inactive subspaces,
°[SAct], and °[SInact], respectively. Subspaces are
optimised for common operations by organising
parses by their scores and grammatical categories
into depth-balanced search trees having the heap
property. The best partial parse in each subspace
can therefore be found in O(1) amortised time.
</bodyText>
<subsectionHeader confidence="0.993762">
2.3 Generalisation
</subsectionHeader>
<bodyText confidence="0.999238263157895">
The GENERALISER monitors the best partial
parse, &apos;Pg, in °[G], and creates new parses &apos;Pi
for all grammar rules Fi having CATEGORY(&apos;Pg)
on the right-hand side. Effectively, these new
parses are instantiations of the relevant Fi, with
one constituent unified to &apos;Pg. This provides
the impetus towards sentence-level parses, as
simplified in Algorithm 1 and exemplified in
Figure 5. Naturally, if rule Fi has more than one
constituent (c &gt; 1) of type CATEGORY(&apos;Pg), then
c new parses are created, each with one of these
being instantiated.
Since the GENERALISER is activated as soon as
input is added to °[G], the process is interactive
(Tomita, 1985), and therefore incorporates the
associated benefits of efficiency. This is contrasted
with the all-paths bottom-up strategy in GEMINI
(Dowding et al, 1993) that finds all admissable
edges of the grammar.
</bodyText>
<table confidence="0.996025272727273">
Algorithm 1: Simplified Generalisation
Data: Subspace °[G], grammar F
while data remains in °[G] do
&apos;Pg := highest scoring graph in °[G]
foreach rule Fi s.t. Cat (&apos;Pg) E RHS(Fi)
do
&apos;Pi := Unify (Fi, [• —RHS • &apos;Pg])
if ]&apos;Pi then
Apply Score (&apos;Pi) to &apos;Pi
Insert &apos;Pi into °[G]
move &apos;Pg into °[SAct]
</table>
<figureCaption confidence="0.994403">
Figure 5: Example of GENERALISATION.
</figureCaption>
<subsectionHeader confidence="0.962227">
2.4 Specification
</subsectionHeader>
<bodyText confidence="0.99868">
The SPECIFIER thread provides the impetus
towards complete coverage of the input, as
simplified in Algorithm 2 (see Figure 6). It
combines parses in its subspaces that have the
same top-level grammar expansion but different
instantiated constituents. The resulting parse
merges the semantics of the two original graphs
only if unification succeeds, providing a hard
constraint against the combination of incongruous
information. The result, &apos;P, of specification must
be written to °[G], otherwise &apos;P could never appear
on the RHS of another partial parse. We show how
associated vulnerabilities are overcome in §3.2
and §3.4.
Specification is commutative and will always
provide more information than its constituent
graphs if it does not fail, unlike the ‘overlay’
</bodyText>
<page confidence="0.995555">
87
</page>
<bodyText confidence="0.52798625">
method of SMARTKOM (Alexandersson and
Becker, 2001), which basically provides a
subsumption mechanism over background
knowledge.
</bodyText>
<table confidence="0.981574625">
Algorithm 2: Simplified Specification
Data: Subspaces Ξ[SAct] and Ξ[SInact]
while data remains in Ξ[SAct] do
Ψs := highest scoring graph in Ξ[SAct]
Ψj := highest scoring graph in Ξ[SInact]
s.t. Cat (Ψj) = Cat (Ψs)
while ]Ψj do
Ψi := Unify (Ψs,Ψj)
if ]Ψi then
Apply Score (Ψi) to Ψi
Insert Ψi into Ξ[G]
Ψj := next highest scoring graph from
Ξ[SInact] s.t. Cat (Ψj) = Cat (Ψs)
; // Optionally stop after I
iterations, for some I
Move Ψs into Ξ
</table>
<figureCaption confidence="0.993636">
Figure 6: Example of SPECIFICATION.
</figureCaption>
<subsectionHeader confidence="0.948569">
2.5 Cognition
</subsectionHeader>
<bodyText confidence="0.9981591">
The COGNITION thread monitors the best
sentence-level hypothesis, ΨB, in Ξ[SInact],
and terminates the search process once ΨB has
remained unchallenged by new competing parses
for some period of time.
Once found, COGNITION communicates ΨB to
the APPLICATION. Both COGNITION and the
APPLICATION read state information from the
MySQL WORLD database, as discussed in §3.5,
though only the latter can modify it.
</bodyText>
<sectionHeader confidence="0.940382" genericHeader="method">
3 Applying Domain-Centric Knowledge
</sectionHeader>
<bodyText confidence="0.984027">
Upon being created, all partial parses are assigned
a score approximating its likelihood of being part
of an accepted multimodal sentence. The score
</bodyText>
<equation confidence="0.757120666666667">
|S|
of partial parse Ψ, SCORE(Ψ) = L wini(Ψ),
i=0
</equation>
<bodyText confidence="0.998725111111111">
is a weighted linear combination of independent
scoring modules (KNOWLEDGE SOURCES). Each
module presents a score function ni : Ψ —* R[0..1]
according to a unique criterion of multimodal
language, weighted by wi, also on R[0..1]. Some
modules provide ‘hard constraints‘ that can
outright forbid unification, returning ri = −oo
in those cases. A subset of the criteria we have
explored are outlined below.
</bodyText>
<subsectionHeader confidence="0.991463">
3.1 Temporal Alignment (n1)
</subsectionHeader>
<bodyText confidence="0.949326857142857">
By modelling the timespans of parses as
Gaussians, where p and Q are determined by the
midpoint and 21 the distance between the two
endpoints, respectively - we can promote parses
whose constituents are closely related in time
with the symmetric Kullback-Leibler divergence,
DKL(Ψ1,Ψ2) = (σ21−σ22)2+((µ1−µ2)(σ21+σ22))2
</bodyText>
<subsectionHeader confidence="0.99616">
3.2 Ancestry Constraint (n2)
</subsectionHeader>
<bodyText confidence="0.999993857142857">
A consequence of accepting n-best lexical
hypotheses for each word is that we risk unifying
parses that include two competing hypotheses.
For example, if our speech TRACKER produces
hypotheses “horse” and “house” for ambiguous
input, then n2 explicitly prohibits the parse “the
horse and the house” with flags on lexical content.
</bodyText>
<subsectionHeader confidence="0.998443">
3.3 Probabilistic Grammars (n3)
</subsectionHeader>
<bodyText confidence="0.960385166666667">
We emphasise more common grammatical
constructions by augmenting each grammar
rule with an associated probability, P(Γi),
and assigning n3(Ψ) = P(RULE(Ψ)) �
r-3(Ψc) where RULE is the
Ψ,=constituent of Ψ
top-level expansion of Ψ.
Probabilities are trainable by maximum
likelihood estimation on annotated data. Within
the context of CLAVIUS , n3 promotes the
processing of new input words and shallower
parse trees.
</bodyText>
<equation confidence="0.919263333333333">
[SInact]
.
4σ21σ22
</equation>
<bodyText confidence="0.9473325">
Therefore, n1 promotes more locally-structured
parses, and co-occuring multimodal utterances.
</bodyText>
<page confidence="0.997915">
88
</page>
<sectionHeader confidence="0.565706" genericHeader="method">
3.4 Information Content (n4), Coverage (n5)
</sectionHeader>
<bodyText confidence="0.999939625">
The n4 module partially orders parses by
preferring those that maximise the joint entropy
between the semantic variables of its constituent
parses. Furthermore, we use a shifted sigmoid
−1, to promote parses
that maximise the number of ‘words’ in a parse.
These two modules together are vital in choosing
fully specified sentences.
</bodyText>
<subsectionHeader confidence="0.965406">
3.5 Functional Constraints (n6)
</subsectionHeader>
<bodyText confidence="0.9999783125">
Each grammar rule Fi can include constraint
functions f : IF —* R[0,1] parametrised by values
in instantiated graphs. For example, the function
T FOLLOWS(IF1, XF2) returns 1 if constituent&apos;&amp;2
follows IF1 in time, and −oc otherwise, thus
maintaining ordering constraints. Functions are
dynamically loaded and executed during scoring.
Since functions are embedded directly within
parse graphs, their return values can be directly
incorporated into those parses, allowing us to
utilise data in the WORLD. For example, the
function OBJECTAT(x, y, &amp;o) determines if an
object exists at point (x, y), as determined by a
pointing gesture, and writes the type of this object,
o, to the graph, which can later further constrain
the search.
</bodyText>
<sectionHeader confidence="0.969365" genericHeader="method">
4 Early Results
</sectionHeader>
<bodyText confidence="0.999976823529412">
We have constructed a simple blocks-world
experiment where a user can move, colour,
create, and delete geometric objects using speech
and pointing gestures with 74 grammar rules,
25 grammatical categories, and a 43-word
vocabulary. Ten users were recorded interacting
with this system, for a combined total of 2.5
hours of speech and gesture data, and 2304
multimodal utterances. Our randomised data
collection mechanism was designed to equitably
explore the four command types. Test subjects
were given no indication as to the types of phrases
we expected - but were rather shown a collection
of objects and were asked to replicate it, given the
four basic types of actions.
Several aspects of the parser have been tested at
this stage and are summarised below.
</bodyText>
<subsectionHeader confidence="0.994797">
4.1 Accuracy
</subsectionHeader>
<bodyText confidence="0.99094995">
Table 1 shows three hand-tuned configurations of
the module weights Wi, with W2 = 0.0, since n2
provides a ‘hard constraint’ (§3.2).
Figure 7 shows sentence-level precision
achieved for each Qi on each of the four tasks,
where precision is defined as the proportion of
correctly executed sentences. These are compared
against the CMU Sphinx-4 speech recogniser
using the unimodal projection of the multimodal
grammar. Here, conjunctive phrases such as “Put
a sphere here and colour it yellow” are classified
according to their first clause.
Presently, correlating the coverage and
probabilistic grammar constraints with higher
weights ( &gt; 30%) appears to provide the best
results. Creation and colouring tasks appeared
to suffer most due to missing or misunderstood
head-noun modifiers (ie., object colour). In these
examples, CLAVIUS ranged from a −51.7% to a
62.5% relative error reduction rate over all tasks.
</bodyText>
<table confidence="0.9988602">
Config W1 W��) W3 W4 W5 W6
2
Q1 0.4 0.0 0.3 0.1 0.1 0.1
Q2 0.2 0.0 0.1 0.3 0.2 0.2
Q3 0.1 0.0 0.3 0.3 0.15 0.15
</table>
<tableCaption confidence="0.998738">
Table 1: Three weight configurations.
</tableCaption>
<figureCaption confidence="0.993513">
Figure 7: Precision across the test tasks.
</figureCaption>
<subsectionHeader confidence="0.992461">
4.2 Work Expenditure
</subsectionHeader>
<bodyText confidence="0.999978692307692">
To test whether the best-first approach
compensates for CLAVIUS ’ looser constraints
(§1.2), a simple bottom-up multichart parser
(§1.1) was constructed and the average number
of edges it produces on sentences of varying
length was measured. Figure 8 compares this
against the average number of edges produced
by CLAVIUS on the same data. In particular,
although CLAVIUS generally finds the parse it will
accept relatively quickly (‘CLAVIUS - found’),
the COGNITION module will delay its acceptance
(‘CLAVIUS - accepted’) for a time. Further tuning
will hopefully reduce this ‘waiting period’.
</bodyText>
<equation confidence="0.9515355">
n5(XF) = 2
1+e− 25 NUMWORDSIN(41)
</equation>
<page confidence="0.996348">
89
</page>
<figureCaption confidence="0.994538">
Figure 8: Number of edges expanded, given
sentence length.
</figureCaption>
<sectionHeader confidence="0.998907" genericHeader="method">
5 Remarks
</sectionHeader>
<bodyText confidence="0.999820636363636">
CLAVIUS consistently ignores over 92% of
dysfluencies (eg. “uh”) and significant noise
events in tracking, apparently as a result of the
partial qualifications discussed in §1.2.3, which is
especially relevant in noisy environments. Early
unquantified observation also suggests that a
result of unordered constituents is that parses
incorporating lead words - head nouns, command
verbs and pointing gestures in particular - are
emphasised and form sentence-level parses early,
and are later ‘filled in’ with function words.
</bodyText>
<subsectionHeader confidence="0.892286">
5.1 Ongoing Work
</subsectionHeader>
<bodyText confidence="0.9999604375">
There are at least four avenues open to exploration
in the near future. First, applying the parser to
directed two-party dialogue will explore context-
sensitivity and a more complex grammar. Second,
the architecture lends itself to further parallelism
- specifically by permitting P &gt; 1 concurrent
processing units to dynamically decide whether to
employ the GENERALISER or SPECIFIER, based
on the sizes of shared active subspaces.
We are also currently working on scoring
modules that incorporate language modelling
(with discriminative training), and prosody-based
co-analysis. Finally, we have already begun work
on automatic methods to train scoring parameters,
including the distribution of wz, and module-
specific training.
</bodyText>
<sectionHeader confidence="0.999615" genericHeader="conclusions">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999968666666667">
Funding has been provided by la bourse de
maitrisse of the fonds qu´eb´ecois de la recherche
sur la nature et les technologies.
</bodyText>
<sectionHeader confidence="0.996399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999581791666666">
Ageno, A., Rodriguez, H. 2000 Extending
Bidirectional Chart Parsing with a Stochastic
Model, in Proc. of TSD 2000, Brno, Czech
Republic.
Alexandersson, J. and Becker, T. 2001 Overlay as
the Basic Operation for Discourse Processing in a
Multimodal Dialogue System in Proc. of the 2nd
IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, Seattle, WA.
Bolt, R.A. 1980 “Put-that-there”: Voice and gesture
at the graphics interface in Proc. of SIGGRAPH 80
ACM Press, New York, NY.
Boussemart, Y., Rioux, F., Rudzicz, F., Wozniewski,
M., Cooperstock, J. 2004 A Framework for 3D
Visualisation and Manipulation in an Immersive
Space using an Untethered Bimanual Gestural
Interface in Proc. of VRST 2004 ACM Press, Hong
Kong.
Dowding, J. et al. 1993 Gemini: A Natural Language
System For Spoken-Language Understanding in
Meeting of the ACL, ACL, Morristown, NJ.
Holzapfel, H., Nickel, K., Stiefelhagen, R. 2004
Implementation and evaluation of a constraint-
based multimodal fusion system for speech and 3D
pointing gestures, in ICMI ’04: Proc. of the 6th intl.
conference on Multimodal interfaces, ACM Press,
New York, NY.
Johnston, M. 1998 Unification-based multimodal
parsing, in Proc. of the 36th annual meeting of the
ACL, ACL, Morristown, NJ.
Johnston, M., Bangalore, S. 2000 Finite-state
multimodal parsing and understanding in Proc. of
the 18th conference on Computational linguistics
ACL, Morristown, NJ.
Kettebekov, S., et al. 2002 Prosody Based Co-
analysis of Deictic Gestures and Speech in Weather
Narration Broadcast, in Workshop on Multimodal
Resources and Multimodal System Evaluation.
(LREC 2002), Las Palmas, Spain.
McNeill, D. 1992 Hand and mind: What gestures
reveal about thought University of Chicago Press
and CSLI Publications, Chicago, IL.
Rocio, V., Lopes, J.G. 1998 Partial Parsing,
Deduction and Tabling in TAPD 98
Tomita, M. 1985 An Efficient Context-Free Parsing
Algorithm for Natural Languages, in Proc. Ninth
Intl. Joint Conf. on Artificial Intelligence, Los
Angeles, CA.
</reference>
<page confidence="0.998633">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.993948">
<title confidence="0.999554">Clavius: Bi-Directional Parsing for Generic Multimodal Interaction</title>
<author confidence="0.999946">Frank Rudzicz</author>
<affiliation confidence="0.999541">Centre for Intelligent Machines McGill University</affiliation>
<address confidence="0.997903">Montr´eal, Canada</address>
<email confidence="0.999321">frudzi@cim.mcgill.ca</email>
<abstract confidence="0.999863857142857">We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments. By lifting some traditional constraints, namely those related to the ordering of constituents, we overcome several difficulties of other systems in this domain. We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions. Some early analyses of our implementation are discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Ageno</author>
<author>H Rodriguez</author>
</authors>
<title>Extending Bidirectional Chart Parsing with a Stochastic Model, in</title>
<date>2000</date>
<booktitle>Proc. of TSD 2000,</booktitle>
<location>Brno, Czech Republic.</location>
<marker>Ageno, Rodriguez, 2000</marker>
<rawString>Ageno, A., Rodriguez, H. 2000 Extending Bidirectional Chart Parsing with a Stochastic Model, in Proc. of TSD 2000, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Alexandersson</author>
<author>T Becker</author>
</authors>
<title>Overlay as the Basic Operation for Discourse Processing in a Multimodal Dialogue System in</title>
<date>2001</date>
<booktitle>Proc. of the 2nd IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="9246" citStr="Alexandersson and Becker, 2001" startWordPosition="1387" endWordPosition="1390">el grammar expansion but different instantiated constituents. The resulting parse merges the semantics of the two original graphs only if unification succeeds, providing a hard constraint against the combination of incongruous information. The result, &apos;P, of specification must be written to °[G], otherwise &apos;P could never appear on the RHS of another partial parse. We show how associated vulnerabilities are overcome in §3.2 and §3.4. Specification is commutative and will always provide more information than its constituent graphs if it does not fail, unlike the ‘overlay’ 87 method of SMARTKOM (Alexandersson and Becker, 2001), which basically provides a subsumption mechanism over background knowledge. Algorithm 2: Simplified Specification Data: Subspaces Ξ[SAct] and Ξ[SInact] while data remains in Ξ[SAct] do Ψs := highest scoring graph in Ξ[SAct] Ψj := highest scoring graph in Ξ[SInact] s.t. Cat (Ψj) = Cat (Ψs) while ]Ψj do Ψi := Unify (Ψs,Ψj) if ]Ψi then Apply Score (Ψi) to Ψi Insert Ψi into Ξ[G] Ψj := next highest scoring graph from Ξ[SInact] s.t. Cat (Ψj) = Cat (Ψs) ; // Optionally stop after I iterations, for some I Move Ψs into Ξ Figure 6: Example of SPECIFICATION. 2.5 Cognition The COGNITION thread monitors </context>
</contexts>
<marker>Alexandersson, Becker, 2001</marker>
<rawString>Alexandersson, J. and Becker, T. 2001 Overlay as the Basic Operation for Discourse Processing in a Multimodal Dialogue System in Proc. of the 2nd IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Bolt</author>
</authors>
<title>Put-that-there”: Voice and gesture at the graphics interface in</title>
<date>1980</date>
<booktitle>Proc. of SIGGRAPH 80</booktitle>
<publisher>ACM Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="734" citStr="Bolt, 1980" startWordPosition="95" endWordPosition="96">versity Montr´eal, Canada frudzi@cim.mcgill.ca Abstract We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments. By lifting some traditional constraints, namely those related to the ordering of constituents, we overcome several difficulties of other systems in this domain. We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions. Some early analyses of our implementation are discussed. 1 Introduction Since the seminal work of Bolt (Bolt, 1980), the methods applied to multimodal interaction (MMI) have diverged towards unreconcilable approaches retrofitted to models not specifically amenable to the problem. For example, the representational differences between neural networks, decision trees, and finite-state machines (Johnston and Bangalore, 2000) have limited the adoption of the results using these models, and the typical reliance on the use of whole unimodal sentences defeats one of the main advantages of MMI - the ability to constrain the search using cross-modal information as early as possible. CLAVIUS is the result of an effor</context>
</contexts>
<marker>Bolt, 1980</marker>
<rawString>Bolt, R.A. 1980 “Put-that-there”: Voice and gesture at the graphics interface in Proc. of SIGGRAPH 80 ACM Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Boussemart</author>
<author>F Rioux</author>
<author>F Rudzicz</author>
<author>M Wozniewski</author>
<author>J Cooperstock</author>
</authors>
<title>A Framework for 3D Visualisation and Manipulation in an Immersive Space using an Untethered Bimanual Gestural Interface in</title>
<date>2004</date>
<booktitle>Proc. of VRST</booktitle>
<publisher>ACM Press,</publisher>
<location>Hong Kong.</location>
<marker>Boussemart, Rioux, Rudzicz, Wozniewski, Cooperstock, 2004</marker>
<rawString>Boussemart, Y., Rioux, F., Rudzicz, F., Wozniewski, M., Cooperstock, J. 2004 A Framework for 3D Visualisation and Manipulation in an Immersive Space using an Untethered Bimanual Gestural Interface in Proc. of VRST 2004 ACM Press, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dowding</author>
</authors>
<title>Gemini: A Natural Language System For Spoken-Language Understanding</title>
<date>1993</date>
<booktitle>in Meeting of the ACL, ACL,</booktitle>
<location>Morristown, NJ.</location>
<marker>Dowding, 1993</marker>
<rawString>Dowding, J. et al. 1993 Gemini: A Natural Language System For Spoken-Language Understanding in Meeting of the ACL, ACL, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Holzapfel</author>
<author>K Nickel</author>
<author>R Stiefelhagen</author>
</authors>
<title>Implementation and evaluation of a constraintbased multimodal fusion system for speech and 3D pointing gestures,</title>
<date>2004</date>
<booktitle>in ICMI ’04: Proc. of the 6th intl. conference on Multimodal interfaces,</booktitle>
<publisher>ACM Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="2180" citStr="Holzapfel et al., 2004" startWordPosition="299" endWordPosition="302">ultimodal phrases such as “put this \ here \ .”, for pointing gestures \, in either command-based or dialogue interaction. CLAVIUS provides a flexible, and trainable new bi-directional parsing algorithm on multidimensional input spaces, and produces modalityindependent semantic interpretation with a low computational cost. Figure 1: The target immersive environment. 1.1 Graphical Models and Unification Unification grammars on typed directed acyclic graphs have been explored previously in MMI, but typically extend existing mechanisms not designed for multi-dimensional input. For example, both (Holzapfel et al., 2004) and (Johnston, 1998) essentially adapt Earley’s chart parser by representing edges as sets of references to terminal input elements - unifying these as new edges are added to the agenda. In practice this has led to systems that analyze every possible subset of the input resulting in a combinatorial explosion that balloons further when considering the complexities of cross-sentential phenomena such as anaphora, and the effects of noise and uncertainty on speech and gesture tracking. We will later show the extent to which CLAVIUS reduces the size of the search space. 85 Proceedings of the COLIN</context>
</contexts>
<marker>Holzapfel, Nickel, Stiefelhagen, 2004</marker>
<rawString>Holzapfel, H., Nickel, K., Stiefelhagen, R. 2004 Implementation and evaluation of a constraintbased multimodal fusion system for speech and 3D pointing gestures, in ICMI ’04: Proc. of the 6th intl. conference on Multimodal interfaces, ACM Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
</authors>
<title>Unification-based multimodal parsing,</title>
<date>1998</date>
<booktitle>in Proc. of the 36th annual meeting of the ACL, ACL,</booktitle>
<location>Morristown, NJ.</location>
<contexts>
<context position="2201" citStr="Johnston, 1998" startWordPosition="304" endWordPosition="305">t this \ here \ .”, for pointing gestures \, in either command-based or dialogue interaction. CLAVIUS provides a flexible, and trainable new bi-directional parsing algorithm on multidimensional input spaces, and produces modalityindependent semantic interpretation with a low computational cost. Figure 1: The target immersive environment. 1.1 Graphical Models and Unification Unification grammars on typed directed acyclic graphs have been explored previously in MMI, but typically extend existing mechanisms not designed for multi-dimensional input. For example, both (Holzapfel et al., 2004) and (Johnston, 1998) essentially adapt Earley’s chart parser by representing edges as sets of references to terminal input elements - unifying these as new edges are added to the agenda. In practice this has led to systems that analyze every possible subset of the input resulting in a combinatorial explosion that balloons further when considering the complexities of cross-sentential phenomena such as anaphora, and the effects of noise and uncertainty on speech and gesture tracking. We will later show the extent to which CLAVIUS reduces the size of the search space. 85 Proceedings of the COLING/ACL 2006 Student Re</context>
</contexts>
<marker>Johnston, 1998</marker>
<rawString>Johnston, M. 1998 Unification-based multimodal parsing, in Proc. of the 36th annual meeting of the ACL, ACL, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
</authors>
<title>Finite-state multimodal parsing and understanding in</title>
<date>2000</date>
<booktitle>Proc. of the 18th conference on Computational linguistics ACL,</booktitle>
<location>Morristown, NJ.</location>
<contexts>
<context position="1043" citStr="Johnston and Bangalore, 2000" startWordPosition="131" endWordPosition="134">ituents, we overcome several difficulties of other systems in this domain. We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions. Some early analyses of our implementation are discussed. 1 Introduction Since the seminal work of Bolt (Bolt, 1980), the methods applied to multimodal interaction (MMI) have diverged towards unreconcilable approaches retrofitted to models not specifically amenable to the problem. For example, the representational differences between neural networks, decision trees, and finite-state machines (Johnston and Bangalore, 2000) have limited the adoption of the results using these models, and the typical reliance on the use of whole unimodal sentences defeats one of the main advantages of MMI - the ability to constrain the search using cross-modal information as early as possible. CLAVIUS is the result of an effort to combine sensing technologies for several modality types, speech and video-tracked gestures chief among them, within the immersive virtual environment (Boussemart, 2004) shown in Figure 1. Its purpose is to comprehend multimodal phrases such as “put this \ here \ .”, for pointing gestures \, in either co</context>
</contexts>
<marker>Johnston, Bangalore, 2000</marker>
<rawString>Johnston, M., Bangalore, S. 2000 Finite-state multimodal parsing and understanding in Proc. of the 18th conference on Computational linguistics ACL, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kettebekov</author>
</authors>
<title>Prosody Based Coanalysis of Deictic Gestures and Speech in Weather Narration Broadcast,</title>
<date>2002</date>
<booktitle>in Workshop on Multimodal Resources and Multimodal System Evaluation. (LREC</booktitle>
<location>Las Palmas,</location>
<marker>Kettebekov, 2002</marker>
<rawString>Kettebekov, S., et al. 2002 Prosody Based Coanalysis of Deictic Gestures and Speech in Weather Narration Broadcast, in Workshop on Multimodal Resources and Multimodal System Evaluation. (LREC 2002), Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>Hand and mind: What gestures reveal about thought</title>
<date>1992</date>
<institution>University of Chicago Press and CSLI Publications,</institution>
<location>Chicago, IL.</location>
<contexts>
<context position="6122" citStr="McNeill, 1992" startWordPosition="900" endWordPosition="901">re 3 shows a component breakdown of CLAVIUS ’s software architecture. The sections that follow explain the flow of information through this system from sensory input to semantic interpretation. Figure 3: Simplified information flow between fundamental software components. 2.1 Lexica and Preprocessing Each unique input modality is asynchronously monitored by one of T TRACKERS, each sending an n-best list of lexical hypotheses to CLAVIUS for any activity as soon as it is detected. For example, a gesture tracker (see Figure 4a) parametrizes the gestures preparation, stroke/point, and retraction (McNeill, 1992), with values reflecting spatial positions and velocities of arm motion, whereas 86 our speech tracker parametrises words with partof-speech tags, and prior probabilities (see Figure 4b). Although preprocessing is reduced to the identification of lexical tokens, this is more involved than simple lexicon lookup due to the modelling of complex signals. Figure 4: Gestural (a) and spoken (b) ‘words’. 2.2 Data Structures All TRACKERS write their hypotheses directly to the first of three SUBSPACES that partition all partial parses in the search space. The first is the GENERALISER’s subspace, °[G], w</context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>McNeill, D. 1992 Hand and mind: What gestures reveal about thought University of Chicago Press and CSLI Publications, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rocio</author>
<author>J G Lopes</author>
</authors>
<date>1998</date>
<booktitle>Partial Parsing, Deduction and Tabling in TAPD 98</booktitle>
<marker>Rocio, Lopes, 1998</marker>
<rawString>Rocio, V., Lopes, J.G. 1998 Partial Parsing, Deduction and Tabling in TAPD 98</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm for Natural Languages, in</title>
<date>1985</date>
<booktitle>Proc. Ninth Intl. Joint Conf. on Artificial Intelligence,</booktitle>
<location>Los Angeles, CA.</location>
<contexts>
<context position="7870" citStr="Tomita, 1985" startWordPosition="1173" endWordPosition="1174">rtial parse, &apos;Pg, in °[G], and creates new parses &apos;Pi for all grammar rules Fi having CATEGORY(&apos;Pg) on the right-hand side. Effectively, these new parses are instantiations of the relevant Fi, with one constituent unified to &apos;Pg. This provides the impetus towards sentence-level parses, as simplified in Algorithm 1 and exemplified in Figure 5. Naturally, if rule Fi has more than one constituent (c &gt; 1) of type CATEGORY(&apos;Pg), then c new parses are created, each with one of these being instantiated. Since the GENERALISER is activated as soon as input is added to °[G], the process is interactive (Tomita, 1985), and therefore incorporates the associated benefits of efficiency. This is contrasted with the all-paths bottom-up strategy in GEMINI (Dowding et al, 1993) that finds all admissable edges of the grammar. Algorithm 1: Simplified Generalisation Data: Subspace °[G], grammar F while data remains in °[G] do &apos;Pg := highest scoring graph in °[G] foreach rule Fi s.t. Cat (&apos;Pg) E RHS(Fi) do &apos;Pi := Unify (Fi, [• —RHS • &apos;Pg]) if ]&apos;Pi then Apply Score (&apos;Pi) to &apos;Pi Insert &apos;Pi into °[G] move &apos;Pg into °[SAct] Figure 5: Example of GENERALISATION. 2.4 Specification The SPECIFIER thread provides the impetus to</context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Tomita, M. 1985 An Efficient Context-Free Parsing Algorithm for Natural Languages, in Proc. Ninth Intl. Joint Conf. on Artificial Intelligence, Los Angeles, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>