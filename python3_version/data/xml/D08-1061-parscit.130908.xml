<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.9938405">
Weakly-Supervised Acquisition of Labeled Class Instances using Graph
Random Walks
</title>
<author confidence="0.987249">
Partha Pratim Talukdar* Joseph Reisinger* Marius Pas¸ca
</author>
<affiliation confidence="0.999502">
University of Pennsylvania University of Texas at Austin Google Inc.
</affiliation>
<address confidence="0.728481">
Philadelphia, PA 19104 Austin, TX 78712 Mountain View, CA 94043
</address>
<email confidence="0.990951">
partha@cis.upenn.edu joeraii@cs.utexas.edu mars@google.com
</email>
<author confidence="0.969004">
Deepak Ravichandran Rahul Bhagat* Fernando Pereira
</author>
<affiliation confidence="0.906114">
Google Inc. USC Information Sciences Institute Google Inc.
</affiliation>
<address confidence="0.920969">
Mountain View, CA 94043 Marina Del Rey, CA 90292 Mountain View, CA 94043
</address>
<email confidence="0.99892">
deepakr@google.com rahul@isi.edu pereira@google.com
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996816111111111">
We present a graph-based semi-supervised la-
bel propagation algorithm for acquiring open-
domain labeled classes and their instances
from a combination of unstructured and struc-
tured text sources. This acquisition method
significantly improves coverage compared to
a previous set of labeled classes and instances
derived from free text, while achieving com-
parable precision.
</bodyText>
<sectionHeader confidence="0.998566" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.981056">
1.1 Motivation
</subsectionHeader>
<bodyText confidence="0.985052794871795">
Users of large document collections can readily ac-
quire information about the instances, classes, and
relationships described in the documents. Such rela-
tions play an important role in both natural language
understanding and Web search, as illustrated by their
prominence in both Web documents and among the
search queries submitted most frequently by Web
users (Jansen et al., 2000). These observations moti-
vate our work on algorithms to extract instance-class
information from Web documents.
While work on named-entity recognition tradi-
tionally focuses on the acquisition and identifica-
tion of instances within a small set of coarse-grained
classes, the distribution of instances within query
logs indicates that Web search users are interested
in a wider range of more fine-grained classes. De-
pending on prior knowledge, personal interests and
immediate needs, users submit for example medi-
cal queries about the symptoms of leptospirosis or
*Contributions made during internships at Google.
the treatment of monkeypox, both of which are in-
stances of zoonotic diseases, or the risks and benefits
of surgical procedures such as PRK and angioplasty.
Other users may be more interested in African coun-
tries such as Uganda and Angola, or active volca-
noes like Etna and Kilauea. Note that zoonotic dis-
eases, surgical procedures, African countries and
active volcanoes serve as useful class labels that cap-
ture the semantics of the associated sets of class in-
stances. Such interest in a wide variety of specific
domains highlights the utility of constructing large
collections of fine-grained classes.
Comprehensive and accurate class-instance in-
formation is useful not only in search but also
in a variety of other text processing tasks includ-
ing co-reference resolution (McCarthy and Lehn-
ert, 1995), named entity recognition (Stevenson and
Gaizauskas, 2000) and seed-based information ex-
traction (Riloff and Jones, 1999).
</bodyText>
<subsectionHeader confidence="0.955339">
1.2 Contributions
</subsectionHeader>
<bodyText confidence="0.9944857">
We study the acquisition of open-domain, labeled
classes and their instances from both structured
and unstructured textual data sources by combin-
ing and ranking individual extractions in a princi-
pled way with the Adsorption label-propagation al-
gorithm (Baluja et al., 2008), reviewed in Section 3
below.
A collection of labeled classes acquired from
text (Van Durme and Pas¸ca, 2008) is extended in two
ways:
</bodyText>
<listItem confidence="0.636247">
1. Class label coverage is increased by identify-
ing additional class labels (such as public agen-
cies and governmental agencies) for existing
</listItem>
<page confidence="0.956111">
582
</page>
<note confidence="0.817867">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 582–590,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
instances such as Office of War Information),
</note>
<bodyText confidence="0.994007909090909">
2. The overall instance coverage is increased by
extracting additional instances (such as Addi-
son Wesley and Zebra Books) for existing class
labels (book publishers).
The WebTables database constructed by Cafarella
et al. (2008) is used as the source of additional
instances. Evaluations on gold-standard labeled
classes and instances from existing linguistic re-
sources (Fellbaum, 1998) indicate coverage im-
provements relative to that of Van Durme and Pas¸ca
(2008), while retaining similar precision levels.
</bodyText>
<sectionHeader confidence="0.949976" genericHeader="method">
2 First Phase Extractors
</sectionHeader>
<bodyText confidence="0.9894226">
To show Adsorption’s ability to uniformly combine
extractions from multiple sources and methods, we
apply it to: 1) high-precision open-domain extrac-
tions from free Web text (Van Durme and Pas¸ca,
2008), and 2) high-recall extractions from WebTa-
bles, a large database of HTML tables mined from
the Web (Cafarella et al., 2008). These two meth-
ods were chosen to be representative of two broad
classes of extraction sources: free text and structured
Web documents.
</bodyText>
<subsectionHeader confidence="0.993254">
2.1 Extraction from Free Text
</subsectionHeader>
<bodyText confidence="0.999925842105263">
Van Durme and Pas¸ca (2008) produce an open-
domain set of instance clusters C E C that parti-
tions a given set of instances I using distributional
similarity (Lin and Pantel, 2002), and labels using
is-a patterns (Hearst, 1992). By filtering the class
labels using distributional similarity, a large number
of high-precision labeled clusters are extracted. The
algorithm proceeds iteratively: at each step, all clus-
ters are tested for label coherence and all coherent
labels are tested for high cluster specificity. Label
L is coherent if it is shared by at least J% of the
instances in cluster C, and it is specific if the total
number of other clusters C&apos; E C, C&apos; =� C containing
instances with label L is less than K. When a cluster
is found to match these criteria, it is removed from
C and added to an output set. The procedure termi-
nates when no new clusters can be removed from C.
Table 1 shows a few randomly chosen classes and
representative instances obtained by this procedure.
</bodyText>
<subsectionHeader confidence="0.998919">
2.2 Extraction from Structured Text
</subsectionHeader>
<bodyText confidence="0.985838434782609">
To expand the instance sets extracted from free
text, we use a table-based extraction method that
mines structured Web data in the form of HTML
tables. A significant fraction of the HTML ta-
bles in Web pages is assumed to contain coherent
lists of instances suitable for extraction. Identifying
such tables from scratch is hard, but seed instance
lists can be used to identify potentially coherent ta-
ble columns. In this paper we use the WebTables
database of around 154 million tables as our struc-
tured data source (Cafarella et al., 2008).
We employ a simple ranking scheme for candi-
date instances in the WebTables corpus T . Each ta-
ble T E T consists of one or more columns. Each
column g E T consists of a set of candidate in-
stances i E g corresponding to row elements. We
define the set of unique seed matches in g relative to
semantic class C E C as
MC(g) def= {i E I(C) : i E g}
where I(C) denotes the set of instances in seed class
C. For each column g, we define its α-unique class
coverage, that is, the set of classes that have at least
α unique seeds in g,
</bodyText>
<equation confidence="0.838501">
Q(g; α) def = {C E C : IMC(g)I &gt;_ α}.
</equation>
<bodyText confidence="0.93181725">
Using M and Q we define a method for scoring
columns relative to each class. Intuitively, such a
score should take into account not only the number
of matches from class C, but also the total num-
ber of classes that contribute to Q and their relative
overlap. Towards this end, we introduce the scoring
function
class coherence
</bodyText>
<equation confidence="0.9983465">
� �� �
IMC(g)�
�
� UC�EQ(g&apos;,α) I(C&apos;)I
</equation>
<bodyText confidence="0.999787714285714">
which is the simplest scoring function combining
the number of seed matches with the coherence of
the table column. Coherence is a critical notion
in WebTables extraction, as some tables contain in-
stances across many diverse seed classes, contribut-
ing to extraction noise. The class coherence intro-
duced here also takes into account class overlap; that
</bodyText>
<equation confidence="0.555618">
score(C, g; α) def = IMC(g)I
11.1
seed matches
</equation>
<page confidence="0.996918">
583
</page>
<table confidence="0.999241875">
Class Size Examples of Instances
Book Publishers 70 crown publishing, kluwer academic, prentice hall, puffin
Federal Agencies 161 catsa, dhs, dod, ex-im bank, fsis, iema, mema, nipc, nmfs, tdh, usdot
Mammals 956 armadillo, elephant shrews, long-tailed weasel, river otter, weddell seals, wild goat
NFL Players 180 aikman, deion sanders, fred taylor, jamal lewis, raghib ismail, troy vincent
Scientific Journals 265 biometrika, european economic review, nature genetics, neuroscience
Social Issues 210 gender inequality, lack of education, substandard housing, welfare dependency
Writers 5089 bronte sisters, hemingway, kipling, proust, torquato tasso, ungaretti, yeats
</table>
<tableCaption confidence="0.999941">
Table 1: A sample of the open-domain classes and associated instances from (Van Durme and Pas¸ca, 2008).
</tableCaption>
<bodyText confidence="0.9979296">
is, a column containing many semantically similar
classes is penalized less than one containing diverse
classes.1 Finally, an extracted instance i is assigned
a score relative to class C equal to the sum of all its
column scores,
</bodyText>
<equation confidence="0.9880735">
�score(i, C; α)def = 1
ZC gET,TET
</equation>
<bodyText confidence="0.9999075">
where ZC is a normalizing constant set to the max-
imum score of any instance in class C. This scor-
ing function assigns high rank to instances that oc-
cur frequently in columns with many seed matches
and high class specificity.
The ranked list of extracted instances is post-
filtered by removing all instances that occur in less
than d unique Internet domains.
</bodyText>
<sectionHeader confidence="0.997508" genericHeader="method">
3 Graph-Based Extraction
</sectionHeader>
<bodyText confidence="0.9991726">
To combine the extractions from both free and struc-
tured text, we need a representation capable of en-
coding efficiently all the available information. We
chose a graph representation for the following rea-
sons:
</bodyText>
<listItem confidence="0.914843235294118">
• Graphs can represent complicated relationships
between classes and instances. For example,
an ambiguous instance such as Michael Jor-
dan could belong to the class of both Profes-
sors and NBA players. Similarly, an instance
may belong to multiple nodes in the hierarchy
of classes. For example, Blue Whales could be-
long to both classes Vertebrates and Mammals,
because Mammals are a subset of Vertebrates.
1Note that this scoring function does not take into account
class containment: if all seeds are both wind Instruments and
instruments, then the column should assign higher score to the
more specific class.
• Extractions from multiple sources, such as Web
queries, Web tables, and text patterns can be
represented in a single graph.
• Graphs make explicit the potential paths of in-
</listItem>
<bodyText confidence="0.990887633333333">
formation propagation that are implicit in the
more common local heuristics used for weakly-
supervised information extraction. For exam-
ple, if we know that the instance Bill Clinton
belongs to both classes President and Politician
then this should be treated as evidence that the
class of President and Politician are related.
Each instance-class pair (i, C) extracted in the
first phase (Section 2) is represented as a weighted
edge in a graph G = (V, E, W), where V is the set
of nodes, E is the set of edges and W : E —* R+
is the weight function which assigns positive weight
to each edge. In particular, for each (i, C, w) triple
from the set of base extractions, i and C are added
to V and (i, C) is added to E, 2 with W (i, C) = w.
The weight w represents the total score of all extrac-
tions with that instance and class. Figure 1 illustrates
a portion of a sample graph. This simple graph rep-
resentation could be refined with additional types of
nodes and edges, as we discuss in Section 7.
In what follows, all nodes are treated in the same
way, regardless of whether they represent instances
or classes. In particular, all nodes can be assigned
class labels. For an instance node, that means that
the instance is hypothesized to belong to the class;
for a class node, that means that the node’s class is
hypothesized to be semantically similar to the label’s
class (Section 5).
We now formulate the task of assigning labels to
nodes as graph label propagation. We are given a
</bodyText>
<footnote confidence="0.882695">
2In practice, we use two directed edges, from i to C and
from C to i, both with weight w.
</footnote>
<equation confidence="0.907788">
score(C, g; α)
</equation>
<page confidence="0.99575">
584
</page>
<figureCaption confidence="0.952829">
Figure 1: Section of a graph used as input into Adsorp-
tion. Though the nodes do not have any type associated
with them, for readability, instance nodes are marked in
pink while class nodes are shown in green.
</figureCaption>
<bodyText confidence="0.992470777777778">
set of instances I and a set of classes C represented
as nodes in the graph, with connecting edges as de-
scribed above. We annotate a few instance nodes
with labels drawn from C. That is, classes are used
both as nodes in the graph and as labels for nodes.
There is no necessary alignment between a class
node and any of the (class) labels, as the final labels
will be assigned by the Adsorption algorithm.
The Adsorption label propagation algo-
rithm (Baluja et al., 2008) is now applied to
the given graph. Adsorption is a general framework
for label propagation, consisting of a few nodes
annotated with labels and a rich graph structure
containing the universe of all labeled and unlabeled
nodes. Adsorption proceeds to label all nodes
based on the graph structure, ultimately producing a
probability distribution over labels for each node.
More specifically, Adsorption works on a graph
G = (V, E, W) and computes for each node v a la-
bel distribution L„ that represents which labels are
more or less appropriate for that node. Several in-
terpretations of Adsorption-type algorithms have ap-
peared in various fields (Azran, 2007; Zhu et al.,
2003; Szummer and Jaakkola, 2002; Indyk and Ma-
tousek, 2004). For details, the reader is referred to
(Baluja et al., 2008). We use two interpretations
here:
Adsorption through Random Walks: Let Gr =
(V, Er, Wr) be the edge-reversed version of the
original graph G = (V, E, W) where (a, b) E
Er iff (b, a) E E; and Wr(a, b) = W (b, a).
Now, choose a node of interest q E V . To es-
timate LQ for q, we perform a random walk on
Gr starting from q to generate values for a ran-
dom label variable L. After reaching a node v
during the walk, we have three choices:
</bodyText>
<listItem confidence="0.8068855">
1. With probability p�°nt
„ , continue the ran-
dom walk to a neighbor of v.
2. With probability p�bnd
</listItem>
<bodyText confidence="0.9954299">
„ , abandon the ran-
dom walk. This abandonment proba-
bility makes the random walk stay rela-
tively close to its source when the graph
has high-degree nodes. When the ran-
dom walk passes through such a node,
it is likely that further transitions will be
into regions of the graph unrelated to the
source. The abandonment probability mit-
igates that effect.
</bodyText>
<listItem confidence="0.841051">
3. With probability p�n�
</listItem>
<bodyText confidence="0.98537075">
„ , stop the random
walk and emit a label L from I„.
LQ is set to the expectation of all labels L emit-
ted from random walks initiated from node q.
Adsorption through Averaging: For this interpre-
tation we make some changes to the original
graph structure and label set. We extend the la-
bel distributions L„ to assign a probability not
only to each label in C but also to the dummy
label L, which represents lack of information
about the actual label(s). We represent the ini-
tial knowledge we have about some node labels
in an augmented graph G&apos; =
follows. For each v E V , we define an ini-
tial distribution I„ = L1, where L1 is the
dummy distribution with L1(L) = 1, repre-
senting lack of label information for v. In addi-
tion, let V3 C V be the set of nodes for which
we have some actual label knowledge, and let
V &apos; = V U {v : v E V3},E&apos; = E U {(v,v) :
v E V3}, and W&apos;(v, v) = 1 for v E V3,
W&apos;(u, v) = W (u, v) for u, v E V . Finally,
let Iv (seed labels) specify the knowledge about
possible labels for v E V3. Less formally, the
v� nodes in G&apos; serve to inject into the graph the
prior label distributions for each v E V3.
The algorithm proceeds as follows: For each
node use a fixed-point computation to find label
</bodyText>
<figure confidence="0.9966951">
musician
singer
0.95
0.87
0.82
0.73
0.75 billy joel
johnny cash
bob dylan
(V &apos;, E&apos;, W&apos;) as
</figure>
<page confidence="0.992839">
585
</page>
<bodyText confidence="0.997650857142857">
distributions that are weighted averages of the
label distributions for all their neighbors. This
causes the non-dummy initial distribution of Vs
nodes to be propagated across the graph.
Baluja et al. (2008) show that those two views are
equivalent. Algorithm 1 combines the two views:
instead of a random walk, for each node v, it itera-
tively computes the weighted average of label distri-
butions from neighboring nodes, and then uses the
random walk probabilities to estimate a new label
distribution for v.
For the experiments reported in Section 4, we
used the following heuristics from Baluja et al.
(2008) to set the random walk probabilities:
</bodyText>
<listItem confidence="0.720329">
• Let cv = log β where H(v) =
</listItem>
<equation confidence="0.992275333333333">
log(β + exp H(v))
− upuv X log(puv) with puv = W(u,v)
� �0 W (u0,v).
</equation>
<bodyText confidence="0.547173">
H(v) can be interpreted as the entropy of v’s
neighborhood. Thus, cv is lower if v has many
neighbors. We set Q = 2.
VI
</bodyText>
<listItem confidence="0.957906">
• jv = (1 − cv) X H(v) if Iv =� LT and 0
otherwise.
• Then let
</listItem>
<equation confidence="0.961234375">
zv = max(cv + jv, 1)
cont
pv
= cv/zv
inj
pv = jv/zv
pabnd = 1 − pcont v− pabnd
v v
</equation>
<bodyText confidence="0.99878">
Thus, abandonment occurs only when the con-
tinuation and injection probabilities are low
enough.
The algorithm is run until convergence which is
achieved when the label distribution on each node
ceases to change within some tolerance value. Alter-
natively, the algorithm can be run for a fixed number
of iterations which is what we used in practice3.
Finally, since Adsorption is memoryless, it eas-
ily scales to tens of millions of nodes with dense
edges and can be easily parallelized, as described
by Baluja et al. (2008).
</bodyText>
<footnote confidence="0.9890935">
3The number of iterations was set to 10 in the experiments
reported in this paper.
</footnote>
<construct confidence="0.616671666666667">
Algorithm 1 Adsorption Algorithm.
Input: G&apos; = (V 0, E0, W&apos;), Iv (Vv E V &apos;).
Output: Distributions {Lv : v E V 1.
</construct>
<listItem confidence="0.9879742">
1: Lv=Iv Vv E V 0
3: repeat
4: Nv = Eu W (u, v)
5: Dv = 1 Eu W (u, v)Lu Vv E V 0
6: for all v E V 0 do
v X Dv +pinj
7: Lv = pcont v X Iv + pabnd
v X LT
8: end for
9: until convergence
</listItem>
<sectionHeader confidence="0.998292" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.894092">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999992565217391">
As mentioned in Section 3, one of the benefits of
using Adsorption is that we can combine extrac-
tions by different methods from diverse sources into
a single framework. To demonstrate this capabil-
ity, we combine extractions from free-text patterns
and from Web tables. To the best of our knowl-
edge, this is one of the first attempts in the area of
minimally-supervised extraction algorithms where
unstructured and structured text are used in a prin-
cipled way within a single system.
Open-domain (instance, class) pairs were ex-
tracted by applying the method described by Van
Durme and Pas¸ca (2008) on a corpus of over 100M
English web documents. A total of 924K (instance,
class) pairs were extracted, containing 263K unique
instances in 9081 classes. We refer to this dataset as
A8.
Using A8, an additional 74M unique (in-
stance,class) pairs are extracted from a random 10%
of the WebTables data, using the method outlined in
Section 2.2. For maximum coverage we set α = 2
and d = 2, resulting in a large, but somewhat noisy
collection. We refer to this data set as WT.
</bodyText>
<subsectionHeader confidence="0.973415">
4.2 Graph Creation
</subsectionHeader>
<bodyText confidence="0.999819">
We applied the graph construction scheme described
in Section 3 on the A8 and WT data combined, re-
sulting in a graph with 1.4M nodes and 75M edges.
Since extractions in A8 are not scored, weight of all
</bodyText>
<page confidence="0.99694">
586
</page>
<table confidence="0.995457428571428">
Seed Class Seed Instances
Book Publishers millbrook press, academic press, springer verlag, chronicle books, shambhala publications
Federal Agencies dod, nsf, office of war information, tsa, fema
Mammals african wild dog, hyaena, hippopotamus, sperm whale, tiger
NFL Players ike hilliard, isaac bruce, torry holt, jon kitna, jamal lewis
Scientific Journals american journal of roentgenology, pnas, journal of bacteriology, american economic review,
ibm systems journal
</table>
<tableCaption confidence="0.998862">
Table 2: Classes and seeds used to initialize Adsorption.
</tableCaption>
<bodyText confidence="0.8376565">
edges originating from A8 were set at 14. This graph
is used in all subsequent experiments.
</bodyText>
<sectionHeader confidence="0.998345" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999684333333333">
We evaluated the Adsorption algorithm under two
experimental settings. First, we evaluate Adsorp-
tion’s extraction precision on (instance, class) pairs
obtained by Adsorption but not present in A8 (Sec-
tion 5.1). This measures whether Adsorption can
add to the A8 extractions at fairly high precision.
Second, we measured Adsorption’s ability to assign
labels to a fixed set of gold instances drawn from
various classes (Section 5.2).
</bodyText>
<figure confidence="0.996181571428571">
100
80
60
40
20
Bookk bli
Publishers
</figure>
<figureCaption confidence="0.99421">
Figure 2: Precision at 100 comparisons for A8 and Ad-
sorption.
</figureCaption>
<subsectionHeader confidence="0.976729">
5.1 Instance Precision
</subsectionHeader>
<bodyText confidence="0.999738571428571">
First we manually evaluated precision across five
randomly selected classes from A8: Book Publish-
ers, Federal Agencies, NFL Players, Scientific Jour-
nals and Mammals. For each class, 5 seed in-
stances were chosen manually to initialize Adsorp-
tion. These classes and seeds are shown in Table 2.
Adsorption was run for each class separately and the
</bodyText>
<footnote confidence="0.976898">
4A8 extractions are assumed to be high-precision and hence
we assign them the highest possible weight.
</footnote>
<bodyText confidence="0.999745826086957">
resulting ranked extractions were manually evalu-
ated.
Since the A8 system does not produce ranked lists
of instances, we chose 100 random instances from
the A8 results to compare to the top 100 instances
produced by Adsorption. Each of the resulting 500
instance-class pairs (i, C) was presented to two hu-
man evaluators, who were asked to evaluate whether
the relation “i is a C” was correct or incorrect. The
user was also presented with Web search link to ver-
ify the results against actual documents. Results
from these experiments are presented in Figure 2
and Table 4. The results in Figure 2 show that the
A8 system has higher precision than the Adsorption
system. This is not surprising since the A8 system is
tuned for high precision. When considering individ-
ual evaluation classes, changes in precision scores
between the A8 system and the Adsorption system
vary from a small increase from 87% to 89% for the
class Book Publishers, to a significant decrease from
52% to 34% for the class Federal Agencies, with a
decrease of 10% as an average over the 5 evaluation
classes.
</bodyText>
<table confidence="0.999789571428571">
Class Precision at 100
(non-A8 extractions)
Book Publishers 87.36
Federal Agencies 29.89
NFL Players 94.95
Scientific Journals 90.82
Mammal Species 84.27
</table>
<tableCaption confidence="0.99378375">
Table 4: Precision of top 100 Adsorption extractions (for
five classes) which were not present in A8.
Table 4 shows the precision of the Adsorption sys-
tem for instances not extracted by the A8 system.
</tableCaption>
<table confidence="0.983773857142857">
dea Agec L Play ietifc Journa Mammals
Federal NFL Scientific
Agencies Players Journals
A8 Adsorption
587
Seed Class Non-Seed Class Labels Discovered by Adsorption
Book Publishers small presses, journal publishers, educational publishers, academic publishers,
commercial publishers
Federal Agencies public agencies, governmental agencies, modulation schemes, private sources,
technical societies
NFL Players sports figures, football greats, football players, backs, quarterbacks
Scientific Journals prestigious journals, peer-reviewed journals, refereed journals, scholarly journals,
academic journals
Mammal Species marine mammal species, whale species, larger mammals, common animals, sea mammals
</table>
<tableCaption confidence="0.996773">
Table 3: Top class labels ranked by their similarity to a given seed class in Adsorption.
</tableCaption>
<table confidence="0.856883">
Seed Class Sample of Top Ranked Instances Discovered by Adsorption
Book Publishers small night shade books, house of anansi press, highwater books,
distributed art publishers, copper canyon press
NFL Players tony gonzales, thabiti davis, taylor stubblefield, ron dixon, rodney hannah
Scientific Journals journal of physics, nature structural and molecular biology,
sciences sociales et sant´e, kidney and blood pressure research,
american journal of physiology–cell physiology
</table>
<tableCaption confidence="0.9909305">
Table 5: Random examples of top ranked extractions (for three classes) found by Adsorption which were not present
in A8.
</tableCaption>
<bodyText confidence="0.999677375">
Such an evaluation is important as one of the main
motivations of the current work is to increase cov-
erage (recall) of existing high-precision extractors
without significantly affecting precision. Results in
Table 4 show that Adsorption is indeed able to ex-
traction with high precision (in 4 out of 5 cases)
new instance-class pairs which were not extracted
by the original high-precision extraction set (in this
case A8). Examples of a few such pairs are shown
in Table 5. This is promising as almost all state-
of-the-art extraction methods are high-precision and
low-recall. The proposed method shows a way to
overcome that limitation.
As noted in Section 3, Adsorption ignores node
type and hence the final ranked extraction may also
contain classes along with instances. Thus, in ad-
dition to finding new instances for classes, it also
finds additional class labels similar to the seed class
labels with which Adsorption was run, at no extra
cost. Some of the top ranked class labels extracted
by Adsorption for the corresponding seed class la-
bels are shown in Table 3. To the best of our knowl-
edge, there are no other systems which perform both
tasks simultaneously.
</bodyText>
<subsectionHeader confidence="0.997455">
5.2 Class Label Recall
</subsectionHeader>
<bodyText confidence="0.999970333333333">
Next we evaluated each extraction method on its rel-
ative ability to assign labels to class instances. For
each test instance, the five most probably class la-
bels are collected using each method and the Mean
Reciprocal Rank (MRR) is computed relative to a
gold standard target set. This target set, WN-gold,
consists of the 38 classes in Wordnet containing 100
or more instances.
In order to extract meaningful output from Ad-
sorption, it is provided with a number of labeled seed
instances (1, 5, 10 or 25) from each of the 38 test
classes. Regardless of the actual number of seeds
used as input, all 25 seed instances from each class
are removed from the output set from all methods,
in order to ensure fair comparison.
The results from this evaluation are summarized
in Table 6; AD x refers to the adsorption run with x
seed instances. Overall, Adsorption exhibits higher
MRR than either of the baseline methods, with MRR
increasing as the amount of supervision is increased.
Due to its high coverage, WT assigns labels to
a larger number of the instance in WN-gold than
any other method. However, the average rank of
the correct class assignment is lower, resulting is
</bodyText>
<page confidence="0.992795">
588
</page>
<table confidence="0.999412">
Method MRR MRR # found
(full) (found only)
A8 0.16 0.47 2718
WT 0.15 0.21 5747
AD 1 0.26 0.45 4687
AD 5 0.29 0.48 4687
AD 10 0.30 0.51 4687
AD 25 0.32 0.55 4687
</table>
<tableCaption confidence="0.9851396">
Table 6: Mean-Reciprocal Rank scores of instance class
labels over 38 Wordnet classes (WN-gold). MRR (full)
refers to evaluation across the entire gold instance set.
MRR (found only) computes MRR only on recalled in-
stances.
</tableCaption>
<bodyText confidence="0.999527">
lower MRR scores compared to Adsorption. This
result highlights Adsorption’s ability to effectively
combine high-precision, low-recall (A8) extractions
with low-precision, high-recall extractions (WT) in
a manner that improves both precision and coverage.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999929153846154">
Graph based algorithms for minimally supervised
information extraction methods have recently been
proposed. For example, Wang and Cohen (2007)
use a random walk on a graph built from entities and
relations extracted from semi-structured text. Our
work differs both conceptually, in terms of its focus
on open-domain extraction, as well as methodologi-
cally, as we incorporate both unstructured and struc-
tured text. The re-ranking algorithm of Bellare et al.
(2007) also constructs a graph whose nodes are in-
stances and attributes, as opposed to instances and
classes here. Adsorption can be seen as a general-
ization of the method proposed in that paper.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999632">
The field of open-domain information extraction has
been driven by the growth of Web-accessible data.
We have staggering amounts of data from various
structured and unstructured sources such as general
Web text, online encyclopedias, query logs, web ta-
bles, or link anchor texts. Any proposed algorithm
to extract information needs to harness several data
sources and do it in a robust and scalable manner.
Our work in this paper represents a first step towards
that goal. In doing so, we achieved the following:
</bodyText>
<listItem confidence="0.9278945">
1. Improved coverage relative to a high accuracy
instance-class extraction system while main-
taining adequate precision.
2. Combined information from two different
sources: free text and web tables.
3. Demonstrated a graph-based label propagation
</listItem>
<bodyText confidence="0.962927">
algorithm that given as little as five seeds per
class achieved good results on a graph with
more than a million nodes and 70 million
edges.
In this paper, we started off with a simple graph.
For future work, we plan to proceed along the fol-
lowing lines:
</bodyText>
<listItem confidence="0.965893333333333">
1. Encode richer relationships between nodes,
for example instance-instance associations and
other types of nodes.
2. Combine information from more data sources
to answer the question of whether more data or
diverse sources are more effective in increasing
precision and coverage.
3. Apply similar ideas to other information extrac-
tion tasks such as relation extraction.
</listItem>
<sectionHeader confidence="0.998355" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999844">
We would like to thank D. Sivakumar for useful dis-
cussions and the anonymous reviewers for helpful
comments.
</bodyText>
<sectionHeader confidence="0.998766" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9942496">
A. Azran. 2007. The rendezvous algorithm: multiclass
semi-supervised learning with markov random walks.
Proceedings of the 24th international conference on
Machine learning, pages 49–56.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random
walks through the view graph.
K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. NIPS 2007 Workshop
on Machine Learning for Web Search.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. Webtables: Exploring the power of tables on the
web. VLDB.
</reference>
<page confidence="0.986297">
589
</page>
<reference confidence="0.999857714285714">
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539–545, Nantes, France.
P. Indyk and J. Matousek. 2004. Low-distortion embed-
dings of finite metric spaces. Handbook of Discrete
and Computational Geometry.
B. Jansen, A. Spink, and T. Saracevic. 2000. Real life,
real users, and real needs: a study and analysis of user
queries on the Web. Information Processing and Man-
agement, 36(2):207–227.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1–
7.
K. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence (IJCAI-95), pages 1050–1055, Montreal, Que-
bec.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI-99), pages 474–479, Or-
lando, Florida.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-
derived name lists for named entity recognition. In
Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), Seattle, Wash-
ington.
M. Szummer and T. Jaakkola. 2002. Partially labeled
classification with markov random walks. Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2002 NIPS Conference.
B. Van Durme and M. Pas¸ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. Twenty-Third AAAI Conference on Artificial In-
telligence.
R. Wang and W. Cohen. 2007. Language-Independent
Set Expansion of Named Entities Using the Web. Data
Mining, 2007. ICDM 2007. Seventh IEEE Interna-
tional Conference on, pages 342–350.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. ICML-03, 20th International Con-
ference on Machine Learning.
</reference>
<page confidence="0.997331">
590
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.458987">
<title confidence="0.8611665">Weakly-Supervised Acquisition of Labeled Class Instances using Graph Random Walks</title>
<author confidence="0.5662">Pratim</author>
<affiliation confidence="0.99996">University of Pennsylvania University of Texas at Austin Google Inc.</affiliation>
<address confidence="0.999429">Philadelphia, PA 19104 Austin, TX 78712 Mountain View, CA 94043</address>
<email confidence="0.998711">partha@cis.upenn.edujoeraii@cs.utexas.edumars@google.com</email>
<author confidence="0.924954">Ravichandran Rahul Pereira</author>
<affiliation confidence="0.999738">Google Inc. USC Information Sciences Institute Google Inc.</affiliation>
<address confidence="0.999967">Mountain View, CA 94043 Marina Del Rey, CA 90292 Mountain View, CA 94043</address>
<email confidence="0.999695">deepakr@google.comrahul@isi.edupereira@google.com</email>
<abstract confidence="0.9938414">We present a graph-based semi-supervised label propagation algorithm for acquiring opendomain labeled classes and their instances from a combination of unstructured and structured text sources. This acquisition method significantly improves coverage compared to a previous set of labeled classes and instances derived from free text, while achieving comparable precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Azran</author>
</authors>
<title>The rendezvous algorithm: multiclass semi-supervised learning with markov random walks.</title>
<date>2007</date>
<booktitle>Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="12958" citStr="Azran, 2007" startWordPosition="2124" endWordPosition="2125"> is a general framework for label propagation, consisting of a few nodes annotated with labels and a rich graph structure containing the universe of all labeled and unlabeled nodes. Adsorption proceeds to label all nodes based on the graph structure, ultimately producing a probability distribution over labels for each node. More specifically, Adsorption works on a graph G = (V, E, W) and computes for each node v a label distribution L„ that represents which labels are more or less appropriate for that node. Several interpretations of Adsorption-type algorithms have appeared in various fields (Azran, 2007; Zhu et al., 2003; Szummer and Jaakkola, 2002; Indyk and Matousek, 2004). For details, the reader is referred to (Baluja et al., 2008). We use two interpretations here: Adsorption through Random Walks: Let Gr = (V, Er, Wr) be the edge-reversed version of the original graph G = (V, E, W) where (a, b) E Er iff (b, a) E E; and Wr(a, b) = W (b, a). Now, choose a node of interest q E V . To estimate LQ for q, we perform a random walk on Gr starting from q to generate values for a random label variable L. After reaching a node v during the walk, we have three choices: 1. With probability p�°nt „ , </context>
</contexts>
<marker>Azran, 2007</marker>
<rawString>A. Azran. 2007. The rendezvous algorithm: multiclass semi-supervised learning with markov random walks. Proceedings of the 24th international conference on Machine learning, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Baluja</author>
<author>R Seth</author>
<author>D Sivakumar</author>
<author>Y Jing</author>
<author>J Yagnik</author>
<author>S Kumar</author>
<author>D Ravichandran</author>
<author>M Aly</author>
</authors>
<title>Video suggestion and discovery for youtube: taking random walks through the view graph.</title>
<date>2008</date>
<contexts>
<context position="3187" citStr="Baluja et al., 2008" startWordPosition="460" endWordPosition="463">asses. Comprehensive and accurate class-instance information is useful not only in search but also in a variety of other text processing tasks including co-reference resolution (McCarthy and Lehnert, 1995), named entity recognition (Stevenson and Gaizauskas, 2000) and seed-based information extraction (Riloff and Jones, 1999). 1.2 Contributions We study the acquisition of open-domain, labeled classes and their instances from both structured and unstructured textual data sources by combining and ranking individual extractions in a principled way with the Adsorption label-propagation algorithm (Baluja et al., 2008), reviewed in Section 3 below. A collection of labeled classes acquired from text (Van Durme and Pas¸ca, 2008) is extended in two ways: 1. Class label coverage is increased by identifying additional class labels (such as public agencies and governmental agencies) for existing 582 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 582–590, Honolulu, October 2008.c�2008 Association for Computational Linguistics instances such as Office of War Information), 2. The overall instance coverage is increased by extracting additional instances (such as Addison </context>
<context position="12301" citStr="Baluja et al., 2008" startWordPosition="2015" endWordPosition="2018"> Though the nodes do not have any type associated with them, for readability, instance nodes are marked in pink while class nodes are shown in green. set of instances I and a set of classes C represented as nodes in the graph, with connecting edges as described above. We annotate a few instance nodes with labels drawn from C. That is, classes are used both as nodes in the graph and as labels for nodes. There is no necessary alignment between a class node and any of the (class) labels, as the final labels will be assigned by the Adsorption algorithm. The Adsorption label propagation algorithm (Baluja et al., 2008) is now applied to the given graph. Adsorption is a general framework for label propagation, consisting of a few nodes annotated with labels and a rich graph structure containing the universe of all labeled and unlabeled nodes. Adsorption proceeds to label all nodes based on the graph structure, ultimately producing a probability distribution over labels for each node. More specifically, Adsorption works on a graph G = (V, E, W) and computes for each node v a label distribution L„ that represents which labels are more or less appropriate for that node. Several interpretations of Adsorption-typ</context>
<context position="15523" citStr="Baluja et al. (2008)" startWordPosition="2626" endWordPosition="2629"> = W (u, v) for u, v E V . Finally, let Iv (seed labels) specify the knowledge about possible labels for v E V3. Less formally, the v� nodes in G&apos; serve to inject into the graph the prior label distributions for each v E V3. The algorithm proceeds as follows: For each node use a fixed-point computation to find label musician singer 0.95 0.87 0.82 0.73 0.75 billy joel johnny cash bob dylan (V &apos;, E&apos;, W&apos;) as 585 distributions that are weighted averages of the label distributions for all their neighbors. This causes the non-dummy initial distribution of Vs nodes to be propagated across the graph. Baluja et al. (2008) show that those two views are equivalent. Algorithm 1 combines the two views: instead of a random walk, for each node v, it iteratively computes the weighted average of label distributions from neighboring nodes, and then uses the random walk probabilities to estimate a new label distribution for v. For the experiments reported in Section 4, we used the following heuristics from Baluja et al. (2008) to set the random walk probabilities: • Let cv = log β where H(v) = log(β + exp H(v)) − upuv X log(puv) with puv = W(u,v) � �0 W (u0,v). H(v) can be interpreted as the entropy of v’s neighborhood.</context>
<context position="16851" citStr="Baluja et al. (2008)" startWordPosition="2878" endWordPosition="2881">ise. • Then let zv = max(cv + jv, 1) cont pv = cv/zv inj pv = jv/zv pabnd = 1 − pcont v− pabnd v v Thus, abandonment occurs only when the continuation and injection probabilities are low enough. The algorithm is run until convergence which is achieved when the label distribution on each node ceases to change within some tolerance value. Alternatively, the algorithm can be run for a fixed number of iterations which is what we used in practice3. Finally, since Adsorption is memoryless, it easily scales to tens of millions of nodes with dense edges and can be easily parallelized, as described by Baluja et al. (2008). 3The number of iterations was set to 10 in the experiments reported in this paper. Algorithm 1 Adsorption Algorithm. Input: G&apos; = (V 0, E0, W&apos;), Iv (Vv E V &apos;). Output: Distributions {Lv : v E V 1. 1: Lv=Iv Vv E V 0 3: repeat 4: Nv = Eu W (u, v) 5: Dv = 1 Eu W (u, v)Lu Vv E V 0 6: for all v E V 0 do v X Dv +pinj 7: Lv = pcont v X Iv + pabnd v X LT 8: end for 9: until convergence 4 Experiments 4.1 Data As mentioned in Section 3, one of the benefits of using Adsorption is that we can combine extractions by different methods from diverse sources into a single framework. To demonstrate this capabi</context>
</contexts>
<marker>Baluja, Seth, Sivakumar, Jing, Yagnik, Kumar, Ravichandran, Aly, 2008</marker>
<rawString>S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly. 2008. Video suggestion and discovery for youtube: taking random walks through the view graph.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bellare</author>
<author>P Talukdar</author>
<author>G Kumaran</author>
<author>F Pereira</author>
<author>M Liberman</author>
<author>A McCallum</author>
<author>M Dredze</author>
</authors>
<title>LightlySupervised Attribute Extraction.</title>
<date>2007</date>
<booktitle>NIPS 2007 Workshop on Machine Learning</booktitle>
<institution>for Web Search.</institution>
<contexts>
<context position="26496" citStr="Bellare et al. (2007)" startWordPosition="4459" endWordPosition="4462">precision, low-recall (A8) extractions with low-precision, high-recall extractions (WT) in a manner that improves both precision and coverage. 6 Related Work Graph based algorithms for minimally supervised information extraction methods have recently been proposed. For example, Wang and Cohen (2007) use a random walk on a graph built from entities and relations extracted from semi-structured text. Our work differs both conceptually, in terms of its focus on open-domain extraction, as well as methodologically, as we incorporate both unstructured and structured text. The re-ranking algorithm of Bellare et al. (2007) also constructs a graph whose nodes are instances and attributes, as opposed to instances and classes here. Adsorption can be seen as a generalization of the method proposed in that paper. 7 Conclusion The field of open-domain information extraction has been driven by the growth of Web-accessible data. We have staggering amounts of data from various structured and unstructured sources such as general Web text, online encyclopedias, query logs, web tables, or link anchor texts. Any proposed algorithm to extract information needs to harness several data sources and do it in a robust and scalabl</context>
</contexts>
<marker>Bellare, Talukdar, Kumaran, Pereira, Liberman, McCallum, Dredze, 2007</marker>
<rawString>K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liberman, A. McCallum, and M. Dredze. 2007. LightlySupervised Attribute Extraction. NIPS 2007 Workshop on Machine Learning for Web Search.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cafarella</author>
<author>A Halevy</author>
<author>D Wang</author>
<author>E Wu</author>
<author>Y Zhang</author>
</authors>
<title>Webtables: Exploring the power of tables on the web.</title>
<date>2008</date>
<publisher>VLDB.</publisher>
<contexts>
<context position="3917" citStr="Cafarella et al. (2008)" startWordPosition="568" endWordPosition="571">08) is extended in two ways: 1. Class label coverage is increased by identifying additional class labels (such as public agencies and governmental agencies) for existing 582 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 582–590, Honolulu, October 2008.c�2008 Association for Computational Linguistics instances such as Office of War Information), 2. The overall instance coverage is increased by extracting additional instances (such as Addison Wesley and Zebra Books) for existing class labels (book publishers). The WebTables database constructed by Cafarella et al. (2008) is used as the source of additional instances. Evaluations on gold-standard labeled classes and instances from existing linguistic resources (Fellbaum, 1998) indicate coverage improvements relative to that of Van Durme and Pas¸ca (2008), while retaining similar precision levels. 2 First Phase Extractors To show Adsorption’s ability to uniformly combine extractions from multiple sources and methods, we apply it to: 1) high-precision open-domain extractions from free Web text (Van Durme and Pas¸ca, 2008), and 2) high-recall extractions from WebTables, a large database of HTML tables mined from </context>
<context position="6278" citStr="Cafarella et al., 2008" startWordPosition="961" endWordPosition="964">sentative instances obtained by this procedure. 2.2 Extraction from Structured Text To expand the instance sets extracted from free text, we use a table-based extraction method that mines structured Web data in the form of HTML tables. A significant fraction of the HTML tables in Web pages is assumed to contain coherent lists of instances suitable for extraction. Identifying such tables from scratch is hard, but seed instance lists can be used to identify potentially coherent table columns. In this paper we use the WebTables database of around 154 million tables as our structured data source (Cafarella et al., 2008). We employ a simple ranking scheme for candidate instances in the WebTables corpus T . Each table T E T consists of one or more columns. Each column g E T consists of a set of candidate instances i E g corresponding to row elements. We define the set of unique seed matches in g relative to semantic class C E C as MC(g) def= {i E I(C) : i E g} where I(C) denotes the set of instances in seed class C. For each column g, we define its α-unique class coverage, that is, the set of classes that have at least α unique seeds in g, Q(g; α) def = {C E C : IMC(g)I &gt;_ α}. Using M and Q we define a method </context>
</contexts>
<marker>Cafarella, Halevy, Wang, Wu, Zhang, 2008</marker>
<rawString>M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang. 2008. Webtables: Exploring the power of tables on the web. VLDB.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and Some of its Applications.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and Some of its Applications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92),</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="4941" citStr="Hearst, 1992" startWordPosition="732" endWordPosition="733"> it to: 1) high-precision open-domain extractions from free Web text (Van Durme and Pas¸ca, 2008), and 2) high-recall extractions from WebTables, a large database of HTML tables mined from the Web (Cafarella et al., 2008). These two methods were chosen to be representative of two broad classes of extraction sources: free text and structured Web documents. 2.1 Extraction from Free Text Van Durme and Pas¸ca (2008) produce an opendomain set of instance clusters C E C that partitions a given set of instances I using distributional similarity (Lin and Pantel, 2002), and labels using is-a patterns (Hearst, 1992). By filtering the class labels using distributional similarity, a large number of high-precision labeled clusters are extracted. The algorithm proceeds iteratively: at each step, all clusters are tested for label coherence and all coherent labels are tested for high cluster specificity. Label L is coherent if it is shared by at least J% of the instances in cluster C, and it is specific if the total number of other clusters C&apos; E C, C&apos; =� C containing instances with label L is less than K. When a cluster is found to match these criteria, it is removed from C and added to an output set. The proc</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Indyk</author>
<author>J Matousek</author>
</authors>
<title>Low-distortion embeddings of finite metric spaces.</title>
<date>2004</date>
<booktitle>Handbook of Discrete and Computational Geometry.</booktitle>
<contexts>
<context position="13031" citStr="Indyk and Matousek, 2004" startWordPosition="2134" endWordPosition="2138">f a few nodes annotated with labels and a rich graph structure containing the universe of all labeled and unlabeled nodes. Adsorption proceeds to label all nodes based on the graph structure, ultimately producing a probability distribution over labels for each node. More specifically, Adsorption works on a graph G = (V, E, W) and computes for each node v a label distribution L„ that represents which labels are more or less appropriate for that node. Several interpretations of Adsorption-type algorithms have appeared in various fields (Azran, 2007; Zhu et al., 2003; Szummer and Jaakkola, 2002; Indyk and Matousek, 2004). For details, the reader is referred to (Baluja et al., 2008). We use two interpretations here: Adsorption through Random Walks: Let Gr = (V, Er, Wr) be the edge-reversed version of the original graph G = (V, E, W) where (a, b) E Er iff (b, a) E E; and Wr(a, b) = W (b, a). Now, choose a node of interest q E V . To estimate LQ for q, we perform a random walk on Gr starting from q to generate values for a random label variable L. After reaching a node v during the walk, we have three choices: 1. With probability p�°nt „ , continue the random walk to a neighbor of v. 2. With probability p�bnd „ </context>
</contexts>
<marker>Indyk, Matousek, 2004</marker>
<rawString>P. Indyk and J. Matousek. 2004. Low-distortion embeddings of finite metric spaces. Handbook of Discrete and Computational Geometry.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Jansen</author>
<author>A Spink</author>
<author>T Saracevic</author>
</authors>
<title>Real life, real users, and real needs: a study and analysis of user queries on the Web. Information Processing and</title>
<date>2000</date>
<journal>Management,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="1359" citStr="Jansen et al., 2000" startWordPosition="183" endWordPosition="186">and structured text sources. This acquisition method significantly improves coverage compared to a previous set of labeled classes and instances derived from free text, while achieving comparable precision. 1 Introduction 1.1 Motivation Users of large document collections can readily acquire information about the instances, classes, and relationships described in the documents. Such relations play an important role in both natural language understanding and Web search, as illustrated by their prominence in both Web documents and among the search queries submitted most frequently by Web users (Jansen et al., 2000). These observations motivate our work on algorithms to extract instance-class information from Web documents. While work on named-entity recognition traditionally focuses on the acquisition and identification of instances within a small set of coarse-grained classes, the distribution of instances within query logs indicates that Web search users are interested in a wider range of more fine-grained classes. Depending on prior knowledge, personal interests and immediate needs, users submit for example medical queries about the symptoms of leptospirosis or *Contributions made during internships </context>
</contexts>
<marker>Jansen, Spink, Saracevic, 2000</marker>
<rawString>B. Jansen, A. Spink, and T. Saracevic. 2000. Real life, real users, and real needs: a study and analysis of user queries on the Web. Information Processing and Management, 36(2):207–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational linguistics (COLING-02),</booktitle>
<pages>1--7</pages>
<contexts>
<context position="4894" citStr="Lin and Pantel, 2002" startWordPosition="723" endWordPosition="726">extractions from multiple sources and methods, we apply it to: 1) high-precision open-domain extractions from free Web text (Van Durme and Pas¸ca, 2008), and 2) high-recall extractions from WebTables, a large database of HTML tables mined from the Web (Cafarella et al., 2008). These two methods were chosen to be representative of two broad classes of extraction sources: free text and structured Web documents. 2.1 Extraction from Free Text Van Durme and Pas¸ca (2008) produce an opendomain set of instance clusters C E C that partitions a given set of instances I using distributional similarity (Lin and Pantel, 2002), and labels using is-a patterns (Hearst, 1992). By filtering the class labels using distributional similarity, a large number of high-precision labeled clusters are extracted. The algorithm proceeds iteratively: at each step, all clusters are tested for label coherence and all coherent labels are tested for high cluster specificity. Label L is coherent if it is shared by at least J% of the instances in cluster C, and it is specific if the total number of other clusters C&apos; E C, C&apos; =� C containing instances with label L is less than K. When a cluster is found to match these criteria, it is remo</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>D. Lin and P. Pantel. 2002. Concept discovery from text. In Proceedings of the 19th International Conference on Computational linguistics (COLING-02), pages 1– 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McCarthy</author>
<author>W Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95),</booktitle>
<pages>1050--1055</pages>
<location>Montreal, Quebec.</location>
<contexts>
<context position="2772" citStr="McCarthy and Lehnert, 1995" startWordPosition="400" endWordPosition="404">more interested in African countries such as Uganda and Angola, or active volcanoes like Etna and Kilauea. Note that zoonotic diseases, surgical procedures, African countries and active volcanoes serve as useful class labels that capture the semantics of the associated sets of class instances. Such interest in a wide variety of specific domains highlights the utility of constructing large collections of fine-grained classes. Comprehensive and accurate class-instance information is useful not only in search but also in a variety of other text processing tasks including co-reference resolution (McCarthy and Lehnert, 1995), named entity recognition (Stevenson and Gaizauskas, 2000) and seed-based information extraction (Riloff and Jones, 1999). 1.2 Contributions We study the acquisition of open-domain, labeled classes and their instances from both structured and unstructured textual data sources by combining and ranking individual extractions in a principled way with the Adsorption label-propagation algorithm (Baluja et al., 2008), reviewed in Section 3 below. A collection of labeled classes acquired from text (Van Durme and Pas¸ca, 2008) is extended in two ways: 1. Class label coverage is increased by identifyi</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>K. McCarthy and W. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95), pages 1050–1055, Montreal, Quebec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence (AAAI-99),</booktitle>
<pages>474--479</pages>
<location>Orlando, Florida.</location>
<contexts>
<context position="2894" citStr="Riloff and Jones, 1999" startWordPosition="417" endWordPosition="420">diseases, surgical procedures, African countries and active volcanoes serve as useful class labels that capture the semantics of the associated sets of class instances. Such interest in a wide variety of specific domains highlights the utility of constructing large collections of fine-grained classes. Comprehensive and accurate class-instance information is useful not only in search but also in a variety of other text processing tasks including co-reference resolution (McCarthy and Lehnert, 1995), named entity recognition (Stevenson and Gaizauskas, 2000) and seed-based information extraction (Riloff and Jones, 1999). 1.2 Contributions We study the acquisition of open-domain, labeled classes and their instances from both structured and unstructured textual data sources by combining and ranking individual extractions in a principled way with the Adsorption label-propagation algorithm (Baluja et al., 2008), reviewed in Section 3 below. A collection of labeled classes acquired from text (Van Durme and Pas¸ca, 2008) is extended in two ways: 1. Class label coverage is increased by identifying additional class labels (such as public agencies and governmental agencies) for existing 582 Proceedings of the 2008 Co</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the 16th National Conference on Artificial Intelligence (AAAI-99), pages 474–479, Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>R Gaizauskas</author>
</authors>
<title>Using corpusderived name lists for named entity recognition.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00),</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="2831" citStr="Stevenson and Gaizauskas, 2000" startWordPosition="408" endWordPosition="411"> Angola, or active volcanoes like Etna and Kilauea. Note that zoonotic diseases, surgical procedures, African countries and active volcanoes serve as useful class labels that capture the semantics of the associated sets of class instances. Such interest in a wide variety of specific domains highlights the utility of constructing large collections of fine-grained classes. Comprehensive and accurate class-instance information is useful not only in search but also in a variety of other text processing tasks including co-reference resolution (McCarthy and Lehnert, 1995), named entity recognition (Stevenson and Gaizauskas, 2000) and seed-based information extraction (Riloff and Jones, 1999). 1.2 Contributions We study the acquisition of open-domain, labeled classes and their instances from both structured and unstructured textual data sources by combining and ranking individual extractions in a principled way with the Adsorption label-propagation algorithm (Baluja et al., 2008), reviewed in Section 3 below. A collection of labeled classes acquired from text (Van Durme and Pas¸ca, 2008) is extended in two ways: 1. Class label coverage is increased by identifying additional class labels (such as public agencies and gov</context>
</contexts>
<marker>Stevenson, Gaizauskas, 2000</marker>
<rawString>M. Stevenson and R. Gaizauskas. 2000. Using corpusderived name lists for named entity recognition. In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00), Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Szummer</author>
<author>T Jaakkola</author>
</authors>
<title>Partially labeled classification with markov random walks.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 14: Proceedings of the 2002 NIPS Conference.</booktitle>
<contexts>
<context position="13004" citStr="Szummer and Jaakkola, 2002" startWordPosition="2130" endWordPosition="2133">el propagation, consisting of a few nodes annotated with labels and a rich graph structure containing the universe of all labeled and unlabeled nodes. Adsorption proceeds to label all nodes based on the graph structure, ultimately producing a probability distribution over labels for each node. More specifically, Adsorption works on a graph G = (V, E, W) and computes for each node v a label distribution L„ that represents which labels are more or less appropriate for that node. Several interpretations of Adsorption-type algorithms have appeared in various fields (Azran, 2007; Zhu et al., 2003; Szummer and Jaakkola, 2002; Indyk and Matousek, 2004). For details, the reader is referred to (Baluja et al., 2008). We use two interpretations here: Adsorption through Random Walks: Let Gr = (V, Er, Wr) be the edge-reversed version of the original graph G = (V, E, W) where (a, b) E Er iff (b, a) E E; and Wr(a, b) = W (b, a). Now, choose a node of interest q E V . To estimate LQ for q, we perform a random walk on Gr starting from q to generate values for a random label variable L. After reaching a node v during the walk, we have three choices: 1. With probability p�°nt „ , continue the random walk to a neighbor of v. 2</context>
</contexts>
<marker>Szummer, Jaakkola, 2002</marker>
<rawString>M. Szummer and T. Jaakkola. 2002. Partially labeled classification with markov random walks. Advances in Neural Information Processing Systems 14: Proceedings of the 2002 NIPS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Van Durme</author>
<author>M Pas¸ca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>Twenty-Third AAAI Conference on Artificial Intelligence.</booktitle>
<marker>Van Durme, Pas¸ca, 2008</marker>
<rawString>B. Van Durme and M. Pas¸ca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. Twenty-Third AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wang</author>
<author>W Cohen</author>
</authors>
<title>Language-Independent Set Expansion of Named Entities Using the Web. Data Mining,</title>
<date>2007</date>
<booktitle>ICDM 2007. Seventh IEEE International Conference on,</booktitle>
<pages>342--350</pages>
<contexts>
<context position="26175" citStr="Wang and Cohen (2007)" startWordPosition="4408" endWordPosition="4411">ciprocal Rank scores of instance class labels over 38 Wordnet classes (WN-gold). MRR (full) refers to evaluation across the entire gold instance set. MRR (found only) computes MRR only on recalled instances. lower MRR scores compared to Adsorption. This result highlights Adsorption’s ability to effectively combine high-precision, low-recall (A8) extractions with low-precision, high-recall extractions (WT) in a manner that improves both precision and coverage. 6 Related Work Graph based algorithms for minimally supervised information extraction methods have recently been proposed. For example, Wang and Cohen (2007) use a random walk on a graph built from entities and relations extracted from semi-structured text. Our work differs both conceptually, in terms of its focus on open-domain extraction, as well as methodologically, as we incorporate both unstructured and structured text. The re-ranking algorithm of Bellare et al. (2007) also constructs a graph whose nodes are instances and attributes, as opposed to instances and classes here. Adsorption can be seen as a generalization of the method proposed in that paper. 7 Conclusion The field of open-domain information extraction has been driven by the growt</context>
</contexts>
<marker>Wang, Cohen, 2007</marker>
<rawString>R. Wang and W. Cohen. 2007. Language-Independent Set Expansion of Named Entities Using the Web. Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on, pages 342–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>ICML-03, 20th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="12976" citStr="Zhu et al., 2003" startWordPosition="2126" endWordPosition="2129"> framework for label propagation, consisting of a few nodes annotated with labels and a rich graph structure containing the universe of all labeled and unlabeled nodes. Adsorption proceeds to label all nodes based on the graph structure, ultimately producing a probability distribution over labels for each node. More specifically, Adsorption works on a graph G = (V, E, W) and computes for each node v a label distribution L„ that represents which labels are more or less appropriate for that node. Several interpretations of Adsorption-type algorithms have appeared in various fields (Azran, 2007; Zhu et al., 2003; Szummer and Jaakkola, 2002; Indyk and Matousek, 2004). For details, the reader is referred to (Baluja et al., 2008). We use two interpretations here: Adsorption through Random Walks: Let Gr = (V, Er, Wr) be the edge-reversed version of the original graph G = (V, E, W) where (a, b) E Er iff (b, a) E E; and Wr(a, b) = W (b, a). Now, choose a node of interest q E V . To estimate LQ for q, we perform a random walk on Gr starting from q to generate values for a random label variable L. After reaching a node v during the walk, we have three choices: 1. With probability p�°nt „ , continue the rando</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semisupervised learning using gaussian fields and harmonic functions. ICML-03, 20th International Conference on Machine Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>