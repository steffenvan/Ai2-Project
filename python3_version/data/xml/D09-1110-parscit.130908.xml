<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001440">
<title confidence="0.998939">
A Compact Forest for Scalable Inference
over Entailment and Paraphrase Rules
</title>
<author confidence="0.999136">
Roy Bar-Haim§, Jonathan Berant*, Ido Dagan§
</author>
<affiliation confidence="0.99288">
§Computer Science Department, Bar-Ilan University, Ramat Gan 52900, Israel
</affiliation>
<email confidence="0.979898">
{barhair,dagan}@cs.biu.ac.il
</email>
<affiliation confidence="0.506874">
*The Blavatnik School of Computer Science, Tel-Aviv University, Tel-Aviv 69978, Israel
</affiliation>
<email confidence="0.992732">
jonatha6@post.tau.ac.il
</email>
<sectionHeader confidence="0.993743" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871285714286">
A large body of recent research has been
investigating the acquisition and applica-
tion of applied inference knowledge. Such
knowledge may be typically captured as
entailment rules, applied over syntactic
representations. Efficient inference with
such knowledge then becomes a funda-
mental problem. Starting out from a for-
malism for entailment-rule application we
present a novel packed data-structure and
a corresponding algorithm for its scalable
implementation. We proved the validity of
the new algorithm and established its effi-
ciency analytically and empirically.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999696192982456">
Applied semantic inference is concerned with de-
riving target meanings from texts. In the textual
entailment framework, this is reduced to infer-
ring a textual statement (the hypothesis h) from
a source text (t). Traditional formal semantics
approaches perform such inferences over logi-
cal forms derived from the text. By contrast,
most practical NLP applications operate over shal-
lower representations such as parse trees, possibly
supplemented with limited semantic information
about named entities, semantic roles etc.
Most commonly, inference over such represen-
tations is made by applying some kind of transfor-
mations or substitutions to the tree or graph rep-
resenting the text. Such transformations may be
generally viewed as entailment (inference) rules,
which capture semantic knowledge about para-
phrases, lexical relations such as synonyms and
hyponyms, syntactic variations etc. Such knowl-
edge is either composed manually, e.g. WordNet
(Fellbaum, 1998), or learned automatically.
A large body of work has been dedicated to
learning paraphrases and entailment rules, e.g.
(Lin and Pantel, 2001; Shinyama et al., 2002;
Szpektor et al., 2004; Bhagat and Ravichandran,
2008), identifying appropriate contexts for their
application (Pantel et al., 2007) and utilizing them
for inference (de Salvo Braz et al., 2005; Bar-
Haim et al., 2007). Although current avail-
able rule bases are still quite noisy and incom-
plete, the progress made in recent years suggests
that they may become increasingly valuable for
text understanding applications. Overall, applied
knowledge-based inference is a prominent line of
research gaining much interest, with recent exam-
ples including the series of workshops on Knowl-
edge and Reasoning for Answering Questions
(KRAQ)1 and the planned evaluation of knowledge
resources in the forthcoming 5th Recognizing Tex-
tual Entailment challenge (RTE-5)2.
While many applied systems utilize semantic
knowledge via such inference rules, their use is
typically limited, application-specific, and quite
heuristic. Formalizing these practices seems im-
portant for applied semantic inference research,
analogously to the role of well-formalized mod-
els in parsing and machine translation. Bar-Haim
et al. (2007) made a step in this direction by in-
troducing a generic formalism for semantic infer-
ence over parse trees. Their formalism uses entail-
ment rules as a unifying representation for various
types of inference knowledge, allowing unified in-
ference as well. In this formalism, rule application
has a clear, intuitive interpretation as generating a
new sentence parse (a consequent), semantically
entailed by the source sentence. The inferred con-
sequent may be subject to further rule applications
</bodyText>
<footnote confidence="0.999925">
1http://www.irit.fr/recherches/ILPL/kraq09.html
2http://www.nist.gov/tac/2009/RTE/
</footnote>
<page confidence="0.874174">
1056
</page>
<note confidence="0.996702">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1056–1065,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.998909375">
and so on. In their implementation, each conse-
quent was generated explicitly as a separate tree.
Following this line of work, our long-term re-
search goal is to investigate effective utilization
of entailment rules for inference. While the for-
malism of Bar-Haim et al. provides a princi-
pled framework for modeling such inferences,
its implementation using explicit generation of
consequents raises severe efficiency issues, since
the number of consequents may grow exponen-
tially in the number of rule applications. Con-
sider, for example, the sentence “Children are
fond of candies.”, and the following entailment
rules: ‘children→kids’, ‘candies→sweets’, and ‘X
is fond of Y→X likes Y’. The number of derivable
sentences (including the source sentence) would
be 23 as each rule can either be applied or not, in-
dependently. Indeed, we found that this exponen-
tial explosion leads to poor scalability in practice.
Intuitively, we would like that each rule applica-
tion would add just the entailed part (e.g. kids) to a
packed sentence representation. Yet, we still want
the resulting structure to represent a set of entailed
sentences, rather than some mixture of sentence
fragments whose semantics is unclear.
As discussed in section 5, previous work pro-
posed only partial solutions to this problem. In this
paper we present a novel data structure, termed
compact forest, and a corresponding inference al-
gorithm, which efficiently generate and represent
all consequents while preserving the identity of
each individual one (section 3). Our work is
inspired by previous work on packed represen-
tations in other fields, such as parsing, genera-
tion and machine translation (section 5). As
we follow a well-defined formalism, we could
prove that all inference operations of Bar-Haim
et al. are equivalently applied over the compact
forest. We compare inference cost over compact
forests to explicit consequent generation both the-
oretically (section 3.4), illustrating an exponential-
to-linear complexity ratio, and empirically (sec-
tion 4), showing improvement by orders of magni-
tude. These results suggest that our data-structure
and algorithm are both valid and scalable, open-
ing up the possibility to investigate large-scale en-
tailment rule application within a well-formalized
framework.
</bodyText>
<sectionHeader confidence="0.996532" genericHeader="method">
2 Inference Framework
</sectionHeader>
<bodyText confidence="0.99504266">
This section briefly presents a (simplified) descrip-
tion of the tree transformations inference formal-
ism of Bar-Haim et al. (2007). Given a source text,
syntactically parsed, and a set of entailment rules
representing tree transformations, the formalism
defines the set of consequents derivable from the
text using the rules. Each consequent is obtained
through a sequence of rule applications, each gen-
erates an intermediate parse tree, similar to a proof
process in logic.
More specifically, sentences are represented as
dependency trees, where nodes are annotated with
lemma and part-of-speech, and edges are anno-
tated with dependency relation. A rule ‘L → R’
is primarily composed of two templates, termed
left-hand-side (L), and right-hand-side (R). Tem-
plates are dependency subtrees which may con-
tain POS-tagged variables, matching any lemma.
Figure 1(a) shows passive-to-active transforma-
tion rule, and (b) illustrates its application.
A rule application generates a derived tree d
from a source tree s through the following steps:
L matching: First, a match of L in the source
tree s is sought. In our example, the variable V is
matched in the verb see, N1 is matched in Mary
and N2 is matched in John.
R instantiation: Next, a copy of R is generated
and its variables are instantiated according to their
matching node in L. In addition, a rule may spec-
ify alignments, defined as a partial function from
L nodes to R nodes. An alignment indicates that
for each modifier m of the source node that is not
part of the rule structure, the subtree rooted at m
should also be copied as a modifier of the target
node. In addition to defining alignments explic-
itly, each variable in L is implicitly aligned to its
counterpart in R. In our example, the alignment
between the V nodes implies that yesterday (mod-
ifying see) should be copied to the generated sen-
tence, and similarly beautiful (modifying Mary) is
copied for N1.
Derived tree generation: Let r be the instanti-
ated R, along with its descendants copied from L
through alignment, and l be the subtree matched
by L. The formalism has two methods for gen-
erating the derived tree d: substitution and intro-
duction, as specified by the rule type. Substitution
rules specify modification of a subtree of s, leav-
ing the rest of s unchanged. Thus, d is formed by
copying s while replacing l (and the descendants
</bodyText>
<page confidence="0.882497">
1057
</page>
<figure confidence="0.8796478">
V VERB
obj XXX by
s����������� ��������+
be 1
N1 NOUN be VERB by PREP
pcomp−n 1
subj V VERB obj �����������+
N2 NOUN N1 NOUN
L N2 NOUN R
(a) Passive to active transformation (substitution rule)
ROOT
i 1
see VERB
r������������ mod
11o
John NOUN Mary NOUN
mod (
subj
yesterday NOUN
ROOT
i (
see VERB
q���������������������
s�����������mod
be ) by
obj
Mary NOUN
mod (
be VERB by PREP
yesterday NOUN
N
pcomp−n
beautiful ADJ John NOUN beautiful ADJ
Source: Beautiful Mary was seen by John yesterday. Derived: John saw beautiful Mary yesterday.
(b) Application of passive to active transformation
</figure>
<figureCaption confidence="0.999939">
Figure 1: An inference rule application. POS and relation labels are based on Minipar (Lin, 1998)
</figureCaption>
<bodyText confidence="0.999917117647059">
of l’s nodes) with r. This is the case for the pas-
sive rule, as well as for lexical rules such as ‘buy
→ purchase’. By contrast, introduction rules are
used to make inferences from a subtree of s, while
the other parts of s are ignored and do not affect d.
A typical example is inferring a proposition em-
bedded as a relative clause in s. In this case, the
derived tree d is simply taken to be r.
In addition to inference rules, the formalism in-
cludes annotation rules which add features to ex-
isting parse tree nodes. These rules have been used
for identifying contexts that affect the polarity of
predicates.
As shown by Bar-Haim et al., this concise, well
defined formalism allows unified representation of
diverse types of knowledge which are commonly
used for applied semantic inference.
</bodyText>
<sectionHeader confidence="0.943203" genericHeader="method">
3 Efficient Inference over Compact Parse
Forests
</sectionHeader>
<bodyText confidence="0.999990894736842">
As shown in the introduction, explicit genera-
tion of consequents (henceforth explicit inference)
leads to an exponential explosion of the number
of generated trees. In this section we present our
efficient implementation for this formalism. Our
implementation is based on a novel data structure,
termed compact forest (Section 3.1), which com-
pactly represents a large set of trees. Each rule
application generates explicitly only the nodes of
the rule right-hand-side while the rest of the con-
sequent tree is shared with the source, which also
reduces the number of redundant rule applications.
As we shall see, this novel representation is based
primarily on disjunction edges, an extension of
dependency edges that specify a set of alterna-
tive edges of multiple trees. Section 3.2 presents
an efficient algorithm for inference over compact
forests, followed by a discussion of its correctness
and complexity (sections 3.3 and 3.4).
</bodyText>
<subsectionHeader confidence="0.998641">
3.1 The Compact Forest Data Structure
</subsectionHeader>
<bodyText confidence="0.988558619047619">
A compact forest F represents a set of dependency
trees. Figure 2 shows an example of a compact
forest, containing both the source and derived sen-
tences of Figure 1. We first define a more general
data structure for directed graphs, and then narrow
the definition to the case of trees.
A Compact Directed Graph (cDG) is a pair
G = (V, E) where V is a set of nodes and E is a
set of disjunction edges (d-edges). Let D be a
set of dependency relations. A d-edge d is a triple
(Sd, reld, Td), where Sd and Td are disjoint sets
of source nodes and target nodes; reld : Sd → D
is a function specifying the dependency relation
corresponding to each source node. Graphically,
d-edges are shown as point nodes, with incoming
edges from source nodes and outgoing edges to
target nodes. For instance, let d be the bottom-
most d-edge in Figure 3. Then Sd = {of, like},
Td = {candy, sweet}, rel(of) = pcomp-n, and
rel(like) = obj.
A d-edge represents, for each si ∈ Sd, a set of
</bodyText>
<page confidence="0.993088">
1058
</page>
<figureCaption confidence="0.990581333333333">
Figure 2: A compact forest containing both the
source and derived sentences of Figure 1. Parts
of speech are omitted.
</figureCaption>
<bodyText confidence="0.9836625">
alternative directed edges I(si, tj) : tj E Td}, all
of which are labeled with the same relation given
by reld(si). Each of these edges, termed embed-
ded edge (e-edge), would correspond to a differ-
ent graph represented in G. In the previous exam-
ple, the e-edges are like candy, likeobj
</bodyText>
<equation confidence="0.453327">
obj ��→sweet,
pcomp−n pcomp−n
</equation>
<bodyText confidence="0.999053">
of�candy and of�sweet (notice
that the definition implies that all source nodes in
Sd have the same set of alternative target nodes
Td). d is called an outgoing d-edge of a node v if
v E Sd and an incoming d-edge of v if v E Td.
A Compact Directed Acyclic Graph (cDAG) is a
cDG that contains no cycles of e-edges.
A DAG G rooted in a node v E V of a cDAG
G is embedded in G if it can be derived as follows:
we initialize G with v alone; then, we expand v
by choosing exactly one target node t E Td from
each outgoing d-edge d of v, and adding t and the
corresponding e-edge (v, t) to G. This expansion
process is repeated recursively for each new node
added to G.
Each such set of choices results in a different
DAG with v as its only root. In Figure 2, we may
choose to connect the root either to the left see,
resulting in the source passive sentence, or to the
right see, resulting in the derived active sentence.
A Compact Forest F is a cDAG with a single
root r (i.e. r has no incoming d-edges) where all
the embedded DAGs rooted in r are trees. This set
of trees, termed embedded trees, comprise the set
of trees represented by F.
</bodyText>
<figureCaption confidence="0.9552345">
Figure 3 shows another example for a compact
Figure 3: A compact forest representing the 23
sentences derivable from the sentence “children
are fond of candies” using the following three
rules: ‘children→kids’, ‘candies→sweets’, and ‘X
is fond of Y→X likes Y’.
</figureCaption>
<bodyText confidence="0.999311">
forest efficiently representing the 23 sentences re-
sulting from three independently-applied rules.
</bodyText>
<subsectionHeader confidence="0.99943">
3.2 The Inference Process
</subsectionHeader>
<bodyText confidence="0.999856434782609">
Next, we describe the algorithm implementing the
inference process of Section 2 over the compact
forest (henceforth, compact inference), illustrating
it through Figures 1 and 2.
Forest initialization F is initialized with the
set of dependency trees representing the text sen-
tences, with their roots connected under the forest
root as the target nodes of a single d-edge. Depen-
dency edges are transformed trivially to d-edges
with a single source and target. Annotation rules
are applied at this stage to the initial F. The black
part of Figure 2 corresponds to the initial forest.
Rule application comprises the following steps:
L matching: L is matched in F if there exists
an embedded tree t in F such that L is matched
in t, as in Section 2. We denote by l the subtree
of t in which L was matched. As in section 2, the
match in our example is (V, N1, N2)=(see, Mary,
John). Notice that this definition does not allow l
to be scattered over multiple embedded trees.
As the target nodes of a d-edge specify alterna-
tives for the same position in the tree, their parts-
of-speech are expected to be of substitutable types.
</bodyText>
<figure confidence="0.999212827586207">
ROOT
i
see
see
by be mod mod obj
obj
subj
by
be yesterday
Mary
mod
pcomp-n
John
beautiful
ROOT
i
be
pred
fond
like
mod subj
subj
obj
of
child
kid
pcomp-n
candy
sweet
</figure>
<page confidence="0.988754">
1059
</page>
<bodyText confidence="0.998437720930233">
In this paper we further assume that all target
nodes of the same d-edge have the same part-of-
speech3. Consequently, variables that are leaves in
L and may match a certain target node of a d-edge
d are mapped to the whole set of target nodes Td
rather than to a single node. This yields a compact
representation of multiple matches, and prevents
redundant rule applications. For instance, given
a compact representation of ‘{Children/kids} are
fond of {candies/sweeets}’ (cf. Figure 3), the rule
‘X is fond of Y→X likes Y’ will be matched and
applied only once, rather than four times (for each
combination of matching X and Y ).
Derived tree generation: A template r consist-
ing of R while excluding variables that are leaves
of both L and R (termed dual leaf-variables)4 is
generated and inserted into F. In case of a substi-
tution rule (as in our example), r is set as an alter-
native to l by adding r’s root to Td, where d is the
incoming d-edge of l’s root. In case of an intro-
duction rule, it is set as an alternative to the other
trees in the forest by adding r’s root to the target
node set of the forest root’s outgoing d-edge. In
our example, r is the gray node (still labeled with
the variable V ) , and it becomes an additional tar-
get node of the d-edge entering the original (left)
see.
Variable instantiation: Each variable in r (i.e.
a non-dual leaf) is instantiated according to its
match in L (as in Section 2), e.g. V is instantiated
with see. As specified above, if the variable is a
leaf in L is not a dual leaf then it is matched in a set
of nodes, and hence each of them should be instan-
tiated in r. This is decomposed into a sequence
of simpler operations: first, r is instantiated with a
representative from the set, and then we apply (ad-
hoc) lexical substitution rules for creating a new
node for each other node in the set5.
Alignment sharing: Modifiers of aligned nodes
are shared (rather than copied) as follows. Given
a node nL in l aligned to a node nR in r, and an
outgoing d-edge d of nL which is not part of l, we
share d between nL and nR by adding nR to Sd
</bodyText>
<footnote confidence="0.9963498">
3This is the case in our current implementation, which is
based on the coarse tag-set of Minipar (Lin, 1998).
4With the following exceptions: variables that are the
only node in R (and hence are both the root and a leaf), and
variables with additional alignments (other than the implicit
alignment between their occurrences in L and R) are not con-
sidered dual-leaves.
5Notice that these nodes, in addition to the usual align-
ment with their source nodes in l, share the same daughters
in r.
</footnote>
<bodyText confidence="0.999337851851852">
and setting reld(nR) = reld(nL). This is illus-
trated by the sharing of yesterday in Figure 2. We
also copy annotation features from nL to nR.
We note at this point that the instantiation of
variables that are not dual leaves (e.g. V in our
example) cannot be shared because they typically
have different modifiers at the two sides of the
rule. Yet, their modifiers which are not part of
the rule are shared through the alignment opera-
tion (recall that common variables are always con-
sidered aligned). Dual leaf variables, on the other
hand, might be shared, as described next, since the
rule doesn’t specify any modifiers for them.
Dual leaf variable sharing: This final step is
performed analogously to alignment sharing. Sup-
pose that a dual leaf variable X is matched in a
node v in l whose incoming d-edge is d. Then
we simply add the parent p of X in r to Sd and
set reld(p) to the relation between p and X (in
R). Since v itself is shared, its modifiers become
shared as well, implicitly implementing the align-
ment operation. The subtrees beautiful Mary and
John are shared this way for variables N1 and N2.
Applying the rule in our example added only
a single node and linked it to four d-edges, com-
pared to duplicating the whole tree in explicit in-
ference.
</bodyText>
<subsectionHeader confidence="0.995749">
3.3 Correctness
</subsectionHeader>
<bodyText confidence="0.948577954545455">
In this section we present two theorems, which
prove that the inference algorithm is a valid imple-
mentation of the inference formalism of Section 2.
Due to space limitations, the proofs themselves
are omitted, and instead we outline their general
scheme.
We first argue that performing any sequence of
rule applications over the set of initial trees results
in a compact forest:
Theorem 1: The compact inference process
generates a compact forest.
Proof scheme: We prove by induction on the
number of rule applications. Initialization gen-
erates a single-rooted cDAG, whose embedded
DAGs are all trees, as required. We then prove that
if applying a rule on a compact forest creates a cy-
cle or an embedded DAG that is not a tree, then
such a cycle or a non-tree DAG already existed
prior to rule application, in contradiction with the
inductive assumption. A crucial observation for
this proof is that for any path from a node u to a
node v that passes through r, where u and v are
</bodyText>
<page confidence="0.937284">
1060
</page>
<bodyText confidence="0.999715833333333">
outside r, there is also an analogous path from u
to v that passes through l instead, QED.
Next, we argue that the inference process over a
compact forest is complete and sound, i.e., it gen-
erates the set of consequents derivable from a text
according to the inference formalism.
</bodyText>
<construct confidence="0.9954324">
Theorem 2: Given a rule base R and a set of
initial trees T, a tree t is embedded in a compact
forest derivable from T by the compact inference
process ⇔ t is a consequent of T according to the
inference formalism.
</construct>
<bodyText confidence="0.999284363636364">
Proof scheme: We first show completeness by
induction on the number of explicit rule applica-
tions. Let t,,,+1 be a tree derived from a tree t,,,
using the rule r,,, according to the inference for-
malism. The inductive assumption asserts that t,,,
is embedded in some derivable compact forest F.
It is easy to verify that applying r,,, to F will yield
a compact forest F′ in which t,,,+1 is embedded.
Next, we show soundness by induction on the
number of rule applications over the compact for-
est. Let t,,,+1 be a tree represented in some derived
compact forest F,,,+1. F,,,+1 was derived from the
compact forest F,,,, using the rule r,,,. It can be
shown that F,,, represents a tree t,,,, such that ap-
plying r,,, on t,,, will yield t,,,+1 according to the
formalism. The inductive assumption asserts that
t,,, is a consequent in the inference formalism and
therefore t,,,+1 is a consequent as well, QED.
These two theorems guarantee that the compact
inference process is valid - i.e., it yields a compact
forest that represents the set of consequents deriv-
able from a given text by a given rule set.
</bodyText>
<subsectionHeader confidence="0.938033">
3.4 Complexity
</subsectionHeader>
<bodyText confidence="0.9995744375">
In this section we explain why compact inference
exponentially reduces the time and space com-
plexity in typical scenarios.
We consider a set of rule matches in a tree T
independent if their matched left-hand-sides (ex-
cluding dual-leaf variables) do not overlap in T,
and their application over T can be chained in any
order. For example, the three rule matches pre-
sented in Figure 3 are independent.
Let us consider explicit inference first. Assume
we start with a single tree T with k independent
rules matched. Applying k rules will yield 2k
trees, since any subset of the rules might be ap-
plied to T. Therefore, the time and space com-
plexity of applying k independent rule matches is
Q(2k). Applying more rules on the newly derived
</bodyText>
<table confidence="0.99897">
Compact Explicit Ratio
Time (msec) 61 24,184 396
Rule applications 12 123 10
Node count 69 5,901 86
Edge endpoints 141 11,552 82
</table>
<tableCaption confidence="0.983360666666667">
Table 1: Compact vs. explicit inference, us-
ing generic rules. Results are averaged per text-
hypothesis pair.
</tableCaption>
<bodyText confidence="0.991548739130435">
consequents behaves in a similar manner.
Next, we examine compact inference. Apply-
ing a rule using compact inference adds the right-
hand-side of the rule and shares with it existing
d-edges. Since that the size of the right-hand-side
and the number of outgoing d-edges per node are
practically bounded by low constants, applying k
rules on a tree T yields a linear increase in the size
of the forest. Thus, the resulting size is O(|T |+k),
as we can see from Figure 3.
The time complexity of rule application is com-
posed of matching the rule in the forest and apply-
ing the matched rule. Applying a matched rule is
linear in its size. Matching a rule of size r in a
forest F takes O(|F|&apos;) time even when perform-
ing an exhaustive search for matches in the forest.
Since r tends to be quite small and can be bounded
by a low constant, this already gives polynomial
time complexity. In practice, indexing the forest
nodes, as well as the typical low connectivity of
the forest, result in a very fast matching procedure,
as illustrated in the empirical evaluation, described
next.
</bodyText>
<sectionHeader confidence="0.996289" genericHeader="method">
4 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.999749071428571">
This section reports empirical evaluation of the ef-
ficiency of compact inference, tested in the recog-
nizing textual entailment setting using the RTE-3
and RTE-4 datasets (Giampiccolo et al., 2007; Gi-
ampiccolo et al., 2009). These datasets consist of
(text, hypothesis) pairs, which need to be classi-
fied as entailing/non entailing. Our first experi-
ment shows, using a small rule set, that compact
inference outperforms explicit inference by orders
of magnitude (Section 4.1). The second experi-
ment shows that compact inference scales well to
a full-blown RTE setting with several large-scale
rule bases, where up to hundreds of rules are ap-
plied for a text (Section 4.2).
</bodyText>
<page confidence="0.973073">
1061
</page>
<subsectionHeader confidence="0.973804">
4.1 Compact vs. Explicit Inference
</subsectionHeader>
<bodyText confidence="0.999978277777778">
To compare explicit and compact inference we
randomly sampled 100 pairs from the RTE-3 de-
velopment set, and parsed the text in each pair
using Minipar (Lin, 1998). We used a set of
manually-composed entailment rules for inference
over generic linguistic phenomena such as pas-
sive, conjunction, relative clause, apposition, pos-
sessives, and determiners, which contains a few
dozens of rules. To make a fair comparison, we
aimed to make the explicit inference implementa-
tion reasonably efficient, e.g. by preventing gen-
eration of the same tree by different permutations
of the same rule applications. Both configurations
perform rule application iteratively, until no new
matches are found. In each iteration we first find
all rule matches and then apply all matching rules.
We compare run time, number of rule applications,
and the overall generated size of nodes and edges,
where edge size is represented by the sum of its
endpoints.
The results are summarized in Table 1. As ex-
pected, the results show that compact inference is
by orders of magnitude more efficient than explicit
inference. To avoid memory overflow, inference
was terminated after reaching 100,000 nodes. 3
out of the 100 pairs reached that limit with explicit
inference, while the maximal node count for com-
pact inference was only 268. The number of rule
applications is reduced thanks to the sharing of
common subtrees in the compact forest, by which
a single rule application operates simultaneously
over a large number of embedded trees. The re-
sults suggest that scaling to larger rule bases and
longer inference chains would be feasible for com-
pact inference, but prohibitive for explicit infer-
ence.
</bodyText>
<subsectionHeader confidence="0.999387">
4.2 Application to an RTE System
</subsectionHeader>
<bodyText confidence="0.999779126984127">
Experimental setting The goal of the second
experiment was to assess that compact inference
scales well for broad entailment rule bases. In
this experiment we used the Bar-Ilan RTE system
(Bar-Haim et al., 2009). The system operates in
two primary stages: Inference, in which entail-
ment rules are applied to the initial compact forest
T, aiming to bring it closer to the hypothesis R,
and Classification, in which a set of features is ex-
tracted from the resulting T and from R and fed
into an SVM classifier, which determines entail-
ment.
The classification setting and its features are
quite typical for the RTE literature. They include
lexical and structural measures for the coverage of
R by T, where high coverage is assumed to cor-
relate with entailment, as well as features aiming
to detect inconsistencies between T and R such
as incompatible arguments for the same predicate
or incompatible verb polarity (see below). For a
complete feature description, see (Bar-Haim et al.,
2009).
Rule Bases In addition to the generic rules de-
scribed in Section 4.1, the following large-scale
sources for entailment rules were used: Wikipeda:
We used the lexical rulebase of Shnarch et al.
(2009), who extracted rules such as ‘Janis Joplin
—* singer’ from Wikipedia based on both its meta-
data (e.g. links and redirects) and text defini-
tions, using patterns such as ‘X is a Y’. Word-
Net: We extracted from WordNet (Fellbaum,
1998) lexical rules based on synonyms, hyper-
nyms and derivation relations. DIRT: The DIRT
algorithm (Lin and Pantel, 2001) learns from a
corpus entailment rules between binary predicates,
e.g. ‘X is fond of Y—*X likes Y’. We used the
version described in (Szpektor and Dagan, 2007),
which learns canonical rule forms. Argument-
Mapped WordNet (AmWN): A resource for entail-
ment rules between verbal and nominal predicates
(Szpektor and Dagan, 2009), including their argu-
ment mapping, based on WordNet and NomLex-
plus (Meyers et al., 2004), verified statistically
through intersection with the unary-DIRT algo-
rithm (Szpektor and Dagan, 2008). In total, these
rule bases represent millions of rules. Polarity An-
notation Rules: We compiled a small set of anno-
tation rules for marking the polarity of predicates
as negative or unknown due to verbal negation,
modal verbs, conditionals etc. (Bar-Haim et al.,
2009).
Search In this work we focus on efficient rep-
resentation of the search space, leaving for future
work the complementary problem of devising ef-
fective search heuristics over our representation.
In the current experiment we implemented a sim-
ple search strategy, in the spirit of (de Salvo Braz
et al., 2005): first, we applied three exhaustive iter-
ations of generic rules. Since these rules have low
fan-out (few possible right-hand-sides for a given
left-hand-side) it is affordable to apply and chain
them more freely. We then perform a single itera-
tion of all other lexical and lexical-syntactic rules,
</bodyText>
<page confidence="0.984239">
1062
</page>
<bodyText confidence="0.997834152173913">
applying them only if their L part was matched in
F and their R part was matched in H.
The system was trained over the RTE-3 devel-
opment set, and tested on both RTE-3 test set and
RTE-4 (which includes only a test set).
Results Table 2 provides statistics on rule appli-
cation using all rule bases, over the RTE-3 devel-
opment set and the RTE-4 dataset6. Overall, the
primary result is that the compact forest indeed ac-
commodates well extensive rule application from
large-scale rule bases. The resulting forest size is
kept small, even in the maximal cases which were
causing memory overflow for explicit inference.
The accuracies obtained in this experiment and
the overall contribution of rule-based inference are
shown in Table 3. The results on RTE-3 are quite
competitive: compared to our 66.4%, only 3 teams
out of the 26 who participated in RTE-3 scored
higher than 67%, and three more systems scored
between 66% and 67%. The results for RTE4 rank
9-10 out of 26, with only 6 teams scoring higher by
more than 1%. Overall, these results validate that
the setting of our experiment represents a state-of-
the-art system.
Inference over the rule bases utilized in our
experiment improved the accuracy on both test
sets. The contribution was more prominent for
the RTE-4 dataset. These results illustrate a typ-
ical contribution of current knowledge sources for
current RTE systems. This contribution is likely
to increase with current and near future research,
on topics such as extending and improving knowl-
edge resources, applying them only in seman-
tically suitable contexts, improved classification
features and broader search strategies. As for our
current experiment, we may conclude that the goal
of assessing the compact forest scalability in a
state-of-the-art setting was achieved 7.
Finally, Tables 4 and 5 illustrate the usage and
contribution of individual rule bases. Table 4
shows the distribution of rule applications over the
various rule bases. Table 5 presents ablation study
showing the marginal accuracy gain for each rule
base. These results show that each of the rule
bases is applicable for a large portion of the pairs,
and contributes to the overall accuracy.
</bodyText>
<footnote confidence="0.985312166666667">
6Running time is omitted since most of it was dedicated
to rule fetching, which was rather slow for our available im-
plementation of some resources. The elapsed time was a few
seconds per (t, h) pair.
7We note that common RTE research issues, such as im-
proving accuracy, fall out of the scope of the current paper.
</footnote>
<table confidence="0.9955806">
RTE3-Dev RTE4
Avg. Max. Avg. Max.
Rule applications 14 275 15 110
Node count 71 606 80 357
Edge endpoints 155 1,741 173 1,062
</table>
<tableCaption confidence="0.956318">
Table 2: Application of compact inference to the
RTE-3 Dev. and RTE-4 datasets, using all rule
types.
</tableCaption>
<table confidence="0.9994355">
Accuracy
Test set No inference Inference D
RTE3 64.6% 66.4% 1.8%
RTE4 57.5% 60.6% *3.1%
</table>
<tableCaption confidence="0.997672">
Table 3: Inference contribution to RTE perfor-
</tableCaption>
<bodyText confidence="0.8734915">
mance. The system was trained on the RTE-3 de-
velopment set. * indicates statistically significant
difference (at level p &lt; 0.02, using McNemar’s
test).
</bodyText>
<table confidence="0.99940925">
Rule base RTE3 -Dev RTE4 App
Rules App Rules
WordNet 0.6 1.2 0.6 1.1
AmWN 0.3 0.4 0.3 0.4
Wikipedia 0.6 1.7 0.6 1.3
DIRT 0.5 0.7 0.5 1.0
Generic 4.7 10.4 5.4 11.5
Polarity 0.2 0.2 0.2 0.2
</table>
<tableCaption confidence="0.971539">
Table 4: Average number of rule applications per
</tableCaption>
<bodyText confidence="0.848621">
(t, h) pair, for each rule base. App counts each rule
application, while Rules ignores multiple matches
of the same rule in the same iteration.
</bodyText>
<table confidence="0.999717285714286">
Rule base DAccuracy (RTE4)
WordNet 0.8%
AmWN 0.7%
Wikipedia 1.0 %
DIRT 0.9 %
Generic 0.4 %
Polarity 0.9 %
</table>
<tableCaption confidence="0.851947333333333">
Table 5: Contribution of various rule bases. Re-
sults show accuracy loss on RTE-4, obtained for
removing each rule base (ablation tests).
</tableCaption>
<sectionHeader confidence="0.999919" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999484888888889">
This section discusses related work on applying
knowledge-based transformations within RTE sys-
tems, as well as on using packed representations in
other NLP tasks.
RTE Systems Previous RTE systems usually re-
stricted both the type of allowed transformations
and the search space. Systems based on lexical
(word-based or phrase-based) matching of h in t
typically applied only lexical rules (without vari-
</bodyText>
<page confidence="0.874034">
1063
</page>
<bodyText confidence="0.999943212121212">
ables), where both sides of the rule are matched
directly in t and h (Haghighi et al., 2005; Mac-
Cartney et al., 2008). The inference formalism
we use is more expressive, allowing also syntac-
tic and lexical-syntactic transformations as well as
rule chaining.
Hickl (2008) derived from a given (t, h) pair
a small set of discourse commitments, which are
quite similar to the kind of consequents we derive
by our syntactic and lexical-syntactic rules. The
commitments were generated by several different
tools and techniques, compared to our generic uni-
fied inference process, and commitment genera-
tion efficiency was not discussed.
Braz et al. (2005) presented a semantic infer-
ence framework which “augments” the text repre-
sentation with only the right-hand-side of an ap-
plied rule, and in this respect is similar to ours.
However, in their work, both rule application and
the semantics of the resulting “augmented” struc-
ture were not fully specified. In particular, the dis-
tinction between individual consequents was lost
in the augmented graph. By contrast, our com-
pact inference is fully formalized and is provably
equivalent to an expressive, well-defined formal-
ism operating over individual trees, and each in-
ferred consequent can be recovered from the com-
pact forest.
Packed representations Packed representations
in various NLP tasks share common principles,
which also underly our compact forest: factor-
ing out common substructures and representing
choice as local disjunctions. Applying this gen-
eral scheme to individual problems typically re-
quires specific representations and algorithms, de-
pending on the type of alternatives that should be
represented and the specified operations for creat-
ing them. In our work, alternatives are created by
rule application, where a newly derived subtree is
set as an alternative to existing subtrees. Alterna-
tives are specified locally using d-edges.
Packed chart representations for parse forests
were introduced in classical parsing algorithms
such as CYK and Earley (Jurafsky and Martin,
2008), and have been extended in later work
for various purposes (Maxwell III and Kaplan,
1991; Kay, 1996). Alternatives in the parse chart
stem from syntactic ambiguities, and are speci-
fied locally as the possible decompositions of each
phrase into its sub-phrases.
Packed representations have been utilized also
in transfer-based machine translation. Emele and
Dorna (1998) translated packed source language
representation to packed target language represen-
tation while avoiding unnecessary unpacking dur-
ing transfer. Unlike our rule application, in their
work transfer rules preserve ambiguity stemming
from source language, rather than generating new
alternatives. Mi et al.(2008) applied statistical ma-
chine translation to a source language parse forest,
rather than to the 1-best parse. Their transfer rules
are tree-to-string, contrary to our tree-to-tree rules,
and chaining is not attempted (rules are applied in
a single top-down pass over the source forest), and
thus their representation and algorithms are quite
different from ours.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999952">
This work addresses the efficiency of entailment
and paraphrase rule application. We presented a
novel compact data structure and a rule application
algorithm for it, which are provably a valid imple-
mentation of a generic inference formalism. We il-
lustrated inference efficiency both analytically and
empirically. Beyond entailment inference, we sug-
gest that the compact forest would also be use-
ful in generation tasks (e.g. paraphrasing). Our
efficient representation of the consequent search
space opens the way to future investigation of the
benefit of larger-scale rule chaining, and to the de-
velopment of efficient search strategies required to
support such inferences.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999624">
This work was partially supported by the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886, the
FBK-irst/Bar-Ilan University collaboration and
the Israel Science Foundation grant 1112/08. The
second author is grateful to the Azrieli Foundation
for the award of an Azrieli Fellowship. The au-
thors would like to thank Yonatan Aumann, Marco
Pennacchiotti and Marc Dymetman for their valu-
able feedback on this work.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9992472">
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings ofAAAI.
Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo
Greental, Shachar Mirkin, Eyal Shnarch, and Idan
</reference>
<page confidence="0.586887">
1064
</page>
<reference confidence="0.9998686">
Szpektor. 2009. Efficient semantic deduction and
approximate matching over compact parse forests.
In Proceedings of the First Text Analysis Conference
(TAC 2008).
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural
language. In Proceedings of AAAI.
Martin C. Emele and Michael Dorna. 1998. Ambi-
guity preserving machine translation using packed
representations. In Proceedings of Coling-ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and
Communication. MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nizing Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Bill Dolan. 2009. The
Fourth PASCAL Recognizing Textual Entailment
Challenge. In Proceedings of the First Text Analy-
sis Conference (TAC 2008).
Aria D. Haghighi, Andrew Y. Ng, and Christopher D.
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of EMNLP.
Andrew Hickl. 2008. Using discourse commitments
to recognize textual entailment. In Proceedings of
COLING.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, second
edition.
Martin Kay. 1996. Chart generation. In Proceedings
ofACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343–360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
EMNLP.
John T. Maxwell III and Ronald M. Kaplan. 1991.
A method for disjunctive constraint satisfaction. In
Masaru Tomita, editor, Current Issues in Parsing
Technology. Kluwer Academic Publishers.
A. Meyers, R. Reeves, Catherine Macleod, Rachel
Szekeley, Veronkia Zielinska, and Brian Young.
2004. The cross-breeding of dictionaries. In Pro-
ceedings of LREC.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of NAACL-HLT.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of Hu-
man Language Technology Conference (HLT 2002),
San Diego, USA.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP.
Idan Szpektor and Ido Dagan. 2007. Learning canon-
ical forms of entailment rules. In Proceedings of
RANLP.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2009. Augmenting
WordNet-based inference with argument mapping.
In Proceedings of ACL-IJCNLP Workshop on Ap-
plied Textual Inference (TextInfer).
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
ture Coppola. 2004. Scaling web based acquisition
of entailment patterns. In Proceedings of EMNLP.
</reference>
<page confidence="0.974139">
1065
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.457733">
<title confidence="0.999014">A Compact Forest for Scalable over Entailment and Paraphrase Rules</title>
<author confidence="0.999902">Jonathan Ido</author>
<affiliation confidence="0.546114">Science Department, Bar-Ilan University, Ramat Gan 52900,</affiliation>
<address confidence="0.638478">Blavatnik School of Computer Science, Tel-Aviv University, Tel-Aviv 69978,</address>
<email confidence="0.985915">jonatha6@post.tau.ac.il</email>
<abstract confidence="0.995537066666667">A large body of recent research has been investigating the acquisition and application of applied inference knowledge. Such knowledge may be typically captured as applied over syntactic representations. Efficient inference with such knowledge then becomes a fundamental problem. Starting out from a formalism for entailment-rule application we present a novel packed data-structure and a corresponding algorithm for its scalable implementation. We proved the validity of the new algorithm and established its efficiency analytically and empirically.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
</authors>
<title>Ido Dagan, Iddo Greental, and Eyal Shnarch.</title>
<date>2007</date>
<booktitle>In Proceedings ofAAAI.</booktitle>
<marker>Bar-Haim, 2007</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal Shnarch. 2007. Semantic inference at the lexicalsyntactic level. In Proceedings ofAAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Jonathan Berant</author>
</authors>
<title>Ido Dagan, Iddo Greental, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.</title>
<date>2009</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<marker>Bar-Haim, Berant, 2009</marker>
<rawString>Roy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Greental, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor. 2009. Efficient semantic deduction and approximate matching over compact parse forests. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Large scale acquisition of paraphrases for learning surface patterns.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="2105" citStr="Bhagat and Ravichandran, 2008" startWordPosition="294" endWordPosition="297">er such representations is made by applying some kind of transformations or substitutions to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions (KRAQ)1 and the planned evaluatio</context>
</contexts>
<marker>Bhagat, Ravichandran, 2008</marker>
<rawString>Rahul Bhagat and Deepak Ravichandran. 2008. Large scale acquisition of paraphrases for learning surface patterns. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodrigo de Salvo Braz</author>
<author>Roxana Girju</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
</authors>
<title>An inference model for semantic entailment in natural language.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="2244" citStr="Braz et al., 2005" startWordPosition="315" endWordPosition="318">ns may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions (KRAQ)1 and the planned evaluation of knowledge resources in the forthcoming 5th Recognizing Textual Entailment challenge (RTE-5)2. While many applied systems utilize seman</context>
<context position="28843" citStr="Braz et al., 2005" startWordPosition="4851" endWordPosition="4854">-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Search In this work we focus on efficient representation of the search space, leaving for future work the complementary problem of devising effective search heuristics over our representation. In the current experiment we implemented a simple search strategy, in the spirit of (de Salvo Braz et al., 2005): first, we applied three exhaustive iterations of generic rules. Since these rules have low fan-out (few possible right-hand-sides for a given left-hand-side) it is affordable to apply and chain them more freely. We then perform a single iteration of all other lexical and lexical-syntactic rules, 1062 applying them only if their L part was matched in F and their R part was matched in H. The system was trained over the RTE-3 development set, and tested on both RTE-3 test set and RTE-4 (which includes only a test set). Results Table 2 provides statistics on rule application using all rule bases</context>
<context position="33853" citStr="Braz et al. (2005)" startWordPosition="5687" endWordPosition="5690">e rule are matched directly in t and h (Haghighi et al., 2005; MacCartney et al., 2008). The inference formalism we use is more expressive, allowing also syntactic and lexical-syntactic transformations as well as rule chaining. Hickl (2008) derived from a given (t, h) pair a small set of discourse commitments, which are quite similar to the kind of consequents we derive by our syntactic and lexical-syntactic rules. The commitments were generated by several different tools and techniques, compared to our generic unified inference process, and commitment generation efficiency was not discussed. Braz et al. (2005) presented a semantic inference framework which “augments” the text representation with only the right-hand-side of an applied rule, and in this respect is similar to ours. However, in their work, both rule application and the semantics of the resulting “augmented” structure were not fully specified. In particular, the distinction between individual consequents was lost in the augmented graph. By contrast, our compact inference is fully formalized and is provably equivalent to an expressive, well-defined formalism operating over individual trees, and each inferred consequent can be recovered f</context>
</contexts>
<marker>Braz, Girju, Punyakanok, Roth, Sammons, 2005</marker>
<rawString>Rodrigo de Salvo Braz, Roxana Girju, Vasin Punyakanok, Dan Roth, and Mark Sammons. 2005. An inference model for semantic entailment in natural language. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin C Emele</author>
<author>Michael Dorna</author>
</authors>
<title>Ambiguity preserving machine translation using packed representations.</title>
<date>1998</date>
<booktitle>In Proceedings of Coling-ACL.</booktitle>
<contexts>
<context position="35616" citStr="Emele and Dorna (1998)" startWordPosition="5953" endWordPosition="5956">d subtree is set as an alternative to existing subtrees. Alternatives are specified locally using d-edges. Packed chart representations for parse forests were introduced in classical parsing algorithms such as CYK and Earley (Jurafsky and Martin, 2008), and have been extended in later work for various purposes (Maxwell III and Kaplan, 1991; Kay, 1996). Alternatives in the parse chart stem from syntactic ambiguities, and are specified locally as the possible decompositions of each phrase into its sub-phrases. Packed representations have been utilized also in transfer-based machine translation. Emele and Dorna (1998) translated packed source language representation to packed target language representation while avoiding unnecessary unpacking during transfer. Unlike our rule application, in their work transfer rules preserve ambiguity stemming from source language, rather than generating new alternatives. Mi et al.(2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. Their transfer rules are tree-to-string, contrary to our tree-to-tree rules, and chaining is not attempted (rules are applied in a single top-down pass over the source forest), and t</context>
</contexts>
<marker>Emele, Dorna, 1998</marker>
<rawString>Martin C. Emele and Michael Dorna. 1998. Ambiguity preserving machine translation using packed representations. In Proceedings of Coling-ACL.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database. Language, Speech and Communication.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="35616" citStr="(1998)" startWordPosition="5956" endWordPosition="5956"> as an alternative to existing subtrees. Alternatives are specified locally using d-edges. Packed chart representations for parse forests were introduced in classical parsing algorithms such as CYK and Earley (Jurafsky and Martin, 2008), and have been extended in later work for various purposes (Maxwell III and Kaplan, 1991; Kay, 1996). Alternatives in the parse chart stem from syntactic ambiguities, and are specified locally as the possible decompositions of each phrase into its sub-phrases. Packed representations have been utilized also in transfer-based machine translation. Emele and Dorna (1998) translated packed source language representation to packed target language representation while avoiding unnecessary unpacking during transfer. Unlike our rule application, in their work transfer rules preserve ambiguity stemming from source language, rather than generating new alternatives. Mi et al.(2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. Their transfer rules are tree-to-string, contrary to our tree-to-tree rules, and chaining is not attempted (rules are applied in a single top-down pass over the source forest), and t</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. Language, Speech and Communication. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The Third PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<contexts>
<context position="23993" citStr="Giampiccolo et al., 2007" startWordPosition="4062" endWordPosition="4065">t F takes O(|F|&apos;) time even when performing an exhaustive search for matches in the forest. Since r tends to be quite small and can be bounded by a low constant, this already gives polynomial time complexity. In practice, indexing the forest nodes, as well as the typical low connectivity of the forest, result in a very fast matching procedure, as illustrated in the empirical evaluation, described next. 4 Empirical Evaluation This section reports empirical evaluation of the efficiency of compact inference, tested in the recognizing textual entailment setting using the RTE-3 and RTE-4 datasets (Giampiccolo et al., 2007; Giampiccolo et al., 2009). These datasets consist of (text, hypothesis) pairs, which need to be classified as entailing/non entailing. Our first experiment shows, using a small rule set, that compact inference outperforms explicit inference by orders of magnitude (Section 4.1). The second experiment shows that compact inference scales well to a full-blown RTE setting with several large-scale rule bases, where up to hundreds of rules are applied for a text (Section 4.2). 1061 4.1 Compact vs. Explicit Inference To compare explicit and compact inference we randomly sampled 100 pairs from the RT</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The Third PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Hoa Trang Dang</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The Fourth PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2009</date>
<booktitle>In Proceedings of the First Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="24020" citStr="Giampiccolo et al., 2009" startWordPosition="4066" endWordPosition="4070">n when performing an exhaustive search for matches in the forest. Since r tends to be quite small and can be bounded by a low constant, this already gives polynomial time complexity. In practice, indexing the forest nodes, as well as the typical low connectivity of the forest, result in a very fast matching procedure, as illustrated in the empirical evaluation, described next. 4 Empirical Evaluation This section reports empirical evaluation of the efficiency of compact inference, tested in the recognizing textual entailment setting using the RTE-3 and RTE-4 datasets (Giampiccolo et al., 2007; Giampiccolo et al., 2009). These datasets consist of (text, hypothesis) pairs, which need to be classified as entailing/non entailing. Our first experiment shows, using a small rule set, that compact inference outperforms explicit inference by orders of magnitude (Section 4.1). The second experiment shows that compact inference scales well to a full-blown RTE setting with several large-scale rule bases, where up to hundreds of rules are applied for a text (Section 4.2). 1061 4.1 Compact vs. Explicit Inference To compare explicit and compact inference we randomly sampled 100 pairs from the RTE-3 development set, and pa</context>
</contexts>
<marker>Giampiccolo, Dang, Magnini, Dagan, Dolan, 2009</marker>
<rawString>Danilo Giampiccolo, Hoa Trang Dang, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2009. The Fourth PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the First Text Analysis Conference (TAC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria D Haghighi</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="33296" citStr="Haghighi et al., 2005" startWordPosition="5600" endWordPosition="5603">ious rule bases. Results show accuracy loss on RTE-4, obtained for removing each rule base (ablation tests). 5 Related Work This section discusses related work on applying knowledge-based transformations within RTE systems, as well as on using packed representations in other NLP tasks. RTE Systems Previous RTE systems usually restricted both the type of allowed transformations and the search space. Systems based on lexical (word-based or phrase-based) matching of h in t typically applied only lexical rules (without vari1063 ables), where both sides of the rule are matched directly in t and h (Haghighi et al., 2005; MacCartney et al., 2008). The inference formalism we use is more expressive, allowing also syntactic and lexical-syntactic transformations as well as rule chaining. Hickl (2008) derived from a given (t, h) pair a small set of discourse commitments, which are quite similar to the kind of consequents we derive by our syntactic and lexical-syntactic rules. The commitments were generated by several different tools and techniques, compared to our generic unified inference process, and commitment generation efficiency was not discussed. Braz et al. (2005) presented a semantic inference framework w</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria D. Haghighi, Andrew Y. Ng, and Christopher D. Manning. 2005. Robust textual inference via graph matching. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
</authors>
<title>Using discourse commitments to recognize textual entailment.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="33475" citStr="Hickl (2008)" startWordPosition="5629" endWordPosition="5630">transformations within RTE systems, as well as on using packed representations in other NLP tasks. RTE Systems Previous RTE systems usually restricted both the type of allowed transformations and the search space. Systems based on lexical (word-based or phrase-based) matching of h in t typically applied only lexical rules (without vari1063 ables), where both sides of the rule are matched directly in t and h (Haghighi et al., 2005; MacCartney et al., 2008). The inference formalism we use is more expressive, allowing also syntactic and lexical-syntactic transformations as well as rule chaining. Hickl (2008) derived from a given (t, h) pair a small set of discourse commitments, which are quite similar to the kind of consequents we derive by our syntactic and lexical-syntactic rules. The commitments were generated by several different tools and techniques, compared to our generic unified inference process, and commitment generation efficiency was not discussed. Braz et al. (2005) presented a semantic inference framework which “augments” the text representation with only the right-hand-side of an applied rule, and in this respect is similar to ours. However, in their work, both rule application and</context>
</contexts>
<marker>Hickl, 2008</marker>
<rawString>Andrew Hickl. 2008. Using discourse commitments to recognize textual entailment. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition.</title>
<date>2008</date>
<publisher>Prentice Hall,</publisher>
<note>second edition.</note>
<contexts>
<context position="35246" citStr="Jurafsky and Martin, 2008" startWordPosition="5898" endWordPosition="5901">t common substructures and representing choice as local disjunctions. Applying this general scheme to individual problems typically requires specific representations and algorithms, depending on the type of alternatives that should be represented and the specified operations for creating them. In our work, alternatives are created by rule application, where a newly derived subtree is set as an alternative to existing subtrees. Alternatives are specified locally using d-edges. Packed chart representations for parse forests were introduced in classical parsing algorithms such as CYK and Earley (Jurafsky and Martin, 2008), and have been extended in later work for various purposes (Maxwell III and Kaplan, 1991; Kay, 1996). Alternatives in the parse chart stem from syntactic ambiguities, and are specified locally as the possible decompositions of each phrase into its sub-phrases. Packed representations have been utilized also in transfer-based machine translation. Emele and Dorna (1998) translated packed source language representation to packed target language representation while avoiding unnecessary unpacking during transfer. Unlike our rule application, in their work transfer rules preserve ambiguity stemming</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition. Prentice Hall, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Chart generation.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="35347" citStr="Kay, 1996" startWordPosition="5917" endWordPosition="5918">lems typically requires specific representations and algorithms, depending on the type of alternatives that should be represented and the specified operations for creating them. In our work, alternatives are created by rule application, where a newly derived subtree is set as an alternative to existing subtrees. Alternatives are specified locally using d-edges. Packed chart representations for parse forests were introduced in classical parsing algorithms such as CYK and Earley (Jurafsky and Martin, 2008), and have been extended in later work for various purposes (Maxwell III and Kaplan, 1991; Kay, 1996). Alternatives in the parse chart stem from syntactic ambiguities, and are specified locally as the possible decompositions of each phrase into its sub-phrases. Packed representations have been utilized also in transfer-based machine translation. Emele and Dorna (1998) translated packed source language representation to packed target language representation while avoiding unnecessary unpacking during transfer. Unlike our rule application, in their work transfer rules preserve ambiguity stemming from source language, rather than generating new alternatives. Mi et al.(2008) applied statistical m</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>Martin Kay. 1996. Chart generation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="2027" citStr="Lin and Pantel, 2001" startWordPosition="282" endWordPosition="285">bout named entities, semantic roles etc. Most commonly, inference over such representations is made by applying some kind of transformations or substitutions to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Kno</context>
<context position="27753" citStr="Lin and Pantel, 2001" startWordPosition="4677" endWordPosition="4680">below). For a complete feature description, see (Bar-Haim et al., 2009). Rule Bases In addition to the generic rules described in Section 4.1, the following large-scale sources for entailment rules were used: Wikipeda: We used the lexical rulebase of Shnarch et al. (2009), who extracted rules such as ‘Janis Joplin —* singer’ from Wikipedia based on both its metadata (e.g. links and redirects) and text definitions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y—*X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: W</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Evaluation of Parsing Systems at LREC.</booktitle>
<contexts>
<context position="9243" citStr="Lin, 1998" startWordPosition="1422" endWordPosition="1423">1 subj V VERB obj �����������+ N2 NOUN N1 NOUN L N2 NOUN R (a) Passive to active transformation (substitution rule) ROOT i 1 see VERB r������������ mod 11o John NOUN Mary NOUN mod ( subj yesterday NOUN ROOT i ( see VERB q��������������������� s�����������mod be ) by obj Mary NOUN mod ( be VERB by PREP yesterday NOUN N pcomp−n beautiful ADJ John NOUN beautiful ADJ Source: Beautiful Mary was seen by John yesterday. Derived: John saw beautiful Mary yesterday. (b) Application of passive to active transformation Figure 1: An inference rule application. POS and relation labels are based on Minipar (Lin, 1998) of l’s nodes) with r. This is the case for the passive rule, as well as for lexical rules such as ‘buy → purchase’. By contrast, introduction rules are used to make inferences from a subtree of s, while the other parts of s are ignored and do not affect d. A typical example is inferring a proposition embedded as a relative clause in s. In this case, the derived tree d is simply taken to be r. In addition to inference rules, the formalism includes annotation rules which add features to existing parse tree nodes. These rules have been used for identifying contexts that affect the polarity of pr</context>
<context position="17472" citStr="Lin, 1998" startWordPosition="2915" endWordPosition="2916">em should be instantiated in r. This is decomposed into a sequence of simpler operations: first, r is instantiated with a representative from the set, and then we apply (adhoc) lexical substitution rules for creating a new node for each other node in the set5. Alignment sharing: Modifiers of aligned nodes are shared (rather than copied) as follows. Given a node nL in l aligned to a node nR in r, and an outgoing d-edge d of nL which is not part of l, we share d between nL and nR by adding nR to Sd 3This is the case in our current implementation, which is based on the coarse tag-set of Minipar (Lin, 1998). 4With the following exceptions: variables that are the only node in R (and hence are both the root and a leaf), and variables with additional alignments (other than the implicit alignment between their occurrences in L and R) are not considered dual-leaves. 5Notice that these nodes, in addition to the usual alignment with their source nodes in l, share the same daughters in r. and setting reld(nR) = reld(nL). This is illustrated by the sharing of yesterday in Figure 2. We also copy annotation features from nL to nR. We note at this point that the instantiation of variables that are not dual </context>
<context position="24672" citStr="Lin, 1998" startWordPosition="4175" endWordPosition="4176">sis) pairs, which need to be classified as entailing/non entailing. Our first experiment shows, using a small rule set, that compact inference outperforms explicit inference by orders of magnitude (Section 4.1). The second experiment shows that compact inference scales well to a full-blown RTE setting with several large-scale rule bases, where up to hundreds of rules are applied for a text (Section 4.2). 1061 4.1 Compact vs. Explicit Inference To compare explicit and compact inference we randomly sampled 100 pairs from the RTE-3 development set, and parsed the text in each pair using Minipar (Lin, 1998). We used a set of manually-composed entailment rules for inference over generic linguistic phenomena such as passive, conjunction, relative clause, apposition, possessives, and determiners, which contains a few dozens of rules. To make a fair comparison, we aimed to make the explicit inference implementation reasonably efficient, e.g. by preventing generation of the same tree by different permutations of the same rule applications. Both configurations perform rule application iteratively, until no new matches are found. In each iteration we first find all rule matches and then apply all match</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Proceedings of the Workshop on Evaluation of Parsing Systems at LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="33322" citStr="MacCartney et al., 2008" startWordPosition="5604" endWordPosition="5608">s show accuracy loss on RTE-4, obtained for removing each rule base (ablation tests). 5 Related Work This section discusses related work on applying knowledge-based transformations within RTE systems, as well as on using packed representations in other NLP tasks. RTE Systems Previous RTE systems usually restricted both the type of allowed transformations and the search space. Systems based on lexical (word-based or phrase-based) matching of h in t typically applied only lexical rules (without vari1063 ables), where both sides of the rule are matched directly in t and h (Haghighi et al., 2005; MacCartney et al., 2008). The inference formalism we use is more expressive, allowing also syntactic and lexical-syntactic transformations as well as rule chaining. Hickl (2008) derived from a given (t, h) pair a small set of discourse commitments, which are quite similar to the kind of consequents we derive by our syntactic and lexical-syntactic rules. The commitments were generated by several different tools and techniques, compared to our generic unified inference process, and commitment generation efficiency was not discussed. Braz et al. (2005) presented a semantic inference framework which “augments” the text r</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John T Maxwell</author>
<author>Ronald M Kaplan</author>
</authors>
<title>A method for disjunctive constraint satisfaction.</title>
<date>1991</date>
<booktitle>Current Issues in Parsing Technology.</booktitle>
<editor>In Masaru Tomita, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Maxwell, Kaplan, 1991</marker>
<rawString>John T. Maxwell III and Ronald M. Kaplan. 1991. A method for disjunctive constraint satisfaction. In Masaru Tomita, editor, Current Issues in Parsing Technology. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekeley</author>
<author>Veronkia Zielinska</author>
<author>Brian Young</author>
</authors>
<title>The cross-breeding of dictionaries.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="28165" citStr="Meyers et al., 2004" startWordPosition="4743" endWordPosition="4746">ons, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y—*X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Search In this work we focus on efficient representation of the search space, leaving for future work the complementary problem of devising effective search heuristics over our representation. In the current experiment we imple</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekeley, Zielinska, Young, 2004</marker>
<rawString>A. Meyers, R. Reeves, Catherine Macleod, Rachel Szekeley, Veronkia Zielinska, and Brian Young. 2004. The cross-breeding of dictionaries. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Bonaventura Coppola</author>
<author>Timothy Chklovski</author>
<author>Eduard Hovy</author>
</authors>
<title>ISP: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2183" citStr="Pantel et al., 2007" startWordPosition="304" endWordPosition="307">s to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions (KRAQ)1 and the planned evaluation of knowledge resources in the forthcoming 5th Recognizing Textual Entailment</context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning inferential selectional preferences. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
<author>Ralph Grishman</author>
</authors>
<title>Automatic paraphrase acquisition from news articles.</title>
<date>2002</date>
<booktitle>In Proceedings of Human Language Technology Conference (HLT 2002),</booktitle>
<location>San Diego, USA.</location>
<contexts>
<context position="2050" citStr="Shinyama et al., 2002" startWordPosition="286" endWordPosition="289">emantic roles etc. Most commonly, inference over such representations is made by applying some kind of transformations or substitutions to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning fo</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, Grishman, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and Ralph Grishman. 2002. Automatic paraphrase acquisition from news articles. In Proceedings of Human Language Technology Conference (HLT 2002), San Diego, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Libby Barak</author>
<author>Ido Dagan</author>
</authors>
<title>Extracting lexical reference rules from Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="27404" citStr="Shnarch et al. (2009)" startWordPosition="4618" endWordPosition="4621">d its features are quite typical for the RTE literature. They include lexical and structural measures for the coverage of R by T, where high coverage is assumed to correlate with entailment, as well as features aiming to detect inconsistencies between T and R such as incompatible arguments for the same predicate or incompatible verb polarity (see below). For a complete feature description, see (Bar-Haim et al., 2009). Rule Bases In addition to the generic rules described in Section 4.1, the following large-scale sources for entailment rules were used: Wikipeda: We used the lexical rulebase of Shnarch et al. (2009), who extracted rules such as ‘Janis Joplin —* singer’ from Wikipedia based on both its metadata (e.g. links and redirects) and text definitions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y—*X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailmen</context>
</contexts>
<marker>Shnarch, Barak, Dagan, 2009</marker>
<rawString>Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Extracting lexical reference rules from Wikipedia. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning canonical forms of entailment rules.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="27912" citStr="Szpektor and Dagan, 2007" startWordPosition="4704" endWordPosition="4707">g large-scale sources for entailment rules were used: Wikipeda: We used the lexical rulebase of Shnarch et al. (2009), who extracted rules such as ‘Janis Joplin —* singer’ from Wikipedia based on both its metadata (e.g. links and redirects) and text definitions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y—*X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc.</context>
</contexts>
<marker>Szpektor, Dagan, 2007</marker>
<rawString>Idan Szpektor and Ido Dagan. 2007. Learning canonical forms of entailment rules. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="28267" citStr="Szpektor and Dagan, 2008" startWordPosition="4756" endWordPosition="4759">al rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y—*X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Search In this work we focus on efficient representation of the search space, leaving for future work the complementary problem of devising effective search heuristics over our representation. In the current experiment we implemented a simple search strategy, in the spirit of (de Salvo Braz et al., 2005): first, we applied thre</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Augmenting WordNet-based inference with argument mapping.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP Workshop on Applied Textual Inference (TextInfer).</booktitle>
<contexts>
<context position="28076" citStr="Szpektor and Dagan, 2009" startWordPosition="4728" endWordPosition="4731">singer’ from Wikipedia based on both its metadata (e.g. links and redirects) and text definitions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y—*X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Search In this work we focus on efficient representation of the search space, leaving for future work the complementary problem of devisin</context>
</contexts>
<marker>Szpektor, Dagan, 2009</marker>
<rawString>Idan Szpektor and Ido Dagan. 2009. Augmenting WordNet-based inference with argument mapping. In Proceedings of ACL-IJCNLP Workshop on Applied Textual Inference (TextInfer).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Hristo Tanev</author>
<author>Ido Dagan</author>
<author>Bonaventure Coppola</author>
</authors>
<title>Scaling web based acquisition of entailment patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2073" citStr="Szpektor et al., 2004" startWordPosition="290" endWordPosition="293"> commonly, inference over such representations is made by applying some kind of transformations or substitutions to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions (</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventure Coppola. 2004. Scaling web based acquisition of entailment patterns. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>