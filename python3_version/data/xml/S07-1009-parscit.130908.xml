<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008827">
<title confidence="0.865132">
SemEval-2007 Task 10: English Lexical Substitution Task
</title>
<author confidence="0.994908">
Diana McCarthy
</author>
<affiliation confidence="0.99791">
University of Sussex
</affiliation>
<address confidence="0.9188095">
Falmer, East Sussex
BN1 9QH, UK
</address>
<email confidence="0.997988">
dianam@sussex.ac.uk
</email>
<author confidence="0.992403">
Roberto Navigli
</author>
<affiliation confidence="0.995357">
University of Rome “La Sapienza”
</affiliation>
<address confidence="0.936001">
Via Salaria, 113
00198 Roma, Italy
</address>
<email confidence="0.995231">
navigli@di.uniroma1.it
</email>
<sectionHeader confidence="0.998567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999457909090909">
In this paper we describe the English Lexical
Substitution task for SemEval. In the task,
annotators and systems find an alternative
substitute word or phrase for a target word in
context. The task involves both finding the
synonyms and disambiguating the context.
Participating systems are free to use any lex-
ical resource. There is a subtask which re-
quires identifying cases where the word is
functioning as part of a multiword in the sen-
tence and detecting what that multiword is.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999760741935484">
Word sense disambiguation (WSD) has been de-
scribed as a task in need of an application. Whilst
researchers believe that it will ultimately prove use-
ful for applications which need some degree of se-
mantic interpretation, the jury is still out on this
point. One problem is that WSD systems have been
tested on fine-grained inventories, rendering the task
harder than it need be for many applications (Ide
and Wilks, 2006). Another significant problem is
that there is no clear choice of inventory for any
given task (other than the use of a parallel corpus
for a specific language pair for a machine translation
application).
The lexical substitution task follows on from
some previous ideas (McCarthy, 2002) to exam-
ine the capabilities of WSD systems built by re-
searchers on a task which has potential for NLP
applications. Finding alternative words that can
occur in given contexts would potentially be use-
ful to many applications such as question answer-
ing, summarisation, paraphrase acquisition (Dagan
et al., 2006), text simplification and lexical acquisi-
tion (McCarthy, 2002). Crucially this task does not
specify the inventory for use beforehand to avoid
bias to one predefined inventory and makes it eas-
ier for those using automatically acquired resources
to enter the arena. Indeed, since the systems in
SemEval did not know the candidate substitutes for
a word before hand, the lexical resource is evaluated
as much as the context based disambiguation com-
ponent.
</bodyText>
<sectionHeader confidence="0.808015" genericHeader="method">
2 Task set up
</sectionHeader>
<bodyText confidence="0.99981005">
The task involves a lexical sample of nouns, verbs,
adjectives and adverbs. Both annotators and sys-
tems select one or more substitutes for the target
word in the context of a sentence. The data was
selected from the English Internet Corpus of En-
glish produced by Sharoff (2006) from the Inter-
net (http://corpus.leeds.ac.uk/internet.html). This is
a balanced corpus similar in flavour to the BNC,
though with less bias to British English, obtained
by sampling data from the web. Annotators are not
provided with the PoS (noun, verb, adjective or ad-
verb) but the systems are. Annotators can provide
up to three substitutes but all should be equally as
good. They are instructed that they can provide a
phrase if they can’t think of a good single word sub-
stitute. They can also use a slightly more general
word if that is close in meaning. There is a “NAME”
response if the target is part of a proper name and
“NIL” response if annotators cannot think of a good
substitute. The subjects are also asked to identify
</bodyText>
<page confidence="0.987925">
48
</page>
<bodyText confidence="0.982340647058824">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53,
Prague, June 2007. c�2007 Association for Computational Linguistics
if they feel the target word is an integral part of
a phrase, and what that phrase was. This option
was envisaged for evaluation of multiword detec-
tion. Annotators did sometimes use it for paraphras-
ing a phrase with another phrase. However, for an
item to be considered a constituent of a multiword,
a majority of at least 2 annotators had to identify the
same multiword.1
The annotators were 5 native English speakers
from the UK. They each annotated the entire dataset.
All annotations were semi-automatically lemma-
tised (substitutes and identified multiwords) unless
the lemmatised version would change the meaning
of the substitute or if it was not obvious what the
canonical version of the multiword should be.
</bodyText>
<subsectionHeader confidence="0.988439">
2.1 Data Selection
</subsectionHeader>
<bodyText confidence="0.999983789473684">
The data set comprises 2010 sentences, 201 target
words each with 10 sentences. We released 300 for
the trial data and kept the remaining 1710 for the
test release. 298 of the trial, and 1696 of the test
release remained after filtering items with less than
2 non NIL and non NAME responses and a few with
erroneous PoS tags. The words included were se-
lected either manually (70 words) from examination
of a variety of lexical resources and corpora or au-
tomatically (131) using information in these lexical
resources. Words were selected from those having a
number of different meanings, each with at least one
synonym. Since typically the distribution of mean-
ings of a word is strongly skewed (Kilgarriff, 2004),
for the test set we randomly selected 20 words in
each PoS for which we manually selected the sen-
tences 2 (we refer to these words as MAN) whilst for
the remaining words (RAND) the sentences were se-
lected randomly.
</bodyText>
<subsectionHeader confidence="0.986945">
2.2 Inter Annotator Agreement
</subsectionHeader>
<bodyText confidence="0.996177333333333">
Since we have sets of substitutes for each item and
annotator, pairwise agreement was calculated be-
tween each pair of sets (p1, p2 E P) from each pos-
</bodyText>
<footnote confidence="0.960757333333333">
1Full instructions given to the annotators are posted at
http://www.informatics.susx.ac.uk/research/nlp/mccarthy/files/
instructions.pdf.
2There were only 19 verbs due to an error in automatic se-
lection of one of the verbs picked for manual selection of sen-
tences.
</footnote>
<bodyText confidence="0.998371727272727">
Pairwise inter-annotator agreement was 27.75%.
73.93% had modes, and pairwise agreement with the
mode was 50.67%. Agreement is increased if we re-
move one annotator who typically gave 2 or 3 sub-
stitutes for each item, which increased coverage but
reduced agreement. Without this annotator, inter-
annotator agreement was 31.13% and 64.7% with
mode.
Multiword detection pairwise agreement was
92.30% and agreement on the identification of the
exact form of the actual multiword was 44.13%.
</bodyText>
<sectionHeader confidence="0.987523" genericHeader="method">
3 Scoring
</sectionHeader>
<bodyText confidence="0.999886961538462">
We have 3 separate subtasks 1) best 2) oot and 3)
mw which we describe below. 3 In the equations
and results tables that follow we use P for precision,
R for recall, and Mode P and Mode R where we
calculate precision and recall against the substitute
chosen by the majority of annotators, provided that
there is a majority.
Let H be the set of annotators, T be the set of test
items with 2 or more responses (non NIL or NAME)
and hi be the set of responses for an item i E T for
annotator h E H.
For each i E T we calculate the mode (mi) i.e.
the most frequent response provided that there is a
response more frequent than the others. The set of
items where there is such a mode is referred to as
TM. Let A (and AM) be the set of items from T
(or TM) where the system provides at least one sub-
stitute. Let ai : i E A (or ai : i E AM) be the set
of guesses from the system for item i. For each i
we calculate the multiset union (Hi) for all hi for all
h E H and for each unique type (res) in Hi will
have an associated frequency (freqTe3) for the num-
ber of times it appears in Hi.
For example: Given an item (id 9999) for happy;a
supposing the annotators had supplied answers as
follows:
</bodyText>
<figure confidence="0.829776">
annotator responses
1 glad merry
2 glad
3 cheerful glad
4 merry
5 jovial
</figure>
<footnote confidence="0.997990333333333">
3The scoring measures are as described in the doc-
ument at http://nlp.cs.swarthmore.edu/semeval/tasks/task10/
task10documentation.pdf released with our trial data.
</footnote>
<page confidence="0.896753">
2
</page>
<figure confidence="0.9524715">
ible pairing (P) as E
p1
p
s
p1,pPi p1Up2
|
</figure>
<page confidence="0.996687">
49
</page>
<bodyText confidence="0.999861555555556">
then Hi would be glad glad glad merry merry
cheerful jovial. The res with associated frequencies
would be glad 3 merry 2 cheerful 1 and jovial 1.
best measures This requires the best file produced
by the system which gives as many guesses as the
system believes are fitting, but where the credit
for each correct guess is divided by the number of
guesses. The first guess in the list is taken as the
best guess (bg).
</bodyText>
<equation confidence="0.986776166666667">
Eres∈ai freqres
|ai|
|Hi |(1)
Eres∈ai freqres
|ai|
|Hi |(2)
Ebgi∈AM 1 if bg = mi
Mode P = (3)
|AM|
Ebgi∈TM 1 if bg = mi
Mode R = (4)
|TM|
</equation>
<bodyText confidence="0.9997392">
A system is permitted to provide more than one
response, just as the annotators were. They can
do this if they are not sure which response is bet-
ter, however systems will maximise the score if they
guess the most frequent response from the annota-
tors. For P and R the credit is divided by the num-
ber of guesses that a system makes to prevent a sys-
tem simply hedging its bets by providing many re-
sponses. The credit is also divided by the number of
responses from annotators. This gives higher scores
to items with less variation. We want to emphasise
test items with better agreement.
Using the example for happy;a id 9999 above, if
the system’s responses for this item was glad; cheer-
ful the credit for a9999 in the numerator of P and R
</bodyText>
<equation confidence="0.742367">
3+1
would be 7 = .286
2
</equation>
<bodyText confidence="0.999169076923077">
For Mode P and Mode R we use the system’s
first guess and compare this to the mode of the anno-
tators responses on items where there was a response
more frequent than the others.
oot measures This allows a system to make up to
10 guesses. The credit for each correct guess is not
divided by the number of guesses. This allows for
the fact that there is a lot of variation for the task and
we only have 5 annotators. With 10 guesses there is
a better chance that the systems find the responses
of these 5 annotators. There is no ordering of the
guesses and the Mode scores give credit where the
mode was found in one of the system’s 10 guesses.
</bodyText>
<equation confidence="0.965275">
Eres∈ai freqres
P = Eai:i∈A |A ||Hi |(5)
|T|
Eai:iEAM 1 if any guess ∈ ai = mi
Mode P = |AM|
Mode R = Eai:i∈TM 1 if any guess ∈ ai = mi
|TM|
</equation>
<bodyText confidence="0.999912888888889">
mw measures For this measure, a system must
identify items where the target is part of a multiword
and what the multiword is. The annotators do not all
have linguistics background, they are simply asked
if the target is an integral part of a phrase, and if so
what the phrase is. Sometimes this option is used
by the subjects for paraphrasing a phrase of the sen-
tence, but typically it is used when there is a mul-
tiword. For scoring, a multiword item is one with
a majority vote for the same multiword with more
than 1 annotator identifying the multiword.
Let MW be the subset of T for which there
is such a multiword response from a majority of
at least 2 annotators. Let mwi ∈ MW be the
multiword identified by majority vote for item i.
Let MW sys be the subset of T for which there is a
multiword response from the system and mwsysi
be a multiword specified by the system for item i.
</bodyText>
<equation confidence="0.93441125">
detection P =
Emwsysi∈MWsys 1 if mwi exists at i
|MWsys|
detection R =
E
mwsysi∈MW 1 if mwi exists at i
|MW|
identification P =
Emwsysi∈MWsys 1 if mwsysi = mwi
|MWsys|
P Eai:i∈A
=
|A|
R Eai:i∈T
=
|T|
R =
Eres∈ai freqres
Eai:i∈T |Hi |(6)
50
identification R =
EmwsysiEMW 1 if MWSYSi = MWi
 |(12)
MWI
</equation>
<subsectionHeader confidence="0.991881">
3.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999609142857143">
We produced baselines using WordNet 2.1 (Miller et
al., 1993a) and a number of distributional similarity
measures. For the WordNet best baseline we found
the best ranked synonym using the criteria 1 to 4
below in order. For WordNet oot we found up to 10
synonyms using criteria 1 to 4 in order until 10 were
found:
</bodyText>
<listItem confidence="0.961975833333333">
1. Synonyms from the first synset of the target
word, and ranked with frequency data obtained
from the BNC (Leech, 1992).
2. synonyms from the hypernyms (verbs and
nouns) or closely related classes (adjectives) of
that first synset, ranked with the frequency data.
3. Synonyms from all synsets of the target word,
and ranked using the BNC frequency data.
4. synonyms from the hypernyms (verbs and
nouns) or closely related classes (adjectives) of
all synsets of the target, ranked with the BNC
frequency data.
</listItem>
<bodyText confidence="0.999868888888889">
We also produced best and oot baselines using
the distributional similarity measures l1, jaccard, co-
sine, lin (Lin, 1998) and aSD (Lee, 1999) 4. We took
the word with the largest similarity (or smallest dis-
tance for aSD and l1) for best and the top 10 for oot.
For mw detection and identification we used
WordNet to detect if a multiword in WordNet which
includes the target word occurs within a window of
2 words before and 2 words after the target word.
</bodyText>
<sectionHeader confidence="0.998487" genericHeader="method">
4 Systems
</sectionHeader>
<bodyText confidence="0.998143714285714">
9 teams registered and 8 participated, and two of
these teams (SWAG and IRST) each entered two sys-
tems, we distinguish the first and second systems
with a 1 and 2 suffix respectively.
The systems all used 1 or more predefined inven-
tories. Most used web queries (HIT, MELB, UNT)
or web data (Brants and Franz, 2006) (IRST2, KU,
</bodyText>
<footnote confidence="0.536065">
4We used 0.99 as the parameter for α for this measure.
</footnote>
<bodyText confidence="0.9986831">
SWAG1, SWAG2, USYD, UNT) to obtain counts for
disambiguation, with some using algorithms to de-
rive domain (IRST1) or co-occurrence (TOR) infor-
mation from the BNC. Most systems did not use
sense tagged data for disambiguation though MELB
did use SemCor (Miller et al., 1993b) for filtering in-
frequent synonyms and UNT used a semi-supervised
word sense disambiguation combined with a host of
other techniques, including machine translation en-
gines.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99993965625">
In tables 1 to 3 we have ordered systems accord-
ing to R on the best task, and in tables 4 to 6 ac-
cording to R on oot. We show all scores as per-
centages i.e. we multiply the scores in section 3
by 100. In tables 3 and 6 we show results using
the subset of items which were i) NOT identified as
multiwords (NMWT) ii) scored only on non multi-
word substitutes from both annotators and systems
(i.e. no spaces) (NMWS). Unfortunately we do not
have space to show the analysis for the MAN and
RAND subsets here. Please refer to the task website
for these results. 5 We retain the same ordering for
the further analysis tables when we look at subsets
of the data. Although there are further differences
in the systems which would warrant reranking on an
individual analysis, since we combined the subanal-
yses in one table we keep the order as for 1 and 4
respectively for ease of comparison.
There is some variation in rank order of the sys-
tems depending on which measures are used. 6 KU
is highest ranking on R for best. UNT is best at find-
ing the mode, particularly on oot, though it is the
most complicated system exploiting a great many
knowledge sources and components. IRST2 does
well at finding the mode in best. The IRST2 best
R score is lower because it supplied many answers
for each item however it achieves the best R score
on the oot task. The baselines are outperformed by
most systems. The WordNet baseline outperforms
those derived from distributional methods. The dis-
tributional methods, especially lin, show promising
results given that these methods are automatic and
</bodyText>
<footnote confidence="0.99945575">
5The task website is at http://www.informatics.sussex.ac.uk/
research/nlp/mccarthy/task10index.html.
6There is not a big difference between P and R because
systems typically supplied answers for most items.
</footnote>
<page confidence="0.992431">
51
</page>
<table confidence="0.999529222222222">
Systems P R Mode P Mode R
KU 12.90 12.90 20.65 20.65
UNT 12.77 12.77 20.73 20.73
MELB 12.68 12.68 20.41 20.41
HIT 11.35 11.35 18.86 18.86
USYD 11.23 10.88 18.22 17.64
IRST1 8.06 8.06 13.09 13.09
IRST2 6.95 6.94 20.33 20.33
TOR 2.98 2.98 4.72 4.72
</table>
<tableCaption confidence="0.933004">
Table 1: best results
</tableCaption>
<table confidence="0.999917714285714">
Systems P R Mode P Mode R
WordNet 9.95 9.95 15.28 15.28
lin 8.84 8.53 14.69 14.23
l1 8.11 7.82 13.35 12.93
lee 6.99 6.74 11.34 10.98
jaccard 6.84 6.60 11.17 10.81
cos 5.07 4.89 7.64 7.40
</table>
<tableCaption confidence="0.998231">
Table 2: best baseline results
</tableCaption>
<bodyText confidence="0.992186">
don’t require hand-crafted inventories. As yet we
haven’t combined the baselines with disambiguation
methods.
Only HIT attempted the mw task. It outperforms
all baselines from WordNet.
</bodyText>
<subsectionHeader confidence="0.99864">
5.1 Post Hoc Analysis
</subsectionHeader>
<bodyText confidence="0.9997336">
Choosing a lexical substitute for a given word is
not clear cut and there is inherently variation in the
task. Since it is quite likely that there will be syn-
onyms that the five annotators do not think of we
conducted a post hoc analysis to see if the synonyms
selected by the original annotators were better, on
the whole, than those in the systems responses. We
randomly selected 100 sentences from the subset of
items which had more than 2 single word substitutes,
no NAME responses, and where the target word was
</bodyText>
<table confidence="0.9958488">
Systems NMWT NMWS
P R P R
KU 13.39 13.39 14.33 13.98
UNT 13.46 13.46 13.79 13.79
MELB 13.35 13.35 14.19 13.82
HIT 11.97 11.97 12.55 12.38
USYD 11.68 11.34 12.48 12.10
IRST1 8.44 8.44 8.98 8.92
IRST2 7.25 7.24 7.67 7.66
TOR 3.22 3.22 3.32 3.32
</table>
<tableCaption confidence="0.998534">
Table 3: Further analysis for best
</tableCaption>
<table confidence="0.9998504">
Systems P R Mode P Mode R
IRST2 69.03 68.90 58.54 58.54
UNT 49.19 49.19 66.26 66.26
KU 46.15 46.15 61.30 61.30
IRST1 41.23 41.20 55.28 55.28
USYD 36.07 34.96 43.66 42.28
SWAG2 37.80 34.66 50.18 46.02
HIT 33.88 33.88 46.91 46.91
SWAG1 35.53 32.83 47.41 43.82
TOR 11.19 11.19 14.63 14.63
</table>
<tableCaption confidence="0.885794">
Table 4: oot results
</tableCaption>
<table confidence="0.999014285714286">
Systems P R Mode P Mode R
WordNet 29.70 29.35 40.57 40.57
lin 27.70 26.72 40.47 39.19
l1 24.09 23.23 36.10 34.96
lee 20.09 19.38 29.81 28.86
jaccard 18.23 17.58 26.87 26.02
cos 14.07 13.58 20.82 20.16
</table>
<tableCaption confidence="0.930963">
Table 5: oot baseline results
</tableCaption>
<table confidence="0.99982">
NMWT NMWS
Systems P R P R
IRST2 72.04 71.90 76.19 76.06
UNT 51.13 51.13 54.01 54.01
KU 48.43 48.43 49.72 49.72
IRST1 43.11 43.08 45.13 45.11
USYD 37.26 36.17 40.13 38.89
SWAG2 39.95 36.51 40.97 37.75
HIT 35.60 35.60 36.63 36.63
SWAG1 37.49 34.64 38.36 35.67
TOR 11.77 11.77 12.22 12.22
</table>
<tableCaption confidence="0.891255">
Table 6: Further analysis for oot
</tableCaption>
<table confidence="0.99769375">
HIT WordNet BL
P R P R
detection 45.34 56.15 43.64 36.92
identification 41.61 51.54 40.00 33.85
</table>
<tableCaption confidence="0.95581">
Table 7: MW results
</tableCaption>
<page confidence="0.900633">
52
</page>
<table confidence="0.962722666666667">
good reasonable bad
sys 9.07 19.08 71.85
origA 37.36 41.01 21.63
</table>
<tableCaption confidence="0.999059">
Table 8: post hoc results
</tableCaption>
<bodyText confidence="0.999890764705882">
not one of those identified as a multiword (i.e. a ma-
jority vote by 2 or more annotators for the same mul-
tiword as described in section 2). We then mixed the
substitutes from the human annotators with those of
the systems. Three fresh annotators7 were given the
test sentence and asked to categorise the randomly
ordered substitutes as good, reasonable or bad. We
take the majority verdict for each substitute, but if
there is one reasonable and one good verdict, then
we categorise the substitute as reasonable. The per-
centage of substitutes for systems (sys) and original
annotators (origA) categorised as good, reasonable
and bad by the post hoc annotators are shown in ta-
ble 8. We see the substitutes from the humans have
a higher proportion of good or reasonable responses
by the post hoc annotators compared to the substi-
tutes from the systems.
</bodyText>
<sectionHeader confidence="0.998932" genericHeader="conclusions">
6 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999991882352941">
We think this task is an interesting one in which to
evaluate automatic approaches of capturing lexical
meaning. There is an inherent variation in the task
because several substitutes may be possible for a
given context. This makes the task hard and scoring
is less straightforward than a task which has fixed
choices. On the other hand, we believe the task taps
into human understanding of word meaning and we
hope that computers that perform well on this task
will have potential in NLP applications. Since a
pre-defined inventory is not used, the task allows us
to compare lexical resources as well as disambigua-
tion techniques without a bias to any predefined in-
ventory. It is possible for those interested in disam-
biguation to focus on this, rather than the choice of
substitutes, by using the union of responses from the
annotators in future experiments.
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998192777777778">
We acknowledge support from the Royal Society UK for fund-
ing the annotation for the project, and for a Dorothy Hodgkin
7Again, these were native English speakers from the UK.
Fellowship to the first author. We also acknowledge support
to the second author from INTEROP NoE (508011, 6th EU
FP). We thank the annotators for their hard work. We thank
Serge Sharoff for the use of his Internet corpus, Julie Weeds for
the software we used for producing the distributional similarity
baselines and Suzanne Stevenson for suggesting the oot task.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999894175">
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
corpus version 1.1. Technical Report.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47–73. Springer.
Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? In Proceedings of Text, Speech,
Dialogue, Brno, Czech Republic.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 25–32.
Geoffrey Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1–13.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
Diana McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 109–115, Philadelphia, USA.
George Miller, Richard Beckwith, Christine Fellbaum,
David Gross, and Katherine Miller, 1993a. Intro-
duction to WordNet: an On-Line Lexical Database.
ftp://clarity.princeton.edu/pub/WordNet/5papers.ps.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993b. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303–308. Morgan Kaufman.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435–462.
</reference>
<page confidence="0.999351">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.776260">
<title confidence="0.986652">SemEval-2007 Task 10: English Lexical Substitution Task</title>
<author confidence="0.999953">Diana McCarthy</author>
<affiliation confidence="0.99998">University of Sussex</affiliation>
<address confidence="0.9149025">Falmer, East Sussex BN1 9QH, UK</address>
<email confidence="0.995778">dianam@sussex.ac.uk</email>
<author confidence="0.999795">Roberto Navigli</author>
<affiliation confidence="0.999195">University of Rome “La Sapienza”</affiliation>
<address confidence="0.995455">Via Salaria, 113 00198 Roma, Italy</address>
<email confidence="0.990803">navigli@di.uniroma1.it</email>
<abstract confidence="0.997526">In this paper we describe the English Lexical Substitution task for SemEval. In the task, annotators and systems find an alternative substitute word or phrase for a target word in context. The task involves both finding the synonyms and disambiguating the context. Participating systems are free to use any lexical resource. There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram corpus version 1.1.</title>
<date>2006</date>
<tech>Technical Report.</tech>
<contexts>
<context position="12407" citStr="Brants and Franz, 2006" startWordPosition="2199" endWordPosition="2202">word with the largest similarity (or smallest distance for aSD and l1) for best and the top 10 for oot. For mw detection and identification we used WordNet to detect if a multiword in WordNet which includes the target word occurs within a window of 2 words before and 2 words after the target word. 4 Systems 9 teams registered and 8 participated, and two of these teams (SWAG and IRST) each entered two systems, we distinguish the first and second systems with a 1 and 2 suffix respectively. The systems all used 1 or more predefined inventories. Most used web queries (HIT, MELB, UNT) or web data (Brants and Franz, 2006) (IRST2, KU, 4We used 0.99 as the parameter for α for this measure. SWAG1, SWAG2, USYD, UNT) to obtain counts for disambiguation, with some using algorithms to derive domain (IRST1) or co-occurrence (TOR) information from the BNC. Most systems did not use sense tagged data for disambiguation though MELB did use SemCor (Miller et al., 1993b) for filtering infrequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host of other techniques, including machine translation engines. 5 Results In tables 1 to 3 we have ordered systems according to R on the best task, a</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram corpus version 1.1. Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Alfio Gliozzo</author>
<author>Efrat Marmorshtein</author>
<author>Carlo Strapparava</author>
</authors>
<title>Direct word sense matching for lexical substitution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1777" citStr="Dagan et al., 2006" startWordPosition="280" endWordPosition="283">(Ide and Wilks, 2006). Another significant problem is that there is no clear choice of inventory for any given task (other than the use of a parallel corpus for a specific language pair for a machine translation application). The lexical substitution task follows on from some previous ideas (McCarthy, 2002) to examine the capabilities of WSD systems built by researchers on a task which has potential for NLP applications. Finding alternative words that can occur in given contexts would potentially be useful to many applications such as question answering, summarisation, paraphrase acquisition (Dagan et al., 2006), text simplification and lexical acquisition (McCarthy, 2002). Crucially this task does not specify the inventory for use beforehand to avoid bias to one predefined inventory and makes it easier for those using automatically acquired resources to enter the arena. Indeed, since the systems in SemEval did not know the candidate substitutes for a word before hand, the lexical resource is evaluated as much as the context based disambiguation component. 2 Task set up The task involves a lexical sample of nouns, verbs, adjectives and adverbs. Both annotators and systems select one or more substitut</context>
</contexts>
<marker>Dagan, Glickman, Gliozzo, Marmorshtein, Strapparava, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Marmorshtein, and Carlo Strapparava. 2006. Direct word sense matching for lexical substitution. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Yorick Wilks</author>
</authors>
<title>Making sense about sense.</title>
<date>2006</date>
<booktitle>In Eneko Agirre and Phil Edmonds, editors, Word Sense Disambiguation, Algorithms and Applications,</booktitle>
<pages>47--73</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1179" citStr="Ide and Wilks, 2006" startWordPosition="184" endWordPosition="187">use any lexical resource. There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is. 1 Introduction Word sense disambiguation (WSD) has been described as a task in need of an application. Whilst researchers believe that it will ultimately prove useful for applications which need some degree of semantic interpretation, the jury is still out on this point. One problem is that WSD systems have been tested on fine-grained inventories, rendering the task harder than it need be for many applications (Ide and Wilks, 2006). Another significant problem is that there is no clear choice of inventory for any given task (other than the use of a parallel corpus for a specific language pair for a machine translation application). The lexical substitution task follows on from some previous ideas (McCarthy, 2002) to examine the capabilities of WSD systems built by researchers on a task which has potential for NLP applications. Finding alternative words that can occur in given contexts would potentially be useful to many applications such as question answering, summarisation, paraphrase acquisition (Dagan et al., 2006), </context>
</contexts>
<marker>Ide, Wilks, 2006</marker>
<rawString>Nancy Ide and Yorick Wilks. 2006. Making sense about sense. In Eneko Agirre and Phil Edmonds, editors, Word Sense Disambiguation, Algorithms and Applications, pages 47–73. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>How dominant is the commonest sense of a word?</title>
<date>2004</date>
<booktitle>In Proceedings of Text,</booktitle>
<location>Speech, Dialogue, Brno, Czech Republic.</location>
<contexts>
<context position="4873" citStr="Kilgarriff, 2004" startWordPosition="798" endWordPosition="799"> for the trial data and kept the remaining 1710 for the test release. 298 of the trial, and 1696 of the test release remained after filtering items with less than 2 non NIL and non NAME responses and a few with erroneous PoS tags. The words included were selected either manually (70 words) from examination of a variety of lexical resources and corpora or automatically (131) using information in these lexical resources. Words were selected from those having a number of different meanings, each with at least one synonym. Since typically the distribution of meanings of a word is strongly skewed (Kilgarriff, 2004), for the test set we randomly selected 20 words in each PoS for which we manually selected the sentences 2 (we refer to these words as MAN) whilst for the remaining words (RAND) the sentences were selected randomly. 2.2 Inter Annotator Agreement Since we have sets of substitutes for each item and annotator, pairwise agreement was calculated between each pair of sets (p1, p2 E P) from each pos1Full instructions given to the annotators are posted at http://www.informatics.susx.ac.uk/research/nlp/mccarthy/files/ instructions.pdf. 2There were only 19 verbs due to an error in automatic selection o</context>
</contexts>
<marker>Kilgarriff, 2004</marker>
<rawString>Adam Kilgarriff. 2004. How dominant is the commonest sense of a word? In Proceedings of Text, Speech, Dialogue, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="11768" citStr="Lee, 1999" startWordPosition="2082" endWordPosition="2083">target word, and ranked with frequency data obtained from the BNC (Leech, 1992). 2. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of that first synset, ranked with the frequency data. 3. Synonyms from all synsets of the target word, and ranked using the BNC frequency data. 4. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of all synsets of the target, ranked with the BNC frequency data. We also produced best and oot baselines using the distributional similarity measures l1, jaccard, cosine, lin (Lin, 1998) and aSD (Lee, 1999) 4. We took the word with the largest similarity (or smallest distance for aSD and l1) for best and the top 10 for oot. For mw detection and identification we used WordNet to detect if a multiword in WordNet which includes the target word occurs within a window of 2 words before and 2 words after the target word. 4 Systems 9 teams registered and 8 participated, and two of these teams (SWAG and IRST) each entered two systems, we distinguish the first and second systems with a 1 and 2 suffix respectively. The systems all used 1 or more predefined inventories. Most used web queries (HIT, MELB, UN</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
</authors>
<title>100 million words of English:</title>
<date>1992</date>
<journal>the British National Corpus. Language Research,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="11237" citStr="Leech, 1992" startWordPosition="1996" endWordPosition="1997">∈MWsys 1 if mwsysi = mwi |MWsys| P Eai:i∈A = |A| R Eai:i∈T = |T| R = Eres∈ai freqres Eai:i∈T |Hi |(6) 50 identification R = EmwsysiEMW 1 if MWSYSi = MWi |(12) MWI 3.1 Baselines We produced baselines using WordNet 2.1 (Miller et al., 1993a) and a number of distributional similarity measures. For the WordNet best baseline we found the best ranked synonym using the criteria 1 to 4 below in order. For WordNet oot we found up to 10 synonyms using criteria 1 to 4 in order until 10 were found: 1. Synonyms from the first synset of the target word, and ranked with frequency data obtained from the BNC (Leech, 1992). 2. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of that first synset, ranked with the frequency data. 3. Synonyms from all synsets of the target word, and ranked using the BNC frequency data. 4. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of all synsets of the target, ranked with the BNC frequency data. We also produced best and oot baselines using the distributional similarity measures l1, jaccard, cosine, lin (Lin, 1998) and aSD (Lee, 1999) 4. We took the word with the largest similarity (or smallest distanc</context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>Geoffrey Leech. 1992. 100 million words of English: the British National Corpus. Language Research, 28(1):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<location>Madison, WI.</location>
<contexts>
<context position="11748" citStr="Lin, 1998" startWordPosition="2078" endWordPosition="2079">first synset of the target word, and ranked with frequency data obtained from the BNC (Leech, 1992). 2. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of that first synset, ranked with the frequency data. 3. Synonyms from all synsets of the target word, and ranked using the BNC frequency data. 4. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of all synsets of the target, ranked with the BNC frequency data. We also produced best and oot baselines using the distributional similarity measures l1, jaccard, cosine, lin (Lin, 1998) and aSD (Lee, 1999) 4. We took the word with the largest similarity (or smallest distance for aSD and l1) for best and the top 10 for oot. For mw detection and identification we used WordNet to detect if a multiword in WordNet which includes the target word occurs within a window of 2 words before and 2 words after the target word. 4 Systems 9 teams registered and 8 participated, and two of these teams (SWAG and IRST) each entered two systems, we distinguish the first and second systems with a 1 and 2 suffix respectively. The systems all used 1 or more predefined inventories. Most used web qu</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, Madison, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Lexical substitution as a task for wsd evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>109--115</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="1466" citStr="McCarthy, 2002" startWordPosition="232" endWordPosition="233">st researchers believe that it will ultimately prove useful for applications which need some degree of semantic interpretation, the jury is still out on this point. One problem is that WSD systems have been tested on fine-grained inventories, rendering the task harder than it need be for many applications (Ide and Wilks, 2006). Another significant problem is that there is no clear choice of inventory for any given task (other than the use of a parallel corpus for a specific language pair for a machine translation application). The lexical substitution task follows on from some previous ideas (McCarthy, 2002) to examine the capabilities of WSD systems built by researchers on a task which has potential for NLP applications. Finding alternative words that can occur in given contexts would potentially be useful to many applications such as question answering, summarisation, paraphrase acquisition (Dagan et al., 2006), text simplification and lexical acquisition (McCarthy, 2002). Crucially this task does not specify the inventory for use beforehand to avoid bias to one predefined inventory and makes it easier for those using automatically acquired resources to enter the arena. Indeed, since the system</context>
</contexts>
<marker>McCarthy, 2002</marker>
<rawString>Diana McCarthy. 2002. Lexical substitution as a task for wsd evaluation. In Proceedings of the ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 109–115, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christine Fellbaum</author>
<author>David Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: an On-Line Lexical Database.</title>
<date>1993</date>
<contexts>
<context position="10862" citStr="Miller et al., 1993" startWordPosition="1926" endWordPosition="1929">ty of at least 2 annotators. Let mwi ∈ MW be the multiword identified by majority vote for item i. Let MW sys be the subset of T for which there is a multiword response from the system and mwsysi be a multiword specified by the system for item i. detection P = Emwsysi∈MWsys 1 if mwi exists at i |MWsys| detection R = E mwsysi∈MW 1 if mwi exists at i |MW| identification P = Emwsysi∈MWsys 1 if mwsysi = mwi |MWsys| P Eai:i∈A = |A| R Eai:i∈T = |T| R = Eres∈ai freqres Eai:i∈T |Hi |(6) 50 identification R = EmwsysiEMW 1 if MWSYSi = MWi |(12) MWI 3.1 Baselines We produced baselines using WordNet 2.1 (Miller et al., 1993a) and a number of distributional similarity measures. For the WordNet best baseline we found the best ranked synonym using the criteria 1 to 4 below in order. For WordNet oot we found up to 10 synonyms using criteria 1 to 4 in order until 10 were found: 1. Synonyms from the first synset of the target word, and ranked with frequency data obtained from the BNC (Leech, 1992). 2. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of that first synset, ranked with the frequency data. 3. Synonyms from all synsets of the target word, and ranked using the BNC freque</context>
<context position="12747" citStr="Miller et al., 1993" startWordPosition="2257" endWordPosition="2260">ed, and two of these teams (SWAG and IRST) each entered two systems, we distinguish the first and second systems with a 1 and 2 suffix respectively. The systems all used 1 or more predefined inventories. Most used web queries (HIT, MELB, UNT) or web data (Brants and Franz, 2006) (IRST2, KU, 4We used 0.99 as the parameter for α for this measure. SWAG1, SWAG2, USYD, UNT) to obtain counts for disambiguation, with some using algorithms to derive domain (IRST1) or co-occurrence (TOR) information from the BNC. Most systems did not use sense tagged data for disambiguation though MELB did use SemCor (Miller et al., 1993b) for filtering infrequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host of other techniques, including machine translation engines. 5 Results In tables 1 to 3 we have ordered systems according to R on the best task, and in tables 4 to 6 according to R on oot. We show all scores as percentages i.e. we multiply the scores in section 3 by 100. In tables 3 and 6 we show results using the subset of items which were i) NOT identified as multiwords (NMWT) ii) scored only on non multiword substitutes from both annotators and systems (i.e. no spaces) (NMWS). U</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George Miller, Richard Beckwith, Christine Fellbaum, David Gross, and Katherine Miller, 1993a. Introduction to WordNet: an On-Line Lexical Database. ftp://clarity.princeton.edu/pub/WordNet/5papers.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<publisher>Morgan Kaufman.</publisher>
<contexts>
<context position="10862" citStr="Miller et al., 1993" startWordPosition="1926" endWordPosition="1929">ty of at least 2 annotators. Let mwi ∈ MW be the multiword identified by majority vote for item i. Let MW sys be the subset of T for which there is a multiword response from the system and mwsysi be a multiword specified by the system for item i. detection P = Emwsysi∈MWsys 1 if mwi exists at i |MWsys| detection R = E mwsysi∈MW 1 if mwi exists at i |MW| identification P = Emwsysi∈MWsys 1 if mwsysi = mwi |MWsys| P Eai:i∈A = |A| R Eai:i∈T = |T| R = Eres∈ai freqres Eai:i∈T |Hi |(6) 50 identification R = EmwsysiEMW 1 if MWSYSi = MWi |(12) MWI 3.1 Baselines We produced baselines using WordNet 2.1 (Miller et al., 1993a) and a number of distributional similarity measures. For the WordNet best baseline we found the best ranked synonym using the criteria 1 to 4 below in order. For WordNet oot we found up to 10 synonyms using criteria 1 to 4 in order until 10 were found: 1. Synonyms from the first synset of the target word, and ranked with frequency data obtained from the BNC (Leech, 1992). 2. synonyms from the hypernyms (verbs and nouns) or closely related classes (adjectives) of that first synset, ranked with the frequency data. 3. Synonyms from all synsets of the target word, and ranked using the BNC freque</context>
<context position="12747" citStr="Miller et al., 1993" startWordPosition="2257" endWordPosition="2260">ed, and two of these teams (SWAG and IRST) each entered two systems, we distinguish the first and second systems with a 1 and 2 suffix respectively. The systems all used 1 or more predefined inventories. Most used web queries (HIT, MELB, UNT) or web data (Brants and Franz, 2006) (IRST2, KU, 4We used 0.99 as the parameter for α for this measure. SWAG1, SWAG2, USYD, UNT) to obtain counts for disambiguation, with some using algorithms to derive domain (IRST1) or co-occurrence (TOR) information from the BNC. Most systems did not use sense tagged data for disambiguation though MELB did use SemCor (Miller et al., 1993b) for filtering infrequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host of other techniques, including machine translation engines. 5 Results In tables 1 to 3 we have ordered systems according to R on the best task, and in tables 4 to 6 according to R on oot. We show all scores as percentages i.e. we multiply the scores in section 3 by 100. In tables 3 and 6 we show results using the subset of items which were i) NOT identified as multiwords (NMWT) ii) scored only on non multiword substitutes from both annotators and systems (i.e. no spaces) (NMWS). U</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T Bunker. 1993b. A semantic concordance. In Proceedings of the ARPA Workshop on Human Language Technology, pages 303–308. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
</authors>
<title>Open-source corpora: Using the net to fish for linguistic data.</title>
<date>2006</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>11</volume>
<issue>4</issue>
<contexts>
<context position="2522" citStr="Sharoff (2006)" startWordPosition="406" endWordPosition="407">nd to avoid bias to one predefined inventory and makes it easier for those using automatically acquired resources to enter the arena. Indeed, since the systems in SemEval did not know the candidate substitutes for a word before hand, the lexical resource is evaluated as much as the context based disambiguation component. 2 Task set up The task involves a lexical sample of nouns, verbs, adjectives and adverbs. Both annotators and systems select one or more substitutes for the target word in the context of a sentence. The data was selected from the English Internet Corpus of English produced by Sharoff (2006) from the Internet (http://corpus.leeds.ac.uk/internet.html). This is a balanced corpus similar in flavour to the BNC, though with less bias to British English, obtained by sampling data from the web. Annotators are not provided with the PoS (noun, verb, adjective or adverb) but the systems are. Annotators can provide up to three substitutes but all should be equally as good. They are instructed that they can provide a phrase if they can’t think of a good single word substitute. They can also use a slightly more general word if that is close in meaning. There is a “NAME” response if the target</context>
</contexts>
<marker>Sharoff, 2006</marker>
<rawString>Serge Sharoff. 2006. Open-source corpora: Using the net to fish for linguistic data. International Journal of Corpus Linguistics, 11(4):435–462.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>