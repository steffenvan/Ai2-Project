<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000243">
<title confidence="0.981356">
Bilingual-LSA Based LM Adaptation for Spoken Language Translation
</title>
<author confidence="0.985974">
Yik-Cheung Tam and Ian Lane and Tanja Schultz
</author>
<affiliation confidence="0.9808355">
InterACT, Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.752638">
Pittsburgh, PA 15213
</address>
<email confidence="0.999698">
{yct,ian.lane,tanja}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995848">
We propose a novel approach to crosslingual
language model (LM) adaptation based on
bilingual Latent Semantic Analysis (bLSA).
A bLSA model is introduced which enables
latent topic distributions to be efficiently
transferred across languages by enforcing
a one-to-one topic correspondence during
training. Using the proposed bLSA frame-
work crosslingual LM adaptation can be per-
formed by, first, inferring the topic poste-
rior distribution of the source text and then
applying the inferred distribution to the tar-
get language N-gram LM via marginal adap-
tation. The proposed framework also en-
ables rapid bootstrapping of LSA models
for new languages based on a source LSA
model from another language. On Chinese
to English speech and text translation the
proposed bLSA framework successfully re-
duced word perplexity of the English LM by
over 27% for a unigram LM and up to 13.6%
for a 4-gram LM. Furthermore, the pro-
posed approach consistently improved ma-
chine translation quality on both speech and
text based adaptation.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976743589744">
Language model adaptation is crucial to numerous
speech and translation tasks as it enables higher-
level contextual information to be effectively incor-
porated into a background LM improving recogni-
tion or translation performance. One approach is
to employ Latent Semantic Analysis (LSA) to cap-
ture in-domain word unigram distributions which
are then integrated into the background N-gram
LM. This approach has been successfully applied
in automatic speech recognition (ASR) (Tam and
Schultz, 2006) using the Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003). The LDA model can
be viewed as a Bayesian topic mixture model with
the topic mixture weights drawn from a Dirichlet
distribution. For LM adaptation, the topic mixture
weights are estimated based on in-domain adapta-
tion text (e.g. ASR hypotheses). The adapted mix-
ture weights are then used to interpolate a topic-
dependent unigram LM, which is finally integrated
into the background N-gram LM using marginal
adaptation (Kneser et al., 1997)
In this paper, we propose a framework to per-
form LM adaptation across languages, enabling the
adaptation of a LM from one language based on the
adaptation text of another language. In statistical
machine translation (SMT), one approach is to ap-
ply LM adaptation on the target language based on
an initial translation of input references (Kim and
Khudanpur, 2003; Paulik et al., 2005). This scheme
is limited by the coverage of the translation model,
and overall by the quality of translation. Since this
approach only allows to apply LM adaptation af-
ter translation, available knowledge cannot be ap-
plied to extend the coverage. We propose a bilingual
LSA model (bLSA) for crosslingual LM adaptation
that can be applied before translation. The bLSA
model consists of two LSA models: one for each
side of the language trained on parallel document
corpora. The key property of the bLSA model is that
</bodyText>
<page confidence="0.947633">
520
</page>
<note confidence="0.9252165">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 520–527,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999939979166667">
the latent topic of the source and target LSA mod-
els can be assumed to be a one-to-one correspon-
dence and thus share a common latent topic space
since the training corpora consist of bilingual paral-
lel data. For instance, say topic 10 of the Chinese
LSA model is about politics. Then topic 10 of the
English LSA model is set to also correspond to pol-
itics and so forth. During LM adaptation, we first
infer the topic mixture weights from the source text
using the source LSA model. Then we transfer the
inferred mixture weights to the target LSA model
and thus obtain the target LSA marginals. The chal-
lenge is to enforce the one-to-one topic correspon-
dence. Our proposal is to share common variational
Dirichlet posteriors over the topic mixture weights
of a document pair in the LDA-style model. The
beauty of the bLSA framework is that the model
searches for a common latent topic space in an un-
supervised fashion, rather than to require manual in-
teraction. Since the topic space is language indepen-
dent, our approach supports topic transfer in multi-
ple language pairs in O(N) where N is the number of
languages.
Related work includes the Bilingual Topic Ad-
mixture Model (BiTAM) for word alignment pro-
posed by (Zhao and Xing, 2006). Basically, the
BiTAM model consists of topic-dependent transla-
tion lexicons modeling Pr(c|e, k) where c, e and
k denotes the source Chinese word, target English
word and the topic index respectively. On the
other hand, the bLSA framework models Pr(c|k)
and Pr(e|k) which is different from the BiTAM
model. By their different modeling nature, the bLSA
model usually supports more topics than the BiTAM
model. Another work by (Kim and Khudanpur,
2004) employed crosslingual LSA using singular
value decomposition which concatenates bilingual
documents into a single input supervector before
projection.
We organize the paper as follows: In Section 2,
we introduce the bLSA framework including La-
tent Dirichlet-Tree Allocation (LDTA) (Tam and
Schultz, 2007) as a correlated LSA model, bLSA
training and crosslingual LM adaptation. In Sec-
tion 3, we present the effect of LM adaptation on
word perplexity, followed by SMT experiments re-
ported in BLEU on both speech and text input in
Section 3.3. Section 4 describes conclusions and fu-
</bodyText>
<figure confidence="0.354469">
Parallel document corpus
</figure>
<figureCaption confidence="0.983341">
Figure 1: Topic transfer in bilingual LSA model.
ture works.
</figureCaption>
<sectionHeader confidence="0.969321" genericHeader="method">
2 Bilingual Latent Semantic Analysis
</sectionHeader>
<bodyText confidence="0.99994035">
The goal of a bLSA model is to enforce a one-
to-one topic correspondence between monolingual
LSA models, each of which can be modeled using
an LDA-style model. The role of the bLSA model
is to transfer the inferred latent topic distribution
from the source language to the target language as-
suming that the topic distributions on both sides are
identical. The assumption is reasonable for parallel
document pairs which are faithful translations. Fig-
ure 1 illustrates the idea of topic transfer between
monolingual LSA models followed by LM adapta-
tion. One observation is that the topic transfer can be
bi-directional meaning that the “flow” of topic can
be from ASR to SMT or vice versa. In this paper,
we only focus on ASR-to-SMT direction. Our tar-
get is to minimize the word perplexity on the target
language through LM adaptation. Before we intro-
duce the heuristic of enforcing a one-to-one topic
correspondence, we describe the Latent Dirichlet-
Tree Allocation (LDTA) for LSA.
</bodyText>
<subsectionHeader confidence="0.998647">
2.1 Latent Dirichlet-Tree Allocation
</subsectionHeader>
<bodyText confidence="0.999850333333333">
The LDTA model extends the LDA model in which
correlation among latent topics are captured using a
Dirichlet-Tree prior. Figure 2 illustrates a depth-two
Dirichlet-Tree. A tree of depth one simply falls back
to the LDA model. The LDTA model is a generative
model with the following generative process:
</bodyText>
<listItem confidence="0.492366">
1. Sample a vector of branch probabilities bj —
</listItem>
<figure confidence="0.998127166666667">
ASR hypo
Chinese N−gram LM
Chinese ASR Chinese−&gt;English SMT
Chinese LSA
Chinese text English text
Adapt
Topic distribution
Chinese−English
English N−gram LM
English LSA
Adapt
MT hypo
</figure>
<page confidence="0.980242">
521
522
</page>
<equation confidence="0.970326783783784">
Yθk =
jc
δjc(k) (1)
bjc
n
Y
i
p(wn1 , zn 1 , bJ 1 ) = p(bJ1 |{αj})
βwizi · θzi
J
Y
j
where p(bJ1|{αj}) =
Dir(bj; αj)
αjc−1
bjc
Y∝
jc
XK
k
Xn
i
γjc = αjc +
qik · δjc(k) (2)
δjc(k)Eq[log bjc]
where
X
Eq[log θk] =
jc
!γjc)
X=
jc δjc(k) Ψ(γjc) − Ψ( X
c
Xn
i
βwk ∝
qik · δ(wi, w) (4)
</equation>
<bodyText confidence="0.991237">
Similar to LDA training, we apply the variational
Bayes approach by optimizing the lower bound of
the marginalized document likelihood:
</bodyText>
<equation confidence="0.999292125">
1 ;Λ)
L(wn 1; Λ, Γ) =Eq[log p(wn 1 ,zn 1 , bJ
q(zn 1 ,bJ1;Γ) ]
1 )
= Eq[log p(wn 1 |zn 1 )] + Eq[log p(zn 1 |bJ
q(zn 1 ) ]
+Eq[log p(bJ 1 ; {αj})
q(bJ 1; {γj}) ]
</equation>
<bodyText confidence="0.96757925">
where q(zn1 , bJ1 ; Γ) = Qni q(zi) · QJ jq(bj) is a fac-
torizable variational posterior distribution over the
latent variables parameterized by Γ which are deter-
mined in the E-step. Λ is the model parameters for
a Dirichlet-Tree {αj} and the topic-dependent uni-
gram LM {βwk}. The LDTA model has an E-step
similar to the LDA model:
E-Step:
</bodyText>
<equation confidence="0.820901">
qik ∝ βwik · eEe[log θk] (3)
</equation>
<bodyText confidence="0.999844125">
where qik denotes q(zi = k) meaning the variational
topic posterior of word wi. Eqn 2 and Eqn 3 are
executed iteratively until convergence is reached.
M-Step:
where δ(wi, w) is a Kronecker Delta function. The
alpha parameters can be estimated with iterative
methods such as Newton-Raphson or simple gradi-
ent ascent procedure.
</bodyText>
<subsectionHeader confidence="0.851991">
2.2 Bilingual LSA training
</subsectionHeader>
<bodyText confidence="0.995760666666667">
For the following explanations, we assume that our
source and target languages are Chinese and En-
glish respectively. The bLSA model training is a
</bodyText>
<figure confidence="0.97851">
Dir(.) Dir(.)
j=2 j=3
Dir(.)
j=1
Dir(.)
Latent topics topic 1 topic 4 topic K
</figure>
<figureCaption confidence="0.999685">
Figure 2: Dirichlet-Tree prior of depth two.
</figureCaption>
<bodyText confidence="0.9950485">
Dir(αj) for each node j = 1...J where αj de-
notes the parameter (aka the pseudo-counts of
its outgoing branches) of the Dirichlet distribu-
tion at node j.
</bodyText>
<listItem confidence="0.448546">
2. Compute the topic proportions as:
</listItem>
<bodyText confidence="0.974272162790698">
where δjc(k) is an indicator function which sets
to unity when the c-th branch of the j-th node
leads to the leaf node of topic k and zero other-
wise. The k-th topic proportion θk is computed
as the product of branch probabilities from the
root node to the leaf node of topic k.
3. Generate a document using the topic multino-
mial for each word wi:
zi ∼ Mult(θ)
wi ∼ Mult(β.zi)
where β.zi denotes the topic-dependent uni-
gram LM indexed by zi.
The joint distribution of the latent variables (topic
sequence zn1 and the Dirichlet nodes over child
branches bj) and an observed document wn1 can be
written as follows:
two-stage procedure. At the first stage, we train
a Chinese LSA model using the Chinese docu-
ments in parallel corpora. We applied the varia-
tional EM algorithm (Eqn 2–4) to train a Chinese
LSA model. Then we used the model to compute
the term eE9[log θk] needed in Eqn 3 for each Chinese
document in parallel corpora. At the second stage,
we apply the same eE9[log θk] to bootstrap an English
LSA model, which is the key to enforce a one-to-one
topic correspondence. Now the hyper-parameters of
the variational Dirichlet posteriors of each node in
the Dirichlet-Tree are shared among the Chinese and
English model. Precisely, we apply only Eqn 3 with
fixed eE9[log θk] in the E-step and Eqn 4 in the M-step
on {βwk} to bootstrap an English LSA model. No-
tice that the E-step is non-iterative resulting in rapid
LSA training. In short, given a monolingual LSA
model, we can rapidly bootstrap LSA models of new
languages using parallel document corpora. Notice
that the English and Chinese vocabulary sizes do not
need to be similar. In our setup, the Chinese vo-
cabulary comes from the ASR system while the En-
glish vocabulary comes from the English part of the
parallel corpora. Since the topic transfer can be bi-
directional, we can perform the bLSA training in a
reverse manner, i.e. training an English LSA model
followed by bootstrapping a Chinese LSA model.
</bodyText>
<subsectionHeader confidence="0.995905">
2.3 Crosslingual LM adaptation
</subsectionHeader>
<bodyText confidence="0.9999825">
Given a source text, we apply the E-step to estimate
variational Dirichlet posterior of each node in the
Dirichlet-Tree. We estimate the topic weights on the
source language using the following equation:
</bodyText>
<equation confidence="0.91826875">
Y � γjc �δ��(k)
ˆθ(CH) a P (5)
k c′ γjc′
jc
</equation>
<bodyText confidence="0.999745">
Then we apply the topic weights into the target LSA
model to obtain an in-domain LSA marginals:
</bodyText>
<equation confidence="0.940446">
(EN) ˆ(CH)
βwk θk (6)
</equation>
<bodyText confidence="0.99977975">
We integrate the LSA marginal into the target back-
ground LM using marginal adaptation (Kneser et al.,
1997) which minimizes the Kullback-Leibler diver-
gence between the adapted LM and the background
</bodyText>
<equation confidence="0.7864185">
LM:
Pra(w|h) aI prrb9(( W) I Prbg(w|h) (7)
</equation>
<bodyText confidence="0.9994418">
Likewise, LM adaptation can /take place on the
source language as well due to the bi-directional na-
ture of the bLSA framework when target-side adap-
tation text is available. In this paper, we focus on
LM adaptation on the target language for SMT.
</bodyText>
<sectionHeader confidence="0.999025" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999707054054054">
We evaluated our bLSA model using the Chinese–
English parallel document corpora consisting of the
Xinhua news, Hong Kong news and Sina news. The
combined corpora contains 67k parallel documents
with 35M Chinese (CH) words and 43M English
(EN) words. Our spoken language translation sys-
tem translates from Chinese to English. The Chinese
vocabulary comes from the ASR decoder while the
English vocabulary is derived from the English por-
tion of the parallel training corpora. The vocabulary
sizes for Chinese and English are 108k and 69k re-
spectively. Our background English LM is a 4-gram
LM trained with the modified Kneser-Ney smooth-
ing scheme using the SRILM toolkit on the same
training text. We explore the bLSA training in both
directions: EN—*CH and CH—*EN meaning that an
English LSA model is trained first and a Chinese
LSA model is bootstrapped or vice versa. Exper-
iments explore which bootstrapping direction yield
best results measured in terms of English word per-
plexity. The number of latent topics is set to 200 and
a balanced binary Dirichlet-Tree prior is used.
With an increasing interest in the ASR-SMT cou-
pling for spoken language translation, we also eval-
uated our approach with Chinese ASR hypotheses
and compared with Chinese manual transcriptions.
We are interested to see the impact due to recog-
nition errors on the ASR hypotheses compared to
the manual transcriptions. We employed the CMU-
InterACT ASR system developed for the GALE
2006 evaluation. We trained acoustic models with
over 500 hours of quickly transcribed speech data re-
leased by the GALE program and the LM with over
800M-word Chinese corpora. The character error
rates on the CCTV, RFA and NTDTV shows in the
RT04 test set are 7.4%, 25.5% and 13.1% respec-
tively.
</bodyText>
<equation confidence="0.9886495">
PrEN(w) = XK
k=1
</equation>
<page confidence="0.994421">
523
</page>
<table confidence="0.876959571428572">
Topic index Top words
“CH-40” flying, submarine, aircraft, air, pilot, land, mission, brand-new
“EN-40” air, sea, submarine, aircraft, flight, flying, ship, test
“CH-41” satellite, han-tian, launch, space, china, technology, astronomy
“EN-41” space, satellite, china, technology, satellites, science
“CH-42” fire, airport, services, marine, accident, air
“EN-42” fire, airport, services, department, marine, air, service
</table>
<tableCaption confidence="0.959077333333333">
Table 1: Parallel topics extracted by the bLSA
model. Top words on the Chinese side are translated
into English for illustration purpose.
</tableCaption>
<figure confidence="0.832774">
2 4 6 8 10 12 14 16 18 20
# of training iterations
</figure>
<figureCaption confidence="0.99207">
Figure 3: Comparison of training log likelihood of
</figureCaption>
<bodyText confidence="0.7847355">
English LSA models bootstrapped from a Chinese
LSA and from a flat monolingual English LSA.
</bodyText>
<subsectionHeader confidence="0.999474">
3.1 Analysis of the bLSA model
</subsectionHeader>
<bodyText confidence="0.999919285714285">
By examining the top-words of the extracted paral-
lel topics, we verify the validity of the heuristic de-
scribed in Section 2.2 which enforces a one-to-one
topic correspondence in the bLSA model. Table 1
shows the latent topics extracted by the CH—*EN
bLSA model. We can see that the Chinese-English
topic words have strong correlations. Many of them
are actually translation pairs with similar word rank-
ings. From this viewpoint, we can interpret bLSA as
a crosslingual word trigger model. The result indi-
cates that our heuristic is effective to extract parallel
latent topics. As a sanity check, we also examine the
likelihood of the training data when an English LSA
model is bootstrapped. We can see from Figure 3
that the likelihood increases monotonically with the
number of training iterations. The figure also shows
that by sharing the variational Dirichlet posteriors
from the Chinese LSA model, we can bootstrap an
English LSA model rapidly compared to monolin-
gual English LSA training with both training proce-
dures started from the same flat model.
</bodyText>
<table confidence="0.99981575">
LM (43M) CCTV RFA NTDTV
BG EN unigram 1065 1220 1549
+CH—*EN (CH ref) 755 880 1113
+EN—*CH (CH ref) 762 896 1111
+CH—*EN (CH hypo) 757 885 1126
+EN—*CH (CH hypo) 766 896 1129
+CH—*EN (EN ref) 731 838 1075
+EN—*CH (EN ref) 747 848 1087
</table>
<tableCaption confidence="0.980854">
Table 2: English word perplexity (PPL) on the RT04
test set using a unigram LM.
</tableCaption>
<subsectionHeader confidence="0.993876">
3.2 LM adaptation results
</subsectionHeader>
<bodyText confidence="0.999852852941176">
We trained the bLSA models on both CH—*EN and
EN—*CH directions and compared their LM adapta-
tion performance using the Chinese ASR hypothe-
ses (hypo) and the manual transcriptions (ref) as in-
put. We adapted the English background LM using
the LSA marginals described in Section 2.3 for each
show on the test set.
We first evaluated the English word perplexity us-
ing the EN unigram LM generated by the bLSA
model. Table 2 shows that the bLSA-based LM
adaptation reduces the word perplexity by over 27%
relative compared to an unadapted EN unigram LM.
The results indicate that the bLSA model success-
fully leverages the text from the source language and
improves the word perplexity on the target language.
We observe that there is almost no performance dif-
ference when either the ASR hypotheses or the man-
ual transcriptions are used for adaptation. The result
is encouraging since the bLSA model may be in-
sensitive to moderate recognition errors through the
projection of the input adaptation text into the latent
topic space. We also apply an English translation
reference for adaptation to show an oracle perfor-
mance. The results using the Chinese hypotheses are
not too far off from the oracle performance. Another
observation is that the CH—*EN bLSA model seems
to give better performance than the EN—*CH bLSA
model. However, their differences are not signifi-
cant. The result may imply that the direction of the
bLSA training is not important since the latent topic
space captured by either language is similar when
parallel training corpora are used. Table 3 shows the
word perplexity when the background 4-gram En-
glish LM is adapted with the tuning parameter Q set
</bodyText>
<figure confidence="0.996879363636364">
Training log likelihood
-2.75e•08
-2.85e•08
-2.95e•08
-3.05e•08
-2.7e•08
-2.8e•08
-2.9e•08
-3e•08
bootstrapped EN LSA
monolingual EN LSA
</figure>
<page confidence="0.992414">
524
</page>
<table confidence="0.999589375">
LM (43M, Q = 0.7) CCTV RFA NTDTV
BG EN 4-gram 118 212 203
+CH→EN (CH ref) 102 191 179
+EN→CH (CH ref) 102 198 179
+CH→EN (CH hypo) 102 193 180
+EN→CH (CH hypo) 103 198 180
+CH→EN (EN ref) 100 186 176
+EN→CH (EN ref) 101 190 176
</table>
<tableCaption confidence="0.9861325">
Table 3: English word perplexity (PPL) on the RT04
test set using a 4-gram LM.
</tableCaption>
<figureCaption confidence="0.939123">
Figure 4: Word perplexity with different Q using
manual reference or ASR hypotheses on CCTV.
</figureCaption>
<bodyText confidence="0.993642">
to 0.7. Figure 4 shows the change of perplexity with
different Q. We see that the adaptation performance
using the ASR hypotheses or the manual transcrip-
tions are almost identical on different Q with an op-
timal value at around 0.7. The results show that the
proposed approach successfully reduces the perplex-
ity in the range of 9–13.6% relative compared to an
unadapted baseline on different shows when ASR
hypotheses are used. Moreover, we observe simi-
lar performance using ASR hypotheses or manual
Chinese transcriptions which is consistent with the
results on Table 2. On the other hand, it is interest-
ing to see that the performance gap from the oracle
adaptation is somewhat related to the degree of mis-
match between the test show and the training condi-
tion. The gap looks wider on the RFA and NTDTV
shows compared to the CCTV show.
</bodyText>
<subsectionHeader confidence="0.9374815">
3.3 Incorporating bLSA into Spoken Language
Translation
</subsectionHeader>
<bodyText confidence="0.999956411764706">
To investigate the effectiveness of bLSA LM adap-
tation for spoken language translation, we incorpo-
rated the proposed approach into our state-of-the-art
phrase-based SMT system. Translation performance
was evaluated on the RT04 broadcast news evalua-
tion set when applied to both the manual transcrip-
tions and 1-best ASR hypotheses. During evalua-
tion two performance metrics, BLEU (Papineni et
al., 2002) and NIST, were computed. In both cases, a
single English reference was used during scoring. In
the transcription case the original English references
were used. For the ASR case, as utterance segmen-
tation was performed automatically, the number of
sentences generated by ASR and SMT differed from
the number of English references. In this case, Lev-
enshtein alignment was used to align the translation
output to the English references before scoring.
</bodyText>
<subsectionHeader confidence="0.956527">
3.4 Baseline SMT Setup
</subsectionHeader>
<bodyText confidence="0.999950586206897">
The baseline SMT system consisted of a non adap-
tive system trained using the same Chinese-English
parallel document corpora used in the previous ex-
periments (Sections 3.1 and 3.2). For phrase extrac-
tion a cleaned subset of these corpora, consisting of
1M Chinese-English sentence pairs, was used. SMT
decoding parameters were optimized using man-
ual transcriptions and translations of 272 utterances
from the RT04 development set (LDC2006E10).
SMT translation was performed in two stages us-
ing an approach similar to that in (Vogel, 2003).
First, a translation lattice was constructed by match-
ing all possible bilingual phrase-pairs, extracted
from the training corpora, to the input sentence.
Phrase extraction was performed using the “PESA”
(Phrase Pair Extraction as Sentence Splitting) ap-
proach described in (Vogel, 2005). Next, a search
was performed to find the best path through the lat-
tice, i.e. that with maximum translation-score. Dur-
ing search reordering was allowed on the target lan-
guage side. The final translation result was that
hypothesis with maximum translation-score, which
is a log-linear combination of 10 scores consist-
ing of Target LM probability, Distortion Penalty,
Word-Count Penalty, Phrase-Count and six Phrase-
Alignment scores. Weights for each component
score were optimized to maximize BLEU-score on
the development set using MER optimization as de-
scribed in (Venugopal et al., 2005).
</bodyText>
<figure confidence="0.9956275">
CCTV (CER=7.4%)
125
120
115
110
105
100
English Word Perplexity
BG 4-gram
+bLSA (CH reference)
+bLSA (CH ASR hypo)
+bLSA (EN reference)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Beta
</figure>
<page confidence="0.988835">
525
</page>
<table confidence="0.999473">
SMT Target LM CCTV Translation - BLEU (NIST) ALL
Quality NTDTV
RFA
Manual Transcription
Baseline LM: 0.162 (5.212) 0.087 (3.854) 0.140 (4.859) 0.132 (5.146)
bLSA (bLSA-Adapted LM): 0.164 (5.212) 0.087 (3.897) 0.143 (4.864) 0.134 (5.162)
1-best ASR Output
CER (%) 7.4 25.5 13.1 14.9
Baseline LM: 0.129 (4.15) 0.051 (2.77) 0.086 (3.50) 0.095 (3.90)
bLSA (bLSA-Adapted LM): 0.132 (4.16) 0.050 (2.79) 0.089 (3.53) 0.096 (3.91)
</table>
<tableCaption confidence="0.9952005">
Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual
transcriptions and 1-best ASR hypotheses
</tableCaption>
<subsectionHeader confidence="0.988181">
3.5 Performance of Baseline SMT System
</subsectionHeader>
<bodyText confidence="0.999910434782609">
First, the baseline system performance was evalu-
ated by applying the system described above to the
reference transcriptions and 1-best ASR hypotheses
generated by our Mandarin speech recognition sys-
tem. The translation accuracy in terms of BLEU and
NIST for each individual show (“CCTV”, “RFA”,
and “NTDTV”), and for the complete test-set, are
shown in Table 4 (Baseline LM). When applied to
the reference transcriptions an overall BLEU score
of 0.132 was obtained. BLEU-scores ranged be-
tween 0.087 and 0.162 for the “RFA”, “NTDTV” and
“CCTV” shows, respectively. As the “RFA” show
contained a large segment of conversational speech,
translation quality was considerably lower for this
show due to genre mismatch with the training cor-
pora of newspaper text.
For the 1-best ASR hypotheses, an overall BLEU
score of 0.095 was achieved. For the ASR case,
the relative reduction in BLEU scores for the RFA
and NTDTV shows is large, due to the significantly
lower recognition accuracies for these shows. BLEU
score is also degraded due to poor alignment of ref-
erences during scoring.
</bodyText>
<subsectionHeader confidence="0.992404">
3.6 Incorporation of bLSA Adaptation
</subsectionHeader>
<bodyText confidence="0.999843">
Next, the effectiveness of bLSA based LM adapta-
tion was evaluated. For each show the target En-
glish LM was adapted using bLSA-adaptation, as
described in Section 2.3. SMT was then applied us-
ing an identical setup to that used in the baseline ex-
periments.
The translation accuracy when bLSA adaptation
was incorporated is shown in Table 4. When ap-
</bodyText>
<sectionHeader confidence="0.673803" genericHeader="method">
CCTV RFA NTDTV All shows
</sectionHeader>
<figureCaption confidence="0.902524">
Figure 5: BLEU score for those 25% utterances
which resulted in different translations after bLSA
adaptation (manual transcriptions)
</figureCaption>
<bodyText confidence="0.999970705882353">
plied to the manual transcriptions, bLSA adaptation
improved the overall BLEU-score by 1.7% relative
(from 0.132 to 0.134). For all three shows bLSA
adaptation gained higher BLEU and NIST metrics.
A similar trend was also observed when the pro-
posed approach was applied to the 1-best ASR out-
put. On the evaluation set a relative improvement in
BLEU score of 1.0% was gained.
The semantic interpretation of the majority of ut-
terances in broadcast news are not affected by topic
context. In the experimental evaluation it was ob-
served that only 25% of utterances produced differ-
ent translation output when bLSA adaptation was
performed compared to the topic-independent base-
line. Although the improvement in translation qual-
ity (BLEU) was small when evaluated over the en-
tire test set, the improvement in BLEU score for
</bodyText>
<figure confidence="0.993564272727273">
Baseline-LM bLSA Adapted LM
BLEU
0.16
0.14
0.12
0.08
0.06
0.04
0.02
0.1
0
</figure>
<page confidence="0.995386">
526
</page>
<bodyText confidence="0.999982">
these 25% utterances was significant. The trans-
lation quality for the baseline and bLSA-adaptive
system when evaluated only on these utterances is
shown in Figure 5 for the manual transcription case.
On this subset of utterances an overall improvement
in BLEU of 0.007 (5.7% relative) was gained, with
a gain of 0.012 (10.6% relative) points for the “NT-
DTV” show. A similar trend was observed when ap-
plied to the 1-best ASR output. In this case a rel-
ative improvement in BLEU of 12.6% was gained
for “NTDTV”, and for “All shows” 0.007 (3.7%)
was gained. Current evaluation metrics for trans-
lation, such as “BLEU”, do not consider the rela-
tive importance of specific words or phrases during
translation and thus are unable to highlight the true
effectiveness of the proposed approach. In future
work, we intend to investigate other evaluation met-
rics which consider the relative informational con-
tent of words.
</bodyText>
<sectionHeader confidence="0.999822" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.998765962962963">
We proposed a bilingual latent semantic model
for crosslingual LM adaptation in spoken language
translation. The bLSA model consists of a set of
monolingual LSA models in which a one-to-one
topic correspondence is enforced between the LSA
models through the sharing of variational Dirich-
let posteriors. Bootstrapping a LSA model for a
new language can be performed rapidly with topic
transfer from a well-trained LSA model of another
language. We transfer the inferred topic distribu-
tion from the input source text to the target lan-
guage effectively to obtain an in-domain target LSA
marginals for LM adaptation. Results showed that
our approach significantly reduces the word per-
plexity on the target language in both cases using
ASR hypotheses and manual transcripts. Interest-
ingly, the adaptation performance is not much af-
fected when ASR hypotheses were used. We eval-
uated the adapted LM on SMT and found that the
evaluation metrics are crucial to reflect the actual
improvement in performance. Future directions in-
clude the exploration of story-dependent LM adap-
tation with automatic story segmentation instead of
show-dependent adaptation due to the possibility of
multiple stories within a show. We will investigate
the incorporation of monolingual documents for po-
tentially better bilingual LSA modeling.
</bodyText>
<sectionHeader confidence="0.969721" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9971455">
This work is partly supported by the Defense Ad-
vanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-2-0001. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998282818181818">
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
Allocation. In Journal ofMachine Learning Research,
pages 1107–1135.
W. Kim and S. Khudanpur. 2003. LM adaptation using
cross-lingual information. In Proc. of Eurospeech.
W. Kim and S. Khudanpur. 2004. Cross-lingual latent
semantic analysis for LM. In Proc. ofICASSP.
R. Kneser, J. Peters, and D. Klakow. 1997. Language
model adaptation using dynamic marginals. In Proc.
ofEurospeech, pages 1971–1974.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proc. ofACL.
M. Paulik, C. F¨ugen, T. Schaaf, T. Schultz, S. St¨uker, and
A. Waibel. 2005. Document driven machine transla-
tion enhanced automatic speech recognition. In Proc.
ofInterspeech.
Y. C. Tam and T. Schultz. 2006. Unsupervised language
model adaptation using latent semantic marginals. In
Proc. ofInterspeech.
Y. C. Tam and T. Schultz. 2007. Correlated latent seman-
tic model for unsupervised language model adaptation.
In Proc. ofICASSP.
A. Venugopal, A. Zollmann, and A. Waibel. 2005. Train-
ing and evaluation error minimization rules for statis-
tical machine translation. In Proc. ofACL.
S. Vogel. 2003. SMT decoder dissected: Word reorder-
ing. In Proc. ofICNLPKE.
S. Vogel. 2005. PESA: Phrase pair extraction as sentence
splitting. In Proc. of the Machine Translation Summit.
B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual topic
admixture models for word alignment. In Proc. of
ACL.
</reference>
<page confidence="0.997122">
527
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.940421">
<title confidence="0.999912">Bilingual-LSA Based LM Adaptation for Spoken Language Translation</title>
<author confidence="0.99407">Yik-Cheung Tam</author>
<author confidence="0.99407">Ian Lane</author>
<author confidence="0.99407">Tanja Schultz</author>
<affiliation confidence="0.999719">InterACT, Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.999911">Pittsburgh, PA 15213</address>
<abstract confidence="0.997874653846154">We propose a novel approach to crosslingual language model (LM) adaptation based on bilingual Latent Semantic Analysis (bLSA). A bLSA model is introduced which enables latent topic distributions to be efficiently transferred across languages by enforcing a one-to-one topic correspondence during training. Using the proposed bLSA framework crosslingual LM adaptation can be performed by, first, inferring the topic posterior distribution of the source text and then applying the inferred distribution to the target language N-gram LM via marginal adaptation. The proposed framework also enables rapid bootstrapping of LSA models for new languages based on a source LSA model from another language. On Chinese to English speech and text translation the proposed bLSA framework successfully reduced word perplexity of the English LM by over 27% for a unigram LM and up to 13.6% for a 4-gram LM. Furthermore, the proposed approach consistently improved machine translation quality on both speech and text based adaptation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<booktitle>In Journal ofMachine Learning Research,</booktitle>
<pages>1107--1135</pages>
<contexts>
<context position="1838" citStr="Blei et al., 2003" startWordPosition="271" endWordPosition="274"> on both speech and text based adaptation. 1 Introduction Language model adaptation is crucial to numerous speech and translation tasks as it enables higherlevel contextual information to be effectively incorporated into a background LM improving recognition or translation performance. One approach is to employ Latent Semantic Analysis (LSA) to capture in-domain word unigram distributions which are then integrated into the background N-gram LM. This approach has been successfully applied in automatic speech recognition (ASR) (Tam and Schultz, 2006) using the Latent Dirichlet Allocation (LDA) (Blei et al., 2003). The LDA model can be viewed as a Bayesian topic mixture model with the topic mixture weights drawn from a Dirichlet distribution. For LM adaptation, the topic mixture weights are estimated based on in-domain adaptation text (e.g. ASR hypotheses). The adapted mixture weights are then used to interpolate a topicdependent unigram LM, which is finally integrated into the background N-gram LM using marginal adaptation (Kneser et al., 1997) In this paper, we propose a framework to perform LM adaptation across languages, enabling the adaptation of a LM from one language based on the adaptation text</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet Allocation. In Journal ofMachine Learning Research, pages 1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kim</author>
<author>S Khudanpur</author>
</authors>
<title>LM adaptation using cross-lingual information.</title>
<date>2003</date>
<booktitle>In Proc. of Eurospeech.</booktitle>
<contexts>
<context position="2640" citStr="Kim and Khudanpur, 2003" startWordPosition="402" endWordPosition="405">e estimated based on in-domain adaptation text (e.g. ASR hypotheses). The adapted mixture weights are then used to interpolate a topicdependent unigram LM, which is finally integrated into the background N-gram LM using marginal adaptation (Kneser et al., 1997) In this paper, we propose a framework to perform LM adaptation across languages, enabling the adaptation of a LM from one language based on the adaptation text of another language. In statistical machine translation (SMT), one approach is to apply LM adaptation on the target language based on an initial translation of input references (Kim and Khudanpur, 2003; Paulik et al., 2005). This scheme is limited by the coverage of the translation model, and overall by the quality of translation. Since this approach only allows to apply LM adaptation after translation, available knowledge cannot be applied to extend the coverage. We propose a bilingual LSA model (bLSA) for crosslingual LM adaptation that can be applied before translation. The bLSA model consists of two LSA models: one for each side of the language trained on parallel document corpora. The key property of the bLSA model is that 520 Proceedings of the 45th Annual Meeting of the Association o</context>
</contexts>
<marker>Kim, Khudanpur, 2003</marker>
<rawString>W. Kim and S. Khudanpur. 2003. LM adaptation using cross-lingual information. In Proc. of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kim</author>
<author>S Khudanpur</author>
</authors>
<title>Cross-lingual latent semantic analysis for LM. In</title>
<date>2004</date>
<booktitle>Proc. ofICASSP.</booktitle>
<contexts>
<context position="5053" citStr="Kim and Khudanpur, 2004" startWordPosition="806" endWordPosition="809">uage pairs in O(N) where N is the number of languages. Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent translation lexicons modeling Pr(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively. On the other hand, the bLSA framework models Pr(c|k) and Pr(e|k) which is different from the BiTAM model. By their different modeling nature, the bLSA model usually supports more topics than the BiTAM model. Another work by (Kim and Khudanpur, 2004) employed crosslingual LSA using singular value decomposition which concatenates bilingual documents into a single input supervector before projection. We organize the paper as follows: In Section 2, we introduce the bLSA framework including Latent Dirichlet-Tree Allocation (LDTA) (Tam and Schultz, 2007) as a correlated LSA model, bLSA training and crosslingual LM adaptation. In Section 3, we present the effect of LM adaptation on word perplexity, followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3. Section 4 describes conclusions and fuParallel document c</context>
</contexts>
<marker>Kim, Khudanpur, 2004</marker>
<rawString>W. Kim and S. Khudanpur. 2004. Cross-lingual latent semantic analysis for LM. In Proc. ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>J Peters</author>
<author>D Klakow</author>
</authors>
<title>Language model adaptation using dynamic marginals.</title>
<date>1997</date>
<booktitle>In Proc. ofEurospeech,</booktitle>
<pages>1971--1974</pages>
<contexts>
<context position="2278" citStr="Kneser et al., 1997" startWordPosition="342" endWordPosition="345">N-gram LM. This approach has been successfully applied in automatic speech recognition (ASR) (Tam and Schultz, 2006) using the Latent Dirichlet Allocation (LDA) (Blei et al., 2003). The LDA model can be viewed as a Bayesian topic mixture model with the topic mixture weights drawn from a Dirichlet distribution. For LM adaptation, the topic mixture weights are estimated based on in-domain adaptation text (e.g. ASR hypotheses). The adapted mixture weights are then used to interpolate a topicdependent unigram LM, which is finally integrated into the background N-gram LM using marginal adaptation (Kneser et al., 1997) In this paper, we propose a framework to perform LM adaptation across languages, enabling the adaptation of a LM from one language based on the adaptation text of another language. In statistical machine translation (SMT), one approach is to apply LM adaptation on the target language based on an initial translation of input references (Kim and Khudanpur, 2003; Paulik et al., 2005). This scheme is limited by the coverage of the translation model, and overall by the quality of translation. Since this approach only allows to apply LM adaptation after translation, available knowledge cannot be ap</context>
<context position="11544" citStr="Kneser et al., 1997" startWordPosition="1945" endWordPosition="1948">LSA training in a reverse manner, i.e. training an English LSA model followed by bootstrapping a Chinese LSA model. 2.3 Crosslingual LM adaptation Given a source text, we apply the E-step to estimate variational Dirichlet posterior of each node in the Dirichlet-Tree. We estimate the topic weights on the source language using the following equation: Y � γjc �δ��(k) ˆθ(CH) a P (5) k c′ γjc′ jc Then we apply the topic weights into the target LSA model to obtain an in-domain LSA marginals: (EN) ˆ(CH) βwk θk (6) We integrate the LSA marginal into the target background LM using marginal adaptation (Kneser et al., 1997) which minimizes the Kullback-Leibler divergence between the adapted LM and the background LM: Pra(w|h) aI prrb9(( W) I Prbg(w|h) (7) Likewise, LM adaptation can /take place on the source language as well due to the bi-directional nature of the bLSA framework when target-side adaptation text is available. In this paper, we focus on LM adaptation on the target language for SMT. 3 Experimental Setup We evaluated our bLSA model using the Chinese– English parallel document corpora consisting of the Xinhua news, Hong Kong news and Sina news. The combined corpora contains 67k parallel documents with</context>
</contexts>
<marker>Kneser, Peters, Klakow, 1997</marker>
<rawString>R. Kneser, J. Peters, and D. Klakow. 1997. Language model adaptation using dynamic marginals. In Proc. ofEurospeech, pages 1971–1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="19408" citStr="Papineni et al., 2002" startWordPosition="3253" endWordPosition="3256">ewhat related to the degree of mismatch between the test show and the training condition. The gap looks wider on the RFA and NTDTV shows compared to the CCTV show. 3.3 Incorporating bLSA into Spoken Language Translation To investigate the effectiveness of bLSA LM adaptation for spoken language translation, we incorporated the proposed approach into our state-of-the-art phrase-based SMT system. Translation performance was evaluated on the RT04 broadcast news evaluation set when applied to both the manual transcriptions and 1-best ASR hypotheses. During evaluation two performance metrics, BLEU (Papineni et al., 2002) and NIST, were computed. In both cases, a single English reference was used during scoring. In the transcription case the original English references were used. For the ASR case, as utterance segmentation was performed automatically, the number of sentences generated by ASR and SMT differed from the number of English references. In this case, Levenshtein alignment was used to align the translation output to the English references before scoring. 3.4 Baseline SMT Setup The baseline SMT system consisted of a non adaptive system trained using the same Chinese-English parallel document corpora us</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paulik</author>
<author>C F¨ugen</author>
<author>T Schaaf</author>
<author>T Schultz</author>
<author>S St¨uker</author>
<author>A Waibel</author>
</authors>
<title>Document driven machine translation enhanced automatic speech recognition.</title>
<date>2005</date>
<booktitle>In Proc. ofInterspeech.</booktitle>
<marker>Paulik, F¨ugen, Schaaf, Schultz, St¨uker, Waibel, 2005</marker>
<rawString>M. Paulik, C. F¨ugen, T. Schaaf, T. Schultz, S. St¨uker, and A. Waibel. 2005. Document driven machine translation enhanced automatic speech recognition. In Proc. ofInterspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Tam</author>
<author>T Schultz</author>
</authors>
<title>Unsupervised language model adaptation using latent semantic marginals.</title>
<date>2006</date>
<booktitle>In Proc. ofInterspeech.</booktitle>
<contexts>
<context position="1774" citStr="Tam and Schultz, 2006" startWordPosition="260" endWordPosition="263"> proposed approach consistently improved machine translation quality on both speech and text based adaptation. 1 Introduction Language model adaptation is crucial to numerous speech and translation tasks as it enables higherlevel contextual information to be effectively incorporated into a background LM improving recognition or translation performance. One approach is to employ Latent Semantic Analysis (LSA) to capture in-domain word unigram distributions which are then integrated into the background N-gram LM. This approach has been successfully applied in automatic speech recognition (ASR) (Tam and Schultz, 2006) using the Latent Dirichlet Allocation (LDA) (Blei et al., 2003). The LDA model can be viewed as a Bayesian topic mixture model with the topic mixture weights drawn from a Dirichlet distribution. For LM adaptation, the topic mixture weights are estimated based on in-domain adaptation text (e.g. ASR hypotheses). The adapted mixture weights are then used to interpolate a topicdependent unigram LM, which is finally integrated into the background N-gram LM using marginal adaptation (Kneser et al., 1997) In this paper, we propose a framework to perform LM adaptation across languages, enabling the a</context>
</contexts>
<marker>Tam, Schultz, 2006</marker>
<rawString>Y. C. Tam and T. Schultz. 2006. Unsupervised language model adaptation using latent semantic marginals. In Proc. ofInterspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Tam</author>
<author>T Schultz</author>
</authors>
<title>Correlated latent semantic model for unsupervised language model adaptation. In</title>
<date>2007</date>
<booktitle>Proc. ofICASSP.</booktitle>
<contexts>
<context position="5358" citStr="Tam and Schultz, 2007" startWordPosition="848" endWordPosition="851">Chinese word, target English word and the topic index respectively. On the other hand, the bLSA framework models Pr(c|k) and Pr(e|k) which is different from the BiTAM model. By their different modeling nature, the bLSA model usually supports more topics than the BiTAM model. Another work by (Kim and Khudanpur, 2004) employed crosslingual LSA using singular value decomposition which concatenates bilingual documents into a single input supervector before projection. We organize the paper as follows: In Section 2, we introduce the bLSA framework including Latent Dirichlet-Tree Allocation (LDTA) (Tam and Schultz, 2007) as a correlated LSA model, bLSA training and crosslingual LM adaptation. In Section 3, we present the effect of LM adaptation on word perplexity, followed by SMT experiments reported in BLEU on both speech and text input in Section 3.3. Section 4 describes conclusions and fuParallel document corpus Figure 1: Topic transfer in bilingual LSA model. ture works. 2 Bilingual Latent Semantic Analysis The goal of a bLSA model is to enforce a oneto-one topic correspondence between monolingual LSA models, each of which can be modeled using an LDA-style model. The role of the bLSA model is to transfer </context>
</contexts>
<marker>Tam, Schultz, 2007</marker>
<rawString>Y. C. Tam and T. Schultz. 2007. Correlated latent semantic model for unsupervised language model adaptation. In Proc. ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venugopal</author>
<author>A Zollmann</author>
<author>A Waibel</author>
</authors>
<title>Training and evaluation error minimization rules for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="21295" citStr="Venugopal et al., 2005" startWordPosition="3543" endWordPosition="3546">pproach described in (Vogel, 2005). Next, a search was performed to find the best path through the lattice, i.e. that with maximum translation-score. During search reordering was allowed on the target language side. The final translation result was that hypothesis with maximum translation-score, which is a log-linear combination of 10 scores consisting of Target LM probability, Distortion Penalty, Word-Count Penalty, Phrase-Count and six PhraseAlignment scores. Weights for each component score were optimized to maximize BLEU-score on the development set using MER optimization as described in (Venugopal et al., 2005). CCTV (CER=7.4%) 125 120 115 110 105 100 English Word Perplexity BG 4-gram +bLSA (CH reference) +bLSA (CH ASR hypo) +bLSA (EN reference) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Beta 525 SMT Target LM CCTV Translation - BLEU (NIST) ALL Quality NTDTV RFA Manual Transcription Baseline LM: 0.162 (5.212) 0.087 (3.854) 0.140 (4.859) 0.132 (5.146) bLSA (bLSA-Adapted LM): 0.164 (5.212) 0.087 (3.897) 0.143 (4.864) 0.134 (5.162) 1-best ASR Output CER (%) 7.4 25.5 13.1 14.9 Baseline LM: 0.129 (4.15) 0.051 (2.77) 0.086 (3.50) 0.095 (3.90) bLSA (bLSA-Adapted LM): 0.132 (4.16) 0.050 (2.79) 0.089 (3.53) 0.096</context>
</contexts>
<marker>Venugopal, Zollmann, Waibel, 2005</marker>
<rawString>A. Venugopal, A. Zollmann, and A. Waibel. 2005. Training and evaluation error minimization rules for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
</authors>
<title>SMT decoder dissected: Word reordering.</title>
<date>2003</date>
<booktitle>In Proc. ofICNLPKE.</booktitle>
<contexts>
<context position="20419" citStr="Vogel, 2003" startWordPosition="3414" endWordPosition="3415">on output to the English references before scoring. 3.4 Baseline SMT Setup The baseline SMT system consisted of a non adaptive system trained using the same Chinese-English parallel document corpora used in the previous experiments (Sections 3.1 and 3.2). For phrase extraction a cleaned subset of these corpora, consisting of 1M Chinese-English sentence pairs, was used. SMT decoding parameters were optimized using manual transcriptions and translations of 272 utterances from the RT04 development set (LDC2006E10). SMT translation was performed in two stages using an approach similar to that in (Vogel, 2003). First, a translation lattice was constructed by matching all possible bilingual phrase-pairs, extracted from the training corpora, to the input sentence. Phrase extraction was performed using the “PESA” (Phrase Pair Extraction as Sentence Splitting) approach described in (Vogel, 2005). Next, a search was performed to find the best path through the lattice, i.e. that with maximum translation-score. During search reordering was allowed on the target language side. The final translation result was that hypothesis with maximum translation-score, which is a log-linear combination of 10 scores con</context>
</contexts>
<marker>Vogel, 2003</marker>
<rawString>S. Vogel. 2003. SMT decoder dissected: Word reordering. In Proc. ofICNLPKE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
</authors>
<title>PESA: Phrase pair extraction as sentence splitting.</title>
<date>2005</date>
<booktitle>In Proc. of the Machine Translation Summit.</booktitle>
<contexts>
<context position="20706" citStr="Vogel, 2005" startWordPosition="3455" endWordPosition="3456"> subset of these corpora, consisting of 1M Chinese-English sentence pairs, was used. SMT decoding parameters were optimized using manual transcriptions and translations of 272 utterances from the RT04 development set (LDC2006E10). SMT translation was performed in two stages using an approach similar to that in (Vogel, 2003). First, a translation lattice was constructed by matching all possible bilingual phrase-pairs, extracted from the training corpora, to the input sentence. Phrase extraction was performed using the “PESA” (Phrase Pair Extraction as Sentence Splitting) approach described in (Vogel, 2005). Next, a search was performed to find the best path through the lattice, i.e. that with maximum translation-score. During search reordering was allowed on the target language side. The final translation result was that hypothesis with maximum translation-score, which is a log-linear combination of 10 scores consisting of Target LM probability, Distortion Penalty, Word-Count Penalty, Phrase-Count and six PhraseAlignment scores. Weights for each component score were optimized to maximize BLEU-score on the development set using MER optimization as described in (Venugopal et al., 2005). CCTV (CER</context>
</contexts>
<marker>Vogel, 2005</marker>
<rawString>S. Vogel. 2005. PESA: Phrase pair extraction as sentence splitting. In Proc. of the Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>E P Xing</author>
</authors>
<title>BiTAM: Bilingual topic admixture models for word alignment.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4602" citStr="Zhao and Xing, 2006" startWordPosition="734" endWordPosition="737">enforce the one-to-one topic correspondence. Our proposal is to share common variational Dirichlet posteriors over the topic mixture weights of a document pair in the LDA-style model. The beauty of the bLSA framework is that the model searches for a common latent topic space in an unsupervised fashion, rather than to require manual interaction. Since the topic space is language independent, our approach supports topic transfer in multiple language pairs in O(N) where N is the number of languages. Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006). Basically, the BiTAM model consists of topic-dependent translation lexicons modeling Pr(c|e, k) where c, e and k denotes the source Chinese word, target English word and the topic index respectively. On the other hand, the bLSA framework models Pr(c|k) and Pr(e|k) which is different from the BiTAM model. By their different modeling nature, the bLSA model usually supports more topics than the BiTAM model. Another work by (Kim and Khudanpur, 2004) employed crosslingual LSA using singular value decomposition which concatenates bilingual documents into a single input supervector before projectio</context>
</contexts>
<marker>Zhao, Xing, 2006</marker>
<rawString>B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual topic admixture models for word alignment. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>