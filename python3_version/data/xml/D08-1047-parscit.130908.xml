<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.988313">
A Discriminative Candidate Generator for String Transformations
</title>
<author confidence="0.787576">
Naoaki Okazaki† Yoshimasa Tsuruoka‡
</author>
<email confidence="0.899978">
okazaki@is.s.u-tokyo.ac.jp yoshimasa.tsuruoka@manchester.ac.uk
</email>
<author confidence="0.743606">
Sophia Ananiadou‡ Jun’ichi Tsujii†‡
</author>
<email confidence="0.76143">
sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp
</email>
<affiliation confidence="0.886522125">
†Graduate School of Information
Science and Technology
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
‡School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
</affiliation>
<address confidence="0.497183">
Manchester Interdisciplinary Biocentre
131 Princess Street, Manchester M1 7DN, UK
</address>
<sectionHeader confidence="0.974684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999788095238095">
String transformation, which maps a source
string s into its desirable form t*, is related
to various applications including stemming,
lemmatization, and spelling correction. The
essential and important step for string trans-
formation is to generate candidates to which
the given string s is likely to be transformed.
This paper presents a discriminative approach
for generating candidate strings. We use sub-
string substitution rules as features and score
them using an Ll-regularized logistic regres-
sion model. We also propose a procedure to
generate negative instances that affect the de-
cision boundary of the model. The advantage
of this approach is that candidate strings can
be enumerated by an efficient algorithm be-
cause the processes of string transformation
are tractable in the model. We demonstrate
the remarkable performance of the proposed
method in normalizing inflected words and
spelling variations.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999371">
String transformation maps a source string s into its
destination string t∗. In the broad sense, string trans-
formation can include labeling tasks such as part-
of-speech tagging and shallow parsing (Brill, 1995).
However, this study addresses string transformation
in its narrow sense, in which a part of a source string
is rewritten with a substring. Typical applications of
this task include stemming, lemmatization, spelling
correction (Brill and Moore, 2000; Wilbur et al.,
2006; Carlson and Fette, 2007), OCR error correc-
tion (Kolak and Resnik, 2002), approximate string
matching (Navarro, 2001), and duplicate record de-
tection (Bilenko and Mooney, 2003).
Recent studies have formalized the task in the dis-
criminative framework (Ahmad and Kondrak, 2005;
Li et al., 2006; Chen et al., 2007),
</bodyText>
<equation confidence="0.996506">
t∗ = argmax P(tIs). (1)
t∈gen(s)
</equation>
<bodyText confidence="0.999924363636363">
Here, the candidate generator gen(s) enumerates
candidates of destination (correct) strings, and the
scorer P(tIs) denotes the conditional probability of
the string t for the given s. The scorer was modeled
by a noisy-channel model (Shannon, 1948; Brill and
Moore, 2000; Ahmad and Kondrak, 2005) and max-
imum entropy framework (Berger et al., 1996; Li et
al., 2006; Chen et al., 2007).
The candidate generator gen(s) also affects the
accuracy of the string transformation. Previous stud-
ies of spelling correction mostly defined gen(s),
</bodyText>
<equation confidence="0.995445">
gen(s) = It I dist(s, t) &lt; 61. (2)
</equation>
<bodyText confidence="0.999962153846154">
Here, the function dist(s, t) denotes the weighted
Levenshtein distance (Levenshtein, 1966) between
strings s and t. Furthermore, the threshold 6 requires
the distance between the source string s and a can-
didate string t to be less than 6.
The choice of dist(s, t) and 6 involves a tradeoff
between the precision, recall, and training/tagging
speed of the scorer. A less restrictive design of these
factors broadens the search space, but it also in-
creases the number of confusing candidates, amount
of feature space, and computational cost for the
scorer. Moreover, the choice is highly dependent on
the target task. It might be sufficient for a spelling
</bodyText>
<page confidence="0.979441">
447
</page>
<note confidence="0.9620675">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 447–456,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999924066666667">
correction program to gather candidates from known
words, but a stemmer must handle unseen words ap-
propriately. The number of candidates can be huge
when we consider transformations from and to un-
seen strings.
This paper addresses these challenges by explor-
ing the discriminative training of candidate genera-
tors. More specifically, we build a binary classifier
that, when given a source string s, decides whether
a candidate t should be included in the candidate set
or not. This approach appears straightforward, but it
must resolve two practical issues. First, the task of
the classifier is not only to make a binary decision
for the two strings s and t, but also to enumerate a
set of positive strings for the string s,
</bodyText>
<equation confidence="0.998784">
gen(s) = {t  |predict(s, t) = 1}. (3)
</equation>
<bodyText confidence="0.999961861111111">
In other words, an efficient algorithm is necessary
to find a set of strings with which the classifier
predict(s, t) yields positive labels for the string s.
Another issue arises when we prepare a training
set. A discriminative model requires a training set
in which each instance (pair of strings) is annotated
with a positive or negative label. Even though some
existing resources (e.g., inflection table and query
log) are available for positive instances, such re-
sources rarely contain negative instances. Therefore,
we must generate negative instances that are effec-
tive for discriminative training.
To address the first issue, we design features that
express transformations from a source string s to its
destination string t. Feature selection and weight-
ing are performed using an L1-regularized logistic
regression model, which can find a sparse solution
to the classification model. We also present an al-
gorithm that utilizes the feature weights to enumer-
ate candidates of destination strings efficiently. We
deal with the second issue by generating negative
instances from unlabeled instances. We describe a
procedure to choose negative instances that affect
the decision boundary of the classifier.
This paper is organized as follows. Section 2 for-
malizes the task of the candidate generator as a bi-
nary classification modeled by logistic regression.
Features for the classifier are designed using the
rules of substring substitution. Therefore, we can
obtain, efficiently, candidates of destination strings
and negative instances for training. Section 3 re-
ports the remarkable performance of the proposed
method in various applications including lemmati-
zation, spelling normalization, and noun derivation.
We briefly review previous work in Section 4, and
conclude this paper in Section 5.
</bodyText>
<sectionHeader confidence="0.963928" genericHeader="method">
2 Candidate generator
</sectionHeader>
<subsectionHeader confidence="0.996755">
2.1 Candidate classification model
</subsectionHeader>
<bodyText confidence="0.9980905">
In this section, we first introduce a binary classifier
that yields a label y ∈ {0, 1} indicating whether a
candidate t should be included in the candidate set
(1) or not (0), given a source string s. We express
the conditional probability P(y|s, t) using a logistic
regression model,
</bodyText>
<equation confidence="0.997498666666667">
1
P(1|s, t) = 1 + exp (−ATF(s, t)), (4)
P(0|s, t) = 1 − P(1|s, t). (5)
</equation>
<bodyText confidence="0.999983">
In these equations, F = {f1,..., fK} denotes a vec-
tor of the Boolean feature functions; K is the num-
ber of feature functions; and A = {λ1, ..., λK}
presents a weight vector of the feature functions.
We obtain the following decision rule to choose
the most probable label y* for a given pair hs, ti,
</bodyText>
<equation confidence="0.9311484">
t) &gt;
={t|P(1|s,
P(0|s, t)}
= {t  |AT F (s, t) &gt; 0}. (7)
s
</equation>
<bodyText confidence="0.923801">
Finally, given a source string s, the generator func-
tion gen(s) is defined to collect all stri
ngs to which
the classifier assigns positive labels:
gen(s)
</bodyText>
<subsectionHeader confidence="0.999176">
2.2 Substitution rules as feature
</subsectionHeader>
<bodyText confidence="0.999152111111111">
The binary classifier can include any arbitrary fea-
ture. This is exemplified by the Levenshtein dis-
tance and distributional similarity (Lee, 1999) be-
tween two strings s and t. These features can im-
prove the classification accuracy, but it is unrealistic
to compute these features for every possible stri
ng,
as in equation 7. For that reason, we specifically
examine substitution rules, with which the process
</bodyText>
<figure confidence="0.615744666666667">
y* = argmax P(y|s, t) = � 1 (ATF (s,t) &gt; 0) .
yE{0,1} 0 (otherwise)
(6)
</figure>
<page confidence="0.984986">
448
</page>
<listItem confidence="0.994868833333333">
(1) S: ^oestrogen$
t: ^estrogen$
(2) S: ^anaemia$
t: ^anemia$
(3) S: ^studies$
t: ^study$
</listItem>
<figureCaption confidence="0.998636">
Figure 1: Generating substitution rules.
</figureCaption>
<bodyText confidence="0.993922916666667">
of transforming a source string s into its destination
form t is tractable.
In this study, we assume that every string has a
prefix ‘-’ and postfix ‘$’, which indicate the head
and tail of a string. A substitution rule r = (α, β)
replaces every occurrence of the substring α in a
source string into the substring β. Assuming that a
string s can be transformed into another string t with
a single substitution operation, substitution rules ex-
press the different portion between strings s and t.
Equation 8 defines a binary feature function with
a substitution rule between two strings s and t,
</bodyText>
<equation confidence="0.530448">
(8)
</equation>
<bodyText confidence="0.99995247826087">
We allow multiple substitution rules for a given pair
of strings. For instance, substitution rules (‘a’,
‘’), (‘na’, ‘n’), (‘ae’, ‘e’), (‘nae’, ‘ne’), etc.
form feature functions that yield 1 for strings s =
‘-anaemia$’ and t = ‘-anemia$’. Equation
6 produces a decision based on the sum of feature
weights, or scores of substitution rules, representing
the different portions between s and t.
Substitution rules for the given two strings s and
t are obtained as follows. Let l denote the longest
common prefix between strings s and t, and r the
longest common postfix. We define cs as the sub-
string in s that is not covered by the longest common
prefix l and postfix r, and define ct for t analogously.
In other words, strings s and t are divided into three
regions, lcsr and lctr, respectively. For strings s =
‘-anaemia$’ and t = ‘-anemia$’ in Figure 1
(2), we obtain cs = ‘a’ and ct = ‘’ because l =
‘-an’ and r = ‘emia$’.
Because substrings cs and ct express different
portions between strings s and t, we obtain the mini-
mum substitution rule (cs, ct), which can convert the
string s into t by replacing substrings cs in s with
ct; the minimum substitution rule for the same ex-
ample is (‘a’, ‘’). However, replacing letters ‘a’
in ‘-anaemia$’ into empty letters does not pro-
duce the correct string ‘-anemia$’ but ‘-nemi$’.
Furthermore, the rule might be inappropriate for ex-
pressing string transformation because it always re-
moves the letter ‘a’ from every string.
Therefore, we also obtain expanded substitution
rules, which insert postfixes of l to the head of min-
imum substitution rules, and/or append prefixes of
r to the rules. For example, we find an expanded
substitution rule (‘na’, ‘n’), by inserting a postfix
of l = ‘-an’ to the head of the minimum substitu-
tion rule (‘a’, ‘’); similarly, we obtain an expanded
substitution rule (‘ae’, ‘e’), by appending a prefix
of r = ‘emia$’ to the tail of the rule (‘a’, ‘’).
Figure 1 displays examples of substitution rules
(the right side) for three pairs of strings (the left
side). Letters in blue, green, and red respectively
represent the longest common prefixes, longest com-
mon postfixes, and different portions. In this study,
we expand substitution rules such that the number of
letters in rules is does not pass a threshold θ1.
</bodyText>
<subsectionHeader confidence="0.998253">
2.3 Parameter estimation
</subsectionHeader>
<bodyText confidence="0.99982">
Given a training set that consists of N instances,
D = ((s(1), t(1), y(1)), ..., (s(N), t(N), y(N))), we
optimize the feature weights in the logistic regres-
sion model by maximizing the log-likelihood of the
conditional probability distribution,
</bodyText>
<equation confidence="0.990183666666667">
N
LA = log P(y(i)|s(i), t(i)). (9)
i=1
</equation>
<bodyText confidence="0.9994305">
The partial derivative of the log-likelihood with re-
spect to a feature weight λk is given as equation 10,
</bodyText>
<equation confidence="0.980368333333333">
{ I
y(i) − P(1|s(i),t(i)) fk(s(i),t(i)).
(10)
</equation>
<bodyText confidence="0.991116">
The maximum likelihood estimation (MLE) is
known to suffer from overfitting the training set. The
</bodyText>
<footnote confidence="0.788923666666667">
1The number of letters for a substitution rule r = (α„,3) is
defined as the sum of the quantities of letters in α and ,3, i.e.,
|α |+ 1,31. We determined the threshold B = 12 experimentally.
</footnote>
<bodyText confidence="0.627077">
(&apos;o&apos;, &apos;&apos;), (&apos;^o&apos;, &apos;^&apos;), (&apos;oe&apos;, &apos;e&apos;),
(&apos;^oe&apos;, &apos;^e&apos;), (&apos;^oes&apos;, &apos;^es&apos;), ...
(&apos;a&apos;, &apos;&apos;), (&apos;na&apos;, &apos;n&apos;), (&apos;ae&apos;, &apos;e&apos;),
(&apos;ana&apos;, &apos;an&apos;), (&apos;nae&apos;, &apos;ne&apos;), (&apos;aem&apos;, &apos;em&apos;),
(&apos;ies&apos;, &apos;y&apos;), (&apos;dies&apos;, &apos;dy&apos;), (&apos;ies$&apos;, &apos;y$&apos;),
(&apos;udies&apos;, &apos;udy&apos;), (&apos;dies$&apos;, &apos;dy$&apos;), ...
f1 (rule rk can convert s into t)
</bodyText>
<equation confidence="0.9696582">
fk(s, t) =
.
0 (otherwise)
∂LA N
∂λk i=1
</equation>
<page confidence="0.990585">
449
</page>
<bodyText confidence="0.999982722222222">
common approach for addressing this issue is to use
the maximum a posteriori (MAP) estimation, intro-
ducing a regularization term of the feature weights
A, i.e., a penalty on large feature weights. In addi-
tion, the generation algorithm of substitution rules
might produce inappropriate rules that transform a
string incorrectly, or overly specific rules that are
used scarcely. Removing unnecessary substitution
rules not only speeds up the classifier but also the
algorithm for candidate generation, as presented in
Section 2.4.
In recent years, L1 regularization has received in-
creasing attention because it produces a sparse so-
lution of feature weights in which numerous fea-
ture weights are zero (Tibshirani, 1996; Ng, 2004).
Therefore, we regularize the log-likelihood with the
L1 norm of the weight vector A and define the final
form the objective function to be minimized as
</bodyText>
<equation confidence="0.998006">
EA = −LA + |A |. (11)
Q
</equation>
<bodyText confidence="0.992898375">
Here, Q is a parameter to control the effect of L1
regularization; the smaller the value we set to Q,
the more features the MAP estimation assigns zero
weights to: it removes a number of features from the
model. Equation 11 is minimized using the Orthant-
Wise Limited-memory Quasi-Newton (OW-LQN)
method (Andrew and Gao, 2007) because the second
term of equation 11 is not differentiable at Ak = 0.
</bodyText>
<subsectionHeader confidence="0.995667">
2.4 Candidate generation
</subsectionHeader>
<bodyText confidence="0.969997769230769">
The advantage of our feature design is that we can
enumerate strings to which the classifier is likely to
assign positive labels. We start by observing the nec-
essary condition for t in equation 7,
ATF(s,t)&gt;0==&gt;.]k: fk(s,t) = 1 n Ak &gt; 0.
The classifier might assign a positive label to strings
s and t when at least one feature function whose
weight is positive can transform s to t.
Let R+ be a set of substitution rules to which
MAP estimation has assigned positive feature
weights. Because each feature corresponds to a sub-
stitution rule, we can obtain gen(s) for a given string
s by application of every substitution rule r E R+,
</bodyText>
<figure confidence="0.703716052631579">
gen(s) = {r(s)  |r E R+ n ATF (s,r(s)) &gt; 01.
Input: s = (s1, ..., sl): an input string s (series of letters)
Input: D: a trie dictionary containing positive features
Output: T: gen(s)
1 T=11;
2 U=11;
3 foreach i E (1, ..., |s|) do
4 F +— D.prefix search(s, i);
5 foreach f E F do
6 if f E� U then
7 t +— f.apply(s);
8 if classify(s, t) = 1 then
9 add t to T;
10 end
11 add f to U;
12 end
13 end
14 end
15 return T;
</figure>
<figureCaption confidence="0.837559">
Algorithm 1: A pseudo-code for gen(s).
</figureCaption>
<bodyText confidence="0.999860166666667">
Here, r(s) presents the string to which the substitu-
tion rule r transforms the source string s. We can
compute gen(s) with a small computational cost if
the MAP estimation with L1 regularization reduces
the number of active features.
Algorithm 1 represents a pseudo-code for obtain-
ing gen(s). To search for positive substitution rules
efficiently, the code stores a set of rules in a trie
structure. In line 4, the code obtains a set of positive
substitution rules F that can rewrite substrings start-
ing at offset #i in the source string s. For each rule
f E F, we obtain a candidate string t by application
of the substitution rule f to the source string s (line
7). The candidate string t is qualified to be included
in gen(s) when the classifier assigns a positive label
to strings s and t (lines 8 and 9). Lines 6 and 11 pre-
vent the algorithm from repeating evaluation of the
same substitution rule.
</bodyText>
<subsectionHeader confidence="0.996192">
2.5 Generating negative instances
</subsectionHeader>
<bodyText confidence="0.9998786">
The parameter estimation requires a training set D
in which each instance (pair of strings) is annotated
with a positive or negative label. Negative instances
(counter examples) are essential for penalizing in-
appropriate substitution rules, e.g. (‘a’, ‘’). Even
though some existing resources (e.g. verb inflection
table) are available for positive instances, such re-
sources rarely contain negative instances.
A common approach for handling this situation
is to assume that every pair of strings in a resource
</bodyText>
<page confidence="0.97898">
450
</page>
<table confidence="0.9892515">
Input: D+ = [(s1, t1),..., (sj, tj)]: positive instances
Input: V : a suffix array of all strings (vocabulary)
Output: D−: negative instances
Output: R: substitution rules (features)
1 D− = [];
2 R=1};
3 foreach d E D+ do
4 foreach r E features(d) do
5 add r to R;
6 end
7 end
8 foreach r E R do
9 S +— V .search(r.src);
10 foreach s E S do
11 t +— r.apply(s);
12 if (s, t) E� D+ then
13 if t E V then
14 append (s, t) to D−;
15 end
16 end
17 end
18 end
19 return D−, R;
Algorithm 2: Generating negative instances.
</table>
<bodyText confidence="0.997747115384615">
is a negative instance; however, negative instances
amount to ca. V (V − 1)/2, where V represents the
total number of strings. Moreover, substitution rules
expressing negative instances are innumerable and
sparse because the different portions are peculiar to
individual negative instances. For instance, the min-
imum substitution rule for unrelated words anaemia
and around is (‘naemia’, ‘round’), but the rule
cannot be too specific to generalize the conditions
for other negative instances.
In this study, we generate negative instances so
that they can penalize inappropriate rules and settle
the decision boundary of the classifier. This strat-
egy is summarized as follows. We consider every
pair of strings as candidates for negative instances.
We obtain substitution rules for the pair using the
same algorithm as that described in Section 2.2 if a
string pair is not included in the dictionary (i.e., not
in positive instances). The pair is used as a nega-
tive instance only when any substitution rule gener-
ated from the pair also exists in the substitution rules
generated from positive instances.
Algorithm 2 presents the pseudo-code that imple-
ments the strategy for generating negative instances
efficiently. First, we presume that we have positive
instances D+ = [(s1, t1),..., (sl, tl)] and unlabeled
</bodyText>
<table confidence="0.9813556">
Table Description # Entries
LRSPL Spelling variants 90,323
LRNOM Nominalizations (derivations) 14,029
LRAGR Agreement and inflection 910,854
LRWD Word index (vocabulary) 850,236
</table>
<tableCaption confidence="0.995524">
Table 1: Excerpt of tables in the SPECIALIST Lexicon.
</tableCaption>
<table confidence="0.9999225">
Data set # + # - # Rules
Orthography 15,830 33,296 11,098
Derivation 12,988 85,928 5,688
Inflection 113,215 124,747 32,278
</table>
<tableCaption confidence="0.998496">
Table 2: Characteristics of datasets.
</tableCaption>
<bodyText confidence="0.9998330625">
strings V . For example, positive instance D+ repre-
sent orthographic variants, and unlabeled strings V
include all possible words (vocabulary). We insert
the vocabulary into a suffix array, which is used to
locate every occurrence of substrings in V .
The algorithm first generates substitution rules R
only from positive instances D+ (lines 3 to 7). For
each substitution rule r E R, we enumerate known
strings S that contain the source substring r.src (line
9). We apply the substitution rule to each string s E
S and obtain its destination string t (line 11). If the
pair of strings (s, t) is not included in D+ (line 12),
and if the destination string t is known (line 13), the
substitution rule r might associate incorrect strings
s and t, which do not exist in D+. Therefore, we
insert the pair to the negative set D− (line 14).
</bodyText>
<sectionHeader confidence="0.998713" genericHeader="method">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.945851">
3.1 Experiments
</subsectionHeader>
<bodyText confidence="0.999649833333333">
We evaluated the candidate generator using three
different tasks: normalization of orthographic vari-
ants, noun derivation, and lemmatization. The
datasets for these tasks were obtained from the
UMLS SPECIALIST Lexicon2, a large lexicon that
includes both commonly occurring English words
and biomedical vocabulary. Table 1 displays the list
of tables in the SPECIALIST Lexicon that were used
in our experiments. We prepared three datasets, Or-
thography, Derivation, and Inflection.
The Orthography dataset includes spelling vari-
ants (e.g., color and colour) in the LRSPL table. We
</bodyText>
<footnote confidence="0.97217">
2UMLS SPECIALIST Lexicon:
http://specialist.nlm.nih.gov/
</footnote>
<page confidence="0.998603">
451
</page>
<bodyText confidence="0.999970047619048">
chose entries as positive instances in which spelling
variants are caused by (case-insensitive) alphanu-
meric changes3. The Derivation dataset was built di-
rectly from the LRNOM table, which includes noun
derivations such as abandon —* abandonment. The
LRAGR table includes base forms and their inflec-
tional variants of nouns (singular and plural forms),
verbs (infinitive, third singular, past, past participle
forms, etc), and adjectives/adverbs (positive, com-
parative, and superlative forms). For the Inflection
dataset, we extracted the entries in which inflec-
tional forms differ from their base forms4, e.g., study
� studies.
For each dataset, we applied the algorithm de-
scribed in Section 2.5 to generate substitution rules
and negative instances. Table 2 shows the number of
positive instances (# +), negative instances (# -), and
substitution rules (# Rules). We evaluated the per-
formance of the proposed method in two different
goals of the tasks: classification (Section 3.2) and
normalization (Section 3.3).
</bodyText>
<subsectionHeader confidence="0.99828">
3.2 Experiment 1: Candidate classification
</subsectionHeader>
<bodyText confidence="0.999974714285714">
In this experiment, we measured the performance
of the classification task in which pairs of strings
were assigned with positive or negative labels.
We trained and evaluated the proposed method
by performing ten-fold cross validation on each
dataset5. Eight baseline systems were prepared
for comparison: Levenshtein distance (LD), nor-
malized Levenshtein distance (NLD), Dice coef-
ficient on letter bigrams (DICE) (Adamson and
Boreham, 1974), Longest Common Substring Ra-
tio (LCSR) (Melamed, 1999), Longest Common
Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s
stemmer (Porter, 1980), Morpha (Minnen et al.,
2001), and CST’s lemmatiser (Dalianis and Jonge-
</bodyText>
<footnote confidence="0.597217909090909">
3LRSPL table includes trivial spelling variants that can be
handled using simple character/string operations. For example,
the table contains spelling variants related to case sensitivity
(e.g., deg and Deg) and symbols (e.g., Feb and Feb.).
4LRAGR table also provides agreement information even
when word forms do not change. For example, the table con-
tains an entry indicating that the first-singular present form of
the verb study is study, which might be readily apparent to En-
glish speakers.
5We determined the regularization parameter v = 5 experi-
mentally. Refer to Figure 2 for the performance change.
</footnote>
<bodyText confidence="0.997029463414634">
jan, 2006)6.
The five systems LD, NLD, DICE, LCSR, and
PREFIX employ corresponding metrics of string
distance or similarity. Each system assigns a posi-
tive label to a given pair of strings (s, t) if the dis-
tance/similarity of strings s and t is smaller/larger
than the threshold 6 (refer to equation 2 for distance
metrics). The threshold of each system was chosen
so that the system achieves the best F1 score.
The remaining three systems assign a positive la-
bel only if the system transforms the strings s and
t into the identical string. For example, a pair of
two words studies and study is classified as positive
by Porter’s stemmer, which yields the identical stem
studi for these words. We trained CST’s lemmatiser
for each dataset to obtain flex patterns that are used
for normalizing word inflections.
To examine the performance of the Li-
regularized logistic regression as a discriminative
model, we also built two classifiers based on the
Support Vector Machine (SVM). These SVM
classifiers were implemented by the SVMp,,f7 on
a linear kernel8. An SVM classifier employs the
same feature set (substitution rules) as the proposed
method so that we can directly compare the Li-
regularized logistic regression and the linear-kernel
SVM. Another SVM classifier incorporates the five
string metrics; this system can be considered as our
reproduction of the discriminative string similarity
proposed by Bergsma and Kondrak (2007).
Table 3 reports the precision (P), recall (R), and
F1 score (F1) based on the number of correct de-
cisions for positive instances. The proposed method
outperformed the baseline systems, achieving 0.919,
0.888, and 0.984 of F1 scores, respectively. Porter’s
stemmer worked on the Inflection set, but not on
the Orthography set, which is beyond the scope of
the stemming algorithms. CST’s lemmatizer suf-
fered from low recall on the Inflection set because
it removed suffixes of base forms, e.g., (cloning,
clone) —* (clone, clo). Morpha and CST’s lemma-
</bodyText>
<footnote confidence="0.997914285714286">
6We used CST’s lemmatiser version 2.13:
http://www.cst.dk/online/lemmatiser/uk/
index.html
7SVM for Multivariate Performance Measures (SVM-&amp;quot;&apos;&apos;f):
http://svmlight.joachims.org/svm_perf.html
8We determined the parameter C = 500 experimentally; it
controls the tradeoff between training error and margin.
</footnote>
<page confidence="0.98986">
452
</page>
<table confidence="0.999911428571429">
System Orthography Derivation Inflection
P R F1 P R F1 P R F1
Levenshtein distance (S = 1) .319 .871 .467 .004 .006 .005 .484 .679 .565
Levenshtein distance .323 .999 .488 .131 1.00 .232 .479 .988 .646
Normalized Levenshtein distance .441 .847 .580 .133 .990 .235 .598 .770 .673
Dice coefficient (letter bigram) .401 .918 .558 .137 .984 .240 .476 1.00 .645
LCSR .322 1.00 .487 .156 .841 .263 .476 1.00 .645
PREFIX .418 .927 .576 .140 .943 .244 .476 1.00 .645
Porter stemmer (Porter, 1980) .084 .074 .079 .197 .846 .320 .926 .839 .881
Morpha (Minnen et al., 2001) .009 .007 .008 .012 .022 .016 .979 .836 .902
CST’s lemmatiser (Dalianis et al. 2006) .119 .008 .016 .383 .682 .491 .821 .176 .290
Proposed method .941 .898 .919 .896 .880 .888 .985 .986 .984
Substitution rules trained with SVM .943 .890 .916 .894 .886 .890 .980 .987 .983
+ LD, NLD, DICE, LCSR, PREFIX .946 .906 .926 .894 .886 .890 .980 .987 .983
</table>
<tableCaption confidence="0.995988">
Table 3: Performance of candidate classification
</tableCaption>
<table confidence="0.999644545454546">
Rank Src Dst Weight Examples
1 uss us 9.81 focussing
2 aev ev 9.56 mediaeval
3 aen en 9.53 ozaena
4 iae$ ae$ 9.44 gadoviae
5 nni ni 9.16 prorennin
6 nne ne 8.84 connexus
7 our or 8.54 colour
8 aea ea 8.31 paean
9 aeu eu 8.22 stomodaeum
10 ooll ool 7.79 woollen
</table>
<tableCaption confidence="0.999672">
Table 4: Feature weights for the Orthography set
</tableCaption>
<bodyText confidence="0.999917571428571">
tizer were not designed for orthographic variants and
noun derivations.
Levenshtein distance (S = 1) did not work for
the Derivation set because noun derivations often
append two or more letters (e.g., happy -* happi-
ness). No string similarity/distance metrics yielded
satisfactory results. Some metrics obtained the best
F1 scores with extreme thresholds only to classify
every instance as positive. These results imply the
difficulty of the string metrics for the tasks.
The Li-regularized logistic regression was com-
parable to the SVM with linear kernel in this exper-
iment. However, the presented model presents the
advantage that it can reduce the number of active
features (features with non-zero weights assigned);
the Li regularization can remove 74%, 48%, and
82% of substitution rules in each dataset. The
performance improvements by incorporating string
metrics as features were very subtle (less than 0.7%).
What is worse, the distance/similarity metrics do not
specifically derive destination strings to which the
classifier is likely to assign positive labels. There-
fore, we can no longer use the efficient algorithm
as a candidate generator (in Section 2.4) with these
features.
Table 4 demonstrates the ability of our approach
to obtain effective features; the table shows the top
10 features with high weights assigned for the Or-
thography data. An interesting aspect of the pro-
posed method is that the process of the orthographic
variants is interpretable through the feature weights.
Figure 2 shows plots of the F1 scores (y-axis) for
the Inflection data when we change the number of
active features (x-axis) by controlling the regular-
ization parameter a from 0.001 to 100. The larger
the value we set for a, the better the classifier per-
forms, generally, with more active features. In ex-
treme cases, the number of active features drops to
97 with a = 0.01; nonetheless, the classifier still
achieves 0.961 of the F1 score. The result suggests
that a small set of substitution rules can accommo-
date most cases of inflectional variations.
</bodyText>
<subsectionHeader confidence="0.998998">
3.3 Experiment 2: String transformation
</subsectionHeader>
<bodyText confidence="0.999895142857143">
The second experiment examined the performance
of the string normalization tasks formalized in equa-
tion 1. In this task, a system was given a string s and
was required to yield either its transformed form t*
(s =� t*) or the string s itself when the transforma-
tion is unnecessary for s. The conditional probabil-
ity distribution (scorer) in equation 1 was modeled
</bodyText>
<page confidence="0.99802">
453
</page>
<table confidence="0.9995654">
System Orthography Derivation Inflection XTAG morph 1.5
P R F1 P R F1 P R F1 P R F1
Morpha .078 .012 .021 .233 .016 .029 .435 .682 .531 .830 .587 .688
CST’s lemmatiser .135 .160 .146 .378 .732 .499 .367 .762 .495 .584 .589 .587
Proposed method .859 .823 .841 .979 .981 .980 .973 .979 .976 .837 .816 .827
</table>
<tableCaption confidence="0.999713">
Table 5: Performance of string transformation
</tableCaption>
<figure confidence="0.9836875">
0 1000 2000 3000 4000 5000 6000 7000
Number of active features (with non-zero weights)
</figure>
<figureCaption confidence="0.999864">
Figure 2: Number of active features and performance.
</figureCaption>
<bodyText confidence="0.999924277777778">
by the maximum entropy framework. Features for
the maximum entropy model consist of: substitution
rules between strings s and t, letter bigrams and tri-
grams in s, and letter bigrams and trigrams in t.
We prepared four datasets, Orthography, Deriva-
tion, Inflection, and XTAG morphology. Each
dataset is a list of string pairs (s, t) that indicate
the transformation of the string s into t. A source
string s is identical to its destination string t when
string s should not be changed. These instances
correspond to the case where string s has already
been lemmatized. For each string pair (s, t) in LR-
SPL9, LRNOM, and LRAGR tables, we generated
two instances (s, t) and (t, t). Consequently, a sys-
tem is expected to leave the string t unchanged. We
also used XTAG morphology10 to perform a cross-
domain evaluation of the lemmatizer trained on the
Inflection dataset11. The entries in XTAG morphol-
</bodyText>
<footnote confidence="0.9903758">
9We define that s precedes t in dictionary order.
10XTAG morphology database 1.5:
ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.
5/morph-1.5.tar.gz
11We found that XTAG morphology contains numerous in-
</footnote>
<bodyText confidence="0.999336434782609">
ogy that also appear in the Inflection dataset were
39,130 out of 317,322 (12.3 %). We evaluated
the proposed method and CST’s lemmatizer by per-
forming ten-fold cross validation.
Table 5 reports the performance based on the
number of correct transformations. The proposed
method again outperformed the baseline systems
with a wide margin. It is noteworthy that the pro-
posed method can accommodate morphological in-
flections in the XTAG morphology corpus with no
manual tuning or adaptation.
Although we introduced no assumptions about
target tasks (e.g. a known vocabulary), the aver-
age number of positive substitution rules relevant
to source strings was as small as 23.9 (in XTAG
morphology data). Therefore, the candidate gen-
erator performed 23.9 substitution operations for a
given string. It applied the decision rules (equa-
tion 7) 21.3 times, and generated 1.67 candidate
strings per source string. The experimental results
described herein demonstrated that the candidate
generator was modeled successfully by the discrim-
inative framework.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.939767933333333">
The task of string transformation has a long history
in natural language processing and information re-
trieval. As described in Section 1, this task is re-
lated closely to various applications. Therefore, we
specifically examine several prior studies that are
relevant to this paper in terms of technical aspects.
Some researchers have reported the effectiveness
of the discriminative framework of string similarity.
MaCallum et al. (2005) proposed a method to train
the costs of edit operations using Conditional Ran-
dom Fields (CRFs). Bergsma and Kondrak (2007)
correct comparative and superlative adjectives, e.g., unpopular
� unpopularer --+ unpopularest and refundable --+ refundabler
� refundablest. Therefore, we removed inflection entries for
comparative and superlative adjectives from the dataset.
</bodyText>
<figure confidence="0.996331375">
Spelling variation
F1 score 0.99
0.985
0.98
0.975
0.97
0.965
0.96
</figure>
<page confidence="0.998667">
454
</page>
<bodyText confidence="0.999884454545454">
presented an alignment-based discriminative string
similarity. They extracted features from substring
pairs that are consistent to a character-based align-
ment of two strings. Aramaki et al. (2008) also used
features that express the different segments of the
two strings. However, these studies are not suited for
a candidate generator because the processes of string
transformations are intractable in their discrimina-
tive models.
Dalianis and Jongejan (2006) presented a lem-
matiser based on suffix rules. Although they pro-
posed a method to obtain suffix rules from a training
data, the method did not use counter-examples (neg-
atives) for reducing incorrect string transformations.
Tsuruoka et al. (2008) proposed a scoring method
for discovering a list of normalization rules for dic-
tionary look-ups. However, their objective was to
transform given strings, so that strings (e.g., studies
and study) referring to the same concept in the dic-
tionary are mapped into the same string (e.g., stud);
in contrast, this study maps strings into their destina-
tion strings that were specified by the training data.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999806">
We have presented a discriminative approach for
generating candidates for string transformation.
Unlike conventional spelling-correction tasks, this
study did not assume a fixed set of destination
strings (e.g. correct words), but could even generate
unseen candidate strings. We used an Li-regularized
logistic regression model with substring-substitution
features so that candidate strings for a given string
can be enumerated using the efficient algorithm. The
results of experiments described herein showed re-
markable improvements and usefulness of the pro-
posed approach in three tasks: normalization of or-
thographic variants, noun derivation, and lemmati-
zation.
The method presented in this paper allows only
one region of change in string transformation. A
natural extension of this study is to handle mul-
tiple regions of changes for morphologically rich
languages (e.g. German) and to handle changes
at the phrase/term level (e.g., “estrogen receptor”
and “receptor of oestrogen”). Another direction
would be to incorporate the methodologies for semi-
supervised machine learning to accommodate situa-
tions in which positive instances and/or unlabeled
strings are insufficient.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99986025">
This work was partially supported by Grants-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and for Solution-Oriented Research for Sci-
ence and Technology (JST, Japan).
</bodyText>
<sectionHeader confidence="0.996899" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990993075">
George W. Adamson and Jillian Boreham. 1974. The
use of an association measure based on character struc-
ture to identify semantically related pairs of words and
document titles. Information Storage and Retrieval,
10(7-8):253–260.
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning
a spelling error model from search query logs. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing (HLT-EMNLP 2005), pages 955–962.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of Ll-regularized log-linear models. In Proceed-
ings of the 24th International Conference on Machine
Learning (ICML 2007), pages 33–40.
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko
Ohe. 2008. Orthographic disambiguation incorporat-
ing transliterated probability. In Proceedings of the
Third International Joint Conference on Natural Lan-
guage Processing (IJCNLP 2008), pages 48–55.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39–71.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
656–663.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adap-
tive duplicate detection using learnable string simi-
larity measures. In Proceedings of the ninth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining (KDD 2003), pages 39–48.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on the As-
sociation for Computational Linguistics (ACL 2000),
pages 286–293.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
</reference>
<page confidence="0.98724">
455
</page>
<reference confidence="0.999877310810811">
in part-of-speech tagging. Computational Linguistics,
21(4):543–565.
Andrew Carlson and Ian Fette. 2007. Memory-based
context-sensitive spelling correction at web scale. In
Proceedings of the Sixth International Conference on
Machine Learning and Applications (ICMLA 2007),
pages 166–171.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improv-
ing query spelling correction using web search results.
In Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 181–189.
Hercules Dalianis and Bart Jongejan. 2006. Hand-
crafted versus machine-learned inflectional rules: The
euroling-siteseeker stemmer and cst’s lemmatiser. In
In Proceedings of the 6th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 663–666.
Okan Kolak and Philip Resnik. 2002. OCR error correc-
tion using a noisy channel model. In Proceedings of
the second international conference on Human Lan-
guage Technology Research (HLT 2002), pages 257–
262.
Grzegorz Kondrak. 2005. Cognates and word alignment
in bitexts. In Proceedings of the Tenth Machine Trans-
lation Summit (MT Summit X), pages 305–312.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 1999),
pages 25–32.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707–710.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association for
Computational Linguistics (Coling-ACL 2006), pages
1025–1032.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceedings
of the 21st Conference on Uncertainty in Artificial In-
telligence (UAI 2005), pages 388–395.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107–130.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207–223.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys (CSUR),
33(1):31–88.
Andrew Y. Ng. 2004. Feature selection, L1 vs. L2 regu-
larization, and rotational invariance. In Proceedings of
the twenty-first international conference on Machine
learning (ICML 2004), pages 78–85.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27(3):379–423.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267–288.
Yoshimasa Tsuruoka, John McNaught, and Sophia Ana-
niadou. 2008. Normalizing biomedical terms by min-
imizing ambiguity and variability. BMC Bioinformat-
ics, Suppl 3(9):S2.
W. John Wilbur, Won Kim, and Natalie Xie. 2006.
Spelling correction in the PubMed search engine. In-
formation Retrieval, 9(5):543–564.
</reference>
<page confidence="0.999083">
456
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.369969">
<title confidence="0.998112">A Discriminative Candidate Generator for String Transformations</title>
<abstract confidence="0.4779625">okazaki@is.s.u-tokyo.ac.jp yoshimasa.tsuruoka@manchester.ac.uk sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp</abstract>
<affiliation confidence="0.988874">School of Science and University of</affiliation>
<address confidence="0.9385815">7-3-1 Hongo, Tokyo 113-8656, Japan</address>
<affiliation confidence="0.993788">of Computer University of National Centre for Text Mining Manchester Interdisciplinary</affiliation>
<address confidence="0.995149">131 Princess Street, Manchester M1 7DN, UK</address>
<abstract confidence="0.999529772727273">which maps a source s into its desirable form is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score using an logistic regression model. We also propose a procedure to generate negative instances that affect the decision boundary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>George W Adamson</author>
<author>Jillian Boreham</author>
</authors>
<title>The use of an association measure based on character structure to identify semantically related pairs of words and document titles.</title>
<date>1974</date>
<journal>Information Storage and Retrieval,</journal>
<pages>10--7</pages>
<contexts>
<context position="20912" citStr="Adamson and Boreham, 1974" startWordPosition="3437" endWordPosition="3440">ated the performance of the proposed method in two different goals of the tasks: classification (Section 3.2) and normalization (Section 3.3). 3.2 Experiment 1: Candidate classification In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels. We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset5. Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s stemmer (Porter, 1980), Morpha (Minnen et al., 2001), and CST’s lemmatiser (Dalianis and Jonge3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.). 4LRAGR table also provides agreement information even when word forms do not change. For example, the table contains an entry indicating that th</context>
</contexts>
<marker>Adamson, Boreham, 1974</marker>
<rawString>George W. Adamson and Jillian Boreham. 1974. The use of an association measure based on character structure to identify semantically related pairs of words and document titles. Information Storage and Retrieval, 10(7-8):253–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farooq Ahmad</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Learning a spelling error model from search query logs.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP</booktitle>
<pages>955--962</pages>
<contexts>
<context position="2250" citStr="Ahmad and Kondrak, 2005" startWordPosition="310" endWordPosition="313">abeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined g</context>
</contexts>
<marker>Ahmad, Kondrak, 2005</marker>
<rawString>Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a spelling error model from search query logs. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP 2005), pages 955–962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of Ll-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning (ICML</booktitle>
<pages>33--40</pages>
<contexts>
<context position="13052" citStr="Andrew and Gao, 2007" startWordPosition="2121" endWordPosition="2124"> produces a sparse solution of feature weights in which numerous feature weights are zero (Tibshirani, 1996; Ng, 2004). Therefore, we regularize the log-likelihood with the L1 norm of the weight vector A and define the final form the objective function to be minimized as EA = −LA + |A |. (11) Q Here, Q is a parameter to control the effect of L1 regularization; the smaller the value we set to Q, the more features the MAP estimation assigns zero weights to: it removes a number of features from the model. Equation 11 is minimized using the OrthantWise Limited-memory Quasi-Newton (OW-LQN) method (Andrew and Gao, 2007) because the second term of equation 11 is not differentiable at Ak = 0. 2.4 Candidate generation The advantage of our feature design is that we can enumerate strings to which the classifier is likely to assign positive labels. We start by observing the necessary condition for t in equation 7, ATF(s,t)&gt;0==&gt;.]k: fk(s,t) = 1 n Ak &gt; 0. The classifier might assign a positive label to strings s and t when at least one feature function whose weight is positive can transform s to t. Let R+ be a set of substitution rules to which MAP estimation has assigned positive feature weights. Because each featu</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of Ll-regularized log-linear models. In Proceedings of the 24th International Conference on Machine Learning (ICML 2007), pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
<author>Takeshi Imai</author>
<author>Kengo Miyo</author>
<author>Kazuhiko Ohe</author>
</authors>
<title>Orthographic disambiguation incorporating transliterated probability.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<pages>48--55</pages>
<contexts>
<context position="31467" citStr="Aramaki et al. (2008)" startWordPosition="5131" endWordPosition="5134">ethod to train the costs of edit operations using Conditional Random Fields (CRFs). Bergsma and Kondrak (2007) correct comparative and superlative adjectives, e.g., unpopular � unpopularer --+ unpopularest and refundable --+ refundabler � refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. Spelling variation F1 score 0.99 0.985 0.98 0.975 0.97 0.965 0.96 454 presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary loo</context>
</contexts>
<marker>Aramaki, Imai, Miyo, Ohe, 2008</marker>
<rawString>Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko Ohe. 2008. Orthographic disambiguation incorporating transliterated probability. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP 2008), pages 48–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2668" citStr="Berger et al., 1996" startWordPosition="378" endWordPosition="381"> approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = It I dist(s, t) &lt; 61. (2) Here, the function dist(s, t) denotes the weighted Levenshtein distance (Levenshtein, 1966) between strings s and t. Furthermore, the threshold 6 requires the distance between the source string s and a candidate string t to be less than 6. The choice of dist(s, t) and 6 involves a tradeoff between the precision, recall, and training/tagging speed of the scorer. A less restr</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Alignment-based discriminative string similarity.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL</booktitle>
<pages>656--663</pages>
<contexts>
<context position="23167" citStr="Bergsma and Kondrak (2007)" startWordPosition="3795" endWordPosition="3798">ons. To examine the performance of the Liregularized logistic regression as a discriminative model, we also built two classifiers based on the Support Vector Machine (SVM). These SVM classifiers were implemented by the SVMp,,f7 on a linear kernel8. An SVM classifier employs the same feature set (substitution rules) as the proposed method so that we can directly compare the Liregularized logistic regression and the linear-kernel SVM. Another SVM classifier incorporates the five string metrics; this system can be considered as our reproduction of the discriminative string similarity proposed by Bergsma and Kondrak (2007). Table 3 reports the precision (P), recall (R), and F1 score (F1) based on the number of correct decisions for positive instances. The proposed method outperformed the baseline systems, achieving 0.919, 0.888, and 0.984 of F1 scores, respectively. Porter’s stemmer worked on the Inflection set, but not on the Orthography set, which is beyond the scope of the stemming algorithms. CST’s lemmatizer suffered from low recall on the Inflection set because it removed suffixes of base forms, e.g., (cloning, clone) —* (clone, clo). Morpha and CST’s lemma6We used CST’s lemmatiser version 2.13: http://ww</context>
<context position="30956" citStr="Bergsma and Kondrak (2007)" startWordPosition="5062" endWordPosition="5065">eled successfully by the discriminative framework. 4 Related work The task of string transformation has a long history in natural language processing and information retrieval. As described in Section 1, this task is related closely to various applications. Therefore, we specifically examine several prior studies that are relevant to this paper in terms of technical aspects. Some researchers have reported the effectiveness of the discriminative framework of string similarity. MaCallum et al. (2005) proposed a method to train the costs of edit operations using Conditional Random Fields (CRFs). Bergsma and Kondrak (2007) correct comparative and superlative adjectives, e.g., unpopular � unpopularer --+ unpopularest and refundable --+ refundabler � refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. Spelling variation F1 score 0.99 0.985 0.98 0.975 0.97 0.965 0.96 454 presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, thes</context>
</contexts>
<marker>Bergsma, Kondrak, 2007</marker>
<rawString>Shane Bergsma and Grzegorz Kondrak. 2007. Alignment-based discriminative string similarity. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2007), pages 656–663.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bilenko</author>
<author>Raymond J Mooney</author>
</authors>
<title>Adaptive duplicate detection using learnable string similarity measures.</title>
<date>2003</date>
<booktitle>In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (KDD</booktitle>
<pages>39--48</pages>
<contexts>
<context position="2152" citStr="Bilenko and Mooney, 2003" startWordPosition="295" endWordPosition="298">rce string s into its destination string t∗. In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects t</context>
</contexts>
<marker>Bilenko, Mooney, 2003</marker>
<rawString>Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive duplicate detection using learnable string similarity measures. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2003), pages 39–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on the Association for Computational Linguistics (ACL</booktitle>
<pages>286--293</pages>
<contexts>
<context position="1954" citStr="Brill and Moore, 2000" startWordPosition="265" endWordPosition="268">e tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 Introduction String transformation maps a source string s into its destination string t∗. In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model </context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings of the 38th Annual Meeting on the Association for Computational Linguistics (ACL 2000), pages 286–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="1704" citStr="Brill, 1995" startWordPosition="230" endWordPosition="231">a procedure to generate negative instances that affect the decision boundary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 Introduction String transformation maps a source string s into its destination string t∗. In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(t</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Ian Fette</author>
</authors>
<title>Memory-based context-sensitive spelling correction at web scale.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixth International Conference on Machine Learning and Applications (ICMLA</booktitle>
<pages>166--171</pages>
<contexts>
<context position="2001" citStr="Carlson and Fette, 2007" startWordPosition="273" endWordPosition="276"> remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 Introduction String transformation maps a source string s into its destination string t∗. In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad an</context>
</contexts>
<marker>Carlson, Fette, 2007</marker>
<rawString>Andrew Carlson and Ian Fette. 2007. Memory-based context-sensitive spelling correction at web scale. In Proceedings of the Sixth International Conference on Machine Learning and Applications (ICMLA 2007), pages 166–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Chen</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Improving query spelling correction using web search results.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>181--189</pages>
<contexts>
<context position="2287" citStr="Chen et al., 2007" startWordPosition="318" endWordPosition="321">g and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = It I dist(s, t) &lt; 61.</context>
</contexts>
<marker>Chen, Li, Zhou, 2007</marker>
<rawString>Qing Chen, Mu Li, and Ming Zhou. 2007. Improving query spelling correction using web search results. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 181–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hercules Dalianis</author>
<author>Bart Jongejan</author>
</authors>
<title>Handcrafted versus machine-learned inflectional rules: The euroling-siteseeker stemmer and cst’s lemmatiser. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>663--666</pages>
<contexts>
<context position="31731" citStr="Dalianis and Jongejan (2006)" startWordPosition="5170" endWordPosition="5173">efore, we removed inflection entries for comparative and superlative adjectives from the dataset. Spelling variation F1 score 0.99 0.985 0.98 0.975 0.97 0.965 0.96 454 presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination stri</context>
</contexts>
<marker>Dalianis, Jongejan, 2006</marker>
<rawString>Hercules Dalianis and Bart Jongejan. 2006. Handcrafted versus machine-learned inflectional rules: The euroling-siteseeker stemmer and cst’s lemmatiser. In In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2006), pages 663–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okan Kolak</author>
<author>Philip Resnik</author>
</authors>
<title>OCR error correction using a noisy channel model.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research (HLT</booktitle>
<pages>257--262</pages>
<contexts>
<context position="2048" citStr="Kolak and Resnik, 2002" startWordPosition="281" endWordPosition="284">n normalizing inflected words and spelling variations. 1 Introduction String transformation maps a source string s into its destination string t∗. In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework </context>
</contexts>
<marker>Kolak, Resnik, 2002</marker>
<rawString>Okan Kolak and Philip Resnik. 2002. OCR error correction using a noisy channel model. In Proceedings of the second international conference on Human Language Technology Research (HLT 2002), pages 257– 262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>Cognates and word alignment in bitexts.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Machine Translation Summit (MT Summit X),</booktitle>
<pages>305--312</pages>
<contexts>
<context position="2250" citStr="Kondrak, 2005" startWordPosition="312" endWordPosition="313">sks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined g</context>
<context position="21021" citStr="Kondrak, 2005" startWordPosition="3454" endWordPosition="3455">zation (Section 3.3). 3.2 Experiment 1: Candidate classification In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels. We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset5. Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s stemmer (Porter, 1980), Morpha (Minnen et al., 2001), and CST’s lemmatiser (Dalianis and Jonge3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.). 4LRAGR table also provides agreement information even when word forms do not change. For example, the table contains an entry indicating that the first-singular present form of the verb study is study, which might be readily apparent to English speakers</context>
</contexts>
<marker>Kondrak, 2005</marker>
<rawString>Grzegorz Kondrak. 2005. Cognates and word alignment in bitexts. In Proceedings of the Tenth Machine Translation Summit (MT Summit X), pages 305–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>25--32</pages>
<contexts>
<context position="7393" citStr="Lee, 1999" startWordPosition="1152" endWordPosition="1153">tions; K is the number of feature functions; and A = {λ1, ..., λK} presents a weight vector of the feature functions. We obtain the following decision rule to choose the most probable label y* for a given pair hs, ti, t) &gt; ={t|P(1|s, P(0|s, t)} = {t |AT F (s, t) &gt; 0}. (7) s Finally, given a source string s, the generator function gen(s) is defined to collect all stri ngs to which the classifier assigns positive labels: gen(s) 2.2 Substitution rules as feature The binary classifier can include any arbitrary feature. This is exemplified by the Levenshtein distance and distributional similarity (Lee, 1999) between two strings s and t. These features can improve the classification accuracy, but it is unrealistic to compute these features for every possible stri ng, as in equation 7. For that reason, we specifically examine substitution rules, with which the process y* = argmax P(y|s, t) = � 1 (ATF (s,t) &gt; 0) . yE{0,1} 0 (otherwise) (6) 448 (1) S: ^oestrogen$ t: ^estrogen$ (2) S: ^anaemia$ t: ^anemia$ (3) S: ^studies$ t: ^study$ Figure 1: Generating substitution rules. of transforming a source string s into its destination form t is tractable. In this study, we assume that every string has a pref</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL 1999), pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="2983" citStr="Levenshtein, 1966" startWordPosition="430" endWordPosition="431">ates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = It I dist(s, t) &lt; 61. (2) Here, the function dist(s, t) denotes the weighted Levenshtein distance (Levenshtein, 1966) between strings s and t. Furthermore, the threshold 6 requires the distance between the source string s and a candidate string t to be less than 6. The choice of dist(s, t) and 6 involves a tradeoff between the precision, recall, and training/tagging speed of the scorer. A less restrictive design of these factors broadens the search space, but it also increases the number of confusing candidates, amount of feature space, and computational cost for the scorer. Moreover, the choice is highly dependent on the target task. It might be sufficient for a spelling 447 Proceedings of the 2008 Conferen</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Yang Zhang</author>
<author>Muhua Zhu</author>
<author>Ming Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spelling correction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (Coling-ACL</booktitle>
<pages>1025--1032</pages>
<contexts>
<context position="2267" citStr="Li et al., 2006" startWordPosition="314" endWordPosition="317">tof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = I</context>
</contexts>
<marker>Li, Zhang, Zhu, Zhou, 2006</marker>
<rawString>Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006. Exploring distributional similarity based models for query spelling correction. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (Coling-ACL 2006), pages 1025–1032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kedar Bellare</author>
<author>Fernando Pereira</author>
</authors>
<title>A conditional random field for discriminativelytrained finite-state string edit distance.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI</booktitle>
<pages>388--395</pages>
<marker>McCallum, Bellare, Pereira, 2005</marker>
<rawString>Andrew McCallum, Kedar Bellare, and Fernando Pereira. 2005. A conditional random field for discriminativelytrained finite-state string edit distance. In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI 2005), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="20967" citStr="Melamed, 1999" startWordPosition="3447" endWordPosition="3448">of the tasks: classification (Section 3.2) and normalization (Section 3.3). 3.2 Experiment 1: Candidate classification In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels. We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset5. Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s stemmer (Porter, 1980), Morpha (Minnen et al., 2001), and CST’s lemmatiser (Dalianis and Jonge3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.). 4LRAGR table also provides agreement information even when word forms do not change. For example, the table contains an entry indicating that the first-singular present form of the verb study is stud</context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>I. Dan Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, 25(1):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="21084" citStr="Minnen et al., 2001" startWordPosition="3461" endWordPosition="3464">ication In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels. We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset5. Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s stemmer (Porter, 1980), Morpha (Minnen et al., 2001), and CST’s lemmatiser (Dalianis and Jonge3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.). 4LRAGR table also provides agreement information even when word forms do not change. For example, the table contains an entry indicating that the first-singular present form of the verb study is study, which might be readily apparent to English speakers. 5We determined the regularization parameter v = 5 experimenta</context>
<context position="24586" citStr="Minnen et al., 2001" startWordPosition="4018" endWordPosition="4021">trols the tradeoff between training error and margin. 452 System Orthography Derivation Inflection P R F1 P R F1 P R F1 Levenshtein distance (S = 1) .319 .871 .467 .004 .006 .005 .484 .679 .565 Levenshtein distance .323 .999 .488 .131 1.00 .232 .479 .988 .646 Normalized Levenshtein distance .441 .847 .580 .133 .990 .235 .598 .770 .673 Dice coefficient (letter bigram) .401 .918 .558 .137 .984 .240 .476 1.00 .645 LCSR .322 1.00 .487 .156 .841 .263 .476 1.00 .645 PREFIX .418 .927 .576 .140 .943 .244 .476 1.00 .645 Porter stemmer (Porter, 1980) .084 .074 .079 .197 .846 .320 .926 .839 .881 Morpha (Minnen et al., 2001) .009 .007 .008 .012 .022 .016 .979 .836 .902 CST’s lemmatiser (Dalianis et al. 2006) .119 .008 .016 .383 .682 .491 .821 .176 .290 Proposed method .941 .898 .919 .896 .880 .888 .985 .986 .984 Substitution rules trained with SVM .943 .890 .916 .894 .886 .890 .980 .987 .983 + LD, NLD, DICE, LCSR, PREFIX .946 .906 .926 .894 .886 .890 .980 .987 .983 Table 3: Performance of candidate classification Rank Src Dst Weight Examples 1 uss us 9.81 focussing 2 aev ev 9.56 mediaeval 3 aen en 9.53 ozaena 4 iae$ ae$ 9.44 gadoviae 5 nni ni 9.16 prorennin 6 nne ne 8.84 connexus 7 our or 8.54 colour 8 aea ea 8.3</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
</authors>
<title>A guided tour to approximate string matching.</title>
<date>2001</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="2093" citStr="Navarro, 2001" startWordPosition="288" endWordPosition="289"> 1 Introduction String transformation maps a source string s into its destination string t∗. In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen e</context>
</contexts>
<marker>Navarro, 2001</marker>
<rawString>Gonzalo Navarro. 2001. A guided tour to approximate string matching. ACM Computing Surveys (CSUR), 33(1):31–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
</authors>
<title>Feature selection, L1 vs. L2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In Proceedings of the twenty-first international conference on Machine learning (ICML</booktitle>
<pages>78--85</pages>
<contexts>
<context position="12549" citStr="Ng, 2004" startWordPosition="2034" endWordPosition="2035">zation term of the feature weights A, i.e., a penalty on large feature weights. In addition, the generation algorithm of substitution rules might produce inappropriate rules that transform a string incorrectly, or overly specific rules that are used scarcely. Removing unnecessary substitution rules not only speeds up the classifier but also the algorithm for candidate generation, as presented in Section 2.4. In recent years, L1 regularization has received increasing attention because it produces a sparse solution of feature weights in which numerous feature weights are zero (Tibshirani, 1996; Ng, 2004). Therefore, we regularize the log-likelihood with the L1 norm of the weight vector A and define the final form the objective function to be minimized as EA = −LA + |A |. (11) Q Here, Q is a parameter to control the effect of L1 regularization; the smaller the value we set to Q, the more features the MAP estimation assigns zero weights to: it removes a number of features from the model. Equation 11 is minimized using the OrthantWise Limited-memory Quasi-Newton (OW-LQN) method (Andrew and Gao, 2007) because the second term of equation 11 is not differentiable at Ak = 0. 2.4 Candidate generation</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Andrew Y. Ng. 2004. Feature selection, L1 vs. L2 regularization, and rotational invariance. In Proceedings of the twenty-first international conference on Machine learning (ICML 2004), pages 78–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="21054" citStr="Porter, 1980" startWordPosition="3458" endWordPosition="3459">nt 1: Candidate classification In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels. We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset5. Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s stemmer (Porter, 1980), Morpha (Minnen et al., 2001), and CST’s lemmatiser (Dalianis and Jonge3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.). 4LRAGR table also provides agreement information even when word forms do not change. For example, the table contains an entry indicating that the first-singular present form of the verb study is study, which might be readily apparent to English speakers. 5We determined the regularizati</context>
<context position="24512" citStr="Porter, 1980" startWordPosition="4006" endWordPosition="4007">rf.html 8We determined the parameter C = 500 experimentally; it controls the tradeoff between training error and margin. 452 System Orthography Derivation Inflection P R F1 P R F1 P R F1 Levenshtein distance (S = 1) .319 .871 .467 .004 .006 .005 .484 .679 .565 Levenshtein distance .323 .999 .488 .131 1.00 .232 .479 .988 .646 Normalized Levenshtein distance .441 .847 .580 .133 .990 .235 .598 .770 .673 Dice coefficient (letter bigram) .401 .918 .558 .137 .984 .240 .476 1.00 .645 LCSR .322 1.00 .487 .156 .841 .263 .476 1.00 .645 PREFIX .418 .927 .576 .140 .943 .244 .476 1.00 .645 Porter stemmer (Porter, 1980) .084 .074 .079 .197 .846 .320 .926 .839 .881 Morpha (Minnen et al., 2001) .009 .007 .008 .012 .022 .016 .979 .836 .902 CST’s lemmatiser (Dalianis et al. 2006) .119 .008 .016 .383 .682 .491 .821 .176 .290 Proposed method .941 .898 .919 .896 .880 .888 .985 .986 .984 Substitution rules trained with SVM .943 .890 .916 .894 .886 .890 .980 .987 .983 + LD, NLD, DICE, LCSR, PREFIX .946 .906 .926 .894 .886 .890 .980 .987 .983 Table 3: Performance of candidate classification Rank Src Dst Weight Examples 1 uss us 9.81 focussing 2 aev ev 9.56 mediaeval 3 aen en 9.53 ozaena 4 iae$ ae$ 9.44 gadoviae 5 nni </context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="2568" citStr="Shannon, 1948" startWordPosition="363" endWordPosition="364"> Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = It I dist(s, t) &lt; 61. (2) Here, the function dist(s, t) denotes the weighted Levenshtein distance (Levenshtein, 1966) between strings s and t. Furthermore, the threshold 6 requires the distance between the source string s and a candidate string t to be less than 6. The choice of dist(s, t) and 6 invol</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude E. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27(3):379–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>58</volume>
<issue>1</issue>
<contexts>
<context position="12538" citStr="Tibshirani, 1996" startWordPosition="2032" endWordPosition="2033">oducing a regularization term of the feature weights A, i.e., a penalty on large feature weights. In addition, the generation algorithm of substitution rules might produce inappropriate rules that transform a string incorrectly, or overly specific rules that are used scarcely. Removing unnecessary substitution rules not only speeds up the classifier but also the algorithm for candidate generation, as presented in Section 2.4. In recent years, L1 regularization has received increasing attention because it produces a sparse solution of feature weights in which numerous feature weights are zero (Tibshirani, 1996; Ng, 2004). Therefore, we regularize the log-likelihood with the L1 norm of the weight vector A and define the final form the objective function to be minimized as EA = −LA + |A |. (11) Q Here, Q is a parameter to control the effect of L1 regularization; the smaller the value we set to Q, the more features the MAP estimation assigns zero weights to: it removes a number of features from the model. Equation 11 is minimized using the OrthantWise Limited-memory Quasi-Newton (OW-LQN) method (Andrew and Gao, 2007) because the second term of equation 11 is not differentiable at Ak = 0. 2.4 Candidate</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>John McNaught</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Normalizing biomedical terms by minimizing ambiguity and variability.</title>
<date>2008</date>
<journal>BMC Bioinformatics, Suppl</journal>
<volume>3</volume>
<issue>9</issue>
<contexts>
<context position="31976" citStr="Tsuruoka et al. (2008)" startWordPosition="5209" endWordPosition="5212">res from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination strings that were specified by the training data. 5 Conclusion We have presented a discriminative approach for generating candidates for string transformation. Unlike conventional spelling-correction tasks, this study did not assume a fixed set of d</context>
</contexts>
<marker>Tsuruoka, McNaught, Ananiadou, 2008</marker>
<rawString>Yoshimasa Tsuruoka, John McNaught, and Sophia Ananiadou. 2008. Normalizing biomedical terms by minimizing ambiguity and variability. BMC Bioinformatics, Suppl 3(9):S2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John Wilbur</author>
<author>Won Kim</author>
<author>Natalie Xie</author>
</authors>
<title>Spelling correction in the PubMed search engine.</title>
<date>2006</date>
<journal>Information Retrieval,</journal>
<volume>9</volume>
<issue>5</issue>
<contexts>
<context position="1975" citStr="Wilbur et al., 2006" startWordPosition="269" endWordPosition="272">l. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 Introduction String transformation maps a source string s into its destination string t∗. In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), t∗ = argmax P(tIs). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P(tIs) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill</context>
</contexts>
<marker>Wilbur, Kim, Xie, 2006</marker>
<rawString>W. John Wilbur, Won Kim, and Natalie Xie. 2006. Spelling correction in the PubMed search engine. Information Retrieval, 9(5):543–564.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>