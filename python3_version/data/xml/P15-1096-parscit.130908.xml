<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990659">
Environment-Driven Lexicon Induction for High-Level Instructions
</title>
<author confidence="0.841724">
Dipendra K. Misra* Kejia Tao* Percy Liang** Ashutosh Saxena*
</author>
<email confidence="0.919512">
dkm@cs.cornell.edu kt454@cornell.edu pliang@cs.stanford.edu asaxena@cs.cornell.edu
</email>
<affiliation confidence="0.9413245">
* Department of Computer Science, Cornell University
**Department of Computer Science, Stanford University
</affiliation>
<sectionHeader confidence="0.978595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908190476191">
We focus on the task of interpreting com-
plex natural language instructions to a
robot, in which we must ground high-level
commands such as microwave the cup to
low-level actions such as grasping. Pre-
vious approaches that learn a lexicon dur-
ing training have inadequate coverage at
test time, and pure search strategies can-
not handle the exponential search space.
We propose a new hybrid approach that
leverages the environment to induce new
lexical entries at test time, even for new
verbs. Our semantic parsing model jointly
reasons about the text, logical forms, and
environment over multi-stage instruction
sequences. We introduce a new dataset
and show that our approach is able to suc-
cessfully ground new verbs such as dis-
tribute, mix, arrange to complex logical
forms, each containing up to four predi-
cates.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.964113277777778">
The task of mapping natural language instructions
to actions for a robot has been gaining momen-
tum in recent years (Artzi and Zettlemoyer, 2013;
Tellex et al., 2011; Misra et al., 2014; Bollini
et al., 2011; Guadarrama et al., 2013; Matuszek
et al., 2012b; Fasola and Mataric, 2013). We
are particularly interested in instructions contain-
ing verbs such as “microwave” denoting high-level
concepts, which correspond to more than 10 low-
level symbolic actions such as grasp. In this
setting, it is common to find new verbs requiring
new concepts at test time. For example, in Fig-
ure 1, suppose that we have never seen the verb
“fill”. Can we impute the correct interpretation,
and moreover seize the opportunity to learn what
“fill” means in a way that generalizes to future in-
structions?
Text: “get the cup, fill it with water and then microwave the cup”
</bodyText>
<figureCaption confidence="0.8367404">
Figure 1: A lexicon learned on the training data
cannot possibly cover all the verb-concept map-
pings needed at test time. Our algorithm learns
the meaning of new verbs (e.g., “fill”) using the
environment context.
</figureCaption>
<bodyText confidence="0.999263">
Previous work in semantic parsing handles lex-
ical coverage in one of two ways. Kwiatkowski et
al. (2010) induces a highly constrained CCG lex-
icon capable of mapping words to complex log-
ical forms, but it would have to skip new words
(which in Figure 1 would lead to microwaving an
empty cup). Others (Berant and Liang, 2014) take
a freer approach by performing a search over log-
ical forms, which can handle new words, but the
logical forms there are much simpler than the ones
we consider.
In this paper, we present an hybrid approach
that uses a lexicon to represent complex concepts
but also strongly leverages the environment to
guide the search space. The environment can pro-
vide helpful cues in several ways:
</bodyText>
<listItem confidence="0.811548125">
• Only a few environments are likely for a given
scenario—e.g., the text is unlikely to ask the
robot to microwave an empty cup or put books
on the floor.
• The logical form of one segment of text con-
strains that of the next segment—e.g., the text is
unlikely to ask the robot to pick a cup and then
put it back immediately in the same spot.
</listItem>
<bodyText confidence="0.784572">
We show that this environment context provides
</bodyText>
<figure confidence="0.995539">
grasping cup3 ∧
near(robot1,cup3)
Unseen verb “ fill ” is grounded
at test time using environment.
state cup3,water ∧
on(cup3,sink)
in cup3,microwave ∧
state(microwave1,is-on)
</figure>
<page confidence="0.961667">
992
</page>
<note confidence="0.983687333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 992–1002,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.977918073170732">
Environment: zk−1 Logical Form z = (ℓ, 𝜉)
ek zk
ℓ: 𝑝𝑢𝑡 ⇒ [𝜆 𝑣. state v1,has-cd
∧ near v1,v2 ,𝜉′]
𝜉: {𝑣1 →𝑥𝑏𝑜𝑥1; 𝑣2 → 𝑟𝑜𝑏𝑜𝑡1}
𝜉′: old mapping
Power-off
𝑥𝑏𝑜𝑥1
Action Sequence: 𝑚𝑜𝑣𝑒𝑡𝑜 𝑥𝑏𝑜𝑥1 ; 𝑔𝑟𝑎𝑠𝑝 𝑥𝑏𝑜𝑥1 ; 𝑝𝑟𝑒𝑠𝑠 𝑝𝑜𝑤𝑒𝑟_𝑏𝑢𝑡𝑡𝑜𝑛1 ; 𝑚𝑜𝑣𝑒𝑡𝑜 𝑐𝑑2 ; 𝑔𝑟𝑎𝑠𝑝 𝑐𝑑2 ; 𝑖𝑛𝑠𝑒𝑟𝑡(𝑐𝑑2, 𝑥𝑏𝑜𝑥1) ⋯
𝑠𝑛𝑎𝑐𝑘𝑡𝑎𝑏𝑙𝑒2
On
e1
planner
𝑐1 𝑐2
𝜙
𝑎1
z1 e2
simulator
Shallow Parsing (Section 3.2)
planner
𝜙
z2
𝑎2
x
⋯
Semantic Parsing Model
(Section 5)
⋯ ⋯
planner
𝑎k−1
Text: “Turn on xbox. Take Far Cry Game CD and put in xbox.
Throw out beer, coke and sketchy stuff in bowl.⋯ ”
simulator
planner
𝑐𝑘
𝜙
𝑎k
Frame Node
𝜈: throw,
𝜔: [ beer, coke, sketchy stuff, bowl ]
r: { in: sketchy stuff → bowl }
</figure>
<figureCaption confidence="0.629402">
Figure 2: Graphical model overview: we first deterministically shallow parse the text x into a control
flow graph consisting of shallow structures {ci}. Given an initial environment e1, our semantic parsing
</figureCaption>
<bodyText confidence="0.996638684210526">
model maps these frame nodes to logical forms {zi} representing the postconditions. From this, a planner
and simulator generate the action sequences {ai} and resulting environments {ei}.
a signal for inducing new lexical entries that map
previously unseen verbs to novel concepts. In the
example in Figure 1, the algorithm learns that mi-
crowaving an empty cup is unlikely and this sug-
gests that the verb “fill” must map to actions that
end up making the cup not empty.
Another contribution of this paper is using post-
conditions as logical forms rather than actions, as
in previous work (Artzi and Zettlemoyer, 2013;
Misra et al., 2014). Postconditions not only re-
duce the search space of logical forms, but are also
a more natural representation of verbs. We define
a conditional random field (CRF) model over post-
conditions, and use a planner to convert postcon-
ditions into action sequences and a simulator to
generate new environments.
At test time, we use the lexicon induced from
the training data, but also perform an environment-
guided search over logical forms to induce new
lexical entries on-the-fly. If the predicted action
sequence uses a new lexical entry generated by the
search, it is added to the lexicon, where it can be
reused in subsequent test examples.
We evaluate our algorithm on a new corpus con-
taining text commands for a household robot. The
two key findings of our experiments are: First, the
environment and task context contain enough in-
formation to allow us to learn lexical entries for
new verbs such as “distribute” and “mix” with
complex semantics. Second, using both lexical
entries generated by a test-time search and those
from the lexicon induced by the training data out-
performs the two individual approaches. This sug-
gests that environment context can help allevi-
ate the problem of having a limited lexicon for
grounded language acquisition.
</bodyText>
<sectionHeader confidence="0.90447" genericHeader="introduction">
2 Problem Statement
</sectionHeader>
<bodyText confidence="0.970975117647059">
At training time, we are given a set of examples
D = {(x(m), e(m), a(m), 7r(m))}Mm=1, where x(m)
is a text containing natural language instructions,
e(m) is an initial environment, a(m) is a human-
annotated sequence of actions, and 7r(m) specifies
a monotonic alignment between segments of x(m)
and segments of a(m). For example, given words
x(m) = x1x2 and a(m) = a1a2a3, 7r(m) might
specify that x1 aligns to a1a2 and x2 aligns to a3.
At test time, given a sequence of text-
environment pairs as input {(x(n), e(n))}N n=1, we
wish to generate a sequence of actions a(n) for
each input pair. Note that our system is allowed to
use information about one test example to improve
performance on subsequent ones. We evaluate a
system on its ability to recover a human-annotated
sequence of actions.
</bodyText>
<sectionHeader confidence="0.963713" genericHeader="method">
3 Approach Overview
</sectionHeader>
<bodyText confidence="0.93632">
Figure 2 shows our approach for mapping text x
to actions a1:k given the initial environment e1.
</bodyText>
<subsectionHeader confidence="0.994969">
3.1 Representation
</subsectionHeader>
<bodyText confidence="0.999413142857143">
We use the following representation for the differ-
ent variables in Figure 2.
Environment. An environment ei is represented
by a graph whose nodes are objects and edges
represent spatial relations between these objects.
We consider five basic spatial relations: near,
grasping, on, in and below. Each object has an
</bodyText>
<page confidence="0.998187">
993
</page>
<bodyText confidence="0.999879056603774">
instance ID (e.g., book9), a category name (e.g.,
chair, xbox), a set of properties such as graspable,
pourable used for planning and a set of boolean
states such as has-water, at-channel3, whose
values can be changed by robot actions. The robot
is also an object in the environment. For example,
the objects xbox,, snacktable2, are two objects
in el in Figure 2 with relation on between them.
Postconditions. A postcondition is a conjunction
of atoms or their negations. Each atom consists of
either a spatial relation between two objects (e.g.,
on(book9, shelf3)) or a state and a value (e.g.,
state(cup4, has-water)). Given an environment
e, the postcondition evaluates to true or false.
Actions. Each action in an action sequence az
consists of an action name with a list of argu-
ments (e.g., grasp(xbox,)). The action name
is one of 15 values (grasp, moveto, wait, etc.),
and each argument is either an object in the envi-
ronment (e.g., xbox,), a spatial relation (e.g., in
for keep(ramen2, in, kettle,), or a postcondi-
tion (e.g., for wait(state(kettle,, boiling))).
Logical Forms. The logical form zz is a pair
(E, ξ) containing a lexical entry E and a map-
ping ξ. The lexical entry E contains a parameter-
ized postcondition such as λ�v.grasping(v,, v2)
∧¬near(v3, v2), and ξ maps the variables v� to ob-
jects in the environment. Applying the parame-
terized postcondition on ξ yields a postcondition;
note that a postcondition can be represented by
different logical forms. A lexical entry contains
other information which are used for defining fea-
tures, which is detailed in Section 4.
Control Flow Graphs. Following previous work
(Tellex et al., 2011; Misra et al., 2014), we convert
the text x to a shallow representation. The par-
ticular representation we choose is a control flow
graph, which encodes the sequential relation be-
tween atomic segments in the text. Figure 3 shows
the control flow graph for an example text. In a
control flow graph, each node is either a frame
node or a conditional node. A frame node rep-
resents a single clause (e.g., “change the chan-
nel to a movie”) and has at most one successor
node. Specifically, a frame node consists of a verb
v (e.g., arrange, collect), a set of object descrip-
tions {wz} which are the arguments of the verb
(e.g., the guinness book, movie channel), and spa-
tial relations r between the arguments (e.g., be-
tween, near). The object description w is either an
anaphoric reference (such as “it”) or a tuple con-
taining the main noun, associated modifiers, and
relative clauses.
</bodyText>
<figure confidence="0.690669666666667">
Text: “If any of the pots have food in them, then dump them out in the
garbage can and then put them on the sink else keep it on the table.”
Be category e,cup ∧state(e,food)
Conditional node (expr)
v: dump v: keep
w: [them, the garbage can] w: [it, the table]
r: [ in: them — garbage can j r: [on: it — the table j
Frame Node v, w, r
v: verb
</figure>
<figureCaption confidence="0.5237244">
w: set of object description w
w: (main noun or pronoun, modifiers)
r: relationship between descriptions
Figure 3: We deterministically parse text into a
shallow structure called a control flow graph.
</figureCaption>
<bodyText confidence="0.999964142857143">
A conditional node contains a logical postcon-
dition with at most one existentially quantified
variable (in contrast to a frame node, which con-
tains natural language). For example, in Figure 3
the conditional node contains the expression cor-
responding to the text “if any of the pots has food”
There are two types of conditional nodes: branch-
ing and temporal. A branching conditional node
represents an “if” statement and has two succes-
sor nodes corresponding to whether the condition
evaluates to true or false in the current environ-
ment. A temporal conditional node represents an
“until” statement and waits until the condition is
false in the environment.
</bodyText>
<subsectionHeader confidence="0.9978">
3.2 Formal Overview
</subsectionHeader>
<bodyText confidence="0.997407652173913">
Shallow Parsing. We deterministically convert
the text x into its control flow graph G using a set
of manual rules applied on its constituency parse
tree from the Stanford parser (Klein and Manning,
2003). Conditionals in our dataset are simple and
can be converted into postconditions directly using
a few rules, unlike the action verbs (e.g., “fill”),
which is the focus of this paper. The details of
our shallow parsing procedure is described in the
appendix.
Given an environment el, G is reduced to a sin-
gle sequence of frame nodes ci, ... , ck, by evalu-
ating all the branch conditionals on ei.
Semantic Parsing Model. For each frame node cz
and given the current environment ez, the seman-
tic parsing model (Section 5) places a distribution
over logical forms zz. This logical form zz rep-
resents a postcondition on the environment after
executing the instructions in cz.
Planner and Simulator. Since our semantic rep-
resentations involve postconditions but our model
is based on the environment, we need to connect
the two. We use planner and a simulator that to-
</bodyText>
<listItem confidence="0.703825666666667">
v: put
w: [them, the sink]
r: [on: them — the sink j
</listItem>
<page confidence="0.980655">
994
</page>
<bodyText confidence="0.999978888888889">
gether specify a deterministic mapping from the
current environment ei and a logical form zi to
a new environment ei+1. Specifically, the plan-
ner takes the current environment ei and a logical
form zi and computes the action sequence ai =
planner(ei, zi) for achieving the post condition
represented by zi.1 The simulator takes the current
environment ei and an action sequence ai and re-
turns a new environment ei+1 = simulator(ei, ai).
</bodyText>
<sectionHeader confidence="0.979197" genericHeader="method">
4 Anchored Verb Lexicons
</sectionHeader>
<bodyText confidence="0.9751018">
Like many semantic parsers, we use a lexicon to
map words to logical forms. Since the environ-
ment plays an central role in our approach, we pro-
pose an anchored verb lexicon, in which we store
additional information about the environment in
which lexical entries were previously used. We
focus only on verbs since they have the most com-
plex semantics; object references such as “cup”
can be mapped easily, as described in Section 5.
More formally, an anchored verb lexicon A con-
tains lexical entries E of the following form: [ν ⇒
(λ�v.S, ξ)] where, ν is a verb, S is a postcondition
with free variables v, and ξ is a mapping of these
variables to objects. An example lexical entry is:
[pour ⇒ (λv1v2v3.S, ξ)], where:
</bodyText>
<equation confidence="0.976292333333333">
S = grasping(v,, v2) n near(v,, v3) n -state(v2, milk)
n state(v3, milk)
ξ ={v, --+ robot,, v2 --+ cup1, v3 --+ bowl3} (anchoring)
</equation>
<bodyText confidence="0.96670312">
As Table 1 shows, a single verb will in general
have multiple entries due to a combination of pol-
ysemy and the fact that language is higher-level
than postconditions.
Advantages of Postconditions. In contrast to pre-
vious work (Artzi and Zettlemoyer, 2013; Misra et
al., 2014), we use postconditions instead of action
sequence for two main reasons. First, postcondi-
tions generalize better. To illustrate this, consider
the action sequence for the simple task of filling a
cup with water. At the time of learning the lexi-
con, the action sequence might correspond to us-
ing a tap for filling the cup while at test time, the
environment may not have a tap but instead have
a pot with water. Thus, if the lexicon maps to ac-
tion sequence, then it will not be applicable at test
time whereas the postcondition state(z,, water)
is valid in both cases. We thus shift the load of in-
ferring environment-specific actions onto planners
1We use the symbolic planner of Rintanen (2012) which
can perform complex planning. For example, to pick up a
bottle that is blocked by a stack of books, the planner will
first remove the books before grasping the bottle. In contrast,
Artzi and Zettlemoyer (2013) use a simple search over im-
plicit actions.
</bodyText>
<tableCaption confidence="0.997004">
Table 1: Some lexical entries for the verb “turn”
</tableCaption>
<subsectionHeader confidence="0.590223">
Sentence Context Lexical entry [turn =:&gt;. (AV.S, ξ)]
</subsectionHeader>
<bodyText confidence="0.875014">
“turn on the TV” state(v,, is-on) n near(v2, v,)
</bodyText>
<equation confidence="0.851722">
ξ : v, --+ tv,, v2 --+ robot,
</equation>
<bodyText confidence="0.649581333333333">
“turn on the right state(v,, fire3) n near(v2, v,)
back burner” ξ : v, --+ stove,, v2 --+ robot,
“turn off the water” -state(v,, tap-on)
</bodyText>
<equation confidence="0.8289485">
ξ : v, --+ sink,
“turn the television state(v,, channel6) n near(v,, v2)
</equation>
<bodyText confidence="0.97203545">
input to xbox” ξ : v, --+ tv,, v2 --+ xbox,
and use postconditions for representation, which
better captures the semantics of verbs.
Second, because postconditions are higher-
level, the number of atoms needed to repre-
sent a verb is much less than the correspond-
ing number of actions. For example, the
text “microwave a cup”, maps to action se-
quence with 10–15 actions, the postcondition
only has two atoms: in(cup2,microwave,) ∧
state(microwave, is-on). This makes search-
ing for new logical forms more tractable.
Advantages of Anchoring. Similar to the VEIL
templates of Misra et al. (2014), the free variables
v� are associated with a mapping ξ to concrete ob-
jects. This is useful for resolving ellipsis. Suppose
the following lexical entry was created at train-
ing time based on the text “throw the drinks in the
trash bag”:
[B: throw =�. Axyz.S(x, y, z)], where
</bodyText>
<equation confidence="0.7542835">
S = in(x,y) n -grasping(z, x) n -state(z, closed)
ξ ={x --+ coke,, y --+ garbageBin,, z --+ robot,}
</equation>
<bodyText confidence="0.999949166666667">
Now consider a new text at test time “throw
away the chips”, which does not explicitly men-
tion where to throw the chips. Our semantic pars-
ing algorithm (Section 5) will use the previous
mapping y → garbabeBin, to choose an object
most similar to a garbage bin.
</bodyText>
<sectionHeader confidence="0.993709" genericHeader="method">
5 Semantic Parsing Model
</sectionHeader>
<bodyText confidence="0.9997918">
Given a sequence of frame nodes c1:k and an ini-
tial environment e1, our semantic parsing model
defines a joint distribution over logical forms z1:k.
Specifically, we define a conditional random field
(CRF) over z1:k, as shown in Figure 2:
</bodyText>
<equation confidence="0.824445">
Iφ(ci, zi−1, zi, ei) · θ , (1)
</equation>
<bodyText confidence="0.999881166666667">
where φ(ci, zi−1, zi, ei) is the feature vector and
θ is the weight vector. Note that the environ-
ments e1:k are a deterministic function of the log-
ical forms z1:k through the recurrence ei+1 =
simulator(ei, planner(ei, zi)), which couples the
different time steps.
</bodyText>
<equation confidence="0.988380333333333">
k
pe(z1:k  |c1:k, e1)ocexp
i=1
</equation>
<page confidence="0.969504">
995
</page>
<bodyText confidence="0.994809777777778">
Features. The feature vector φ(ci, zi_1, zi, ei)
contains 16 features which capture the dependen-
cies between text, logical forms, and environment.
Recall that zi = ([ν ⇒ (λ~v.S, ξ)], ξi), where ξ
is the environment in which the lexical entry was
created and ξi is the current environment. Let
fi = (λ~v.S)(ξi) be the current postcondition.
Here we briefly describe the important features
(see the supplemental material for the full list):
</bodyText>
<listItem confidence="0.987946083333333">
• Language and logical form: The logical form
zi should generally reference objects mentioned
in the text. Assume we have computed a cor-
relation ρ(ω, o) between each object description
ω and object o, whose construction is described
later. We then define two features: precision cor-
relation, which encourages zi to only use objects
referred to in ci; and recall correlation, which
encourages zi to use all the objects referred to in
ci.
• Logical form: The postcondition fi should be
based on previously seen environments. For ex-
</listItem>
<bodyText confidence="0.917613119047619">
ample, microwaving an empty cup and grasp-
ing a couch are unlikely postconditions. We
define features corresponding to the average
probability (based on the training data) of all
conjunctions of at most two atoms in the
postcondition (e.g., grasping(robot, cup)}).
We do the same with their abstract versions
({grasping(v,, v2)}). In addition, we build the
same set of four probability tables conditioned
on verbs in the training data. For example, the
abstract postcondition state(v,, water) has a
higher probability conditioned on the verb “fill”.
This gives us a total of 8 features of this type.
• Logical form and environment: Recall that an-
choring helps us in dealing with ellipsis and
noise. We add a feature based on the average
correlation between the objects of the new map-
ping ξi with the corresponding objects in the an-
chored mapping ξ.
The other features are based on the relationship
between object descriptions, similarity between ξ
and ξi and transition probabilities between logi-
cal forms zi_1 and zi. These probabilities are also
learned from training data.
Mapping Object Descriptions. Our features rely
on a mapping from object descriptions ω (e.g.,
“the red shiny cup”) to objects o (e.g., cup$),
which has been addressed in many recent works
(Matuszek et al., 2012a; Guadarrama et al., 2014;
Fasola and Matari’c, 2014).
One key idea is: instead of computing rigid lex-
ical entries such as cup → cup,, we use a contin-
uous correlation score ρ(ω, o) E [0, 1] that mea-
sures how well ω describes o. This flexibility al-
lows the algorithm to use objects not explicitly
mentioned in text. Given “get me a tank of wa-
ter”, we might choose an approximate vessel (e.g.,
cup2).
Given an object description ω, an object o, and a
set of previously seen objects (used for anaphoric
resolution), we define the correlation ρ(ω, o) using
the following approach:
</bodyText>
<listItem confidence="0.994777294117647">
• If ω is a pronoun, ρ(ω, o) is the ratio of the posi-
tion of the last reference of o to the length of the
action sequence computed so far, thus preferring
recent objects.
• Otherwise, we compute the correlation using
various sources: the object’s category; the
object’s state for handling metonymy (e.g.,
the description “coffee” correlates well with
the object mug, if mug, contains coffee—
state(mug,, has-coffee) is true), WordNet
(Fellbaum, 1998) for dealing synonymy and hy-
ponymy; and word alignments between the ob-
jects and text from Giza++ (Och and Ney, 2003)
to learn domain-specific references (e.g., “Guin-
ness book” refers to book,, not book2). More
details can be found in the supplemental mate-
rial.
</listItem>
<sectionHeader confidence="0.591876" genericHeader="method">
6 Lexicon Induction from Training Data
</sectionHeader>
<bodyText confidence="0.999927916666667">
In order to map text to logical forms, we first in-
duce an initial anchored lexicon A from the train-
ing data {(x(m), e(m), a(m), π(m))}Mm=1. At test
time, we add new lexical entries (Section 7) to A.
Recall that shallow parsing x(m) yields a list of
frame nodes c1:k. For each frame node ci and its
aligned action sequence ai, we take the conjunc-
tion of all the atoms (and their negations) which
are false in the current one ei but true in the
next environment ei+1. We parametrize this con-
junction by replacing each object with a variable,
yielding a postcondition S parametrized by free
variables v~ and the mapping ξ from v~ to objects
in ei. We then add the lexical entry [verb(ci) ⇒
(λ~v.S, ξ)] to A.
Instantiating Lexical Entries. At test time, for
a given clause ci and environment ei, we generate
set of logical forms zi = (`i, ξi). To do this, we
consider the lexical entries in A with the same verb
as ci. For each such lexical entry `i, we can map its
free variables v~ to objects in ei in an exponential
number of ways. Therefore, for each `i we only
consider the logical form (`i, ξi) where the map-
ping ξi obtains the highest score under the current
</bodyText>
<page confidence="0.993474">
996
</page>
<bodyText confidence="0.9981076">
model: ξz = arg maxi, φ(cz, zz−1, (Ez, ξ0), ez) ·
θ. For the feature vector φ that we consider,
this approximately translates to solving an integer
quadratic program with variables [yzj] ∈ {0, 1},
where yzj = 1 only if vz maps to object j.
</bodyText>
<sectionHeader confidence="0.9882405" genericHeader="method">
7 Environment-Driven Lexicon
Induction at Test Time
</sectionHeader>
<bodyText confidence="0.998764531914894">
Unfortunately, we cannot expect the initial lexicon
A induced from the training set to have full cover-
age of the required postconditions. Even after us-
ing 90% of the data for training, we encountered
17% new postconditions on the remaining 10%.
We therefore propose generating new lexical en-
tries at test time and adding them to A.
Formally, for a given environment ez and frame
node cz, we want to generate likely logical forms.
Although the space of all possible logical forms
is very large, the environment constrains the pos-
sible interpretations. We first compute the set
of atoms that are false in ez and that only con-
tain objects o that are “referred” to by either
cz or cz−1, where “refers” means that there ex-
ists some argument w in cz for which o ∈
arg maxof ρ(w, o0). For example, if cz corresponds
to the text “distribute pillows among the couches”,
we consider the atom on(pillow,, armchair,)
but not on(pillow,, snacktable2) since the ob-
ject armchair, has the highest correlation to the
description “couches”.
Next, for each atom, we convert it into a logi-
cal form z = (E, ξ) by replacing each object with
a variable. While this generalization gives us a
mapping ξ, we create a lexical entry Ez = [ν ⇒
(A�v.5, ∅)] without it, where 5 is the parameter-
ized atom. Note that the anchored mapping is
empty, representing the fact that this lexical en-
try was unseen during training time. For example,
the atom state(tv,, mute) would be converted to
the logical form (E, ξ), where E = [verb(cz) ⇒
(Av.state(v,mute), ∅] and ξ = {v → tv,}. We
do not generalize state names (e.g., mute) because
they generally are part of the meaning of the verb.
The score φ(cz, zz−1, zz, ez) · θ is computed
for the logical form zz produced by each post-
condition. We then take the conjunction of ev-
ery pair of postconditions corresponding to the
200 highest-scoring logical forms. This gives us
new set of postconditions on which we repeat
the generalization-scoring-conjunction cycle. We
keep doing this while the scores of the new logi-
cal forms is increasing or while there are logical
forms remaining.
e = [verb (;AS, f)]
such that verb = v(cq)
</bodyText>
<subsectionHeader confidence="0.528126">
Test Time Search for Logical Forms (Sec 7)
</subsectionHeader>
<bodyText confidence="0.895098">
x, = e, f,
where L = [verb a (A0. S, 0)]
is a test time lexical entry
</bodyText>
<subsectionHeader confidence="0.547396">
Set of Logical Forms for cr_yc, er,xr_c
</subsectionHeader>
<bodyText confidence="0.991193333333333">
Figure 4: Logical forms for a given clause cz, en-
vironment ez, and previous logical form zz−1 are
generated from both a lexicon induced from train-
ing data and a test-time search procedure based on
the environment.
If a logical form z = ([ν ⇒ (A�v.5, ∅)], ξ) is
used by the predicted action sequence, we add the
lexical entry [ν ⇒ (A�v.5, ξ)] to the lexicon A.
This is different to other lexicon induction proce-
dures such as GENLEX (Zettlemoyer and Collins,
2007) which are done at training time only and
require more supervision. Moreover, GENLEX
does not use the environment context in creating
new lexical entries and thus is not appropriate at
test time, since it would vastly overgenerate lexi-
cal entries compared to our approach. For us, the
environment thus provides implicit supervision for
lexicon induction.
</bodyText>
<sectionHeader confidence="0.950664" genericHeader="method">
8 Inference and Parameter Estimation
</sectionHeader>
<bodyText confidence="0.9999518">
Inference. Given a text x (which is converted to
c1:k via Section 3.2) and an initial environment
e1, we wish to predict an action sequence a based
on po(a1:k  |c1:k, e1), which marginalizes over all
logical forms z1:k (see Figure 2).
To enumerate possible logical forms, semantic
parsers typically lean heavily on a lexicon (Artzi
and Zettlemoyer, 2013), leading to high preci-
sion but lower recall, or search more aggressively
(Berant et al., 2013), leading to higher recall but
lower precision. We adopt the following hybrid
approach: Given ez, cz−1, cz and zz−1, we use both
the lexical entries in A as explained in Section 6
and the search procedure in Section 7 to generate
the set of possible logical forms for zz (see Fig-
ure 4). We use beam search, keeping only the
highest-scoring logical form with satisfiable post-
conditions for each i ∈ {1, ... , k} and resulting
action sequence a1:z.
Parameter Estimation. We split 10% of our
training data into a separate tuning set (the 90%
was used to infer the lexicon). On each example
in this set, we extracted the full sequence of logi-
cal forms z1:k from the action sequence a1:k based
on Section 6. For efficiency, we used an objective
</bodyText>
<figure confidence="0.647078666666667">
xr = (�,f�)
fr is the new assignment
Train Time Anchored Lexicon A (Sec 6)
</figure>
<page confidence="0.98117">
997
</page>
<bodyText confidence="0.96569225">
similar to pseudolikelihood to estimate the param-
eters θ. Specifically, we maximize the average log-
likelihood over each adjacent pair of logical forms
under ˜pθ:
</bodyText>
<equation confidence="0.96354">
˜fiB(zi  |zi−1, ci, ei) ∝ exp(o(ci, zi−1, zi, ei)TO). (2)
</equation>
<bodyText confidence="0.943414">
The weights were initialized to 0. We per-
formed 300 iterations over the validation set with
a learning rate of0.005
N .
</bodyText>
<sectionHeader confidence="0.914744" genericHeader="method">
9 Dataset and Experiments
</sectionHeader>
<subsectionHeader confidence="0.910372">
9.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999992192307692">
We collected a dataset of 500 examples from 62
people using a crowdsourcing system similar to
Misra et al. (2014). We consider two different 3D
scenarios: a kitchen and a living room, each con-
taining an average of 40 objects. Both of these
scenarios have 10 environments consisting of dif-
ferent sets of objects in different configurations.
We define 10 high-level objectives, 5 per scenario,
such as clean the room, make coffee, prepare room
for movie night, etc.
One group of users wrote natural language com-
mands to achieve the high-level objectives. An-
other group controlled a virtual robot to accom-
plish the commands given by the first group. The
dataset contains considerable variety, consisting of
148 different verbs, an average of 48.7 words per
text, and an average of 21.5 actions per action se-
quence. Users make spelling and grammar errors
in addition to occasionally taking random actions
not relevant to the text. The supplementary mate-
rial contains more details.
We filtered out 31 examples containing fewer
than two action sequences. Of the remaining ex-
amples, 378 were used for training and 91 were
used for test. Our algorithm is tested on four new
environments (two from each scenario).
</bodyText>
<subsectionHeader confidence="0.983294">
9.2 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.9951925">
Evaluation Metrics. We consider two metrics,
IED and END, which measure accuracy based on
the action sequence and environment, respectively.
Specifically, the IED metric (Misra et al., 2014) is
the edit distance between predicted and true action
sequence. The END metric is the Jaccard index of
sets A and B, where A is the set of atoms (e.g.,
on(cup1rtable1)) whose truth value changed
due to simulating the predicted action sequence,
and B is that of the true action sequence.
Baselines. We compare our algorithm with the
following baselines:
</bodyText>
<tableCaption confidence="0.991097">
Table 3: Results on the metrics and baselines de-
scribed in section 9.2. The numbers are normal-
ized to 100 with larger values being better.
</tableCaption>
<table confidence="0.999752">
Algorithm IED END
Chance 0.3 0.5
Manually Defined Templates 2.5 1.8
UBL- Best Parse (Kwiatkowski et al., 2010) 5.3 6.9
VEIL (Misra et al., 2014) 14.8 20.7
Model with only train-time lexicon induction 20.8 26.8
Model with only test-time lexicon induction 21.9 25.9
Full Model 22.3 28.8
</table>
<listItem confidence="0.992467705882353">
1. Chance: Randomly selects a logical form for
every frame node from the set of logical forms
generated by generalizing all possible postcon-
ditions that do not hold in the current environ-
ment. These postconditions could contain up to
93 atoms.
2. Manually Defined Templates: Defines a set
of postcondition templates for verbs similar to
Guadarrama (2013).
3. UBL-Best Parse (Kwiatkowski et al., 2010):
UBL algorithm trained on text aligned with post-
conditions and a noun-phrase seed lexicon. The
planner uses the highest scoring postcondition
given by UBL to infer the action sequence.
4. VEIL (Misra et al., 2014): Uses action se-
quences as logical forms and does not generate
lexical entries at test time.
</listItem>
<bodyText confidence="0.99994368">
We also consider two variations of our model: (i)
using only lexical entries induced using the train-
ing data, and (ii) using only the logical forms in-
duced at test-time by the search procedure.
The results are presented in Table 3. We ob-
serve that our full model outperforms the baseline
and the two pure search- and lexicon-based varia-
tions of our model. We further observe that adding
the search procedure (Section 7) improved the ac-
curacy by 1.5% on IED and 2% on END. The log-
ical forms generated by the search were able to
successfully map 48% of the new verbs.
Table 2 shows new verbs and concepts that the
algorithm was able to induce at test time. The
algorithm was able to correctly learn the lexi-
cal entries for the verbs “distribute” and “mix”,
while the ones for verbs “change” and “boil” were
only partly correct. The postconditions in Table 2
are not structurally isomorphic to previously-seen
logical forms; hence they could not have been
handled by using synonyms or factored lexicons
(Kwiatkowski et al., 2011). The poor performance
of UBL was because the best logical form often
produced an unsatisfiable postcondition. This can
be remedied by joint modeling with the environ-
</bodyText>
<page confidence="0.998453">
998
</page>
<tableCaption confidence="0.988498">
Table 2: New verbs and concepts induced at test time (Section 7).
</tableCaption>
<subsectionHeader confidence="0.789873">
Text Postcondition represented by the learned logical form # Log. forms explored
</subsectionHeader>
<bodyText confidence="0.997188611111111">
“mix it with ice cream and syrup”state(cup2, ice-cream1) ∧ state(cup2, vanilla) 15
“distribute among the couches” ∧j∈{1,3}on(pillowj, loveseat1) ∧ on(pillowi+1, armchairi+1)386
“boil it on the stove” state(stove, stovefire1) ∧ state(kettle, water) 109
“change the channel to a movie” state(tv1, channel4) ∧ on(book1, loveseat1) 98
ment. The VEIL baseline used actions for repre-
sentation and does not generalize as well as the
postconditions in our logical forms.
It is also instructive to examine the alternate
postconditions that the search procedure consid-
ers. For the first example in Table 2, the following
postcondition was considered by not selected:
grasping(robot, icecream2)∧grasping(robot, syrup1)
While this postcondition uses all the objects de-
scribed in the text, the environment-based features
suggest it makes little sense for the task to end with
the robot eternally grasping objects. For the sec-
ond example, alternate postconditions considered
included:
</bodyText>
<equation confidence="0.872720666666667">
1. on(pillow1, pillow2) ∧ on(pillow3, pillow4)
2. ∧4j=1 on(pillowj, loveseat1)
3. ∧3j=1 near(robot1, armchairj)
</equation>
<bodyText confidence="0.999973470588235">
The algorithm did not choose options 1 or 3
since the environment-based features recognizes
these as unlikely configurations. Option 2 was
ruled out since the recall correlation feature real-
izes that not all the couches are mentioned in the
postcondition.
To test how much features on the environment
help, we removed all such features from our full
model. We found that the accuracy fell to 16.0%
on the IED metric and 16.6% on the END metric,
showing that the environment is crucial.
In this work, we relied on a simple deterministic
shallow parsing step. We found that shallow pars-
ing was able to correctly process the text in only
46% of the test examples, suggesting that improv-
ing this initial component or at least modeling the
uncertainty there would be beneficial.
</bodyText>
<sectionHeader confidence="0.999317" genericHeader="method">
10 Related Work
</sectionHeader>
<bodyText confidence="0.999544839285715">
Our work uses semantic parsing to map nat-
ural language instructions to actions via novel
concepts, which brings together several themes:
actions, semantic parsing, novel concepts, and
robotics.
Mapping Text to Actions. Several works (Brana-
van et al., 2009; Branavan et al., 2010; Vogel and
Jurafsky, 2010) use reinforcement learning to di-
rectly map to text to actions, and do not even re-
quire an explicit model of the environment. How-
ever, they can only handle simple actions, whereas
our planner and simulator allows us to work with
postconditions, and thus tackle high-level instruc-
tions. Branavan et al. (2012) extract precondi-
tion relations from text, learn to map text to sub-
goals (postconditions) for a planner. However,
their postconditions are atomic, whereas ours are
complex conjunctions.
Other works (Chen and Mooney, 2011; Kim
and Mooney, 2012; Kollar et al., 2010; Fasola and
Mataric, 2013) have focused only on navigational
verbs and spatial relations, but do not handle high-
level verbs. Artzi and Zettlemoyer (2013) also fall
into the above category and offer a more composi-
tional treatment. They focus on how words com-
pose; we focus on unraveling single words.
The broader problem of grounded language ac-
quisition, involving connecting words to aspects of
a situated context has been heavily studied (Duval-
let et al., 2014; Yu and Siskind, 2013; Chu et al.,
2013; Chen and Mooney, 2008; Mooney, 2008;
Fleischman and Roy, 2005; Liang et al., 2009).
Semantic Parsing. In semantic parsing, much
work has leveraged CCG (Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Kwiatkowski et al., 2010). One challenge behind
lexically-heavy approaches is ensuring adequate
lexical coverage. Kwiatkowski et al. (2011) en-
hanced generalization by factoring a lexical entry
into a template plus a lexeme, but the rigidity of
the template remains. This is satisfactory when
words map to one (or two) predicates, which is the
case in most existing semantic parsing tasks. For
example, in Artzi and Zettlemoyer (2013), verbs
are associated with single predicates (“move” to
move, “walk” to walk, etc.) In our setting, verbs
contain multi-predicate postconditions, for which
these techniques would not be suitable.
As annotated logical forms for training seman-
tic parsers are expensive to obtain, several works
(Clarke et al., 2010; Liang et al., 2011; Berant et
al., 2013; Kwiatkowski et al., 2013) have devel-
oped methods to learn from weaker supervision,
and as in our work, use the execution of the logi-
cal forms to guide the search. Our supervision is
even weaker in that we are able to learn at test time
</bodyText>
<page confidence="0.996933">
999
</page>
<bodyText confidence="0.99878148">
from partial environment constraints.
Grounding to Novel Concepts. Guadarrama et
al. (2014) map open vocabulary text to objects in
an image using a large database. Matuszek et al.
(2012a) create new predicates for every new ad-
jective at test time. Others (Kirk et al., 2014) ask
users for clarification. In contrast, we neither have
access to large databases for this problem, nor do
we do create new predicates or use explicit super-
vision at test time.
Robotic Applications. Our motivation behind this
work is to build robotic systems capable of taking
commands from users. Other works in this area
have considered mapping text to a variety of ma-
nipulation actions (Sung et al., 2015). Levine et
al. (2015) and Lenz et al. (2015) focus on spe-
cific manipulation actions. In order to build a rep-
resentation of the environment, Ren et al. (2012)
and Wu et al. (2014) present vision algorithms but
only output symbolic labels, which could act as
inputs to our system. In future work, we also plan
to integrate our work with RoboBrain (Saxena et
al., 2014) to leverage these existing systems for
building a robotic system capable of working with
physical world data.
</bodyText>
<sectionHeader confidence="0.982389" genericHeader="method">
11 Conclusion
</sectionHeader>
<bodyText confidence="0.99991755">
We have presented an algorithm for mapping text
to actions that induces lexical entries at test time
using the environment. Our algorithm couples the
lexicon extracted from training data with a test-
time search that uses the environment to reduce
the space of logical forms. Our results suggest that
using the environment to provide lexical coverage
of high-level concepts is a promising avenue for
further research.
Acknowledgements. This research was supported
by the ONR (award N00014-14-1-0156), a Sloan
Research Fellowship to the third author, and a Mi-
crosoft Research Faculty Fellowship and NSF Ca-
reer Award to the fourth author. We thank Aditya
Jami and Jaeyong Sung for useful discussions. We
also thank Jiaqi Su for her help with data collec-
tion and all the people who participated in the user
study.
Reproducibility. Code, data, and experiments for
this paper are available on the CodaLab platform
</bodyText>
<footnote confidence="0.632331">
at https://www.codalab.org/worksheets/
0x7f9151ec074f4f589e4d4786db7bb6de/. De-
mos can be found at http://tellmedave.com.
</footnote>
<sectionHeader confidence="0.7252225" genericHeader="method">
Appendix: Parsing Text into Control Flow
Graph.
</sectionHeader>
<bodyText confidence="0.95549">
We first decompose the text x into its control
flow graph G using a simple set of rules:
</bodyText>
<listItem confidence="0.9490108">
• The parse tree of x is generated using the Stan-
ford parser (Klein and Manning, 2003) and a
frame node is created for each non-auxiliary
verb node in the tree.
• Conditional nodes are discovered by look-
</listItem>
<bodyText confidence="0.986326888888889">
ing for the keywords until, if, after, when.
The associated subtree is then parsed deter-
ministically using a set of a rules. For
example, a rule parses “for x minutes” to
for(digit:x,unit:minutes). We found that
all conditionals can be interpreted against the
initial environment el, since our world is fully-
observable, deterministic, and the user giving
the command has full view of the world.
</bodyText>
<listItem confidence="0.804666363636364">
• To find objects, we look for anaphoric terminal
nodes or nominals whose parent is not a nominal
or which have a PP sibling. These are processed
into object descriptions w.
• Object descriptions w are attached to the frame
node, whose verb is nearest in the parse tree to
the main noun of w.
• Nodes corresponding to {IN,TO,CC,“,”} are
added as the relation between the corresponding
argument objects.
• If there is a conjunction between two objects in
</listItem>
<bodyText confidence="0.942157733333333">
a frame node and if these objects have the same
relation to other objects, then we split the frame
node into two sequential frame nodes around
these objects. For example, a frame node corre-
sponding to the text segment “take the cup and
bowl from table” is split into two frame nodes
corresponding to “take the cup from table” and
“take bowl from table”.
• A temporal edge is added between successive
frame nodes in the same branch of a condition.
A temporal edge is added between a conditional
node and head of the true and false branches of
the condition. The end of all branches in a sen-
tence are joined to the starting node of the suc-
cessive sentence.
</bodyText>
<sectionHeader confidence="0.998562" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996210090909091">
Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instruc-
tions to actions. Transactions of the Association for
Computational Linguistics (TACL), 1:49–62.
J. Berant and P. Liang. 2014. Semantic parsing via
paraphrasing. In Association for Computational
Linguistics (ACL).
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.
Semantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
</reference>
<page confidence="0.991304">
1000
</page>
<bodyText confidence="0.994928">
M. Bollini, J. Barry, and D. Rus. 2011. Bakebot: Bak-
ing cookies with the PR2. In The PR2 Workshop,
IROS.
M. Fleischman and D. Roy. 2005. Intentional context
in situated natural language learning. In Computa-
tional Natural Language Learning (CoNLL), pages
104–111.
</bodyText>
<reference confidence="0.999032555555556">
S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping
instructions to actions. In Association for Com-
putational Linguistics and International Joint Con-
ference on Natural Language Processing (ACL-
IJCNLP), pages 82–90.
S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-
level instructions to commands. In Association
for Computational Linguistics (ACL), pages 1268–
1277.
S. Branavan, N. Kushman, T. Lei, and R. Barzilay.
2012. Learning high-level planning from text. In
Association for Computational Linguistics (ACL),
pages 126–135.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In International Conference on Machine Learning
(ICML), pages 128–135.
D. L. Chen and R. J. Mooney. 2011. Learning to in-
terpret natural language navigation instructions from
observations. In Association for the Advancement of
Artificial Intelligence (AAAI), pages 859–865.
V. Chu, I. McMahon, L. Riano, C. McDonald, Q. He,
J. Perez-Tejada, M. Arrigo, N. Fitter, J. Nappo,
T. Darrell, et al. 2013. Using robotic exploratory
procedures to learn the meaning of haptic adjectives.
In International Conference on Intelligent Robots
and Systems (IROS).
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL), pages 18–27.
F. Duvallet, M. R. Walter, T. Howard, S. Hemachan-
dra, J. Oh, S. Teller, N. Roy, and A. Stentz. 2014.
Inferring maps and behaviors from natural language
instructions. In International Symposium on Exper-
imental Robotics (ISER).
J. Fasola and M. Mataric. 2013. Using semantic fields
to model dynamic spatial relations in a robot archi-
tecture for natural language instruction of service
robots. In International Conference on Intelligent
Robots and Systems (IROS).
J. Fasola and M. J. Matari’c. 2014. Interpreting
instruction sequences in spatial language discourse
with pragmatics towards natural human-robot inter-
action. In International Conference on Robotics and
Automation (ICRA), pages 6667–6672.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
S. Guadarrama, L. Riano, D. Golland, D. Gouhring,
Y. Jia, D. Klein, P. Abbeel, and T. Darrell. 2013.
Grounding spatial relations for human-robot inter-
action. In International Conference on Intelligent
Robots and Systems (IROS).
S. Guadarrama, E. Rodner, K. Saenko, N. Zhang,
R. Farrell, J. Donahue, and T. Darrell. 2014. Open-
vocabulary object retrieval. In Robotics: Science
and Systems (RSS).
J. Kim and R. Mooney. 2012. Unsupervised PCFG in-
duction for grounded language learning with highly
ambiguous supervision. In Computational Natural
Language Learning (CoNLL), pages 433–444.
N. H. Kirk, D. Nyga, and M. Beetz. 2014. Con-
trolled natural languages for language generation in
artificial cognition. In International Conference on
Robotics and Automation (ICRA), pages 6667–6672.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Association for Computational Lin-
guistics (ACL), pages 423–430.
T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010.
Grounding verbs of motion in natural language com-
mands to robots. In International Symposium on Ex-
perimental Robotics (ISER).
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order uni-
fication. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1223–1233.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical generalization in
CCG grammar induction for semantic parsing. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1512–1523.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
I. Lenz, R. Knepper, and A. Saxena. 2015. Deepmpc:
Learning deep latent features for model predictive
control. In Robotics Science and Systems (RSS).
S. Levine, C. Finn, T. Darrell, and P. Abbeel. 2015.
End-to-end training of deep visuomotor policies.
arXiv preprint arXiv:1504.00702.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and In-
ternational Joint Conference on Natural Language
Processing (ACL-IJCNLP), pages 91–99.
</reference>
<page confidence="0.692785">
1001
</page>
<reference confidence="0.999702887096774">
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL),
pages 590–599.
C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo,
and D. Fox. 2012a. A joint model of language and
perception for grounded attribute learning. In Inter-
national Conference on Machine Learning (ICML),
pages 1671–1678.
C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox.
2012b. Learning to parse natural language com-
mands to a robot control system. In International
Symposium on Experimental Robotics (ISER).
D. Misra, J. Sung, K. Lee, and A. Saxena. 2014. Tell
Me Dave: Context-sensitive grounding of natural
language to mobile manipulation instructions. In
Robotics: Science and Systems (RSS).
R. Mooney. 2008. Learning to connect language and
perception. In Association for the Advancement of
Artificial Intelligence (AAAI), pages 1598–1601.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29:19–51.
X. Ren, L. Bo, and D. Fox. 2012. Rgb-(d) scene label-
ing: Features and algorithms. In Computer Vision
and Pattern Recognition (CVPR), pages 2759–2766.
J. Rintanen. 2012. Planning as satisfiability: Heuris-
tics. Artificial Intelligence, 193.
A. Saxena, A. Jain, O. Sener, A. Jami, D. K. Misra,
and H. S. Koppula. 2014. Robobrain: Large-
scale knowledge engine for robots. arXiv preprint
arXiv:1412.0691.
J. Sung, S. H. Jin, and A. Saxena. 2015. Robobarista:
Object part based transfer of manipulation trajecto-
ries from crowd-sourcing in 3d pointclouds. arXiv
preprint arXiv:1504.03071.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.
Banerjee, S. J. Teller, and N. Roy. 2011. Un-
derstanding natural language commands for robotic
navigation and mobile manipulation. In Associa-
tion for the Advancement of Artificial Intelligence
(AAAI).
A. Vogel and D. Jurafsky. 2010. Learning to follow
navigational directions. In Association for Compu-
tational Linguistics (ACL), pages 806–814.
C. Wu, I. Lenz, and A. Saxena. 2014. Hierarchical se-
mantic labeling for task-relevant RGB-D perception.
In Robotics: Science and Systems (RSS).
H. Yu and J. M. Siskind. 2013. Grounded language
learning from video described with sentences. In
Association for Computational Linguistics (ACL),
pages 53–63.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Un-
certainty in Artificial Intelligence (UAI), pages 658–
666.
L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to log-
ical form. In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL), pages 678–687.
</reference>
<page confidence="0.995257">
1002
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.538436">
<title confidence="0.999858">Environment-Driven Lexicon Induction for High-Level Instructions</title>
<author confidence="0.999244">K Kejia Percy Ashutosh</author>
<email confidence="0.976195">dkm@cs.cornell.edukt454@cornell.edupliang@cs.stanford.eduasaxena@cs.cornell.edu</email>
<affiliation confidence="0.792707">of Computer Science, Cornell of Computer Science, Stanford University</affiliation>
<abstract confidence="0.996337545454546">We focus on the task of interpreting complex natural language instructions to a robot, in which we must ground high-level such as the cup low-level actions such as grasping. Previous approaches that learn a lexicon during training have inadequate coverage at test time, and pure search strategies cannot handle the exponential search space. We propose a new hybrid approach that leverages the environment to induce new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to sucground new verbs such as dismix, arrange complex logical forms, each containing up to four predicates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>1--49</pages>
<contexts>
<context position="1297" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="189" endWordPosition="192">ial search space. We propose a new hybrid approach that leverages the environment to induce new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute, mix, arrange to complex logical forms, each containing up to four predicates. 1 Introduction The task of mapping natural language instructions to actions for a robot has been gaining momentum in recent years (Artzi and Zettlemoyer, 2013; Tellex et al., 2011; Misra et al., 2014; Bollini et al., 2011; Guadarrama et al., 2013; Matuszek et al., 2012b; Fasola and Mataric, 2013). We are particularly interested in instructions containing verbs such as “microwave” denoting high-level concepts, which correspond to more than 10 lowlevel symbolic actions such as grasp. In this setting, it is common to find new verbs requiring new concepts at test time. For example, in Figure 1, suppose that we have never seen the verb “fill”. Can we impute the correct interpretation, and moreover seize the opportunity to learn what “fill” means in a wa</context>
<context position="5241" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="867" endWordPosition="870">ntic parsing model maps these frame nodes to logical forms {zi} representing the postconditions. From this, a planner and simulator generate the action sequences {ai} and resulting environments {ei}. a signal for inducing new lexical entries that map previously unseen verbs to novel concepts. In the example in Figure 1, the algorithm learns that microwaving an empty cup is unlikely and this suggests that the verb “fill” must map to actions that end up making the cup not empty. Another contribution of this paper is using postconditions as logical forms rather than actions, as in previous work (Artzi and Zettlemoyer, 2013; Misra et al., 2014). Postconditions not only reduce the search space of logical forms, but are also a more natural representation of verbs. We define a conditional random field (CRF) model over postconditions, and use a planner to convert postconditions into action sequences and a simulator to generate new environments. At test time, we use the lexicon induced from the training data, but also perform an environmentguided search over logical forms to induce new lexical entries on-the-fly. If the predicted action sequence uses a new lexical entry generated by the search, it is added to the lex</context>
<context position="14211" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="2397" endWordPosition="2400">contains lexical entries E of the following form: [ν ⇒ (λ�v.S, ξ)] where, ν is a verb, S is a postcondition with free variables v, and ξ is a mapping of these variables to objects. An example lexical entry is: [pour ⇒ (λv1v2v3.S, ξ)], where: S = grasping(v,, v2) n near(v,, v3) n -state(v2, milk) n state(v3, milk) ξ ={v, --+ robot,, v2 --+ cup1, v3 --+ bowl3} (anchoring) As Table 1 shows, a single verb will in general have multiple entries due to a combination of polysemy and the fact that language is higher-level than postconditions. Advantages of Postconditions. In contrast to previous work (Artzi and Zettlemoyer, 2013; Misra et al., 2014), we use postconditions instead of action sequence for two main reasons. First, postconditions generalize better. To illustrate this, consider the action sequence for the simple task of filling a cup with water. At the time of learning the lexicon, the action sequence might correspond to using a tap for filling the cup while at test time, the environment may not have a tap but instead have a pot with water. Thus, if the lexicon maps to action sequence, then it will not be applicable at test time whereas the postcondition state(z,, water) is valid in both cases. We thus shi</context>
<context position="25930" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="4436" endWordPosition="4439">text in creating new lexical entries and thus is not appropriate at test time, since it would vastly overgenerate lexical entries compared to our approach. For us, the environment thus provides implicit supervision for lexicon induction. 8 Inference and Parameter Estimation Inference. Given a text x (which is converted to c1:k via Section 3.2) and an initial environment e1, we wish to predict an action sequence a based on po(a1:k |c1:k, e1), which marginalizes over all logical forms z1:k (see Figure 2). To enumerate possible logical forms, semantic parsers typically lean heavily on a lexicon (Artzi and Zettlemoyer, 2013), leading to high precision but lower recall, or search more aggressively (Berant et al., 2013), leading to higher recall but lower precision. We adopt the following hybrid approach: Given ez, cz−1, cz and zz−1, we use both the lexical entries in A as explained in Section 6 and the search procedure in Section 7 to generate the set of possible logical forms for zz (see Figure 4). We use beam search, keeping only the highest-scoring logical form with satisfiable postconditions for each i ∈ {1, ... , k} and resulting action sequence a1:z. Parameter Estimation. We split 10% of our training data in</context>
<context position="34376" citStr="Artzi and Zettlemoyer (2013)" startWordPosition="5824" endWordPosition="5827">it model of the environment. However, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One chall</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics (TACL), 1:49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2545" citStr="Berant and Liang, 2014" startWordPosition="403" endWordPosition="406">ure instructions? Text: “get the cup, fill it with water and then microwave the cup” Figure 1: A lexicon learned on the training data cannot possibly cover all the verb-concept mappings needed at test time. Our algorithm learns the meaning of new verbs (e.g., “fill”) using the environment context. Previous work in semantic parsing handles lexical coverage in one of two ways. Kwiatkowski et al. (2010) induces a highly constrained CCG lexicon capable of mapping words to complex logical forms, but it would have to skip new words (which in Figure 1 would lead to microwaving an empty cup). Others (Berant and Liang, 2014) take a freer approach by performing a search over logical forms, which can handle new words, but the logical forms there are much simpler than the ones we consider. In this paper, we present an hybrid approach that uses a lexicon to represent complex concepts but also strongly leverages the environment to guide the search space. The environment can provide helpful cues in several ways: • Only a few environments are likely for a given scenario—e.g., the text is unlikely to ask the robot to microwave an empty cup or put books on the floor. • The logical form of one segment of text constrains th</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>J. Berant and P. Liang. 2014. Semantic parsing via paraphrasing. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>A Chou</author>
<author>R Frostig</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="26025" citStr="Berant et al., 2013" startWordPosition="4452" endWordPosition="4455">ergenerate lexical entries compared to our approach. For us, the environment thus provides implicit supervision for lexicon induction. 8 Inference and Parameter Estimation Inference. Given a text x (which is converted to c1:k via Section 3.2) and an initial environment e1, we wish to predict an action sequence a based on po(a1:k |c1:k, e1), which marginalizes over all logical forms z1:k (see Figure 2). To enumerate possible logical forms, semantic parsers typically lean heavily on a lexicon (Artzi and Zettlemoyer, 2013), leading to high precision but lower recall, or search more aggressively (Berant et al., 2013), leading to higher recall but lower precision. We adopt the following hybrid approach: Given ez, cz−1, cz and zz−1, we use both the lexical entries in A as explained in Section 6 and the search procedure in Section 7 to generate the set of possible logical forms for zz (see Figure 4). We use beam search, keeping only the highest-scoring logical form with satisfiable postconditions for each i ∈ {1, ... , k} and resulting action sequence a1:z. Parameter Estimation. We split 10% of our training data into a separate tuning set (the 90% was used to infer the lexicon). On each example in this set, </context>
<context position="35727" citStr="Berant et al., 2013" startWordPosition="6038" endWordPosition="6041">actoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time 999 from partial environment constraints. Grounding to Novel Concepts. Guadarrama et al. (2014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>H Chen</author>
<author>L S Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACLIJCNLP),</booktitle>
<pages>82--90</pages>
<contexts>
<context position="33601" citStr="Branavan et al., 2009" startWordPosition="5698" endWordPosition="5702">% on the END metric, showing that the environment is crucial. In this work, we relied on a simple deterministic shallow parsing step. We found that shallow parsing was able to correctly process the text in only 46% of the test examples, suggesting that improving this initial component or at least modeling the uncertainty there would be beneficial. 10 Related Work Our work uses semantic parsing to map natural language instructions to actions via novel concepts, which brings together several themes: actions, semantic parsing, novel concepts, and robotics. Mapping Text to Actions. Several works (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010) use reinforcement learning to directly map to text to actions, and do not even require an explicit model of the environment. However, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACLIJCNLP), pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>L Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reading between the lines: Learning to map highlevel instructions to commands.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1268--1277</pages>
<contexts>
<context position="33624" citStr="Branavan et al., 2010" startWordPosition="5703" endWordPosition="5706">owing that the environment is crucial. In this work, we relied on a simple deterministic shallow parsing step. We found that shallow parsing was able to correctly process the text in only 46% of the test examples, suggesting that improving this initial component or at least modeling the uncertainty there would be beneficial. 10 Related Work Our work uses semantic parsing to map natural language instructions to actions via novel concepts, which brings together several themes: actions, semantic parsing, novel concepts, and robotics. Mapping Text to Actions. Several works (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010) use reinforcement learning to directly map to text to actions, and do not even require an explicit model of the environment. However, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; </context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010. Reading between the lines: Learning to map highlevel instructions to commands. In Association for Computational Linguistics (ACL), pages 1268– 1277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Branavan</author>
<author>N Kushman</author>
<author>T Lei</author>
<author>R Barzilay</author>
</authors>
<title>Learning high-level planning from text.</title>
<date>2012</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>126--135</pages>
<contexts>
<context position="33959" citStr="Branavan et al. (2012)" startWordPosition="5759" endWordPosition="5762">ted Work Our work uses semantic parsing to map natural language instructions to actions via novel concepts, which brings together several themes: actions, semantic parsing, novel concepts, and robotics. Mapping Text to Actions. Several works (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010) use reinforcement learning to directly map to text to actions, and do not even require an explicit model of the environment. However, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded lang</context>
</contexts>
<marker>Branavan, Kushman, Lei, Barzilay, 2012</marker>
<rawString>S. Branavan, N. Kushman, T. Lei, and R. Barzilay. 2012. Learning high-level planning from text. In Association for Computational Linguistics (ACL), pages 126–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
<author>R J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>128--135</pages>
<contexts>
<context position="34747" citStr="Chen and Mooney, 2008" startWordPosition="5888" endWordPosition="5891">plex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Ar</context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>D. L. Chen and R. J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In International Conference on Machine Learning (ICML), pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
<author>R J Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>859--865</pages>
<contexts>
<context position="34179" citStr="Chen and Mooney, 2011" startWordPosition="5792" endWordPosition="5795">tions. Several works (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010) use reinforcement learning to directly map to text to actions, and do not even require an explicit model of the environment. However, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and R</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>D. L. Chen and R. J. Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In Association for the Advancement of Artificial Intelligence (AAAI), pages 859–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Chu</author>
<author>I McMahon</author>
<author>L Riano</author>
<author>C McDonald</author>
<author>Q He</author>
<author>J Perez-Tejada</author>
<author>M Arrigo</author>
<author>N Fitter</author>
<author>J Nappo</author>
<author>T Darrell</author>
</authors>
<title>Using robotic exploratory procedures to learn the meaning of haptic adjectives.</title>
<date>2013</date>
<booktitle>In International Conference on Intelligent Robots and Systems (IROS).</booktitle>
<contexts>
<context position="34724" citStr="Chu et al., 2013" startWordPosition="5884" endWordPosition="5887">ereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing ta</context>
</contexts>
<marker>Chu, McMahon, Riano, McDonald, He, Perez-Tejada, Arrigo, Fitter, Nappo, Darrell, 2013</marker>
<rawString>V. Chu, I. McMahon, L. Riano, C. McDonald, Q. He, J. Perez-Tejada, M. Arrigo, N. Fitter, J. Nappo, T. Darrell, et al. 2013. Using robotic exploratory procedures to learn the meaning of haptic adjectives. In International Conference on Intelligent Robots and Systems (IROS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>D Goldwasser</author>
<author>M Chang</author>
<author>D Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<journal>In Computational Natural Language Learning (CoNLL),</journal>
<pages>18--27</pages>
<contexts>
<context position="35686" citStr="Clarke et al., 2010" startWordPosition="6030" endWordPosition="6033">t al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time 999 from partial environment constraints. Grounding to Novel Concepts. Guadarrama et al. (2014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, </context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010. Driving semantic parsing from the world’s response. In Computational Natural Language Learning (CoNLL), pages 18–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Duvallet</author>
<author>M R Walter</author>
<author>T Howard</author>
<author>S Hemachandra</author>
<author>J Oh</author>
<author>S Teller</author>
<author>N Roy</author>
<author>A Stentz</author>
</authors>
<title>Inferring maps and behaviors from natural language instructions.</title>
<date>2014</date>
<booktitle>In International Symposium on Experimental Robotics (ISER).</booktitle>
<contexts>
<context position="34684" citStr="Duvallet et al., 2014" startWordPosition="5875" endWordPosition="5879"> However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the c</context>
</contexts>
<marker>Duvallet, Walter, Howard, Hemachandra, Oh, Teller, Roy, Stentz, 2014</marker>
<rawString>F. Duvallet, M. R. Walter, T. Howard, S. Hemachandra, J. Oh, S. Teller, N. Roy, and A. Stentz. 2014. Inferring maps and behaviors from natural language instructions. In International Symposium on Experimental Robotics (ISER).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fasola</author>
<author>M Mataric</author>
</authors>
<title>Using semantic fields to model dynamic spatial relations in a robot architecture for natural language instruction of service robots.</title>
<date>2013</date>
<booktitle>In International Conference on Intelligent Robots and Systems (IROS).</booktitle>
<contexts>
<context position="1436" citStr="Fasola and Mataric, 2013" startWordPosition="213" endWordPosition="216">rbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute, mix, arrange to complex logical forms, each containing up to four predicates. 1 Introduction The task of mapping natural language instructions to actions for a robot has been gaining momentum in recent years (Artzi and Zettlemoyer, 2013; Tellex et al., 2011; Misra et al., 2014; Bollini et al., 2011; Guadarrama et al., 2013; Matuszek et al., 2012b; Fasola and Mataric, 2013). We are particularly interested in instructions containing verbs such as “microwave” denoting high-level concepts, which correspond to more than 10 lowlevel symbolic actions such as grasp. In this setting, it is common to find new verbs requiring new concepts at test time. For example, in Figure 1, suppose that we have never seen the verb “fill”. Can we impute the correct interpretation, and moreover seize the opportunity to learn what “fill” means in a way that generalizes to future instructions? Text: “get the cup, fill it with water and then microwave the cup” Figure 1: A lexicon learned o</context>
<context position="34249" citStr="Fasola and Mataric, 2013" startWordPosition="5804" endWordPosition="5807">; Vogel and Jurafsky, 2010) use reinforcement learning to directly map to text to actions, and do not even require an explicit model of the environment. However, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, </context>
</contexts>
<marker>Fasola, Mataric, 2013</marker>
<rawString>J. Fasola and M. Mataric. 2013. Using semantic fields to model dynamic spatial relations in a robot architecture for natural language instruction of service robots. In International Conference on Intelligent Robots and Systems (IROS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fasola</author>
<author>M J Matari’c</author>
</authors>
<title>Interpreting instruction sequences in spatial language discourse with pragmatics towards natural human-robot interaction.</title>
<date>2014</date>
<booktitle>In International Conference on Robotics and Automation (ICRA),</booktitle>
<pages>6667--6672</pages>
<marker>Fasola, Matari’c, 2014</marker>
<rawString>J. Fasola and M. J. Matari’c. 2014. Interpreting instruction sequences in spatial language discourse with pragmatics towards natural human-robot interaction. In International Conference on Robotics and Automation (ICRA), pages 6667–6672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="20677" citStr="Fellbaum, 1998" startWordPosition="3503" endWordPosition="3504">ct description ω, an object o, and a set of previously seen objects (used for anaphoric resolution), we define the correlation ρ(ω, o) using the following approach: • If ω is a pronoun, ρ(ω, o) is the ratio of the position of the last reference of o to the length of the action sequence computed so far, thus preferring recent objects. • Otherwise, we compute the correlation using various sources: the object’s category; the object’s state for handling metonymy (e.g., the description “coffee” correlates well with the object mug, if mug, contains coffee— state(mug,, has-coffee) is true), WordNet (Fellbaum, 1998) for dealing synonymy and hyponymy; and word alignments between the objects and text from Giza++ (Och and Ney, 2003) to learn domain-specific references (e.g., “Guinness book” refers to book,, not book2). More details can be found in the supplemental material. 6 Lexicon Induction from Training Data In order to map text to logical forms, we first induce an initial anchored lexicon A from the training data {(x(m), e(m), a(m), π(m))}Mm=1. At test time, we add new lexical entries (Section 7) to A. Recall that shallow parsing x(m) yields a list of frame nodes c1:k. For each frame node ci and its al</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Guadarrama</author>
<author>L Riano</author>
<author>D Golland</author>
<author>D Gouhring</author>
<author>Y Jia</author>
<author>D Klein</author>
<author>P Abbeel</author>
<author>T Darrell</author>
</authors>
<title>Grounding spatial relations for human-robot interaction.</title>
<date>2013</date>
<booktitle>In International Conference on Intelligent Robots and Systems (IROS).</booktitle>
<contexts>
<context position="1385" citStr="Guadarrama et al., 2013" startWordPosition="205" endWordPosition="208">new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute, mix, arrange to complex logical forms, each containing up to four predicates. 1 Introduction The task of mapping natural language instructions to actions for a robot has been gaining momentum in recent years (Artzi and Zettlemoyer, 2013; Tellex et al., 2011; Misra et al., 2014; Bollini et al., 2011; Guadarrama et al., 2013; Matuszek et al., 2012b; Fasola and Mataric, 2013). We are particularly interested in instructions containing verbs such as “microwave” denoting high-level concepts, which correspond to more than 10 lowlevel symbolic actions such as grasp. In this setting, it is common to find new verbs requiring new concepts at test time. For example, in Figure 1, suppose that we have never seen the verb “fill”. Can we impute the correct interpretation, and moreover seize the opportunity to learn what “fill” means in a way that generalizes to future instructions? Text: “get the cup, fill it with water and th</context>
</contexts>
<marker>Guadarrama, Riano, Golland, Gouhring, Jia, Klein, Abbeel, Darrell, 2013</marker>
<rawString>S. Guadarrama, L. Riano, D. Golland, D. Gouhring, Y. Jia, D. Klein, P. Abbeel, and T. Darrell. 2013. Grounding spatial relations for human-robot interaction. In International Conference on Intelligent Robots and Systems (IROS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Guadarrama</author>
<author>E Rodner</author>
<author>K Saenko</author>
<author>N Zhang</author>
<author>R Farrell</author>
<author>J Donahue</author>
<author>T Darrell</author>
</authors>
<title>Openvocabulary object retrieval.</title>
<date>2014</date>
<booktitle>In Robotics: Science and Systems (RSS).</booktitle>
<contexts>
<context position="19675" citStr="Guadarrama et al., 2014" startWordPosition="3328" endWordPosition="3331">nd noise. We add a feature based on the average correlation between the objects of the new mapping ξi with the corresponding objects in the anchored mapping ξ. The other features are based on the relationship between object descriptions, similarity between ξ and ξi and transition probabilities between logical forms zi_1 and zi. These probabilities are also learned from training data. Mapping Object Descriptions. Our features rely on a mapping from object descriptions ω (e.g., “the red shiny cup”) to objects o (e.g., cup$), which has been addressed in many recent works (Matuszek et al., 2012a; Guadarrama et al., 2014; Fasola and Matari’c, 2014). One key idea is: instead of computing rigid lexical entries such as cup → cup,, we use a continuous correlation score ρ(ω, o) E [0, 1] that measures how well ω describes o. This flexibility allows the algorithm to use objects not explicitly mentioned in text. Given “get me a tank of water”, we might choose an approximate vessel (e.g., cup2). Given an object description ω, an object o, and a set of previously seen objects (used for anaphoric resolution), we define the correlation ρ(ω, o) using the following approach: • If ω is a pronoun, ρ(ω, o) is the ratio of the</context>
<context position="36060" citStr="Guadarrama et al. (2014)" startWordPosition="6096" endWordPosition="6099">“walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time 999 from partial environment constraints. Grounding to Novel Concepts. Guadarrama et al. (2014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit supervision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levi</context>
</contexts>
<marker>Guadarrama, Rodner, Saenko, Zhang, Farrell, Donahue, Darrell, 2014</marker>
<rawString>S. Guadarrama, E. Rodner, K. Saenko, N. Zhang, R. Farrell, J. Donahue, and T. Darrell. 2014. Openvocabulary object retrieval. In Robotics: Science and Systems (RSS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>R Mooney</author>
</authors>
<title>Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision.</title>
<date>2012</date>
<booktitle>In Computational Natural Language Learning (CoNLL),</booktitle>
<pages>433--444</pages>
<contexts>
<context position="34201" citStr="Kim and Mooney, 2012" startWordPosition="5796" endWordPosition="5799">ranavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010) use reinforcement learning to directly map to text to actions, and do not even require an explicit model of the environment. However, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al.</context>
</contexts>
<marker>Kim, Mooney, 2012</marker>
<rawString>J. Kim and R. Mooney. 2012. Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision. In Computational Natural Language Learning (CoNLL), pages 433–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N H Kirk</author>
<author>D Nyga</author>
<author>M Beetz</author>
</authors>
<title>Controlled natural languages for language generation in artificial cognition.</title>
<date>2014</date>
<booktitle>In International Conference on Robotics and Automation (ICRA),</booktitle>
<pages>6667--6672</pages>
<contexts>
<context position="36243" citStr="Kirk et al., 2014" startWordPosition="6129" endWordPosition="6132">s are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time 999 from partial environment constraints. Grounding to Novel Concepts. Guadarrama et al. (2014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit supervision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levine et al. (2015) and Lenz et al. (2015) focus on specific manipulation actions. In order to build a representation of the environment, Ren et al. (2012) and Wu et al. (2014) present v</context>
</contexts>
<marker>Kirk, Nyga, Beetz, 2014</marker>
<rawString>N. H. Kirk, D. Nyga, and M. Beetz. 2014. Controlled natural languages for language generation in artificial cognition. In International Conference on Robotics and Automation (ICRA), pages 6667–6672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>423--430</pages>
<contexts>
<context position="11729" citStr="Klein and Manning, 2003" startWordPosition="1965" endWordPosition="1968">text “if any of the pots has food” There are two types of conditional nodes: branching and temporal. A branching conditional node represents an “if” statement and has two successor nodes corresponding to whether the condition evaluates to true or false in the current environment. A temporal conditional node represents an “until” statement and waits until the condition is false in the environment. 3.2 Formal Overview Shallow Parsing. We deterministically convert the text x into its control flow graph G using a set of manual rules applied on its constituency parse tree from the Stanford parser (Klein and Manning, 2003). Conditionals in our dataset are simple and can be converted into postconditions directly using a few rules, unlike the action verbs (e.g., “fill”), which is the focus of this paper. The details of our shallow parsing procedure is described in the appendix. Given an environment el, G is reduced to a single sequence of frame nodes ci, ... , ck, by evaluating all the branch conditionals on ei. Semantic Parsing Model. For each frame node cz and given the current environment ez, the semantic parsing model (Section 5) places a distribution over logical forms zz. This logical form zz represents a p</context>
<context position="38397" citStr="Klein and Manning, 2003" startWordPosition="6481" endWordPosition="6484">author. We thank Aditya Jami and Jaeyong Sung for useful discussions. We also thank Jiaqi Su for her help with data collection and all the people who participated in the user study. Reproducibility. Code, data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x7f9151ec074f4f589e4d4786db7bb6de/. Demos can be found at http://tellmedave.com. Appendix: Parsing Text into Control Flow Graph. We first decompose the text x into its control flow graph G using a simple set of rules: • The parse tree of x is generated using the Stanford parser (Klein and Manning, 2003) and a frame node is created for each non-auxiliary verb node in the tree. • Conditional nodes are discovered by looking for the keywords until, if, after, when. The associated subtree is then parsed deterministically using a set of a rules. For example, a rule parses “for x minutes” to for(digit:x,unit:minutes). We found that all conditionals can be interpreted against the initial environment el, since our world is fullyobservable, deterministic, and the user giving the command has full view of the world. • To find objects, we look for anaphoric terminal nodes or nominals whose parent is not </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In Association for Computational Linguistics (ACL), pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kollar</author>
<author>S Tellex</author>
<author>D Roy</author>
<author>N Roy</author>
</authors>
<title>Grounding verbs of motion in natural language commands to robots.</title>
<date>2010</date>
<booktitle>In International Symposium on Experimental Robotics (ISER).</booktitle>
<contexts>
<context position="34222" citStr="Kollar et al., 2010" startWordPosition="5800" endWordPosition="5803">Branavan et al., 2010; Vogel and Jurafsky, 2010) use reinforcement learning to directly map to text to actions, and do not even require an explicit model of the environment. However, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Par</context>
</contexts>
<marker>Kollar, Tellex, Roy, Roy, 2010</marker>
<rawString>T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. Grounding verbs of motion in natural language commands to robots. In International Symposium on Experimental Robotics (ISER).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1223--1233</pages>
<contexts>
<context position="2325" citStr="Kwiatkowski et al. (2010)" startWordPosition="363" endWordPosition="366">test time. For example, in Figure 1, suppose that we have never seen the verb “fill”. Can we impute the correct interpretation, and moreover seize the opportunity to learn what “fill” means in a way that generalizes to future instructions? Text: “get the cup, fill it with water and then microwave the cup” Figure 1: A lexicon learned on the training data cannot possibly cover all the verb-concept mappings needed at test time. Our algorithm learns the meaning of new verbs (e.g., “fill”) using the environment context. Previous work in semantic parsing handles lexical coverage in one of two ways. Kwiatkowski et al. (2010) induces a highly constrained CCG lexicon capable of mapping words to complex logical forms, but it would have to skip new words (which in Figure 1 would lead to microwaving an empty cup). Others (Berant and Liang, 2014) take a freer approach by performing a search over logical forms, which can handle new words, but the logical forms there are much simpler than the ones we consider. In this paper, we present an hybrid approach that uses a lexicon to represent complex concepts but also strongly leverages the environment to guide the search space. The environment can provide helpful cues in seve</context>
<context position="29248" citStr="Kwiatkowski et al., 2010" startWordPosition="4999" endWordPosition="5002"> (Misra et al., 2014) is the edit distance between predicted and true action sequence. The END metric is the Jaccard index of sets A and B, where A is the set of atoms (e.g., on(cup1rtable1)) whose truth value changed due to simulating the predicted action sequence, and B is that of the true action sequence. Baselines. We compare our algorithm with the following baselines: Table 3: Results on the metrics and baselines described in section 9.2. The numbers are normalized to 100 with larger values being better. Algorithm IED END Chance 0.3 0.5 Manually Defined Templates 2.5 1.8 UBL- Best Parse (Kwiatkowski et al., 2010) 5.3 6.9 VEIL (Misra et al., 2014) 14.8 20.7 Model with only train-time lexicon induction 20.8 26.8 Model with only test-time lexicon induction 21.9 25.9 Full Model 22.3 28.8 1. Chance: Randomly selects a logical form for every frame node from the set of logical forms generated by generalizing all possible postconditions that do not hold in the current environment. These postconditions could contain up to 93 atoms. 2. Manually Defined Templates: Defines a set of postcondition templates for verbs similar to Guadarrama (2013). 3. UBL-Best Parse (Kwiatkowski et al., 2010): UBL algorithm trained o</context>
<context position="34965" citStr="Kwiatkowski et al., 2010" startWordPosition="5920" endWordPosition="5923"> verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suita</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Empirical Methods in Natural Language Processing (EMNLP), pages 1223–1233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1512--1523</pages>
<contexts>
<context position="31156" citStr="Kwiatkowski et al., 2011" startWordPosition="5322" endWordPosition="5325">cedure (Section 7) improved the accuracy by 1.5% on IED and 2% on END. The logical forms generated by the search were able to successfully map 48% of the new verbs. Table 2 shows new verbs and concepts that the algorithm was able to induce at test time. The algorithm was able to correctly learn the lexical entries for the verbs “distribute” and “mix”, while the ones for verbs “change” and “boil” were only partly correct. The postconditions in Table 2 are not structurally isomorphic to previously-seen logical forms; hence they could not have been handled by using synonyms or factored lexicons (Kwiatkowski et al., 2011). The poor performance of UBL was because the best logical form often produced an unsatisfiable postcondition. This can be remedied by joint modeling with the environ998 Table 2: New verbs and concepts induced at test time (Section 7). Text Postcondition represented by the learned logical form # Log. forms explored “mix it with ice cream and syrup”state(cup2, ice-cream1) ∧ state(cup2, vanilla) 15 “distribute among the couches” ∧j∈{1,3}on(pillowj, loveseat1) ∧ on(pillowi+1, armchairi+1)386 “boil it on the stove” state(stove, stovefire1) ∧ state(kettle, water) 109 “change the channel to a movie”</context>
<context position="35079" citStr="Kwiatkowski et al. (2011)" startWordPosition="5934" endWordPosition="5937">hey focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Empirical Methods in Natural Language Processing (EMNLP), pages 1512–1523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>E Choi</author>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="35754" citStr="Kwiatkowski et al., 2013" startWordPosition="6042" endWordPosition="6045">try into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time 999 from partial environment constraints. Grounding to Novel Concepts. Guadarrama et al. (2014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do w</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Lenz</author>
<author>R Knepper</author>
<author>A Saxena</author>
</authors>
<title>Deepmpc: Learning deep latent features for model predictive control.</title>
<date>2015</date>
<booktitle>In Robotics Science and Systems (RSS).</booktitle>
<contexts>
<context position="36699" citStr="Lenz et al. (2015)" startWordPosition="6207" endWordPosition="6210">text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit supervision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levine et al. (2015) and Lenz et al. (2015) focus on specific manipulation actions. In order to build a representation of the environment, Ren et al. (2012) and Wu et al. (2014) present vision algorithms but only output symbolic labels, which could act as inputs to our system. In future work, we also plan to integrate our work with RoboBrain (Saxena et al., 2014) to leverage these existing systems for building a robotic system capable of working with physical world data. 11 Conclusion We have presented an algorithm for mapping text to actions that induces lexical entries at test time using the environment. Our algorithm couples the lex</context>
</contexts>
<marker>Lenz, Knepper, Saxena, 2015</marker>
<rawString>I. Lenz, R. Knepper, and A. Saxena. 2015. Deepmpc: Learning deep latent features for model predictive control. In Robotics Science and Systems (RSS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Levine</author>
<author>C Finn</author>
<author>T Darrell</author>
<author>P Abbeel</author>
</authors>
<title>End-to-end training of deep visuomotor policies. arXiv preprint arXiv:1504.00702.</title>
<date>2015</date>
<contexts>
<context position="36676" citStr="Levine et al. (2015)" startWordPosition="6202" endWordPosition="6205">014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit supervision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levine et al. (2015) and Lenz et al. (2015) focus on specific manipulation actions. In order to build a representation of the environment, Ren et al. (2012) and Wu et al. (2014) present vision algorithms but only output symbolic labels, which could act as inputs to our system. In future work, we also plan to integrate our work with RoboBrain (Saxena et al., 2014) to leverage these existing systems for building a robotic system capable of working with physical world data. 11 Conclusion We have presented an algorithm for mapping text to actions that induces lexical entries at test time using the environment. Our al</context>
</contexts>
<marker>Levine, Finn, Darrell, Abbeel, 2015</marker>
<rawString>S. Levine, C. Finn, T. Darrell, and P. Abbeel. 2015. End-to-end training of deep visuomotor policies. arXiv preprint arXiv:1504.00702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP),</booktitle>
<pages>91--99</pages>
<contexts>
<context position="34808" citStr="Liang et al., 2009" startWordPosition="5898" endWordPosition="5901">Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single </context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>590--599</pages>
<contexts>
<context position="35706" citStr="Liang et al., 2011" startWordPosition="6034" endWordPosition="6037"> generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time 999 from partial environment constraints. Grounding to Novel Concepts. Guadarrama et al. (2014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have acce</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Association for Computational Linguistics (ACL), pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>N FitzGerald</author>
<author>L Zettlemoyer</author>
<author>L Bo</author>
<author>D Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>1671--1678</pages>
<contexts>
<context position="1408" citStr="Matuszek et al., 2012" startWordPosition="209" endWordPosition="212">st time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute, mix, arrange to complex logical forms, each containing up to four predicates. 1 Introduction The task of mapping natural language instructions to actions for a robot has been gaining momentum in recent years (Artzi and Zettlemoyer, 2013; Tellex et al., 2011; Misra et al., 2014; Bollini et al., 2011; Guadarrama et al., 2013; Matuszek et al., 2012b; Fasola and Mataric, 2013). We are particularly interested in instructions containing verbs such as “microwave” denoting high-level concepts, which correspond to more than 10 lowlevel symbolic actions such as grasp. In this setting, it is common to find new verbs requiring new concepts at test time. For example, in Figure 1, suppose that we have never seen the verb “fill”. Can we impute the correct interpretation, and moreover seize the opportunity to learn what “fill” means in a way that generalizes to future instructions? Text: “get the cup, fill it with water and then microwave the cup” F</context>
<context position="19649" citStr="Matuszek et al., 2012" startWordPosition="3324" endWordPosition="3327"> dealing with ellipsis and noise. We add a feature based on the average correlation between the objects of the new mapping ξi with the corresponding objects in the anchored mapping ξ. The other features are based on the relationship between object descriptions, similarity between ξ and ξi and transition probabilities between logical forms zi_1 and zi. These probabilities are also learned from training data. Mapping Object Descriptions. Our features rely on a mapping from object descriptions ω (e.g., “the red shiny cup”) to objects o (e.g., cup$), which has been addressed in many recent works (Matuszek et al., 2012a; Guadarrama et al., 2014; Fasola and Matari’c, 2014). One key idea is: instead of computing rigid lexical entries such as cup → cup,, we use a continuous correlation score ρ(ω, o) E [0, 1] that measures how well ω describes o. This flexibility allows the algorithm to use objects not explicitly mentioned in text. Given “get me a tank of water”, we might choose an approximate vessel (e.g., cup2). Given an object description ω, an object o, and a set of previously seen objects (used for anaphoric resolution), we define the correlation ρ(ω, o) using the following approach: • If ω is a pronoun, ρ</context>
<context position="36154" citStr="Matuszek et al. (2012" startWordPosition="6113" endWordPosition="6116">se techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time 999 from partial environment constraints. Grounding to Novel Concepts. Guadarrama et al. (2014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit supervision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levine et al. (2015) and Lenz et al. (2015) focus on specific manipulation actions. In order to bu</context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox. 2012a. A joint model of language and perception for grounded attribute learning. In International Conference on Machine Learning (ICML), pages 1671–1678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matuszek</author>
<author>E Herbst</author>
<author>L Zettlemoyer</author>
<author>D Fox</author>
</authors>
<title>Learning to parse natural language commands to a robot control system.</title>
<date>2012</date>
<booktitle>In International Symposium on Experimental Robotics (ISER).</booktitle>
<contexts>
<context position="1408" citStr="Matuszek et al., 2012" startWordPosition="209" endWordPosition="212">st time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute, mix, arrange to complex logical forms, each containing up to four predicates. 1 Introduction The task of mapping natural language instructions to actions for a robot has been gaining momentum in recent years (Artzi and Zettlemoyer, 2013; Tellex et al., 2011; Misra et al., 2014; Bollini et al., 2011; Guadarrama et al., 2013; Matuszek et al., 2012b; Fasola and Mataric, 2013). We are particularly interested in instructions containing verbs such as “microwave” denoting high-level concepts, which correspond to more than 10 lowlevel symbolic actions such as grasp. In this setting, it is common to find new verbs requiring new concepts at test time. For example, in Figure 1, suppose that we have never seen the verb “fill”. Can we impute the correct interpretation, and moreover seize the opportunity to learn what “fill” means in a way that generalizes to future instructions? Text: “get the cup, fill it with water and then microwave the cup” F</context>
<context position="19649" citStr="Matuszek et al., 2012" startWordPosition="3324" endWordPosition="3327"> dealing with ellipsis and noise. We add a feature based on the average correlation between the objects of the new mapping ξi with the corresponding objects in the anchored mapping ξ. The other features are based on the relationship between object descriptions, similarity between ξ and ξi and transition probabilities between logical forms zi_1 and zi. These probabilities are also learned from training data. Mapping Object Descriptions. Our features rely on a mapping from object descriptions ω (e.g., “the red shiny cup”) to objects o (e.g., cup$), which has been addressed in many recent works (Matuszek et al., 2012a; Guadarrama et al., 2014; Fasola and Matari’c, 2014). One key idea is: instead of computing rigid lexical entries such as cup → cup,, we use a continuous correlation score ρ(ω, o) E [0, 1] that measures how well ω describes o. This flexibility allows the algorithm to use objects not explicitly mentioned in text. Given “get me a tank of water”, we might choose an approximate vessel (e.g., cup2). Given an object description ω, an object o, and a set of previously seen objects (used for anaphoric resolution), we define the correlation ρ(ω, o) using the following approach: • If ω is a pronoun, ρ</context>
<context position="36154" citStr="Matuszek et al. (2012" startWordPosition="6113" endWordPosition="6116">se techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time 999 from partial environment constraints. Grounding to Novel Concepts. Guadarrama et al. (2014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit supervision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levine et al. (2015) and Lenz et al. (2015) focus on specific manipulation actions. In order to bu</context>
</contexts>
<marker>Matuszek, Herbst, Zettlemoyer, Fox, 2012</marker>
<rawString>C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox. 2012b. Learning to parse natural language commands to a robot control system. In International Symposium on Experimental Robotics (ISER).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Misra</author>
<author>J Sung</author>
<author>K Lee</author>
<author>A Saxena</author>
</authors>
<title>Tell Me Dave: Context-sensitive grounding of natural language to mobile manipulation instructions.</title>
<date>2014</date>
<booktitle>In Robotics: Science and Systems (RSS).</booktitle>
<contexts>
<context position="1338" citStr="Misra et al., 2014" startWordPosition="197" endWordPosition="200"> that leverages the environment to induce new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute, mix, arrange to complex logical forms, each containing up to four predicates. 1 Introduction The task of mapping natural language instructions to actions for a robot has been gaining momentum in recent years (Artzi and Zettlemoyer, 2013; Tellex et al., 2011; Misra et al., 2014; Bollini et al., 2011; Guadarrama et al., 2013; Matuszek et al., 2012b; Fasola and Mataric, 2013). We are particularly interested in instructions containing verbs such as “microwave” denoting high-level concepts, which correspond to more than 10 lowlevel symbolic actions such as grasp. In this setting, it is common to find new verbs requiring new concepts at test time. For example, in Figure 1, suppose that we have never seen the verb “fill”. Can we impute the correct interpretation, and moreover seize the opportunity to learn what “fill” means in a way that generalizes to future instructions</context>
<context position="5262" citStr="Misra et al., 2014" startWordPosition="871" endWordPosition="874"> frame nodes to logical forms {zi} representing the postconditions. From this, a planner and simulator generate the action sequences {ai} and resulting environments {ei}. a signal for inducing new lexical entries that map previously unseen verbs to novel concepts. In the example in Figure 1, the algorithm learns that microwaving an empty cup is unlikely and this suggests that the verb “fill” must map to actions that end up making the cup not empty. Another contribution of this paper is using postconditions as logical forms rather than actions, as in previous work (Artzi and Zettlemoyer, 2013; Misra et al., 2014). Postconditions not only reduce the search space of logical forms, but are also a more natural representation of verbs. We define a conditional random field (CRF) model over postconditions, and use a planner to convert postconditions into action sequences and a simulator to generate new environments. At test time, we use the lexicon induced from the training data, but also perform an environmentguided search over logical forms to induce new lexical entries on-the-fly. If the predicted action sequence uses a new lexical entry generated by the search, it is added to the lexicon, where it can be</context>
<context position="9446" citStr="Misra et al., 2014" startWordPosition="1566" endWordPosition="1569">e,, boiling))). Logical Forms. The logical form zz is a pair (E, ξ) containing a lexical entry E and a mapping ξ. The lexical entry E contains a parameterized postcondition such as λ�v.grasping(v,, v2) ∧¬near(v3, v2), and ξ maps the variables v� to objects in the environment. Applying the parameterized postcondition on ξ yields a postcondition; note that a postcondition can be represented by different logical forms. A lexical entry contains other information which are used for defining features, which is detailed in Section 4. Control Flow Graphs. Following previous work (Tellex et al., 2011; Misra et al., 2014), we convert the text x to a shallow representation. The particular representation we choose is a control flow graph, which encodes the sequential relation between atomic segments in the text. Figure 3 shows the control flow graph for an example text. In a control flow graph, each node is either a frame node or a conditional node. A frame node represents a single clause (e.g., “change the channel to a movie”) and has at most one successor node. Specifically, a frame node consists of a verb v (e.g., arrange, collect), a set of object descriptions {wz} which are the arguments of the verb (e.g., </context>
<context position="14232" citStr="Misra et al., 2014" startWordPosition="2401" endWordPosition="2404"> the following form: [ν ⇒ (λ�v.S, ξ)] where, ν is a verb, S is a postcondition with free variables v, and ξ is a mapping of these variables to objects. An example lexical entry is: [pour ⇒ (λv1v2v3.S, ξ)], where: S = grasping(v,, v2) n near(v,, v3) n -state(v2, milk) n state(v3, milk) ξ ={v, --+ robot,, v2 --+ cup1, v3 --+ bowl3} (anchoring) As Table 1 shows, a single verb will in general have multiple entries due to a combination of polysemy and the fact that language is higher-level than postconditions. Advantages of Postconditions. In contrast to previous work (Artzi and Zettlemoyer, 2013; Misra et al., 2014), we use postconditions instead of action sequence for two main reasons. First, postconditions generalize better. To illustrate this, consider the action sequence for the simple task of filling a cup with water. At the time of learning the lexicon, the action sequence might correspond to using a tap for filling the cup while at test time, the environment may not have a tap but instead have a pot with water. Thus, if the lexicon maps to action sequence, then it will not be applicable at test time whereas the postcondition state(z,, water) is valid in both cases. We thus shift the load of inferr</context>
<context position="16167" citStr="Misra et al. (2014)" startWordPosition="2736" endWordPosition="2739">(v,, channel6) n near(v,, v2) input to xbox” ξ : v, --+ tv,, v2 --+ xbox, and use postconditions for representation, which better captures the semantics of verbs. Second, because postconditions are higherlevel, the number of atoms needed to represent a verb is much less than the corresponding number of actions. For example, the text “microwave a cup”, maps to action sequence with 10–15 actions, the postcondition only has two atoms: in(cup2,microwave,) ∧ state(microwave, is-on). This makes searching for new logical forms more tractable. Advantages of Anchoring. Similar to the VEIL templates of Misra et al. (2014), the free variables v� are associated with a mapping ξ to concrete objects. This is useful for resolving ellipsis. Suppose the following lexical entry was created at training time based on the text “throw the drinks in the trash bag”: [B: throw =�. Axyz.S(x, y, z)], where S = in(x,y) n -grasping(z, x) n -state(z, closed) ξ ={x --+ coke,, y --+ garbageBin,, z --+ robot,} Now consider a new text at test time “throw away the chips”, which does not explicitly mention where to throw the chips. Our semantic parsing algorithm (Section 5) will use the previous mapping y → garbabeBin, to choose an obj</context>
<context position="27334" citStr="Misra et al. (2014)" startWordPosition="4684" endWordPosition="4687">n Section 6. For efficiency, we used an objective xr = (�,f�) fr is the new assignment Train Time Anchored Lexicon A (Sec 6) 997 similar to pseudolikelihood to estimate the parameters θ. Specifically, we maximize the average loglikelihood over each adjacent pair of logical forms under ˜pθ: ˜fiB(zi |zi−1, ci, ei) ∝ exp(o(ci, zi−1, zi, ei)TO). (2) The weights were initialized to 0. We performed 300 iterations over the validation set with a learning rate of0.005 N . 9 Dataset and Experiments 9.1 Dataset We collected a dataset of 500 examples from 62 people using a crowdsourcing system similar to Misra et al. (2014). We consider two different 3D scenarios: a kitchen and a living room, each containing an average of 40 objects. Both of these scenarios have 10 environments consisting of different sets of objects in different configurations. We define 10 high-level objectives, 5 per scenario, such as clean the room, make coffee, prepare room for movie night, etc. One group of users wrote natural language commands to achieve the high-level objectives. Another group controlled a virtual robot to accomplish the commands given by the first group. The dataset contains considerable variety, consisting of 148 diffe</context>
<context position="28644" citStr="Misra et al., 2014" startWordPosition="4896" endWordPosition="4899">quence. Users make spelling and grammar errors in addition to occasionally taking random actions not relevant to the text. The supplementary material contains more details. We filtered out 31 examples containing fewer than two action sequences. Of the remaining examples, 378 were used for training and 91 were used for test. Our algorithm is tested on four new environments (two from each scenario). 9.2 Experiments and Results Evaluation Metrics. We consider two metrics, IED and END, which measure accuracy based on the action sequence and environment, respectively. Specifically, the IED metric (Misra et al., 2014) is the edit distance between predicted and true action sequence. The END metric is the Jaccard index of sets A and B, where A is the set of atoms (e.g., on(cup1rtable1)) whose truth value changed due to simulating the predicted action sequence, and B is that of the true action sequence. Baselines. We compare our algorithm with the following baselines: Table 3: Results on the metrics and baselines described in section 9.2. The numbers are normalized to 100 with larger values being better. Algorithm IED END Chance 0.3 0.5 Manually Defined Templates 2.5 1.8 UBL- Best Parse (Kwiatkowski et al., 2</context>
<context position="30037" citStr="Misra et al., 2014" startWordPosition="5127" endWordPosition="5130">28.8 1. Chance: Randomly selects a logical form for every frame node from the set of logical forms generated by generalizing all possible postconditions that do not hold in the current environment. These postconditions could contain up to 93 atoms. 2. Manually Defined Templates: Defines a set of postcondition templates for verbs similar to Guadarrama (2013). 3. UBL-Best Parse (Kwiatkowski et al., 2010): UBL algorithm trained on text aligned with postconditions and a noun-phrase seed lexicon. The planner uses the highest scoring postcondition given by UBL to infer the action sequence. 4. VEIL (Misra et al., 2014): Uses action sequences as logical forms and does not generate lexical entries at test time. We also consider two variations of our model: (i) using only lexical entries induced using the training data, and (ii) using only the logical forms induced at test-time by the search procedure. The results are presented in Table 3. We observe that our full model outperforms the baseline and the two pure search- and lexicon-based variations of our model. We further observe that adding the search procedure (Section 7) improved the accuracy by 1.5% on IED and 2% on END. The logical forms generated by the </context>
</contexts>
<marker>Misra, Sung, Lee, Saxena, 2014</marker>
<rawString>D. Misra, J. Sung, K. Lee, and A. Saxena. 2014. Tell Me Dave: Context-sensitive grounding of natural language to mobile manipulation instructions. In Robotics: Science and Systems (RSS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mooney</author>
</authors>
<title>Learning to connect language and perception.</title>
<date>2008</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>1598--1601</pages>
<contexts>
<context position="34747" citStr="Mooney, 2008" startWordPosition="5890" endWordPosition="5891">unctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Ar</context>
</contexts>
<marker>Mooney, 2008</marker>
<rawString>R. Mooney. 2008. Learning to connect language and perception. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1598–1601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="20793" citStr="Och and Ney, 2003" startWordPosition="3522" endWordPosition="3525">e correlation ρ(ω, o) using the following approach: • If ω is a pronoun, ρ(ω, o) is the ratio of the position of the last reference of o to the length of the action sequence computed so far, thus preferring recent objects. • Otherwise, we compute the correlation using various sources: the object’s category; the object’s state for handling metonymy (e.g., the description “coffee” correlates well with the object mug, if mug, contains coffee— state(mug,, has-coffee) is true), WordNet (Fellbaum, 1998) for dealing synonymy and hyponymy; and word alignments between the objects and text from Giza++ (Och and Ney, 2003) to learn domain-specific references (e.g., “Guinness book” refers to book,, not book2). More details can be found in the supplemental material. 6 Lexicon Induction from Training Data In order to map text to logical forms, we first induce an initial anchored lexicon A from the training data {(x(m), e(m), a(m), π(m))}Mm=1. At test time, we add new lexical entries (Section 7) to A. Recall that shallow parsing x(m) yields a list of frame nodes c1:k. For each frame node ci and its aligned action sequence ai, we take the conjunction of all the atoms (and their negations) which are false in the curr</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ren</author>
<author>L Bo</author>
<author>D Fox</author>
</authors>
<title>Rgb-(d) scene labeling: Features and algorithms.</title>
<date>2012</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR),</booktitle>
<pages>2759--2766</pages>
<contexts>
<context position="36812" citStr="Ren et al. (2012)" startWordPosition="6227" endWordPosition="6230">jective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit supervision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levine et al. (2015) and Lenz et al. (2015) focus on specific manipulation actions. In order to build a representation of the environment, Ren et al. (2012) and Wu et al. (2014) present vision algorithms but only output symbolic labels, which could act as inputs to our system. In future work, we also plan to integrate our work with RoboBrain (Saxena et al., 2014) to leverage these existing systems for building a robotic system capable of working with physical world data. 11 Conclusion We have presented an algorithm for mapping text to actions that induces lexical entries at test time using the environment. Our algorithm couples the lexicon extracted from training data with a testtime search that uses the environment to reduce the space of logical</context>
</contexts>
<marker>Ren, Bo, Fox, 2012</marker>
<rawString>X. Ren, L. Bo, and D. Fox. 2012. Rgb-(d) scene labeling: Features and algorithms. In Computer Vision and Pattern Recognition (CVPR), pages 2759–2766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rintanen</author>
</authors>
<title>Planning as satisfiability: Heuristics.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<contexts>
<context position="14926" citStr="Rintanen (2012)" startWordPosition="2524" endWordPosition="2525"> postconditions generalize better. To illustrate this, consider the action sequence for the simple task of filling a cup with water. At the time of learning the lexicon, the action sequence might correspond to using a tap for filling the cup while at test time, the environment may not have a tap but instead have a pot with water. Thus, if the lexicon maps to action sequence, then it will not be applicable at test time whereas the postcondition state(z,, water) is valid in both cases. We thus shift the load of inferring environment-specific actions onto planners 1We use the symbolic planner of Rintanen (2012) which can perform complex planning. For example, to pick up a bottle that is blocked by a stack of books, the planner will first remove the books before grasping the bottle. In contrast, Artzi and Zettlemoyer (2013) use a simple search over implicit actions. Table 1: Some lexical entries for the verb “turn” Sentence Context Lexical entry [turn =:&gt;. (AV.S, ξ)] “turn on the TV” state(v,, is-on) n near(v2, v,) ξ : v, --+ tv,, v2 --+ robot, “turn on the right state(v,, fire3) n near(v2, v,) back burner” ξ : v, --+ stove,, v2 --+ robot, “turn off the water” -state(v,, tap-on) ξ : v, --+ sink, “tur</context>
</contexts>
<marker>Rintanen, 2012</marker>
<rawString>J. Rintanen. 2012. Planning as satisfiability: Heuristics. Artificial Intelligence, 193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Saxena</author>
<author>A Jain</author>
<author>O Sener</author>
<author>A Jami</author>
<author>D K Misra</author>
<author>H S Koppula</author>
</authors>
<title>Robobrain: Largescale knowledge engine for robots. arXiv preprint arXiv:1412.0691.</title>
<date>2014</date>
<contexts>
<context position="37021" citStr="Saxena et al., 2014" startWordPosition="6264" endWordPosition="6267">rvision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levine et al. (2015) and Lenz et al. (2015) focus on specific manipulation actions. In order to build a representation of the environment, Ren et al. (2012) and Wu et al. (2014) present vision algorithms but only output symbolic labels, which could act as inputs to our system. In future work, we also plan to integrate our work with RoboBrain (Saxena et al., 2014) to leverage these existing systems for building a robotic system capable of working with physical world data. 11 Conclusion We have presented an algorithm for mapping text to actions that induces lexical entries at test time using the environment. Our algorithm couples the lexicon extracted from training data with a testtime search that uses the environment to reduce the space of logical forms. Our results suggest that using the environment to provide lexical coverage of high-level concepts is a promising avenue for further research. Acknowledgements. This research was supported by the ONR (a</context>
</contexts>
<marker>Saxena, Jain, Sener, Jami, Misra, Koppula, 2014</marker>
<rawString>A. Saxena, A. Jain, O. Sener, A. Jami, D. K. Misra, and H. S. Koppula. 2014. Robobrain: Largescale knowledge engine for robots. arXiv preprint arXiv:1412.0691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sung</author>
<author>S H Jin</author>
<author>A Saxena</author>
</authors>
<title>Robobarista: Object part based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds. arXiv preprint arXiv:1504.03071.</title>
<date>2015</date>
<contexts>
<context position="36654" citStr="Sung et al., 2015" startWordPosition="6198" endWordPosition="6201">Guadarrama et al. (2014) map open vocabulary text to objects in an image using a large database. Matuszek et al. (2012a) create new predicates for every new adjective at test time. Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit supervision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levine et al. (2015) and Lenz et al. (2015) focus on specific manipulation actions. In order to build a representation of the environment, Ren et al. (2012) and Wu et al. (2014) present vision algorithms but only output symbolic labels, which could act as inputs to our system. In future work, we also plan to integrate our work with RoboBrain (Saxena et al., 2014) to leverage these existing systems for building a robotic system capable of working with physical world data. 11 Conclusion We have presented an algorithm for mapping text to actions that induces lexical entries at test time using t</context>
</contexts>
<marker>Sung, Jin, Saxena, 2015</marker>
<rawString>J. Sung, S. H. Jin, and A. Saxena. 2015. Robobarista: Object part based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds. arXiv preprint arXiv:1504.03071.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>T Kollar</author>
<author>S Dickerson</author>
<author>M R Walter</author>
<author>A G Banerjee</author>
<author>S J Teller</author>
<author>N Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation.</title>
<date>2011</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="1318" citStr="Tellex et al., 2011" startWordPosition="193" endWordPosition="196">a new hybrid approach that leverages the environment to induce new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute, mix, arrange to complex logical forms, each containing up to four predicates. 1 Introduction The task of mapping natural language instructions to actions for a robot has been gaining momentum in recent years (Artzi and Zettlemoyer, 2013; Tellex et al., 2011; Misra et al., 2014; Bollini et al., 2011; Guadarrama et al., 2013; Matuszek et al., 2012b; Fasola and Mataric, 2013). We are particularly interested in instructions containing verbs such as “microwave” denoting high-level concepts, which correspond to more than 10 lowlevel symbolic actions such as grasp. In this setting, it is common to find new verbs requiring new concepts at test time. For example, in Figure 1, suppose that we have never seen the verb “fill”. Can we impute the correct interpretation, and moreover seize the opportunity to learn what “fill” means in a way that generalizes to</context>
<context position="9425" citStr="Tellex et al., 2011" startWordPosition="1562" endWordPosition="1565"> for wait(state(kettle,, boiling))). Logical Forms. The logical form zz is a pair (E, ξ) containing a lexical entry E and a mapping ξ. The lexical entry E contains a parameterized postcondition such as λ�v.grasping(v,, v2) ∧¬near(v3, v2), and ξ maps the variables v� to objects in the environment. Applying the parameterized postcondition on ξ yields a postcondition; note that a postcondition can be represented by different logical forms. A lexical entry contains other information which are used for defining features, which is detailed in Section 4. Control Flow Graphs. Following previous work (Tellex et al., 2011; Misra et al., 2014), we convert the text x to a shallow representation. The particular representation we choose is a control flow graph, which encodes the sequential relation between atomic segments in the text. Figure 3 shows the control flow graph for an example text. In a control flow graph, each node is either a frame node or a conditional node. A frame node represents a single clause (e.g., “change the channel to a movie”) and has at most one successor node. Specifically, a frame node consists of a verb v (e.g., arrange, collect), a set of object descriptions {wz} which are the argument</context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller, and N. Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In Association for the Advancement of Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vogel</author>
<author>D Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>806--814</pages>
<contexts>
<context position="33651" citStr="Vogel and Jurafsky, 2010" startWordPosition="5707" endWordPosition="5710">ent is crucial. In this work, we relied on a simple deterministic shallow parsing step. We found that shallow parsing was able to correctly process the text in only 46% of the test examples, suggesting that improving this initial component or at least modeling the uncertainty there would be beneficial. 10 Related Work Our work uses semantic parsing to map natural language instructions to actions via novel concepts, which brings together several themes: actions, semantic parsing, novel concepts, and robotics. Mapping Text to Actions. Several works (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010) use reinforcement learning to directly map to text to actions, and do not even require an explicit model of the environment. However, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) h</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>A. Vogel and D. Jurafsky. 2010. Learning to follow navigational directions. In Association for Computational Linguistics (ACL), pages 806–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wu</author>
<author>I Lenz</author>
<author>A Saxena</author>
</authors>
<title>Hierarchical semantic labeling for task-relevant RGB-D perception.</title>
<date>2014</date>
<booktitle>In Robotics: Science and Systems (RSS).</booktitle>
<contexts>
<context position="36833" citStr="Wu et al. (2014)" startWordPosition="6232" endWordPosition="6235">Others (Kirk et al., 2014) ask users for clarification. In contrast, we neither have access to large databases for this problem, nor do we do create new predicates or use explicit supervision at test time. Robotic Applications. Our motivation behind this work is to build robotic systems capable of taking commands from users. Other works in this area have considered mapping text to a variety of manipulation actions (Sung et al., 2015). Levine et al. (2015) and Lenz et al. (2015) focus on specific manipulation actions. In order to build a representation of the environment, Ren et al. (2012) and Wu et al. (2014) present vision algorithms but only output symbolic labels, which could act as inputs to our system. In future work, we also plan to integrate our work with RoboBrain (Saxena et al., 2014) to leverage these existing systems for building a robotic system capable of working with physical world data. 11 Conclusion We have presented an algorithm for mapping text to actions that induces lexical entries at test time using the environment. Our algorithm couples the lexicon extracted from training data with a testtime search that uses the environment to reduce the space of logical forms. Our results s</context>
</contexts>
<marker>Wu, Lenz, Saxena, 2014</marker>
<rawString>C. Wu, I. Lenz, and A. Saxena. 2014. Hierarchical semantic labeling for task-relevant RGB-D perception. In Robotics: Science and Systems (RSS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>J M Siskind</author>
</authors>
<title>Grounded language learning from video described with sentences.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>53--63</pages>
<contexts>
<context position="34706" citStr="Yu and Siskind, 2013" startWordPosition="5880" endWordPosition="5883">ditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing s</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>H. Yu and J. M. Siskind. 2013. Grounded language learning from video described with sentences. In Association for Computational Linguistics (ACL), pages 53–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>658--666</pages>
<contexts>
<context position="34907" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5912" endWordPosition="5915">ional verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate pos</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence (UAI), pages 658– 666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>678--687</pages>
<contexts>
<context position="25185" citStr="Zettlemoyer and Collins, 2007" startWordPosition="4318" endWordPosition="4321">t verb = v(cq) Test Time Search for Logical Forms (Sec 7) x, = e, f, where L = [verb a (A0. S, 0)] is a test time lexical entry Set of Logical Forms for cr_yc, er,xr_c Figure 4: Logical forms for a given clause cz, environment ez, and previous logical form zz−1 are generated from both a lexicon induced from training data and a test-time search procedure based on the environment. If a logical form z = ([ν ⇒ (A�v.5, ∅)], ξ) is used by the predicted action sequence, we add the lexical entry [ν ⇒ (A�v.5, ξ)] to the lexicon A. This is different to other lexicon induction procedures such as GENLEX (Zettlemoyer and Collins, 2007) which are done at training time only and require more supervision. Moreover, GENLEX does not use the environment context in creating new lexical entries and thus is not appropriate at test time, since it would vastly overgenerate lexical entries compared to our approach. For us, the environment thus provides implicit supervision for lexicon induction. 8 Inference and Parameter Estimation Inference. Given a text x (which is converted to c1:k via Section 3.2) and an initial environment e1, we wish to predict an action sequence a based on po(a1:k |c1:k, e1), which marginalizes over all logical f</context>
<context position="34938" citStr="Zettlemoyer and Collins, 2007" startWordPosition="5916" endWordPosition="5919">ns, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these te</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>