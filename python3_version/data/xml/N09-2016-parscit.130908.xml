<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005730">
<title confidence="0.9991345">
Learning Bayesian Networks for Semantic Frame Composition
in a Spoken Dialog System
</title>
<author confidence="0.794568">
Marie-Jean Meurs, Fabrice Lef`evre and Renato de Mori
</author>
<bodyText confidence="0.561691333333333">
Universit´e d’Avignon et des Pays de Vaucluse
Laboratoire Informatique d’Avignon (EA 931), F-84911 Avignon, France.
{marie-jean.meurs,fabrice.lefevre,renato.demori}@univ-avignon.fr
</bodyText>
<sectionHeader confidence="0.97751" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999431909090909">
A stochastic approach based on Dynamic
Bayesian Networks (DBNs) is introduced for
spoken language understanding. DBN-based
models allow to infer and then to compose
semantic frame-based tree structures from
speech transcriptions. Experimental results on
the French MEDIA dialog corpus show the
appropriateness of the technique which both
lead to good tree identification results and can
provide the dialog system with n-best lists of
scored hypotheses.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999784476190476">
Recent developments in Spoken Dialog Systems
(SDSs) have renewed the interest for the extrac-
tion of rich and high-level semantics from users’
utterances. Shifting every SDS component from
hand-crafted to stochastic is foreseen as a good op-
tion to improve their overall performance by an in-
creased robustness to speech variabilities. For in-
stance stochastic methods are now efficient alter-
natives to rule-based techniques for Spoken Lan-
guage Understanding (SLU) (He and Young, 2005;
Lef`evre, 2007).
The SLU module links up the automatic speech
recognition (ASR) module and the dialog manager.
From the user’s utterance analysis, it derives a repre-
sentation of its semantic content upon which the di-
alog manager can decide the next best action to per-
form, taking into account the current dialog context.
In this work, the overall objective is to increase the
relevancy of the semantic information used by the
system. Generally the internal meaning representa-
tion is based on flat concept sets obtained by either
</bodyText>
<page confidence="0.98888">
61
</page>
<bodyText confidence="0.999953558823529">
keyword spotting or conceptual decoding. In some
cases a dialog act can be added on top of the concept
set. Here we intend to consider an additional se-
mantic composition step which will capture the ab-
stract semantic structures conveyed by the basic con-
cept representation. A frame formalism is applied to
specify these nested structures. As such structures
do not rely on sequential constraints, pure left-right
branching semantic parser (such as (He and Young,
2005)) will not apply in this case.
To derive automatically such frame meaning rep-
resentations we propose a system based on a two
decoding step process using dynamic Bayesian net-
works (DBNs) (Bilmes and Zweig, 2002): first ba-
sic concepts are derived from the user’s utterance
transcriptions, then inferences are made on sequen-
tial semantic frame structures, considering all the
available previous annotation levels (words and con-
cepts). The inference process extracts all possible
sub-trees (branches) according to lower level infor-
mation (generation) and composes the hypothesized
branches into a single utterance-span tree (composi-
tion). A hand-craft rule-based approach is used to
derive the seed annotated training data. So both ap-
proaches are not competing and the stochastic ap-
proach is justified as only the DBN system is able
to provide n-best lists of tree hypotheses with confi-
dence scores to a stochastic dialog manager (such as
the very promising POMDP-based approaches).
The paper is organized as follows. The next sec-
tion presents the semantic frame annotation on the
MEDIA corpus. Then Section 3 introduces the DBN-
based models for semantic composition and finally
Section 4 reports on the experiments.
</bodyText>
<subsubsectionHeader confidence="0.765249">
Proceedings of NAACL HLT 2009: Short Papers, pages 61–64,
</subsubsectionHeader>
<note confidence="0.380291">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.999316">
lodging_hotel lodging_location
FE FE
HOTEL LOCATION
frame frame
LODGING
frame
FE
location_event
Frame trans Frame trans
Frame-FE trans Frame-FE trans
co ncept
trans
Frame
FE
word
concept concept
concept
ns
tra
Frame
FE
word
FE trans
concept
trans
FE FE
Frame Frame
word
concept concept
FE trans
concept
trans
word
</figure>
<figureCaption confidence="0.998968">
Figure 1: Frames, FEs and relations associated to the se-
</figureCaption>
<construct confidence="0.32762">
quence “staying in a hotel near the Festival de Cannes”
</construct>
<sectionHeader confidence="0.643469" genericHeader="method">
2 Semantic Frames on the MEDIA corpus
</sectionHeader>
<bodyText confidence="0.99986827027027">
MEDIA is a French corpus of negotiation di-
alogs among users and a tourist information phone
server (Bonneau-Maynard et al., 2005). The corpus
contains 1,257 dialogs recorded using a Wizard of
Oz system. The semantic corpus is annotated with
concept-value pairs corresponding to word segments
with the addition of specifier tags representing some
relations between concepts. The annotation utilizes
83 basic concepts and 19 specifiers.
Amongst the available semantic representations,
the semantic frames (Lowe et al., 1997) are probably
the most suited to the task, mostly because of their
ability to represent negotiation dialogs. Semantic
frames are computational models describing com-
mon or abstract situations involving roles, the frame
elements (FEs). The FrameNet project (Fillmore et
al., 2003) provides a large frame database for En-
glish. As no such resource exists for French, we
elaborated a frame ontology to describe the semantic
knowledge of the MEDIA domain. The MEDIA on-
tology is composed of 21 frames and 86 FEs. All are
described by a set of manually defined patterns made
of lexical units and conceptual units (frame and FE
evoking words and concepts). Figure 1 gives the an-
notation of word sequence “staying in a hotel near
the Festival de Cannes”. The training data are auto-
matically annotated by a rule-based process. Pattern
matching triggers the instantiation of frames and
FEs which are composed using a set of logical rules.
Composition may involve creation, modification or
deletion of frame and FE instances. About 70 rules
are currently used. This process is task-oriented and
is progressively enriched with new rules to improve
its accuracy. A reference frame annotation for the
training corpus is established in this way and used
for learning the parameters of the stochastic models
introduced in the next section.
</bodyText>
<figureCaption confidence="0.980728">
Figure 2: Frames, FEs as one or 2 unobserved variables
</figureCaption>
<figure confidence="0.815284">
Frame trans Frame trans
</figure>
<figureCaption confidence="0.99943">
Figure 3: 2-level decoding of frames and FEs
</figureCaption>
<sectionHeader confidence="0.983179" genericHeader="method">
3 DBN-based Frame Models
</sectionHeader>
<bodyText confidence="0.999973782608696">
The generative DBN models used in the system are
depicted on two time slices (two words) in figures 2
and 3. In practice, a regular pattern is repeated suffi-
ciently to fit the entire word sequence. Shaded nodes
are observed variables whereas empty nodes are hid-
den. Plain lines represent conditional dependencies
between variables and dashed lines indicate switch-
ing parents (variables modifying the conditional re-
lationship between others). An example of a switch-
ing parent is given by the trans nodes which in-
fluence the frame and FE nodes: when trans node
is null the frame or FE stays the same from slice to
slice, when trans is 1 a new frame or FE value is
predicted based on the values of its parent nodes in
the word sequence using frame (or FE) n-grams.
In the left DBN model of Figure 2 frames and FEs
are merged in a single compound variable. They
are factorized in the right model using two variables
jointly decoded. Figure 3 shows the 2-level model
where frames are first decoded then used as observed
values in the FE decoding step. Merging frames and
FEs into a variable reduces the decoding complex-
ity but leads to deterministic links between frames
</bodyText>
<figure confidence="0.999522555555555">
Frame trans Frame trans
concept
trans
Frame Frame
word word
concept concept
concept
trans
concept
trans
FE trans
FE FE
Frame Frame
word word
concept concept
FE trans
concept
trans
</figure>
<page confidence="0.994688">
62
</page>
<bodyText confidence="0.999913130434783">
and FEs. With their factorization, on the contrary, it
is possible to deal with the ambiguities in the frame
and FE links. During the decoding step, every com-
bination is tested, even not encountered in the train-
ing data, by means of a back-off technique. Due
to the increase in model complexity, a sub-optimal
beam search is applied for decoding. In this way,
the 2-level approach reduces the complexity of the
factored approach while preserving model general-
ization.
Because all variables are observed at training
time, the edge’s conditional probability tables are
directly derived from observation counts. To im-
prove their estimates, factored language models
(FLMs) are used along with generalized parallel
backoff (Bilmes and Kirchhoff, 2003). Several FLM
implementations of the joint distributions are used
in the DBN models, corresponding to the arrows in
Figures 2 and 3. In the FLMs given below, n is the
history length (n = 1 for bigrams), the uppercase
and lowercase letters FFE, F, FE, C and W re-
spectively stand for frame/FE (one variable), frame,
FE, concept and word variables:
</bodyText>
<listItem confidence="0.984229">
• Frame/FE compound variable:
</listItem>
<equation confidence="0.999424333333333">
P(FFE) � Qnk=0 P(ffek|ffek−1);
P(C|FFE) _ Qnk=0 P(ck|ck−1, ffek);
P(W|C,FFE) _ Qnk=0 P(wk|wk−1,ck, ffek).
</equation>
<listItem confidence="0.801491">
• Frame an77��77d FE variables, joint decoding:
</listItem>
<equation confidence="0.99738025">
P(F) _ 11k=0P(fk |fk−1);
P(FE|F) _ Qnk=0 P(fek|fek−1, fk);
P(C|FE, F) Qnk=0 P(ck|ck−1, fek, fk);
P(W |C, FE, F) Qnk=0 P(wk|wk−1, ck, fek, fk).
</equation>
<listItem confidence="0.991603">
• Frame and FE variables, 2-level decoding:
</listItem>
<bodyText confidence="0.711507">
− First stage: same as frame/FE compound variables
but only decoding frames
− Second stage: same as joint decodind but frames are
observed
</bodyText>
<equation confidence="0.9974526">
P(
Fˆ) Qnk=0 P( ˆfk |ˆfk−1);
P(FE |Fˆ) �Qnk=0 P(fek|fek−1, ˆfk);
P(C |Fˆ, FE) _ Qnk=0 P(ck|ck−1, ˆfk, fek);
P(W|C, Fˆ,FE) _ Qnk=0 P(wk|wk−1,ck, ˆfk,fek).
</equation>
<bodyText confidence="0.9963979375">
Variables with hat have observed values.
Due to the frame hierarchical representation,
some overlapping situations can occurred when de-
termining the frame and FE associated to a concept.
To address this difficulty, a tree-projection algorithm
is performed on the utterance tree-structured frame
annotation and allows to derive sub-branches associ-
ated to a concept (possibly more than one). Starting
from a leaf of the tree, a compound frame/FE class
is obtained by aggregating the father vertices (either
frames or FEs) as long as they are associated to the
same concept (or none). The edges are defined both
by the frame→FE attachments and the FE→frame
sub-frame relations.
Thereafter, either the branches are considered di-
rectly as compound classes or the frame and FE in-
terleaved components are separated to produce two
class sets. These compound classes are considered
in the decoding process then projected back after-
wards to recover the two types of frame↔FE con-
nections. However, some links are lost because de-
coding is sequential. A set of manually defined rules
is used to retrieve the missing connections from the
set of hypothesized branches. Theses rules are sim-
ilar to those used in the semi-automatic annotation
of the training data but differ mostly because the
available information is different. For instance, the
frames cannot anymore be associated to a particular
word inside a concept but rather to the whole seg-
ment. The training corpus provides the set of frame
and FE class sequences on which the DBN parame-
ters are estimated.
</bodyText>
<sectionHeader confidence="0.994843" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999226769230769">
The DBN-based composition systems were evalu-
ated on a test set of 225 speakers’ turns manually
annotated in terms of frames and FEs. The rule-
based system was used to perform a frame annota-
tion of the MEDIA data. On the test set, an aver-
age F-measure of 0.95 for frame identification con-
firms the good reliability of the process. The DBN
model parameters were trained on the training data
using jointly the manual transcriptions, the manual
concept annotations and the rule-based frame anno-
tations.
Experiments were carried out on the test set under
three conditions varying the input noise level:
</bodyText>
<listItem confidence="0.998587833333333">
• REF (reference): speaker turns manually tran-
scribed and annotated;
• SLU: concepts decoded from manual transcrip-
tions using a DBN-based SLU model comparable
to (Lef`evre, 2007) (10.6% concept error rate);
• ASR+SLU: 1-best hypotheses of transcriptions
</listItem>
<page confidence="0.995347">
63
</page>
<table confidence="0.997529">
DBN models Inputs Frames REF Links Frames SLU Links Frames ASR + SLU Links
FE FE FE
frame/FEs ¯p/¯r 0.91/0.93 0.91/0.86 0.93/0.98 0.87/0.82 0.91/0.83 0.93/0.98 0.86/0.80 0.90/0.86 0.92/0.98
(compound) ¯F-m 0.89 0.86 0.92 0.81 0.82 0.92 0.78 0.84 0.92
frames and FEs ¯p/¯r 0.92/0.92 0.92/0.85 0.94/0.98 0.88/0.81 0.92/0.83 0.93/0.97 0.87/0.79 0.90/0.86 0.94/0.97
(2 variables) ¯F-m 0.90 0.86 0.94 0.80 0.83 0.91 0.78 0.84 0.93
frames then FEs ¯p/¯r 0.92/0.94 0.91/0.82 0.92/0.98 0.88/0.86 0.91/0.80 0.92/0.97 0.87/0.81 0.89/0.82 0.93/0.98
(2-level) ¯F-m 0.91 0.83 0.93 0.83 0.80 0.90 0.79 0.80 0.92
</table>
<tableCaption confidence="0.991652">
Table 1: Precision (¯p), Recall (¯r) and F-measure (¯F-m) on the MEDIA test set for the DBN-based frame composition
systems.
</tableCaption>
<bodyText confidence="0.998480068965517">
generated by an ASR system and concepts decoded
using them (14.8% word error rate, 24.3% concept
error rate).
All the experiments reported in the paper were per-
formed using GMTK (Bilmes and Zweig, 2002),
a general purpose graphical model toolkit and
SRILM (Stolcke, 2002), a language modeling
toolkit.
Table 1 is populated with the results on the test
set for the DBN-based frame composition systems
in terms of precision, recall and F-measure. For the
FE figures, only the reference FEs corresponding to
correctly identified frames are considered. Only the
frame and FE names are considered, neither their
constituents nor their order matter. Finally, results
are given for the sub-frame links between frames
and FEs. Table 1 shows that the performances of the
3 DBN-based systems are quite comparable. Any-
how the 2-level system can be considered the best
as besides its good F-measure results, it is also the
most efficient model in terms of decoding complex-
ity. The good results obtained for the sub-frame
links confirm that the DBN models combined with a
small rule set can be used to generate consistent hi-
erarchical structures. Moreover, as they can provide
hypotheses with confidence scores they can be used
in a multiple input/output context (lattices and n-best
lists) or in a validation process (evaluating and rank-
ing hypotheses from other systems).
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999989411764706">
This work investigates a stochastic process for gen-
erating and composing semantic frames using dy-
namic Bayesian networks. The proposed approach
offers a convenient way to automatically derive se-
mantic annotations of speech utterances based on
a complete frame and frame element hierarchical
structure. Experimental results, obtained on the ME-
DIA dialog corpus, show that the performance of the
DBN-based models are definitely good enough to be
used in a dialog system in order to supply the dialog
manager with a rich and thorough representation of
the user’s request semantics. Though this can also
be obtained using a rule-based approach, the DBN
models alone are able to derive n-best lists of se-
mantic tree hypotheses with confidence scores. The
incidence of such outputs on the dialog manager de-
cision accuracy needs to be asserted.
</bodyText>
<sectionHeader confidence="0.973856" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.995893">
This work is supported by the 6th Framework Re-
search Program of the European Union (EU), LUNA
Project, IST contract no 33549,www.ist-luna.eu
</bodyText>
<sectionHeader confidence="0.999291" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998811916666667">
J. Bilmes and K. Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In NAACL
HLT.
J. Bilmes and G. Zweig. 2002. The graphical models
toolkit: An open source software system for speech
and time-series processing. In IEEE ICASSP.
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
D. Mostefa, and the Media consortium. 2005. Seman-
tic annotation of the MEDIA corpus for spoken dialog.
In ISCA Eurospeech.
C.J. Fillmore, C.R. Johnson, and M.R.L. Petruck. 2003.
Background to framenet. International Journal of
Lexicography, 16.3:235–250.
Y. He and S. Young. 2005. Spoken language understand-
ing using the hidden vector state model. Speech Com-
munication, 48(3-4):262–275.
F. Lef`evre. 2007. Dynamic bayesian networks and dis-
criminative classifiers for multi-stage semantic inter-
pretation. In IEEE ICASSP.
J.B. Lowe, C.F. Baker, and C.J. Fillmore. 1997. A frame-
semantic approach to semantic annotation. In SIGLEX
Workshop: Why, What, and How?
A. Stolcke. 2002. Srilm an extensible language model-
ing toolkit. In IEEE ICASSP.
</reference>
<page confidence="0.999417">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.811643">
<title confidence="0.999928">Learning Bayesian Networks for Semantic Frame in a Spoken Dialog System</title>
<author confidence="0.997289">Marie-Jean Meurs</author>
<author confidence="0.997289">Fabrice Lef`evre</author>
<author confidence="0.997289">Renato de</author>
<affiliation confidence="0.929112">Universit´e d’Avignon et des Pays de Vaucluse</affiliation>
<address confidence="0.836019">Laboratoire Informatique d’Avignon (EA 931), F-84911 Avignon, France.</address>
<abstract confidence="0.99864175">A stochastic approach based on Dynamic Bayesian Networks (DBNs) is introduced for spoken language understanding. DBN-based models allow to infer and then to compose semantic frame-based tree structures from speech transcriptions. Experimental results on French corpus show the appropriateness of the technique which both lead to good tree identification results and can provide the dialog system with n-best lists of scored hypotheses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>K Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In NAACL HLT.</booktitle>
<contexts>
<context position="8178" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="1292" endWordPosition="1295">ks. During the decoding step, every combination is tested, even not encountered in the training data, by means of a back-off technique. Due to the increase in model complexity, a sub-optimal beam search is applied for decoding. In this way, the 2-level approach reduces the complexity of the factored approach while preserving model generalization. Because all variables are observed at training time, the edge’s conditional probability tables are directly derived from observation counts. To improve their estimates, factored language models (FLMs) are used along with generalized parallel backoff (Bilmes and Kirchhoff, 2003). Several FLM implementations of the joint distributions are used in the DBN models, corresponding to the arrows in Figures 2 and 3. In the FLMs given below, n is the history length (n = 1 for bigrams), the uppercase and lowercase letters FFE, F, FE, C and W respectively stand for frame/FE (one variable), frame, FE, concept and word variables: • Frame/FE compound variable: P(FFE) � Qnk=0 P(ffek|ffek−1); P(C|FFE) _ Qnk=0 P(ck|ck−1, ffek); P(W|C,FFE) _ Qnk=0 P(wk|wk−1,ck, ffek). • Frame an77��77d FE variables, joint decoding: P(F) _ 11k=0P(fk |fk−1); P(FE|F) _ Qnk=0 P(fek|fek−1, fk); P(C|FE, F) </context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>J. Bilmes and K. Kirchhoff. 2003. Factored language models and generalized parallel backoff. In NAACL HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>G Zweig</author>
</authors>
<title>The graphical models toolkit: An open source software system for speech and time-series processing.</title>
<date>2002</date>
<booktitle>In IEEE ICASSP.</booktitle>
<contexts>
<context position="2487" citStr="Bilmes and Zweig, 2002" startWordPosition="373" endWordPosition="376"> a dialog act can be added on top of the concept set. Here we intend to consider an additional semantic composition step which will capture the abstract semantic structures conveyed by the basic concept representation. A frame formalism is applied to specify these nested structures. As such structures do not rely on sequential constraints, pure left-right branching semantic parser (such as (He and Young, 2005)) will not apply in this case. To derive automatically such frame meaning representations we propose a system based on a two decoding step process using dynamic Bayesian networks (DBNs) (Bilmes and Zweig, 2002): first basic concepts are derived from the user’s utterance transcriptions, then inferences are made on sequential semantic frame structures, considering all the available previous annotation levels (words and concepts). The inference process extracts all possible sub-trees (branches) according to lower level information (generation) and composes the hypothesized branches into a single utterance-span tree (composition). A hand-craft rule-based approach is used to derive the seed annotated training data. So both approaches are not competing and the stochastic approach is justified as only the </context>
<context position="12543" citStr="Bilmes and Zweig, 2002" startWordPosition="1992" endWordPosition="1995">0.88/0.81 0.92/0.83 0.93/0.97 0.87/0.79 0.90/0.86 0.94/0.97 (2 variables) ¯F-m 0.90 0.86 0.94 0.80 0.83 0.91 0.78 0.84 0.93 frames then FEs ¯p/¯r 0.92/0.94 0.91/0.82 0.92/0.98 0.88/0.86 0.91/0.80 0.92/0.97 0.87/0.81 0.89/0.82 0.93/0.98 (2-level) ¯F-m 0.91 0.83 0.93 0.83 0.80 0.90 0.79 0.80 0.92 Table 1: Precision (¯p), Recall (¯r) and F-measure (¯F-m) on the MEDIA test set for the DBN-based frame composition systems. generated by an ASR system and concepts decoded using them (14.8% word error rate, 24.3% concept error rate). All the experiments reported in the paper were performed using GMTK (Bilmes and Zweig, 2002), a general purpose graphical model toolkit and SRILM (Stolcke, 2002), a language modeling toolkit. Table 1 is populated with the results on the test set for the DBN-based frame composition systems in terms of precision, recall and F-measure. For the FE figures, only the reference FEs corresponding to correctly identified frames are considered. Only the frame and FE names are considered, neither their constituents nor their order matter. Finally, results are given for the sub-frame links between frames and FEs. Table 1 shows that the performances of the 3 DBN-based systems are quite comparable</context>
</contexts>
<marker>Bilmes, Zweig, 2002</marker>
<rawString>J. Bilmes and G. Zweig. 2002. The graphical models toolkit: An open source software system for speech and time-series processing. In IEEE ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bonneau-Maynard</author>
<author>S Rosset</author>
<author>C Ayache</author>
<author>A Kuhn</author>
<author>D Mostefa</author>
</authors>
<title>and the Media consortium.</title>
<date>2005</date>
<booktitle>In ISCA Eurospeech.</booktitle>
<contexts>
<context position="4219" citStr="Bonneau-Maynard et al., 2005" startWordPosition="643" endWordPosition="646">ation for Computational Linguistics lodging_hotel lodging_location FE FE HOTEL LOCATION frame frame LODGING frame FE location_event Frame trans Frame trans Frame-FE trans Frame-FE trans co ncept trans Frame FE word concept concept concept ns tra Frame FE word FE trans concept trans FE FE Frame Frame word concept concept FE trans concept trans word Figure 1: Frames, FEs and relations associated to the sequence “staying in a hotel near the Festival de Cannes” 2 Semantic Frames on the MEDIA corpus MEDIA is a French corpus of negotiation dialogs among users and a tourist information phone server (Bonneau-Maynard et al., 2005). The corpus contains 1,257 dialogs recorded using a Wizard of Oz system. The semantic corpus is annotated with concept-value pairs corresponding to word segments with the addition of specifier tags representing some relations between concepts. The annotation utilizes 83 basic concepts and 19 specifiers. Amongst the available semantic representations, the semantic frames (Lowe et al., 1997) are probably the most suited to the task, mostly because of their ability to represent negotiation dialogs. Semantic frames are computational models describing common or abstract situations involving roles,</context>
</contexts>
<marker>Bonneau-Maynard, Rosset, Ayache, Kuhn, Mostefa, 2005</marker>
<rawString>H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn, D. Mostefa, and the Media consortium. 2005. Semantic annotation of the MEDIA corpus for spoken dialog. In ISCA Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>C R Johnson</author>
<author>M R L Petruck</author>
</authors>
<title>Background to framenet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<pages>16--3</pages>
<contexts>
<context position="4890" citStr="Fillmore et al., 2003" startWordPosition="740" endWordPosition="743">g a Wizard of Oz system. The semantic corpus is annotated with concept-value pairs corresponding to word segments with the addition of specifier tags representing some relations between concepts. The annotation utilizes 83 basic concepts and 19 specifiers. Amongst the available semantic representations, the semantic frames (Lowe et al., 1997) are probably the most suited to the task, mostly because of their ability to represent negotiation dialogs. Semantic frames are computational models describing common or abstract situations involving roles, the frame elements (FEs). The FrameNet project (Fillmore et al., 2003) provides a large frame database for English. As no such resource exists for French, we elaborated a frame ontology to describe the semantic knowledge of the MEDIA domain. The MEDIA ontology is composed of 21 frames and 86 FEs. All are described by a set of manually defined patterns made of lexical units and conceptual units (frame and FE evoking words and concepts). Figure 1 gives the annotation of word sequence “staying in a hotel near the Festival de Cannes”. The training data are automatically annotated by a rule-based process. Pattern matching triggers the instantiation of frames and FEs </context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>C.J. Fillmore, C.R. Johnson, and M.R.L. Petruck. 2003. Background to framenet. International Journal of Lexicography, 16.3:235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>Spoken language understanding using the hidden vector state model.</title>
<date>2005</date>
<journal>Speech Communication,</journal>
<pages>48--3</pages>
<contexts>
<context position="1276" citStr="He and Young, 2005" startWordPosition="174" endWordPosition="177">ch both lead to good tree identification results and can provide the dialog system with n-best lists of scored hypotheses. 1 Introduction Recent developments in Spoken Dialog Systems (SDSs) have renewed the interest for the extraction of rich and high-level semantics from users’ utterances. Shifting every SDS component from hand-crafted to stochastic is foreseen as a good option to improve their overall performance by an increased robustness to speech variabilities. For instance stochastic methods are now efficient alternatives to rule-based techniques for Spoken Language Understanding (SLU) (He and Young, 2005; Lef`evre, 2007). The SLU module links up the automatic speech recognition (ASR) module and the dialog manager. From the user’s utterance analysis, it derives a representation of its semantic content upon which the dialog manager can decide the next best action to perform, taking into account the current dialog context. In this work, the overall objective is to increase the relevancy of the semantic information used by the system. Generally the internal meaning representation is based on flat concept sets obtained by either 61 keyword spotting or conceptual decoding. In some cases a dialog ac</context>
</contexts>
<marker>He, Young, 2005</marker>
<rawString>Y. He and S. Young. 2005. Spoken language understanding using the hidden vector state model. Speech Communication, 48(3-4):262–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Lef`evre</author>
</authors>
<title>Dynamic bayesian networks and discriminative classifiers for multi-stage semantic interpretation.</title>
<date>2007</date>
<booktitle>In IEEE ICASSP.</booktitle>
<marker>Lef`evre, 2007</marker>
<rawString>F. Lef`evre. 2007. Dynamic bayesian networks and discriminative classifiers for multi-stage semantic interpretation. In IEEE ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Lowe</author>
<author>C F Baker</author>
<author>C J Fillmore</author>
</authors>
<title>A framesemantic approach to semantic annotation.</title>
<date>1997</date>
<booktitle>In SIGLEX Workshop: Why,</booktitle>
<location>What, and How?</location>
<contexts>
<context position="4612" citStr="Lowe et al., 1997" startWordPosition="699" endWordPosition="702">sequence “staying in a hotel near the Festival de Cannes” 2 Semantic Frames on the MEDIA corpus MEDIA is a French corpus of negotiation dialogs among users and a tourist information phone server (Bonneau-Maynard et al., 2005). The corpus contains 1,257 dialogs recorded using a Wizard of Oz system. The semantic corpus is annotated with concept-value pairs corresponding to word segments with the addition of specifier tags representing some relations between concepts. The annotation utilizes 83 basic concepts and 19 specifiers. Amongst the available semantic representations, the semantic frames (Lowe et al., 1997) are probably the most suited to the task, mostly because of their ability to represent negotiation dialogs. Semantic frames are computational models describing common or abstract situations involving roles, the frame elements (FEs). The FrameNet project (Fillmore et al., 2003) provides a large frame database for English. As no such resource exists for French, we elaborated a frame ontology to describe the semantic knowledge of the MEDIA domain. The MEDIA ontology is composed of 21 frames and 86 FEs. All are described by a set of manually defined patterns made of lexical units and conceptual u</context>
</contexts>
<marker>Lowe, Baker, Fillmore, 1997</marker>
<rawString>J.B. Lowe, C.F. Baker, and C.J. Fillmore. 1997. A framesemantic approach to semantic annotation. In SIGLEX Workshop: Why, What, and How?</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In IEEE ICASSP.</booktitle>
<contexts>
<context position="12612" citStr="Stolcke, 2002" startWordPosition="2004" endWordPosition="2005"> 0.90 0.86 0.94 0.80 0.83 0.91 0.78 0.84 0.93 frames then FEs ¯p/¯r 0.92/0.94 0.91/0.82 0.92/0.98 0.88/0.86 0.91/0.80 0.92/0.97 0.87/0.81 0.89/0.82 0.93/0.98 (2-level) ¯F-m 0.91 0.83 0.93 0.83 0.80 0.90 0.79 0.80 0.92 Table 1: Precision (¯p), Recall (¯r) and F-measure (¯F-m) on the MEDIA test set for the DBN-based frame composition systems. generated by an ASR system and concepts decoded using them (14.8% word error rate, 24.3% concept error rate). All the experiments reported in the paper were performed using GMTK (Bilmes and Zweig, 2002), a general purpose graphical model toolkit and SRILM (Stolcke, 2002), a language modeling toolkit. Table 1 is populated with the results on the test set for the DBN-based frame composition systems in terms of precision, recall and F-measure. For the FE figures, only the reference FEs corresponding to correctly identified frames are considered. Only the frame and FE names are considered, neither their constituents nor their order matter. Finally, results are given for the sub-frame links between frames and FEs. Table 1 shows that the performances of the 3 DBN-based systems are quite comparable. Anyhow the 2-level system can be considered the best as besides its</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm an extensible language modeling toolkit. In IEEE ICASSP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>