<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002198">
<title confidence="0.991967">
Improved Unsupervised POS Induction through Prototype Discovery
</title>
<author confidence="0.999034">
Omri Abend&apos;* Roi Reichart2 Ari Rappoport&apos;
</author>
<affiliation confidence="0.999589">
&apos;Institute of Computer Science, 2ICNC
Hebrew University of Jerusalem
</affiliation>
<email confidence="0.996793">
{omria01|roiri|arir}@cs.huji.ac.il
</email>
<sectionHeader confidence="0.993838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972692307692">
We present a novel fully unsupervised al-
gorithm for POS induction from plain text,
motivated by the cognitive notion of proto-
types. The algorithm first identifies land-
mark clusters of words, serving as the
cores of the induced POS categories. The
rest of the words are subsequently mapped
to these clusters. We utilize morpho-
logical and distributional representations
computed in a fully unsupervised manner.
We evaluate our algorithm on English and
German, achieving the best reported re-
sults for this task.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.974118210526316">
Part-of-speech (POS) tagging is a fundamental
NLP task, used by a wide variety of applications.
However, there is no single standard POS tag-
ging scheme, even for English. Schemes vary
significantly across corpora and even more so
across languages, creating difficulties in using
POS tags across domains and for multi-lingual
systems (Jiang et al., 2009). Automatic induction
of POS tags from plain text can greatly alleviate
this problem, as well as eliminate the efforts in-
curred by manual annotations. It is also a problem
of great theoretical interest. Consequently, POS
induction is a vibrant research area (see Section 2).
In this paper we present an algorithm based
on the theory of prototypes (Taylor, 2003), which
posits that some members in cognitive categories
are more central than others. These practically de-
fine the category, while the membership of other
elements is based on their association with the
* Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
central members. Our algorithm first clusters
words based on a fine morphological representa-
tion. It then clusters the most frequent words,
defining landmark clusters which constitute the
cores of the categories. Finally, it maps the rest
of the words to these categories. The last two
stages utilize a distributional representation that
has been shown to be effective for unsupervised
parsing (Seginer, 2007).
We evaluated the algorithm in both English and
German, using four different mapping-based and
information theoretic clustering evaluation mea-
sures. The results obtained are generally better
than all existing POS induction algorithms.
Section 2 reviews related work. Sections 3 and
4 detail the algorithm. Sections 5, 6 and 7 describe
the evaluation, experimental setup and results.
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998012157894737">
Unsupervised and semi-supervised POS tagging
have been tackled using a variety of methods.
Sch¨utze (1995) applied latent semantic analysis.
The best reported results (when taking into ac-
count all evaluation measures, see Section 5) are
given by (Clark, 2003), which combines dis-
tributional and morphological information with
the likelihood function of the Brown algorithm
(Brown et al., 1992). Clark’s tagger is very sen-
sitive to its initialization. Reichart et al. (2010b)
propose a method to identify the high quality runs
of this algorithm. In this paper, we show that
our algorithm outperforms not only Clark’s mean
performance, but often its best among 100 runs.
Most research views the task as a sequential la-
beling problem, using HMMs (Merialdo, 1994;
Banko and Moore, 2004; Wang and Schuurmans,
2005) and discriminative models (Smith and Eis-
ner, 2005; Haghighi and Klein, 2006). Several
</bodyText>
<page confidence="0.933741">
1298
</page>
<note confidence="0.9420735">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999372872727273">
techniques were proposed to improve the HMM
model. A Bayesian approach was employed by
(Goldwater and Griffiths, 2007; Johnson, 2007;
Gao and Johnson, 2008). Van Gael et al. (2009)
used the infinite HMM with non-parametric pri-
ors. Grac¸a et al. (2009) biased the model to induce
a small number of possible tags for each word.
The idea of utilizing seeds and expanding them
to less reliable data has been used in several pa-
pers. Haghighi and Klein (2006) use POS ‘pro-
totypes’ that are manually provided and tailored
to a particular POS tag set of a corpus. Fre-
itag (2004) and Biemann (2006) induce an ini-
tial clustering and use it to train an HMM model.
Dasgupta and Ng (2007) generate morphological
clusters and use them to bootstrap a distributional
model. Goldberg et al. (2008) use linguistic con-
siderations for choosing a good starting point for
the EM algorithm. Zhao and Marcus (2009) ex-
pand a partial dictionary and use it to learn dis-
ambiguation rules. Their evaluation is only at the
type level and only for half of the words. Ravi
and Knight (2009) use a dictionary and an MDL-
inspired modification to the EM algorithm.
Many of these works use a dictionary provid-
ing allowable tags for each or some of the words.
While this scenario might reduce human annota-
tion efforts, it does not induce a tagging scheme
but remains tied to an existing one. It is further
criticized in (Goldwater and Griffiths, 2007).
Morphological representation. Many POS in-
duction models utilize morphology to some ex-
tent. Some use simplistic representations of termi-
nal letter sequences (e.g., (Smith and Eisner, 2005;
Haghighi and Klein, 2006)). Clark (2003) models
the entire letter sequence as an HMM and uses it
to define a morphological prior. Dasgupta and Ng
(2007) use the output of the Morfessor segmenta-
tion algorithm for their morphological representa-
tion. Morfessor (Creutz and Lagus, 2005), which
we use here as well, is an unsupervised algorithm
that segments words and classifies each segment
as being a stem or an affix. It has been tested on
several languages with strong results.
Our work has several unique aspects. First,
our clustering method discovers prototypes in a
fully unsupervised manner, mapping the rest of
the words according to their association with the
prototypes. Second, we use a distributional repre-
sentation which has been shown to be effective for
unsupervised parsing (Seginer, 2007). Third, we
use a morphological representation based on sig-
natures, which are sets of affixes that represent a
family of words sharing an inflectional or deriva-
tional morphology (Goldsmith, 2001).
</bodyText>
<sectionHeader confidence="0.994984" genericHeader="method">
3 Distributional Algorithm
</sectionHeader>
<bodyText confidence="0.999187409090909">
Our algorithm is given a plain text corpus and op-
tionally a desired number of clusters k. Its output
is a partitioning of words into clusters. The al-
gorithm utilizes two representations, distributional
and morphological. Although eventually the latter
is used before the former, for clarity of presenta-
tion we begin by detailing the base distributional
algorithm. In the next section we describe the mor-
phological representation and its integration into
the base algorithm.
Overview. The algorithm consists of two main
stages: landmark clusters discovery, and word
mapping. For the former, we first compute a dis-
tributional representation for each word. We then
cluster the coordinates corresponding to high fre-
quency words. Finally, we define landmark clus-
ters. In the word mapping stage we map each word
to the most similar landmark cluster.
The rationale behind using only the high fre-
quency words in the first stage is twofold. First,
prototypical members of a category are frequent
(Taylor, 2003), and therefore we can expect the
salient POS tags to be represented in this small
subset. Second, higher frequency implies more re-
liable statistics. Since this stage determines the
cores of all resulting clusters, it should be as accu-
rate as possible.
Distributional representation. We use a sim-
plified form of the elegant representation of lexi-
cal entries used by the Seginer unsupervised parser
(Seginer, 2007). Since a POS tag reflects the
grammatical role of the word and since this rep-
resentation is effective to parsing, we were moti-
vated to apply it to the present task.
Let W be the set of word types in the corpus.
The right context entry of a word x E W is a pair
of mappings r intx : W → [0, 1] and r adjx :
W → [0, 1]. For each w E W, r adjx(w) is an
adjacency score of w to x, reflecting w’s tendency
to appear on the right hand side of x.
For each w E W, r intx(w) is an interchange-
ability score of x with w, reflecting the tendency
of w to appear to the left of words that tend to ap-
pear to the right of x. This can be viewed as a
</bodyText>
<page confidence="0.995399">
1299
</page>
<bodyText confidence="0.999556238095238">
similarity measure between words with respect to
their right context. The higher the scores the more
the words tend to be adjacent/interchangeable.
Left context parameters l intx and l adjx are
defined analogously.
There are important subtleties in these defini-
tions. First, for two words x, w E W, r adjx(w)
is generally different from l adjw(x). For exam-
ple, if w is a high frequency word and x is a low
frequency word, it is likely that w appears many
times to the right of x, yielding a high r adjx(w),
but that x appears only a few times to the left of w
yielding a low l adjw(x). Second, from the defi-
nition of r intx(w) and r intw(x), it is clear that
they need not be equal.
These functions are computed incrementally by
a bootstrapping process. We initialize all map-
pings to be identically 0. We iterate over the words
in the training corpus. For every word instance x,
we take the word immediately to its right y and
update x’s right context using y’s left context:
</bodyText>
<equation confidence="0.98955325">
bw E W : r intx(w) += l adjy (w)N(y)
1 w = y
bw E W : r adjx(w) += l int��w) N�y)y
w
</equation>
<bodyText confidence="0.999341594594595">
The division by N(y) (the number of times y
appears in the corpus before the update) is done in
order not to give a disproportional weight to high
frequency words. Also, r intx(w) and r adjx(w)
might become larger than 1. We therefore nor-
malize them after all updates are performed by the
number of occurrences of x in the corpus.
We update l intx and l adjx analogously using
the word z immediately to the left of x. The up-
dates of the left and right functions are done in
parallel.
We define the distributional representation of a
word type x to be a 4|W  |+ 2 dimensional vector
vx. Each word w yields four coordinates, one for
each direction (left/right) and one for each map-
ping type (int/adj). Two additional coordinates
represent the frequency in which the word appears
to the left and to the right of a stopping punc-
tuation. Of the 4|W |coordinates corresponding
to words, we allow only 2n to be non-zero: the
n top scoring among the right side coordinates
(those of r intx and r adjx), and the n top scoring
among the left side coordinates (those of l intx
and l adjx). We used n = 50.
The distance between two words is defined to
be one minus the cosine of the angle between their
representation vectors.
Coordinate clustering. Each of our landmark
clusters will correspond to a set of high frequency
words (HFWs). The number of HFWs is much
larger than the number of expected POS tags.
Hence we should cluster HFWs. Our algorithm
does that by unifying some of the non-zero coordi-
nates corresponding to HFWs in the distributional
representation defined above.
We extract the words that appear more than N
times per million1 and apply the following proce-
dure I times (5 in our experiments).
We run average link clustering with a threshold
α (AVGLINKα, (Jain et al., 1999)) on these words,
in each iteration initializing every HFW to have
its own cluster. AVGLINKα means running the av-
erage link algorithm until the two closest clusters
have a distance larger than α. We then use the in-
duced clustering to update the distributional rep-
resentation, by collapsing all coordinates corre-
sponding to words appearing in the same cluster
into a single coordinate whose value is the sum
of the collapsed coordinates’ values. In order to
produce a conservative (fine) clustering, we used a
relatively low α value of 0.25.
Note that the AVGLINKα initialization in each
of the I iterations assigns each HFW to a sepa-
rate cluster. The iterations differ in the distribu-
tional representation of the HFWs, resulting from
the previous iterations.
In our English experiments, this process re-
duced the dimension of the HFWs set (the num-
ber of coordinates that are non-zero in at least one
of the HFWs) from 14365 to 10722. The aver-
age number of non-zero coordinates per word de-
creased from 102 to 55.
Since all eventual POS categories correspond to
clusters produced at this stage, to reduce noise we
delete clusters of less than five elements.
Landmark detection. We define landmark clus-
ters using the clustering obtained in the final iter-
ation of the coordinate clustering stage. However,
the number of clusters might be greater than the
desired number k, which is an optional parame-
ter of the algorithm. In this case we select a sub-
set of k clusters that best covers the HFW space.
We use the following heuristic. We start from the
most frequent cluster, and greedily select the clus-
</bodyText>
<footnote confidence="0.9990235">
1We used N = 100, yielding 1242 words for English and
613 words for German.
</footnote>
<page confidence="0.98746">
1300
</page>
<bodyText confidence="0.998594">
ter farthest from the clusters already selected. The
distance between two clusters is defined to be the
average distance between their members. A clus-
ter’s distance from a set of clusters is defined to
be its minimal distance from the clusters in the
set. The final set of clusters {Li, ..., Lk} and their
members are referred to as landmark clusters and
prototypes, respectively.
Mapping all words. Each word w E W is as-
signed the cluster Li that contains its nearest pro-
totype:
</bodyText>
<equation confidence="0.988286">
d(w, Li) = minimELi{1 − cos(vw, vim)}
Map(w) = argminLi{d(w, Li)}
</equation>
<bodyText confidence="0.99996275">
Words that appear less than 5 times are consid-
ered as unknown words. We consider two schemes
for handling unknown words. One randomly maps
each such word to a cluster, using a probabil-
ity proportional to the number of unique known
words already assigned to that cluster. However,
when the number k of landmark clusters is rela-
tively large, it is beneficial to assign all unknown
words to a separate new cluster (after running the
algorithm with k − 1). In our experiments, we use
the first option when k is below some threshold
(we used 15), otherwise we use the second.
</bodyText>
<sectionHeader confidence="0.998917" genericHeader="method">
4 Morphological Model
</sectionHeader>
<bodyText confidence="0.99996725">
The morphological model generates another word
clustering, based on the notion of a signature.
This clustering is integrated with the distributional
model as described below.
</bodyText>
<subsectionHeader confidence="0.997939">
4.1 Morphological Representation
</subsectionHeader>
<bodyText confidence="0.9999834375">
We use the Morfessor (Creutz and Lagus, 2005)
word segmentation algorithm. First, all words in
the corpus are segmented. Then, for each stem,
the set of all affixes with which it appears (its sig-
nature, (Goldsmith, 2001)) is collected. The mor-
phological representation of a word type is then
defined to be its stem’s signature in conjunction
with its specific affixes2 (See Figure 1).
We now collect all words having the same rep-
resentation. For instance, if the words joined and
painted are found to have the same signature, they
would share the same cluster since both have the
affix ‘ ed’. The wordjoins does not share the same
cluster with them since it has a different affix, ‘ s’.
This results in coarse-grained clusters exclusively
defined according to morphology.
</bodyText>
<footnote confidence="0.657215">
2A word may contain more than a single affix.
</footnote>
<table confidence="0.373308">
Types join joins joined joining
Stem join join join join
Affixes s ed ing
Signature {0, ed, s, ing}
</table>
<figureCaption confidence="0.946237666666667">
Figure 1: An example for a morphological representation,
defined to be the conjunction of its affix(es) with the stem’s
signature.
</figureCaption>
<bodyText confidence="0.999909041666667">
In addition, we incorporate capitalization infor-
mation into the model, by constraining all words
that appear capitalized in more than half of their
instances to belong to a separate cluster, regard-
less of their morphological representation. The
motivation for doing so is practical: capitalization
is used in many languages to mark grammatical
categories. For instance, in English capitalization
marks the category of proper names and in Ger-
man it marks the noun category . We report En-
glish results both with and without this modifica-
tion.
Words that contain non-alphanumeric charac-
ters are represented as the sequence of the non-
alphanumeric characters they include, e.g., ‘vis-`a-
vis’ is represented as (“-”, “-”). We do not as-
sign a morphological representation to words in-
cluding more than one stem (like weatherman), to
words that have a null affix (i.e., where the word
is identical to its stem) and to words whose stem
is not shared by any other word (signature of size
1). Words that were not assigned a morphologi-
cal representation are included as singletons in the
morphological clustering.
</bodyText>
<subsectionHeader confidence="0.808195">
4.2 Distributional-Morphological Algorithm
</subsectionHeader>
<bodyText confidence="0.996419941176471">
We detail the modifications made to our base
distributional algorithm given the morphological
clustering defined above.
Coordinate clustering and landmarks. We
constrain AVGLINKα to begin by forming links be-
tween words appearing in the same morphologi-
cal cluster. Only when the distance between the
two closest clusters gets above α we remove this
constraint and proceed as before. This is equiv-
alent to performing AVGLINKα separately within
each morphological cluster and then using the re-
sult as an initial condition for an AVGLINKα coor-
dinate clustering. The modified algorithm in this
stage is otherwise identical to the distributional al-
gorithm.
Word mapping. In this stage words that are not
prototypes are mapped to one of the landmark
</bodyText>
<page confidence="0.961008">
1301
</page>
<bodyText confidence="0.999917428571429">
clusters. A reasonable strategy would be to map
all words sharing a morphological cluster as a sin-
gle unit. However, these clusters are too coarse-
grained. We therefore begin by partitioning the
morphological clusters into sub-clusters according
to their distributional behavior. We do so by apply-
ing AVGLINK,3 (the same as AVGLINK, but with a
different parameter) to each morphological clus-
ter. Since our goal is cluster refinement, we use a
Q that is considerably higher than α (0.9).
We then find the closest prototype to each such
sub-cluster (averaging the distance across all of
the latter’s members) and map it as a single unit
to the cluster containing that prototype.
</bodyText>
<sectionHeader confidence="0.988374" genericHeader="method">
5 Clustering Evaluation
</sectionHeader>
<bodyText confidence="0.999842027777778">
We evaluate the clustering produced by our algo-
rithm using an external quality measure: we take
a corpus tagged by gold standard tags, tag it using
the induced tags, and compare the two taggings.
There is no single accepted measure quantifying
the similarity between two taggings. In order to
be as thorough as possible, we report results using
four known measures, two mapping-based mea-
sures and two information theoretic ones.
Mapping-based measures. The induced clus-
ters have arbitrary names. We define two map-
ping schemes between them and the gold clus-
ters. After the induced clusters are mapped, we
can compute a derived accuracy. The Many-to-1
measure finds the mapping between the gold stan-
dard clusters and the induced clusters which max-
imizes accuracy, allowing several induced clusters
to be mapped to the same gold standard cluster.
The 1-to-1 measure finds the mapping between
the induced and gold standard clusters which max-
imizes accuracy such that no two induced clus-
ters are mapped to the same gold cluster. Com-
puting this mapping is equivalent to finding the
maximal weighted matching in a bipartite graph,
whose weights are given by the intersection sizes
between matched classes/clusters. As in (Reichart
and Rappoport, 2008), we use the Kuhn-Munkres
algorithm (Kuhn, 1955; Munkres, 1957) to solve
this problem.
Information theoretic measures. These are
based on the observation that a good clustering re-
duces the uncertainty of the gold tag given the in-
duced cluster, and vice-versa. Several such mea-
sures exist; we use V (Rosenberg and Hirschberg,
2007) and NVI (Reichart and Rappoport, 2009),
VI’s (Meila, 2007) normalized version.
</bodyText>
<sectionHeader confidence="0.998462" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999981212765957">
Since a goal of unsupervised POS tagging is in-
ducing an annotation scheme, comparison to an
existing scheme is problematic. To address this
problem we compare to three different schemes
in two languages. In addition, the two English
schemes we compare with were designed to tag
corpora contained in our training set, and have
been widely and successfully used with these cor-
pora by a large number of applications.
Our algorithm was run with the exact same pa-
rameters on both languages: N = 100 (high fre-
quency threshold), n = 50 (the parameter that
determines the effective number of coordinates),
α = 0.25 (cluster separation during landmark
cluster generation), 0 = 0.9 (cluster separation
during refinement of morphological clusters).
The algorithm we compare with in most detail
is (Clark, 2003), which reports the best current
results for this problem (see Section 7). Since
Clark’s algorithm is sensitive to its initialization,
we ran it a 100 times and report its average and
standard deviation in each of the four measures.
In addition, we report the percentile in which our
result falls with respect to these 100 runs.
Punctuation marks are very frequent in corpora
and are easy to cluster. As a result, including them
in the evaluation greatly inflates the scores. For
this reason we do not assign a cluster to punctua-
tion marks and we report results using this policy,
which we recommend for future work. However,
to be able to directly compare with previous work,
we also report results for the full POS tag set.
We do so by assigning a singleton cluster to each
punctuation mark (in addition to the k required
clusters). This simple heuristic yields very high
performance on punctuation, scoring (when all
other words are assumed perfect tagging) 99.6%
(99.1%) 1-to-1 accuracy when evaluated against
the English fine (coarse) POS tag sets, and 97.2%
when evaluated against the German POS tag set.
For English, we trained our model on the
39832 sentences which constitute sections 2-21 of
the PTB-WSJ and on the 500K sentences from
the NYT section of the NANC newswire corpus
(Graff, 1995). We report results on the WSJ part
of our data, which includes 950028 words tokens
in 44389 types. Of the tokens, 832629 (87.6%)
</bodyText>
<page confidence="0.987188">
1302
</page>
<table confidence="0.999922863636364">
English Fine k=13 Coarse k=13 Fine k=34
Prototype Clark Prototype Clark Prototype Clark
Tagger µ v % Tagger µ v % Tagger µ v %
Many–to–1 61.0 55.1 1.6 100 70.0 66.9 2.1 94 71.6 69.8 1.5 90
55.5 48.8 1.8 100 66.1 62.6 2.3 94 67.5 65.5 1.7 90
1–to–1 60.0 52.2 1.9 100 58.1 49.4 2.9 100 63.5 54.5 1.6 100
54.9 46.0 2.2 100 53.7 43.8 3.3 100 58.8 48.5 1.8 100
NVI 0.652 0.773 0.027 100 0.841 0.972 0.036 100 0.663 0.725 0.018 100
0.795 0.943 0.033 100 1.052 1.221 0.046 100 0.809 0.885 0.022 100
V 0.636 0.581 0.015 100 0.590 0.543 0.018 100 0.677 0.659 0.008 100
0.542 0.478 0.019 100 0.484 0.429 0.023 100 0.608 0.588 0.010 98
German k=17 k=26
Prototype Clark Prototype Clark
Tagger µ v % Tagger µ v %
Many–to-1 64.6 64.7 1.2 41 68.2 67.8 1.0 60
58.9 59.1 1.4 40 63.2 62.8 1.2 60
1–to–1 53.7 52.0 1.8 77 56.0 52.0 2.1 99
48.0 46.0 2.3 78 50.7 45.9 2.6 99
NVI 0.667 0.675 0.019 66 0.640 0.682 0.019 100
0.819 0.829 0.025 66 0.785 0.839 0.025 100
V 0.646 0.645 0.010 50 0.675 0.657 0.008 100
0.552 0.553 0.013 48 0.596 0.574 0.010 100
</table>
<tableCaption confidence="0.805167">
Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark’s average score (µ),
Clark’s standard deviation (v) and the fraction of Clark’s results that scored worse than our model (%). For the mapping based
measures, results are accuracy percentage. For V E [0, 1], higher is better. For high quality output, NVI E [0, 1] as well, and
lower is better. In each entry, the top number indicates the score when including punctuation and the bottom number the score
when excluding it. In English, our results are always better than Clark’s. In German, they are almost always better.
</tableCaption>
<bodyText confidence="0.999900189655173">
are not punctuation. The percentage of unknown
words (those appearing less than five times) is
1.6%. There are 45 clusters in this annotation
scheme, 34 of which are not punctuation.
We ran each algorithm both with k=13 and
k=34 (the number of desired clusters). We com-
pare the output to two annotation schemes: the fine
grained PTB WSJ scheme, and the coarse grained
tags defined in (Smith and Eisner, 2005). The
output of the k=13 run is evaluated both against
the coarse POS tag annotation (the ‘Coarse k=13’
scenario) and against the full PTB-WSJ annotation
scheme (the ‘Fine k=13’ scenario). The k=34 run
is evaluated against the full PTB-WSJ annotation
scheme (the ‘Fine k=34’ scenario).
The POS cluster frequency distribution tends to
be skewed: each of the 13 most frequent clusters
in the PTB-WSJ cover more than 2.5% of the to-
kens (excluding punctuation) and together 86.3%
of them. We therefore chose k=13, since it is both
the number of coarse POS tags (excluding punctu-
ation) as well as the number of frequent POS tags
in the PTB-WSJ annotation scheme. We chose
k=34 in order to evaluate against the full 34 tags
PTB-WSJ annotation scheme (excluding punctua-
tion) using the same number of clusters.
For German, we trained our model on the 20296
sentences of the NEGRA corpus (Brants, 1997)
and on the first 450K sentences of the DeWAC
corpus (Baroni et al., 2009). DeWAC is a cor-
pus extracted by web crawling and is therefore
out of domain. We report results on the NEGRA
part, which includes 346320 word tokens of 49402
types. Of the tokens, 289268 (83.5%) are not
punctuation. The percentage of unknown words
(those appearing less than five times) is 8.1%.
There are 62 clusters in this annotation scheme,
51 of which are not punctuation.
We ran the algorithms with k=17 and k=26.
k=26 was chosen since it is the number of clus-
ters that cover each more than 0.5% of the NE-
GRA tokens, and in total cover 96% of the (non-
punctuation) tokens. In order to test our algo-
rithm in another scenario, we conducted experi-
ments with k=17 as well, which covers 89.9% of
the tokens. All outputs are compared against NE-
GRA’s gold standard scheme.
We do not report results for k=51 (where the
number of gold clusters is the same as the number
of induced clusters), since our algorithm produced
only 42 clusters in the landmark detection stage.
We could of course have modified the parame-
ters to allow our algorithm to produce 51 clusters.
However, we wanted to use the exact same param-
eters as those used for the English experiments to
minimize the issue of parameter tuning.
In addition to the comparisons described above,
we present results of experiments (in the ‘Fine
</bodyText>
<page confidence="0.919739">
1303
</page>
<table confidence="0.999665">
B B+M B+C F(I=1) F
M-to-1 53.3 54.8 58.2 57.3 61.0
1-to-1 50.2 51.7 55.1 54.8 60.0
NVI 0.782 0.720 0.710 0.742 0.652
V 0.569 0.598 0.615 0.597 0.636
</table>
<tableCaption confidence="0.92367">
Table 2: A comparison of partial versions of the model in
</tableCaption>
<figureCaption confidence="0.570656166666667">
the ‘Fine k=13’ WSJ scenario. M-to-1 and 1-to-1 results are
reported in accuracy percentage. Lower NVI is better. B is the
strictly distributional algorithm, B+M adds the morphologi-
cal model, B+C adds capitalization to B, F(I=1) consists of
all components, where only one iteration of coordinate clus-
tering is performed, and F is the full model.
</figureCaption>
<table confidence="0.9995666">
M-to-1 1-to-1 V VI
Prototype 71.6 63.5 0.677 2.00
Clark 69.8 54.5 0.659 2.18
HK – 41.3 – –
J 43–62 37–47 – 4.23–5.74
GG – – – 2.8
GJ – 40–49.9 – 4.03–4.47
VG – – 0.54-0.59 2.5–2.9
GGTP-45 65.4 44.5 – –
GGTP-17 70.2 49.5 – –
</table>
<tableCaption confidence="0.970425">
Table 4: Comparison of our algorithms with the recent fully
</tableCaption>
<figureCaption confidence="0.910814846153846">
unsupervised POS taggers for which results are reported. The
models differ in the annotation scheme, the corpus size and
the number of induced clusters (k) that they used. HK:
(Haghighi and Klein, 2006), 193K tokens, fine tags, k=45.
GG: (Goldwater and Griffiths, 2007), 24K tokens, coarse
tags, k=17. J : (Johnson, 2007), 1.17M tokens, fine tags,
k=25–50. GJ: (Gao and Johnson, 2008), 1.17M tokens, fine
tags, k=50. VG: (Van Gael et al., 2009), 1.17M tokens, fine
tags, k=47–192. GGTP-45: (Grac¸a et al., 2009), 1.17M to-
kens, fine tags, k=45. GGTP-17: (Grac¸a et al., 2009), 1.17M
tokens, coarse tags, k=17. Lower VI values indicate better
clustering. VI is computed using a as the base of the loga-
rithm. Our algorithm gives the best results.
</figureCaption>
<bodyText confidence="0.999857222222222">
k=13’ scenario) that quantify the contribution of
each component of the algorithm. We ran the base
distributional algorithm, a variant which uses only
capitalization information (i.e., has only one non-
singleton morphological class, that of words ap-
pearing capitalized in most of their instances) and
a variant which uses no capitalization information,
defining the morphological clusters according to
the morphological representation alone.
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999807111111111">
Table 1 presents results for the English and Ger-
man experiments. For English, our algorithm ob-
tains better results than Clark’s in all measures and
scenarios. It is without exception better than the
average score of Clark’s and in most cases better
than the maximal Clark score obtained in 100 runs.
A significant difference between our algorithm
and Clark’s is that the latter, like most algorithms
which addressed the task, induces the clustering
</bodyText>
<figure confidence="0.997444">
1
0.8
0.6
Gold Standard
Induced
0.2
0
0 5 10 15 20 25 30 35 40 45
</figure>
<figureCaption confidence="0.999172">
Figure 2: POS class frequency distribution for our model
and the gold standard, in the ‘Fine k=34’ scenario. The dis-
tributions are similar.
</figureCaption>
<bodyText confidence="0.999858131578947">
by maximizing a non-convex function. These
functions have many local maxima and the specific
solution to which algorithms that maximize them
converge strongly depends on their (random) ini-
tialization. Therefore, their output’s quality often
significantly diverges from the average. This issue
is discussed in depth in (Reichart et al., 2010b).
Our algorithm is deterministic3.
For German, in the k=26 scenario our algorithm
outperforms Clark’s, often outperforming even its
maximum in 100 runs. In the k=17 scenario, our
algorithm obtains a higher score than Clark with
probability 0.4 to 0.78, depending on the measure
and scenario. Clark’s average score is slightly bet-
ter in the Many-to-1 measure, while our algorithm
performs somewhat better than Clark’s average in
the 1-to-1 and NVI measures.
The DeWAC corpus from which we extracted
statistics for the German experiments is out of do-
main with respect to NEGRA. The correspond-
ing corpus in English, NANC, is a newswire cor-
pus and therefore clearly in-domain with respect
to WSJ. This is reflected by the percentage of un-
known words, which was much higher in German
than in English (8.1% and 1.6%), lowering results.
Table 2 shows the effect of each of our algo-
rithm’s components. Each component provides
an improvement over the base distributional algo-
rithm. The full coordinate clustering stage (sev-
eral iterations, F) considerably improves the score
over a single iteration (F(I=1)). Capitalization in-
formation increases the score more than the mor-
phological information, which might stem from
the granularity of the POS tag set with respect to
names. This analysis is supported by similar ex-
periments we made in the ‘Coarse k=13’ scenario
(not shown in tables here). There, the decrease in
performance was only of 1%–2% in the mapping
</bodyText>
<footnote confidence="0.957296">
3The fluctuations inflicted on our algorithm by the random
mapping of unknown words are of less than 0.1% .
</footnote>
<note confidence="0.591918">
0.4
</note>
<page confidence="0.907734">
1304
</page>
<table confidence="0.99965925">
Excluding Punctuation Including Punctuation Perfect Punctuation
M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V
Van Gael 59.1 48.4 0.999 0.530 62.3 51.3 0.861 0.591 64.0 54.6 0.820 0.610
Prototype 67.5 58.8 0.809 0.608 71.6 63.5 0.663 0.677 71.6 63.9 0.659 0.679
</table>
<tableCaption confidence="0.965784666666667">
Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
ment schemes. Left section: punctuation tokens are excluded. Middle section: punctuation tokens are included. Right section:
perfect assignment of punctuation is assumed.
</tableCaption>
<bodyText confidence="0.999763208333333">
based measures and 3.5% in the V measure.
Finally, Table 4 presents reported results for all
recent algorithms we are aware of that tackled the
task of unsupervised POS induction from plain
text. Results for our algorithm’s and Clark’s are
reported for the ‘Fine, k=34’ scenario. The set-
tings of the various experiments vary in terms of
the exact annotation scheme used (coarse or fine
grained) and the size of the test set. However, the
score differences are sufficiently large to justify
the claim that our algorithm is currently the best
performing algorithm on the PTB-WSJ corpus for
POS induction from plain text4.
Since previous works provided results only for
the scenario in which punctuation is included, the
reported results are not directly comparable. In
order to quantify the effect various punctuation
schemes have on the results, we evaluated the
‘iHMM: PY-fixed’ model (Van Gael et al., 2009)
and ours when punctuation is excluded, included
or perfectly tagged5. The results (Table 3) indi-
cate that most probably even after an appropriate
correction for punctuation, our model remains the
best performing one.
</bodyText>
<sectionHeader confidence="0.996108" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999809384615384">
In this work we presented a novel unsupervised al-
gorithm for POS induction from plain text. The al-
gorithm first generates relatively accurate clusters
of high frequency words, which are subsequently
used to bootstrap the entire clustering. The dis-
tributional and morphological representations that
we use are novel for this task.
We experimented on two languages with map-
ping and information theoretic clustering evalua-
tion measures. Our algorithm obtains the best re-
ported results on the English PTB-WSJ corpus. In
addition, our results are almost always better than
Clark’s on the German NEGRA corpus.
</bodyText>
<footnote confidence="0.9652754">
4Grac¸a et al. (2009) report very good results for 17 tags in
the M-1 measure. However, their 1-1 results are quite poor,
and results for the common IT measures were not reported.
Their results for 45 tags are considerably lower.
5We thank the authors for sending us their data.
</footnote>
<bodyText confidence="0.999939666666667">
We have also performed a manual error anal-
ysis, which showed that our algorithm performs
much better on closed classes than on open
classes. In order to asses this quantitatively, let
us define a random variable for each of the gold
clusters, which receives a value corresponding to
each induced cluster with probability proportional
to their intersection size. For each gold cluster,
we compute the entropy of this variable. In ad-
dition, we greedily map each induced cluster to a
gold cluster and compute the ratio between their
intersection size and the size of the gold cluster
(mapping accuracy).
We experimented in the ‘Fine k=34’ scenario.
The clusters that obtained the best scores were
(brackets indicate mapping accuracy and entropy
for each of these clusters) coordinating conjunc-
tions (95%, 0.32), prepositions (94%, 0.32), de-
terminers (94%, 0.44) and modals (93%, 0.45).
These are all closed classes.
The classes on which our algorithm performed
worst consist of open classes, mostly verb types:
past tense verbs (47%, 2.2), past participle verbs
(44%, 2.32) and the morphologically unmarked
non-3rd person singular present verbs (32%, 2.86).
Another class with low performance is the proper
nouns (37%, 2.9). The errors there are mostly
of three types: confusions between common and
proper nouns (sometimes due to ambiguity), un-
known words which were put in the unknown
words cluster, and abbreviations which were given
a separate class by our algorithm. Finally, the al-
gorithm’s performance on the heterogeneous ad-
verbs class (19%, 3.73) is the lowest.
Clark’s algorithm exhibits6 a similar pattern
with respect to open and closed classes. While
his algorithm performs considerably better on ad-
verbs (15% mapping accuracy difference and 0.71
entropy difference), our algorithm scores consid-
erably better on prepositions (17%, 0.77), su-
perlative adjectives (38%, 1.37) and plural proper
names (45%, 1.26).
</bodyText>
<footnote confidence="0.998194">
6Using average mapping accuracy and entropy over the
100 runs.
</footnote>
<page confidence="0.992812">
1305
</page>
<bodyText confidence="0.999807666666667">
Naturally, this analysis might reflect the arbi-
trary nature of a manually design POS tag set
rather than deficiencies in automatic POS induc-
tion algorithms. In future work we intend to ana-
lyze the output of such algorithms in order to im-
prove POS tag sets.
Our algorithm and Clark’s are monosemous
(i.e., they assign each word exactly one tag), while
most other algorithms are polysemous. In order to
assess the performance loss caused by the monose-
mous nature of our algorithm, we took the M-1
greedy mapping computed for the entire dataset
and used it to compute accuracy over the monose-
mous and polysemous words separately. Results
are reported for the English ‘Fine k=34’ scenario
(without punctuation). We define a word to be
monosemous if more than 95% of its tokens are
assigned the same gold standard tag. For English,
there are approximately 255K polysemous tokens
and 578K monosemous ones. As expected, our
algorithm is much more accurate on the monose-
mous tokens, achieving 76.6% accuracy, com-
pared to 47.1% on the polysemous tokens.
The evaluation in this paper is done at the token
level. Type level evaluation, reflecting the algo-
rithm’s ability to detect the set of possible POS
tags for each word type, is important as well. It
could be expected that a monosemous algorithm
such as ours would perform poorly in a type level
evaluation. In (Reichart et al., 2010a) we discuss
type level evaluation at depth and propose type
level evaluation measures applicable to the POS
induction problem. In that paper we compare the
performance of our Prototype Tagger with lead-
ing unsupervised POS tagging algorithms (Clark,
2003; Goldwater and Griffiths, 2007; Gao and
Johnson, 2008; Van Gael et al., 2009). Our al-
gorithm obtained the best results in 4 of the 6
measures in a margin of 4–6%, and was second
best in the other two measures. Our results were
better than Clark’s (the only other monosemous
algorithm evaluated there) on all measures in a
margin of 5–21%. The fact that our monose-
mous algorithm was better than good polysemous
algorithms in a type level evaluation can be ex-
plained by the prototypical nature of the POS phe-
nomenon (a longer discussion is given in (Reichart
et al., 2010a)). However, the quality upper bound
for monosemous algorithms is obviously much
lower than that for polysemous algorithms, and
we expect polysemous algorithms to outperform
monosemous algorithms in the future in both type
level and token level evaluations.
The skewed (Zipfian) distribution of POS class
frequencies in corpora is a problem for many POS
induction algorithms, which by default tend to in-
duce a clustering having a balanced distribution.
Explicit modifications to these algorithms were in-
troduced in order to bias their model to produce
such a distribution (see (Clark, 2003; Johnson,
2007; Reichart et al., 2010b)). An appealing prop-
erty of our model is its ability to induce a skewed
distribution without being explicitly tuned to do
so, as seen in Figure 2.
Acknowledgements. We would like to thank
Yoav Seginer for his help with his parser.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999174764705882">
Michele Banko and Robert C. Moore, 2004. Part of
Speech Tagging in Context. COLING ’04.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi and
Eros Zanchetta, 2009. The WaCky Wide Web: A
Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation.
Chris Biemann, 2006. Unsupervised Part-of-
Speech Tagging Employing Efficient Graph Cluster-
ing. COLING-ACL ’06 Student Research Work-
shop.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de
Souze, Jenifer C. Lai and Robert Mercer, 1992.
Class-Based N-Gram Models of Natural Language.
Computational Linguistics, 18(4):467–479.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ’03.
Mathias Creutz and Krista Lagus, 2005. Inducing the
Morphological Lexicon of a Natural Language from
Unannotated Text. AKRR ’05.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP-CoNLL ’07.
Dayne Freitag, 2004. Toward Unsupervised Whole-
Corpus Tagging. COLING ’04.
Jianfeng Gao and Mark Johnson, 2008. A Compar-
ison of Bayesian Estimators for Unsupervised Hid-
den Markov Model POS Taggers. EMNLP ’08.
Yoav Goldberg, Meni Adler and Michael Elhadad,
2008. EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). ACL ’08.
</reference>
<page confidence="0.884674">
1306
</page>
<reference confidence="0.968757704225352">
John Goldsmith, 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2):153–198.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. EMNLP ’07.
Sharon Goldwater and Tom Grif�ths, 2007. Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. ACL ’07.
Joao Grac¸a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ’09.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Learningfor Sequence Labeling. HLT–NAACL ’06.
Anil K. Jain, Narasimha M. Murty and Patrick J. Flynn,
1999. Data Clustering: A Review. ACM Computing
Surveys 31(3):264–323.
Wenbin Jiang, Liang Huang and Qun Liu, 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging – A Case
Study. ACL ’09.
Mark Johnson, 2007. Why Doesnt EM Find Good
HMMPOS-Taggers? EMNLP-CoNLL ’07.
Harold W. Kuhn, 1955. The Hungarian method for
the Assignment Problem. Naval Research Logistics
Quarterly, 2:83-97.
Marina Meila, 2007. Comparing Clustering – an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873–895.
Bernard Merialdo, 1994. Tagging English Text with
a Probabilistic Model. Computational Linguistics,
20(2):155–172.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32–38.
Sujith Ravi and Kevin Knight, 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. ACL
’09.
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction ofLabeled Parse Trees by Clustering with
Syntactic Features. COLING ’08.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ’09.
Roi Reichart, Omri Abend and Ari Rappoport, 2010a.
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study. CoNLL ’10.
Roi Reichart, Raanan Fattal and Ari Rappoport, 2010b.
Improved Unsupervised POS Induction Using In-
trinsic Clustering Quality and a Zipfian Constraint.
CoNLL ’10.
Hinrich Sch¨utze, 1995. Distributional part-of-speech
tagging. EACL ’95.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ’07.
Noah A. Smith and Jason Eisner, 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. ACL ’05.
John R. Taylor, 2003. Linguistic Categorization: Pro-
totypes in Linguistic Theory, Third Edition. Oxford
University Press.
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ’09.
Qin Iris Wang and Dale Schuurmans, 2005. Im-
proved Estimation for Unsupervised Part-of-Speech
Tagging. IEEE NLP–KE ’05.
Qiuye Zhao and Mitch Marcus, 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. EMNLP ’09.
</reference>
<page confidence="0.99351">
1307
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956362">
<title confidence="0.999337">Improved Unsupervised POS Induction through Prototype Discovery</title>
<author confidence="0.999989">Roi Ari</author>
<affiliation confidence="0.999115">of Computer Science, Hebrew University of Jerusalem</affiliation>
<abstract confidence="0.995880714285714">We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of proto- The algorithm first identifies landof words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Robert C Moore</author>
</authors>
<title>Part of Speech Tagging in Context.</title>
<date>2004</date>
<journal>COLING</journal>
<volume>04</volume>
<contexts>
<context position="3341" citStr="Banko and Moore, 2004" startWordPosition="509" endWordPosition="512">esults (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small numb</context>
</contexts>
<marker>Banko, Moore, 2004</marker>
<rawString>Michele Banko and Robert C. Moore, 2004. Part of Speech Tagging in Context. COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Adriano Ferraresi and Eros Zanchetta,</title>
<date>2009</date>
<marker>Baroni, Bernardini, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi and Eros Zanchetta, 2009. The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised Part-ofSpeech Tagging Employing Efficient Graph Clustering.</title>
<date>2006</date>
<journal>COLING-ACL ’06 Student Research Workshop.</journal>
<contexts>
<context position="4239" citStr="Biemann (2006)" startWordPosition="656" endWordPosition="657">omputational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tag</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann, 2006. Unsupervised Part-ofSpeech Tagging Employing Efficient Graph Clustering. COLING-ACL ’06 Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>The NEGRA Export Format.</title>
<date>1997</date>
<tech>CLAUS Report,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="24699" citStr="Brants, 1997" startWordPosition="4165" endWordPosition="4166">rio). The POS cluster frequency distribution tends to be skewed: each of the 13 most frequent clusters in the PTB-WSJ cover more than 2.5% of the tokens (excluding punctuation) and together 86.3% of them. We therefore chose k=13, since it is both the number of coarse POS tags (excluding punctuation) as well as the number of frequent POS tags in the PTB-WSJ annotation scheme. We chose k=34 in order to evaluate against the full 34 tags PTB-WSJ annotation scheme (excluding punctuation) using the same number of clusters. For German, we trained our model on the 20296 sentences of the NEGRA corpus (Brants, 1997) and on the first 450K sentences of the DeWAC corpus (Baroni et al., 2009). DeWAC is a corpus extracted by web crawling and is therefore out of domain. We report results on the NEGRA part, which includes 346320 word tokens of 49402 types. Of the tokens, 289268 (83.5%) are not punctuation. The percentage of unknown words (those appearing less than five times) is 8.1%. There are 62 clusters in this annotation scheme, 51 of which are not punctuation. We ran the algorithms with k=17 and k=26. k=26 was chosen since it is the number of clusters that cover each more than 0.5% of the NEGRA tokens, and</context>
</contexts>
<marker>Brants, 1997</marker>
<rawString>Thorsten Brants, 1997. The NEGRA Export Format. CLAUS Report, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V de Souze</author>
<author>Jenifer C Lai</author>
<author>Robert Mercer</author>
</authors>
<title>Class-Based N-Gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Brown, Pietra, de Souze, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souze, Jenifer C. Lai and Robert Mercer, 1992. Class-Based N-Gram Models of Natural Language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining Distributional and Morphological Information for Part of Speech Induction.</title>
<date>2003</date>
<journal>EACL</journal>
<volume>03</volume>
<contexts>
<context position="2819" citStr="Clark, 2003" startWordPosition="427" endWordPosition="428">nd German, using four different mapping-based and information theoretic clustering evaluation measures. The results obtained are generally better than all existing POS induction algorithms. Section 2 reviews related work. Sections 3 and 4 detail the algorithm. Sections 5, 6 and 7 describe the evaluation, experimental setup and results. 2 Related Work Unsupervised and semi-supervised POS tagging have been tackled using a variety of methods. Sch¨utze (1995) applied latent semantic analysis. The best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005</context>
<context position="5293" citStr="Clark (2003)" startWordPosition="833" endWordPosition="834">rds. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology to some extent. Some use simplistic representations of terminal letter sequences (e.g., (Smith and Eisner, 2005; Haghighi and Klein, 2006)). Clark (2003) models the entire letter sequence as an HMM and uses it to define a morphological prior. Dasgupta and Ng (2007) use the output of the Morfessor segmentation algorithm for their morphological representation. Morfessor (Creutz and Lagus, 2005), which we use here as well, is an unsupervised algorithm that segments words and classifies each segment as being a stem or an affix. It has been tested on several languages with strong results. Our work has several unique aspects. First, our clustering method discovers prototypes in a fully unsupervised manner, mapping the rest of the words according to </context>
<context position="20311" citStr="Clark, 2003" startWordPosition="3390" endWordPosition="3391">n two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high frequency threshold), n = 50 (the parameter that determines the effective number of coordinates), α = 0.25 (cluster separation during landmark cluster generation), 0 = 0.9 (cluster separation during refinement of morphological clusters). The algorithm we compare with in most detail is (Clark, 2003), which reports the best current results for this problem (see Section 7). Since Clark’s algorithm is sensitive to its initialization, we ran it a 100 times and report its average and standard deviation in each of the four measures. In addition, we report the percentile in which our result falls with respect to these 100 runs. Punctuation marks are very frequent in corpora and are easy to cluster. As a result, including them in the evaluation greatly inflates the scores. For this reason we do not assign a cluster to punctuation marks and we report results using this policy, which we recommend </context>
<context position="36875" citStr="Clark, 2003" startWordPosition="6168" endWordPosition="6169"> the polysemous tokens. The evaluation in this paper is done at the token level. Type level evaluation, reflecting the algorithm’s ability to detect the set of possible POS tags for each word type, is important as well. It could be expected that a monosemous algorithm such as ours would perform poorly in a type level evaluation. In (Reichart et al., 2010a) we discuss type level evaluation at depth and propose type level evaluation measures applicable to the POS induction problem. In that paper we compare the performance of our Prototype Tagger with leading unsupervised POS tagging algorithms (Clark, 2003; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Van Gael et al., 2009). Our algorithm obtained the best results in 4 of the 6 measures in a margin of 4–6%, and was second best in the other two measures. Our results were better than Clark’s (the only other monosemous algorithm evaluated there) on all measures in a margin of 5–21%. The fact that our monosemous algorithm was better than good polysemous algorithms in a type level evaluation can be explained by the prototypical nature of the POS phenomenon (a longer discussion is given in (Reichart et al., 2010a)). However, the quality uppe</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark, 2003. Combining Distributional and Morphological Information for Part of Speech Induction. EACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Inducing the Morphological Lexicon of a Natural Language from Unannotated Text.</title>
<date>2005</date>
<journal>AKRR</journal>
<volume>05</volume>
<contexts>
<context position="5535" citStr="Creutz and Lagus, 2005" startWordPosition="870" endWordPosition="873">nnotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology to some extent. Some use simplistic representations of terminal letter sequences (e.g., (Smith and Eisner, 2005; Haghighi and Klein, 2006)). Clark (2003) models the entire letter sequence as an HMM and uses it to define a morphological prior. Dasgupta and Ng (2007) use the output of the Morfessor segmentation algorithm for their morphological representation. Morfessor (Creutz and Lagus, 2005), which we use here as well, is an unsupervised algorithm that segments words and classifies each segment as being a stem or an affix. It has been tested on several languages with strong results. Our work has several unique aspects. First, our clustering method discovers prototypes in a fully unsupervised manner, mapping the rest of the words according to their association with the prototypes. Second, we use a distributional representation which has been shown to be effective for unsupervised parsing (Seginer, 2007). Third, we use a morphological representation based on signatures, which are s</context>
<context position="14242" citStr="Creutz and Lagus, 2005" startWordPosition="2405" endWordPosition="2408">e known words already assigned to that cluster. However, when the number k of landmark clusters is relatively large, it is beneficial to assign all unknown words to a separate new cluster (after running the algorithm with k − 1). In our experiments, we use the first option when k is below some threshold (we used 15), otherwise we use the second. 4 Morphological Model The morphological model generates another word clustering, based on the notion of a signature. This clustering is integrated with the distributional model as described below. 4.1 Morphological Representation We use the Morfessor (Creutz and Lagus, 2005) word segmentation algorithm. First, all words in the corpus are segmented. Then, for each stem, the set of all affixes with which it appears (its signature, (Goldsmith, 2001)) is collected. The morphological representation of a word type is then defined to be its stem’s signature in conjunction with its specific affixes2 (See Figure 1). We now collect all words having the same representation. For instance, if the words joined and painted are found to have the same signature, they would share the same cluster since both have the affix ‘ ed’. The wordjoins does not share the same cluster with t</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus, 2005. Inducing the Morphological Lexicon of a Natural Language from Unannotated Text. AKRR ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised Part-of-Speech Acquisition for ResourceScarce Languages. EMNLP-CoNLL ’07.</title>
<date>2007</date>
<contexts>
<context position="4325" citStr="Dasgupta and Ng (2007)" startWordPosition="671" endWordPosition="674">Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation eff</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng, 2007. Unsupervised Part-of-Speech Acquisition for ResourceScarce Languages. EMNLP-CoNLL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Toward Unsupervised WholeCorpus Tagging.</title>
<date>2004</date>
<journal>COLING</journal>
<volume>04</volume>
<contexts>
<context position="4220" citStr="Freitag (2004)" startWordPosition="652" endWordPosition="654">0 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary prov</context>
</contexts>
<marker>Freitag, 2004</marker>
<rawString>Dayne Freitag, 2004. Toward Unsupervised WholeCorpus Tagging. COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A Comparison of Bayesian Estimators for Unsupervised Hidden Markov Model POS Taggers.</title>
<date>2008</date>
<journal>EMNLP</journal>
<volume>08</volume>
<contexts>
<context position="3806" citStr="Gao and Johnson, 2008" startWordPosition="575" endWordPosition="578">formance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model</context>
<context position="27294" citStr="Gao and Johnson, 2008" startWordPosition="4624" endWordPosition="4627">.00 Clark 69.8 54.5 0.659 2.18 HK – 41.3 – – J 43–62 37–47 – 4.23–5.74 GG – – – 2.8 GJ – 40–49.9 – 4.03–4.47 VG – – 0.54-0.59 2.5–2.9 GGTP-45 65.4 44.5 – – GGTP-17 70.2 49.5 – – Table 4: Comparison of our algorithms with the recent fully unsupervised POS taggers for which results are reported. The models differ in the annotation scheme, the corpus size and the number of induced clusters (k) that they used. HK: (Haghighi and Klein, 2006), 193K tokens, fine tags, k=45. GG: (Goldwater and Griffiths, 2007), 24K tokens, coarse tags, k=17. J : (Johnson, 2007), 1.17M tokens, fine tags, k=25–50. GJ: (Gao and Johnson, 2008), 1.17M tokens, fine tags, k=50. VG: (Van Gael et al., 2009), 1.17M tokens, fine tags, k=47–192. GGTP-45: (Grac¸a et al., 2009), 1.17M tokens, fine tags, k=45. GGTP-17: (Grac¸a et al., 2009), 1.17M tokens, coarse tags, k=17. Lower VI values indicate better clustering. VI is computed using a as the base of the logarithm. Our algorithm gives the best results. k=13’ scenario) that quantify the contribution of each component of the algorithm. We ran the base distributional algorithm, a variant which uses only capitalization information (i.e., has only one nonsingleton morphological class, that of </context>
<context position="36929" citStr="Gao and Johnson, 2008" startWordPosition="6174" endWordPosition="6177">is paper is done at the token level. Type level evaluation, reflecting the algorithm’s ability to detect the set of possible POS tags for each word type, is important as well. It could be expected that a monosemous algorithm such as ours would perform poorly in a type level evaluation. In (Reichart et al., 2010a) we discuss type level evaluation at depth and propose type level evaluation measures applicable to the POS induction problem. In that paper we compare the performance of our Prototype Tagger with leading unsupervised POS tagging algorithms (Clark, 2003; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Van Gael et al., 2009). Our algorithm obtained the best results in 4 of the 6 measures in a margin of 4–6%, and was second best in the other two measures. Our results were better than Clark’s (the only other monosemous algorithm evaluated there) on all measures in a margin of 5–21%. The fact that our monosemous algorithm was better than good polysemous algorithms in a type level evaluation can be explained by the prototypical nature of the POS phenomenon (a longer discussion is given in (Reichart et al., 2010a)). However, the quality upper bound for monosemous algorithms is obviously much lo</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson, 2008. A Comparison of Bayesian Estimators for Unsupervised Hidden Markov Model POS Taggers. EMNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<date>2008</date>
<journal>EM Can Find Pretty Good HMM POSTaggers (When Given a Good Start). ACL</journal>
<volume>08</volume>
<contexts>
<context position="4430" citStr="Goldberg et al. (2008)" startWordPosition="686" endWordPosition="689">Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized i</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler and Michael Elhadad, 2008. EM Can Find Pretty Good HMM POSTaggers (When Given a Good Start). ACL ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised Learning of the Morphology of a Natural Language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="6251" citStr="Goldsmith, 2001" startWordPosition="985" endWordPosition="986">gment as being a stem or an affix. It has been tested on several languages with strong results. Our work has several unique aspects. First, our clustering method discovers prototypes in a fully unsupervised manner, mapping the rest of the words according to their association with the prototypes. Second, we use a distributional representation which has been shown to be effective for unsupervised parsing (Seginer, 2007). Third, we use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology (Goldsmith, 2001). 3 Distributional Algorithm Our algorithm is given a plain text corpus and optionally a desired number of clusters k. Its output is a partitioning of words into clusters. The algorithm utilizes two representations, distributional and morphological. Although eventually the latter is used before the former, for clarity of presentation we begin by detailing the base distributional algorithm. In the next section we describe the morphological representation and its integration into the base algorithm. Overview. The algorithm consists of two main stages: landmark clusters discovery, and word mappin</context>
<context position="14417" citStr="Goldsmith, 2001" startWordPosition="2436" endWordPosition="2437">uster (after running the algorithm with k − 1). In our experiments, we use the first option when k is below some threshold (we used 15), otherwise we use the second. 4 Morphological Model The morphological model generates another word clustering, based on the notion of a signature. This clustering is integrated with the distributional model as described below. 4.1 Morphological Representation We use the Morfessor (Creutz and Lagus, 2005) word segmentation algorithm. First, all words in the corpus are segmented. Then, for each stem, the set of all affixes with which it appears (its signature, (Goldsmith, 2001)) is collected. The morphological representation of a word type is then defined to be its stem’s signature in conjunction with its specific affixes2 (See Figure 1). We now collect all words having the same representation. For instance, if the words joined and painted are found to have the same signature, they would share the same cluster since both have the affix ‘ ed’. The wordjoins does not share the same cluster with them since it has a different affix, ‘ s’. This results in coarse-grained clusters exclusively defined according to morphology. 2A word may contain more than a single affix. Ty</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith, 2001. Unsupervised Learning of the Morphology of a Natural Language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>VMeasure: A Conditional Entropy-Based External Cluster Evaluation Measure.</title>
<date>2007</date>
<journal>EMNLP</journal>
<volume>07</volume>
<contexts>
<context position="19411" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="3243" endWordPosition="3246"> accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg, 2007. VMeasure: A Conditional Entropy-Based External Cluster Evaluation Measure. EMNLP ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Grif�ths</author>
</authors>
<title>Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging.</title>
<date>2007</date>
<journal>ACL</journal>
<volume>07</volume>
<marker>Goldwater, Grif�ths, 2007</marker>
<rawString>Sharon Goldwater and Tom Grif�ths, 2007. Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging. ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Grac¸a</author>
</authors>
<title>Kuzman Ganchev, Ben Taskar and Frenando Pereira,</title>
<date>2009</date>
<journal>NIPS</journal>
<volume>09</volume>
<marker>Grac¸a, 2009</marker>
<rawString>Joao Grac¸a, Kuzman Ganchev, Ben Taskar and Frenando Pereira, 2009. Posterior vs. Parameter Sparsity in Latent Variable Models. NIPS ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>North American News Text Corpus. Linguistic Data Consortium.</title>
<date>1995</date>
<tech>LDC95T21.</tech>
<contexts>
<context position="21613" citStr="Graff, 1995" startWordPosition="3610" endWordPosition="3611">t results for the full POS tag set. We do so by assigning a singleton cluster to each punctuation mark (in addition to the k required clusters). This simple heuristic yields very high performance on punctuation, scoring (when all other words are assumed perfect tagging) 99.6% (99.1%) 1-to-1 accuracy when evaluated against the English fine (coarse) POS tag sets, and 97.2% when evaluated against the German POS tag set. For English, we trained our model on the 39832 sentences which constitute sections 2-21 of the PTB-WSJ and on the 500K sentences from the NYT section of the NANC newswire corpus (Graff, 1995). We report results on the WSJ part of our data, which includes 950028 words tokens in 44389 types. Of the tokens, 832629 (87.6%) 1302 English Fine k=13 Coarse k=13 Fine k=34 Prototype Clark Prototype Clark Prototype Clark Tagger µ v % Tagger µ v % Tagger µ v % Many–to–1 61.0 55.1 1.6 100 70.0 66.9 2.1 94 71.6 69.8 1.5 90 55.5 48.8 1.8 100 66.1 62.6 2.3 94 67.5 65.5 1.7 90 1–to–1 60.0 52.2 1.9 100 58.1 49.4 2.9 100 63.5 54.5 1.6 100 54.9 46.0 2.2 100 53.7 43.8 3.3 100 58.8 48.5 1.8 100 NVI 0.652 0.773 0.027 100 0.841 0.972 0.036 100 0.663 0.725 0.018 100 0.795 0.943 0.033 100 1.052 1.221 0.046</context>
</contexts>
<marker>Graff, 1995</marker>
<rawString>David Graff, 1995. North American News Text Corpus. Linguistic Data Consortium. LDC95T21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<date>2006</date>
<booktitle>Prototype-driven Learningfor Sequence Labeling. HLT–NAACL ’06.</booktitle>
<contexts>
<context position="3446" citStr="Haghighi and Klein, 2006" startWordPosition="525" endWordPosition="528">which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data h</context>
<context position="5278" citStr="Haghighi and Klein, 2006" startWordPosition="829" endWordPosition="832"> and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology to some extent. Some use simplistic representations of terminal letter sequences (e.g., (Smith and Eisner, 2005; Haghighi and Klein, 2006)). Clark (2003) models the entire letter sequence as an HMM and uses it to define a morphological prior. Dasgupta and Ng (2007) use the output of the Morfessor segmentation algorithm for their morphological representation. Morfessor (Creutz and Lagus, 2005), which we use here as well, is an unsupervised algorithm that segments words and classifies each segment as being a stem or an affix. It has been tested on several languages with strong results. Our work has several unique aspects. First, our clustering method discovers prototypes in a fully unsupervised manner, mapping the rest of the word</context>
<context position="27112" citStr="Haghighi and Klein, 2006" startWordPosition="4595" endWordPosition="4598">lization to B, F(I=1) consists of all components, where only one iteration of coordinate clustering is performed, and F is the full model. M-to-1 1-to-1 V VI Prototype 71.6 63.5 0.677 2.00 Clark 69.8 54.5 0.659 2.18 HK – 41.3 – – J 43–62 37–47 – 4.23–5.74 GG – – – 2.8 GJ – 40–49.9 – 4.03–4.47 VG – – 0.54-0.59 2.5–2.9 GGTP-45 65.4 44.5 – – GGTP-17 70.2 49.5 – – Table 4: Comparison of our algorithms with the recent fully unsupervised POS taggers for which results are reported. The models differ in the annotation scheme, the corpus size and the number of induced clusters (k) that they used. HK: (Haghighi and Klein, 2006), 193K tokens, fine tags, k=45. GG: (Goldwater and Griffiths, 2007), 24K tokens, coarse tags, k=17. J : (Johnson, 2007), 1.17M tokens, fine tags, k=25–50. GJ: (Gao and Johnson, 2008), 1.17M tokens, fine tags, k=50. VG: (Van Gael et al., 2009), 1.17M tokens, fine tags, k=47–192. GGTP-45: (Grac¸a et al., 2009), 1.17M tokens, fine tags, k=45. GGTP-17: (Grac¸a et al., 2009), 1.17M tokens, coarse tags, k=17. Lower VI values indicate better clustering. VI is computed using a as the base of the logarithm. Our algorithm gives the best results. k=13’ scenario) that quantify the contribution of each com</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein, 2006. Prototype-driven Learningfor Sequence Labeling. HLT–NAACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil K Jain</author>
<author>Narasimha M Murty</author>
<author>Patrick J Flynn</author>
</authors>
<title>Data Clustering: A Review.</title>
<date>1999</date>
<journal>ACM Computing Surveys</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="11173" citStr="Jain et al., 1999" startWordPosition="1880" endWordPosition="1883">e of the angle between their representation vectors. Coordinate clustering. Each of our landmark clusters will correspond to a set of high frequency words (HFWs). The number of HFWs is much larger than the number of expected POS tags. Hence we should cluster HFWs. Our algorithm does that by unifying some of the non-zero coordinates corresponding to HFWs in the distributional representation defined above. We extract the words that appear more than N times per million1 and apply the following procedure I times (5 in our experiments). We run average link clustering with a threshold α (AVGLINKα, (Jain et al., 1999)) on these words, in each iteration initializing every HFW to have its own cluster. AVGLINKα means running the average link algorithm until the two closest clusters have a distance larger than α. We then use the induced clustering to update the distributional representation, by collapsing all coordinates corresponding to words appearing in the same cluster into a single coordinate whose value is the sum of the collapsed coordinates’ values. In order to produce a conservative (fine) clustering, we used a relatively low α value of 0.25. Note that the AVGLINKα initialization in each of the I iter</context>
</contexts>
<marker>Jain, Murty, Flynn, 1999</marker>
<rawString>Anil K. Jain, Narasimha M. Murty and Patrick J. Flynn, 1999. Data Clustering: A Review. ACM Computing Surveys 31(3):264–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging – A Case Study.</title>
<date>2009</date>
<journal>ACL</journal>
<volume>09</volume>
<contexts>
<context position="1096" citStr="Jiang et al., 2009" startWordPosition="158" endWordPosition="161"> words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task. 1 Introduction Part-of-speech (POS) tagging is a fundamental NLP task, used by a wide variety of applications. However, there is no single standard POS tagging scheme, even for English. Schemes vary significantly across corpora and even more so across languages, creating difficulties in using POS tags across domains and for multi-lingual systems (Jiang et al., 2009). Automatic induction of POS tags from plain text can greatly alleviate this problem, as well as eliminate the efforts incurred by manual annotations. It is also a problem of great theoretical interest. Consequently, POS induction is a vibrant research area (see Section 2). In this paper we present an algorithm based on the theory of prototypes (Taylor, 2003), which posits that some members in cognitive categories are more central than others. These practically define the category, while the membership of other elements is based on their association with the * Omri Abend is grateful to the Azr</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang and Qun Liu, 2009. Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging – A Case Study. ACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why Doesnt EM Find Good HMMPOS-Taggers? EMNLP-CoNLL ’07.</title>
<date>2007</date>
<contexts>
<context position="3782" citStr="Johnson, 2007" startWordPosition="573" endWordPosition="574">lark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstra</context>
<context position="27231" citStr="Johnson, 2007" startWordPosition="4616" endWordPosition="4617">l model. M-to-1 1-to-1 V VI Prototype 71.6 63.5 0.677 2.00 Clark 69.8 54.5 0.659 2.18 HK – 41.3 – – J 43–62 37–47 – 4.23–5.74 GG – – – 2.8 GJ – 40–49.9 – 4.03–4.47 VG – – 0.54-0.59 2.5–2.9 GGTP-45 65.4 44.5 – – GGTP-17 70.2 49.5 – – Table 4: Comparison of our algorithms with the recent fully unsupervised POS taggers for which results are reported. The models differ in the annotation scheme, the corpus size and the number of induced clusters (k) that they used. HK: (Haghighi and Klein, 2006), 193K tokens, fine tags, k=45. GG: (Goldwater and Griffiths, 2007), 24K tokens, coarse tags, k=17. J : (Johnson, 2007), 1.17M tokens, fine tags, k=25–50. GJ: (Gao and Johnson, 2008), 1.17M tokens, fine tags, k=50. VG: (Van Gael et al., 2009), 1.17M tokens, fine tags, k=47–192. GGTP-45: (Grac¸a et al., 2009), 1.17M tokens, fine tags, k=45. GGTP-17: (Grac¸a et al., 2009), 1.17M tokens, coarse tags, k=17. Lower VI values indicate better clustering. VI is computed using a as the base of the logarithm. Our algorithm gives the best results. k=13’ scenario) that quantify the contribution of each component of the algorithm. We ran the base distributional algorithm, a variant which uses only capitalization information</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson, 2007. Why Doesnt EM Find Good HMMPOS-Taggers? EMNLP-CoNLL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian method for the Assignment Problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<pages>2--83</pages>
<contexts>
<context position="19128" citStr="Kuhn, 1955" startWordPosition="3199" endWordPosition="3200">en the gold standard clusters and the induced clusters which maximizes accuracy, allowing several induced clusters to be mapped to the same gold standard cluster. The 1-to-1 measure finds the mapping between the induced and gold standard clusters which maximizes accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition,</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn, 1955. The Hungarian method for the Assignment Problem. Naval Research Logistics Quarterly, 2:83-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
</authors>
<title>Comparing Clustering – an Information Based Distance.</title>
<date>2007</date>
<journal>Journal of Multivariate Analysis,</journal>
<pages>98--873</pages>
<contexts>
<context position="19470" citStr="Meila, 2007" startWordPosition="3254" endWordPosition="3255">r. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high frequency threshold), n = 50 (the parameter that determine</context>
</contexts>
<marker>Meila, 2007</marker>
<rawString>Marina Meila, 2007. Comparing Clustering – an Information Based Distance. Journal of Multivariate Analysis, 98:873–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English Text with a Probabilistic Model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="3318" citStr="Merialdo, 1994" startWordPosition="507" endWordPosition="508"> best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo, 1994. Tagging English Text with a Probabilistic Model. Computational Linguistics, 20(2):155–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Munkres</author>
</authors>
<title>Algorithms for the Assignment and Transportation Problems.</title>
<date>1957</date>
<journal>Journal of the SIAM,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="19144" citStr="Munkres, 1957" startWordPosition="3201" endWordPosition="3202">standard clusters and the induced clusters which maximizes accuracy, allowing several induced clusters to be mapped to the same gold standard cluster. The 1-to-1 measure finds the mapping between the induced and gold standard clusters which maximizes accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English</context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>James Munkres, 1957. Algorithms for the Assignment and Transportation Problems. Journal of the SIAM, 5(1):32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized Models for Unsupervised Part-of-Speech Tagging.</title>
<date>2009</date>
<journal>ACL</journal>
<volume>09</volume>
<contexts>
<context position="4708" citStr="Ravi and Knight (2009)" startWordPosition="736" endWordPosition="739"> and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology to some extent. Some use simplistic representations of terminal letter sequences (e.g., (Smith and Eisner, 2005; Haghighi and Klein, 2006)). Clark (2003) models the ent</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight, 2009. Minimized Models for Unsupervised Part-of-Speech Tagging. ACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised Induction ofLabeled Parse Trees by Clustering with Syntactic Features.</title>
<date>2008</date>
<journal>COLING</journal>
<volume>08</volume>
<contexts>
<context position="19081" citStr="Reichart and Rappoport, 2008" startWordPosition="3190" endWordPosition="3193"> a derived accuracy. The Many-to-1 measure finds the mapping between the gold standard clusters and the induced clusters which maximizes accuracy, allowing several induced clusters to be mapped to the same gold standard cluster. The 1-to-1 measure finds the mapping between the induced and gold standard clusters which maximizes accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three d</context>
</contexts>
<marker>Reichart, Rappoport, 2008</marker>
<rawString>Roi Reichart and Ari Rappoport, 2008. Unsupervised Induction ofLabeled Parse Trees by Clustering with Syntactic Features. COLING ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>The NVI Clustering Evaluation Measure.</title>
<date>2009</date>
<journal>CoNLL</journal>
<volume>09</volume>
<contexts>
<context position="19450" citStr="Reichart and Rappoport, 2009" startWordPosition="3249" endWordPosition="3252">rs are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high frequency threshold), n = 50 (the para</context>
</contexts>
<marker>Reichart, Rappoport, 2009</marker>
<rawString>Roi Reichart and Ari Rappoport, 2009. The NVI Clustering Evaluation Measure. CoNLL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Omri Abend</author>
<author>Ari Rappoport</author>
</authors>
<title>Type Level Clustering Evaluation: New Measures and a POS Induction Case Study.</title>
<date>2010</date>
<journal>CoNLL</journal>
<volume>10</volume>
<contexts>
<context position="3032" citStr="Reichart et al. (2010" startWordPosition="457" endWordPosition="460">ews related work. Sections 3 and 4 detail the algorithm. Sections 5, 6 and 7 describe the evaluation, experimental setup and results. 2 Related Work Unsupervised and semi-supervised POS tagging have been tackled using a variety of methods. Sch¨utze (1995) applied latent semantic analysis. The best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computat</context>
<context position="29100" citStr="Reichart et al., 2010" startWordPosition="4913" endWordPosition="4916">ter, like most algorithms which addressed the task, induces the clustering 1 0.8 0.6 Gold Standard Induced 0.2 0 0 5 10 15 20 25 30 35 40 45 Figure 2: POS class frequency distribution for our model and the gold standard, in the ‘Fine k=34’ scenario. The distributions are similar. by maximizing a non-convex function. These functions have many local maxima and the specific solution to which algorithms that maximize them converge strongly depends on their (random) initialization. Therefore, their output’s quality often significantly diverges from the average. This issue is discussed in depth in (Reichart et al., 2010b). Our algorithm is deterministic3. For German, in the k=26 scenario our algorithm outperforms Clark’s, often outperforming even its maximum in 100 runs. In the k=17 scenario, our algorithm obtains a higher score than Clark with probability 0.4 to 0.78, depending on the measure and scenario. Clark’s average score is slightly better in the Many-to-1 measure, while our algorithm performs somewhat better than Clark’s average in the 1-to-1 and NVI measures. The DeWAC corpus from which we extracted statistics for the German experiments is out of domain with respect to NEGRA. The corresponding corp</context>
<context position="36620" citStr="Reichart et al., 2010" startWordPosition="6127" endWordPosition="6130">5% of its tokens are assigned the same gold standard tag. For English, there are approximately 255K polysemous tokens and 578K monosemous ones. As expected, our algorithm is much more accurate on the monosemous tokens, achieving 76.6% accuracy, compared to 47.1% on the polysemous tokens. The evaluation in this paper is done at the token level. Type level evaluation, reflecting the algorithm’s ability to detect the set of possible POS tags for each word type, is important as well. It could be expected that a monosemous algorithm such as ours would perform poorly in a type level evaluation. In (Reichart et al., 2010a) we discuss type level evaluation at depth and propose type level evaluation measures applicable to the POS induction problem. In that paper we compare the performance of our Prototype Tagger with leading unsupervised POS tagging algorithms (Clark, 2003; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Van Gael et al., 2009). Our algorithm obtained the best results in 4 of the 6 measures in a margin of 4–6%, and was second best in the other two measures. Our results were better than Clark’s (the only other monosemous algorithm evaluated there) on all measures in a margin of 5–21%. The f</context>
</contexts>
<marker>Reichart, Abend, Rappoport, 2010</marker>
<rawString>Roi Reichart, Omri Abend and Ari Rappoport, 2010a. Type Level Clustering Evaluation: New Measures and a POS Induction Case Study. CoNLL ’10.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Roi Reichart</author>
</authors>
<title>Raanan Fattal and Ari Rappoport, 2010b. Improved Unsupervised POS Induction Using Intrinsic Clustering Quality and a Zipfian Constraint.</title>
<journal>CoNLL</journal>
<volume>10</volume>
<marker>Reichart, </marker>
<rawString>Roi Reichart, Raanan Fattal and Ari Rappoport, 2010b. Improved Unsupervised POS Induction Using Intrinsic Clustering Quality and a Zipfian Constraint. CoNLL ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<journal>EACL</journal>
<volume>95</volume>
<marker>Sch¨utze, 1995</marker>
<rawString>Hinrich Sch¨utze, 1995. Distributional part-of-speech tagging. EACL ’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast Unsupervised Incremental Parsing.</title>
<date>2007</date>
<journal>ACL</journal>
<volume>07</volume>
<contexts>
<context position="2161" citStr="Seginer, 2007" startWordPosition="329" endWordPosition="330">se practically define the category, while the membership of other elements is based on their association with the * Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. central members. Our algorithm first clusters words based on a fine morphological representation. It then clusters the most frequent words, defining landmark clusters which constitute the cores of the categories. Finally, it maps the rest of the words to these categories. The last two stages utilize a distributional representation that has been shown to be effective for unsupervised parsing (Seginer, 2007). We evaluated the algorithm in both English and German, using four different mapping-based and information theoretic clustering evaluation measures. The results obtained are generally better than all existing POS induction algorithms. Section 2 reviews related work. Sections 3 and 4 detail the algorithm. Sections 5, 6 and 7 describe the evaluation, experimental setup and results. 2 Related Work Unsupervised and semi-supervised POS tagging have been tackled using a variety of methods. Sch¨utze (1995) applied latent semantic analysis. The best reported results (when taking into account all eval</context>
<context position="6056" citStr="Seginer, 2007" startWordPosition="955" endWordPosition="956">entation algorithm for their morphological representation. Morfessor (Creutz and Lagus, 2005), which we use here as well, is an unsupervised algorithm that segments words and classifies each segment as being a stem or an affix. It has been tested on several languages with strong results. Our work has several unique aspects. First, our clustering method discovers prototypes in a fully unsupervised manner, mapping the rest of the words according to their association with the prototypes. Second, we use a distributional representation which has been shown to be effective for unsupervised parsing (Seginer, 2007). Third, we use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology (Goldsmith, 2001). 3 Distributional Algorithm Our algorithm is given a plain text corpus and optionally a desired number of clusters k. Its output is a partitioning of words into clusters. The algorithm utilizes two representations, distributional and morphological. Although eventually the latter is used before the former, for clarity of presentation we begin by detailing the base distributional algorithm. In the next </context>
<context position="7694" citStr="Seginer, 2007" startWordPosition="1213" endWordPosition="1214"> word to the most similar landmark cluster. The rationale behind using only the high frequency words in the first stage is twofold. First, prototypical members of a category are frequent (Taylor, 2003), and therefore we can expect the salient POS tags to be represented in this small subset. Second, higher frequency implies more reliable statistics. Since this stage determines the cores of all resulting clusters, it should be as accurate as possible. Distributional representation. We use a simplified form of the elegant representation of lexical entries used by the Seginer unsupervised parser (Seginer, 2007). Since a POS tag reflects the grammatical role of the word and since this representation is effective to parsing, we were motivated to apply it to the present task. Let W be the set of word types in the corpus. The right context entry of a word x E W is a pair of mappings r intx : W → [0, 1] and r adjx : W → [0, 1]. For each w E W, r adjx(w) is an adjacency score of w to x, reflecting w’s tendency to appear on the right hand side of x. For each w E W, r intx(w) is an interchangeability score of x with w, reflecting the tendency of w to appear to the left of words that tend to appear to the ri</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer, 2007. Fast Unsupervised Incremental Parsing. ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive Estimation: Training Log-Linear Models on Unlabeled Data.</title>
<date>2005</date>
<journal>ACL</journal>
<volume>05</volume>
<contexts>
<context position="3419" citStr="Smith and Eisner, 2005" startWordPosition="520" endWordPosition="524">given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding t</context>
<context position="5251" citStr="Smith and Eisner, 2005" startWordPosition="825" endWordPosition="828">s only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology to some extent. Some use simplistic representations of terminal letter sequences (e.g., (Smith and Eisner, 2005; Haghighi and Klein, 2006)). Clark (2003) models the entire letter sequence as an HMM and uses it to define a morphological prior. Dasgupta and Ng (2007) use the output of the Morfessor segmentation algorithm for their morphological representation. Morfessor (Creutz and Lagus, 2005), which we use here as well, is an unsupervised algorithm that segments words and classifies each segment as being a stem or an affix. It has been tested on several languages with strong results. Our work has several unique aspects. First, our clustering method discovers prototypes in a fully unsupervised manner, m</context>
<context position="23806" citStr="Smith and Eisner, 2005" startWordPosition="4013" endWordPosition="4016">ry, the top number indicates the score when including punctuation and the bottom number the score when excluding it. In English, our results are always better than Clark’s. In German, they are almost always better. are not punctuation. The percentage of unknown words (those appearing less than five times) is 1.6%. There are 45 clusters in this annotation scheme, 34 of which are not punctuation. We ran each algorithm both with k=13 and k=34 (the number of desired clusters). We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in (Smith and Eisner, 2005). The output of the k=13 run is evaluated both against the coarse POS tag annotation (the ‘Coarse k=13’ scenario) and against the full PTB-WSJ annotation scheme (the ‘Fine k=13’ scenario). The k=34 run is evaluated against the full PTB-WSJ annotation scheme (the ‘Fine k=34’ scenario). The POS cluster frequency distribution tends to be skewed: each of the 13 most frequent clusters in the PTB-WSJ cover more than 2.5% of the tokens (excluding punctuation) and together 86.3% of them. We therefore chose k=13, since it is both the number of coarse POS tags (excluding punctuation) as well as the numb</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner, 2005. Contrastive Estimation: Training Log-Linear Models on Unlabeled Data. ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Taylor</author>
</authors>
<title>Linguistic Categorization: Prototypes in Linguistic Theory, Third Edition.</title>
<date>2003</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1457" citStr="Taylor, 2003" startWordPosition="219" endWordPosition="220">er, there is no single standard POS tagging scheme, even for English. Schemes vary significantly across corpora and even more so across languages, creating difficulties in using POS tags across domains and for multi-lingual systems (Jiang et al., 2009). Automatic induction of POS tags from plain text can greatly alleviate this problem, as well as eliminate the efforts incurred by manual annotations. It is also a problem of great theoretical interest. Consequently, POS induction is a vibrant research area (see Section 2). In this paper we present an algorithm based on the theory of prototypes (Taylor, 2003), which posits that some members in cognitive categories are more central than others. These practically define the category, while the membership of other elements is based on their association with the * Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. central members. Our algorithm first clusters words based on a fine morphological representation. It then clusters the most frequent words, defining landmark clusters which constitute the cores of the categories. Finally, it maps the rest of the words to these categories. The last two stages utilize a di</context>
<context position="7281" citStr="Taylor, 2003" startWordPosition="1147" endWordPosition="1148">escribe the morphological representation and its integration into the base algorithm. Overview. The algorithm consists of two main stages: landmark clusters discovery, and word mapping. For the former, we first compute a distributional representation for each word. We then cluster the coordinates corresponding to high frequency words. Finally, we define landmark clusters. In the word mapping stage we map each word to the most similar landmark cluster. The rationale behind using only the high frequency words in the first stage is twofold. First, prototypical members of a category are frequent (Taylor, 2003), and therefore we can expect the salient POS tags to be represented in this small subset. Second, higher frequency implies more reliable statistics. Since this stage determines the cores of all resulting clusters, it should be as accurate as possible. Distributional representation. We use a simplified form of the elegant representation of lexical entries used by the Seginer unsupervised parser (Seginer, 2007). Since a POS tag reflects the grammatical role of the word and since this representation is effective to parsing, we were motivated to apply it to the present task. Let W be the set of w</context>
</contexts>
<marker>Taylor, 2003</marker>
<rawString>John R. Taylor, 2003. Linguistic Categorization: Prototypes in Linguistic Theory, Third Edition. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<date>2009</date>
<booktitle>The Infinite HMM for Unsupervised POS Tagging. EMNLP ’09.</booktitle>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahramani, 2009. The Infinite HMM for Unsupervised POS Tagging. EMNLP ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Dale Schuurmans</author>
</authors>
<title>Improved Estimation for Unsupervised Part-of-Speech Tagging.</title>
<date>2005</date>
<journal>IEEE NLP–KE</journal>
<volume>05</volume>
<contexts>
<context position="3369" citStr="Wang and Schuurmans, 2005" startWordPosition="513" endWordPosition="516">o account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each</context>
</contexts>
<marker>Wang, Schuurmans, 2005</marker>
<rawString>Qin Iris Wang and Dale Schuurmans, 2005. Improved Estimation for Unsupervised Part-of-Speech Tagging. IEEE NLP–KE ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiuye Zhao</author>
<author>Mitch Marcus</author>
</authors>
<title>A Simple Unsupervised Learner for POS Disambiguation Rules Given Only a Minimal Lexicon.</title>
<date>2009</date>
<journal>EMNLP</journal>
<volume>09</volume>
<contexts>
<context position="4540" citStr="Zhao and Marcus (2009)" startWordPosition="704" endWordPosition="707"> to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology </context>
</contexts>
<marker>Zhao, Marcus, 2009</marker>
<rawString>Qiuye Zhao and Mitch Marcus, 2009. A Simple Unsupervised Learner for POS Disambiguation Rules Given Only a Minimal Lexicon. EMNLP ’09.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>