<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000515">
<title confidence="0.995165">
Automatic Generation of Tamil Lyrics for Melodies
</title>
<author confidence="0.962089">
Ananth Ramakrishnan A Sankar Kuppan Sobha Lalitha Devi
</author>
<affiliation confidence="0.947969111111111">
AU-KBC Research Centre
MIT Campus, Anna University
Chennai, India
AU-KBC Research Centre
MIT Campus, Anna University
Chennai, India
AU-KBC Research Centre
MIT Campus, Anna University
Chennai, India
</affiliation>
<email confidence="0.463566">
ananthrk@au-kbc.org sankar@au­kbc.org sobha@au­kbc.org
</email>
<sectionHeader confidence="0.993372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999487705882353">
This paper presents our on-going work to
automatically generate lyrics for a given
melody, for phonetic languages such as
Tamil. We approach the task of identifying
the required syllable pattern for the lyric as
a sequence labeling problem and hence use
the popular CRF++ toolkit for learning. A
corpus comprising of 10 melodies was used
to train the system to understand the syllable
patterns. The trained model is then used to
guess the syllabic pattern for a new melody
to produce an optimal sequence of syllables.
This sequence is presented to the Sentence
Generation module which uses the Dijkstra&apos;s
shortest path algorithm to come up with a
meaningful phrase matching the syllabic
pattern.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962111111111">
In an attempt to define poetry (Manurung, 2004),
provides three properties for a natural language arti-
fact to be considered a poetic work, viz., Meaning-
fulness (M), Grammaticality (G) and Poeticness (P).
A complete poetry generation system must generate
texts that adhere to all the three properties. In this
work, our attempt would be to generate meaningful
lyrics that match the melody and the poetic aspects
of the lyric will be tackled in future works.
</bodyText>
<page confidence="0.975308">
40
</page>
<bodyText confidence="0.999524782608696">
According to on-line resources such as How to
write lyrics (Demeter, 2001), the generated lyric
must have Rhythm, Rhyme and Repetition.
One of the recent attempts for automatically gen-
erating lyrics for a given melody is the Tra-la-
Lyrics system (Oliveira et al., 2007). This system
uses the ABC notation (Gonzato, 2003) for repre-
senting melody and the corresponding suite of tools
for analyzing the melodies. The key aspect of the
system is its attempt to detect the strong beats
present in the given melody and associating words
with stressed syllables in the corresponding posi-
tions. It also evaluates three lyric generation strate-
gies (Oliveira et al., 2007) – random
words+rhymes, sentence templates+rhymes and
grammar+rhymes. Of these strategies, the sentence
templates+rhymes approach attempts for syntactical
coherence and the grammar+rhymes approach uses
a grammar to derive Portuguese sentence templates.
From the demo runs presented, we see that the sys-
tem can generate grammatical sentences (when us-
ing an appropriate strategy). However, there is no
attempt to bring Meaningfulness in the lyrics.
</bodyText>
<sectionHeader confidence="0.949624" genericHeader="method">
2 Lyric Generation for Tamil
</sectionHeader>
<bodyText confidence="0.999534666666667">
Tamil, our target language for generating lyrics, is a
phonetic language. There is a one-to-one relation
between the grapheme and phoneme. We make use
of this property in coming up with a generic repre-
sentation for all words in the language. This repre-
sentation, based on the phonemic syllables, consists
</bodyText>
<note confidence="0.6698505">
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 40–46,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998756">
of the following three labels: Kuril (short vowel,
represented by K), Nedil (long vowel, represented
by N) and Mei (consonants, represented by M). For
example, the word thA-ma-rai (lotus) will be repre-
sented as N-K-N (long vowel followed by short
vowel followed by another long vowel). This repre-
sentation scheme, herein after referred as KNM rep-
resentation, is used throughout our system - train-
ing, melody analysis and as input to the sentence
generation module.
</bodyText>
<sectionHeader confidence="0.994582" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.99995404">
Our approach to generating lyrics for the given
melody is a two-step process (Figure 1). The first
step is to analyze the input melody and output a se-
ries of syllable patterns in KNM representation
scheme along with tentative word and sentence
boundary. The subsequent step involves filling the
syllable pattern with words from the corpus that
match the given syllable pattern and any rhyme re-
quirements. We approach the first aspect as a Se-
quence Labeling problem and use the popular
CRF++ toolkit (Kudo, 2005) to label the input
melody in ABC notation (Gonzato, 2003) with ap-
propriate syllable categories (Kuril, Nedil and Mei).
This system is trained with sample film songs and
their corresponding lyrics (in KNM scheme) as in-
put. The trained model is then used to label the giv-
en input melody. The syllable pattern, thus generat-
ed for the input melody, is provided to a Sentence
Generation Module that finds suitable lyrics satisfy-
ing the following constraints: a.) Words should
match the syllable pattern b.)The sequence of words
should have a meaning. We achieve this by using
the popular Dijkstra&apos;s Shortest Path Algorithm (Cor-
men et al., 1990) against a pre-built corpus of Uni-
gram and Bigram of Words.
</bodyText>
<sectionHeader confidence="0.9728" genericHeader="method">
4 Melody Analysis
</sectionHeader>
<bodyText confidence="0.999172285714286">
The goal of the Melody Analysis is to analyze the
input melody and suggest a possible KNM represen-
tation scheme that will match the melody. Since our
representation of melody is based on the ABC Nota-
tion (Gonzato, 2003), which is textual, we approach
this problem as labeling the ABC notation using the
KNM representation scheme.
</bodyText>
<figureCaption confidence="0.995408">
Figure 1. System Approach
</figureCaption>
<subsectionHeader confidence="0.999852">
4.1 Characteristics of Melody
</subsectionHeader>
<bodyText confidence="0.9957226">
Every melody follows a Meter, which provides the
basic design principles in music. Some of the most
frequently used meters we encountered in the film
songs that we used are 2/4, 3/4, 4/4, 6/8, 9/8 and
12/8 – that indicate the number of Notes played in
</bodyText>
<page confidence="0.998913">
41
</page>
<bodyText confidence="0.999905555555556">
the given interval. Each Note is represented by the
character set A, B, C, D, E, F and G – which are
called as main notes and A#, C#, D#, F# and G# -
which are called Sharp Notes. Thus, for any given
Meter in the melody, we can find the sequence of
Notes with the corresponding duration for which the
Note is played in that meter.
For the purpose of generating lyrics, we need to
fit one syllable for each of the notes in the melody.
</bodyText>
<subsectionHeader confidence="0.996435">
4.2 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.997964407407408">
Conditional Random Fields(CRF) (Lafferty et al.,
2001) is a Machine Learning technique that has per-
formed well for sequence labeling problems such as
POS tagging, Chunking and Named Entity Recogni-
tion. It overcomes the difficulties faced in other
techniques like Hidden Markov Models(HMM) and
Maximum Entropy Markov Model(MEMM).
(Lafferty et al., 2001) define Conditional Ran-
dom Fields as follows: “Let G = (V, E) be a graph
such that Y = (Yv) the
v ∈ V, so that Y is indexed by
vertices of G. Then (X,Y) is a conditional random
field in case, when conditioned on X, the random
variables Yv obey the Markov property with respect
to the graph: p(Yv|X,Yw,w≠v) = p(Yv|X,Yw,w~v),
where w~v means that w and v are neighbors in G”.
Here X denotes a sentence and Y denotes the label se-
quence. The label sequence y which maximizes the like-
lihood probability pӨ(y|x) will be considered as the cor-
rect sequence, while testing for new sentence x with
CRF model . The likelihood probability pӨ(y|x) is ex-
pressed as follows.
where λk and μk are parameters from CRF model θ
and fk and gk are the binary feature functions that we
need to give for training the CRF model. This is
where we integrate the specific features of the prob-
lem into the machine learning models like CRF.
</bodyText>
<subsectionHeader confidence="0.999034">
4.3 Feature Templates
</subsectionHeader>
<bodyText confidence="0.999892">
There are three models that need to be learnt, viz,
labeling notes with KNM scheme, identifying word
boundaries and identifying line boundaries. We
present below the features used to learn each of the
above.
</bodyText>
<subsectionHeader confidence="0.951879">
4.3.1 Learning KNM labels
</subsectionHeader>
<bodyText confidence="0.810933333333333">
In addition to the labels K, N and M, there are also
other non-syllable features that need to be identified
in the melody. Thus, the complete list of labels in-
clude, K, N, KM, NM, TIE, OPEN, CLOSE, PRE
and BAR.
K – short vowel
N – long vowel
KM – short vowel followed by consonant
NM – long vowel followed by consonants
TIE – presence of a Tie in the meter
OPEN – opening of a tie
CLOSE – closing of a tie
PRE – Note that follows a tie
BAR – End of meter.
The following are the list of features considered:
</bodyText>
<listItem confidence="0.993738166666667">
• Current Note
• Previous Note + Current Note + Next Note
• Previous-to-previous Note + Previous Note
+ Current Note + Next Note + Next-to-next
Note
• Current Note/Duration
• Previous Note/Duration + Current
Note/Duration + Next Note/Duration
• Previous-to-previous Note/Duration + Pre-
vious Note/Duration + Current Note/Dura-
tion + Next Note/Duration + Next-to-next
Note/Duration.
</listItem>
<page confidence="0.989803">
42
</page>
<subsubsectionHeader confidence="0.546066">
4.3.2 Word Boundary pus. The Bigram list contains only the frequency of
</subsubsectionHeader>
<bodyText confidence="0.986238166666667">
occurrence.
Another important aspect in analyzing the melody is
to spot potential word boundaries. While in many
cases, the presence of bars could indicate potential
word boundaries, there are also cases where a given
word can span a bar (especially due to the presence
of Ties). Hence, we need to explicitly train our sys-
tem to identify potential word boundaries. The fea-
tures used to identify the boundaries of words are
mostly the same as for learning the KNM labels, but
with the addition of considering two more previous
notes along with their durations.
</bodyText>
<subsectionHeader confidence="0.527788">
4.3.3 Sentence Boundary
</subsectionHeader>
<bodyText confidence="0.999827">
As with Word Boundary ,we cannot assume sen-
tence boundaries based on the musical notation and
hence we also train our system to identify potential
sentence boundaries. Sentence boundary identifica-
tion happens after the word boundaries are identi-
fied and hence this additional feature is used along
with the above-mentioned features for sentence
boundary training.
</bodyText>
<sectionHeader confidence="0.9875" genericHeader="method">
5 Sentence Generation
</sectionHeader>
<bodyText confidence="0.999985833333333">
The goal of the Sentence Generation module is to
generate a meaningful phrase that matches the input
pattern given in KNM scheme. For example, given
an input pattern such as &apos;KMKM NKM NKN&apos;, it
should generate a phrase consisting of three words
each of them matching their respective pattern.
</bodyText>
<subsectionHeader confidence="0.99948">
5.1 Corpus selection and pre­processing
</subsectionHeader>
<bodyText confidence="0.99997675">
Since we are interested in generating lyrics for
melody, the corpus we chose consisted mainly of
poems and short stories. The only pre-processing
involved was to remove any special characters
(such as “( ), $ % &amp;, etc.) from the text. From this
corpus, we index all Unigram and Bigram of
Words. Each word is marked with its KNM syllable
pattern and their frequency of occurrence in the cor-
</bodyText>
<subsectionHeader confidence="0.995787">
5.2 Graph Construction
</subsectionHeader>
<bodyText confidence="0.999223666666667">
Given an input pattern (say &apos;KMKM NKM NKN&apos;),
we construct a directed graph with the list of words
satisfying each pattern, as represented by Figure 2.
</bodyText>
<figureCaption confidence="0.998558">
Figure 2. Graph Construction
</figureCaption>
<bodyText confidence="0.99976275">
The edge from word Wij (of, say pattern KMKM) to
Wrs (of, say pattern NKM) is weighted based on the
frequency values collected from the corpus and is
calculated as follows:
</bodyText>
<equation confidence="0.920229666666667">
# (Wij followed by Wrs)
P(Wrs / Wij) = (Eqn. 1)
# (Wij)
</equation>
<bodyText confidence="0.999644">
Since the Shortest Path Algorithm picks the path
with the least cost, we need to weight the edges in
such a way that a higher probability sequence gets
the least cost (C). Thus, we measure Cost(Wrs/Wij)
as:
</bodyText>
<equation confidence="0.910742">
Cost(Wrs/Wij) = 1 - P(Wrs/Wij) (from Eqn. 1) (Eqn. 2)
</equation>
<bodyText confidence="0.999209333333333">
By default, the cost from the START node to the
first list of words and the cost from the last list of
words to END node is fixed as 1.
</bodyText>
<subsectionHeader confidence="0.996021">
5.3 Preferential selection of paths
</subsectionHeader>
<bodyText confidence="0.999604">
One of the shortcomings of using the Shortest Path
Algorithm is that, for the given input pattern and the
given Corpus, the algorithm will always generate
the same phrase (with the least cost). In addition to
</bodyText>
<page confidence="0.999508">
43
</page>
<bodyText confidence="0.999972666666667">
this problem, when the melody demands, we need
to generate rhyming words. Lastly, we need to han-
dle the case where the corpus may not have a phrase
that matches the complete pattern. We tackle all the
above issues by biasing the Shortest Path Algorithm
by changing the cost of the edges.
</bodyText>
<subsectionHeader confidence="0.809099">
5.3.1 Bias initial word
</subsectionHeader>
<bodyText confidence="0.999950538461538">
In order to generate different phrases for the same
pattern (say KMKM NKM NKN), we pick a random
word that matches the initial pattern (KMKM) and
fix the cost of the edge from START to the random
word to 0. As the default cost from START to all
leading words is 1, this biases the algorithm to find
a pattern that starts with the random word. Howev-
er, if there exists a phrase, whose “overall cost” is
still less than the one starting with the random
phrase, the algorithm will output the same phrase.
In order to avoid this, we provide multiple random
words and pick the one that truly generates a unique
phrase.
</bodyText>
<subsectionHeader confidence="0.775865">
5.3.2 Rhyming Words
</subsectionHeader>
<bodyText confidence="0.99998815625">
When there is a need to generate phrases that rhyme
with any previously generated phrases, especially in
line endings, we use the same biasing technique to
prefer certain words over others. The motivation to
concentrate on line endings is based on our assump-
tion that the notes in melody would be similar for
the rhyming words and thus our representation
scheme involving Mei(M) (consonants) would han-
dle the stressed syllables. The path finding algo-
rithm, can take as input a word and a position, with
which the new phrase should rhyme in the given po-
sition. In this case, we generate all the words that
rhyme with the given word by using the Maximum
substring matching technique. That is, the word
with the maximum substring common to the input
word, in word endings, is considered as a rhyming
word. For example, given an input word &apos;kOyil&apos;
(temple), the rhyming words would be &apos;vAyil&apos; (gate)
and &apos;veyil&apos; (sun). As can be seen, both the words
have the suffix &apos;yil&apos; common with the input word.
Thus, as earlier, the cost of the edges in the paths
leading to such rhyming words will be set to 0, thus
biasing the algorithm to pick these paths. One an-
other way would be have only those nodes corre-
sponding to the rhyming words (discarding other
non-rhyming words). However, in the case where
no rhyming words are present in the corpus, this ap-
proach can lead to a graph with an incomplete path.
Hence we use the approach of biasing the graph
paths that can pick the rhyming words, if present
and provide a non-rhyming word, if none was avail-
able.
</bodyText>
<subsubsectionHeader confidence="0.830599">
5.3.3 Edit-Distance Matching
</subsubsectionHeader>
<bodyText confidence="0.999910714285714">
There can also be cases when there is no phrase that
exactly matches the given input pattern sequence,
though the corpus might contain individual words
matching each pattern in the sequence. In this case,
we relax the matches using the Edit-Distance met-
ric. Thus, for the given pattern NKN, we also list
words that match NKK, KKN, etc. Since the input
patterns are deemed to fit the given melody, an
Edit-Distance Matching can turn up words that need
not match the given melody and hence should be
used only when there are no phrases matching the
input pattern. Another approach, though practically
not possible, is to have a “big enough corpus” that
contains at least one phrase matching each pattern.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999844666666667">
We conducted the experiments as two separate
steps, one for the CRF engine and another for the
Sentence Generation module.
For the CRF engine, we collected and used Tamil
film songs&apos; tune and lyrics, as they were easily
available from the web. The tunes were converted
to the ABC notation and their lyrics were converted
to the KNM representation scheme. The notes from
the tune and the syllables in the corresponding lyric
</bodyText>
<page confidence="0.998034">
44
</page>
<bodyText confidence="0.9992705">
(in their respective representation schemes) were
manually mapped with each other. An example
training file for the CRF engine for learning the
KNM representation scheme is presented below:
</bodyText>
<table confidence="0.9994685">
Note Duration Label
B &apos;/z K
C &apos;/z N
B &apos;/z K
A &apos;/z KM
G &apos;/z K
- 0 tie
[ 0 open
A &apos;/z pre
G &apos;/z K
] 0 close
B 4 K
</table>
<tableCaption confidence="0.999901">
Table 1. KNM scheme learning – training file
</tableCaption>
<bodyText confidence="0.9433385">
Similarly, for the word boundary identification, the
same input is used but with the labels corresponding
to word boundaries such as W-B (word beginning),
W-I (word intermediate), etc. (Table 2):
</bodyText>
<table confidence="0.999298583333333">
Note Duration Label
B &apos;/z W-B
C &apos;/z W-I
B &apos;/z W-I
A &apos;/z W-I
G &apos;/z W-B
- 0 Tie
[ 0 open
A &apos;/z pre
G &apos;/z W-I
] 0 close
B 4 W-I
</table>
<tableCaption confidence="0.99991">
Table 2. Word boundary learning – training file
</tableCaption>
<bodyText confidence="0.998132375">
For sentence boundary identification, the output
from the word boundary identification is used and
hence it is run after the word boundary identifica-
tion is complete. Thus, the input to the CRF engine
in this case would be like the one in (Table 3), with
labels corresponding to sentence boundary such as
S-B (sentence beginning) and S-I (sentence interme-
diate):
</bodyText>
<table confidence="0.999703615384615">
Note Duration Word Sentence
Boundary Boundary
B &apos;/z W-B S-B
c &apos;/z W-I S-I
B &apos;/z W-I S-I
A &apos;/z W-I S-I
G &apos;/z W-B S-I
- 0 Tie S-I
[ 0 Open S-I
A &apos;/z Pre S-I
G &apos;/z W-I S-I
] 0 close S-I
B 4 W-I S-B
</table>
<tableCaption confidence="0.999688">
Table 3. Sentence boundary learning – training file
</tableCaption>
<bodyText confidence="0.999867736842105">
For the Sentence Generation module, we used
short stories, poems and Tamil lyrics across various
themes such as love, appreciation of nature, patrio-
tism, etc. From this, all the special characters were
removed and the list of Unigram and Bigram Words
were collected along with their frequencies.
Based on the limited experiments performed on
the trained CRF model, we observe that the feature
set presented for Syllable identification seem to per-
form reasonably and identifies the syllables with
70% accuracy for manually tagged melodies. How-
ever, we could not objectively evaluate the Word
and Sentence Boundary identification process as the
resulting boundaries can also be considered as valid
boundaries. In general, the word and sentence
boundaries are the choice of the lyricist and hence
the results can be considered as another valid way
to generate lyric. Also, we feel that the number of
training samples (10 melodies) supplied for training
</bodyText>
<page confidence="0.997608">
45
</page>
<bodyText confidence="0.996877833333333">
the CRF engine is very less for it to reasonably
learn the nuances that are present in real-word
lyrics.
Some of the syllable patterns identified from the
tune and the corresponding sentences generated are
given below:
</bodyText>
<figure confidence="0.475425833333333">
Pattern: &apos;KK KK KKK
NKKM KMKK&apos;
Output: 93 61a @Juwgd
(grrua�n ur apgd
Translation: In small age
I recollected
</figure>
<bodyText confidence="0.932363">
As the syllable patterns get longer, we had to re-
sort to using Edit Distance in order to find matching
sentences. One such output is presented below:
</bodyText>
<figure confidence="0.553772666666667">
Pattern: ’KMKMKM KMKM NKN
NKMKM NMKKM NKN’
Output: aprb §6i) 91ri5jva a�rr6m6urrn
6r urg (pmpLugL_ ur Glarr6i)a
Translation: We can see here in Tamil
Proclaiming aloud
</figure>
<sectionHeader confidence="0.967892" genericHeader="conclusions">
7 Limitations and Future Work
</sectionHeader>
<bodyText confidence="0.999982777777778">
From the initial set of experiments, we see that it is
possible to generate a syllable pattern that closely
matches the input tune. Currently, we do not consid-
er the identification of strong beats in the melody
and are expecting the presence of Mei (M) to take
care of stressed syllables. We also expect the same
strategy to work for other South Indian languages as
well. The current Lyric Generation algorithm is
simplistic, in that it can generate short meaningful
phrases, but generating longer phrases require
adding constraints (such as closest matching pat-
terns) that defeats the purpose of matching with the
tune. Also, the current method generates phrases
that are independent of the previous phrases. This
leads to lyrics that are meaningful in parts, but
meaningless on the whole.
Future work can involve introducing “semantic
similarity” across phrases in a lyric, thereby gener-
ating lyrics that provide a coherent meaning. Also,
experiments can be conducted with different do-
main corpus to generate lyrics for a given situation
(such as Love, Death, Travel, etc.) Other sentence
generation strategies, such as an Evolutionary Algo-
rithm (as suggested in (Manurung, 2004)) can also
be attempted. Once a coherent meaningful lyric is
generated, further improvements can be towards in-
corporating poetic aspects in the lyric.
</bodyText>
<sectionHeader confidence="0.999169" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999812823529412">
Demeter. 2001. How to write Lyrics
http://everything2.com/index.pl?node=How to write
lyrics.
Guido Gonzato. 2003. The ABCPlus Project http://abc-
plus.sourceforge.net.
Hanna M. Wallach. 2004. Conditional Random Fields:
An Introduction. Technical Report MS-CIS-04-21.
Department of Computer and Information Science,
University of Pennsylvania.
Hisar Maruli Manurung. 2004. An evolutionary ap-
proach to poetry generation. Ph.D. Thesis, University
of Edinburg.
Hugo R. Goncalo Oliveira, F. Amilcar Cardoso, and F.
Camara Perreira. 2007. Tra-La Lyrics: An approach
to generate text based on rhythm. Proceedings of the
Fourth International Joint Workshop on Computa-
tional Creativity, IJWCC&apos;07, London:47-54.
Hugo R. Goncalo Oliveira, F. Amilcar Cardoso, and F.
Camara Perreira. 2007. Exploring difference strate-
gies for the automatic generation of song lyrics with
Tra-La Lyrics. Proceedings of the Portuguese Confer-
ence on Artificial Intelligence (EPIA 2007),
Guimarães, Portugal:57-68
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
data. Proceedings of the Eighteenth International
Conference on Machine Learning (ICML 2001),
Williams College, Willamstown, MA, USA:282-289
Taku Kudo. 2005. CRF++: Yet Another CRF toolkit.
http://crfpp.sourceforge.net.
Thomas H. Cormen., Charles E. Leiserson., Ronald L.
Rivest. 1990. Introduction to Algorithms. Prentice-
Hall: 527-531.
</reference>
<page confidence="0.999612">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.165596">
<title confidence="0.99997">Automatic Generation of Tamil Lyrics for Melodies</title>
<author confidence="0.989154">Ananth Ramakrishnan A Sankar Kuppan Sobha Lalitha Devi</author>
<affiliation confidence="0.85497">AU-KBC Research MIT Campus, Anna</affiliation>
<address confidence="0.874051">Chennai, India</address>
<affiliation confidence="0.838701">AU-KBC Research MIT Campus, Anna</affiliation>
<address confidence="0.873903">Chennai, India</address>
<affiliation confidence="0.8267">AU-KBC Research MIT Campus, Anna</affiliation>
<address confidence="0.974818">Chennai, India</address>
<email confidence="0.903846">ananthrk@au-kbc.orgsankar@au­kbc.orgsobha@au­kbc.org</email>
<abstract confidence="0.989519388888889">This paper presents our on-going work to automatically generate lyrics for a given melody, for phonetic languages such as Tamil. We approach the task of identifying the required syllable pattern for the lyric as a sequence labeling problem and hence use the popular CRF++ toolkit for learning. A corpus comprising of 10 melodies was used to train the system to understand the syllable patterns. The trained model is then used to guess the syllabic pattern for a new melody to produce an optimal sequence of syllables. This sequence is presented to the Sentence Generation module which uses the Dijkstra&apos;s shortest path algorithm to come up with a meaningful phrase matching the syllabic pattern.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Demeter</author>
</authors>
<title>How to write Lyrics http://everything2.com/index.pl?node=How to write lyrics.</title>
<date>2001</date>
<contexts>
<context position="1612" citStr="Demeter, 2001" startWordPosition="247" endWordPosition="248"> come up with a meaningful phrase matching the syllabic pattern. 1 Introduction In an attempt to define poetry (Manurung, 2004), provides three properties for a natural language artifact to be considered a poetic work, viz., Meaningfulness (M), Grammaticality (G) and Poeticness (P). A complete poetry generation system must generate texts that adhere to all the three properties. In this work, our attempt would be to generate meaningful lyrics that match the melody and the poetic aspects of the lyric will be tackled in future works. 40 According to on-line resources such as How to write lyrics (Demeter, 2001), the generated lyric must have Rhythm, Rhyme and Repetition. One of the recent attempts for automatically generating lyrics for a given melody is the Tra-laLyrics system (Oliveira et al., 2007). This system uses the ABC notation (Gonzato, 2003) for representing melody and the corresponding suite of tools for analyzing the melodies. The key aspect of the system is its attempt to detect the strong beats present in the given melody and associating words with stressed syllables in the corresponding positions. It also evaluates three lyric generation strategies (Oliveira et al., 2007) – random wor</context>
</contexts>
<marker>Demeter, 2001</marker>
<rawString>Demeter. 2001. How to write Lyrics http://everything2.com/index.pl?node=How to write lyrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Gonzato</author>
</authors>
<date>2003</date>
<booktitle>The ABCPlus Project http://abcplus.sourceforge.net.</booktitle>
<contexts>
<context position="1857" citStr="Gonzato, 2003" startWordPosition="287" endWordPosition="288">Grammaticality (G) and Poeticness (P). A complete poetry generation system must generate texts that adhere to all the three properties. In this work, our attempt would be to generate meaningful lyrics that match the melody and the poetic aspects of the lyric will be tackled in future works. 40 According to on-line resources such as How to write lyrics (Demeter, 2001), the generated lyric must have Rhythm, Rhyme and Repetition. One of the recent attempts for automatically generating lyrics for a given melody is the Tra-laLyrics system (Oliveira et al., 2007). This system uses the ABC notation (Gonzato, 2003) for representing melody and the corresponding suite of tools for analyzing the melodies. The key aspect of the system is its attempt to detect the strong beats present in the given melody and associating words with stressed syllables in the corresponding positions. It also evaluates three lyric generation strategies (Oliveira et al., 2007) – random words+rhymes, sentence templates+rhymes and grammar+rhymes. Of these strategies, the sentence templates+rhymes approach attempts for syntactical coherence and the grammar+rhymes approach uses a grammar to derive Portuguese sentence templates. From </context>
<context position="4209" citStr="Gonzato, 2003" startWordPosition="656" endWordPosition="657">o the sentence generation module. 3 Approach Our approach to generating lyrics for the given melody is a two-step process (Figure 1). The first step is to analyze the input melody and output a series of syllable patterns in KNM representation scheme along with tentative word and sentence boundary. The subsequent step involves filling the syllable pattern with words from the corpus that match the given syllable pattern and any rhyme requirements. We approach the first aspect as a Sequence Labeling problem and use the popular CRF++ toolkit (Kudo, 2005) to label the input melody in ABC notation (Gonzato, 2003) with appropriate syllable categories (Kuril, Nedil and Mei). This system is trained with sample film songs and their corresponding lyrics (in KNM scheme) as input. The trained model is then used to label the given input melody. The syllable pattern, thus generated for the input melody, is provided to a Sentence Generation Module that finds suitable lyrics satisfying the following constraints: a.) Words should match the syllable pattern b.)The sequence of words should have a meaning. We achieve this by using the popular Dijkstra&apos;s Shortest Path Algorithm (Cormen et al., 1990) against a pre-bui</context>
</contexts>
<marker>Gonzato, 2003</marker>
<rawString>Guido Gonzato. 2003. The ABCPlus Project http://abcplus.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Conditional Random Fields: An Introduction.</title>
<date>2004</date>
<tech>Technical Report MS-CIS-04-21.</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<marker>Wallach, 2004</marker>
<rawString>Hanna M. Wallach. 2004. Conditional Random Fields: An Introduction. Technical Report MS-CIS-04-21. Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hisar Maruli Manurung</author>
</authors>
<title>An evolutionary approach to poetry generation.</title>
<date>2004</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Edinburg.</institution>
<contexts>
<context position="1125" citStr="Manurung, 2004" startWordPosition="167" endWordPosition="168">e task of identifying the required syllable pattern for the lyric as a sequence labeling problem and hence use the popular CRF++ toolkit for learning. A corpus comprising of 10 melodies was used to train the system to understand the syllable patterns. The trained model is then used to guess the syllabic pattern for a new melody to produce an optimal sequence of syllables. This sequence is presented to the Sentence Generation module which uses the Dijkstra&apos;s shortest path algorithm to come up with a meaningful phrase matching the syllabic pattern. 1 Introduction In an attempt to define poetry (Manurung, 2004), provides three properties for a natural language artifact to be considered a poetic work, viz., Meaningfulness (M), Grammaticality (G) and Poeticness (P). A complete poetry generation system must generate texts that adhere to all the three properties. In this work, our attempt would be to generate meaningful lyrics that match the melody and the poetic aspects of the lyric will be tackled in future works. 40 According to on-line resources such as How to write lyrics (Demeter, 2001), the generated lyric must have Rhythm, Rhyme and Repetition. One of the recent attempts for automatically genera</context>
</contexts>
<marker>Manurung, 2004</marker>
<rawString>Hisar Maruli Manurung. 2004. An evolutionary approach to poetry generation. Ph.D. Thesis, University of Edinburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo R Goncalo Oliveira</author>
<author>F Amilcar Cardoso</author>
<author>F Camara Perreira</author>
</authors>
<title>Tra-La Lyrics: An approach to generate text based on rhythm.</title>
<date>2007</date>
<booktitle>Proceedings of the Fourth International Joint Workshop on Computational Creativity, IJWCC&apos;07,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="1806" citStr="Oliveira et al., 2007" startWordPosition="277" endWordPosition="280"> to be considered a poetic work, viz., Meaningfulness (M), Grammaticality (G) and Poeticness (P). A complete poetry generation system must generate texts that adhere to all the three properties. In this work, our attempt would be to generate meaningful lyrics that match the melody and the poetic aspects of the lyric will be tackled in future works. 40 According to on-line resources such as How to write lyrics (Demeter, 2001), the generated lyric must have Rhythm, Rhyme and Repetition. One of the recent attempts for automatically generating lyrics for a given melody is the Tra-laLyrics system (Oliveira et al., 2007). This system uses the ABC notation (Gonzato, 2003) for representing melody and the corresponding suite of tools for analyzing the melodies. The key aspect of the system is its attempt to detect the strong beats present in the given melody and associating words with stressed syllables in the corresponding positions. It also evaluates three lyric generation strategies (Oliveira et al., 2007) – random words+rhymes, sentence templates+rhymes and grammar+rhymes. Of these strategies, the sentence templates+rhymes approach attempts for syntactical coherence and the grammar+rhymes approach uses a gra</context>
</contexts>
<marker>Oliveira, Cardoso, Perreira, 2007</marker>
<rawString>Hugo R. Goncalo Oliveira, F. Amilcar Cardoso, and F. Camara Perreira. 2007. Tra-La Lyrics: An approach to generate text based on rhythm. Proceedings of the Fourth International Joint Workshop on Computational Creativity, IJWCC&apos;07, London:47-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo R Goncalo Oliveira</author>
<author>F Amilcar Cardoso</author>
<author>F Camara Perreira</author>
</authors>
<title>Exploring difference strategies for the automatic generation of song lyrics with Tra-La Lyrics.</title>
<date>2007</date>
<booktitle>Proceedings of the Portuguese Conference on Artificial Intelligence (EPIA</booktitle>
<location>Guimarães, Portugal:57-68</location>
<contexts>
<context position="1806" citStr="Oliveira et al., 2007" startWordPosition="277" endWordPosition="280"> to be considered a poetic work, viz., Meaningfulness (M), Grammaticality (G) and Poeticness (P). A complete poetry generation system must generate texts that adhere to all the three properties. In this work, our attempt would be to generate meaningful lyrics that match the melody and the poetic aspects of the lyric will be tackled in future works. 40 According to on-line resources such as How to write lyrics (Demeter, 2001), the generated lyric must have Rhythm, Rhyme and Repetition. One of the recent attempts for automatically generating lyrics for a given melody is the Tra-laLyrics system (Oliveira et al., 2007). This system uses the ABC notation (Gonzato, 2003) for representing melody and the corresponding suite of tools for analyzing the melodies. The key aspect of the system is its attempt to detect the strong beats present in the given melody and associating words with stressed syllables in the corresponding positions. It also evaluates three lyric generation strategies (Oliveira et al., 2007) – random words+rhymes, sentence templates+rhymes and grammar+rhymes. Of these strategies, the sentence templates+rhymes approach attempts for syntactical coherence and the grammar+rhymes approach uses a gra</context>
</contexts>
<marker>Oliveira, Cardoso, Perreira, 2007</marker>
<rawString>Hugo R. Goncalo Oliveira, F. Amilcar Cardoso, and F. Camara Perreira. 2007. Exploring difference strategies for the automatic generation of song lyrics with Tra-La Lyrics. Proceedings of the Portuguese Conference on Artificial Intelligence (EPIA 2007), Guimarães, Portugal:57-68</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence data.</title>
<date>2001</date>
<booktitle>Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001),</booktitle>
<pages>282--289</pages>
<location>Williams College, Willamstown, MA,</location>
<contexts>
<context position="6028" citStr="Lafferty et al., 2001" startWordPosition="974" endWordPosition="977">ongs that we used are 2/4, 3/4, 4/4, 6/8, 9/8 and 12/8 – that indicate the number of Notes played in 41 the given interval. Each Note is represented by the character set A, B, C, D, E, F and G – which are called as main notes and A#, C#, D#, F# and G# - which are called Sharp Notes. Thus, for any given Meter in the melody, we can find the sequence of Notes with the corresponding duration for which the Note is played in that meter. For the purpose of generating lyrics, we need to fit one syllable for each of the notes in the melody. 4.2 Conditional Random Fields Conditional Random Fields(CRF) (Lafferty et al., 2001) is a Machine Learning technique that has performed well for sequence labeling problems such as POS tagging, Chunking and Named Entity Recognition. It overcomes the difficulties faced in other techniques like Hidden Markov Models(HMM) and Maximum Entropy Markov Model(MEMM). (Lafferty et al., 2001) define Conditional Random Fields as follows: “Let G = (V, E) be a graph such that Y = (Yv) the v ∈ V, so that Y is indexed by vertices of G. Then (X,Y) is a conditional random field in case, when conditioned on X, the random variables Yv obey the Markov property with respect to the graph: p(Yv|X,Yw,w</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence data. Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), Williams College, Willamstown, MA, USA:282-289</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<date>2005</date>
<journal>CRF++: Yet Another CRF</journal>
<note>toolkit. http://crfpp.sourceforge.net.</note>
<contexts>
<context position="4151" citStr="Kudo, 2005" startWordPosition="646" endWordPosition="647">t our system - training, melody analysis and as input to the sentence generation module. 3 Approach Our approach to generating lyrics for the given melody is a two-step process (Figure 1). The first step is to analyze the input melody and output a series of syllable patterns in KNM representation scheme along with tentative word and sentence boundary. The subsequent step involves filling the syllable pattern with words from the corpus that match the given syllable pattern and any rhyme requirements. We approach the first aspect as a Sequence Labeling problem and use the popular CRF++ toolkit (Kudo, 2005) to label the input melody in ABC notation (Gonzato, 2003) with appropriate syllable categories (Kuril, Nedil and Mei). This system is trained with sample film songs and their corresponding lyrics (in KNM scheme) as input. The trained model is then used to label the given input melody. The syllable pattern, thus generated for the input melody, is provided to a Sentence Generation Module that finds suitable lyrics satisfying the following constraints: a.) Words should match the syllable pattern b.)The sequence of words should have a meaning. We achieve this by using the popular Dijkstra&apos;s Short</context>
</contexts>
<marker>Kudo, 2005</marker>
<rawString>Taku Kudo. 2005. CRF++: Yet Another CRF toolkit. http://crfpp.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<tech>PrenticeHall:</tech>
<pages>527--531</pages>
<contexts>
<context position="4791" citStr="Cormen et al., 1990" startWordPosition="750" endWordPosition="754">melody in ABC notation (Gonzato, 2003) with appropriate syllable categories (Kuril, Nedil and Mei). This system is trained with sample film songs and their corresponding lyrics (in KNM scheme) as input. The trained model is then used to label the given input melody. The syllable pattern, thus generated for the input melody, is provided to a Sentence Generation Module that finds suitable lyrics satisfying the following constraints: a.) Words should match the syllable pattern b.)The sequence of words should have a meaning. We achieve this by using the popular Dijkstra&apos;s Shortest Path Algorithm (Cormen et al., 1990) against a pre-built corpus of Unigram and Bigram of Words. 4 Melody Analysis The goal of the Melody Analysis is to analyze the input melody and suggest a possible KNM representation scheme that will match the melody. Since our representation of melody is based on the ABC Notation (Gonzato, 2003), which is textual, we approach this problem as labeling the ABC notation using the KNM representation scheme. Figure 1. System Approach 4.1 Characteristics of Melody Every melody follows a Meter, which provides the basic design principles in music. Some of the most frequently used meters we encountere</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Thomas H. Cormen., Charles E. Leiserson., Ronald L. Rivest. 1990. Introduction to Algorithms. PrenticeHall: 527-531.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>