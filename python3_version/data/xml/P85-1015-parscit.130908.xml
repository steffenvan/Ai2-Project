<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<note confidence="0.791062">
Parsing with Discontinuous Constituents
Mark Johnson
Center for the Study of Language and Information
and
Department of Linguistics, Stanford University.
</note>
<sectionHeader confidence="0.974654" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999070833333333">
By generalizing the notion of location of a constituent to allow
discontinuous loctaions, one can describe the discontinuous consti-
tuents of non-configurational languages. These discontinuous consti-
tuents can be described by a variant of definite clause grammars,
and these grammars can be used in conjunction with a proof pro-
cedure to create a parser for non-configurational languages.
</bodyText>
<sectionHeader confidence="0.996389" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999297071428571">
In this paper I discuss the problem of describing and computa-
tionally processing the discontinuous constituents of non-
configurational languages. In these languages the grammatical func-
tion that an argument plays in the clause or the sentence is not
determined by its position or configuration in the sentence, as it is
in configurational languages like English, but rather by some kind of
morphological marking on the argument or on the verb. Word order
in non-configurational languages is often extremely free: it has been
claimed that if some string of words S is grammatical in one of these
languages, then the string S&apos; formed by any arbitrary permutation
of the words in S is also grammatical. Most attempts to describe
this word order freedom take mechanisms designed to handle fairly
rigid word order systems and modify them in order to account for
the greater word order freedom of non-configurational languages.
Although it is doubtful whether any natural language ever exhibits
such total scrambling, it is interesting to investigate the computa-
tional and linguistic implications of systems that allow a high degree
of word order freedom. So the approach here is the opposite to the
usual one: I start with a system which, unconstrained, allows for
unrestricted permutation of the words of a sentence, and capture
any word order regularities the language may have by adding res-
trictions to the system. The extremely free word order of non.
configurational languages is described by allowing constituents to
have discontinuous locations. To demonstrate that it is possible to
parse with such discontinuous constituents. I show how they can be
incorporated into a variant of definite clause grammars. and that
these grammars can be used in conjunction with a proof procedure,
such as Earley deduction, to coestruct a parser, as shown in Pereira
and Warren (1983).
This paper is organized as follows: section 2 contains an infor-
mal introduction to Definite Clause Grammars and discusses how
they can be used in parsing, section 3 gives a brief description of
some of the grammatical features of one non-configurational
language, Guugu Yimidliirr, section 4 presents a definite clause frag-
ment for this language, and shows how this can he used for parsing.
Section 5 notes that the use or discontinuous constituents is not lim-
ited to definite clause grammars, but they could be incorporated
into such disparate formalisms as GPSG, I,FG or G1.3. Section 6
discusses whether a unified account of parsing both configurational
and non-configurational languages can be given, and section 7 com-
pares the notion of discontinuous constituents with other approaches
to free word order.
</bodyText>
<sectionHeader confidence="0.962914" genericHeader="method">
2. Definite Clause Grammars and Parsing
</sectionHeader>
<bodyText confidence="0.997072470588235">
In this section I .show how to represent both an utterance and
a context free grammar (CFG) so that, the locations of constituents
are explicitly represented in the grammar formalism. Given this, it
will be easy to generalize the notion of location so that it can
describe the discontinuous constituents of non-configurational
Languages. The formalism I use here is the Definite Clause Gram-
mar formalism described in Clocicsin and Mellish (1984). To fami-
liarize the reader with the DCG notation, I discuss a fragment for
English in this section. In fact, the DCG representation is even
more general than is brought out here: as Pereira and Warren (1983)
demonstrated, one can view parsing algorithms highly specialized
proof procedures, and the process of parsing as logical inferencing on
the representation of the utterance, with the grammar functioning
as the axioms of the logical system.
Given a context free grammar, as in (1), the parsing problem is
to determine whether a particular utterance, such as the one in (2),
is an S with respect to it.
</bodyText>
<listItem confidence="0.7949072">
(1) S NP VP
vp — V NP
NP --• Det N
Dei j-i-NPCeni
(2) the boy&apos;s 2 father 3 hit, 4 the 5 dog 5
</listItem>
<bodyText confidence="0.998716833333333">
The subscripts in (2) serve to locate the lexical items: they
indicate that, for instance, the utterance of the word dog began at
time tv and ended at time t4. That is, the location of the utterance
&amp;quot;dog&amp;quot; in example (2) was the interval its,f ej. I interpret the sub-
scripts as the points in time that segment the utterance into indivi-
dual words or morphemes. Note that they perform the same func-
tion as the vertices of a standard chart parsing system.
Parsing (2) is the same as searching for an S node that dom-
inates the entire string, ie. whose location is [0,61. By looking at the
rules in (1), we see that an S node is composed of an NP and a VP
node. The interpretation conventions associated with phrase struc-
ture rules like those in (1) tell us this, and also tell us that the loca-
tion of the S is the concatenation of the location of the NP and the
VP. That is, the existence of an S node located at j0.6I would be
implied by the existence of an NP node located at interval &amp;quot;0.xj ( r
a variable) and a VP node located at lz ,61.
The relationship between the mother constituent&apos;s location
and those of its daughters is made explicit in the definite clause
grammar, shown in (3), that corresponds to the CFG (1). The utter-
ance (1) (after lexical analysis) would be represented as in (4).
Those familiar with Prolog should note that I have reversed the
usual orthographic convention by writing variables with a lower case
intial letter (they are also italicized), while constants begin with an
upper case letter.
</bodyText>
<equation confidence="0.850962909090909">
(3) S(r ) NP(r ,y VP(y ).
VP(z ) V(z ,y ) SzNP(y
NP(r ,z ,case ) — Det(r .g) &amp; N(y ,case ).
Det(z. ,y) NP(r ,y ,Gen)
127
(4) Det(0,1).
N(1,2,Gen).
N(2,3,0).
V(3,4).
Det(4,5).
N(5,6,0).
</equation>
<bodyText confidence="0.997216609756098">
(3) contains four definite clauses; each corresponds to one
phrase structure rule in the CFG (1). The major difference for us
between (1) and (3) is that in (3) the locations of the constituents
(ie. the endpoints) are explicit in the formalism: they are arguments
of the constituents, the first argument being the beginning time of
the constituent&apos;s location, the second argument its ending time.
Also note the way that syntactic features are treated in this system:
the difference between genitive and non-genitive case marked nouns
is indicated by a third argument in both the N and NP constituents.
Genitive nouns and noun phrases have the value Gen for their third
argument, while non-genitive NP have the value 0, and the rule that
expands NP explicitly passes the case of the mother to the head
daughter&apos;.
We can use (3) and (4) to make inferences about the existence
of constituents in utterance (2). For example, using the rule that
expands NP in (3) together with the first two facts of (4), we can
infer the existence of constituent NP(0,2,Gen).
The simplest approach to parsing is probably to use (1) in a
top-down fashion, and start by searching for an S with location 10,61;
that is, search for goal S(0,6). This method, top down recursive des-
cent, is the method the programming language Prolog uses to per-
form deduction on definite clauses systems, so Prolog can easily be
used to make very efficient top-down parsers.
Unfortunately, despite their intuitive simplicity, top down
recursive descent parsers have certain properties that make them
less than optimal for handling natural language phenomena. Unless
the grammar the parser is using is very specially crafted, these
parsers tend to go into infinite loops. For example, the rule that
expands NP into Det and N in (3) above would be used by a top-
down parser to create a Det subgoal from an NP goal. But Det
itself can be expanded as a genitive NP, so the parser would create
another NP subgoal from the Det subgoal, and so on infinitely.
The problem is that the parser is searching for the same NP
many times over: what is needed is a strategy that reduces multiple
searches for the same item to a single search, and arranges to share
its results. Earley deduction, based on the Earley parsing algorithm.
is capable of doing this. For reasons of time, I won&apos;t go into details
of Earley Deduction (see Pereira and Warren (1983) for details); I
will simply note here that using Earley Deduction on the definite
clause grammar in (3) results in behaviour that corresponds exactly
to the way an Earley chart parser would parse (1).
</bodyText>
<sectionHeader confidence="0.99457" genericHeader="method">
3. Non-configurational Languages
</sectionHeader>
<bodyText confidence="0.995959520833334">
In this section I identify some of the properties of non-
configurational languages. Since this is a paper on discontinuous
constituents, I focus on word order properties, as exemplified in the
non-configurational language Guugu Yimidhirr. The treatment here
is necessarily superficial: I have completely ignored many complex
phonological, inflectional and syntactic processes that, a complete
grammar would have to deal with.
A non-configurational language differs from configurational
languages like English in that morphological form (eg. affixes),
rather than position (ie. configuration), indicates which words are
syntactically connected to each other. In English the grammatical,
and hence semantic, relationship between boy. father and dog in (5)
are indicated in surface form by their positions, and changing these
positions changes these relationships, and hence the meaning, a.s in
&apos; Of course, there is nothing special about these two values:
any two distinct values would have done.
(6).
(5) The boy&apos;s father hit the dog
(8) The father&apos;s dog hit the boy
In Guugu Yimidhirr, an Australian language spoken in north-
east Queensland, the relationships in (7)2 are indicated by the affixes
on the various nouns, and to change the relationships, one would
have to change the affixes.
Yarraga-aga-mu-n gudaa gunda-y biiba-ngun
boy-GEN-mu-ERG dog+ABS hit-PAST father-ERG
&apos;The boy&apos;s father hit the dog&apos;
The idea, then, is that in these languages morphological form
plays the same role that word order does in a configurational
language like English. One might suspect that word order would be
rather irrelevant in a non-configurational language, and infact
Guugu Yimidhirr speakers remark that their language, un-
like English, can be spoken &apos;back to front&apos;: that is, it is
possible to scramble words and still produce a grammatical
utterance (Haviland 1979, p 26.)
Interestingly, in some Guugu Yimidhirr constructions it
appears that information about grammatical relations can be obtain
either through word order or morphology: in the possessive construc-
tion
When a complex NP carries case inflection, each element
(in this case, both possession and possessive expression)
may bear case inflection - and both must be inflected for
case if they are not contiguous - but frequently the &apos;head
noun&apos; (the possession) (directly Mil precedes the possessive
expression, and only the latter has explicit case inflection
(Haviland 1979,p.56)
Thus in (8), Iliad &apos;father&apos; shows up without an ergative suffix
because it is immediately to the left of the NP that possesses it (ie.
possession is indicated by position).
</bodyText>
<listItem confidence="0.692013">
(8) Biiba yarragasaga-mu-n gudaa gunda-y
father boy-GEN-mu-ERG dog+ABS hit-PAST
&apos;The boy&apos;s father hit the dog&apos;
</listItem>
<bodyText confidence="0.996376857142857">
While ultimate judgement will have to await a full analysis of
these constructions, it does seem as if word order and morphological
form do supply the same sort of information.
In the sections that follow, I will show how a variant of
definite clause grammar can be used to describe the examples given
above, and how this grammar can be used in conjunction with a
proof procedure to construct a parser.
</bodyText>
<sectionHeader confidence="0.924799" genericHeader="method">
4. Representing Discontinuous Constituents
</sectionHeader>
<bodyText confidence="0.9976054">
propose to represent discontinuous constituents rather
directly, in terms of a syntactic category and a discontinuous loca-
tion in the utterance. For example, I represent the location of the
discontinous constituent in (7), Yarraga-aga-mu-n biiba-ngun
&apos;boy&apos;s father&apos; as a set of continuous locations, as in (9).
</bodyText>
<listItem confidence="0.587952">
(9) (10.1143.41)
</listItem>
<bodyText confidence="0.949004">
Alternatively, one could represent discontinuous locations in
terms of a &apos;bit-pattern&apos;, as in (10), where a &apos;1&apos; indicates that the
constituent occupies this position.
</bodyText>
<equation confidence="0.95388">
(10) (1 0 0 1 I
</equation>
<bodyText confidence="0.951647">
While the descriptive power of both representations is the
same, I will use the representation of (9) because it is somewhat
All examples are from Haviland (1979). The constructions
shown here are used to indicate alienable possession (which includes
kinship relationships).
</bodyText>
<equation confidence="0.773471">
(7)
</equation>
<page confidence="0.964865">
128
</page>
<bodyText confidence="0.97803303125">
easier to state configurational notions in it. For example, the
requirement that a constituent be contiguous can be expressed by
requiring its location set to have no more than a single interval
member.
To represent the morphological form of NP constituents I use
two argument positions, rather than the single argument position
used in the DCG in (3). The first takes as values either Erg , Abs or
0, and the second either Gen or 0. Thus our discontinuous NP has
three argument positions in total, and would be represented as (11).
(11) NP([10,11,[3,41(,Erg,0).
In (11), the first argument position identifies the constituent&apos;s
location, while the next two are the two morphological form argu-
ments discussed immediately above. The grammar rules must tell
us under what conditions we can infer the existence of a constituent
like (11). The morphological form features seem to pose no particu-
lar problem: they can be handled in a similiar way to the genitive
feature in the mini-DCG for English in (3) (although a full account
would have to deal with the dual ergative/absolutive and
nominative/accusative systems that Guugu Yimidhirr possesses).
But the DCG rule format must be extended to allow for discontinu-
ous locations of constituents, like (11).
In the rules in (3), the end-points of the mother&apos;s location are
explicitly constructed from the end-points of the daughter&apos;s loca-
tions. In general, the realtionship between the mother&apos;s location and
that of its daughters can be represented in terms of a predicate that
holds between them. In the DCG rules for Guugu Yimidhirr, (12) to
(14), the relationship between the mother&apos;s location and those of its
daughters is represented by the predicate combines.
The definition of combines is as follows: combines(1,11,12) is
true if and only if I is equal to the (bit-wise) union of 11 and 12, and
the (bit-wise) intersection of 1, and 12 is null (ie. 1, and 12 must be
non-overlapping locations).
</bodyText>
<listItem confidence="0.9829978">
(12) S(1) •-•- V(11) &amp; NP(12,Erg,0) &amp; NP(12,Ab3,0)
Ss combines(&apos;
(13) NP(1,case ,0) N(11,cass ,0) &amp; NP(12,ense ,Gen)
Ss combines(&apos;
(14) NP(( ,case bcase 2) N(1, case 1, Cale 2)
</listItem>
<bodyText confidence="0.988309388888889">
Following Hale (1983), I have not posited a VP node in this
grammar, although it would have trivial to do so. To account for
the &apos;configurational&apos; possessive shown in (8), I add the additional
clause in (15) to the grammar.
(15) NP(11x ,z [(,ease ,0) NP([[z ,y11,0,0) &amp; N(11y ,r1bease ,Gen)
Given this definite clause grammar and a proof procedure such
as Earley Deduction, it is quite straight-forward to construct a
parser. In (16) I show how one can parse (7) using the above gram-
mar and the Earley Deduction proof procedure. The Prolog predi-
cate `13&apos; starts the parser. First the lexical analyser adds lexical
items to the state (the working store of the deduction procedure),
when this is finished the deduction procedure uses the DCG rules
above to make inferences about the utterance. The answer &apos;yes&apos;
given by the parser indicates that it was able to find an S that spans
the entire utterance. The command &apos;print_state&apos; prints the state of
the deduction system; since new inferences are always added to the
bottom of the state, it provides a chonological records of the deduc-
tions made by the system. The state is printed in Prolog notation:
</bodyText>
<table confidence="0.909004434782609">
variables are written as , `_2&apos;, etc., and the implication symbol
%prolog eariey aux nscr
UNSW - PROLOG
: payarragaagarnun,gudaa,gunday,biibangunl)?
Word yarragsagsmun is a n([10, 111, erg, gen)
Word raga is a na[1, 211, aba, o)
Word geinday is a va(2, 31))
Word 6116ongun is a n(1(31 4fi, erg, 0)
st,y2,2
: print_state I
.(r,1„, erg, gen)
n([ 1, 211, abs, o)
v([2, 31))
n([3, 41), erg, o)
s(110, 4)1) :- v(_1) , np(_2, erg, o) , abs, o) ,
combines(0, 411, _1, _2, _3)
s(0, 411) np(_L, erg, o) , np(_2, abs, o) ,
cornbines(0, 411, 1[2, 311, _1, _2)
np(_1, erg, o) n(_2, erg, o) , np(_3, erg, gen) ,
combines(_1, -3)
np(_1, erg, o) np(_2, erg, gen) , combines(_1, ri13, 411, _2)
np(_1, erg, gen) :- n(_1, erg, gen)
npa[0, Ill, erg, gen)
</table>
<construct confidence="0.540896428571428">
np(_1, erg, o) combines(_1, 1[3, 411, 1[0, 111)
combines010, 11, is, 411, 1(3, 4)), [10, LH)
npa[0, 11, [3, 4)), erg, o)
s((10, 4)1) np(_1, abs, o) ,
combines([10, 411, ([2, 31), 1[0, [3, 411, _1)
np(_1, abs, o) n(_2, abs, o) , np(_3, abs. gen) ,
combines(_1, _2, _3)
np(_1, she, o) np(_2, abs, gen) , combines(_1, 1[1, 211, _2)
np(_1, abs, gen) :- n(_1, abs, gen)
np(_1, abs, o) n(_1, abe, o)
np((11, 2)), abs, o)
30[0, 4)i) cornbinesa[0, 411, ([2, 31), ((0, 11, [3, 411, 1[1, 211)
combinesa(0, 4)1, ([2, 311, ff0, if, f3, 411, ([1, 211)
s(ff0, 411)
np(a_1, _211, abs, o) np(11_1, _311, o, o) , _211, abs. gen)
np(1(_1, _21), o, o) n(_3, o, o) , np(_4. o, gen) ,
combines(L1, _21(, _3, _4)
np(((_1, _211, o, o) n(f(_l, _211, o, o)
np(ff_l, _21(, o, o) npuf_1, _3(1, o, o) , n(([_3, _21), o, gen)
np(_1, erg, o) n(_1, erg, o)
np(1[3, 411, erg, o)
</construct>
<bodyText confidence="0.99284575">
s(([0, 411) np(_1, abs, o) , combinesth0, 411, f12, 311, 113, 411, _I)
saf0, 411) combines(1[0, 4(1, [[2, 31), 113, 41), (1I, 21))
np(1L1, _211, erg, o) np((1_1, _311, o, o) , n(11_3, _21), erg, gen/
:
</bodyText>
<sectionHeader confidence="0.864136" genericHeader="method">
6. Using Discontinuous Constituents in Grammars
</sectionHeader>
<bodyText confidence="0.999703875">
Although the previous section introduced discontinuous consti-
tuents in terms of definite clause grammar, there is no reason we
could not invent a notation that abbreviates or implies the &apos;com-
bines&apos; relationship between mother and daughters, just as the CFG
in (1) &amp;quot;implies&amp;quot; the mother-daughter location relationships made
explicit in the DCG (3). For instance, we could choose to interpret
a rule like (17) as implying the &apos;combines&apos; relationship between the
mother and its daughter&apos;s.
</bodyText>
<listItem confidence="0.825324">
(17) A -• B ; D
</listItem>
<bodyText confidence="0.99483">
Then the DCG grammar presented in the last section could be
written in the GPSG like notation of (18). Note that the third rule
is a standard phrase structure rule: it expresses the `configurational&apos;
possessive shown in (8).
</bodyText>
<page confidence="0.990012">
129
</page>
<figure confidence="0.9397888">
(18) S ICASTErgi ; V; [CASTAbNP NP sl
[CASE al I {cGenE+a] ; [CAI al
[CASE al [ Ge
cAsNr+
Ea &apos;
</figure>
<bodyText confidence="0.9991515">
It is easy to show that grammars based on the &apos;combines&apos;
predicate lie outside the class of context free languages: the strings
the grammar (19) accepts are the permutations of a* 6* c • ; thus
this grammar does not have weakly equivalent CFG.
</bodyText>
<listItem confidence="0.9307425">
(19) S a ;b ;c (s)
(S)
</listItem>
<bodyText confidence="0.993814180722892">
While it would be interesting to investigate other properties of
the &apos;combines&apos; predicate, I suspect that it is not optimal for describ-
ing linguistic systems in general, including non-configurational
languages. It is difficult to state word order requirements that refer
to a particular constituent position in the utterance. For instance,
the only word order requirement in Warlpiri, another non-
configurational language, is that the auxilary element must follow
exactly one syntactic constituent, and this would be difficult to state
in a system with only the predicate &apos;combines&apos;, although it would be
easy to write a special DCG predicate which forces this behaviour.
Rather, I suspect it would be more profitable to investigate
other predicates on constituent locations besides &apos;combines&apos; to see
what implications they have. In particular, the wrapping operations
of Pollard (1984) would seem to be excellent candidates for such
research.
Finally, I note that the discontinuous constituent analysis
described here is by no means incompatible with standard theories
of grammar. As I noted before, the rules in (18) look very much like
GPSG rules, and with a little work much of the machinery of GSPG
could be grafted on to such a formalism. Similiarly, the CFG part
of LFG, the C-structure, could be enriched to allow discontinuous
constituents if one wished. And introducing some version of discon-
tinuous constituents to GB could make the mysterious &amp;quot;mapping&amp;quot;
between P-structure and L-structure that Hale (1983) talks about a
little less perplexing.
My own feeling is that the approach that would bring the most
immediate results would be to adopt some of the &amp;quot;head driven&amp;quot;
aspects of Pollard&apos;s (1984) Head Grammars. In his conception,
heads contain as lexical information a list of the items they sub-
categorize for. This strongly suggests that one should parse accord-
ing to a &amp;quot;head first&amp;quot; strategy: when one parses a sentence, one looks
for its verb first, and then, based on the lexical form of the verb, one
looks for the other arguments in the clause. Not only would such an
approach be easy to implement in a DCG framework, but given the
empirical fact that the nature of argument NPs in a clause is
strongly determined by that clause&apos;s verb, it seems a very reasonable
thing to do.
8. Implementing the Parser
In their 1983 paper, Pereira and Warren point out several
problems involved in implementing the Earley proof procedure, and
proposed ways of circumventing or minimizing these problems. In
this section I only consider the specialized case of Earley Deduction
working with clauses that correspond to grammars of either the con-
tinuous or discontinuous constituent type, rather than the general
case of performing deduction on an arbitrary set of clauses.
Considering first the case of Earley Deduction applying to a
set of clauses like (3) that correspond to a CFG, a sensible thing to
do would be to index the derived clauses (ie. the intermediate
results) on the left edge of their location. Because Earley Deduction
on such a set of clauses always proceeds in exactly the same manner
as Earley chart parsing, namely strictly left to right within a consti-
tuent, the position of the left edge of any constituent being searched
for is always determined by the ending location of the constituent
immediately preceeding it in the derivation. That is, the proof pro-
cedure is always searching for constituents with hard, ie. non-
variable, left edges. I have no empirical data on this point, but the
reduction in the number of clauses that need to be checked because
of this indexing could be quite important. Note that the vertices in
a chart act essentially as indices to edges in the manner described.
Unfortunately, indexing on the left edge in system working
with discontinuous constituents in the manner suggested above
would not be very useful, since the inferencing does not proceed in a
left to right fashion. Rather, if the suggestions at the end of the last
section are heeded, the parser proceeds in a &amp;quot;head first&amp;quot; fashion,
looking first for the head of a constituent and then for its comple-
ments, the nature and number of which are partially determined by
information available from the head. In such a strategy, it would
seem reasonable to index clauses not on their location, but on mor-
phological or categorial features, such as category, case, etc., since
these are the features they will be identified by when they are
searched for.
It seems then that the optimal data structure for one type of
constituent is not optimal for the other. The question then arises
whether there is a unified parsing strategy for both configurational
and non-configurational languages. Languages with contiguous con-
stituents could be parsed with a head first strategy, but I suspect
that this would prove less efficient than a strategy that indexed on
left edge position. Locations have the useful property that their
number grows as the size of the sentence (and hence the number of
constituents) increases, thus giving more indexing resolution where it
is needed, namely in longer sentences. But of course, one could
always index on both morphological category and utterance loca-
tion...
</bodyText>
<subsectionHeader confidence="0.936375">
T. Comparison with other Frameworks
</subsectionHeader>
<bodyText confidence="0.999593333333333">
In this section I compare the discontinuous location approach I
have developed above to some other approaches to free word order:
the ID/LP rule format of GPSG, and the non-configurational encod-
ing of LFG. I have omitted a discussion of the scrambling and rais-
ing rules of Standard Theory and their counterparts in current GB
theory because their properties depend strongly on properties of the
grammatical system as a whole (such as a universal theory of &amp;quot;land-
ing sites&amp;quot;, etc.): which (as far as I know) have not been given in
sufficiently specific form to enable a comparison.
The ID/LP rule format (Gazdar et al. 1985) can be regarded as
a factoring of &amp;quot;normal&amp;quot; context free rules&apos; into two components, one
expressing immediate domination relationships, the other the linear
precedence relationships that hold between daughter constituents.
For example, the ID rule in (20) and the LP rule in (21) express the
same mother-daughter relationships as the rules in (22).
</bodyText>
<listItem confidence="0.987588625">
(20) S (V, NP, NP, S&apos;
(21) V &lt; S&apos;
(22) S V NP NP S&apos;
S — V NP S&apos; NP
S V S&apos; NP NP
S NP V NP S&apos;
S NP V S&apos; NP
S NP NP V S&apos;
</listItem>
<bodyText confidence="0.994420333333333">
Because a grammar in ID/LP format always has a strongly
equivalent context free grammar, only context free languages can be
generated by these grammars. Since it is possible to write grammars
3 In Gazdar et al. (1985) the system is more complicated than
this, since the ID/LP component interacts with the feature instan-
tiation principles and other components of the grammar.
</bodyText>
<page confidence="0.989516">
130
</page>
<bodyText confidence="0.996385789473684">
for non-tontext-free languages using discontinuous constituents (as
shown above), it is clear that ID/LP format is less powerful than the
discontinuous constituent analysis proposed here. In particular,
1DfLP allows only reordering of the daughters of a constitiuent rela-
tive to the other daughters: it does not allow a constituent to be
&amp;quot;scattered&amp;quot; accross the sentence in the way a discontinuous consti-
tuent analysis allows. Thus an ID/LP grammar of Guugu Yimidhirr
could not analyse sentence (7) in the same way we did here. In fact,
if we added the requirement that all locations be continuous (ie. that
the location sets contain at most one member) to the DCG rules
using the &apos;combines&apos; predicate, the word order freedom allowed
would be the same as that allowed by an ID rule without any LP
restrictions. I don&apos;t claim that it is impossible to write a GPSG
grammar for a language as Guissu Yitnidhirr on the basis of the
formalism&apos;s not allowing discontinuous constituents: on closer inves-
tigation it might turn out that the &amp;quot;discontinuities&amp;quot; could be
described by some set of medium or long distance dependencies.
In LFG the nature of the mapping between c-structure and I..
structure enables it to achieve many of the effects of discontinuous
constituents, even though the phrase structure component (the c-
structure) does not allow discontinuous constituents as such. In par-
ticular, the information represented in one component of the f-
structure may come from several different c-structure constituents
located throughout the sentence. For example, in their analysis of
the cross serial dependencies in Dutch, Bresnan, Kaplan, Peters and
Zaenen (1982) propose that the FRED feature of the VCOMP com-
ponent of the f-structure is set by a verb located down one branch
of the c-structure tree, while the OBJ feature of that component is
set by an NP located on another branch of the c-structure tree.
Thus in LFG one would not claim that there was a discontinuous
NP in (7), but rather that both the ergative NP and the genitive
marked ergative NP were contributing information to the same com-
ponent of the f-structure.
In the non-configurational encoding of Bresnan (1982, p.297),
the c-structure is relatively impoverished, and the morphology on
the lexical items identifies the component of the f-structure they
supply information to. For example, the c-structure in (23) together
with the lexical items in (24) give sentence (7) the f-structure (25).
</bodyText>
<figure confidence="0.549735">
j NP V 1*
(23) s 1 &apos; 1=4 I
(24)
</figure>
<bodyText confidence="0.997832282608696">
LFG is capable of describing the &amp;quot;discontinuity&amp;quot; of (7)
without using discontinuous constituents. There is, however, a sub-
tle difference in the amount of &amp;quot;discontinuity&amp;quot; allowed by the LFG
and the discontinuous constituent analyses. As I remarked at the
beginning of the paper, the discontinuous constituent approach
allows grammars that accept total scrambling of the lexical items: if
a string S is accepted, then so is any permutation of S. In particu-
lar, the discontinuous constituent approach allows unrestricted
scrambling of elements out of embedded clauses and stacked NPs.
which the LFC non-configurational encoding analysis cannot. This
is because the position in the sentence&apos;s f-structure that any lexical
item occupies is determined solely by the f-equation annotations
attached to that lexical item, since the only equations in the c-
structure are of the form 1•=1„ and these create no new components
in the f-structure for the clause to embed the f-structures from lexi-
cal items into.
Suppose, for example. Guugu Yimidhirr allowed stacked NP
possessors, in the same way that English allows them in construc-
tions like my mother&apos;s father&apos;s brother, except that, because the
language is non-configurational, the lexical elements could be scat-
tered throughout the entire sentence. The LFG analysis would run
into problems here, because there would be a potentially infinite
number of positions in the f-structure where the possessor could be
located: implying that there are an infinite number of lexical entries
for each possessive NP.
Guugu Yimidhirr does not exhibit such stacked possessives.
Rather, the possessor of the possessor is indicated by a dative con-
struction and so the LFG analysis is supported here. None the less,
a similiar argument shows that embedded clausal f-structure com-
ponents such as adjunts or VCOMP must have corresponding c-
structure nodes so that the lexical items in these clauses can be
attached sufficiently &amp;quot;far down&amp;quot; in the f-structure for the entire sen-
tence. (Another possibility, which I won&apos;t explore here, would be to
allow f-equation annotations to include regular expressions over
items like VCOMP ). Still, it would be interesting to investigate
further the restrictions on scrambling that follow from the non-
configurational encoding analysis and the basic principles of LFG.
For instance, the offline pal-ability property (Pereira and Warren
1983) that is required to assure decidablity in LFG (Bresnan and
Kaplan 1982) essentially prohibits scrambling of single lexical ele-
ments from doubly embedded clauses, because such scrambling
would entail one S node exhaustively dominating another. But these
distinctions are quite subtle, and, unfortunately, our knowledge of
non-configurational languages is insufficient to determine whether
the scrambling they exhibit is within • the limits allowed by non-
configurational encoding.
</bodyText>
<sectionHeader confidence="0.966471" genericHeader="conclusions">
8. Conclusion
</sectionHeader>
<bodyText confidence="0.999637125">
Hale (1983) begins his paper by listing three properties that
have come to be associated with the typological label &apos;non-
configurational&apos;, namely (i) free word order, (ii) the use of syntacti-
cally discontinuous constituents and (iii) the extensive use of null
anaphora. In this paper I have shown that the first two properties
follow from a system that allows constituents that have discontinu-
ous constituents and that captures the mother daughter location
relationships using a predicate like &apos;combines&apos;.
</bodyText>
<figure confidence="0.99882124">
NP
(I SUBS P0SS)=-1
yarrage-ags-rnu-fl
(1 CASE)=Erg
(lGen)=+
NP
(1 OBJ)=.1
(1 CASE).---Abs
V
(I PRED)=hit((t SUBJ),(( OBJ))
NP
(1 SUBJ)=i
(1 CASE)=-Erg
gudaa
gunda-y
biiba-ngun
CASE = Erg I
POSS { Gen = +
FRED boy
SLJBJ= CASE = Erg
{FRED father
CASE = Abs
FRED =i dog
FRED = hit(
OBJ
</figure>
<page confidence="0.991807">
131
</page>
<bodyText confidence="0.997057">
It is still far too early to tell whether this approach really is
the most appropriate way to deal with discontinuous conatituenta: it
may be that for a grammar of reasonable size some other technique,
such as the non-configurational encoding of LFG, will be superior on
linguistic and computational grounds.
</bodyText>
<sectionHeader confidence="0.936807" genericHeader="references">
9. Bibliography
</sectionHeader>
<reference confidence="0.9997796">
J. Bresnan (1982), &amp;quot;Control and Complementation,&amp;quot; in The
Mental Representation of Grammatical Relations, J. Bresnan, ed.,
pp. 173-281, MIT Press, Cambridge, Mass.
J. Bresnan and R. Kaplan (1982), &amp;quot;Lexical Functional Gram-
mar: A Formal System for Grammatical Representation,&amp;quot; in The
Mental Representation of Grammatical Relations, J. Bresnan, ed.,
pp. 173-281, MIT Press, Cambridge, Maas.
J. Bresnan, R. Kaplan, S. Peters and A. Zaenen (1982),
Cross-Serial Dependencies in Dutch, Linguistic Inquiry, 11.4, pp.
613-635.
G. Gazdar, E. Klein, G. Pullum and I. Sag, (1985) Generalized
Phrase Structure Grammar, Havard University Press, Cambridge,
Maas.
K. Hale (1983), &amp;quot;Warlpiri and the Grammar of Non-
configurational Languages&amp;quot;, Natural Language and Linguistic
Theory, 1.1, pp. 5-49.
.1. Haviland (1979), &amp;quot;Guugu Yimidhirr&amp;quot;, in Handbook of Aus-
tralian Languages, R. Dixon and B. Blake, eds., Benjamins. Amster-
dam.
F.C.N. Pereira and D.H.D. Warren (1983), &amp;quot;Parsing as Deduc-
tion&amp;quot;, Proc. of the 21st Annual Meeting of the ACL, pp. 137-143,
Association for Computational Linguistics.
C. Pollard (1984), Generalized Phrase Structure Grammars,
Head Grammars, and Natural Language, unpublished thesis, Stan-
ford University.
</reference>
<page confidence="0.997721">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839805">
<title confidence="0.997128">Discontinuous Constituents</title>
<author confidence="0.999996">Mark Johnson</author>
<affiliation confidence="0.962267333333333">Center for the Study of Language and Information and Department of Linguistics, Stanford University.</affiliation>
<abstract confidence="0.991047571428571">generalizing the notion of a constituent to allow discontinuous loctaions, one can describe the discontinuous constituents of non-configurational languages. These discontinuous constituents can be described by a variant of definite clause grammars, and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bresnan</author>
</authors>
<title>Control and Complementation,&amp;quot; in The Mental Representation of Grammatical Relations,</title>
<date>1982</date>
<pages>173--281</pages>
<editor>J. Bresnan, ed.,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="27807" citStr="Bresnan (1982" startWordPosition="4651" endWordPosition="4652">ir analysis of the cross serial dependencies in Dutch, Bresnan, Kaplan, Peters and Zaenen (1982) propose that the FRED feature of the VCOMP component of the f-structure is set by a verb located down one branch of the c-structure tree, while the OBJ feature of that component is set by an NP located on another branch of the c-structure tree. Thus in LFG one would not claim that there was a discontinuous NP in (7), but rather that both the ergative NP and the genitive marked ergative NP were contributing information to the same component of the f-structure. In the non-configurational encoding of Bresnan (1982, p.297), the c-structure is relatively impoverished, and the morphology on the lexical items identifies the component of the f-structure they supply information to. For example, the c-structure in (23) together with the lexical items in (24) give sentence (7) the f-structure (25). j NP V 1* (23) s 1 &apos; 1=4 I (24) LFG is capable of describing the &amp;quot;discontinuity&amp;quot; of (7) without using discontinuous constituents. There is, however, a subtle difference in the amount of &amp;quot;discontinuity&amp;quot; allowed by the LFG and the discontinuous constituent analyses. As I remarked at the beginning of the paper, the dis</context>
</contexts>
<marker>Bresnan, 1982</marker>
<rawString>J. Bresnan (1982), &amp;quot;Control and Complementation,&amp;quot; in The Mental Representation of Grammatical Relations, J. Bresnan, ed., pp. 173-281, MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
<author>R Kaplan</author>
</authors>
<title>Lexical Functional Grammar: A Formal System for Grammatical Representation,&amp;quot; in The Mental Representation of Grammatical Relations,</title>
<date>1982</date>
<pages>173--281</pages>
<editor>J. Bresnan, ed.,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Maas.</location>
<contexts>
<context position="30594" citStr="Bresnan and Kaplan 1982" startWordPosition="5087" endWordPosition="5090">orresponding cstructure nodes so that the lexical items in these clauses can be attached sufficiently &amp;quot;far down&amp;quot; in the f-structure for the entire sentence. (Another possibility, which I won&apos;t explore here, would be to allow f-equation annotations to include regular expressions over items like VCOMP ). Still, it would be interesting to investigate further the restrictions on scrambling that follow from the nonconfigurational encoding analysis and the basic principles of LFG. For instance, the offline pal-ability property (Pereira and Warren 1983) that is required to assure decidablity in LFG (Bresnan and Kaplan 1982) essentially prohibits scrambling of single lexical elements from doubly embedded clauses, because such scrambling would entail one S node exhaustively dominating another. But these distinctions are quite subtle, and, unfortunately, our knowledge of non-configurational languages is insufficient to determine whether the scrambling they exhibit is within • the limits allowed by nonconfigurational encoding. 8. Conclusion Hale (1983) begins his paper by listing three properties that have come to be associated with the typological label &apos;nonconfigurational&apos;, namely (i) free word order, (ii) the use</context>
</contexts>
<marker>Bresnan, Kaplan, 1982</marker>
<rawString>J. Bresnan and R. Kaplan (1982), &amp;quot;Lexical Functional Grammar: A Formal System for Grammatical Representation,&amp;quot; in The Mental Representation of Grammatical Relations, J. Bresnan, ed., pp. 173-281, MIT Press, Cambridge, Maas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
<author>R Kaplan</author>
<author>S Peters</author>
<author>A Zaenen</author>
</authors>
<date>1982</date>
<journal>Cross-Serial Dependencies in Dutch, Linguistic Inquiry,</journal>
<volume>11</volume>
<pages>613--635</pages>
<marker>Bresnan, Kaplan, Peters, Zaenen, 1982</marker>
<rawString>J. Bresnan, R. Kaplan, S. Peters and A. Zaenen (1982), Cross-Serial Dependencies in Dutch, Linguistic Inquiry, 11.4, pp. 613-635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar,</title>
<date>1985</date>
<publisher>Havard University Press,</publisher>
<location>Cambridge, Maas.</location>
<contexts>
<context position="24784" citStr="Gazdar et al. 1985" startWordPosition="4135" endWordPosition="4138"> this section I compare the discontinuous location approach I have developed above to some other approaches to free word order: the ID/LP rule format of GPSG, and the non-configurational encoding of LFG. I have omitted a discussion of the scrambling and raising rules of Standard Theory and their counterparts in current GB theory because their properties depend strongly on properties of the grammatical system as a whole (such as a universal theory of &amp;quot;landing sites&amp;quot;, etc.): which (as far as I know) have not been given in sufficiently specific form to enable a comparison. The ID/LP rule format (Gazdar et al. 1985) can be regarded as a factoring of &amp;quot;normal&amp;quot; context free rules&apos; into two components, one expressing immediate domination relationships, the other the linear precedence relationships that hold between daughter constituents. For example, the ID rule in (20) and the LP rule in (21) express the same mother-daughter relationships as the rules in (22). (20) S (V, NP, NP, S&apos; (21) V &lt; S&apos; (22) S V NP NP S&apos; S — V NP S&apos; NP S V S&apos; NP NP S NP V NP S&apos; S NP V S&apos; NP S NP NP V S&apos; Because a grammar in ID/LP format always has a strongly equivalent context free grammar, only context free languages can be generate</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>G. Gazdar, E. Klein, G. Pullum and I. Sag, (1985) Generalized Phrase Structure Grammar, Havard University Press, Cambridge, Maas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hale</author>
</authors>
<title>Warlpiri and the Grammar of Nonconfigurational Languages&amp;quot;,</title>
<date>1983</date>
<journal>Natural Language and Linguistic Theory,</journal>
<volume>1</volume>
<pages>5--49</pages>
<contexts>
<context position="14875" citStr="Hale (1983)" startWordPosition="2444" endWordPosition="2445">en them. In the DCG rules for Guugu Yimidhirr, (12) to (14), the relationship between the mother&apos;s location and those of its daughters is represented by the predicate combines. The definition of combines is as follows: combines(1,11,12) is true if and only if I is equal to the (bit-wise) union of 11 and 12, and the (bit-wise) intersection of 1, and 12 is null (ie. 1, and 12 must be non-overlapping locations). (12) S(1) •-•- V(11) &amp; NP(12,Erg,0) &amp; NP(12,Ab3,0) Ss combines(&apos; (13) NP(1,case ,0) N(11,cass ,0) &amp; NP(12,ense ,Gen) Ss combines(&apos; (14) NP(( ,case bcase 2) N(1, case 1, Cale 2) Following Hale (1983), I have not posited a VP node in this grammar, although it would have trivial to do so. To account for the &apos;configurational&apos; possessive shown in (8), I add the additional clause in (15) to the grammar. (15) NP(11x ,z [(,ease ,0) NP([[z ,y11,0,0) &amp; N(11y ,r1bease ,Gen) Given this definite clause grammar and a proof procedure such as Earley Deduction, it is quite straight-forward to construct a parser. In (16) I show how one can parse (7) using the above grammar and the Earley Deduction proof procedure. The Prolog predicate `13&apos; starts the parser. First the lexical analyser adds lexical items t</context>
<context position="20446" citStr="Hale (1983)" startWordPosition="3407" endWordPosition="3408">cellent candidates for such research. Finally, I note that the discontinuous constituent analysis described here is by no means incompatible with standard theories of grammar. As I noted before, the rules in (18) look very much like GPSG rules, and with a little work much of the machinery of GSPG could be grafted on to such a formalism. Similiarly, the CFG part of LFG, the C-structure, could be enriched to allow discontinuous constituents if one wished. And introducing some version of discontinuous constituents to GB could make the mysterious &amp;quot;mapping&amp;quot; between P-structure and L-structure that Hale (1983) talks about a little less perplexing. My own feeling is that the approach that would bring the most immediate results would be to adopt some of the &amp;quot;head driven&amp;quot; aspects of Pollard&apos;s (1984) Head Grammars. In his conception, heads contain as lexical information a list of the items they subcategorize for. This strongly suggests that one should parse according to a &amp;quot;head first&amp;quot; strategy: when one parses a sentence, one looks for its verb first, and then, based on the lexical form of the verb, one looks for the other arguments in the clause. Not only would such an approach be easy to implement in</context>
<context position="31027" citStr="Hale (1983)" startWordPosition="5148" endWordPosition="5149"> and the basic principles of LFG. For instance, the offline pal-ability property (Pereira and Warren 1983) that is required to assure decidablity in LFG (Bresnan and Kaplan 1982) essentially prohibits scrambling of single lexical elements from doubly embedded clauses, because such scrambling would entail one S node exhaustively dominating another. But these distinctions are quite subtle, and, unfortunately, our knowledge of non-configurational languages is insufficient to determine whether the scrambling they exhibit is within • the limits allowed by nonconfigurational encoding. 8. Conclusion Hale (1983) begins his paper by listing three properties that have come to be associated with the typological label &apos;nonconfigurational&apos;, namely (i) free word order, (ii) the use of syntactically discontinuous constituents and (iii) the extensive use of null anaphora. In this paper I have shown that the first two properties follow from a system that allows constituents that have discontinuous constituents and that captures the mother daughter location relationships using a predicate like &apos;combines&apos;. NP (I SUBS P0SS)=-1 yarrage-ags-rnu-fl (1 CASE)=Erg (lGen)=+ NP (1 OBJ)=.1 (1 CASE).---Abs V (I PRED)=hit(</context>
</contexts>
<marker>Hale, 1983</marker>
<rawString>K. Hale (1983), &amp;quot;Warlpiri and the Grammar of Nonconfigurational Languages&amp;quot;, Natural Language and Linguistic Theory, 1.1, pp. 5-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haviland</author>
</authors>
<title>Guugu Yimidhirr&amp;quot;,</title>
<date>1979</date>
<booktitle>in Handbook of Australian Languages,</booktitle>
<editor>R. Dixon and B. Blake, eds.,</editor>
<location>Benjamins. Amsterdam.</location>
<contexts>
<context position="10629" citStr="Haviland 1979" startWordPosition="1762" endWordPosition="1763">ships, one would have to change the affixes. Yarraga-aga-mu-n gudaa gunda-y biiba-ngun boy-GEN-mu-ERG dog+ABS hit-PAST father-ERG &apos;The boy&apos;s father hit the dog&apos; The idea, then, is that in these languages morphological form plays the same role that word order does in a configurational language like English. One might suspect that word order would be rather irrelevant in a non-configurational language, and infact Guugu Yimidhirr speakers remark that their language, unlike English, can be spoken &apos;back to front&apos;: that is, it is possible to scramble words and still produce a grammatical utterance (Haviland 1979, p 26.) Interestingly, in some Guugu Yimidhirr constructions it appears that information about grammatical relations can be obtain either through word order or morphology: in the possessive construction When a complex NP carries case inflection, each element (in this case, both possession and possessive expression) may bear case inflection - and both must be inflected for case if they are not contiguous - but frequently the &apos;head noun&apos; (the possession) (directly Mil precedes the possessive expression, and only the latter has explicit case inflection (Haviland 1979,p.56) Thus in (8), Iliad &apos;fa</context>
<context position="12610" citStr="Haviland (1979)" startWordPosition="2074" endWordPosition="2075">ectly, in terms of a syntactic category and a discontinuous location in the utterance. For example, I represent the location of the discontinous constituent in (7), Yarraga-aga-mu-n biiba-ngun &apos;boy&apos;s father&apos; as a set of continuous locations, as in (9). (9) (10.1143.41) Alternatively, one could represent discontinuous locations in terms of a &apos;bit-pattern&apos;, as in (10), where a &apos;1&apos; indicates that the constituent occupies this position. (10) (1 0 0 1 I While the descriptive power of both representations is the same, I will use the representation of (9) because it is somewhat All examples are from Haviland (1979). The constructions shown here are used to indicate alienable possession (which includes kinship relationships). (7) 128 easier to state configurational notions in it. For example, the requirement that a constituent be contiguous can be expressed by requiring its location set to have no more than a single interval member. To represent the morphological form of NP constituents I use two argument positions, rather than the single argument position used in the DCG in (3). The first takes as values either Erg , Abs or 0, and the second either Gen or 0. Thus our discontinuous NP has three argument </context>
</contexts>
<marker>Haviland, 1979</marker>
<rawString>.1. Haviland (1979), &amp;quot;Guugu Yimidhirr&amp;quot;, in Handbook of Australian Languages, R. Dixon and B. Blake, eds., Benjamins. Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Parsing as Deduction&amp;quot;,</title>
<date>1983</date>
<booktitle>Proc. of the 21st Annual Meeting of the ACL,</booktitle>
<pages>137--143</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="2436" citStr="Pereira and Warren (1983)" startWordPosition="377" endWordPosition="380">d, allows for unrestricted permutation of the words of a sentence, and capture any word order regularities the language may have by adding restrictions to the system. The extremely free word order of non. configurational languages is described by allowing constituents to have discontinuous locations. To demonstrate that it is possible to parse with such discontinuous constituents. I show how they can be incorporated into a variant of definite clause grammars. and that these grammars can be used in conjunction with a proof procedure, such as Earley deduction, to coestruct a parser, as shown in Pereira and Warren (1983). This paper is organized as follows: section 2 contains an informal introduction to Definite Clause Grammars and discusses how they can be used in parsing, section 3 gives a brief description of some of the grammatical features of one non-configurational language, Guugu Yimidliirr, section 4 presents a definite clause fragment for this language, and shows how this can he used for parsing. Section 5 notes that the use or discontinuous constituents is not limited to definite clause grammars, but they could be incorporated into such disparate formalisms as GPSG, I,FG or G1.3. Section 6 discusses</context>
<context position="3942" citStr="Pereira and Warren (1983)" startWordPosition="620" endWordPosition="623">to represent both an utterance and a context free grammar (CFG) so that, the locations of constituents are explicitly represented in the grammar formalism. Given this, it will be easy to generalize the notion of location so that it can describe the discontinuous constituents of non-configurational Languages. The formalism I use here is the Definite Clause Grammar formalism described in Clocicsin and Mellish (1984). To familiarize the reader with the DCG notation, I discuss a fragment for English in this section. In fact, the DCG representation is even more general than is brought out here: as Pereira and Warren (1983) demonstrated, one can view parsing algorithms highly specialized proof procedures, and the process of parsing as logical inferencing on the representation of the utterance, with the grammar functioning as the axioms of the logical system. Given a context free grammar, as in (1), the parsing problem is to determine whether a particular utterance, such as the one in (2), is an S with respect to it. (1) S NP VP vp — V NP NP --• Det N Dei j-i-NPCeni (2) the boy&apos;s 2 father 3 hit, 4 the 5 dog 5 The subscripts in (2) serve to locate the lexical items: they indicate that, for instance, the utterance </context>
<context position="8538" citStr="Pereira and Warren (1983)" startWordPosition="1442" endWordPosition="1445">Det and N in (3) above would be used by a topdown parser to create a Det subgoal from an NP goal. But Det itself can be expanded as a genitive NP, so the parser would create another NP subgoal from the Det subgoal, and so on infinitely. The problem is that the parser is searching for the same NP many times over: what is needed is a strategy that reduces multiple searches for the same item to a single search, and arranges to share its results. Earley deduction, based on the Earley parsing algorithm. is capable of doing this. For reasons of time, I won&apos;t go into details of Earley Deduction (see Pereira and Warren (1983) for details); I will simply note here that using Earley Deduction on the definite clause grammar in (3) results in behaviour that corresponds exactly to the way an Earley chart parser would parse (1). 3. Non-configurational Languages In this section I identify some of the properties of nonconfigurational languages. Since this is a paper on discontinuous constituents, I focus on word order properties, as exemplified in the non-configurational language Guugu Yimidhirr. The treatment here is necessarily superficial: I have completely ignored many complex phonological, inflectional and syntactic </context>
<context position="30522" citStr="Pereira and Warren 1983" startWordPosition="5075" endWordPosition="5078">dded clausal f-structure components such as adjunts or VCOMP must have corresponding cstructure nodes so that the lexical items in these clauses can be attached sufficiently &amp;quot;far down&amp;quot; in the f-structure for the entire sentence. (Another possibility, which I won&apos;t explore here, would be to allow f-equation annotations to include regular expressions over items like VCOMP ). Still, it would be interesting to investigate further the restrictions on scrambling that follow from the nonconfigurational encoding analysis and the basic principles of LFG. For instance, the offline pal-ability property (Pereira and Warren 1983) that is required to assure decidablity in LFG (Bresnan and Kaplan 1982) essentially prohibits scrambling of single lexical elements from doubly embedded clauses, because such scrambling would entail one S node exhaustively dominating another. But these distinctions are quite subtle, and, unfortunately, our knowledge of non-configurational languages is insufficient to determine whether the scrambling they exhibit is within • the limits allowed by nonconfigurational encoding. 8. Conclusion Hale (1983) begins his paper by listing three properties that have come to be associated with the typologi</context>
</contexts>
<marker>Pereira, Warren, 1983</marker>
<rawString>F.C.N. Pereira and D.H.D. Warren (1983), &amp;quot;Parsing as Deduction&amp;quot;, Proc. of the 21st Annual Meeting of the ACL, pp. 137-143, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
</authors>
<title>Generalized Phrase Structure Grammars, Head Grammars, and Natural Language, unpublished thesis,</title>
<date>1984</date>
<institution>Stanford University.</institution>
<contexts>
<context position="19815" citStr="Pollard (1984)" startWordPosition="3305" endWordPosition="3306">to a particular constituent position in the utterance. For instance, the only word order requirement in Warlpiri, another nonconfigurational language, is that the auxilary element must follow exactly one syntactic constituent, and this would be difficult to state in a system with only the predicate &apos;combines&apos;, although it would be easy to write a special DCG predicate which forces this behaviour. Rather, I suspect it would be more profitable to investigate other predicates on constituent locations besides &apos;combines&apos; to see what implications they have. In particular, the wrapping operations of Pollard (1984) would seem to be excellent candidates for such research. Finally, I note that the discontinuous constituent analysis described here is by no means incompatible with standard theories of grammar. As I noted before, the rules in (18) look very much like GPSG rules, and with a little work much of the machinery of GSPG could be grafted on to such a formalism. Similiarly, the CFG part of LFG, the C-structure, could be enriched to allow discontinuous constituents if one wished. And introducing some version of discontinuous constituents to GB could make the mysterious &amp;quot;mapping&amp;quot; between P-structure a</context>
</contexts>
<marker>Pollard, 1984</marker>
<rawString>C. Pollard (1984), Generalized Phrase Structure Grammars, Head Grammars, and Natural Language, unpublished thesis, Stanford University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>