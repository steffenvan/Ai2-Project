<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000205">
<title confidence="0.970632">
Learning to Explain Entity Relationships in Knowledge Graphs
</title>
<author confidence="0.996728">
Nikos Voskarides∗ Edgar Meij
</author>
<affiliation confidence="0.997638">
University of Amsterdam Yahoo Labs, London
</affiliation>
<email confidence="0.975185">
n.voskarides@uva.nl emeij@yahoo-inc.com
</email>
<author confidence="0.387606">
Manos Tsagkias
</author>
<affiliation confidence="0.28312">
904Labs, Amsterdam
</affiliation>
<email confidence="0.991776">
manos@904labs.com
</email>
<author confidence="0.94665">
Maarten de Rijke
</author>
<affiliation confidence="0.991311">
University of Amsterdam
</affiliation>
<email confidence="0.983273">
derijke@uva.nl
</email>
<author confidence="0.735445">
Wouter Weerkamp
</author>
<affiliation confidence="0.528027">
904Labs, Amsterdam
</affiliation>
<email confidence="0.992983">
wouter@904labs.com
</email>
<sectionHeader confidence="0.993784" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999549785714286">
We study the problem of explaining re-
lationships between pairs of knowledge
graph entities with human-readable de-
scriptions. Our method extracts and en-
riches sentences that refer to an entity pair
from a corpus and ranks the sentences ac-
cording to how well they describe the re-
lationship between the entities. We model
this task as a learning to rank problem for
sentences and employ a rich set of fea-
tures. When evaluated on a large set of
manually annotated sentences, we find that
our method significantly improves over
state-of-the-art baseline models.
</bodyText>
<sectionHeader confidence="0.998803" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976852466666667">
Knowledge graphs are a powerful tool for support-
ing a large spectrum of search applications includ-
ing ranking, recommendation, exploratory search,
and web search (Dong et al., 2014). A knowl-
edge graph aggregates information around enti-
ties across multiple content sources and links these
entities together, while at the same time provid-
ing entity-specific properties (such as age or em-
ployer) and types (such as actor or movie).
Although there is a growing interest in au-
tomatically constructing knowledge graphs, e.g.,
from unstructured web data (Weston et al., 2013;
Craven et al., 2000; Fan et al., 2012), the prob-
lem of providing evidence on why two entities
are related in a knowledge graph remains largely
unaddressed. Extracting and presenting evidence
for linking two entities, however, is an impor-
tant aspect of knowledge graphs, as it can enforce
trust between the user and a search engine, which
in turn can improve long-term user engagement,
e.g., in the context of related entity recommenda-
tion (Blanco et al., 2013). Although knowledge
∗This work was carried out while this author was visiting
Yahoo Labs.
graphs exist that provide this functionality to a
certain degree (e.g., when hovering over Google’s
suggested entities, see Figure 1), to the best of
our knowledge there is no previously published re-
search on methods for entity relationship explana-
tion.
</bodyText>
<figureCaption confidence="0.602763">
Figure 1: Part of Google’s search result page for
</figureCaption>
<bodyText confidence="0.994606529411765">
the query “barack obama”. When hovering over
the related entity “Michelle Obama”, an explana-
tion of the relationship between her and “Barack
Obama” is shown.
In this paper we propose a method for explain-
ing the relationship between two entities, which
we evaluate on a newly constructed annotated
dataset that we make publicly available. In par-
ticular, we consider the task of explaining rela-
tionships between pairs of Wikipedia entities. We
aim to infer a human-readable description for an
entity pair given a relationship between the two
entities. Since Wikipedia does not explicitly de-
fine relationships between entities we use a knowl-
edge graph to obtain these relations. We cast our
task as a sentence ranking problem: we automat-
ically extract sentences from a corpus and rank
</bodyText>
<page confidence="0.969354">
564
</page>
<note confidence="0.977628666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 564–574,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999593">
them according to how well they describe a given
relationship between a pair of entities. For rank-
ing purposes, we extract a rich set of features and
use learning to rank to effectively combine them.
Our feature set includes both traditional informa-
tion retrieval and natural language processing fea-
tures that we augment with entity-dependent fea-
tures. These features leverage information from
the structure of the knowledge graph. On top of
this, we use features that capture the presence in
a sentence of the relationship of interest. For our
evaluation we focus on “people” entities and we
use a large, manually annotated dataset of sen-
tences.
The research questions we address are the fol-
lowing. First, we ask what the effectiveness of
state-of-the-art sentence retrieval models is for
explaining a relationship between two entities
(RQ1). Second, we consider whether we can im-
prove over sentence retrieval models by casting the
task in a learning to rank framework (RQ2). Third,
we examine whether we can further improve per-
formance by using relationship-dependent models
instead of a relationship-independent one (RQ3).
We complement these research questions with an
error and feature analysis.
Our main contributions are a robust and effec-
tive method for explaining entity relationships, de-
tailed insights into the performance of our method
and features, and a manually annotated dataset.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999922133333333">
We combine ideas from sentence retrieval, learn-
ing to rank, and question answering to address the
task of explaining relationships between entities.
Previous work that is closest to the task we ad-
dress in this paper is that of Blanco and Zaragoza
(2010) and Fang et al. (2011). First, Blanco and
Zaragoza (2010) focus on finding and ranking sen-
tences that explain the relationship between an en-
tity and a query. Our work is different in that we
want to explain the relationship between two enti-
ties, rather than a query. Fang et al. (2011) explore
the generation of a ranked list of knowledge base
relationships for an entity pair. Instead, we try to
select sentences that describe a particular relation-
ship, assuming that this is given.
Our approach builds on sentence retrieval,
where one retrieves sentences rather than docu-
ments that answer an information need. Docu-
ment retrieval models such as tf-idf, BM25, and
language modeling (Baeza-Yates et al., 1999) have
been extended to tackle sentence retrieval. Three
of the most successful sentence retrieval methods
are TFISF (Allan et al., 2003), which is a vari-
ant of the vector space model with tf-idf weight-
ing, language modeling with local context (Mur-
dock, 2006; Fern´andez et al., 2011), and a recur-
sive version of TFISF that accounts for local con-
text (Doko et al., 2013). TFISF is very competi-
tive compared to document retrieval models tuned
specifically for sentence retrieval (e.g., BM25 and
language modeling (Losada, 2008)) and we there-
fore include it as a baseline.
Sentences that are suitable for explaining rela-
tionships can have attributes that are important for
ranking but cannot be captured by term-based re-
trieval models. One way to combine a wide range
of ranking features is learning to rank (LTR). Re-
cent years have witnessed a rapid increase in the
work on learning to rank, and it has proven to be a
very successful method for combining large num-
bers of ranking features, for web search, but also
other information retrieval applications (Burges et
al., 2011; Surdeanu et al., 2011; Agarwal et al.,
2012). We use learning to rank and represent each
sentence with a set of features that aim to capture
different dimensions of the sentence.
Question answering (QA) is the task of provid-
ing direct and concise answers to questions formed
in natural language (Hirschman and Gaizauskas,
2001). QA can be regarded as a similar task to
ours, assuming that the combination of entity pair
and relationship form the “question” and that the
“answer” is the sentence describing the relation-
ship of interest. Even though we do not follow the
QA paradigm in this paper, some of the features
we use are inspired by QA systems. In addition,
we employ learning to rank to combine the devised
features, which has recently been successfully ap-
plied for QA (Surdeanu et al., 2011; Agarwal et
al., 2012).
</bodyText>
<sectionHeader confidence="0.977514" genericHeader="method">
3 Problem Statement
</sectionHeader>
<bodyText confidence="0.999936875">
We address the problem of explaining relation-
ships between pairs of entities in a knowledge
graph. We operationalize the problem as a prob-
lem of ranking sentences from documents in a
corpus that is related to the knowledge graph.
More specifically, given two entities eZ and ej that
form an entity pair (eZ, ej), and a relation r be-
tween them, the task is to extract a set of can-
</bodyText>
<page confidence="0.997106">
565
</page>
<bodyText confidence="0.997736923076923">
didate sentences Sij _ {sij1, ... , sijk} that refer
to (ei, ej) and to impose a ranking on the sen-
tences in Sij. The relation r has the general form
(type(ei), terms(r), type(ej)), where type(e) is
the type of the entity e (e.g., Person or Actor)
and terms(r) are the terms of the relation (e.g.,
CoCastsWith or IsSpouseOf).
We are left with two specific tasks: (1) extract-
ing candidate sentences Sij, and (2) ranking Sij,
where the goal is to have sentences that provide
a perfect explanation of the relationship at the top
position of the ranking. The next section describes
our methods for both tasks.
</bodyText>
<sectionHeader confidence="0.988123" genericHeader="method">
4 Explaining Entity Relationships
</sectionHeader>
<bodyText confidence="0.9999801">
We follow a two-step approach for automatically
explaining relationships between entity pairs.
First, in Section 4.1, we extract and enrich sen-
tences that refer to an entity pair (ei, ej) from a
corpus in order to construct a set of candidate sen-
tences. Second, in Section 4.2, we extract a rich
set of features describing the entities’ relationship
r and use supervised machine learning in order to
rank the sentences in Sij according to how well
they describe the relationship r.
</bodyText>
<subsectionHeader confidence="0.999725">
4.1 Extracting candidate sentences
</subsectionHeader>
<bodyText confidence="0.999703620689656">
To create a set of candidate sentences for a given
entity pair and relationship, we require a corpus of
documents that is pertinent to the entities at hand.
Although any kind of document collection can be
used, we focus on Wikipedia in this paper, as it
provides good coverage for the majority of entities
in our knowledge graph.
First, we extract surface forms for the given en-
tities: the title of the entity’s Wikipedia article
(e.g., “Barack Obama”), the titles of all redirect
pages linking to that article (e.g., “Obama”), and
all anchor text associated with hyperlinks to the ar-
ticle within Wikipedia (e.g., “president obama”).
We then split all Wikipedia articles into sentences
and consider a sentence as a candidate if (i) the
sentence is part of either entities’ Wikipedia arti-
cle and contains a surface form of, or a link to,
the other entity; or (ii) the sentence contains sur-
face forms of, or links to, both entities in the entity
pair.
Next, we apply two sentence enrichment steps
for (i) making sentences self-contained and read-
able outside the context of the source document
and (ii) linking the sentences to entities. For (i),
we replace pronouns in candidate sentences with
the title of the entity. We apply a simple heuristic
for the people entities, inspired by (Wu and Weld,
2010):1 we count the frequency of the terms “he”
and “she” in the article for determining the gender
of the entity, and we replace the first appearance
of “he” or “she” in each sentence with the entity’s
title. We skip this step if any surface form of the
entity occurs in the sentence.
For (ii), we apply entity linking to provide links
from the sentence to additional entities (Milne and
Witten, 2008). This need arises from the fact
that not every sentence in an article contains ex-
plicit links to the entities it mentions, as Wikipedia
guidelines only allow one link to another article in
the article’s text.2 The algorithm takes a sentence
as input and iterates over n-grams that are not yet
linked to an entity. If an n-gram matches a surface
form of an entity, we establish a link between the
n-gram and the entity. We restrict our search space
to entities that are linked from within the source
article of the sentence and from within articles to
which the source article links. This way, our entity
linking method achieves high precision as almost
no disambiguation is necessary.
As an example, consider the sentence “He
gave critically acclaimed performances in the
crime thriller Seven...” on the Wikipedia page
for Brad Pitt. After applying our enrichment
steps, we obtain “Brad Pitt gave critically
acclaimed performances in the crime thriller
Seven...”, making the sentence human read-
able and link to the entities Brad Pitt and
Seven (1995 film).
</bodyText>
<subsectionHeader confidence="0.998974">
4.2 Ranking sentences
</subsectionHeader>
<bodyText confidence="0.894970357142857">
After extracting candidate sentences, we rank
them by how well they describe the relationship
of interest r between entities ei and ej. There
are many signals beyond simple term statistics that
can indicate relevance. Automatically construct-
ing a ranking model using supervised machine
learning techniques is therefore an obvious choice.
For ranking we use learning to rank (LTR) and rep-
resent each sentence with a rich set of features. Ta-
ble 1 lists the features we use. Below we provide
1We experimented with the Stanford co-reference reso-
lution system (Lee et al., 2011) and Apache OpenNLP and
found that they were not able to consistently achieve the level
of effectiveness that we require.
</bodyText>
<footnote confidence="0.908398">
2http://en.Wikipedia.org/wiki/
Wikipedia:Manual_of_Style/Linking
</footnote>
<page confidence="0.992872">
566
</page>
<figure confidence="0.936296263157895">
# Name Gloss
Textfeatures
1 Sentence length Length of s in words
2 Sum of idf Sum of IDF of terms of s in Wikipedia
3 Average idf Average IDF of terms of s in Wikipedia
4 Sentence density Lexical density of s, see Equation 1 (Lee et al., 2001)
5–8 POS fractions Fraction of verbs, nouns, adjectives, others in s (Mintz et al., 2009)
Entity features
9 #entities Total number of entities in s
10 Link to ei Whether s contains a link to the entity ei
11 Link to ej Whether s contains a link to the entity ej
12 Links to ei and ej Whether s contains links to both entities ei and ej
13 Entity first Is ei or ej the first entity in the sentence?
14 Spread of ei, ej Distance between the last match of ei and ej in s (Blanco and Zaragoza, 2010)
15–22 POS fractions left/right Fraction of verbs, nouns, adjectives, others to the left/right window of ei and ej in
s (Mintz et al., 2009)
23–25 #entities left/right/between Number of entities to the left/right or between entities ei and ej in s
26 common links ei, ej Whether s contains any common link of ei and ej
27 #common links The number of common links of ei and ej in s
28 Score common links ei, ej Sum of the scores of the common links of ei and ej in s
29–30 #common links prev/next The number of common links of ei and ej in previous/next sentence of s
Relationship features
31 Match terms(r)? Whether s contains any term in terms(r)
32 Match wordnet(r)? Whether s contains any phrase in wordnet(r)
33 Match word2vec(r)? Whether s contains any phrase in word2vec(r)
34–36 or’s Boolean OR of feature 31 and one or both of features 32 and 33
37–38 or(31, 32, 33) prev/next Boolean OR of features 31, 32, 33 for the previous/next sentence of s
39 Average word2vec(r) Average cosine similarity of phrases in word2vec(r) that are matched in s
40 Maximum word2vec(r) Maximum cosine similarity of phrases in word2vec(r) that are matched in s
41 Sum word2vec(r) Sum of cosine similarity of phrases in word2vec(r) that are matched in s
42 Score LC Lucene score of s with titles(ei, ej), terms(r), wordnet(r), word2vec(r) as
query
43 Score R-TFISF R-TFISF score of s with queries constructed as above
Source features
44 Sentence position Position of s in document from which it originates
45 From ei or ej? Does s originate from the Wikipedia article of ei or ej?
46 #(ei or ej) Number of occurrences of ei or ej in document from which s originates, inspired
by document smoothing for sentence retrieval (Murdock and Croft, 2005)
</figure>
<tableCaption confidence="0.995076">
Table 1: Features used for sentence ranking.
</tableCaption>
<bodyText confidence="0.9169685">
a brief description of the more complex ones.
Text features This feature type regards the im-
portance of the sentence s at the term level. We
compute the density of s (feature 4) as:
</bodyText>
<equation confidence="0.999339">
idf(tj) • idf(tj+1) (1)
</equation>
<bodyText confidence="0.98791424137931">
distance(tj , tj+1) 2
where K is the number of keyword terms in
s and distance(tj, tj+1) is the number of non-
keyword terms between keyword terms tj and
tj+1. We treat stop words and numbers in s as non-
keywords and the remaining terms as keywords.
Features 5–8 capture the distribution of part-of-
speech tags in the sentence.
Entity features These features partly build
on (Tsagkias et al., 2011; Meij et al., 2012) and de-
scribe the entities and are dependent on the knowl-
edge graph. Whether ei or ej is the first appearing
entity in a sentence might be an indicator of impor-
tance (feature 13). The spread of ei and ej in the
sentence (feature 14) might be an indicator of their
centrality in the sentence (Blanco and Zaragoza,
2010). Features 15–22 capture the distribution of
part-of-speech tags in the sentence in a window of
four words around ei or ej in s (Mintz et al., 2009),
complemented by the number of entities between,
to the left of, and to the right of the entity pair
(features 23–25).
We assume that two articles that have many
common articles that point to them are strongly
related (Witten and Milne, 2008). We hypothesize
that, if a sentence contains common inlinks from
ei and ej, the sentence might contain important in-
formation about their relationship. Hence, we add
whether the sentence contains a common link (fea-
</bodyText>
<equation confidence="0.997803166666667">
density(s) _
1
K • (K + 1)
n
E
j=1
</equation>
<page confidence="0.97341">
567
</page>
<listItem confidence="0.730677666666667">
ture 26) and the number of common links (feature
27) as features. We score a common link l between
ei and ej using:
</listItem>
<equation confidence="0.989784">
score(l, ei, ej) = sim(l, ei) • sim(l, ej), (2)
</equation>
<bodyText confidence="0.999874645833333">
where sim(•, •) is defined as the similarity between
two Wikipedia articles, computed using a vari-
ant of Normalized Google Distance (Witten and
Milne, 2008). Feature 28 then measures the sum
of the scores of the common links.
Previous research shows that using surrounding
sentences is beneficial for sentence retrieval (Doko
et al., 2013). We therefore consider the number of
common links in the previous and next sentence
(features 29–30).
Relationship features Feature 31 indicates
whether any of the relationship-specific terms oc-
curs in the sentence. Only matching the terms
in the relationship may have low coverage since
terms such as “spouse” may have many synonyms
and/or highly related terms, e.g., “husband” or
“married”. Therefore, we use WordNet to find
synonym phrases of r (feature 32); we refer to this
method as wordnet(r).
Alternatively, we use word embeddings to find
such similar phrases (Mikolov et al., 2013). Such
embeddings take a text corpus as input and learn
vector representations of words and phrases con-
sisting of real numbers. Given the set Vr consist-
ing of the vector representations of all the relation-
ship terms and the set V which consists of the vec-
tor representations of all the candidate phrases in
the data, we calculate the distance between a can-
didate phrase represented by a vector vi E V and
the vectors in Vr as:
where ∑Vj∈Vr vj is the element-wise sum of the
vectors in Vr and the distance between two vec-
tors v1 and v2 is measured using cosine similarity.
The candidate phrases in V are then ranked using
Equation 3 and the top-m phrases are selected, re-
sulting in features 33, 39, 40, and 41; we refer to
the ranked set of phrases that are selected using
this procedure as word2vec(r).
In addition, we employ state-of-the-art retrieval
functions and include the scores for queries that
are constructed using the entities ei and ej, the re-
lation r, wordnet(r), and word2vec(r). We use
the titles of the entity articles titles(e) to repre-
sent the entities in the query and two ranking func-
tions, Recursive TFISF (R-TFISF) and LC,3 (fea-
tures 42–43). TFISF is a sentence retrieval model
that determines the level of relevance of a sentence
s given a query q as:
</bodyText>
<equation confidence="0.999558">
R(s, q) = � log(tf t,q + 1)•
t∈q
log(tf t,s + 1) • log n + 1 (4)
(0.5 + sf t) ,
</equation>
<bodyText confidence="0.99998525">
where tf t,q and tf t,s are the number of occur-
rences of term t in the query q and the sentence
s respectively, sf t is the number of sentences in
which t appears, and n is the number of sentences
in the collection. R-TFISF is an improved ex-
tension of the TFISF method (Doko et al., 2013),
which incorporates context from neighboring sen-
tences in the ranking function:
</bodyText>
<equation confidence="0.999726">
Rc(s, q) = (1 (— p)R(s, q)+ (5)
p[Rc(sprev(s), q) + Rc(snext(s), q)],
</equation>
<bodyText confidence="0.999956714285714">
where p is a free parameter and sprev(s) and
snext(s) indicate functions to retrieve the previous
and next sentence, respectively. We use a maxi-
mum of three recursive calls.
Source features Here, we refer to features that
are dependent on the source document of the sen-
tences. We have three such features.
</bodyText>
<sectionHeader confidence="0.998495" genericHeader="method">
5 Experimental setup
</sectionHeader>
<bodyText confidence="0.999794">
In this section we describe the dataset, manual an-
notations, learning to rank algorithm, and evalu-
ation metrics that we use to answer our research
questions.
</bodyText>
<subsectionHeader confidence="0.894638">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999484625">
We draw entities and their relationships from a
proprietary knowledge graph that is created from
Wikipedia, Freebase, IMDB, and other sources,
and that is used by the Yahoo web search engine.
We focus on “people” entities and relationships
between them.4 For our experiments we need to
select a manageable set of entities, which we ob-
tain as follows. We consider a year of query logs
</bodyText>
<footnote confidence="0.9879802">
3In preliminary experiments R-TFISF and LC were the
best performing among a pool of sentence retrieval methods.
4Note that, except for the co-reference resolution step de-
scribed in Section 4.1, our method does not depend on this
restriction.
</footnote>
<equation confidence="0.996090333333333">
distance(vi, V ) = cos � �
vi, E vj � ,(3)
Vj∈Vr
</equation>
<page confidence="0.980383">
568
</page>
<bodyText confidence="0.999968486486487">
from a large commercial search engine, count the
number of times a user clicks on a Wikipedia ar-
ticle of an entity in the results page and perform
stratified sampling of entities according to this dis-
tribution. As we are bounded by limited resources
for our manual assessments, we sample 1476 en-
tity pairs that together with nine unique relation-
ship types form our experimental dataset.
We use an English Wikipedia dump dated July
8, 2013, containing approximately 4M articles, of
which 50 638 belong to “people” entities that are
also in our knowledge graph. We extract sentences
using the approach described in Section 4.1, re-
sulting in 36 823 candidate sentences for our enti-
ties. On average we have 24.94 sentences per en-
tity pair (maximum 423 and minimum 0). Because
of the large variance, it is not feasible to obtain ex-
haustive annotations for all sentences. We rank the
sentences using R-TFISF and keep the top-10 sen-
tences per entity pair for annotation. This results
in a total of 5 689 sentences.
Five human annotators provided relevance judg-
ments, manually judging sentences based on how
well they describe the relationship for an entity
pair, for which we use a five-level graded rele-
vance scale (perfect, excellent, good, fair, bad).5
Of all relevance grades 8.1% is perfect, 15.69%
excellent, 19.98% good, 8.05% fair, and 48.15%
bad. Out of 1476 entity pairs, 1093 have at least
one sentence annotated as fair. As is common in
information retrieval evaluation, we discard entity
pairs that have only “bad” sentences. We examine
the difficulty of the task for human annotators by
measuring inter-annotator agreement on a subset
of 105 sentences that are judged by 3 annotators.
Fleiss’ kappa is k = 0.449, which is considered to
be moderate agreement.
</bodyText>
<subsectionHeader confidence="0.997503">
5.2 Machine learning
</subsectionHeader>
<bodyText confidence="0.999978285714286">
For ranking sentences we use a Random Forest
(RF) classifier (Breiman, 2001).6 We set the num-
ber of iterations to 300 and the sampling rate to
0.3. Experiments with varying these two parame-
ters did not show any significant differences. We
also tried several feature normalization methods,
none of them being able to significantly outper-
</bodyText>
<footnote confidence="0.972146666666667">
5https://github.com/nickvosk/acl2015-
dataset-learning-to-explain-entity-
relationships
6In preliminary experiments, we contrasted RF with gra-
dient boosted regression trees and LambdaMART and found
that RF consistently outperformed other methods.
</footnote>
<table confidence="0.999890666666667">
Baseline NDCG@1 NDCG@10 ERR@1 ERR@10
B1 0.7508 0.8961 0.3577 0.4531
B2 0.7511 0.8958 0.3584 0.4530
B3 0.7595 0.8997 0.3696 0.4600
B4 0.7767 0.9070 0.3774 0.4672
B5 0.7801 0.9093 0.3787 0.4682
</table>
<tableCaption confidence="0.957984">
Table 2: Results for five baseline variants. See text
</tableCaption>
<bodyText confidence="0.957303111111111">
for their description and significant differences.
form the runs without feature normalization.
We obtain POS tags using the Stanford part-of-
speech tagger and filter out a standard list of 33
English stopwords. For the word embeddings we
use word2vec and train our model on all text in
Wikipedia using negative sampling and the con-
tinuous bag of words architecture. We set the size
of the phrase vectors to 500 and m = 30.
</bodyText>
<subsectionHeader confidence="0.983266">
5.3 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999982923076923">
We employ two main evaluation metrics in our
experiments, NDCG (J¨arvelin and Kek¨al¨ainen,
2002) and ERR (Chapelle et al., 2009). The for-
mer measures the total accumulated gain from
the top of the ranking that is discounted at lower
ranks and is normalized by the ideal cumulative
gain. The latter models user behavior and mea-
sures the expected reciprocal rank at which a user
will stop her search. We consider these ranking-
based graded evaluation metrics at two cut-off
points: position 1, corresponding to showing a sin-
gle sentence to a user, and 10, which accounts for
users who might look at more results. We report
on NDCG@1, NDCG@10, ERR@1, ERR@10,
and Exc@1, which indicates whether we have an
“excellent” or “perfect” sentence at the top of the
ranking. Likewise, Per@1 indicates whether we
have a “perfect” sentence at the top of the ranking
(not all entity pairs have an excellent or a perfect
sentence).
We perform 5-fold cross validation and test for
statistical significance using a paired two-tailed t-
test. We depict a significant difference in perfor-
mance for p &lt; 0.01 with ▲ (gain) and ▼ (loss) and
for p &lt; 0.05 with △ (gain) and v (loss). Boldface
indicates the best score for a metric.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="evaluation">
6 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999851">
We compare the performance of typical docu-
ment retrieval models and state-of-the-art sentence
retrieval models in order to answer RQ1. We
consider five sentence retrieval models: Lucene
ranking (LC), language modeling with Dirichlet
</bodyText>
<page confidence="0.996625">
569
</page>
<table confidence="0.999766333333333">
Has one # pairs # sentences Method NDCG@1 NDCG@10 ERR@1 ERR@10 Exc@1 Per@1
fair 1093 4435 B5 0.7801 0.9093 0.3787 0.4682 – –
LTR 0.8489▲ 0.9375▲ 0.4242▲ 0.4980▲ – –
good 1038 4 285 B5 0.7742 0.9078 0.3958 0.4894 – –
LTR 0.8486▲ 0.9374▲ 0.4438▲ 0.5208▲ – –
excellent 752 3 387 B5 0.7455 0.8999 0.4858 0.5981 0.7314 –
LTR 0.8372▲ 0.9340▲ 0.5500▲ 0.6391▲ 0.8298▲ –
perfect 339 1687 B5 0.7082 0.8805 0.6639 0.7878 0.7729 0.6136
LTR 0.8150▲ 0.9245▲ 0.7640▲ 0.8518▲ 0.8909▲ 0.7227▲
</table>
<tableCaption confidence="0.999324">
Table 3: Results for the best baseline (B5) and the learning to rank method (LTR).
</tableCaption>
<bodyText confidence="0.987670909090909">
smoothing (LM), BM25, TFISF, and Recursive
TF-ISF (R-TFISF). We follow related work and
set p = 0.1 for R-TFISF, k = 1 and b = 0 for BM25
and p = 250 for LM (Fern´andez et al., 2011).
In our experiments, a query q is constructed us-
ing various combinations of surface forms of the
two entities ei and ej and the relationship r. Each
entity in the entity pair can be represented by its
title, the titles of any redirect pages pointing to
the entity’s article, the n-grams used as anchors in
Wikipedia to link to the article of the entity, or the
union of them all. The relationship r can be repre-
sented by the terms in the relationship, synonyms
in wordnet(r), or by phrases in word2vec(r).
First, we fix the way we represent r. Base-
line B1 does not include any representation of r
in the query. B2 includes the relationship terms
of r, while B3 includes the relationship terms of r
and the synonyms in wordnet(r). B4 includes the
terms of r and the phrases in word2vec(r), and B5
includes the relationship terms of r, the synonyms
in wordnet(r) and the phrases in word2vec(r).
Combining these variations with the entity repre-
sentations, we find that all combinations that use
the titles as representation and R-TFISF as the
retrieval function outperform all other combina-
tions.7 This can be explained by the fact that titles
are least ambiguous, thus reducing the possibility
of accidentally referring to other entities. BM25
and LC perform almost as well as R-TFISF, with
only insignificant differences in performance.
Table 2 shows the best performing combination
of each baseline, i.e., varying the representation
of r and using titles and R-TFISF. B4 and B5
are the best performing baselines, suggesting that
word2vec(r) and wordnet(r) are beneficial. B5
significantly outperforms all baselines except B4.
We also experiment with a supervised combina-
7We omit a full table of results due to space constraints.
tion of the baseline rankers using LTR. Here, we
consider each baseline ranker as a separate feature
and train a ranking model. The trained model is
not able to outperform the best individual baseline,
however.
</bodyText>
<subsectionHeader confidence="0.999767">
6.1 Learning to rank sentences
</subsectionHeader>
<bodyText confidence="0.999982461538462">
Next, we provide the results of our method us-
ing the features described in Section 4.2, exploring
whether our learning to rank (LTR) approach im-
proves over sentence retrieval models (RQ2). We
compare an LTR model using Table 1’s features
against the best baseline (B5). Table 3 shows the
results. Each group in the table contains the results
for the entity pairs that have at least one candidate
sentence of that relevance grade for B5 and LTR.
We find that LTR significantly outperforms B5
by a large margin. The absolute performance dif-
ference between LTR and B5 becomes larger for
all metrics as we move from “fair” to “perfect,”
which shows that LTR is more robust than the
baseline for entity pairs that have at least one high
quality candidate sentence. LTR ranks the best
possible sentence at the top of the ranking for
-83% of the cases for entity pairs that contain an
“excellent” sentence and for -72% of the cases for
entity pairs that contain a “perfect” sentence.
Note that, as indicated in Section 5.1, we dis-
card entity pairs that have only “bad” sentences
in our experiments. For the sake of complete-
ness, we report on the results for all entity pairs in
our dataset—including those without any relevant
sentences—in Table 4.
</bodyText>
<subsectionHeader confidence="0.993672">
6.2 Relationship-dependent models
</subsectionHeader>
<bodyText confidence="0.999917">
Relevant sentences may have different properties
for different relationship types. For example, a
sentence describing two entities being partners
would have a different form than one describing
that two entities costar in a movie. A similar
</bodyText>
<page confidence="0.992497">
570
</page>
<table confidence="0.988222666666667">
Has one # pairs # sentences Method NDCG@1 NDCG@10 ERR@1 ERR@10 Exc@1 Per@1
- 1476 5 689 B5 0.5776 0.6733 0.2804 0.3467 – –
LTR 0.6285▲ 0.6940▲ 0.3155▲ 0.3694▲ – –
</table>
<tableCaption confidence="0.9628685">
Table 4: Results for the best baseline (B5) and the learning to rank method (LTR), using all entity pairs
in the dataset, including those without any relevant sentences.
</tableCaption>
<table confidence="0.99991475">
Relationship # pairs # sentences NDCG@1 NDCG@10 ERR@1 ERR@10
(MovieActor, CoCastsWith, MovieActor) 410 1403 0.8604 0.9436 0.3809 0.4546
(TvActor, CoCastsWith, TvActor) 210 626 0.8729 0.9482 0.3271 0.3845
(MovieActor, IsDirectedBy, MovieDirector) 112 492 0.8795 0.9396 0.4709 0.5261
(MovieDirector, Directs, MovieActor)
(Person, isChildOf, Person) 108 716 0.8428 0.9081 0.6395 0.7136
(Person, isParentOf, Person)
(Person, isPartnerOf, Person) 155 877 0.8623 0.9441 0.6153 0.6939
(Person, isSpouseOf , Person)
(Athlete, PlaysSameSportTeamAs, Athlete) 98 321 0.8787 0.9535 0.3350 0.3996
Average results over all data 1093 4 435 0.8661 0.9395 0.4615 0.5287
LTR (Table 3; fair) 0.8489 0.9375 0.4242 0.4980
</table>
<tableCaption confidence="0.998269">
Table 5: Results for relationship-dependent models. Similar relationships are grouped together.
</tableCaption>
<bodyText confidence="0.999935862068965">
idea was investigated in the context of QA for as-
sociating question and answer types (Yao et al.,
2013). To answer (RQ3) we examine whether
learning a relationship-dependent model improves
over learning a single model for all types. We split
our dataset per relationship type and train a model
per type using 5-fold cross-validation within each.
Table 5 shows the results.8 Our method is ro-
bust across different relationships in terms of
NDCG. However, we observe some variation in
ERR as this metric is more sensitive to the distri-
bution of relevant items than NDCG—the distri-
bution over relevance grades varies per relation-
ship type. For example, it is much more likely to
find candidate sentences that have a high relevance
grade for (Person, isSpouseOf , Person) than for
(Athlete, PlaysSameSportTeamAs, Athlete) in
our dataset. We plan to address this issue by ex-
ploring other corpora in the future.
The second-to-last row in Table 5 shows the av-
eraged results over the different relationship types,
which is a significant improvement over LTR at
p &lt; 0.01 for all metrics. This method ranks the
best possible sentence at the top of the ranking for
-85% of the cases for entity pairs that contain an
“excellent” sentence (-2% absolute improvement
over LTR) and for -75% of the cases for entity
pairs that contain a “perfect” sentence (-3% abso-
lute improvement over LTR).
</bodyText>
<subsectionHeader confidence="0.7379345">
8We omit Exc@1 and Per@1 due to space constraints.
6.3 Feature type analysis
</subsectionHeader>
<bodyText confidence="0.999852125">
Next, we analyze the impact of the feature types.
Table 6 shows how performance varies when re-
moving one feature type at a time from the full
feature set. Relationship type features are the most
important, although entity type features are impor-
tant as well. This indicates that introducing fea-
tures based on entities identified in the sentences
and the relationship is beneficial for this task. Fur-
thermore, the limited dependency on the source
feature type indicates that our method might be
able to generalize in other domains. Finally, text
type features do contribute to retrieval effective-
ness, although not significantly. Note that calcu-
lating the sentence features is straightforward, as
none of our features requires heavy linguistic anal-
ysis.
</bodyText>
<table confidence="0.999849666666667">
Features NDCG@1 NDCG@10 ERR@1 ERR@10
All 0.8661 0.9395 0.4615 0.5287
All�text 0.8620 0.9372 0.4606 0.5274
All source 0.8598 0.9372 0.4582 0.5261
All�entity 0.8421▽ 0.9282▼ 0.4497 0.5202▽
All�relation 0.8183▼ 0.9201▼ 0.4352▼ 0.5112▼
</table>
<tableCaption confidence="0.98031">
Table 6: Results using relationship-dependent
models, removing individual feature types.
</tableCaption>
<subsectionHeader confidence="0.995201">
6.4 Error analysis
</subsectionHeader>
<bodyText confidence="0.999854333333333">
When looking at errors made by the system, we
find that some are due to the fact that entity pairs
might have more than one relationship (e.g., ac-
</bodyText>
<page confidence="0.994394">
571
</page>
<bodyText confidence="0.999946419354839">
tors that costar in movies also being partners) but
the selected sentence covers only one of the re-
lationships.9 For example, Liza Minnelli is
the daughter of Judy Garland, but they have
also costarred in a movie, which is the relationship
of interest. The model ranks the sentence “Liza
Minnelli is the daughter of singer and actress Judy
Garland...” at the top, while the most relevant
sentence is: “Judy Garland performed at the Lon-
don Palladium with her then 18-year-old daughter
Liza Minnelli in November 1964.”
Sentences that contain the relationship in which
we are interested, but for which this cannot be
directly inferred, are another source of error.
Consider, for example, the following sentence,
which explains director Christopher Nolan
directed actor Christian Bale: “Jackman
starred in the 2006 film The Prestige, directed by
Christopher Nolan and costarring Christian Bale,
Michael Caine, and Scarlett Johansson”. Even
though the sentence contains the relationship of in-
terest, it focuses on actor Hugh Jackman. The
sentence “In 2004, after completing filming for
The Machinist, Bale won the coveted role of Bat-
man and his alter ego Bruce Wayne in Christopher
Nolan’s Batman Begins...”, in contrast, refers to
the two entities and the relationship of interest di-
rectly, resulting in a higher relevance grade. Our
method, however, ranks the first sentence on top,
as it contains more phrases that refer to the rela-
tionship.
</bodyText>
<sectionHeader confidence="0.997273" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9991576">
We have presented a method for explaining rela-
tionships between knowledge graph entities with
human-readable descriptions. We first extract and
enrich sentences that refer to an entity pair and
then rank the sentences according to how well
they describe the relationship. For ranking, we
use learning to rank with a diverse set of fea-
tures. Evaluation on a manually annotated dataset
of “people” entities shows that our method sig-
nificantly outperforms state-of-the-art sentence re-
trieval models for this task. Experimental results
also show that using relationship-dependent mod-
els is beneficial.
In future work we aim to evaluate how our
method performs on entities and relationships of
</bodyText>
<footnote confidence="0.9717215">
9The annotators marked sentences that do not refer to the
relationship of interest as “bad” but indicated whether they
describe another relationship or not. We plan to account for
such cases in future work.
</footnote>
<bodyText confidence="0.999951304347826">
any type and popularity, including tail entities and
miscellaneous relationships. We also want to in-
vestigate moving beyond Wikipedia and extract
candidate sentences from documents that are not
related to the knowledge graph, such as web pages
or news articles. Employing such documents also
implies an investigation into more advanced co-
reference resolution methods.
Our analysis showed that sentences may cover
different relationships between entities or differ-
ent aspects of a single relationship—we aim to ac-
count for such cases in follow-up work. Further-
more, sentences may contain unnecessary infor-
mation for explaining the relation of interest be-
tween two entities. Especially when we want to
show the obtained results to end users, we may
need to apply further processing of the sentences
to improve their quality and readability. We would
like to explore sentence compression techniques
to address this. Finally, relationships between en-
tities have an inherit temporal nature and we aim
to explore ways to explain entity relationships and
their changes over time.
</bodyText>
<sectionHeader confidence="0.997476" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999409">
This research was partially supported by the Eu-
ropean Community’s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
nr 312827 (VOX-Pol), the Netherlands Organi-
sation for Scientific Research (NWO) under pro-
ject nrs 727.011.005, 612.001.116, HOR-11-10,
640.006.013, 612.066.930, CI-14-25, SH-322-15,
Amsterdam Data Science, the Dutch national pro-
gram COMMIT, the ESF Research Network Pro-
gram ELIAS, the Elite Network Shifts project
funded by the Royal Dutch Academy of Sciences
(KNAW), the Netherlands eScience Center under
project nr 027.012.105, the Yahoo! Faculty Re-
search and Engagement Program, the Microsoft
Research PhD program, and the HPC Fund.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9848652">
Arvind Agarwal, Hema Raghavan, Karthik Subbian,
Prem Melville, Richard D. Lawrence, David C.
Gondek, and James Fan. 2012. Learning to rank for
robust question answering. In Proceedings of the
21st ACM international conference on Information
and knowledge management, pages 833–842. ACM.
James Allan, Courtney Wade, and Alvaro Bolivar.
2003. Retrieval and novelty detection at the sen-
tence level. In Proceedings of the 26th annual inter-
national ACM SIGIR conference on Research and
</reference>
<page confidence="0.987232">
572
</page>
<reference confidence="0.99919336036036">
development in informaion retrieval, pages 314–
321. ACM.
Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al.
1999. Modern information retrieval, volume 463.
ACM press New York.
Roi Blanco and Hugo Zaragoza. 2010. Finding sup-
port sentences for entities. In Proceedings of the
33rd international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 339–346. ACM.
Roi Blanco, Berkant Barla Cambazoglu, Peter Mika,
and Nicolas Torzec. 2013. Entity recommendations
in web search. In The Semantic Web–ISWC 2013,
pages 33–48. Springer.
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5–32.
Christopher J.C. Burges, Krysta Marie Svore, Paul N.
Bennett, Andrzej Pastusiak, and Qiang Wu. 2011.
Learning to rank using an ensemble of lambda-
gradient models. In Yahoo! Learning to Rank Chal-
lenge, pages 25–35.
Olivier Chapelle, Donald Metzler, Ya Zhang, and
Pierre Grinspan. 2009. Expected reciprocal rank for
graded relevance. In Proceedings of the 18th ACM
conference on Information and knowledge manage-
ment, pages 621–630. ACM.
Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew
McCallum, Tom Mitchell, Kamal Nigam, and Se´an
Slattery. 2000. Learning to construct knowledge
bases from the world wide web. Artificial Intelli-
gence, 118(1–2):69–113.
Alen Doko, Maja ˇStula, and Darko Stipaniˇcev. 2013.
A recursive TF-ISF based sentence retrieval method
with local context. International Journal of Ma-
chine Learning and Computing, 3(2):195–200.
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowl-
edge vault: A web-scale approach to probabilistic
knowledge fusion. In Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’14, pages 601–
610. ACM.
James Fan, Raphael Hoffman, Aditya Kalyanpur, Se-
bastian Riedel, Fabian Suchanek, and Pratim Partha
Talukdar, 2012. Proceedings of the Joint Workshop
on Automatic Knowledge Base Construction and
Web-scale Knowledge Extraction (AKBC-WEKEX),
chapter Proceedings of the Joint Workshop on Auto-
matic Knowledge Base Construction and Web-scale
Knowledge Extraction (AKBC-WEKEX). Associa-
tion for Computational Linguistics.
Lujun Fang, Anish Das Sarma, Cong Yu, and Philip
Bohannon. 2011. Rex: explaining relationships be-
tween entity pairs. Proceedings of the VLDB En-
dowment, 5(3):241–252.
Ronald T Fern´andez, David E. Losada, and Leif Az-
zopardi. 2011. Extending the language model-
ing framework for sentence retrieval to include local
context. Information Retrieval, 14(4):355–389.
Lynette Hirschman and Robert Gaizauskas. 2001. Nat-
ural language question answering: the view from
here. Natural Language Engineering, 7(04):275–
300.
Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cu-
mulated gain-based evaluation of IR techniques.
ACM Transactions on Information Systems (TOIS),
20(4):422–446.
Gary Geunbae Lee, Jungyun Seo, Seungwoo Lee, Han-
min Jung, Bong-Hyun Cho, Changki Lee, Byung-
Kwan Kwak, Jeongwon Cha, Dongseok Kim,
JooHui An, et al. 2001. SiteQ: Engineering high
performance QA system using lexico-semantic pat-
tern matching and shallow NLP. In TREC.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28–34. Association for Computational
Linguistics.
David E. Losada. 2008. A study of statistical query
expansion strategies for sentence retrieval. In Pro-
ceedings of the SIGIR 2008 Workshop on Focused
Retrieval, pages 37–44.
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts. In
Proceedings of the fifth ACM international confer-
ence on Web search and data mining, pages 563–
572. ACM.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
David Milne and Ian H. Witten. 2008. Learning to link
with Wikipedia. In Proceedings of the 17th ACM
conference on Information and knowledge manage-
ment, pages 509–518. ACM.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.
Vanessa Murdock and W. Bruce Croft. 2005. A trans-
lation model for sentence retrieval. In Proceedings
of the conference on Human Language Technology
</reference>
<page confidence="0.986799">
573
</page>
<reference confidence="0.999430842105263">
and Empirical Methods in Natural Language Pro-
cessing, pages 684–691. Association for Computa-
tional Linguistics.
Vanessa Graham Murdock. 2006. Aspects of Sentence
Retrieval. Ph.D. thesis, University of Massachusetts
Amherst.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351–383.
Manos Tsagkias, Maarten de Rijke, and Wouter
Weerkamp. 2011. Linking online news and social
media. In WSDM 2011: Fourth ACM International
Conference on Web Search and Data Mining. ACM,
February.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2013.
Ian Witten and David Milne. 2008. An effective, low-
cost measure of semantic relatedness obtained from
wikipedia links. In Proceeding of AAAI Workshop
on Wikipedia and Artificial Intelligence: an Evolv-
ing Synergy, AAAI Press, Chicago, USA, pages 25–
30.
Fei Wu and Daniel S Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 118–127. Association for
Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, and Peter Clark.
2013. Automatic coupling of answer extraction and
information retrieval. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 159–
165. Association for Computational Linguistics.
</reference>
<page confidence="0.998282">
574
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.080393">
<title confidence="0.999592">Learning to Explain Entity Relationships in Knowledge Graphs</title>
<author confidence="0.897215">Meij</author>
<affiliation confidence="0.992087">University of Amsterdam Yahoo Labs, London</affiliation>
<email confidence="0.922928">n.voskarides@uva.nlemeij@yahoo-inc.com</email>
<author confidence="0.648996">Manos</author>
<email confidence="0.7092025">904Labs,manos@904labs.com</email>
<author confidence="0.887157">Maarten de</author>
<affiliation confidence="0.990307">University of</affiliation>
<email confidence="0.913438">derijke@uva.nl</email>
<note confidence="0.365107">Wouter 904Labs,</note>
<email confidence="0.989011">wouter@904labs.com</email>
<abstract confidence="0.999560266666667">We study the problem of explaining relationships between pairs of knowledge graph entities with human-readable descriptions. Our method extracts and enriches sentences that refer to an entity pair from a corpus and ranks the sentences according to how well they describe the relationship between the entities. We model this task as a learning to rank problem for sentences and employ a rich set of features. When evaluated on a large set of manually annotated sentences, we find that our method significantly improves over state-of-the-art baseline models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arvind Agarwal</author>
<author>Hema Raghavan</author>
<author>Karthik Subbian</author>
<author>Prem Melville</author>
<author>Richard D Lawrence</author>
<author>David C Gondek</author>
<author>James Fan</author>
</authors>
<title>Learning to rank for robust question answering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>833--842</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6880" citStr="Agarwal et al., 2012" startWordPosition="1091" endWordPosition="1094">ge modeling (Losada, 2008)) and we therefore include it as a baseline. Sentences that are suitable for explaining relationships can have attributes that are important for ranking but cannot be captured by term-based retrieval models. One way to combine a wide range of ranking features is learning to rank (LTR). Recent years have witnessed a rapid increase in the work on learning to rank, and it has proven to be a very successful method for combining large numbers of ranking features, for web search, but also other information retrieval applications (Burges et al., 2011; Surdeanu et al., 2011; Agarwal et al., 2012). We use learning to rank and represent each sentence with a set of features that aim to capture different dimensions of the sentence. Question answering (QA) is the task of providing direct and concise answers to questions formed in natural language (Hirschman and Gaizauskas, 2001). QA can be regarded as a similar task to ours, assuming that the combination of entity pair and relationship form the “question” and that the “answer” is the sentence describing the relationship of interest. Even though we do not follow the QA paradigm in this paper, some of the features we use are inspired by QA s</context>
</contexts>
<marker>Agarwal, Raghavan, Subbian, Melville, Lawrence, Gondek, Fan, 2012</marker>
<rawString>Arvind Agarwal, Hema Raghavan, Karthik Subbian, Prem Melville, Richard D. Lawrence, David C. Gondek, and James Fan. 2012. Learning to rank for robust question answering. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 833–842. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Courtney Wade</author>
<author>Alvaro Bolivar</author>
</authors>
<title>Retrieval and novelty detection at the sentence level.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>314--321</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5896" citStr="Allan et al., 2003" startWordPosition="923" endWordPosition="926">etween two entities, rather than a query. Fang et al. (2011) explore the generation of a ranked list of knowledge base relationships for an entity pair. Instead, we try to select sentences that describe a particular relationship, assuming that this is given. Our approach builds on sentence retrieval, where one retrieves sentences rather than documents that answer an information need. Document retrieval models such as tf-idf, BM25, and language modeling (Baeza-Yates et al., 1999) have been extended to tackle sentence retrieval. Three of the most successful sentence retrieval methods are TFISF (Allan et al., 2003), which is a variant of the vector space model with tf-idf weighting, language modeling with local context (Murdock, 2006; Fern´andez et al., 2011), and a recursive version of TFISF that accounts for local context (Doko et al., 2013). TFISF is very competitive compared to document retrieval models tuned specifically for sentence retrieval (e.g., BM25 and language modeling (Losada, 2008)) and we therefore include it as a baseline. Sentences that are suitable for explaining relationships can have attributes that are important for ranking but cannot be captured by term-based retrieval models. One</context>
</contexts>
<marker>Allan, Wade, Bolivar, 2003</marker>
<rawString>James Allan, Courtney Wade, and Alvaro Bolivar. 2003. Retrieval and novelty detection at the sentence level. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 314– 321. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern information retrieval, volume 463.</title>
<date>1999</date>
<publisher>ACM press</publisher>
<location>New York.</location>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999. Modern information retrieval, volume 463. ACM press New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Blanco</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Finding support sentences for entities.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>339--346</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5051" citStr="Blanco and Zaragoza (2010)" startWordPosition="784" endWordPosition="787">by using relationship-dependent models instead of a relationship-independent one (RQ3). We complement these research questions with an error and feature analysis. Our main contributions are a robust and effective method for explaining entity relationships, detailed insights into the performance of our method and features, and a manually annotated dataset. 2 Related Work We combine ideas from sentence retrieval, learning to rank, and question answering to address the task of explaining relationships between entities. Previous work that is closest to the task we address in this paper is that of Blanco and Zaragoza (2010) and Fang et al. (2011). First, Blanco and Zaragoza (2010) focus on finding and ranking sentences that explain the relationship between an entity and a query. Our work is different in that we want to explain the relationship between two entities, rather than a query. Fang et al. (2011) explore the generation of a ranked list of knowledge base relationships for an entity pair. Instead, we try to select sentences that describe a particular relationship, assuming that this is given. Our approach builds on sentence retrieval, where one retrieves sentences rather than documents that answer an infor</context>
<context position="13501" citStr="Blanco and Zaragoza, 2010" startWordPosition="2232" endWordPosition="2235">dia 3 Average idf Average IDF of terms of s in Wikipedia 4 Sentence density Lexical density of s, see Equation 1 (Lee et al., 2001) 5–8 POS fractions Fraction of verbs, nouns, adjectives, others in s (Mintz et al., 2009) Entity features 9 #entities Total number of entities in s 10 Link to ei Whether s contains a link to the entity ei 11 Link to ej Whether s contains a link to the entity ej 12 Links to ei and ej Whether s contains links to both entities ei and ej 13 Entity first Is ei or ej the first entity in the sentence? 14 Spread of ei, ej Distance between the last match of ei and ej in s (Blanco and Zaragoza, 2010) 15–22 POS fractions left/right Fraction of verbs, nouns, adjectives, others to the left/right window of ei and ej in s (Mintz et al., 2009) 23–25 #entities left/right/between Number of entities to the left/right or between entities ei and ej in s 26 common links ei, ej Whether s contains any common link of ei and ej 27 #common links The number of common links of ei and ej in s 28 Score common links ei, ej Sum of the scores of the common links of ei and ej in s 29–30 #common links prev/next The number of common links of ei and ej in previous/next sentence of s Relationship features 31 Match te</context>
<context position="16217" citStr="Blanco and Zaragoza, 2010" startWordPosition="2715" endWordPosition="2718"> nonkeyword terms between keyword terms tj and tj+1. We treat stop words and numbers in s as nonkeywords and the remaining terms as keywords. Features 5–8 capture the distribution of part-ofspeech tags in the sentence. Entity features These features partly build on (Tsagkias et al., 2011; Meij et al., 2012) and describe the entities and are dependent on the knowledge graph. Whether ei or ej is the first appearing entity in a sentence might be an indicator of importance (feature 13). The spread of ei and ej in the sentence (feature 14) might be an indicator of their centrality in the sentence (Blanco and Zaragoza, 2010). Features 15–22 capture the distribution of part-of-speech tags in the sentence in a window of four words around ei or ej in s (Mintz et al., 2009), complemented by the number of entities between, to the left of, and to the right of the entity pair (features 23–25). We assume that two articles that have many common articles that point to them are strongly related (Witten and Milne, 2008). We hypothesize that, if a sentence contains common inlinks from ei and ej, the sentence might contain important information about their relationship. Hence, we add whether the sentence contains a common link</context>
</contexts>
<marker>Blanco, Zaragoza, 2010</marker>
<rawString>Roi Blanco and Hugo Zaragoza. 2010. Finding support sentences for entities. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 339–346. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Blanco</author>
<author>Berkant Barla Cambazoglu</author>
<author>Peter Mika</author>
<author>Nicolas Torzec</author>
</authors>
<title>Entity recommendations in web search.</title>
<date>2013</date>
<booktitle>In The Semantic Web–ISWC 2013,</booktitle>
<pages>33--48</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1945" citStr="Blanco et al., 2013" startWordPosition="294" endWordPosition="297">as actor or movie). Although there is a growing interest in automatically constructing knowledge graphs, e.g., from unstructured web data (Weston et al., 2013; Craven et al., 2000; Fan et al., 2012), the problem of providing evidence on why two entities are related in a knowledge graph remains largely unaddressed. Extracting and presenting evidence for linking two entities, however, is an important aspect of knowledge graphs, as it can enforce trust between the user and a search engine, which in turn can improve long-term user engagement, e.g., in the context of related entity recommendation (Blanco et al., 2013). Although knowledge ∗This work was carried out while this author was visiting Yahoo Labs. graphs exist that provide this functionality to a certain degree (e.g., when hovering over Google’s suggested entities, see Figure 1), to the best of our knowledge there is no previously published research on methods for entity relationship explanation. Figure 1: Part of Google’s search result page for the query “barack obama”. When hovering over the related entity “Michelle Obama”, an explanation of the relationship between her and “Barack Obama” is shown. In this paper we propose a method for explainin</context>
</contexts>
<marker>Blanco, Cambazoglu, Mika, Torzec, 2013</marker>
<rawString>Roi Blanco, Berkant Barla Cambazoglu, Peter Mika, and Nicolas Torzec. 2013. Entity recommendations in web search. In The Semantic Web–ISWC 2013, pages 33–48. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<journal>Mach. Learn.,</journal>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="22788" citStr="Breiman, 2001" startWordPosition="3857" endWordPosition="3858">Of all relevance grades 8.1% is perfect, 15.69% excellent, 19.98% good, 8.05% fair, and 48.15% bad. Out of 1476 entity pairs, 1093 have at least one sentence annotated as fair. As is common in information retrieval evaluation, we discard entity pairs that have only “bad” sentences. We examine the difficulty of the task for human annotators by measuring inter-annotator agreement on a subset of 105 sentences that are judged by 3 annotators. Fleiss’ kappa is k = 0.449, which is considered to be moderate agreement. 5.2 Machine learning For ranking sentences we use a Random Forest (RF) classifier (Breiman, 2001).6 We set the number of iterations to 300 and the sampling rate to 0.3. Experiments with varying these two parameters did not show any significant differences. We also tried several feature normalization methods, none of them being able to significantly outper5https://github.com/nickvosk/acl2015- dataset-learning-to-explain-entityrelationships 6In preliminary experiments, we contrasted RF with gradient boosted regression trees and LambdaMART and found that RF consistently outperformed other methods. Baseline NDCG@1 NDCG@10 ERR@1 ERR@10 B1 0.7508 0.8961 0.3577 0.4531 B2 0.7511 0.8958 0.3584 0.4</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Mach. Learn., 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher J C Burges</author>
<author>Krysta Marie Svore</author>
<author>Paul N Bennett</author>
<author>Andrzej Pastusiak</author>
<author>Qiang Wu</author>
</authors>
<title>Learning to rank using an ensemble of lambdagradient models. In Yahoo! Learning to Rank Challenge,</title>
<date>2011</date>
<pages>25--35</pages>
<contexts>
<context position="6834" citStr="Burges et al., 2011" startWordPosition="1083" endWordPosition="1086">or sentence retrieval (e.g., BM25 and language modeling (Losada, 2008)) and we therefore include it as a baseline. Sentences that are suitable for explaining relationships can have attributes that are important for ranking but cannot be captured by term-based retrieval models. One way to combine a wide range of ranking features is learning to rank (LTR). Recent years have witnessed a rapid increase in the work on learning to rank, and it has proven to be a very successful method for combining large numbers of ranking features, for web search, but also other information retrieval applications (Burges et al., 2011; Surdeanu et al., 2011; Agarwal et al., 2012). We use learning to rank and represent each sentence with a set of features that aim to capture different dimensions of the sentence. Question answering (QA) is the task of providing direct and concise answers to questions formed in natural language (Hirschman and Gaizauskas, 2001). QA can be regarded as a similar task to ours, assuming that the combination of entity pair and relationship form the “question” and that the “answer” is the sentence describing the relationship of interest. Even though we do not follow the QA paradigm in this paper, so</context>
</contexts>
<marker>Burges, Svore, Bennett, Pastusiak, Wu, 2011</marker>
<rawString>Christopher J.C. Burges, Krysta Marie Svore, Paul N. Bennett, Andrzej Pastusiak, and Qiang Wu. 2011. Learning to rank using an ensemble of lambdagradient models. In Yahoo! Learning to Rank Challenge, pages 25–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Donald Metzler</author>
<author>Ya Zhang</author>
<author>Pierre Grinspan</author>
</authors>
<title>Expected reciprocal rank for graded relevance.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management,</booktitle>
<pages>621--630</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="24114" citStr="Chapelle et al., 2009" startWordPosition="4058" endWordPosition="4061">: Results for five baseline variants. See text for their description and significant differences. form the runs without feature normalization. We obtain POS tags using the Stanford part-ofspeech tagger and filter out a standard list of 33 English stopwords. For the word embeddings we use word2vec and train our model on all text in Wikipedia using negative sampling and the continuous bag of words architecture. We set the size of the phrase vectors to 500 and m = 30. 5.3 Evaluation metrics We employ two main evaluation metrics in our experiments, NDCG (J¨arvelin and Kek¨al¨ainen, 2002) and ERR (Chapelle et al., 2009). The former measures the total accumulated gain from the top of the ranking that is discounted at lower ranks and is normalized by the ideal cumulative gain. The latter models user behavior and measures the expected reciprocal rank at which a user will stop her search. We consider these rankingbased graded evaluation metrics at two cut-off points: position 1, corresponding to showing a single sentence to a user, and 10, which accounts for users who might look at more results. We report on NDCG@1, NDCG@10, ERR@1, ERR@10, and Exc@1, which indicates whether we have an “excellent” or “perfect” se</context>
</contexts>
<marker>Chapelle, Metzler, Zhang, Grinspan, 2009</marker>
<rawString>Olivier Chapelle, Donald Metzler, Ya Zhang, and Pierre Grinspan. 2009. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 621–630. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Dan DiPasquo</author>
<author>Dayne Freitag</author>
<author>Andrew McCallum</author>
<author>Tom Mitchell</author>
<author>Kamal Nigam</author>
<author>Se´an Slattery</author>
</authors>
<title>Learning to construct knowledge bases from the world wide web.</title>
<date>2000</date>
<journal>Artificial Intelligence,</journal>
<pages>118--1</pages>
<contexts>
<context position="1504" citStr="Craven et al., 2000" startWordPosition="221" endWordPosition="224">aseline models. 1 Introduction Knowledge graphs are a powerful tool for supporting a large spectrum of search applications including ranking, recommendation, exploratory search, and web search (Dong et al., 2014). A knowledge graph aggregates information around entities across multiple content sources and links these entities together, while at the same time providing entity-specific properties (such as age or employer) and types (such as actor or movie). Although there is a growing interest in automatically constructing knowledge graphs, e.g., from unstructured web data (Weston et al., 2013; Craven et al., 2000; Fan et al., 2012), the problem of providing evidence on why two entities are related in a knowledge graph remains largely unaddressed. Extracting and presenting evidence for linking two entities, however, is an important aspect of knowledge graphs, as it can enforce trust between the user and a search engine, which in turn can improve long-term user engagement, e.g., in the context of related entity recommendation (Blanco et al., 2013). Although knowledge ∗This work was carried out while this author was visiting Yahoo Labs. graphs exist that provide this functionality to a certain degree (e.</context>
</contexts>
<marker>Craven, DiPasquo, Freitag, McCallum, Mitchell, Nigam, Slattery, 2000</marker>
<rawString>Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew McCallum, Tom Mitchell, Kamal Nigam, and Se´an Slattery. 2000. Learning to construct knowledge bases from the world wide web. Artificial Intelligence, 118(1–2):69–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alen Doko</author>
<author>Maja ˇStula</author>
<author>Darko Stipaniˇcev</author>
</authors>
<title>A recursive TF-ISF based sentence retrieval method with local context.</title>
<date>2013</date>
<journal>International Journal of Machine Learning and Computing,</journal>
<volume>3</volume>
<issue>2</issue>
<marker>Doko, ˇStula, Stipaniˇcev, 2013</marker>
<rawString>Alen Doko, Maja ˇStula, and Darko Stipaniˇcev. 2013. A recursive TF-ISF based sentence retrieval method with local context. International Journal of Machine Learning and Computing, 3(2):195–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Dong</author>
<author>Evgeniy Gabrilovich</author>
<author>Geremy Heitz</author>
<author>Wilko Horn</author>
<author>Ni Lao</author>
<author>Kevin Murphy</author>
<author>Thomas Strohmann</author>
<author>Shaohua Sun</author>
<author>Wei Zhang</author>
</authors>
<title>Knowledge vault: A web-scale approach to probabilistic knowledge fusion.</title>
<date>2014</date>
<booktitle>In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14,</booktitle>
<pages>601--610</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1097" citStr="Dong et al., 2014" startWordPosition="156" endWordPosition="159"> extracts and enriches sentences that refer to an entity pair from a corpus and ranks the sentences according to how well they describe the relationship between the entities. We model this task as a learning to rank problem for sentences and employ a rich set of features. When evaluated on a large set of manually annotated sentences, we find that our method significantly improves over state-of-the-art baseline models. 1 Introduction Knowledge graphs are a powerful tool for supporting a large spectrum of search applications including ranking, recommendation, exploratory search, and web search (Dong et al., 2014). A knowledge graph aggregates information around entities across multiple content sources and links these entities together, while at the same time providing entity-specific properties (such as age or employer) and types (such as actor or movie). Although there is a growing interest in automatically constructing knowledge graphs, e.g., from unstructured web data (Weston et al., 2013; Craven et al., 2000; Fan et al., 2012), the problem of providing evidence on why two entities are related in a knowledge graph remains largely unaddressed. Extracting and presenting evidence for linking two entit</context>
</contexts>
<marker>Dong, Gabrilovich, Heitz, Horn, Lao, Murphy, Strohmann, Sun, Zhang, 2014</marker>
<rawString>Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, pages 601– 610. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Fan</author>
<author>Raphael Hoffman</author>
<author>Aditya Kalyanpur</author>
<author>Sebastian Riedel</author>
<author>Fabian Suchanek</author>
</authors>
<title>and Pratim Partha Talukdar,</title>
<date>2012</date>
<booktitle>Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX), chapter Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1523" citStr="Fan et al., 2012" startWordPosition="225" endWordPosition="228">roduction Knowledge graphs are a powerful tool for supporting a large spectrum of search applications including ranking, recommendation, exploratory search, and web search (Dong et al., 2014). A knowledge graph aggregates information around entities across multiple content sources and links these entities together, while at the same time providing entity-specific properties (such as age or employer) and types (such as actor or movie). Although there is a growing interest in automatically constructing knowledge graphs, e.g., from unstructured web data (Weston et al., 2013; Craven et al., 2000; Fan et al., 2012), the problem of providing evidence on why two entities are related in a knowledge graph remains largely unaddressed. Extracting and presenting evidence for linking two entities, however, is an important aspect of knowledge graphs, as it can enforce trust between the user and a search engine, which in turn can improve long-term user engagement, e.g., in the context of related entity recommendation (Blanco et al., 2013). Although knowledge ∗This work was carried out while this author was visiting Yahoo Labs. graphs exist that provide this functionality to a certain degree (e.g., when hovering o</context>
</contexts>
<marker>Fan, Hoffman, Kalyanpur, Riedel, Suchanek, 2012</marker>
<rawString>James Fan, Raphael Hoffman, Aditya Kalyanpur, Sebastian Riedel, Fabian Suchanek, and Pratim Partha Talukdar, 2012. Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX), chapter Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lujun Fang</author>
<author>Anish Das Sarma</author>
<author>Cong Yu</author>
<author>Philip Bohannon</author>
</authors>
<title>Rex: explaining relationships between entity pairs.</title>
<date>2011</date>
<booktitle>Proceedings of the VLDB Endowment,</booktitle>
<pages>5--3</pages>
<contexts>
<context position="5074" citStr="Fang et al. (2011)" startWordPosition="789" endWordPosition="792"> models instead of a relationship-independent one (RQ3). We complement these research questions with an error and feature analysis. Our main contributions are a robust and effective method for explaining entity relationships, detailed insights into the performance of our method and features, and a manually annotated dataset. 2 Related Work We combine ideas from sentence retrieval, learning to rank, and question answering to address the task of explaining relationships between entities. Previous work that is closest to the task we address in this paper is that of Blanco and Zaragoza (2010) and Fang et al. (2011). First, Blanco and Zaragoza (2010) focus on finding and ranking sentences that explain the relationship between an entity and a query. Our work is different in that we want to explain the relationship between two entities, rather than a query. Fang et al. (2011) explore the generation of a ranked list of knowledge base relationships for an entity pair. Instead, we try to select sentences that describe a particular relationship, assuming that this is given. Our approach builds on sentence retrieval, where one retrieves sentences rather than documents that answer an information need. Document r</context>
</contexts>
<marker>Fang, Sarma, Yu, Bohannon, 2011</marker>
<rawString>Lujun Fang, Anish Das Sarma, Cong Yu, and Philip Bohannon. 2011. Rex: explaining relationships between entity pairs. Proceedings of the VLDB Endowment, 5(3):241–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald T Fern´andez</author>
<author>David E Losada</author>
<author>Leif Azzopardi</author>
</authors>
<title>Extending the language modeling framework for sentence retrieval to include local context. Information Retrieval,</title>
<date>2011</date>
<marker>Fern´andez, Losada, Azzopardi, 2011</marker>
<rawString>Ronald T Fern´andez, David E. Losada, and Leif Azzopardi. 2011. Extending the language modeling framework for sentence retrieval to include local context. Information Retrieval, 14(4):355–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Natural language question answering: the view from here.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>04</issue>
<pages>300</pages>
<contexts>
<context position="7163" citStr="Hirschman and Gaizauskas, 2001" startWordPosition="1137" endWordPosition="1140">ing features is learning to rank (LTR). Recent years have witnessed a rapid increase in the work on learning to rank, and it has proven to be a very successful method for combining large numbers of ranking features, for web search, but also other information retrieval applications (Burges et al., 2011; Surdeanu et al., 2011; Agarwal et al., 2012). We use learning to rank and represent each sentence with a set of features that aim to capture different dimensions of the sentence. Question answering (QA) is the task of providing direct and concise answers to questions formed in natural language (Hirschman and Gaizauskas, 2001). QA can be regarded as a similar task to ours, assuming that the combination of entity pair and relationship form the “question” and that the “answer” is the sentence describing the relationship of interest. Even though we do not follow the QA paradigm in this paper, some of the features we use are inspired by QA systems. In addition, we employ learning to rank to combine the devised features, which has recently been successfully applied for QA (Surdeanu et al., 2011; Agarwal et al., 2012). 3 Problem Statement We address the problem of explaining relationships between pairs of entities in a k</context>
</contexts>
<marker>Hirschman, Gaizauskas, 2001</marker>
<rawString>Lynette Hirschman and Robert Gaizauskas. 2001. Natural language question answering: the view from here. Natural Language Engineering, 7(04):275– 300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo J¨arvelin</author>
<author>Jaana Kek¨al¨ainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>20</volume>
<issue>4</issue>
<marker>J¨arvelin, Kek¨al¨ainen, 2002</marker>
<rawString>Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS), 20(4):422–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Geunbae Lee</author>
<author>Jungyun Seo</author>
<author>Seungwoo Lee</author>
<author>Hanmin Jung</author>
<author>Bong-Hyun Cho</author>
<author>Changki Lee</author>
<author>ByungKwan Kwak</author>
<author>Jeongwon Cha</author>
<author>Dongseok Kim</author>
<author>JooHui An</author>
</authors>
<title>SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP.</title>
<date>2001</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="13006" citStr="Lee et al., 2001" startWordPosition="2132" endWordPosition="2135">ent each sentence with a rich set of features. Table 1 lists the features we use. Below we provide 1We experimented with the Stanford co-reference resolution system (Lee et al., 2011) and Apache OpenNLP and found that they were not able to consistently achieve the level of effectiveness that we require. 2http://en.Wikipedia.org/wiki/ Wikipedia:Manual_of_Style/Linking 566 # Name Gloss Textfeatures 1 Sentence length Length of s in words 2 Sum of idf Sum of IDF of terms of s in Wikipedia 3 Average idf Average IDF of terms of s in Wikipedia 4 Sentence density Lexical density of s, see Equation 1 (Lee et al., 2001) 5–8 POS fractions Fraction of verbs, nouns, adjectives, others in s (Mintz et al., 2009) Entity features 9 #entities Total number of entities in s 10 Link to ei Whether s contains a link to the entity ei 11 Link to ej Whether s contains a link to the entity ej 12 Links to ei and ej Whether s contains links to both entities ei and ej 13 Entity first Is ei or ej the first entity in the sentence? 14 Spread of ei, ej Distance between the last match of ei and ej in s (Blanco and Zaragoza, 2010) 15–22 POS fractions left/right Fraction of verbs, nouns, adjectives, others to the left/right window of </context>
</contexts>
<marker>Lee, Seo, Lee, Jung, Cho, Lee, Kwak, Cha, Kim, An, 2001</marker>
<rawString>Gary Geunbae Lee, Jungyun Seo, Seungwoo Lee, Hanmin Jung, Bong-Hyun Cho, Changki Lee, ByungKwan Kwak, Jeongwon Cha, Dongseok Kim, JooHui An, et al. 2001. SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12572" citStr="Lee et al., 2011" startWordPosition="2059" endWordPosition="2062">nd Seven (1995 film). 4.2 Ranking sentences After extracting candidate sentences, we rank them by how well they describe the relationship of interest r between entities ei and ej. There are many signals beyond simple term statistics that can indicate relevance. Automatically constructing a ranking model using supervised machine learning techniques is therefore an obvious choice. For ranking we use learning to rank (LTR) and represent each sentence with a rich set of features. Table 1 lists the features we use. Below we provide 1We experimented with the Stanford co-reference resolution system (Lee et al., 2011) and Apache OpenNLP and found that they were not able to consistently achieve the level of effectiveness that we require. 2http://en.Wikipedia.org/wiki/ Wikipedia:Manual_of_Style/Linking 566 # Name Gloss Textfeatures 1 Sentence length Length of s in words 2 Sum of idf Sum of IDF of terms of s in Wikipedia 3 Average idf Average IDF of terms of s in Wikipedia 4 Sentence density Lexical density of s, see Equation 1 (Lee et al., 2001) 5–8 POS fractions Fraction of verbs, nouns, adjectives, others in s (Mintz et al., 2009) Entity features 9 #entities Total number of entities in s 10 Link to ei Whet</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 28–34. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Losada</author>
</authors>
<title>A study of statistical query expansion strategies for sentence retrieval.</title>
<date>2008</date>
<booktitle>In Proceedings of the SIGIR 2008 Workshop on Focused Retrieval,</booktitle>
<pages>37--44</pages>
<contexts>
<context position="6285" citStr="Losada, 2008" startWordPosition="990" endWordPosition="991"> retrieval models such as tf-idf, BM25, and language modeling (Baeza-Yates et al., 1999) have been extended to tackle sentence retrieval. Three of the most successful sentence retrieval methods are TFISF (Allan et al., 2003), which is a variant of the vector space model with tf-idf weighting, language modeling with local context (Murdock, 2006; Fern´andez et al., 2011), and a recursive version of TFISF that accounts for local context (Doko et al., 2013). TFISF is very competitive compared to document retrieval models tuned specifically for sentence retrieval (e.g., BM25 and language modeling (Losada, 2008)) and we therefore include it as a baseline. Sentences that are suitable for explaining relationships can have attributes that are important for ranking but cannot be captured by term-based retrieval models. One way to combine a wide range of ranking features is learning to rank (LTR). Recent years have witnessed a rapid increase in the work on learning to rank, and it has proven to be a very successful method for combining large numbers of ranking features, for web search, but also other information retrieval applications (Burges et al., 2011; Surdeanu et al., 2011; Agarwal et al., 2012). We </context>
</contexts>
<marker>Losada, 2008</marker>
<rawString>David E. Losada. 2008. A study of statistical query expansion strategies for sentence retrieval. In Proceedings of the SIGIR 2008 Workshop on Focused Retrieval, pages 37–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Meij</author>
<author>Wouter Weerkamp</author>
<author>Maarten de Rijke</author>
</authors>
<title>Adding semantics to microblog posts.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining,</booktitle>
<pages>563--572</pages>
<publisher>ACM.</publisher>
<marker>Meij, Weerkamp, de Rijke, 2012</marker>
<rawString>Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 563– 572. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="17955" citStr="Mikolov et al., 2013" startWordPosition="3010" endWordPosition="3013"> retrieval (Doko et al., 2013). We therefore consider the number of common links in the previous and next sentence (features 29–30). Relationship features Feature 31 indicates whether any of the relationship-specific terms occurs in the sentence. Only matching the terms in the relationship may have low coverage since terms such as “spouse” may have many synonyms and/or highly related terms, e.g., “husband” or “married”. Therefore, we use WordNet to find synonym phrases of r (feature 32); we refer to this method as wordnet(r). Alternatively, we use word embeddings to find such similar phrases (Mikolov et al., 2013). Such embeddings take a text corpus as input and learn vector representations of words and phrases consisting of real numbers. Given the set Vr consisting of the vector representations of all the relationship terms and the set V which consists of the vector representations of all the candidate phrases in the data, we calculate the distance between a candidate phrase represented by a vector vi E V and the vectors in Vr as: where ∑Vj∈Vr vj is the element-wise sum of the vectors in Vr and the distance between two vectors v1 and v2 is measured using cosine similarity. The candidate phrases in V a</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>509--518</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10919" citStr="Milne and Witten, 2008" startWordPosition="1787" endWordPosition="1790">ource document and (ii) linking the sentences to entities. For (i), we replace pronouns in candidate sentences with the title of the entity. We apply a simple heuristic for the people entities, inspired by (Wu and Weld, 2010):1 we count the frequency of the terms “he” and “she” in the article for determining the gender of the entity, and we replace the first appearance of “he” or “she” in each sentence with the entity’s title. We skip this step if any surface form of the entity occurs in the sentence. For (ii), we apply entity linking to provide links from the sentence to additional entities (Milne and Witten, 2008). This need arises from the fact that not every sentence in an article contains explicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article’s text.2 The algorithm takes a sentence as input and iterates over n-grams that are not yet linked to an entity. If an n-gram matches a surface form of an entity, we establish a link between the n-gram and the entity. We restrict our search space to entities that are linked from within the source article of the sentence and from within articles to which the source article links. This way, our en</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. Learning to link with Wikipedia. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 509–518. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13095" citStr="Mintz et al., 2009" startWordPosition="2147" endWordPosition="2150"> we provide 1We experimented with the Stanford co-reference resolution system (Lee et al., 2011) and Apache OpenNLP and found that they were not able to consistently achieve the level of effectiveness that we require. 2http://en.Wikipedia.org/wiki/ Wikipedia:Manual_of_Style/Linking 566 # Name Gloss Textfeatures 1 Sentence length Length of s in words 2 Sum of idf Sum of IDF of terms of s in Wikipedia 3 Average idf Average IDF of terms of s in Wikipedia 4 Sentence density Lexical density of s, see Equation 1 (Lee et al., 2001) 5–8 POS fractions Fraction of verbs, nouns, adjectives, others in s (Mintz et al., 2009) Entity features 9 #entities Total number of entities in s 10 Link to ei Whether s contains a link to the entity ei 11 Link to ej Whether s contains a link to the entity ej 12 Links to ei and ej Whether s contains links to both entities ei and ej 13 Entity first Is ei or ej the first entity in the sentence? 14 Spread of ei, ej Distance between the last match of ei and ej in s (Blanco and Zaragoza, 2010) 15–22 POS fractions left/right Fraction of verbs, nouns, adjectives, others to the left/right window of ei and ej in s (Mintz et al., 2009) 23–25 #entities left/right/between Number of entities</context>
<context position="16365" citStr="Mintz et al., 2009" startWordPosition="2742" endWordPosition="2745">capture the distribution of part-ofspeech tags in the sentence. Entity features These features partly build on (Tsagkias et al., 2011; Meij et al., 2012) and describe the entities and are dependent on the knowledge graph. Whether ei or ej is the first appearing entity in a sentence might be an indicator of importance (feature 13). The spread of ei and ej in the sentence (feature 14) might be an indicator of their centrality in the sentence (Blanco and Zaragoza, 2010). Features 15–22 capture the distribution of part-of-speech tags in the sentence in a window of four words around ei or ej in s (Mintz et al., 2009), complemented by the number of entities between, to the left of, and to the right of the entity pair (features 23–25). We assume that two articles that have many common articles that point to them are strongly related (Witten and Milne, 2008). We hypothesize that, if a sentence contains common inlinks from ei and ej, the sentence might contain important information about their relationship. Hence, we add whether the sentence contains a common link (feadensity(s) _ 1 K • (K + 1) n E j=1 567 ture 26) and the number of common links (feature 27) as features. We score a common link l between ei an</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Murdock</author>
<author>W Bruce Croft</author>
</authors>
<title>A translation model for sentence retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>684--691</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15234" citStr="Murdock and Croft, 2005" startWordPosition="2537" endWordPosition="2540">es in word2vec(r) that are matched in s 41 Sum word2vec(r) Sum of cosine similarity of phrases in word2vec(r) that are matched in s 42 Score LC Lucene score of s with titles(ei, ej), terms(r), wordnet(r), word2vec(r) as query 43 Score R-TFISF R-TFISF score of s with queries constructed as above Source features 44 Sentence position Position of s in document from which it originates 45 From ei or ej? Does s originate from the Wikipedia article of ei or ej? 46 #(ei or ej) Number of occurrences of ei or ej in document from which s originates, inspired by document smoothing for sentence retrieval (Murdock and Croft, 2005) Table 1: Features used for sentence ranking. a brief description of the more complex ones. Text features This feature type regards the importance of the sentence s at the term level. We compute the density of s (feature 4) as: idf(tj) • idf(tj+1) (1) distance(tj , tj+1) 2 where K is the number of keyword terms in s and distance(tj, tj+1) is the number of nonkeyword terms between keyword terms tj and tj+1. We treat stop words and numbers in s as nonkeywords and the remaining terms as keywords. Features 5–8 capture the distribution of part-ofspeech tags in the sentence. Entity features These fe</context>
</contexts>
<marker>Murdock, Croft, 2005</marker>
<rawString>Vanessa Murdock and W. Bruce Croft. 2005. A translation model for sentence retrieval. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 684–691. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Graham Murdock</author>
</authors>
<title>Aspects of Sentence Retrieval.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts Amherst.</institution>
<contexts>
<context position="6017" citStr="Murdock, 2006" startWordPosition="946" endWordPosition="948">ships for an entity pair. Instead, we try to select sentences that describe a particular relationship, assuming that this is given. Our approach builds on sentence retrieval, where one retrieves sentences rather than documents that answer an information need. Document retrieval models such as tf-idf, BM25, and language modeling (Baeza-Yates et al., 1999) have been extended to tackle sentence retrieval. Three of the most successful sentence retrieval methods are TFISF (Allan et al., 2003), which is a variant of the vector space model with tf-idf weighting, language modeling with local context (Murdock, 2006; Fern´andez et al., 2011), and a recursive version of TFISF that accounts for local context (Doko et al., 2013). TFISF is very competitive compared to document retrieval models tuned specifically for sentence retrieval (e.g., BM25 and language modeling (Losada, 2008)) and we therefore include it as a baseline. Sentences that are suitable for explaining relationships can have attributes that are important for ranking but cannot be captured by term-based retrieval models. One way to combine a wide range of ranking features is learning to rank (LTR). Recent years have witnessed a rapid increase </context>
</contexts>
<marker>Murdock, 2006</marker>
<rawString>Vanessa Graham Murdock. 2006. Aspects of Sentence Retrieval. Ph.D. thesis, University of Massachusetts Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers to nonfactoid questions from web collections.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="6857" citStr="Surdeanu et al., 2011" startWordPosition="1087" endWordPosition="1090"> (e.g., BM25 and language modeling (Losada, 2008)) and we therefore include it as a baseline. Sentences that are suitable for explaining relationships can have attributes that are important for ranking but cannot be captured by term-based retrieval models. One way to combine a wide range of ranking features is learning to rank (LTR). Recent years have witnessed a rapid increase in the work on learning to rank, and it has proven to be a very successful method for combining large numbers of ranking features, for web search, but also other information retrieval applications (Burges et al., 2011; Surdeanu et al., 2011; Agarwal et al., 2012). We use learning to rank and represent each sentence with a set of features that aim to capture different dimensions of the sentence. Question answering (QA) is the task of providing direct and concise answers to questions formed in natural language (Hirschman and Gaizauskas, 2001). QA can be regarded as a similar task to ours, assuming that the combination of entity pair and relationship form the “question” and that the “answer” is the sentence describing the relationship of interest. Even though we do not follow the QA paradigm in this paper, some of the features we u</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2011</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank answers to nonfactoid questions from web collections. Computational Linguistics, 37(2):351–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manos Tsagkias</author>
<author>Maarten de Rijke</author>
<author>Wouter Weerkamp</author>
</authors>
<title>Linking online news and social media.</title>
<date>2011</date>
<booktitle>In WSDM 2011: Fourth ACM International Conference on Web Search and Data Mining. ACM,</booktitle>
<marker>Tsagkias, de Rijke, Weerkamp, 2011</marker>
<rawString>Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp. 2011. Linking online news and social media. In WSDM 2011: Fourth ACM International Conference on Web Search and Data Mining. ACM, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Oksana Yakhnenko</author>
<author>Nicolas Usunier</author>
</authors>
<title>Connecting language and knowledge bases with embedding models for relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<contexts>
<context position="1483" citStr="Weston et al., 2013" startWordPosition="217" endWordPosition="220">er state-of-the-art baseline models. 1 Introduction Knowledge graphs are a powerful tool for supporting a large spectrum of search applications including ranking, recommendation, exploratory search, and web search (Dong et al., 2014). A knowledge graph aggregates information around entities across multiple content sources and links these entities together, while at the same time providing entity-specific properties (such as age or employer) and types (such as actor or movie). Although there is a growing interest in automatically constructing knowledge graphs, e.g., from unstructured web data (Weston et al., 2013; Craven et al., 2000; Fan et al., 2012), the problem of providing evidence on why two entities are related in a knowledge graph remains largely unaddressed. Extracting and presenting evidence for linking two entities, however, is an important aspect of knowledge graphs, as it can enforce trust between the user and a search engine, which in turn can improve long-term user engagement, e.g., in the context of related entity recommendation (Blanco et al., 2013). Although knowledge ∗This work was carried out while this author was visiting Yahoo Labs. graphs exist that provide this functionality to</context>
</contexts>
<marker>Weston, Bordes, Yakhnenko, Usunier, 2013</marker>
<rawString>Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Witten</author>
<author>David Milne</author>
</authors>
<title>An effective, lowcost measure of semantic relatedness obtained from wikipedia links.</title>
<date>2008</date>
<booktitle>In Proceeding of AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy,</booktitle>
<pages>25--30</pages>
<publisher>AAAI Press,</publisher>
<location>Chicago, USA,</location>
<contexts>
<context position="16608" citStr="Witten and Milne, 2008" startWordPosition="2784" endWordPosition="2787">is the first appearing entity in a sentence might be an indicator of importance (feature 13). The spread of ei and ej in the sentence (feature 14) might be an indicator of their centrality in the sentence (Blanco and Zaragoza, 2010). Features 15–22 capture the distribution of part-of-speech tags in the sentence in a window of four words around ei or ej in s (Mintz et al., 2009), complemented by the number of entities between, to the left of, and to the right of the entity pair (features 23–25). We assume that two articles that have many common articles that point to them are strongly related (Witten and Milne, 2008). We hypothesize that, if a sentence contains common inlinks from ei and ej, the sentence might contain important information about their relationship. Hence, we add whether the sentence contains a common link (feadensity(s) _ 1 K • (K + 1) n E j=1 567 ture 26) and the number of common links (feature 27) as features. We score a common link l between ei and ej using: score(l, ei, ej) = sim(l, ei) • sim(l, ej), (2) where sim(•, •) is defined as the similarity between two Wikipedia articles, computed using a variant of Normalized Google Distance (Witten and Milne, 2008). Feature 28 then measures </context>
</contexts>
<marker>Witten, Milne, 2008</marker>
<rawString>Ian Witten and David Milne. 2008. An effective, lowcost measure of semantic relatedness obtained from wikipedia links. In Proceeding of AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy, AAAI Press, Chicago, USA, pages 25– 30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>118--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10521" citStr="Wu and Weld, 2010" startWordPosition="1716" endWordPosition="1719">es and consider a sentence as a candidate if (i) the sentence is part of either entities’ Wikipedia article and contains a surface form of, or a link to, the other entity; or (ii) the sentence contains surface forms of, or links to, both entities in the entity pair. Next, we apply two sentence enrichment steps for (i) making sentences self-contained and readable outside the context of the source document and (ii) linking the sentences to entities. For (i), we replace pronouns in candidate sentences with the title of the entity. We apply a simple heuristic for the people entities, inspired by (Wu and Weld, 2010):1 we count the frequency of the terms “he” and “she” in the article for determining the gender of the entity, and we replace the first appearance of “he” or “she” in each sentence with the entity’s title. We skip this step if any surface form of the entity occurs in the sentence. For (ii), we apply entity linking to provide links from the sentence to additional entities (Milne and Witten, 2008). This need arises from the fact that not every sentence in an article contains explicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article’</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118–127. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Peter Clark</author>
</authors>
<title>Automatic coupling of answer extraction and information retrieval.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>159--165</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yao, Van Durme, Clark, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, and Peter Clark. 2013. Automatic coupling of answer extraction and information retrieval. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 159– 165. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>