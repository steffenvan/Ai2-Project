<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99792">
Quantifier Scope Disambiguation Using Extracted Pragmatic Knowledge:
Preliminary Results
</title>
<author confidence="0.993989">
Prakash Srinivasan
</author>
<affiliation confidence="0.995429">
Temple University
</affiliation>
<address confidence="0.697124333333333">
1805 N. Broad St.
Wachman Hall 324
Philadelphia, PA 19122
</address>
<email confidence="0.998017">
prakash.srinivasan@temple.edu
</email>
<author confidence="0.989364">
Alexander Yates
</author>
<affiliation confidence="0.993817">
Temple University
</affiliation>
<address confidence="0.696774666666667">
1805 N. Broad St.
Wachman Hall 324
Philadelphia, PA 19122
</address>
<email confidence="0.998819">
yates@temple.edu
</email>
<sectionHeader confidence="0.997386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999817923076923">
It is well known that pragmatic knowl-
edge is useful and necessary in many dif-
ficult language processing tasks, but be-
cause this knowledge is difficult to acquire
and process automatically, it is rarely used.
We present an open information extrac-
tion technique for automatically extracting
a particular kind of pragmatic knowledge
from text, and we show how to integrate
the knowledge into a Markov Logic Net-
work model for quantifier scope disam-
biguation. Our model improves quantifier
scope judgments in experiments.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998615684210526">
It has long been a goal of the natural language
processing (NLP) community to be able to inter-
pret language utterances into logical representa-
tions of their meaning. Quantifier scope ambigu-
ity has been recognized as one particularly chal-
lenging aspect of this problem. For example, the
following sentence has two possible readings, de-
pending on the scope of its quantifiers:
Every boy wants a dog.
One reading of this sentence is that there exists a
single dog in the world which all boys want. The
second, and usually preferred, reading is that the
sentence is describing a separate “wanting” rela-
tion for each boy, and that the dog in question is
a function of the boy who wants it. In this read-
ing, there may be as many different dogs as boys,
although it leaves open the possibility that several
of the boys want the same dog. In logic, these two
readings can be represented as follows:
</bodyText>
<listItem confidence="0.859416">
1. �IdEDogs bbEBoys wants(b,d)
2. bbEBoys �IdEDogs wants(b,d)
</listItem>
<bodyText confidence="0.9974971">
The readings differ only in the order of the quanti-
fiers. The quantifier that comes first in each ex-
pression is said to have wide scope; the second
quantifier has narrow scope.
Linguists and NLP researchers have come up
with several theories and mechanisms for automat-
ically determining the scope of quantified linguis-
tic expressions. Despite a long history of proposed
solutions, however, researchers have for the most
part abandoned this task as hopeless because of
“overwhelming evidence suggesting that quanti-
fier scope is a phenomenon that must be treated at
the pragmatic level” (Saba and Corriveau, 2001).
For example, in active voice clauses, the quantifier
for the subject noun is usually preferred for wide
scope over the quantifier of the predicate noun
(Kurtzman and MacDonald, 1993). But such pref-
erences can easily be overruled by world knowl-
edge:
A doctor lives in every city.
</bodyText>
<listItem confidence="0.971600333333333">
1. �IdEDocs bcECities lives in(d,c)
(A single doctor lives in all cities.)
2. bcECities �IdEDocs lives in(d,c)
</listItem>
<bodyText confidence="0.981841210526316">
(Each city has a different doctor living there.)
Syntactic preferences would normally indicate
that reading 1 is better, but in this particular case
common-sense knowledge of the world overrules
that preference and makes reading 2 far more
probable.
Open-domain pragmatic knowledge is usually
not available to language processing systems, but
that is beginning to change. Recent research in
open information extraction (Banko and Etzioni,
2008; Davidov and Rappaport, 2008) has shown
that we can extract large amounts of relational data
from open-domain text with high accuracy. Here,
we show how we can connect the two fields, by ex-
tracting a targeted form of pragmatic knowledge
for use in quantifier scope disambiguation. Our
contributions are:
1) We build an extraction mechanism for extract-
ing pragmatic knowledge about relations. In par-
</bodyText>
<page confidence="0.919597">
1465
</page>
<note confidence="0.996577">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1465–1474,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.992461666666667">
ticular, we extract knowledge about the expected
sizes of the sets of objects that participate in the
relations. The task of identifying functional re-
lationships is a subtask of our extraction problem
that has received recent attention in the literature
(Ritter et al., 2008).
2) We devise a novel probabilistic model in the
Markov Logic Network framework for reasoning
over possible readings of sentences that involve
quantifier scope ambiguities. The model is able
to assign a probability that a particular reading is
plausible, given the pragmatic knowledge we ex-
tract.
3) We provide an empirical demonstration that our
system is able to resolve quantifier scope ambigu-
ities in cases where the syntactic and lexical fea-
tures used by previous systems are of no help.
The remainder of this paper is organized as fol-
lows. The next section describes previous work.
Section 3 shows how the problem can be formu-
lated as a task of assigning probabilities to possi-
ble worlds, and that the crucial difference between
them has to do with the number of objects partic-
ipating in individual relationships. Section 4 dis-
cusses our techniques for extracting the pragmatic
knowledge that allows us to make judgments about
quantifier scope. Section 5 presents our proba-
bilistic model for resolving scope ambiguities. We
present an empirical study in section 6, and section
7 concludes and suggests items for future work.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999777">
Quantifier scope disambiguation has received at-
tention in linguistics and computational linguis-
tics since at least the 1970s. Montague (1973)
gave a seminal treatment of quantifier ambiguities,
and argued that a particular syntax-based mech-
anism known as “quantifying-in” could resolve
scope ambiguities. Since then, most work on dis-
ambiguation has focused on syntactic clues for de-
termining which readings of an ambiguous state-
ment are possible, and of the set of possible read-
ings, which ones are preferred (Van Lehn, 1978;
Hobbs and Shieber, 1987; Poesio, 1993a; Hurum,
1988; Moran, 1988). For instance, one linguis-
tic study (Kurtzman and MacDonald, 1993) deter-
mined that in active voice sentences where quan-
tifiers in the subject and object give rise to scope
ambiguity, there is a preference for the reading in
which the subject quantifier has wide scope — the
direct reading is acceptable 70-80% of the time,
whereas the indirect reading is acceptable 30-40%
of the time. Sentences that are similar in all re-
spects except that they are passive voice have no
such preference. Nevertheless, in these studies
both readings are often quite plausible. In addition
to syntactic clues, other studies have noted that
the choice of quantifier has a significant effect on
scope disambiguation (e.g., “each” has a greater
tendency for wide scope than “every”) (Van Lehn,
1978; Alshawi, 1990). Most authors have noted
that both syntactic and lexical evidence fall short
of a full solution, and that pragmatic knowledge
(knowledge about the world) is necessary for this
task (Van Lehn, 1978; Saba and Corriveau, 1997;
Moran, 1988). Saba and Corriveau (2001) recently
proposed a test for quantifier scope disambigua-
tion using pragmatic knowledge. However, they
do not show how to extract the necessary infor-
mation, nor do they implement or evaluate their
proposed test.
Due to the difficulty of the problem, several
authors have devised techniques for “underspec-
ified” logical representations that can efficiently
store multiple ambiguous readings, and they de-
vise techniques for automated reasoning using un-
derspecified representations (Reyle, 1995; Late-
cki, 1992; Poesio, 1993b). Others (Hobbs and
Shieber, 1987; Park, 1988) have devised compu-
tational mechanisms for generating all of the pos-
sible readings of statements exhibiting quantifier
ambiguity, especially in cases involving more than
two quantifiers.
Detecting functions in extracted relational data
has been studied in several contexts. Ritter et
al.(2008) use knowledge of functions to determine
when two extracted relationships contradict one
another. Knowledge of functions has also been
important in finding synonyms (Yates and Etzioni,
2009) and in review mining (Popescu, 2007). We
extend this work by extracting not just a binary
determination of whether a relation is functional,
but a distribution over the expected number of ar-
guments for that relation. Our technique also dif-
fers from previous work based on extracted rela-
tionships between named entities. We leverage
domain-independent extraction patterns involving
numeric phrases, as discussed below; our tech-
nique is complementary to existing approaches
and could in fact be combined with them for even
greater accuracy. Finally, we apply the extracted
knowledge in a novel way to quantifier scope dis-
ambiguation.
Our work is similar in spirit to several recent
</bodyText>
<page confidence="0.987018">
1466
</page>
<bodyText confidence="0.9997864">
projects that use semantic reasoning over extracted
knowledge for a novel approach to well-known
tasks. For example, Schoenmackers et al.(2008)
have recently used extracted knowledge for the
task of predicting whether a new extracted fact is
correct. Yates et al.(2006) use extracted knowl-
edge to determine whether a parse of a sentence
has a plausible semantic interpretation. We extend
this new line of attack to a hard problem in lan-
guage understanding.
</bodyText>
<sectionHeader confidence="0.990258" genericHeader="method">
3 Possible Worlds Framework
</sectionHeader>
<bodyText confidence="0.999971777777778">
We now present a framework for reasoning about
quantifier scope ambiguities, and for choosing
among possible readings based on pragmatic
knowledge (or world knowledge — we use the
terms interchangeably). We first present a formal
description of the quantifier scope disambiguation
(QSD) problem. We then describe the crucial dif-
ferences between the “possible worlds” evoked by
different readings of an ambiguous statement.
</bodyText>
<subsectionHeader confidence="0.999914">
3.1 Representation of Readings
</subsectionHeader>
<bodyText confidence="0.999596818181818">
We follow Copestake et al. (2005), among oth-
ers, in representing quantifiers as modal oper-
ators with three arguments: a variable name
for the variable being quantified; a logical for-
mula, called the restriction, which defines the set
of objects over which the variable may range;
and a second logical formula, called the body,
which defines the expression in which the quan-
tified variable takes part. For example, we
represent the sentence “Every dog barks” as:
every(x, dog(x), barks(x)).
For the sake of clarity and convenience, we re-
strict our attention to a common syntactic form
of sentences, where the semantic representation
is relatively well-understood: active-voice English
sentences in which the subject noun phrase is
quantified, and a noun phrase in the predicate (ei-
ther an object of the verb, or an object of a prepo-
sition attached to the verb) is also quantified. For
a sentence with the following structure, in which
pi and qj represent predicates introduced by mod-
ifiers like adjectives and prepositional phrases,
</bodyText>
<equation confidence="0.99545275">
(S (NP (DET Q1)(N [p1, . . . ,pn]C1))
(V P (V R)(NP (DET Q2)(N [q1, . . . , qm]C2))
we can represent the two possible readings of the
sentence as:
direct reading:
Q1(x, C1(x) n p1(x) n ... n pn(x),
Q2(y, C2(y) n q1(y) n ... n qm(y),
R(x, y)))
indirect reading:
Q2(y, C2(y) n q1(y) n ... n qm(y),
Q1(x, C1(x) n p1(x) n ... n pn(x),
R(x, y)))
</equation>
<bodyText confidence="0.9999784">
By making the restriction to this type of sen-
tences, we can isolate the effects of pragmatics on
scope disambiguation decisions from the effects
of syntax, since all test cases have essentially the
same syntax. As we show below, for certain types
of relations, the preference for interpretations may
be drastically different from the general preference
for the direct reading, even though the syntax of
the sentences we investigate matches the syntax
studied by Kurtzman and MacDonald (1993).
</bodyText>
<subsectionHeader confidence="0.9712255">
3.2 Readings, Possible Worlds, and World
Knowledge
</subsectionHeader>
<bodyText confidence="0.999976346153846">
The different logical forms for the direct and indi-
rect readings describe different “possible worlds.”
For instance, the direct reading of “A doctor lives
in every city” describes worlds in which there is a
single doctor who manages to reside in each city
of the world simultaneously. This reading is “pos-
sible” in the sense that it does not contradict itself.
In logical terms, if φ represents the direct reading
of this sentence, φ Y L. Using some imagination
one could devise a scenario, perhaps in an online
game world, that satisfies φ.
Nevertheless, the indirect reading is strongly
preferred for this statement in the absence of any
context that indicates an abnormal world. The
indirect reading φ&apos; describes worlds where every
city is inhabited by some doctor, but potentially a
different doctor per city. Using pragmatic knowl-
edge, the reader can easily deduce that this logical
statement is a much more likely reading than φ.
Let B represent the reader’s pragmatic knowledge,
including facts like “People don’t simultaneously
live in more than one city,” and, “There are at least
hundreds of cities in the world.” The reader can
easily deduce that B � -,φ. We now turn to meth-
ods for extracting the necessary pragmatic knowl-
edge B from text.
</bodyText>
<page confidence="0.989357">
1467
</page>
<sectionHeader confidence="0.9591795" genericHeader="method">
4 Extraction Techniques to Support QSD
Decisions
</sectionHeader>
<bodyText confidence="0.99994288">
Saba and Corriveau (2001) point out that there is
a restricted form of pragmatic knowledge that can
be used in many instances of QSD. Consider the
facts that were used above to determine that 0&apos;
is preferable to 0. The facts fall into two basic
categories of knowledge: 1) the size of class C
(e.g., how many cities are there?), and 2) the ex-
pected number of Y participants in a relationship
R, given that there is exactly 1 X participant (e.g.,
how many cities does 1 doctor live in?). In both
cases, we are concerned with extracting sizes of
sets.
Previous extraction systems have attempted to
estimate set sizes based on extracted named en-
tities. Downey et al. (2005)estimate the size of
classes based on the number of named-entities ex-
tracted for the class. As far as we are aware, find-
ing the expected size of an argument set for a rela-
tion is a novel task for information extraction, but
several researchers (Ritter et al., 2008; Yates and
Etzioni, 2009) have investigated the special case
of detecting functional relations — those relations
where the expected size of the Y argument set is
precisely 1. As with class size extraction, they
use extractions involving named-entity arguments
to find functional relations.
Approaches that depend on named-entity ex-
tractions have several disadvantages: they must
find a large set of named-entities for every set,
which can be time-consuming and difficult. Also,
many classes, like “trees” and “hot dogs,” have no
or very few named instances, but many un-named
instances, so approaches based on named entities
have little hope. In fact, besides classes like peo-
ple, locations, and organizations (and their sub-
classes), there are few classes that have a large
number of named instances. For classes that do
have named instances, synonymy, polysemy, and
extraction errors are common problems that can
all affect estimates of size (Ritter et al., 2008).
Rather than indirectly determining set sizes
from extracted instances, our system directly ex-
tracts estimates of set sizes. It uses numeric
phrases, like “two trees,” “hundreds of students,”
or “billions of stars,” to associate numeric values
with sets. Table 1 lists the numeric phrases we use.
Currently, we use only numeric phrases with ex-
plicit values or ranges of values, but it may be pos-
sible to increase the recall of our extraction tech-
nique by incorporating more approximate phrases
</bodyText>
<figure confidence="0.974421363636364">
Numeric Phrase Value
no  |none  |zero 0
a  |one  |this  |the 1
two 2
... ...
one hundred  |a hundred 100
... ...
hundreds of 100
thousands of 1,000
tens of thousands of 10,000
... ...
</figure>
<tableCaption confidence="0.819682666666667">
Table 1: Numeric phrases used in our extraction pat-
terns. For the word “the”, we require that it be followed di-
rectly by a singular noun, to try to weed out plural usages.
</tableCaption>
<bodyText confidence="0.999925954545455">
like “several,” “many,” or even bare plurals. We
do not match numbers expressed in digits (e.g.,
1234) because we found that they produced too
many noisy extractions, such as dates and times.
For words like “hundreds,” we set the value of the
word to be the lower limit (i.e., 100). This gives
a conservative estimate of the value, but our tech-
niques described below can help to compensate for
this bias.
Table 2 lists examples of the hand-crafted,
domain-independent extraction patterns we use.
Our extraction patterns generate two types of ex-
tractions, one for classes and one for relations. For
classes, each extraction E consists of a class name
Ec and a number En indicating the size of some
subset 5 C_ Ec. For instance, the 4gram “hun-
dreds of students are” matches our first pattern.
The numeric phrase “hundreds of” here indicates
that some subset 5 C_ Ec = students has a size in
the hundreds. After processing a large corpus, our
system can determine a probability distribution for
the size of a class given by:
</bodyText>
<equation confidence="0.996515">
VC (size(C) = N) = |{E  |Ec = C ∧ En = N1|
|{E  |Ec = C1|
</equation>
<bodyText confidence="0.999805777777778">
In practice, we only include the largest 20% of the
numbers N in the set of extractions for a class to
estimate that class’s size.
The second type of extraction we get from our
patterns are relational extractions. Each relational
extraction F consists of a relation name Fr, and
possibly names for the classes of its two argu-
ments, Fc1, Fc2. In addition, the extraction con-
tains values for the size of both arguments, Fn1
</bodyText>
<page confidence="0.822642">
1468
</page>
<equation confidence="0.867970125">
Pattern Extraction
&lt;numeric&gt; &lt;word&gt;+ (of  |are  |have) Ec = &lt;word&gt;+, En = value(&lt;numeric&gt;)
(I  |he  |she) &lt;word&gt;+ &lt;numeric&gt; &lt;noun&gt; Fr = &lt;word&gt;+, Fc1 = people, Fc2 = &lt;noun&gt;,
Fn1 = 1, Fn2 = value(&lt;numeric&gt;)
it is pastParticiple(&lt;verb&gt;) by &lt;numeric&gt; Fr = &lt;verb&gt;, Fc2 = thing,
Fn1 = value(&lt;numeric&gt;), Fn2 = 1
is the &lt;word&gt; of &lt;numeric&gt; Fr = is the &lt;word&gt; of,
Fn1 = 1, Fn2 = value(&lt;numeric&gt;)
</equation>
<tableCaption confidence="0.8895865">
Table 2: Sample extraction patterns for discovering classes (Ec) and their sizes (En), or relations (Fr) and the expected
set size of their arguments (Fn1 and Fn2).
</tableCaption>
<bodyText confidence="0.917578636363636">
and Fn2 respectively. For example, the fragment
“she visited four countries” matches the second
pattern in Table 2, with Fr = visited, Fn1 = 1,
and Fn2 = 4. Note that in our extraction patterns,
one of of the arguments is always constrained to be
a singleton set, like “he” or “it.” This restriction
allows us to avoid quantifier scope ambiguity in
the extraction process: if we extracted phrases like
“Two men married two women,” it would be un-
clear which quantifier has wide scope, and there-
fore how many men and women are participating
in each “married” relationship. By using singular
pronouns, we avoid this confusion; in almost all
cases, these pronouns have wide scope, and indi-
cate a single element.1
Based on these extractions, our system de-
termines two distributions for each relation:
PLeft Rand PRight The PLeft distribution
n) R (n). R
represents the probability that the left argument of
R is a set of size n, given that the right argument
is a singleton set, and likewise for P Right
</bodyText>
<equation confidence="0.46588">
R . We de-
</equation>
<bodyText confidence="0.9438955">
termine the distributions from the extractions by
maximum likelihood estimation:
</bodyText>
<equation confidence="0.9863316">
PLeft R(n) = |{F  |Fr = R, Fn1 = n, Fn2 = 1}|
|{F  |Fr = R,Fn2 = 1}|
P Right
R (n) = |{F  |Fr = R, Fn2 = n, Fn1 = 1}|
|{F  |Fr = R,Fn1 = 1}|
</equation>
<bodyText confidence="0.973949285714286">
For example, for the relation is the father
of, we might see the fragment “he is the father
of two children” far more often than “he is the
father of twenty children.” P Right
is the father of
would therefore have a relatively low probability
for n = 20. As one would expect, the relation
</bodyText>
<footnote confidence="0.93589725">
1An example of an exception to this rule from our data set
is the sentence “It is worn by millions of women.” Here, “it”
refers to a class of items such as a brand, and thus may refer
to a different item for each of the “millions of women.”
</footnote>
<bodyText confidence="0.999878434782609">
visited appears more often with “twenty,” and
the relation married never does. Their PRight
distributions are comparatively higher and lower,
respectively than the one for is the father
ofatn=20.
In practice, we create histograms of the ex-
tracted counts for both our E and F extractions,
and our probability distributions are really dis-
tributions over the buckets in these histograms,
rather than over all possible set sizes. To help
combat sparse counts for large numeric values, we
use buckets of exponentially increasing width for
larger numeric values. Thus between n = 0 and
10, buckets have size 1, between 10 and 100 they
have size 10, and so on.
We also create distributions in the same way for
relations together with their extracted argument
classes. Since counts for these extractions tend to
be much more sparse, we interpolate these distri-
butions with the distribution for just the relation,
and with the distribution for the relation and just
one class. We use equal weights for all interpo-
lated distributions.
</bodyText>
<sectionHeader confidence="0.7537915" genericHeader="method">
5 A Probabilistic Model for Quantifier
Scope Disambiguation
</sectionHeader>
<bodyText confidence="0.99990725">
QSD requires reasoning about different possible
states of the world. This involves logical reason-
ing, since the direct and indirect readings differ in
the number of objects that exist in models satis-
fying each reading, and the number of relation-
ships between those objects. QSD also involves
probabilistic reasoning, since none of the extracted
knowledge is certain. We leverage recent work on
Markov Logic Networks (MLNs) (Richardson and
Domingos, 2006) to incorporate both types of rea-
soning into our technique for QSD. We next briefly
review MLNs, before describing our model and
</bodyText>
<page confidence="0.993511">
1469
</page>
<bodyText confidence="0.64689">
methods for training it.
</bodyText>
<subsectionHeader confidence="0.992778">
5.1 Markov Logic Networks
</subsectionHeader>
<bodyText confidence="0.999816333333333">
Syntactically, an MLN consists of a set of first-
order logical formulas F and a real-valued weight
wF for each F ∈ F. Semantically, an MLN
defines a probability distribution over possible
groundings of the logical formulas. That is, if U
denotes the set of all objects in the universe, and G
denotes the set of all possible ways to ground ev-
ery F ∈ F (i.e., substitute an element from U for
every variable in F), then an MLN defines a distri-
bution over truth assignments to the grounded for-
mulas G ∈ G. Let I denote the set of all possible
interpretations of G — that is, each I ∈ I assigns
true or false to every G ∈ G. The probabil-
ity of a particular interpretation I according to the
MLN is given by :
</bodyText>
<equation confidence="0.99936">
1 X
P(I) = Z exp
FEF
!wF · n(F, I)
</equation>
<bodyText confidence="0.999991666666667">
where n(F, I) gives the number of groundings of
F that are true in interpretation I.
The equation above provides an expression for
P(I) when U, or at least the size of U, is known
and fixed. When we are interpreting expressions
like “every city” or “every doctor”, however, we
require extracted knowledge to inform the system
of the correct number of “city” or “doctor” objects.
Since our extractions are uncertain, they provide a
distribution P(|U |= n) for the size of a class.
Using P(|U|), we can still calculate P(I), even
without knowing the exact size of U:
</bodyText>
<equation confidence="0.985612">
P (I) = X P(|U |= n)P(I  ||U |= n)
n
</equation>
<subsectionHeader confidence="0.994284">
5.2 MLN Classifier for QSD
</subsectionHeader>
<bodyText confidence="0.880605545454545">
Let Q be a QSD problem, consisting of a rela-
tion Qr, a class for the first argument of the re-
lation Qc1, a class for the second argument Qc2,
and quantifiiers Qq1, Qq2 for each argument. We
construct an MLN model for Q using the follow-
ing logical formulas:
1) Clustering: We allow members of each class
to belong to clusters denoted by γ, but each ele-
ment can belong to no more than one cluster. This
is represented by the following formula, which has
infinite weight.
</bodyText>
<equation confidence="0.439694">
∀xEQc1uQc2,γ,γ&apos;x ∈ γ ∧ x ∈ γ&apos; ⇒ γ = γ&apos;
</equation>
<listItem confidence="0.8770935">
2) Relation between clusters: Every cluster of
class 1 elements must participate in the relation
Qr with exactly one cluster of class 2 elements,
and vice versa. We represent this participation in
Qr with a series of logical relations Rm,n, each of
which indicates that a cluster of size m is partic-
ipating in Qr with a cluster of size n. We use a
set of formulas for each setting of m and n, each
having infinite weight.
∀γCQc1∃!γ&apos;CQc2,m,n Rm,n(γ,γ&apos;)
∀γ&apos;CQc2∃!γCQc1,m,n Rm,n(γ,γ&apos;)
∀γ,γ&apos; Rm,n(γ,γ&apos;) ⇒ |γ |= m ∧ |γ&apos; |= n
3) Prefer relations between clusters of the ap-
propriate size: We include a set of formulas with
</listItem>
<bodyText confidence="0.93821165625">
finite weight that express the preference for a par-
ticular relation to have arguments of a certain size.
There is a separate formula for each setting of m
and n, with a separate weight wm,n for each.
∀γ,γ&apos;Rm,n(γ,γ&apos;)
This formula does most of the work of our clas-
sifier. For a given relation, such as the lives
in(Person, City) relation, we can set the
weights wm,n so that the model prefers worlds
where each person lives in just one place. For in-
stance, we can set the weight w1,1 relatively high,
so that the model is more likely to make clusters of
size 1, which then participate in the R1,1 relation.
We describe how we choose the wm,n weights
below, but first we explain how to incorporate the
quantifiers Qq1 and Qq2 into the model. Unfortu-
nately, every natural language quantifier has dif-
ferent semantics (Barwise and Cooper, 1981), and
thus they affect our model in different ways. Here,
we restrict our attention to the two common quan-
tifiers “a” and “every”, but note that the MLN
framework is a powerful tool for incorporating
the logical semantics and statistical preferences of
other quantifiers.
For the quantifier “a”, we require that the rela-
tion have no argument clusters with size more than
1 for that class. Thus if Qq1 = “a”, we restrict
Rm,n to R1,n, and vice versa if Qq2 = “a.” Fur-
thermore, we require that at least one element of
the class belong to a cluster: ∃x,γx ∈ γ has infi-
nite weight. For “every,” we require that every el-
ement of the class that “every” modifies to be part
</bodyText>
<equation confidence="0.955419333333333">
!wF · n(F, I)
XZ = Xexp
IEZ FEF
</equation>
<page confidence="0.913055">
1470
</page>
<bodyText confidence="0.999838363636363">
of some cluster. To effect this change, we simply
put an infinite weight on the formula bx]γx E γ.
Our MLN model is general in the sense that
for any QSD problem Q, it can determine prob-
abilities for any possible world corresponding to
a reading of Q. For our purposes, we are primar-
ily interested in the direct and indirect readings of
any Q involving “a” and “every.” To predict the
correct reading for a given Q, we simply check to
see which has the higher probability according to
our MLN model.
</bodyText>
<subsectionHeader confidence="0.997345">
5.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.995608785714286">
Our MLN model for QSD requires settings for
the wm,n parameters for each QSD problem Q.
The standard approach to this problem would be
to estimate these parameters from labeled train-
ing data. We reject the standard supervised frame-
work, however, because each distinct relation Qr
requires different settings of the parameters, and
therefore a standard supervised approach would
require manually labeled training data for every
relation Qr.
A second approach that is made possible by
our extraction technique is to set the parameters
using the extracted distributions. We tried this
approach by setting w1,n = log PRight
</bodyText>
<equation confidence="0.9794585">
Qr (n) and
wm,1 = log PLeft
</equation>
<bodyText confidence="0.99896625">
Qr (m); since we only consider
sentences containing the quantifier “a”, one of m
and n will always be 1. Unfortunately, in our ex-
periments we found that this setting for the param-
eters often gave far too little weight for large val-
ues of m and n, and as a consequence, the classi-
fier would systematically judge one reading to be
more likely than another.
To counteract this problem, we take a hybrid ap-
proach to parameter estimation, informed by both
labeled training data and the extracted distribu-
tions. Crucially, our approach, which we call ZIPF
FLATTENING, has only two parameters that need
to be trained using a supervised approach, and
these parameters do not depend on the relation R.
Thus, the approach minimizes the amount of train-
ing data we need to a practical level.
ZIPF FLATTENING works by correcting the PR
distributions to give higher weight to larger values
of m and n. First, we estimate a Zipf distribution
from the raw extracted counts for each argument
of relation R. To fit a Zipf curve, we use least-
squares linear regression on the log-log plot of the
extracted counts to find parameters zR and cR such
</bodyText>
<equation confidence="0.879627333333333">
that
log (count) = zR · log (arg5ize) + cR
==&gt;. count = ecR ·arg5izezR
</equation>
<bodyText confidence="0.999876875">
We can perform this part automatically, using only
the extraction data and no manually labeled train-
ing data, for every relation. However, the fitted
Zipf distribution needs to be corrected for the sys-
tematic bias in the extracted counts. To do this, we
introduce two parameters, α1 and α2, that we use
to scale back the sharp falloff in the Zipf distribu-
tion. Our flattened distribution has the form:
</bodyText>
<equation confidence="0.988884">
count = eα1cR · arg5izeα2zR
</equation>
<bodyText confidence="0.96498275">
When α2 is less than 1, the resulting curve has a
less steep slope, and greater weight is placed on
the large values of m and n, as desired. Our last
step is to interpolate the PRight
</bodyText>
<sectionHeader confidence="0.305897" genericHeader="method">
R and PLeft
</sectionHeader>
<bodyText confidence="0.987704444444444">
R distri-
butions with the flattened Zipf distribution to come
up with corrected distributions for the right and
left argument sizes of R. We use equal weights
on the two distributions to interpolate. Note that if
the original counts from the extraction system in-
clude counts for only one argument size, then it is
impossible to estimate a Zipf distribution, and we
simply fall back on the extracted distribution. We
do not include counts for an argument size of zero
in this process.
To estimate the parameters αi, we collect a
training set of QSD problems Q, labeled with the
correct reading for each (direct or indirect), and
run the extractor for the relations Qr appearing in
the training set. We then perform a gradient de-
scent search to find optimal settings for the αi on
the training data.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999175">
We report on two sets of experiments. The first
tests our extraction technique on its own, and the
second tests the accuracy of our complete QSD
system, including the extraction mechanisms and
the prediction model, on a quantifier scope disam-
biguation task.
</bodyText>
<subsectionHeader confidence="0.9924">
6.1 Function Detection Experiment
</subsectionHeader>
<bodyText confidence="0.999945">
Function detection is an important task in its own
right, and has been used in several previous ap-
plications (Ritter et al., 2008; Yates and Etzioni,
2009; Popescu, 2007). To turn our extraction
system into a classifier for functions vs. non-
functions, we simply checked whether there were
</bodyText>
<page confidence="0.983615">
1471
</page>
<table confidence="0.991488333333333">
Num Precision Recall F1
Functions 54 .79 .76 .77
Non-functions 74 .83 .85 .84
</table>
<tableCaption confidence="0.9797985">
Table 3: Precision and recall for detecting functions us-
ing the numeric extraction technique.
</tableCaption>
<bodyText confidence="0.99268476744186">
any extractions for R with Fn2 &gt; 1. If so, we pre-
dicted that the R was nonfunctional, and otherwise
we predicted it was functional.
We used the Web1Tgram Corpus of n-grams
provided by Google, Inc to extract classes, rela-
tions, and counts. This corpus contains counts for
2- through 5-grams that appear on the Web pages
indexed by Google. Counts are included in this
data set for all n-grams that appeared at least 40
times in their text. We ran our extraction tech-
niques on the 3-, 4- and 5-grams. To create a test
set, we sampled a set of 200 relations from our ex-
tractions, removed any relations that consisted of
punctuations, stopwords, or other non-relational
items. We then manually labeled the remainder
as functions or non-functions.
Table 3 shows our results. A baseline sys-
tem that simply predicts the majority class (non-
functions) on this data set would achieve an ac-
curacy of 56%, well below the 81% accuracy
of our classifier. Many of the relations in our
test set, like built(Person, House) and is
riding(Person,Animal), do not ordinarily
have named-entity extractions for both arguments,
and would therefore not be amenable to previous
function detection approaches.
Some of our technique’s errors highlight inter-
esting difficulties with function detection. For in-
stance, while we labeled the is capital of
relation as a function, our technique predicted that
it was not. It turns out that the country of Bolivia
has two capitals, and the South Asian region of
Jammu and Kashmir also has two capitals. Both
of these facts are prominent enough on the Web
to cause our system to detect a small probability
for P Right
capital of(2). Thus any label for this rela-
tion is somewhat unsatisfying: it is almost entirely
functional, but not strictly so. By generalizing the
problem to one of determining a distribution for
the size of the argument, we can handle these bor-
der cases in a useful way for QSD, as discussed
below.
</bodyText>
<subsectionHeader confidence="0.995548">
6.2 Preliminary QSD Experiments
</subsectionHeader>
<bodyText confidence="0.999949156862745">
We test our complete QSD system on two impor-
tant tasks. In the first, the system is presented
with a series of QSD problems Q in which the
first quantifier Qq1 is always “a,” and the second
(Qq2) is always “every.” Each example is manu-
ally labeled to indicate whether a direct or indirect
reading of the sentence is preferred, and the sys-
tem is charged with predicting the preferred read-
ing. In the second task, each Q has “every” as
the first quantifier, and “a” as the second quanti-
fier. Since indirect readings are very rarely pre-
ferred for active-voice sentences of this form,we
charge the system with making a different type of
prediction: determine whether the indirect reading
is plausible or not. The system assumes that ev-
ery sentence has a plausible direct reading, but by
determining whether the indirect reading is plau-
sible, it can determine whether the sentence is am-
biguous between the two readings.
We created data sets for these tasks by sampling
our 5grams for examples containing the relations
in our function experiment. From this set, we se-
lected phrases that involved named classes for the
arguments to the relation. When a class was miss-
ing, we either manually supplied one, or discarded
the example. We then constructed two examples
from each combination of relation and argument
classes: one example in which the first argument
is constrained by the quantifier “a” and the sec-
ond by “every,” and a second example in which
the quantifiers are reversed. Finally, we manually
labeled every example with a preference for direct
or indirect reading (in the case of “a/every” exam-
ples) or with a plausibility judgment for the indi-
rect reading (in the case of “every/a” examples).
Our final test sets included 46 labeled examples
for each task. Further experiments involving mul-
tiple annotators, as in the experiments of Kurtz-
man and MacDonald (1993), are of course desir-
able, but note that even their experiments included
just 32 labeled examples.
Table 4 shows our results for the first QSD task,
and Table 5 shows our results for the second one.
In each case, we compare our supervised Cor-
rected MLN model against an Uncorrected MLN
model that uses no supervised data, and simply
takes its weights straight from our extracted distri-
butions. The supervised model uses a training cor-
pus of 10 manually labeled examples for each task,
five from each class. We also compare against a
majority class baseline. Note that the Corrected
</bodyText>
<page confidence="0.966479">
1472
</page>
<table confidence="0.9996396">
System Acc. Direct Indirect
P R P R
All-Direct BL .53 .53 1.0 0.0 0.0
Uncorrected MLN .58 .78 .30 .53 .90
Corrected MLN .74 .77 .74 .71 .75
</table>
<tableCaption confidence="0.901714833333333">
Table 4: Our trained MLN outperforms two other sys-
tems at predicting whether sentences of the form “A/some
&lt;class 1&gt; &lt;relation&gt; every &lt;class 2&gt;” should have di-
rect or indirect readings. We measure accuracy over the
whole dataset, as well as precision and recall for the two sub-
sets labeled with direct and indirect readings, respectively.
</tableCaption>
<table confidence="0.9997202">
System Acc. Plausible Implaus.
P R P R
All-Plausible BL .67 .67 1.0 0.0 0.0
Uncorrected MLN .49 .89 .28 .38 .93
Corrected MLN .72 .76 .86 .60 .43
</table>
<tableCaption confidence="0.9708175">
Table 5: Our trained MLN outperforms two other sys-
tems at predicting whether sentences of the form “Every
</tableCaption>
<bodyText confidence="0.924026130434783">
&lt;class 1&gt; &lt;relation&gt; a/some &lt;class 2&gt;” have a plausi-
ble indirect reading or not. We measure accuracy over the
whole dataset, as well as precision and recall for the two sub-
sets labeled with plausible and implausible indirect readings.
MLN model has balanced recall numbers for the
two classes in both of our tasks, compared with the
Uncorrected MLN. This indicates that our ZIPF
FLATTENING technique is accurately learning bet-
ter weights to remove the systematic bias in the
Uncorrected MLN.
Our results demonstrate the utility of our ex-
tracted distributions for these difficult tasks. Al-
though the extracted data prevents us from deter-
mining that is capital of should be classi-
fied as a function, since almost all of the prob-
ability mass in PRight is still on n ∈ {0, 1}.
Thus, the probability for the direct reading of
a sentence like “Some city is the capital of ev-
ery country” is still very low. Likewise, even
though our system (correctly) determines that the
relation is a parent of is non-functional,
it does not therefore group it with other non-
functional relations like visited. The dis-
</bodyText>
<sectionHeader confidence="0.472026" genericHeader="method">
tribution PRight
</sectionHeader>
<bodyText confidence="0.999356933333333">
is parent of(n) is skewed to much
smaller numbers for n than is the distribution for
visited, and thus the indirect reading for “A
person is the parent of every child” is much more
likely than the indirect reading of “A person vis-
ited every country.”
The biggest hurdle for better performance is
noise in our extraction technique. Polysemous re-
lations sometimes have large counts for large ar-
gument sizes in one sense, but not another. Us-
ing argument classes to disambiguate relations can
help, but extractions for relations in combination
with argument classes are much more sparse. Im-
proved extraction techniques could directly impact
performance on the QSD task.
</bodyText>
<sectionHeader confidence="0.997215" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999993">
We have demonstrated targeted methods for ex-
tracting world knowledge that is necessary for
making quantifier scope disambiguation deci-
sions. We have also demonstrated a novel,
minimally-supervised, statistical relational model
in the Markov Logic Network framework for mak-
ing QSD decisions based on extracted pragmatics.
While our preliminary results for QSD are
promising, there are clearly many areas for im-
provement. We will need to handle more kinds of
quantifiers in our MLN model. Our current system
is biased towards using purely pragmatic knowl-
edge, but a complete system should also integrate
syntactic and lexical constraints and preferences.
Also, discourses can introduce knowledge that di-
rectly affects QSD problems, such as constraints
on the size of a particular set that is discussed in
the discourse. Integrating our technique for QSD
with discourse processing is a major challenge that
we hope to address.
</bodyText>
<sectionHeader confidence="0.999552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999904947368421">
Hiyan Alshawi. 1990. Resolving quasi logical forms.
Computational Linguistics, 16(3):133–144.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional information extraction.
In Proceedings of the ACL.
J. Barwise and R. Cooper. 1981. Generalized quanti-
fiers and natural language. Linguistics and Philoso-
phy, 4(2):150–219.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 3:281–332.
D. Davidov and A. Rappaport. 2008. Unsupervised
discovery of generic relationships using pattern clus-
ters and its evaluation by automatically generated
SAT analogy questions. In Proceedings of the ACL.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A Probabilistic Model of Redundancy in In-
formation Extraction. In IJCAI.
</reference>
<page confidence="0.976237">
1473
</page>
<bodyText confidence="0.927086333333333">
Jerry R. Hobbs and Stuart M. Shieber. 1987. An algo-
rithm for generating quantifier scopings. Computa-
tional Linguistics, 13(1-2):47–63.
Kurt Van Lehn. 1978. Determining the scope of En-
glish quantifiers. Technical Report AI-TR-483, AI
Lab, MIT.
</bodyText>
<reference confidence="0.999639763636364">
Sven Hurum. 1988. Handling scope ambiguities in
English. In Proceedings of the Second Conference
on Applied Natural Language Processing, pages 58–
65.
Howard S. Kurtzman and Maryellen C. MacDonald.
1993. Resolution of quantifier scope ambiguities.
Cognition, 48:243–279.
Longin Latecki. 1992. Connection relations and quan-
tifier scope. In Proceedings of the ACL.
Richard Montague. 1973. The proper treatment of
quantification in ordinary English. In Jaakko Hin-
tikka, Julius Moravcsik, and Patrick Suppes, editors,
Approaches to Natural Languages, pages 221–242.
Reidel, Dordrecht.
Douglas B. Moran. 1988. Quantifier scoping in the
SRI core language engine. In Proceedings of the
26th Annual Meeting of the Assoc. for Comp. Lin-
guistics, pages 33–40.
Jong C. Park. 1988. Quantifier scope and constituency.
In Proceedings of the 26th Annual Meeting of the
Assoc. for Comp. Linguistics, pages 33–40.
Massimo Poesio. 1993a. Assigning a semantic scope
to operators. In Proceedings of the ACL.
Massimo Poesio. 1993b. Assigning a semantic scope
to operators. In Proceedings of the Second Confer-
ence on Situation Theory and Its Applications.
Ana-Maria Popescu. 2007. Information Extraction
from Unstructured Web Text. Ph.D. thesis, Univer-
sity of Washington.
Uwe Reyle. 1995. On reasoning with ambiguities. In
Proceedings of the EACL, pages 1–8.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107–136.
Alan Ritter, Doug Downey, Stephen Soderland, and
Oren Etzioni. 2008. It’s a contradiction — No, it’s
not: A case study using functional relations. In Em-
pirical Methods in Natural Language Processing.
Walid S. Saba and Jean-Pierre Corriveau. 1997. A
pragmatic treatment of quantification in natural lan-
guage. In Proceedings of the National Conference
on Artificial Intelligence.
Walid S. Saba and Jean-Pierre Corriveau. 2001. Plau-
sible reasoning and the resolution of quantifier scope
ambiguities. Studia Logica, 67:271–289.
Stefan Schoenmackers, Oren Etzioni, and Dan Weld.
2008. Scaling textual inference to the web. In Pro-
ceedings of EMNLP.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research (JAIR), 34:255–296, March.
Alexander Yates, Stefan Schoenmackers, and Oren Et-
zioni. 2006. Detecting parser errors using web-
based semantic filters. In Proceedings of EMNLP.
</reference>
<page confidence="0.995017">
1474
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.137844">
<title confidence="0.9987775">Quantifier Scope Disambiguation Using Extracted Pragmatic Knowledge: Preliminary Results</title>
<author confidence="0.523539">Prakash</author>
<affiliation confidence="0.350998">Temple</affiliation>
<address confidence="0.941578">1805 N. Broad Wachman Hall Philadelphia, PA</address>
<email confidence="0.998442">prakash.srinivasan@temple.edu</email>
<author confidence="0.879981">Alexander</author>
<affiliation confidence="0.632658">Temple</affiliation>
<address confidence="0.946423666666667">1805 N. Broad Wachman Hall Philadelphia, PA</address>
<email confidence="0.999483">yates@temple.edu</email>
<abstract confidence="0.999679785714286">It is well known that pragmatic knowledge is useful and necessary in many difficult language processing tasks, but because this knowledge is difficult to acquire and process automatically, it is rarely used. We present an open information extraction technique for automatically extracting a particular kind of pragmatic knowledge from text, and we show how to integrate the knowledge into a Markov Logic Network model for quantifier scope disambiguation. Our model improves quantifier scope judgments in experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
</authors>
<title>Resolving quasi logical forms.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="6629" citStr="Alshawi, 1990" startWordPosition="1055" endWordPosition="1056">y, there is a preference for the reading in which the subject quantifier has wide scope — the direct reading is acceptable 70-80% of the time, whereas the indirect reading is acceptable 30-40% of the time. Sentences that are similar in all respects except that they are passive voice have no such preference. Nevertheless, in these studies both readings are often quite plausible. In addition to syntactic clues, other studies have noted that the choice of quantifier has a significant effect on scope disambiguation (e.g., “each” has a greater tendency for wide scope than “every”) (Van Lehn, 1978; Alshawi, 1990). Most authors have noted that both syntactic and lexical evidence fall short of a full solution, and that pragmatic knowledge (knowledge about the world) is necessary for this task (Van Lehn, 1978; Saba and Corriveau, 1997; Moran, 1988). Saba and Corriveau (2001) recently proposed a test for quantifier scope disambiguation using pragmatic knowledge. However, they do not show how to extract the necessary information, nor do they implement or evaluate their proposed test. Due to the difficulty of the problem, several authors have devised techniques for “underspecified” logical representations t</context>
</contexts>
<marker>Alshawi, 1990</marker>
<rawString>Hiyan Alshawi. 1990. Resolving quasi logical forms. Computational Linguistics, 16(3):133–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional information extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="3263" citStr="Banko and Etzioni, 2008" startWordPosition="513" endWordPosition="516">asily be overruled by world knowledge: A doctor lives in every city. 1. �IdEDocs bcECities lives in(d,c) (A single doctor lives in all cities.) 2. bcECities �IdEDocs lives in(d,c) (Each city has a different doctor living there.) Syntactic preferences would normally indicate that reading 1 is better, but in this particular case common-sense knowledge of the world overrules that preference and makes reading 2 far more probable. Open-domain pragmatic knowledge is usually not available to language processing systems, but that is beginning to change. Recent research in open information extraction (Banko and Etzioni, 2008; Davidov and Rappaport, 2008) has shown that we can extract large amounts of relational data from open-domain text with high accuracy. Here, we show how we can connect the two fields, by extracting a targeted form of pragmatic knowledge for use in quantifier scope disambiguation. Our contributions are: 1) We build an extraction mechanism for extracting pragmatic knowledge about relations. In par1465 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1465–1474, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ticular, we extract knowledge about the ex</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The tradeoffs between open and traditional information extraction. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>R Cooper</author>
</authors>
<title>Generalized quantifiers and natural language.</title>
<date>1981</date>
<journal>Linguistics and Philosophy,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="24398" citStr="Barwise and Cooper, 1981" startWordPosition="4101" endWordPosition="4104">&apos;) This formula does most of the work of our classifier. For a given relation, such as the lives in(Person, City) relation, we can set the weights wm,n so that the model prefers worlds where each person lives in just one place. For instance, we can set the weight w1,1 relatively high, so that the model is more likely to make clusters of size 1, which then participate in the R1,1 relation. We describe how we choose the wm,n weights below, but first we explain how to incorporate the quantifiers Qq1 and Qq2 into the model. Unfortunately, every natural language quantifier has different semantics (Barwise and Cooper, 1981), and thus they affect our model in different ways. Here, we restrict our attention to the two common quantifiers “a” and “every”, but note that the MLN framework is a powerful tool for incorporating the logical semantics and statistical preferences of other quantifiers. For the quantifier “a”, we require that the relation have no argument clusters with size more than 1 for that class. Thus if Qq1 = “a”, we restrict Rm,n to R1,n, and vice versa if Qq2 = “a.” Furthermore, we require that at least one element of the class belong to a cluster: ∃x,γx ∈ γ has infinite weight. For “every,” we requir</context>
</contexts>
<marker>Barwise, Cooper, 1981</marker>
<rawString>J. Barwise and R. Cooper. 1981. Generalized quantifiers and natural language. Linguistics and Philosophy, 4(2):150–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<booktitle>Research on Language and Computation,</booktitle>
<pages>3--281</pages>
<contexts>
<context position="9602" citStr="Copestake et al. (2005)" startWordPosition="1502" endWordPosition="1505">ble semantic interpretation. We extend this new line of attack to a hard problem in language understanding. 3 Possible Worlds Framework We now present a framework for reasoning about quantifier scope ambiguities, and for choosing among possible readings based on pragmatic knowledge (or world knowledge — we use the terms interchangeably). We first present a formal description of the quantifier scope disambiguation (QSD) problem. We then describe the crucial differences between the “possible worlds” evoked by different readings of an ambiguous statement. 3.1 Representation of Readings We follow Copestake et al. (2005), among others, in representing quantifiers as modal operators with three arguments: a variable name for the variable being quantified; a logical formula, called the restriction, which defines the set of objects over which the variable may range; and a second logical formula, called the body, which defines the expression in which the quantified variable takes part. For example, we represent the sentence “Every dog barks” as: every(x, dog(x), barks(x)). For the sake of clarity and convenience, we restrict our attention to a common syntactic form of sentences, where the semantic representation i</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag. 2005. Minimal recursion semantics: An introduction. Research on Language and Computation, 3:281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>A Rappaport</author>
</authors>
<title>Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated SAT analogy questions.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="3293" citStr="Davidov and Rappaport, 2008" startWordPosition="517" endWordPosition="520">ld knowledge: A doctor lives in every city. 1. �IdEDocs bcECities lives in(d,c) (A single doctor lives in all cities.) 2. bcECities �IdEDocs lives in(d,c) (Each city has a different doctor living there.) Syntactic preferences would normally indicate that reading 1 is better, but in this particular case common-sense knowledge of the world overrules that preference and makes reading 2 far more probable. Open-domain pragmatic knowledge is usually not available to language processing systems, but that is beginning to change. Recent research in open information extraction (Banko and Etzioni, 2008; Davidov and Rappaport, 2008) has shown that we can extract large amounts of relational data from open-domain text with high accuracy. Here, we show how we can connect the two fields, by extracting a targeted form of pragmatic knowledge for use in quantifier scope disambiguation. Our contributions are: 1) We build an extraction mechanism for extracting pragmatic knowledge about relations. In par1465 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1465–1474, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ticular, we extract knowledge about the expected sizes of the sets of ob</context>
</contexts>
<marker>Davidov, Rappaport, 2008</marker>
<rawString>D. Davidov and A. Rappaport. 2008. Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated SAT analogy questions. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Oren Etzioni</author>
<author>Stephen Soderland</author>
</authors>
<title>A Probabilistic Model of Redundancy in Information Extraction.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="13462" citStr="Downey et al. (2005)" startWordPosition="2159" endWordPosition="2162">a restricted form of pragmatic knowledge that can be used in many instances of QSD. Consider the facts that were used above to determine that 0&apos; is preferable to 0. The facts fall into two basic categories of knowledge: 1) the size of class C (e.g., how many cities are there?), and 2) the expected number of Y participants in a relationship R, given that there is exactly 1 X participant (e.g., how many cities does 1 doctor live in?). In both cases, we are concerned with extracting sizes of sets. Previous extraction systems have attempted to estimate set sizes based on extracted named entities. Downey et al. (2005)estimate the size of classes based on the number of named-entities extracted for the class. As far as we are aware, finding the expected size of an argument set for a relation is a novel task for information extraction, but several researchers (Ritter et al., 2008; Yates and Etzioni, 2009) have investigated the special case of detecting functional relations — those relations where the expected size of the Y argument set is precisely 1. As with class size extraction, they use extractions involving named-entity arguments to find functional relations. Approaches that depend on named-entity extrac</context>
</contexts>
<marker>Downey, Etzioni, Soderland, 2005</marker>
<rawString>Doug Downey, Oren Etzioni, and Stephen Soderland. 2005. A Probabilistic Model of Redundancy in Information Extraction. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hurum</author>
</authors>
<title>Handling scope ambiguities in English.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>58--65</pages>
<contexts>
<context position="5820" citStr="Hurum, 1988" startWordPosition="923" endWordPosition="924">s for future work. 2 Related Work Quantifier scope disambiguation has received attention in linguistics and computational linguistics since at least the 1970s. Montague (1973) gave a seminal treatment of quantifier ambiguities, and argued that a particular syntax-based mechanism known as “quantifying-in” could resolve scope ambiguities. Since then, most work on disambiguation has focused on syntactic clues for determining which readings of an ambiguous statement are possible, and of the set of possible readings, which ones are preferred (Van Lehn, 1978; Hobbs and Shieber, 1987; Poesio, 1993a; Hurum, 1988; Moran, 1988). For instance, one linguistic study (Kurtzman and MacDonald, 1993) determined that in active voice sentences where quantifiers in the subject and object give rise to scope ambiguity, there is a preference for the reading in which the subject quantifier has wide scope — the direct reading is acceptable 70-80% of the time, whereas the indirect reading is acceptable 30-40% of the time. Sentences that are similar in all respects except that they are passive voice have no such preference. Nevertheless, in these studies both readings are often quite plausible. In addition to syntactic</context>
</contexts>
<marker>Hurum, 1988</marker>
<rawString>Sven Hurum. 1988. Handling scope ambiguities in English. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 58– 65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard S Kurtzman</author>
<author>Maryellen C MacDonald</author>
</authors>
<title>Resolution of quantifier scope ambiguities.</title>
<date>1993</date>
<journal>Cognition,</journal>
<pages>48--243</pages>
<contexts>
<context position="2612" citStr="Kurtzman and MacDonald, 1993" startWordPosition="413" endWordPosition="416">row scope. Linguists and NLP researchers have come up with several theories and mechanisms for automatically determining the scope of quantified linguistic expressions. Despite a long history of proposed solutions, however, researchers have for the most part abandoned this task as hopeless because of “overwhelming evidence suggesting that quantifier scope is a phenomenon that must be treated at the pragmatic level” (Saba and Corriveau, 2001). For example, in active voice clauses, the quantifier for the subject noun is usually preferred for wide scope over the quantifier of the predicate noun (Kurtzman and MacDonald, 1993). But such preferences can easily be overruled by world knowledge: A doctor lives in every city. 1. �IdEDocs bcECities lives in(d,c) (A single doctor lives in all cities.) 2. bcECities �IdEDocs lives in(d,c) (Each city has a different doctor living there.) Syntactic preferences would normally indicate that reading 1 is better, but in this particular case common-sense knowledge of the world overrules that preference and makes reading 2 far more probable. Open-domain pragmatic knowledge is usually not available to language processing systems, but that is beginning to change. Recent research in o</context>
<context position="5901" citStr="Kurtzman and MacDonald, 1993" startWordPosition="933" endWordPosition="936">on has received attention in linguistics and computational linguistics since at least the 1970s. Montague (1973) gave a seminal treatment of quantifier ambiguities, and argued that a particular syntax-based mechanism known as “quantifying-in” could resolve scope ambiguities. Since then, most work on disambiguation has focused on syntactic clues for determining which readings of an ambiguous statement are possible, and of the set of possible readings, which ones are preferred (Van Lehn, 1978; Hobbs and Shieber, 1987; Poesio, 1993a; Hurum, 1988; Moran, 1988). For instance, one linguistic study (Kurtzman and MacDonald, 1993) determined that in active voice sentences where quantifiers in the subject and object give rise to scope ambiguity, there is a preference for the reading in which the subject quantifier has wide scope — the direct reading is acceptable 70-80% of the time, whereas the indirect reading is acceptable 30-40% of the time. Sentences that are similar in all respects except that they are passive voice have no such preference. Nevertheless, in these studies both readings are often quite plausible. In addition to syntactic clues, other studies have noted that the choice of quantifier has a significant </context>
<context position="11433" citStr="Kurtzman and MacDonald (1993)" startWordPosition="1813" endWordPosition="1816"> C2(y) n q1(y) n ... n qm(y), R(x, y))) indirect reading: Q2(y, C2(y) n q1(y) n ... n qm(y), Q1(x, C1(x) n p1(x) n ... n pn(x), R(x, y))) By making the restriction to this type of sentences, we can isolate the effects of pragmatics on scope disambiguation decisions from the effects of syntax, since all test cases have essentially the same syntax. As we show below, for certain types of relations, the preference for interpretations may be drastically different from the general preference for the direct reading, even though the syntax of the sentences we investigate matches the syntax studied by Kurtzman and MacDonald (1993). 3.2 Readings, Possible Worlds, and World Knowledge The different logical forms for the direct and indirect readings describe different “possible worlds.” For instance, the direct reading of “A doctor lives in every city” describes worlds in which there is a single doctor who manages to reside in each city of the world simultaneously. This reading is “possible” in the sense that it does not contradict itself. In logical terms, if φ represents the direct reading of this sentence, φ Y L. Using some imagination one could devise a scenario, perhaps in an online game world, that satisfies φ. Never</context>
<context position="33501" citStr="Kurtzman and MacDonald (1993)" startWordPosition="5678" endWordPosition="5682">cted two examples from each combination of relation and argument classes: one example in which the first argument is constrained by the quantifier “a” and the second by “every,” and a second example in which the quantifiers are reversed. Finally, we manually labeled every example with a preference for direct or indirect reading (in the case of “a/every” examples) or with a plausibility judgment for the indirect reading (in the case of “every/a” examples). Our final test sets included 46 labeled examples for each task. Further experiments involving multiple annotators, as in the experiments of Kurtzman and MacDonald (1993), are of course desirable, but note that even their experiments included just 32 labeled examples. Table 4 shows our results for the first QSD task, and Table 5 shows our results for the second one. In each case, we compare our supervised Corrected MLN model against an Uncorrected MLN model that uses no supervised data, and simply takes its weights straight from our extracted distributions. The supervised model uses a training corpus of 10 manually labeled examples for each task, five from each class. We also compare against a majority class baseline. Note that the Corrected 1472 System Acc. D</context>
</contexts>
<marker>Kurtzman, MacDonald, 1993</marker>
<rawString>Howard S. Kurtzman and Maryellen C. MacDonald. 1993. Resolution of quantifier scope ambiguities. Cognition, 48:243–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longin Latecki</author>
</authors>
<title>Connection relations and quantifier scope.</title>
<date>1992</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="7399" citStr="Latecki, 1992" startWordPosition="1170" endWordPosition="1172"> necessary for this task (Van Lehn, 1978; Saba and Corriveau, 1997; Moran, 1988). Saba and Corriveau (2001) recently proposed a test for quantifier scope disambiguation using pragmatic knowledge. However, they do not show how to extract the necessary information, nor do they implement or evaluate their proposed test. Due to the difficulty of the problem, several authors have devised techniques for “underspecified” logical representations that can efficiently store multiple ambiguous readings, and they devise techniques for automated reasoning using underspecified representations (Reyle, 1995; Latecki, 1992; Poesio, 1993b). Others (Hobbs and Shieber, 1987; Park, 1988) have devised computational mechanisms for generating all of the possible readings of statements exhibiting quantifier ambiguity, especially in cases involving more than two quantifiers. Detecting functions in extracted relational data has been studied in several contexts. Ritter et al.(2008) use knowledge of functions to determine when two extracted relationships contradict one another. Knowledge of functions has also been important in finding synonyms (Yates and Etzioni, 2009) and in review mining (Popescu, 2007). We extend this w</context>
</contexts>
<marker>Latecki, 1992</marker>
<rawString>Longin Latecki. 1992. Connection relations and quantifier scope. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English.</title>
<date>1973</date>
<booktitle>Approaches to Natural Languages,</booktitle>
<pages>221--242</pages>
<editor>In Jaakko Hintikka, Julius Moravcsik, and Patrick Suppes, editors,</editor>
<location>Reidel, Dordrecht.</location>
<contexts>
<context position="5384" citStr="Montague (1973)" startWordPosition="854" endWordPosition="855">es to possible worlds, and that the crucial difference between them has to do with the number of objects participating in individual relationships. Section 4 discusses our techniques for extracting the pragmatic knowledge that allows us to make judgments about quantifier scope. Section 5 presents our probabilistic model for resolving scope ambiguities. We present an empirical study in section 6, and section 7 concludes and suggests items for future work. 2 Related Work Quantifier scope disambiguation has received attention in linguistics and computational linguistics since at least the 1970s. Montague (1973) gave a seminal treatment of quantifier ambiguities, and argued that a particular syntax-based mechanism known as “quantifying-in” could resolve scope ambiguities. Since then, most work on disambiguation has focused on syntactic clues for determining which readings of an ambiguous statement are possible, and of the set of possible readings, which ones are preferred (Van Lehn, 1978; Hobbs and Shieber, 1987; Poesio, 1993a; Hurum, 1988; Moran, 1988). For instance, one linguistic study (Kurtzman and MacDonald, 1993) determined that in active voice sentences where quantifiers in the subject and obj</context>
</contexts>
<marker>Montague, 1973</marker>
<rawString>Richard Montague. 1973. The proper treatment of quantification in ordinary English. In Jaakko Hintikka, Julius Moravcsik, and Patrick Suppes, editors, Approaches to Natural Languages, pages 221–242. Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Moran</author>
</authors>
<title>Quantifier scoping in the SRI core language engine.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Assoc. for Comp. Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="5834" citStr="Moran, 1988" startWordPosition="925" endWordPosition="926">work. 2 Related Work Quantifier scope disambiguation has received attention in linguistics and computational linguistics since at least the 1970s. Montague (1973) gave a seminal treatment of quantifier ambiguities, and argued that a particular syntax-based mechanism known as “quantifying-in” could resolve scope ambiguities. Since then, most work on disambiguation has focused on syntactic clues for determining which readings of an ambiguous statement are possible, and of the set of possible readings, which ones are preferred (Van Lehn, 1978; Hobbs and Shieber, 1987; Poesio, 1993a; Hurum, 1988; Moran, 1988). For instance, one linguistic study (Kurtzman and MacDonald, 1993) determined that in active voice sentences where quantifiers in the subject and object give rise to scope ambiguity, there is a preference for the reading in which the subject quantifier has wide scope — the direct reading is acceptable 70-80% of the time, whereas the indirect reading is acceptable 30-40% of the time. Sentences that are similar in all respects except that they are passive voice have no such preference. Nevertheless, in these studies both readings are often quite plausible. In addition to syntactic clues, other </context>
</contexts>
<marker>Moran, 1988</marker>
<rawString>Douglas B. Moran. 1988. Quantifier scoping in the SRI core language engine. In Proceedings of the 26th Annual Meeting of the Assoc. for Comp. Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong C Park</author>
</authors>
<title>Quantifier scope and constituency.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Assoc. for Comp. Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="7461" citStr="Park, 1988" startWordPosition="1180" endWordPosition="1181">97; Moran, 1988). Saba and Corriveau (2001) recently proposed a test for quantifier scope disambiguation using pragmatic knowledge. However, they do not show how to extract the necessary information, nor do they implement or evaluate their proposed test. Due to the difficulty of the problem, several authors have devised techniques for “underspecified” logical representations that can efficiently store multiple ambiguous readings, and they devise techniques for automated reasoning using underspecified representations (Reyle, 1995; Latecki, 1992; Poesio, 1993b). Others (Hobbs and Shieber, 1987; Park, 1988) have devised computational mechanisms for generating all of the possible readings of statements exhibiting quantifier ambiguity, especially in cases involving more than two quantifiers. Detecting functions in extracted relational data has been studied in several contexts. Ritter et al.(2008) use knowledge of functions to determine when two extracted relationships contradict one another. Knowledge of functions has also been important in finding synonyms (Yates and Etzioni, 2009) and in review mining (Popescu, 2007). We extend this work by extracting not just a binary determination of whether a</context>
</contexts>
<marker>Park, 1988</marker>
<rawString>Jong C. Park. 1988. Quantifier scope and constituency. In Proceedings of the 26th Annual Meeting of the Assoc. for Comp. Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
</authors>
<title>Assigning a semantic scope to operators.</title>
<date>1993</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="5806" citStr="Poesio, 1993" startWordPosition="921" endWordPosition="922">d suggests items for future work. 2 Related Work Quantifier scope disambiguation has received attention in linguistics and computational linguistics since at least the 1970s. Montague (1973) gave a seminal treatment of quantifier ambiguities, and argued that a particular syntax-based mechanism known as “quantifying-in” could resolve scope ambiguities. Since then, most work on disambiguation has focused on syntactic clues for determining which readings of an ambiguous statement are possible, and of the set of possible readings, which ones are preferred (Van Lehn, 1978; Hobbs and Shieber, 1987; Poesio, 1993a; Hurum, 1988; Moran, 1988). For instance, one linguistic study (Kurtzman and MacDonald, 1993) determined that in active voice sentences where quantifiers in the subject and object give rise to scope ambiguity, there is a preference for the reading in which the subject quantifier has wide scope — the direct reading is acceptable 70-80% of the time, whereas the indirect reading is acceptable 30-40% of the time. Sentences that are similar in all respects except that they are passive voice have no such preference. Nevertheless, in these studies both readings are often quite plausible. In additio</context>
<context position="7413" citStr="Poesio, 1993" startWordPosition="1173" endWordPosition="1174">this task (Van Lehn, 1978; Saba and Corriveau, 1997; Moran, 1988). Saba and Corriveau (2001) recently proposed a test for quantifier scope disambiguation using pragmatic knowledge. However, they do not show how to extract the necessary information, nor do they implement or evaluate their proposed test. Due to the difficulty of the problem, several authors have devised techniques for “underspecified” logical representations that can efficiently store multiple ambiguous readings, and they devise techniques for automated reasoning using underspecified representations (Reyle, 1995; Latecki, 1992; Poesio, 1993b). Others (Hobbs and Shieber, 1987; Park, 1988) have devised computational mechanisms for generating all of the possible readings of statements exhibiting quantifier ambiguity, especially in cases involving more than two quantifiers. Detecting functions in extracted relational data has been studied in several contexts. Ritter et al.(2008) use knowledge of functions to determine when two extracted relationships contradict one another. Knowledge of functions has also been important in finding synonyms (Yates and Etzioni, 2009) and in review mining (Popescu, 2007). We extend this work by extract</context>
</contexts>
<marker>Poesio, 1993</marker>
<rawString>Massimo Poesio. 1993a. Assigning a semantic scope to operators. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
</authors>
<title>Assigning a semantic scope to operators.</title>
<date>1993</date>
<booktitle>In Proceedings of the Second Conference on Situation Theory and Its Applications.</booktitle>
<contexts>
<context position="5806" citStr="Poesio, 1993" startWordPosition="921" endWordPosition="922">d suggests items for future work. 2 Related Work Quantifier scope disambiguation has received attention in linguistics and computational linguistics since at least the 1970s. Montague (1973) gave a seminal treatment of quantifier ambiguities, and argued that a particular syntax-based mechanism known as “quantifying-in” could resolve scope ambiguities. Since then, most work on disambiguation has focused on syntactic clues for determining which readings of an ambiguous statement are possible, and of the set of possible readings, which ones are preferred (Van Lehn, 1978; Hobbs and Shieber, 1987; Poesio, 1993a; Hurum, 1988; Moran, 1988). For instance, one linguistic study (Kurtzman and MacDonald, 1993) determined that in active voice sentences where quantifiers in the subject and object give rise to scope ambiguity, there is a preference for the reading in which the subject quantifier has wide scope — the direct reading is acceptable 70-80% of the time, whereas the indirect reading is acceptable 30-40% of the time. Sentences that are similar in all respects except that they are passive voice have no such preference. Nevertheless, in these studies both readings are often quite plausible. In additio</context>
<context position="7413" citStr="Poesio, 1993" startWordPosition="1173" endWordPosition="1174">this task (Van Lehn, 1978; Saba and Corriveau, 1997; Moran, 1988). Saba and Corriveau (2001) recently proposed a test for quantifier scope disambiguation using pragmatic knowledge. However, they do not show how to extract the necessary information, nor do they implement or evaluate their proposed test. Due to the difficulty of the problem, several authors have devised techniques for “underspecified” logical representations that can efficiently store multiple ambiguous readings, and they devise techniques for automated reasoning using underspecified representations (Reyle, 1995; Latecki, 1992; Poesio, 1993b). Others (Hobbs and Shieber, 1987; Park, 1988) have devised computational mechanisms for generating all of the possible readings of statements exhibiting quantifier ambiguity, especially in cases involving more than two quantifiers. Detecting functions in extracted relational data has been studied in several contexts. Ritter et al.(2008) use knowledge of functions to determine when two extracted relationships contradict one another. Knowledge of functions has also been important in finding synonyms (Yates and Etzioni, 2009) and in review mining (Popescu, 2007). We extend this work by extract</context>
</contexts>
<marker>Poesio, 1993</marker>
<rawString>Massimo Poesio. 1993b. Assigning a semantic scope to operators. In Proceedings of the Second Conference on Situation Theory and Its Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
</authors>
<title>Information Extraction from Unstructured Web Text.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Washington.</institution>
<contexts>
<context position="7981" citStr="Popescu, 2007" startWordPosition="1254" endWordPosition="1255">ions (Reyle, 1995; Latecki, 1992; Poesio, 1993b). Others (Hobbs and Shieber, 1987; Park, 1988) have devised computational mechanisms for generating all of the possible readings of statements exhibiting quantifier ambiguity, especially in cases involving more than two quantifiers. Detecting functions in extracted relational data has been studied in several contexts. Ritter et al.(2008) use knowledge of functions to determine when two extracted relationships contradict one another. Knowledge of functions has also been important in finding synonyms (Yates and Etzioni, 2009) and in review mining (Popescu, 2007). We extend this work by extracting not just a binary determination of whether a relation is functional, but a distribution over the expected number of arguments for that relation. Our technique also differs from previous work based on extracted relationships between named entities. We leverage domain-independent extraction patterns involving numeric phrases, as discussed below; our technique is complementary to existing approaches and could in fact be combined with them for even greater accuracy. Finally, we apply the extracted knowledge in a novel way to quantifier scope disambiguation. Our </context>
<context position="29372" citStr="Popescu, 2007" startWordPosition="4981" endWordPosition="4982">ons Qr appearing in the training set. We then perform a gradient descent search to find optimal settings for the αi on the training data. 6 Experiments We report on two sets of experiments. The first tests our extraction technique on its own, and the second tests the accuracy of our complete QSD system, including the extraction mechanisms and the prediction model, on a quantifier scope disambiguation task. 6.1 Function Detection Experiment Function detection is an important task in its own right, and has been used in several previous applications (Ritter et al., 2008; Yates and Etzioni, 2009; Popescu, 2007). To turn our extraction system into a classifier for functions vs. nonfunctions, we simply checked whether there were 1471 Num Precision Recall F1 Functions 54 .79 .76 .77 Non-functions 74 .83 .85 .84 Table 3: Precision and recall for detecting functions using the numeric extraction technique. any extractions for R with Fn2 &gt; 1. If so, we predicted that the R was nonfunctional, and otherwise we predicted it was functional. We used the Web1Tgram Corpus of n-grams provided by Google, Inc to extract classes, relations, and counts. This corpus contains counts for 2- through 5-grams that appear on</context>
</contexts>
<marker>Popescu, 2007</marker>
<rawString>Ana-Maria Popescu. 2007. Information Extraction from Unstructured Web Text. Ph.D. thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uwe Reyle</author>
</authors>
<title>On reasoning with ambiguities.</title>
<date>1995</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7384" citStr="Reyle, 1995" startWordPosition="1168" endWordPosition="1169">the world) is necessary for this task (Van Lehn, 1978; Saba and Corriveau, 1997; Moran, 1988). Saba and Corriveau (2001) recently proposed a test for quantifier scope disambiguation using pragmatic knowledge. However, they do not show how to extract the necessary information, nor do they implement or evaluate their proposed test. Due to the difficulty of the problem, several authors have devised techniques for “underspecified” logical representations that can efficiently store multiple ambiguous readings, and they devise techniques for automated reasoning using underspecified representations (Reyle, 1995; Latecki, 1992; Poesio, 1993b). Others (Hobbs and Shieber, 1987; Park, 1988) have devised computational mechanisms for generating all of the possible readings of statements exhibiting quantifier ambiguity, especially in cases involving more than two quantifiers. Detecting functions in extracted relational data has been studied in several contexts. Ritter et al.(2008) use knowledge of functions to determine when two extracted relationships contradict one another. Knowledge of functions has also been important in finding synonyms (Yates and Etzioni, 2009) and in review mining (Popescu, 2007). W</context>
</contexts>
<marker>Reyle, 1995</marker>
<rawString>Uwe Reyle. 1995. On reasoning with ambiguities. In Proceedings of the EACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="20884" citStr="Richardson and Domingos, 2006" startWordPosition="3449" endWordPosition="3452">with the distribution for the relation and just one class. We use equal weights for all interpolated distributions. 5 A Probabilistic Model for Quantifier Scope Disambiguation QSD requires reasoning about different possible states of the world. This involves logical reasoning, since the direct and indirect readings differ in the number of objects that exist in models satisfying each reading, and the number of relationships between those objects. QSD also involves probabilistic reasoning, since none of the extracted knowledge is certain. We leverage recent work on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) to incorporate both types of reasoning into our technique for QSD. We next briefly review MLNs, before describing our model and 1469 methods for training it. 5.1 Markov Logic Networks Syntactically, an MLN consists of a set of firstorder logical formulas F and a real-valued weight wF for each F ∈ F. Semantically, an MLN defines a probability distribution over possible groundings of the logical formulas. That is, if U denotes the set of all objects in the universe, and G denotes the set of all possible ways to ground every F ∈ F (i.e., substitute an element from U for every variable in F), the</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Doug Downey</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>It’s a contradiction — No, it’s not: A case study using functional relations.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4096" citStr="Ritter et al., 2008" startWordPosition="644" endWordPosition="647">m of pragmatic knowledge for use in quantifier scope disambiguation. Our contributions are: 1) We build an extraction mechanism for extracting pragmatic knowledge about relations. In par1465 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1465–1474, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ticular, we extract knowledge about the expected sizes of the sets of objects that participate in the relations. The task of identifying functional relationships is a subtask of our extraction problem that has received recent attention in the literature (Ritter et al., 2008). 2) We devise a novel probabilistic model in the Markov Logic Network framework for reasoning over possible readings of sentences that involve quantifier scope ambiguities. The model is able to assign a probability that a particular reading is plausible, given the pragmatic knowledge we extract. 3) We provide an empirical demonstration that our system is able to resolve quantifier scope ambiguities in cases where the syntactic and lexical features used by previous systems are of no help. The remainder of this paper is organized as follows. The next section describes previous work. Section 3 s</context>
<context position="13726" citStr="Ritter et al., 2008" startWordPosition="2207" endWordPosition="2210"> are there?), and 2) the expected number of Y participants in a relationship R, given that there is exactly 1 X participant (e.g., how many cities does 1 doctor live in?). In both cases, we are concerned with extracting sizes of sets. Previous extraction systems have attempted to estimate set sizes based on extracted named entities. Downey et al. (2005)estimate the size of classes based on the number of named-entities extracted for the class. As far as we are aware, finding the expected size of an argument set for a relation is a novel task for information extraction, but several researchers (Ritter et al., 2008; Yates and Etzioni, 2009) have investigated the special case of detecting functional relations — those relations where the expected size of the Y argument set is precisely 1. As with class size extraction, they use extractions involving named-entity arguments to find functional relations. Approaches that depend on named-entity extractions have several disadvantages: they must find a large set of named-entities for every set, which can be time-consuming and difficult. Also, many classes, like “trees” and “hot dogs,” have no or very few named instances, but many un-named instances, so approache</context>
<context position="29331" citStr="Ritter et al., 2008" startWordPosition="4973" endWordPosition="4976">ndirect), and run the extractor for the relations Qr appearing in the training set. We then perform a gradient descent search to find optimal settings for the αi on the training data. 6 Experiments We report on two sets of experiments. The first tests our extraction technique on its own, and the second tests the accuracy of our complete QSD system, including the extraction mechanisms and the prediction model, on a quantifier scope disambiguation task. 6.1 Function Detection Experiment Function detection is an important task in its own right, and has been used in several previous applications (Ritter et al., 2008; Yates and Etzioni, 2009; Popescu, 2007). To turn our extraction system into a classifier for functions vs. nonfunctions, we simply checked whether there were 1471 Num Precision Recall F1 Functions 54 .79 .76 .77 Non-functions 74 .83 .85 .84 Table 3: Precision and recall for detecting functions using the numeric extraction technique. any extractions for R with Fn2 &gt; 1. If so, we predicted that the R was nonfunctional, and otherwise we predicted it was functional. We used the Web1Tgram Corpus of n-grams provided by Google, Inc to extract classes, relations, and counts. This corpus contains cou</context>
</contexts>
<marker>Ritter, Downey, Soderland, Etzioni, 2008</marker>
<rawString>Alan Ritter, Doug Downey, Stephen Soderland, and Oren Etzioni. 2008. It’s a contradiction — No, it’s not: A case study using functional relations. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walid S Saba</author>
<author>Jean-Pierre Corriveau</author>
</authors>
<title>A pragmatic treatment of quantification in natural language.</title>
<date>1997</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6852" citStr="Saba and Corriveau, 1997" startWordPosition="1089" endWordPosition="1092">es that are similar in all respects except that they are passive voice have no such preference. Nevertheless, in these studies both readings are often quite plausible. In addition to syntactic clues, other studies have noted that the choice of quantifier has a significant effect on scope disambiguation (e.g., “each” has a greater tendency for wide scope than “every”) (Van Lehn, 1978; Alshawi, 1990). Most authors have noted that both syntactic and lexical evidence fall short of a full solution, and that pragmatic knowledge (knowledge about the world) is necessary for this task (Van Lehn, 1978; Saba and Corriveau, 1997; Moran, 1988). Saba and Corriveau (2001) recently proposed a test for quantifier scope disambiguation using pragmatic knowledge. However, they do not show how to extract the necessary information, nor do they implement or evaluate their proposed test. Due to the difficulty of the problem, several authors have devised techniques for “underspecified” logical representations that can efficiently store multiple ambiguous readings, and they devise techniques for automated reasoning using underspecified representations (Reyle, 1995; Latecki, 1992; Poesio, 1993b). Others (Hobbs and Shieber, 1987; Pa</context>
</contexts>
<marker>Saba, Corriveau, 1997</marker>
<rawString>Walid S. Saba and Jean-Pierre Corriveau. 1997. A pragmatic treatment of quantification in natural language. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walid S Saba</author>
<author>Jean-Pierre Corriveau</author>
</authors>
<title>Plausible reasoning and the resolution of quantifier scope ambiguities. Studia Logica,</title>
<date>2001</date>
<pages>67--271</pages>
<contexts>
<context position="2428" citStr="Saba and Corriveau, 2001" startWordPosition="384" endWordPosition="387">gs wants(b,d) The readings differ only in the order of the quantifiers. The quantifier that comes first in each expression is said to have wide scope; the second quantifier has narrow scope. Linguists and NLP researchers have come up with several theories and mechanisms for automatically determining the scope of quantified linguistic expressions. Despite a long history of proposed solutions, however, researchers have for the most part abandoned this task as hopeless because of “overwhelming evidence suggesting that quantifier scope is a phenomenon that must be treated at the pragmatic level” (Saba and Corriveau, 2001). For example, in active voice clauses, the quantifier for the subject noun is usually preferred for wide scope over the quantifier of the predicate noun (Kurtzman and MacDonald, 1993). But such preferences can easily be overruled by world knowledge: A doctor lives in every city. 1. �IdEDocs bcECities lives in(d,c) (A single doctor lives in all cities.) 2. bcECities �IdEDocs lives in(d,c) (Each city has a different doctor living there.) Syntactic preferences would normally indicate that reading 1 is better, but in this particular case common-sense knowledge of the world overrules that preferen</context>
<context position="6893" citStr="Saba and Corriveau (2001)" startWordPosition="1095" endWordPosition="1098">pt that they are passive voice have no such preference. Nevertheless, in these studies both readings are often quite plausible. In addition to syntactic clues, other studies have noted that the choice of quantifier has a significant effect on scope disambiguation (e.g., “each” has a greater tendency for wide scope than “every”) (Van Lehn, 1978; Alshawi, 1990). Most authors have noted that both syntactic and lexical evidence fall short of a full solution, and that pragmatic knowledge (knowledge about the world) is necessary for this task (Van Lehn, 1978; Saba and Corriveau, 1997; Moran, 1988). Saba and Corriveau (2001) recently proposed a test for quantifier scope disambiguation using pragmatic knowledge. However, they do not show how to extract the necessary information, nor do they implement or evaluate their proposed test. Due to the difficulty of the problem, several authors have devised techniques for “underspecified” logical representations that can efficiently store multiple ambiguous readings, and they devise techniques for automated reasoning using underspecified representations (Reyle, 1995; Latecki, 1992; Poesio, 1993b). Others (Hobbs and Shieber, 1987; Park, 1988) have devised computational mech</context>
<context position="12817" citStr="Saba and Corriveau (2001)" startWordPosition="2043" endWordPosition="2046">describes worlds where every city is inhabited by some doctor, but potentially a different doctor per city. Using pragmatic knowledge, the reader can easily deduce that this logical statement is a much more likely reading than φ. Let B represent the reader’s pragmatic knowledge, including facts like “People don’t simultaneously live in more than one city,” and, “There are at least hundreds of cities in the world.” The reader can easily deduce that B � -,φ. We now turn to methods for extracting the necessary pragmatic knowledge B from text. 1467 4 Extraction Techniques to Support QSD Decisions Saba and Corriveau (2001) point out that there is a restricted form of pragmatic knowledge that can be used in many instances of QSD. Consider the facts that were used above to determine that 0&apos; is preferable to 0. The facts fall into two basic categories of knowledge: 1) the size of class C (e.g., how many cities are there?), and 2) the expected number of Y participants in a relationship R, given that there is exactly 1 X participant (e.g., how many cities does 1 doctor live in?). In both cases, we are concerned with extracting sizes of sets. Previous extraction systems have attempted to estimate set sizes based on e</context>
</contexts>
<marker>Saba, Corriveau, 2001</marker>
<rawString>Walid S. Saba and Jean-Pierre Corriveau. 2001. Plausible reasoning and the resolution of quantifier scope ambiguities. Studia Logica, 67:271–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Dan Weld</author>
</authors>
<title>Scaling textual inference to the web.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Schoenmackers, Etzioni, Weld, 2008</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, and Dan Weld. 2008. Scaling textual inference to the web. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>34--255</pages>
<contexts>
<context position="7944" citStr="Yates and Etzioni, 2009" startWordPosition="1246" endWordPosition="1249">ated reasoning using underspecified representations (Reyle, 1995; Latecki, 1992; Poesio, 1993b). Others (Hobbs and Shieber, 1987; Park, 1988) have devised computational mechanisms for generating all of the possible readings of statements exhibiting quantifier ambiguity, especially in cases involving more than two quantifiers. Detecting functions in extracted relational data has been studied in several contexts. Ritter et al.(2008) use knowledge of functions to determine when two extracted relationships contradict one another. Knowledge of functions has also been important in finding synonyms (Yates and Etzioni, 2009) and in review mining (Popescu, 2007). We extend this work by extracting not just a binary determination of whether a relation is functional, but a distribution over the expected number of arguments for that relation. Our technique also differs from previous work based on extracted relationships between named entities. We leverage domain-independent extraction patterns involving numeric phrases, as discussed below; our technique is complementary to existing approaches and could in fact be combined with them for even greater accuracy. Finally, we apply the extracted knowledge in a novel way to </context>
<context position="13752" citStr="Yates and Etzioni, 2009" startWordPosition="2211" endWordPosition="2214">the expected number of Y participants in a relationship R, given that there is exactly 1 X participant (e.g., how many cities does 1 doctor live in?). In both cases, we are concerned with extracting sizes of sets. Previous extraction systems have attempted to estimate set sizes based on extracted named entities. Downey et al. (2005)estimate the size of classes based on the number of named-entities extracted for the class. As far as we are aware, finding the expected size of an argument set for a relation is a novel task for information extraction, but several researchers (Ritter et al., 2008; Yates and Etzioni, 2009) have investigated the special case of detecting functional relations — those relations where the expected size of the Y argument set is precisely 1. As with class size extraction, they use extractions involving named-entity arguments to find functional relations. Approaches that depend on named-entity extractions have several disadvantages: they must find a large set of named-entities for every set, which can be time-consuming and difficult. Also, many classes, like “trees” and “hot dogs,” have no or very few named instances, but many un-named instances, so approaches based on named entities </context>
<context position="29356" citStr="Yates and Etzioni, 2009" startWordPosition="4977" endWordPosition="4980"> extractor for the relations Qr appearing in the training set. We then perform a gradient descent search to find optimal settings for the αi on the training data. 6 Experiments We report on two sets of experiments. The first tests our extraction technique on its own, and the second tests the accuracy of our complete QSD system, including the extraction mechanisms and the prediction model, on a quantifier scope disambiguation task. 6.1 Function Detection Experiment Function detection is an important task in its own right, and has been used in several previous applications (Ritter et al., 2008; Yates and Etzioni, 2009; Popescu, 2007). To turn our extraction system into a classifier for functions vs. nonfunctions, we simply checked whether there were 1471 Num Precision Recall F1 Functions 54 .79 .76 .77 Non-functions 74 .83 .85 .84 Table 3: Precision and recall for detecting functions using the numeric extraction technique. any extractions for R with Fn2 &gt; 1. If so, we predicted that the R was nonfunctional, and otherwise we predicted it was functional. We used the Web1Tgram Corpus of n-grams provided by Google, Inc to extract classes, relations, and counts. This corpus contains counts for 2- through 5-gram</context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. Journal of Artificial Intelligence Research (JAIR), 34:255–296, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
</authors>
<title>Detecting parser errors using webbased semantic filters.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Yates, Schoenmackers, Etzioni, 2006</marker>
<rawString>Alexander Yates, Stefan Schoenmackers, and Oren Etzioni. 2006. Detecting parser errors using webbased semantic filters. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>