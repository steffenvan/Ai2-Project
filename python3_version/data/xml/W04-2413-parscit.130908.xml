<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.034347">
<title confidence="0.98801">
Semantic Role Labelling With Chunk Sequences
</title>
<author confidence="0.996963">
Ulrike Baldewein, Katrin Erk, Sebastian Padó Detlef Prescher
</author>
<affiliation confidence="0.783222">
Saarland University University of Amsterdam
Saarbrücken, Germany Amsterdam, The Netherlands
</affiliation>
<email confidence="0.996416">
{ulrike,erk,pado}@coli.uni-sb.de prescher@science.uva.nl
</email>
<sectionHeader confidence="0.995602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999907222222222">
We describe a statistical approach to semantic
role labelling that employs only shallow infor-
mation. We use a Maximum Entropy learner,
augmented by EM-based clustering to model
the fit between a verb and its argument can-
didate. The instances to be classified are se-
quences of chunks that occur frequently as ar-
guments in the training corpus. Our best model
obtains an F score of 51.70 on the test set.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964636363637">
This paper describes a statistical approach to semantic
role labelling addressing the CoNLL shared task 2004,
which is based on the the current release of the English
PropBank data (Kingsbury et al., 2002). For further de-
tails of the task, see (Carreras and Màrquez, 2004).
We address the main challenge of the task, the absence
of deep syntactic information, with three main ideas:
Proper constituents being unavailable, we use chunk
sequences as instances for classification.
The classification is performed by a maximum en-
tropy model, which can integrate features from het-
erogeneous data sources.
We model the fit between verb and argument can-
didate by clusters induced with EM on the training
data, which we use as features during classification.
Sections 2 through 4 describe the systems’ architec-
ture. First, we compute chunk sequences for all sentences
(Sec. 2). Then, we classify these sequences with max-
imum entropy models (Sec. 3). Finally, we determine
the most probable chain of sequences covering the whole
sentence (Sec. 4). Section 5 discusses the impact of dif-
ferent parameters and gives final results.
</bodyText>
<sectionHeader confidence="0.905892" genericHeader="method">
2 Chunk Sequences as Instances
</sectionHeader>
<bodyText confidence="0.998416763157895">
All studies of semantic role labelling we are aware of
have used constituents as instances for classification.
However, constituents are not available in the shallow
syntactic information provided by this task. Two other
levels of granularity are available in the data: words and
chunks. In a pretest, we found that words are too fine
grained, such that learners find it very difficult to identify
argument boundaries on the word level. Chunks, too, are
problematic, since one third of the arguments span more
than one chunk, and for one tenth of the arguments the
boundaries do not coincide with any chunk boundaries.
We decided to use chunk sequences as instances for
classification. They can describe multi-chunk and part-
chunk arguments, and by approximating constituents,
they allow the use of linguistically informed features. In
the sentence in Figure 1, Britain’s manufacturing indus-
try forms a sequence of type NP_NP. To make sequences
more distinctive, we conflate whole clauses embedded
deeper than the target to S: For the target transform-
ing, we characterise the sequence for to boost exports
as S rather than VP_NP. An argument boundary inside
a chunk is indicated by the part of speech of the last in-
cluded word: For boost the sequence is VP(NN).
To determine “good” sequences, we collected argu-
ment realisations from the training corpus, generalising
them by simple heuristics (e.g. removing anything en-
closed in brackets). The generalised argument sequences
exhibit a Zipfian distribution (see Fig. 2). NP is by
far the most frequent sequence, followed by S. An ex-
ample of a very infrequent argument chunk sequence
is NP_PP_NP_PP_NP_VP_PP_NP_NP (in words: a
bonus in the form of charitable donations made from an
employer ’s treasury).
The chunk sequence approach also allows us to con-
sider the divider chunk sequences that separate arguments
and targets. For example, A0s are usually divided from
the target by the empty divider, while A2 arguments are
Britain ’s manufacturing industry is transforming itself to boost exports
</bodyText>
<equation confidence="0.963035666666667">
NNP POS VBG NN VBZ VBG PRP TO NN NNS
[NP ] [NP ] [VP ] [NP] [VP ] [NP ]
[S ]
</equation>
<figureCaption confidence="0.98043">
Figure 1: Part of a sentence with part of speech, chunk and clause information
Figure 2: Frequency distribution for the 20 most frequent
sequences and dividers in the training data
</figureCaption>
<bodyText confidence="0.999700882352941">
separated from it by e.g. a typical A1 sequence. Gen-
eralised divider chunk sequences separating actual argu-
ments and targets in the training set show a Zipfian distri-
bution similar to the chunk sequences (see Fig. 2).
As instances to be classified, we consider all sequences
whose generalised sequence and divider each appear at
least 10 times for an argument in the training corpus, and
whose generalised sequence and divider appear together
at least 5 times. The first cutoff reduces the number of
sequences from 1089 to 87, and the number of dividers
from 999 to 120, giving us 581,813 sequences as training
data (about twice as many as words), of which 45,707
are actual argument labels. The additional filter for se-
quence/divider pairs reduces the training data to 354,916
sequences, of which 43,622 are actual arguments. We pay
for the filtering by retaining only 87.49% of arguments on
the training set (83.32% on the development set).
</bodyText>
<sectionHeader confidence="0.999445" genericHeader="method">
3 Classification
</sectionHeader>
<subsectionHeader confidence="0.999772">
3.1 Maximum Entropy Modelling
</subsectionHeader>
<bodyText confidence="0.999313352941176">
We use a log-linear model as classifier, which defines the
probability of a class given an feature vector as
where is a normalisation constant, the value
of feature for class, and the weight assigned to.
The model is trained by optimising the weights subject
to the maximum entropy constraint which ensures that the
least committal optimal model is learnt. We used the es-
timate software for estimation, which implements the
LMVM algorithm (Malouf, 2002) and was kindly pro-
vided by Rob Malouf.
We chose a maximum entropy approach because it
can integrate many different sources of information with-
out assuming independence of features. Also, models
with minimal commitment are good predictors of future
data. Maxent models have found wide application in NLP
during the recent years; for semantic role labelling (on
FrameNet data) see (Fleischman et al., 2003).
</bodyText>
<subsectionHeader confidence="0.998385">
3.2 Classification Procedure
</subsectionHeader>
<bodyText confidence="0.999983266666667">
The most straightforward procedure would be to have the
classifier assign all argument classes plus NOLABEL to
sequences. However, this proved ineffective due to the
prevalence of NOLABEL: Since this class makes up more
than 80% of the training sequences, the classifier concen-
trates on assigning NOLABEL well.
Therefore, we divide the task of automatic semantic
role assignment into two classification subtasks: argu-
ment identification and argument labelling. Argument
identification is a binary decision for all sequences be-
tween LABEL (semantic argument) and NOLABEL (no
semantic argument), which allows us to pool the frequen-
cies of all argument labels. Argument labelling then as-
signs proper semantic roles only to those sequences that
were recognised as LABELs in the first step.
</bodyText>
<subsectionHeader confidence="0.978229">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999965722222222">
We experimented with four types of features: shallow
(mostly co-occurrence and distance statistics), higher-
level (linguistically informed), divider and em (results of
the EM-clustering).
Shallow Features. Our shallow features comprise
statistics on the current sequence and its position as well
as on the target: the sequence itself, the target lemma,
the length of the current sequence in chunks, its abso-
lute frequency, its position (before or after the target, as
first or last sequence in the sentence), its distance to the
target in question and its embedding depth in compari-
son with the target (with regard to clause embedding).
We also count how often we have seen the current se-
quence as an argument for the current target lemma, and
as which argument. Other features describe the context
of the sequence: whether it is embedded in an admissible
sequence or embeds one, and a two-chunk history. We
also list the arguments for which the sequence is the best
</bodyText>
<figure confidence="0.997348888888889">
Frequency in training data
25000
20000
15000
10000
5000
0
Sequence frequencies
Divider frequencies
</figure>
<bodyText confidence="0.997947441860465">
candidate, judging by its frequency.
Higher-Level Features. Our higher-level features
comprise a heuristically determined superchunk label
which is an abstraction of the chunk sequence (one of
NP, VP, PP, S, ADVP, ADJP, and the rest class THING),
the preposition of the sequence (if it either starts with or
is directly preceded by a preposition), and the lemma and
part of speech of the heuristically determined head of
the sequence. We also check if the sequence in question
is an NP (by its superchunk) directly before or after the
target, if the sequence contains prepositions in unusual
positions, if it consists of the word n’t or not, and if the
target lemma is passive.
Divider Features. These are shallow and higher-level
features related to the divider sequences: the divider it-
self, its superchunk, and we state whether, judging by the
divider, the sequence is an argument. A similar feature
judges this by the combination of divider and sequence.
Features based on EM-Based Clustering. We use
EM-based clustering to measure the fit between a tar-
get verb, an argument position of the verb, and the head
lemma (or head named entity) of a sequence.
EM-based clustering, originally introduced for the in-
duction of a semantically annotated lexicon (Rooth et al.,
1999), regards classes as hidden variables in the context
of maximum likelihood estimation from incomplete data
via the expectation maximisation algorithm.
In our application, we aim at deriving a probability dis-
tribution on verb-argument pairs from the training
data. Using the key idea that is conditioned on an un-
observed class , we define the probability of a pair
as:
The last line is warranted by the assumption that and
are independent and are only conditioned on. This
assumption makes clustering feasible in the first place.
We use the EM algorithm to maximise the incomplete
data log-likelihood as a function of
the probability distribution for a given empirical proba-
bility distribution .
In two additional features, we substitute the head word
by the sequence and divider characterisation respectively,
using EM clustering to measure the fit between target
verb, argument position, and sequence (or divider).
</bodyText>
<sectionHeader confidence="0.949197" genericHeader="method">
4 Finding the Best Chain of Sequences
</sectionHeader>
<bodyText confidence="0.9997855">
Classification only determines probable argument labels
for individual chunk sequences. We still have to deter-
mine the most probable chain of chunk sequences (such
as A0 A1) that covers the whole sentence.
Recall that there are about 1.6 times as many sequences
as words, many of which overlap; therefore, exhaustive
searching is infeasible. Instead, we first run a beam
search with a simple probability model to identify the
most probable chains of chunk sequences. Then, we re-
rank them to take global considerations into account.
Beam Search. For each sentence, we build an agenda
of partial argument chains. We calculate the proba-
bility of each chain as ,
thereby assuming independence of sequences. For each
sequence, we add the three most probable classes as-
signed by the argument labelling step. The result of the
beam search are the most probable (according to )
chains that cover the whole sentence. We found that in-
creasing the beam width to more than 20 increased per-
formance only marginally.
Re-ranking. Due to the independence assumption in
the beam search, chains that are assigned high probabil-
ity may still be globally improbable. We therefore mul-
tiply each chain’s probability by its empirical proba-
bility in the training data, using as a prior. How-
ever, since these counts are still sparse, we exploit the
fact that duplicate argument labels (i.e. discontinuous ar-
guments) are relatively infrequent in the PropBank data
by discounting chains with duplicate arguments by a fac-
tor, which we empirically optimised as .
</bodyText>
<sectionHeader confidence="0.995923" genericHeader="conclusions">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.998509142857143">
Optimising Step 1 (Argument Identification). On the
development set, we explored the impact of different fea-
tures from Section 3.3 on Step 1. Our optimal model con-
tained as shallow features: all except the sequence’s posi-
tion; as divider features: divider sequence; as higher-level
features: the preposition and the superchunk; as EM fea-
tures: all. Adding more features deteriorated the model.
</bodyText>
<table confidence="0.999661142857143">
Precision Recall
0.733 0.601
0.549 0.149
0.683 0.636
0.648 0.617
0.681 0.649
0.683 0.648
</table>
<tableCaption confidence="0.9206825">
Table 1: Different models for argument identification
(evaluation scores category-specific for LABEL)
</tableCaption>
<bodyText confidence="0.7603428">
Table 1 presents an overview of different combinations
of feature sets. We optimised category-specific F-score
for LABEL, since only examples with LABEL are for-
warded to Step 2. The first line (all) shows that the main
problem in the first step is the recall, which limits the
</bodyText>
<figure confidence="0.986340071428572">
Feature Sets
all
all - shallow
all - higher-level
all - divider
all - em
all ( )
F-Score
0.661
0.234
0.658
0.632
0.664
0.665
</figure>
<bodyText confidence="0.886364058823529">
amount of arguments available for Step 2. For this rea-
son, we varied the parameter of the classification pro-
cedure: LABEL() if LABEL . We found the
optimal category-specific F-score for , increas-
ing the recall at the cost of precision.
Optimising Step 2 (Argument Labelling). We per-
formed the same optimisation for Step 2, using the output
of our best model of Step 1 as input. The best model for
Step 2 uses all shallow features except the sequence’s po-
sition; all higher-level features but negation; all divider
features; no EM-clustering features. Table 2 shows the
performance of the complete system for different feature
sets. We also give two upper bounds for our system, one
caused by the arguments lost in the sequence computa-
tion, and one caused by the arguments missed by Step 1.
Table 2: Different models for argument labelling
(based on the best argument identification model)
</bodyText>
<figure confidence="0.968385761904762">
Precision Recall
1.00 0.833
1.00 0.648
0.649 0.416
0.104 0.064
0.616 0.393
0.642 0.415
F-Score
0.833
0.786
0.507
0.079
0.482
0.504
Feature Sets
Upper Bound 1
Upper Bound 2
all
all - shallow
all - higher-level
all - divider
</figure>
<table confidence="0.999904620689655">
Precision Recall F
Overall 65.73% 42.60% 51.70
A0 80.70% 56.92% 66.76
A1 59.60% 48.32% 53.37
A2 53.49% 28.99% 37.60
A3 45.10% 15.33% 22.89
A4 60.00% 18.00% 27.69
A5 0.00% 0.00% 0.00
AM-ADV 35.88% 15.31% 21.46
AM-CAU 14.29% 2.04% 3.57
AM-DIR 45.00% 18.00% 25.71
AM-DIS 58.33% 29.58% 39.25
AM-EXT 36.36% 28.57% 32.00
AM-LOC 40.23% 15.35% 22.22
AM-MNR 39.36% 14.51% 21.20
AM-MOD 97.98% 72.11% 83.08
AM-NEG 87.37% 65.35% 74.77
AM-PNC 43.48% 11.76% 18.52
AM-PRD 0.00% 0.00% 0.00
AM-TMP 55.94% 25.84% 35.35
R-A0 0.00% 0.00% 0.00
R-A1 0.00% 0.00% 0.00
R-A2 0.00% 0.00% 0.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 0.00% 0.00% 0.00
</table>
<tableCaption confidence="0.999646">
Table 3: Details of the best model on the test set
</tableCaption>
<bodyText confidence="0.999981264705882">
The final model on the test set. Our best model com-
bines the two models for Steps 1 and 2 indicated in bold-
face. Table 3 shows detailed results on the test set.
Discussion. During the development phase, we com-
pared the performance of our final architecture with one
that did not filter out on the basis of infrequent dividers
as outlines in Sec. 2. Even though we lose 7.5% of the ar-
guments in the development set by filtering, the F-score
improves by about 12%. This shows that intelligent filter-
ing is a crucial factor in a chunk sequence-based system.
The main problem for both subtasks is recall. This
might also be the reason for the disappointing perfor-
mance of the EM features, since the small amount of
available training data limits the coverage of the mod-
els. As a consequence, EM features tend to increase the
precision of a model at the cost of recall. At the overall
low level of recall, the addition of EM features results in
a virtually unchanged performance for Step 1 and even a
suboptimal result for Step 2.
For both of our subtasks, adding more features to a
given model can harm its performance. Evidently, some
features predict the training data better than the develop-
ment data, and can mislead the model. This can be seen
as a kind of overfitting. Therefore, it is important to test
not only feature sets, but also single features.
The two subtasks have rather different profiles. Ta-
ble 1 shows that Step 1 hardly uses higher-level features,
while the single divider feature has some impact. Step 2,
on the other hand, improves considerably when higher-
level features are added; divider features are less impor-
tant (see Table 2). It appears that the split of semantic
role labelling into argument identification and argument
labelling mirrors a natural division of the problem, whose
two parts rely on different types of information.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998296">
X. Carreras and L. Màrquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labelling. In
Proc. of CoNLL-2004, Boston, MA.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum
entropy models for FrameNet classification. In Proc.
of EMNLP’03, Sapporo, Japan.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
semantic annotation to the Penn TreeBank. In Proc. of
HLT, San Diego, California.
R. Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-02, Taipei, Taiwan.
M. Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil.
1999. Inducing a semantically annotated lexicon via
EM-based clustering. In Proc. ofACL’99.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839912">
<title confidence="0.998913">Semantic Role Labelling With Chunk Sequences</title>
<author confidence="0.972737">Ulrike Baldewein</author>
<author confidence="0.972737">Katrin Erk</author>
<author confidence="0.972737">Sebastian Padó Detlef Prescher</author>
<affiliation confidence="0.99963">Saarland University University of Amsterdam</affiliation>
<address confidence="0.963424">Saarbrücken, Germany Amsterdam, The</address>
<email confidence="0.906936">ulrike@coli.uni-sb.deprescher@science.uva.nl</email>
<email confidence="0.906936">erk@coli.uni-sb.deprescher@science.uva.nl</email>
<email confidence="0.906936">pado@coli.uni-sb.deprescher@science.uva.nl</email>
<abstract confidence="0.9983346">We describe a statistical approach to semantic role labelling that employs only shallow information. We use a Maximum Entropy learner, augmented by EM-based clustering to model the fit between a verb and its argument can- The instances to be classified are sechunks that occur frequently as arguments in the training corpus. Our best model obtains an F score of 51.70 on the test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labelling.</title>
<date>2004</date>
<booktitle>In Proc. of CoNLL-2004,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="953" citStr="Carreras and Màrquez, 2004" startWordPosition="140" endWordPosition="143">that employs only shallow information. We use a Maximum Entropy learner, augmented by EM-based clustering to model the fit between a verb and its argument candidate. The instances to be classified are sequences of chunks that occur frequently as arguments in the training corpus. Our best model obtains an F score of 51.70 on the test set. 1 Introduction This paper describes a statistical approach to semantic role labelling addressing the CoNLL shared task 2004, which is based on the the current release of the English PropBank data (Kingsbury et al., 2002). For further details of the task, see (Carreras and Màrquez, 2004). We address the main challenge of the task, the absence of deep syntactic information, with three main ideas: Proper constituents being unavailable, we use chunk sequences as instances for classification. The classification is performed by a maximum entropy model, which can integrate features from heterogeneous data sources. We model the fit between verb and argument candidate by clusters induced with EM on the training data, which we use as features during classification. Sections 2 through 4 describe the systems’ architecture. First, we compute chunk sequences for all sentences (Sec. 2). Th</context>
</contexts>
<marker>Carreras, Màrquez, 2004</marker>
<rawString>X. Carreras and L. Màrquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labelling. In Proc. of CoNLL-2004, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>N Kwon</author>
<author>E Hovy</author>
</authors>
<title>Maximum entropy models for FrameNet classification.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP’03,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5981" citStr="Fleischman et al., 2003" startWordPosition="960" endWordPosition="963">sing the weights subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt. We used the estimate software for estimation, which implements the LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. We chose a maximum entropy approach because it can integrate many different sources of information without assuming independence of features. Also, models with minimal commitment are good predictors of future data. Maxent models have found wide application in NLP during the recent years; for semantic role labelling (on FrameNet data) see (Fleischman et al., 2003). 3.2 Classification Procedure The most straightforward procedure would be to have the classifier assign all argument classes plus NOLABEL to sequences. However, this proved ineffective due to the prevalence of NOLABEL: Since this class makes up more than 80% of the training sequences, the classifier concentrates on assigning NOLABEL well. Therefore, we divide the task of automatic semantic role assignment into two classification subtasks: argument identification and argument labelling. Argument identification is a binary decision for all sequences between LABEL (semantic argument) and NOLABEL</context>
</contexts>
<marker>Fleischman, Kwon, Hovy, 2003</marker>
<rawString>M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum entropy models for FrameNet classification. In Proc. of EMNLP’03, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
<author>M Marcus</author>
</authors>
<title>Adding semantic annotation to the Penn TreeBank.</title>
<date>2002</date>
<booktitle>In Proc. of HLT,</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="886" citStr="Kingsbury et al., 2002" startWordPosition="128" endWordPosition="131"> We describe a statistical approach to semantic role labelling that employs only shallow information. We use a Maximum Entropy learner, augmented by EM-based clustering to model the fit between a verb and its argument candidate. The instances to be classified are sequences of chunks that occur frequently as arguments in the training corpus. Our best model obtains an F score of 51.70 on the test set. 1 Introduction This paper describes a statistical approach to semantic role labelling addressing the CoNLL shared task 2004, which is based on the the current release of the English PropBank data (Kingsbury et al., 2002). For further details of the task, see (Carreras and Màrquez, 2004). We address the main challenge of the task, the absence of deep syntactic information, with three main ideas: Proper constituents being unavailable, we use chunk sequences as instances for classification. The classification is performed by a maximum entropy model, which can integrate features from heterogeneous data sources. We model the fit between verb and argument candidate by clusters induced with EM on the training data, which we use as features during classification. Sections 2 through 4 describe the systems’ architectur</context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding semantic annotation to the Penn TreeBank. In Proc. of HLT, San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL-02,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5576" citStr="Malouf, 2002" startWordPosition="898" endWordPosition="899">pay for the filtering by retaining only 87.49% of arguments on the training set (83.32% on the development set). 3 Classification 3.1 Maximum Entropy Modelling We use a log-linear model as classifier, which defines the probability of a class given an feature vector as where is a normalisation constant, the value of feature for class, and the weight assigned to. The model is trained by optimising the weights subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt. We used the estimate software for estimation, which implements the LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. We chose a maximum entropy approach because it can integrate many different sources of information without assuming independence of features. Also, models with minimal commitment are good predictors of future data. Maxent models have found wide application in NLP during the recent years; for semantic role labelling (on FrameNet data) see (Fleischman et al., 2003). 3.2 Classification Procedure The most straightforward procedure would be to have the classifier assign all argument classes plus NOLABEL to sequences. However, this proved ineffective due to th</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>R. Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proc. of CoNLL-02, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rooth</author>
<author>S Riezler</author>
<author>D Prescher</author>
<author>G Carroll</author>
<author>F Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proc. ofACL’99.</booktitle>
<contexts>
<context position="9151" citStr="Rooth et al., 1999" startWordPosition="1466" endWordPosition="1469">get lemma is passive. Divider Features. These are shallow and higher-level features related to the divider sequences: the divider itself, its superchunk, and we state whether, judging by the divider, the sequence is an argument. A similar feature judges this by the combination of divider and sequence. Features based on EM-Based Clustering. We use EM-based clustering to measure the fit between a target verb, an argument position of the verb, and the head lemma (or head named entity) of a sequence. EM-based clustering, originally introduced for the induction of a semantically annotated lexicon (Rooth et al., 1999), regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm. In our application, we aim at deriving a probability distribution on verb-argument pairs from the training data. Using the key idea that is conditioned on an unobserved class , we define the probability of a pair as: The last line is warranted by the assumption that and are independent and are only conditioned on. This assumption makes clustering feasible in the first place. We use the EM algorithm to maximise the incomplete data log-likelihood </context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>M. Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proc. ofACL’99.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>