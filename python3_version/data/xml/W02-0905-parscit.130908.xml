<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000146">
<title confidence="0.998129">
Using Co-Composition for Acquiring Syntactic and Semantic
Subcategorisation
</title>
<author confidence="0.998109">
Pablo Gamallo Alexandre Agustini Gabriel P. Lopes
</author>
<affiliation confidence="0.9992755">
Department of Computer Science
New University of Lisbon, Portugal
</affiliation>
<email confidence="0.995191">
gamallo,aagustini,gpl @di.fct.unl.pt
</email>
<sectionHeader confidence="0.995594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999784272727273">
Natural language parsing requires ex-
tensive lexicons containing subcategori-
sation information for specific sublan-
guages. This paper describes an unsuper-
vised method for acquiring both syntac-
tic and semantic subcategorisation restric-
tions from corpora. Special attention will
be paid to the role of co-composition in
the acquisition strategy. The acquired in-
formation is used for lexicon tuning and
parsing improvement.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987325">
Recent lexicalist Grammars project the subcat-
egorisation information encoding in the lexicon
onto syntactic structures. These grammars use
accurate subcategorised lexicons to restrict potential
syntactic structures. In terms of parsing devel-
opment, it is broadly assumed that parsers need
such information in order to reduce the number of
possible analyses and, therefore, solve syntactic
ambiguity. Over the last years various methods
for acquiring subcategorisation information from
corpora has been proposed. Some of them induce
syntactic subcategorisation from tagged texts
(Brent, 1993; Briscoe and Carrol, 1997; Marques,
2000). Unfortunately, syntactic information is not
enough to solve structural ambiguity. Consider the
following verbal phrases:
</bodyText>
<listItem confidence="0.9993595">
(1) [peel [ the potato] [ with a knife]]
(2) [peel [ [ the potato] [ with a rough stain]]]
</listItem>
<bodyText confidence="0.999913181818182">
The attachment of “with PP” to both the verb
“peel” in phrase (1) and to the NP “the potato” in
(2) does not depend only on syntactic requirements.
Indeed, it is not possible to attach the PP “with
a knife” to the verb “peel” by asserting that this
verb subcategorises a “with PP’. Such a subcate-
gorisation information cannot be used to explain
the analysis of phrase (2), where it is the NP “the
potato” that is attached to the “with PP”. In order
to decide the correct analysis in both phrases, we
are helped by our world knowledge about the action
of peeling, the use of knifes, and the attributes
of potatoes. In general, we know that knifes are
used for peeling, and potatoes can have different
kinds of stains. So, the parser is able to propose a
correct analysis only if the lexicon is provided with,
not only syntactic subcategorisation information,
but also with information on semantic-pragmatic
requirements (i.e., with selection restrictions).
Other works attempt to acquire selection restric-
tions requiring pre-existing lexical ressources. The
learning algorithm requires sample corpora to be
constituted by verb-noun, noun-verb, or verb-prep-
noun dependencies, where the nouns are semanti-
cally tagged by using lexical hierarchies such as
WordNet (Resnik, 1997; Framis, 1995). Selection
restrictions are induced by considering those depen-
dencies associated with the same semantic tags. For
instance, if verb ratify frequently appears with nouns
semantically tagged as “legal documents” in the di-
rect object position (e.g., article, law, precept, ... ),
then it follows that it must select for nouns denot-
ing legal documents. Unfortunately, if a pre-defined
</bodyText>
<note confidence="0.487888333333333">
Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
July 2002, pp. 34-41. Association for Computational Linguistics.
</note>
<bodyText confidence="0.999932176470589">
set of semantic tags is used to annotate the training
corpus, it is not obvious that the tags available are
the more appropriate for extracting domain-specific
semantic restrictions. If the tags were created specif-
ically to capture corpus dependent restrictions, there
could be serious problems concerning portability to
a new specific domain.
By contrast, unsupervised strategies to acquire
selection restrictions do not require a training cor-
pus to be semantically annotated using pre-existing
lexical hierarchies (Sekine et al., 1992; Dagan et
al., 1998; Grishman and Sterling, 1994). They re-
quire only a minimum of linguistic knowledge in or-
der to identify “meaningful” syntactic dependencies.
According to the Grefenstette’s terminology, they
can be classified as “knowledge-poor approaches”
(Grefenstette, 1994). Semantic preferences are in-
duced by merely using co-occurrence data, i.e., by
using a similarity measure to identify words which
occur in the same dependencies. It is assumed that
two words are semantically similar if they appear in
the same contexts and syntactic dependencies. Con-
sider for instance that the verb ratify frequently ap-
pear with the noun organisation in the subject po-
sition. Moreover, suppose that this noun turns to
be similar in a particular corpus to other nouns:
e.g., secretary and council. It follows that ratify not
only selects for organisation, but also for its simi-
lar words. This seems to be right. However, suppose
that organisation also appears in expressions like the
organisation of society began to be disturbed in the
last decade, or they are involved in the actual organ-
isation of things, with a significant different word
meaning. In this case, the noun means a particu-
lar kind of process. It seems obvious that its sim-
ilar words, secretary and council, cannot appear in
such subcategorisation contexts, since they are re-
lated to the other sense of the word. Soft clusters,
in which words can be members of different clusters
to different degrees, might solve this problem to a
certain extent (Pereira et al., 1993). We claim, how-
ever, that class membership should be modeled by
boolean decisions. Since subcategorisation contexts
require words in boolean terms (i.e., words are either
required or not required), words are either members
or not members of specific subcagorisation classes.
Hence, we propose a clustering method in which a
word may be gathered into different boolean clus-
ters, each cluster representing the semantic restric-
tions imposed by a class of subcategorisation con-
texts.
This paper describes an unsupervised method for
acquiring information on syntactic and semantic
subcategorisation from partially parsed text corpora.
The main assumptions underlying our proposal will
be introduced in the following section. Then, sec-
tion 3 will present the different steps -extraction of
candidate subcategorisation restrictions and concep-
tual clustering- of our learning method. In section
4, we will show how the dictionary entries are pro-
vided with the learned information. The accuracy
and coverage of this information will be measured
in a particular application: attachment resolution.
The experiments presented in this paper were per-
formed on 1,5 million of words belonging to the
P.G.R. (Portuguese General Attorney Opinions) cor-
pus, which is a domain-specific Portuguese corpus
containing case-law documents.
</bodyText>
<sectionHeader confidence="0.913662" genericHeader="method">
2 Underlying Assumptions
</sectionHeader>
<bodyText confidence="0.999979112676057">
Our acquisition method is based on two theoretical
assumptions. First, we assume a very general no-
tion of linguistic subcategorisation. More precisely,
we consider that in a “head-complement” depen-
dency, not only the head imposes constraints on the
complement, but also the complement imposes lin-
guistic requirements on the head. Following Puste-
jovsky’s terminology, we call this phenomenon “co-
composition” (Pustejovsky, 1995). So, for a particu-
lar word, we attempt to learn both what kind of com-
plements and what kind of heads it subcategorises.
For instance, consider the compositional behavior of
the noun republic in a domain-specific corpus. On
the one hand, this word appears in the head position
within dependencies such as republic of Ireland, re-
public of Portugal, and so on. On the other hand, it
appears in the complement position in dependencies
like president of the republic, government of the re-
public, etc. Given that there are interesting semantic
regularities among the words cooccurring with re-
public in such linguistic contexts, we attempt to im-
plement an algorithm letting us learn two different
subcategorisation contexts:
where preposition
introduces a binary relation between the word
republic in the role of “head” (role noted by ar-
row “ ”), and those words that can be their “com-
plements” (the role complement is noted by arrow
“ ”). This subcategorisation context semantically
requires the complements referring to particular na-
tions or states (indeed, only nations or states can be
republics).
this represents a
subcategorisation context that must be filled by
those heads denoting specific parts of the republic:
e.g., institutions, organisations, functions, and so on.
Note that the notion of subcategorisation restric-
tion we use in this paper embraces both syntactic and
semantic preferences.
The second assumption concerns the procedure
for building classes of similar subcategorisation con-
texts. We assume, in particular, that different sub-
categorisation contexts are considered to be seman-
tically similar if they have the same word distribu-
tion. Let’s take, for instance, the following contexts:
All of them seem to share the same semantic pref-
erences. As these contexts require words denot-
ing the same semantic class, they tend to possess
the same word distribution. Moreover, we also as-
sume that the set of words required by these simi-
lar subcategorisation contexts represents the exten-
sional description of their semantic preferences. In-
deed, since words minister, president, assembly, ...
have similar distribution on those contexts, they may
be used to build the extensional class of nouns that
actually fill the semantic requirements of the con-
texts. Such words are, then, semantically subcate-
gorised by them. Unlike most unsupervised methods
to selection restrictions acquisition, we do not use
the well-known strategy for measuring word simi-
larity based on distributional hypothesis. Accord-
ing to this assumption, words cooccurring in similar
subcategorisation contexts are semantically similar.
Yet, as has been said in the Introduction, such a no-
tion of word similarity is not sensitive to word poly-
semia. By contrast, the aim of our method is to mea-
sure semantic similarity between subcategorisation
contexts. This allows us to assign a polysemic word
to different contextual classes of subcategorisation.
This strategy is also used in the Asium system (Faure
and N´edellec, 1998; Faure, 2000).
</bodyText>
<sectionHeader confidence="0.955621" genericHeader="method">
3 Subcategorisation Acquisition
</sectionHeader>
<bodyText confidence="0.999982727272727">
To evaluate the hypotheses presented above, a soft-
ware package was developed to support the auto-
matic acquisition of syntactic and semantic subcat-
egorisation information. The learning strategy is
mainly constituted by two sequential procedures.
The first one aims to extract subcategorisation can-
didates, while the second one leads us to both iden-
tify correct subcategorisation candidates and gather
them into semantic classes of subcategorisation. The
two procedures will be accurately described in the
remainder of the section.
</bodyText>
<subsectionHeader confidence="0.999919">
3.1 Extraction of Candidates
</subsectionHeader>
<bodyText confidence="0.981131222222223">
We have developed the following procedure for ex-
tracting those syntactic patterns that could become
later true subcategorisation contexts. Raw text is
tagged (Marques, 2000) and then analyzed using
some potentialities of the shallow parser introduced
in (Rocio et al., 2001). The parser yields a single
partial syntactic description of sentences, which are
analyzed as sequences of basic chunks (NP, PP, VP,
... ). Then, attachment is temporarily resolved by a
simple heuristic based on right association (a chunk
tend to attach to another chunk immediately to its
right). Following our first assumption in section 2,
we consider that the word heads of two attached
chunks form a binary dependency that is likely to
be split in two subcategorisation contexts. It can be
easily seen that syntactic errors may appear since the
attachment heuristic does not take into account dis-
tant dependencies.1 For reasons of attachment er-
rors, it is argued here that the identified subcategori-
sation contexts are mere hypotheses; hence they are
mere subcategorisation candidates. Finally, the set
of words appearing in each subcategorisation con-
text are viewed as candidates to be a semantic class.
For example, the phrase
emanou de facto da lei
([it] emanated in fact from the law)
1The errors are caused, not only due to this restrictive at-
tachment heuristic, but also due to further misleadings, e.g.,
words missing from the dictionary, words incorrectly tagged,
other sorts of parser limitations, etc.
would produce the following two attachments:
from which the following 4 subcategorisation candi-
dates are generated:
Since the prepositional complement de facto
represents an adverbial locution interpolated be-
tween the verb and its real complement da lei,
the two proposed attachments are odd. Hence, the
four subcategorisation contexts should not be ac-
quired. We will see how our algorithm allows us to
learn subcategorisation information that will be used
later to invalidate such odd attachments and propose
new ones. The algorithm basically works by com-
paring the similarity between the word sets associ-
ated to each subcategorisation candidate.
Let’s note finally that unlike many learning ap-
proaches, information on co-composition is avail-
able for the characterization of syntactic subcate-
gorisation contexts. In (Gamallo et al., 2001b),
a strategy for measuring word similarity based on
the co-composition hypothesis was compared to
Grefensetette’s strategy (Grefenstette, 1994). Ex-
perimental tests demonstrated that co-composition
allows a finer-grained characterization of “meaning-
ful” syntactic contexts.
</bodyText>
<subsectionHeader confidence="0.999776">
3.2 Clustering Similar Contexts
</subsectionHeader>
<bodyText confidence="0.996243666666667">
According to the second assumption introduced
above (section 2), two subcategorisation contexts
with similar word distribution should have the same
extensional definition and, then, the same selection
restrictions. This way, the word sets associated with
two similar contexts are merged into a more gen-
eral set, which represents their extensional seman-
tic preferences. Consider the two following sub-
categorisation contexts and the words that appear in
them:
Since both contexts have a similar word dis-
tribution, it can be argued that they share the
same selection restrictions. Furthermore, it
must be inferred that the words associated to
them are all co-hyponyms belonging to the
same context-dependent semantic class. In
our corpus, context
(to infringe) is not only considered similar
to context c¸˜a (infringe-
ment of) , but also to other contexts such as:
(to respect) and
(to apply) .
In this section, we will specify the procedure
for learning context-dependent semantic classes by
comparing similarity between the previously ex-
tracted contextual word sets. This will be done in
two steps: filtering and clustering.
</bodyText>
<subsectionHeader confidence="0.645424">
3.2.1 Filtering
</subsectionHeader>
<bodyText confidence="0.99995671875">
As has been said in the introduction, the cooper-
ative system Asium also extract similar subcategori-
sation contexts (Faure and N´edellec, 1998; Faure,
2000). This system requires the interactive partici-
pation of a language specialist in order to the contex-
tual word sets be filtered and cleaned when they are
taken as input of the clustering strategy. Such a co-
operative method requires manual removal of those
words that have been incorrectly tagged or analyzed
from the sets. Our strategy, by contrast, attempts to
automatically remove incorrect words from the con-
textual sets. Automatic filtering requires the follow-
ing subtasks:
First, each word set is associated with a list of
its most similar contextual sets. Intuitively, two sets
are considered as similar if they share a significant
number of words. Various similarity measure co-
efficients were tested to create lists of similar sets.
The best results were achieved using a particular
weighted version of the Jaccard coefficient, where
words are weighted considering both their disper-
sion and their relative frequency for each context
(Gamallo et al., 2001a).
Then, once each contextual set has been com-
pared to the other sets, we select the words shared
by each pair of similar sets, i.e., we select the in-
tersection between each pair of sets considered as
similar. Since words that are not shared by two sim-
ilar sets could be incorrect words, we remove them.
Intersection allows us to clear words that are not se-
mantically homogeneous. Thus, the intersection of
two similar sets represents a class of co-hyponyms,
</bodyText>
<figure confidence="0.6394165">
[CONTXij]
[CONTXi ] [CONTXj ]
</figure>
<figureCaption confidence="0.999756">
Figure 1: Clustering step
</figureCaption>
<bodyText confidence="0.9956085">
which we call basic class. Let’s take an example.
In our corpus, the most similar set extracted from
c¸˜a (infringement of)) is the set
extracted from (infringe) .
Both sets share the following words:
sigilo principios preceito plano norma lei
estatuto disposto disposic¸˜ao direito
(secret principle precept plan norm law statute dis-
position disposition right)
This basic class does not contain incorrect
words such as vez, flagrantemente,
obrigac¸˜ao, interesse (time, notoriously,
obligation, interest), which were oddly associated to
the context c¸˜a , but which do
. This
class seems to be semantically homogeneous be-
cause it contains only co-hyponym words referring
to legal documents. Once basic classes have been
created, they are used by the conceptual clustering
algorithm to build more general classes.
</bodyText>
<subsectionHeader confidence="0.678966">
3.2.2 Conceptual Clustering
</subsectionHeader>
<bodyText confidence="0.995150571428571">
We use an agglomerative (bottom-up) cluster-
ing for successively aggregating the previously cre-
ated basic classes. Unlike most research on con-
ceptual clustering, aggregation does not rely on a
statistical distance between classes, but on empir-
ically set conditions and constraints (Talavera and
B´ejar, 1999). These conditions are discussed in
(Gamallo et al., 2001a). Figure 1 shows two ba-
sic classes associated with two pairs of similar sub-
categorisation contexts. represents a
pair of similar subcategorisation contexts sharing the
words preceito, lei, norma (precept, law,
norm, while represents another pair
of similar contexts sharing the words preceito,
</bodyText>
<tableCaption confidence="0.998217">
Table 1: Class Membership of trabalho
</tableCaption>
<table confidence="0.981542666666667">
Cluster 1 contrato execuc¸˜ao exercicio prazo pro-
cesso procedimento trabalho (agreement
execution practice term/time process procedure work)
Cluster 2 contrato exerciicio prestac¸˜ao recurso
servic¸o trabalho (agreement practice installment
appeal service work)
Cluster 3 actividade atribuic¸˜ao cargo exercicio
func¸˜ao lugar trabalho (activity attribution post
practice function post work/job)
</table>
<bodyText confidence="0.977246333333333">
lei, direito (precept, law, right). Both basic
classes are obtained from the filtering process de-
scribed in the previous section. This figure illus-
trates more precisely how the basic classes are ag-
gregated into more general clusters. If two classes
fill the clustering conditions, they can be merged
into a new class. The two basic classes of the ex-
ample are clustered into the more general class con-
stituted by preceito, lei, norma, dire-
ito. At the same time, the two pairs of contexts
and are merged into the
cluster . Such a generalization leads
us to induce syntactic data that does not appear in
the corpus. Indeed, we induce both that the word
norma may appear in the syntactic contexts repre-
sented by , and that the word dire-
ito may be attached to the syntactic contexts rep-
resented by .
</bodyText>
<subsectionHeader confidence="0.728401">
3.2.3 Polysemic Words Representation
</subsectionHeader>
<bodyText confidence="0.998759">
Polysemic words are placed in different clus-
ters. For instance, consider the word trabalho
(work/job). Table 1 situates this word as a member
of at least three different contextual classes. Clus-
ter 1 aggregates words referring to temporal objects.
Indeed, they are co-hyponyms because they appear
in subcategorisation contexts sharing the same selec-
tion restrictions: e.g., , (in-
terruption of), (in course).
Cluster 2 represents the result of an action. Such
a meaning becomes salient in contexts like for in-
stance (to receive in
payment for). Indeed, the cause of receiving money
is not the action of working, but the object done
or the state achieved by working. Finally, Clus-
ter 3 illustrates the more typical meaning of tra-
balho: it is a job, function or task, which can be
carried out by professionals. This is why these co-
</bodyText>
<figure confidence="0.4962015">
lei
norma preceito
direito
not appear in context
</figure>
<tableCaption confidence="0.864701">
Table 2: Dictionary entries
</tableCaption>
<bodyText confidence="0.901776">
hyponyms can appear in subcategorisation contexts
such as: , (of the inspector),
(to accomplish).
</bodyText>
<sectionHeader confidence="0.984723" genericHeader="method">
4 Application and Evaluation
</sectionHeader>
<bodyText confidence="0.9998276">
The acquired classes are used in the following way.
First, the lexicon is provided with subcategorisa-
tion information, and then, a second parsing cycle is
performed in order to syntactic attachments be cor-
rected.
</bodyText>
<subsectionHeader confidence="0.998769">
4.1 Lexicon Update
</subsectionHeader>
<bodyText confidence="0.99996571875">
Table 2 shows how the acquired classes are used to
provide lexical entries with syntactic and semantic
subcategorisation information. Each entry contains
both the list of subcategorisation contexts and the
list of word sets required by the syntactic contexts.
As we have said before, such word sets are viewed
as the extensional definition of the semantic pref-
erences required by the subcategorisation contexts.
Consider the information our system learnt for the
verb emanar (see table 2). It syntactically subcat-
egorises two kinds of “de-complements”: the one
semantically requires words referring to legal doc-
uments (emana da lei - emanate from the law;
law prescribes), the other selects words referring to
institutions (emana da autoridade - emanate
from the authority; authority proposes). The seman-
tic restrictions enables us to correct the odd attach-
ments proposed by our syntactic heuristics for the
phrase emanou de facto da lei (emanated
in fact from the law). As word facto does not be-
long to the semantic class required by the verb in
the “de-complement” position, we test the follow-
ing “de-complement”. As lei does belong, a new
correct attachment is proposed.
Consider now the nouns abono (loan) and
presidente (president). They subcategorise not
only complements, but also different kinds of heads.
For instance, the noun abono selects for “de-head
nouns” like fixac¸˜ao (fixac¸˜ao do abono -
fixing the loan), as well as for verbs like fixar in
the direct object position: fixar o abono (to fix
the loan).
</bodyText>
<subsectionHeader confidence="0.999324">
4.2 Attachment Resolution Algorithm
</subsectionHeader>
<bodyText confidence="0.999891294117647">
The syntactic and semantic subcategorisation infor-
mation provided by the lexical entries is used to
check whether the subcategorisation candidates pre-
viously extracted by the parser are true attachments.
The degree of efficiency in such a task may serve as
a reliable evaluation for measuring the soundness of
our learning strategy.
We assume the use of both a traditional chart
parser (Kay, 1980) and a set of simple heuristics for
identifying attachment candidates. Then, in order to
improve the analysis, a “diagnosis parser” (Rocio et
al., 2001) receives as input the sequences of chunks
proposed as attachment candidates, checks them and
raises correction procedures. Consider, for instance,
the expression editou o artigo (edited the ar-
ticle). The diagnoser reads the sequence of chunks
VP(editar) and NP(artigo), and then proposes the
</bodyText>
<table confidence="0.64537148">
(assistance expense pension amount remuneration subsidy
additional tax value salary)
conceder conter definir determinar fixar manter prever
(concede comprise define determine fix maintain foresee)
emanar (emanate)
alinea artigo c´odigo decreto diploma disposic¸˜ao estatuto
legislac¸˜ao lei norma regulamento
(paragraph article code decree diploma disposition statute
legislation law norm regulation)
administrac¸˜ao autoridade comiss˜ao conselho direcc¸˜ao es-
tado governo ministro tribunal ´org˜ao
(administration authority commission council direction
state government minister tribunal organ)
presidente (president)
assembleia cˆamara comis˜ao conselho direcc¸˜ao estado em-
presa gest˜ao instituto regi˜ao rep´ublica secc¸˜ao tribunal
(assembly chamber council direction state enterprise man-
agement institute region republic section tribunal)
cargo categoria func¸˜ao lugar remunerac¸˜ao vencimento
(post rank function place/post remuneration salary)
abono (loan)
aplicac¸˜ao caso fixac¸˜ao montante pagamento titulo
(diligence case fixing amount payment bond)
ajuda despesa pens˜ao quantia remunerac¸˜ao subsidio suple-
mento valor vencimento
</table>
<bodyText confidence="0.9943754">
attachment to be corrected
by the system. Correction is performed by ac-
cepting or rejecting the proposed attachment. This
is done looking for the subcategorisation informa-
tion contained in the lexicon dictionary, information
which has been acquired by the clustering method
described above. Four tasks are performed to check
the attachment heuristics:
Task 1a - Syntactic checking of artigo: check
word artigo in the lexicon. Look for the syntac-
tic restriction . If artigo
has this syntactic restriction, then, pass to the seman-
tic checking. Otherwise, pass to task 2a.
Task 1b - Semantic checking of artigo:
check the semantic restriction associated with
. If word editar be-
longs to that restricted class, then we can infer that
is a binary relation. At-
tachment is then confirmed. Otherwise, pass to task
2a.
Task 2a -Syntactic checking of editar: check
word editar in the lexicon. Look for the syntac-
tic restriction . If editar
has this syntactic restriction, then, pass to the seman-
tic checking. Otherwise, attachment cannot be con-
firmed.
Task 2b - Semantic checking of editar:
check the semantic restriction associated with
. If word artigo be-
longs to that restricted class, then we can infer that
is a binary relation. At-
tachment is then confirmed. Otherwise, attachment
cannot be confirmed.
Semantic checking is based on the co-
specification hypothesis stated above. According
to this hypothesis, two chunks are syntactically
attached only if one of these two conditions is
verified: either the complement is semantically
required by the head, or the head is semantically
required by the complement.
</bodyText>
<subsectionHeader confidence="0.752916">
4.3 Evaluating Performance of Attachment
Resolution
</subsectionHeader>
<bodyText confidence="0.998117">
Table 3 shows some results of the corrections pro-
posed by the diagnosis parser. Accuracy and cover-
age were evaluated on three types of attachment can-
didates: NP-PP, VP-NP, and VP-PP. We call accu-
racy the proportion of corrections that actually cor-
respond to true dependencies and, then, to correct
attachments. Coverage indicates the proportion of
candidate dependencies that were actually corrected.
Coverage evaluation was performed by randomly se-
lecting as test data three sets of about 100-150 oc-
currences of candidate attachments from the parsed
corpus. Each test set only contained one type of can-
didate attachments. Because of low coverage, accu-
racy was evaluated by using larger sets of test can-
didates. A brief description of the evaluation results
are depicted in Table 3.
</bodyText>
<tableCaption confidence="0.9574465">
Table 3: Evaluation of Attachment Resolution on
NP-PP, VP-NP, and VP-PP attachment candidates
</tableCaption>
<table confidence="0.9690462">
Attachment Candidate Accuracy ( ) Coverage ( )
NP-PP
VP-NP
VP-PP
Total
</table>
<bodyText confidence="0.991007529411765">
Even though accuracy reaches a very promising
value (about ), coverage merely achieves .
There are two main reasons for low coverage: on the
one hand, the learning method needs words to have
significant frequencies through the corpus; on the
other hand, words are sparse through the corpus, i.e.,
most words of a corpus have few occurrences. How-
ever, the significant differences between the cover-
age for NP-PP attachments and that for verbal at-
tachments (i.e., VP-NP and VP-PP), leads us to be-
lieve that the values reached by coverage should in-
crease as corpus size grows. Indeed, given that verbs
are less frequent than nouns, verb occurrences are
still very low in a corpus containing millions of
word occurrences. We need larger annotated corpora
to improve the learning task, in particular, concern-
ing verb subcategorisation.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.99995835">
As we do not propose long distance attachments, our
method can not be compared with other standard
corpus-based approaches to attachment resolution
(Hindle and Rooth, 1993; Brill and Resnik, 1994;
Li and Abe, 1998). Long distance attachments only
will be considered after having achieved the correc-
tions for immediate dependencies in the first cycle of
syntactic analysis. We are currently working on the
specification of new analysis cycles in order to long
distance attachments be solved. Consider again the
phrase emanou de facto da lei. Atthe sec-
ond cycle, the diagnoser proposed that the first PP
de facto is not corrected attached to emanou.
At the third cycle, the system will check whether the
second PP da lei maybe attached to the verb. We
will perform n-cycles of attachment propositions,
until no candidates are available. At the end of the
process, we will be able to measure in a more accu-
rate way what is the degree of robustness the parser
may achieve.
</bodyText>
<sectionHeader confidence="0.999585" genericHeader="conclusions">
6 Acknowledgement
</sectionHeader>
<reference confidence="0.547628833333333">
This work is supported in part by grants of Fundac¸˜ao
para a Ciˆencia e Tecnologia, Portugal; Federal
Agency for Post-Graduate Education (CAPES),
Brazil; Pontifical Catholic University of Rio Grande
do Sul (PUCRS), Brazil; and the MLIS 4005 Euro-
pean project TRADAUT-PT.
</reference>
<sectionHeader confidence="0.952799" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997539333333333">
Michael Brent. 1993. From grammar to lexicon: un-
supervised learning of lexical syntax. Computational
Linguistics, 19(3):243–262.
Eric Brill and Philip Resnik. 1994. A rule-based ap-
proach to prepositional phrase attachment disambigua-
tion. In COLING.
Ted Briscoe and John Carrol. 1997. Automatic extrac-
tion of subcategorization from corpora. In ANCP’97,
Washington, DC, USA.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1998.
Similarity-based methods of word coocurrence prob-
abilities. Machine Learning, 43.
David Faure and Claire N´edellec. 1998. Asium: Learn-
ing subcategorization frames and restrictions of selec-
tion. In ECML98, Workshop on Text Mining.
David Faure. 2000. Conception de m ´ethode
d’aprentissage symbolique et automatique pour
l’acquisition de cadres de sous-cat´egorisation de
verbes et de connaissances s´emantiques a` partir de
textes : le syst`eme ASIUM. Ph.D. thesis, Universit´e
Paris XI Orsay, Paris, France.
Francesc Ribas Framis. 1995. On learning more appro-
priate selectional restrictions. In Proceedings of the
7th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, Dublin.
Pablo Gamallo, Alexandre Agustini, and Gabriel P.
Lopes. 2001a. Selection restrictions acquisition from
corpora. In EPIA’01, pages 30–43, Porto, Portugal.
LNAI, Springer-Verlag.
Pablo Gamallo, Caroline Gasperin, Alexandre Agustini,
and Gabriel P. Lopes. 2001b. Syntactic-based meth-
ods for measuring word similarity. In TSD-2001,
pages 116–125. Berlin:Springer Verlag.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
USA.
Ralph Grishman and John Sterling. 1994. Generalizing
automatically generated selectional patterns. In COL-
ING’94.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103–120.
Martin Kay. 1980. Alghorith schemata and data struc-
tures in syntactic processing. Technical report, XE-
ROX PARK, Palo Alto, Ca., Report CSL-80-12.
Hang Li and Naoki Abe. 1998. Word clustering and dis-
ambiguation based on co-occurrence data. In Coling-
ACL’98), pages 749–755.
Nuno Marques. 2000. Uma Metodologia para a
Modelac¸˜ao Estat´istica da Subcategorizac¸ ˜ao Verbal.
Ph.D. thesis, Univ. Nova de Lisboa, Lisboa, Portugal.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In ACL’93,
pages 183–190, Ohio.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In ACL-SIGLEX Workshop on Tag-
ging with Lexical Semantics, Washinton DC.
V. Rocio, E. de la Clergerie, and J.G.P. Lopes. 2001.
Tabulation for multi-purpose partial parsing. Journal
of Grammars, 4(1).
Satoshi Sekine, Jeremy Carrol, Sofia Ananiadou, and
Jun’ichi Tsujii. 1992. Automatic learning for seman-
tic collocation. In Applied Natural Language Process-
ing, pages 104–110.
Luis Talavera and Javier B´ejar. 1999. Integrating declar-
ative knowledge in hierarchical clustering tasks. In In-
telligent Data Analysis, pages 211–222.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.927977">
<title confidence="0.9998115">Using Co-Composition for Acquiring Syntactic and Subcategorisation</title>
<author confidence="0.999908">Pablo Gamallo Alexandre Agustini Gabriel P Lopes</author>
<affiliation confidence="0.99981">Department of Computer New University of Lisbon,</affiliation>
<email confidence="0.960659">gamallo,aagustini,gpl@di.fct.unl.pt</email>
<abstract confidence="0.997211166666667">Natural language parsing requires extensive lexicons containing subcategorisation information for specific sublanguages. This paper describes an unsupervised method for acquiring both syntactic and semantic subcategorisation restrictions from corpora. Special attention will be paid to the role of co-composition in the acquisition strategy. The acquired information is used for lexicon tuning and parsing improvement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work is supported in part by grants of Fundac¸˜ao para a Ciˆencia e Tecnologia, Portugal; Federal Agency for Post-Graduate Education</title>
<booktitle>Grande do Sul (PUCRS), Brazil; and the MLIS 4005 European project TRADAUT-PT.</booktitle>
<institution>(CAPES), Brazil; Pontifical Catholic University of Rio</institution>
<marker></marker>
<rawString>This work is supported in part by grants of Fundac¸˜ao para a Ciˆencia e Tecnologia, Portugal; Federal Agency for Post-Graduate Education (CAPES), Brazil; Pontifical Catholic University of Rio Grande do Sul (PUCRS), Brazil; and the MLIS 4005 European project TRADAUT-PT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brent</author>
</authors>
<title>From grammar to lexicon: unsupervised learning of lexical syntax.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="1262" citStr="Brent, 1993" startWordPosition="168" endWordPosition="169">ment. 1 Introduction Recent lexicalist Grammars project the subcategorisation information encoding in the lexicon onto syntactic structures. These grammars use accurate subcategorised lexicons to restrict potential syntactic structures. In terms of parsing development, it is broadly assumed that parsers need such information in order to reduce the number of possible analyses and, therefore, solve syntactic ambiguity. Over the last years various methods for acquiring subcategorisation information from corpora has been proposed. Some of them induce syntactic subcategorisation from tagged texts (Brent, 1993; Briscoe and Carrol, 1997; Marques, 2000). Unfortunately, syntactic information is not enough to solve structural ambiguity. Consider the following verbal phrases: (1) [peel [ the potato] [ with a knife]] (2) [peel [ [ the potato] [ with a rough stain]]] The attachment of “with PP” to both the verb “peel” in phrase (1) and to the NP “the potato” in (2) does not depend only on syntactic requirements. Indeed, it is not possible to attach the PP “with a knife” to the verb “peel” by asserting that this verb subcategorises a “with PP’. Such a subcategorisation information cannot be used to explain</context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Michael Brent. 1993. From grammar to lexicon: unsupervised learning of lexical syntax. Computational Linguistics, 19(3):243–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Philip Resnik</author>
</authors>
<title>A rule-based approach to prepositional phrase attachment disambiguation.</title>
<date>1994</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="27454" citStr="Brill and Resnik, 1994" startWordPosition="4211" endWordPosition="4214">ents and that for verbal attachments (i.e., VP-NP and VP-PP), leads us to believe that the values reached by coverage should increase as corpus size grows. Indeed, given that verbs are less frequent than nouns, verb occurrences are still very low in a corpus containing millions of word occurrences. We need larger annotated corpora to improve the learning task, in particular, concerning verb subcategorisation. 5 Future Work As we do not propose long distance attachments, our method can not be compared with other standard corpus-based approaches to attachment resolution (Hindle and Rooth, 1993; Brill and Resnik, 1994; Li and Abe, 1998). Long distance attachments only will be considered after having achieved the corrections for immediate dependencies in the first cycle of syntactic analysis. We are currently working on the specification of new analysis cycles in order to long distance attachments be solved. Consider again the phrase emanou de facto da lei. Atthe second cycle, the diagnoser proposed that the first PP de facto is not corrected attached to emanou. At the third cycle, the system will check whether the second PP da lei maybe attached to the verb. We will perform n-cycles of attachment propositi</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>Eric Brill and Philip Resnik. 1994. A rule-based approach to prepositional phrase attachment disambiguation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carrol</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In ANCP’97,</booktitle>
<location>Washington, DC, USA.</location>
<contexts>
<context position="1288" citStr="Briscoe and Carrol, 1997" startWordPosition="170" endWordPosition="173">duction Recent lexicalist Grammars project the subcategorisation information encoding in the lexicon onto syntactic structures. These grammars use accurate subcategorised lexicons to restrict potential syntactic structures. In terms of parsing development, it is broadly assumed that parsers need such information in order to reduce the number of possible analyses and, therefore, solve syntactic ambiguity. Over the last years various methods for acquiring subcategorisation information from corpora has been proposed. Some of them induce syntactic subcategorisation from tagged texts (Brent, 1993; Briscoe and Carrol, 1997; Marques, 2000). Unfortunately, syntactic information is not enough to solve structural ambiguity. Consider the following verbal phrases: (1) [peel [ the potato] [ with a knife]] (2) [peel [ [ the potato] [ with a rough stain]]] The attachment of “with PP” to both the verb “peel” in phrase (1) and to the NP “the potato” in (2) does not depend only on syntactic requirements. Indeed, it is not possible to attach the PP “with a knife” to the verb “peel” by asserting that this verb subcategorises a “with PP’. Such a subcategorisation information cannot be used to explain the analysis of phrase (2</context>
</contexts>
<marker>Briscoe, Carrol, 1997</marker>
<rawString>Ted Briscoe and John Carrol. 1997. Automatic extraction of subcategorization from corpora. In ANCP’97, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based methods of word coocurrence probabilities.</title>
<date>1998</date>
<booktitle>Machine Learning,</booktitle>
<volume>43</volume>
<contexts>
<context position="3943" citStr="Dagan et al., 1998" startWordPosition="584" endWordPosition="587">y 2002, pp. 34-41. Association for Computational Linguistics. set of semantic tags is used to annotate the training corpus, it is not obvious that the tags available are the more appropriate for extracting domain-specific semantic restrictions. If the tags were created specifically to capture corpus dependent restrictions, there could be serious problems concerning portability to a new specific domain. By contrast, unsupervised strategies to acquire selection restrictions do not require a training corpus to be semantically annotated using pre-existing lexical hierarchies (Sekine et al., 1992; Dagan et al., 1998; Grishman and Sterling, 1994). They require only a minimum of linguistic knowledge in order to identify “meaningful” syntactic dependencies. According to the Grefenstette’s terminology, they can be classified as “knowledge-poor approaches” (Grefenstette, 1994). Semantic preferences are induced by merely using co-occurrence data, i.e., by using a similarity measure to identify words which occur in the same dependencies. It is assumed that two words are semantically similar if they appear in the same contexts and syntactic dependencies. Consider for instance that the verb ratify frequently appe</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1998</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1998. Similarity-based methods of word coocurrence probabilities. Machine Learning, 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Faure</author>
<author>Claire N´edellec</author>
</authors>
<title>Asium: Learning subcategorization frames and restrictions of selection.</title>
<date>1998</date>
<booktitle>In ECML98, Workshop on Text Mining.</booktitle>
<marker>Faure, N´edellec, 1998</marker>
<rawString>David Faure and Claire N´edellec. 1998. Asium: Learning subcategorization frames and restrictions of selection. In ECML98, Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Faure</author>
</authors>
<title>Conception de m ´ethode d’aprentissage symbolique et automatique pour l’acquisition de cadres de sous-cat´egorisation de verbes et de connaissances s´emantiques a` partir de textes : le syst`eme ASIUM.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit´e Paris XI Orsay,</institution>
<location>Paris, France.</location>
<contexts>
<context position="10234" citStr="Faure, 2000" startWordPosition="1562" endWordPosition="1563">o not use the well-known strategy for measuring word similarity based on distributional hypothesis. According to this assumption, words cooccurring in similar subcategorisation contexts are semantically similar. Yet, as has been said in the Introduction, such a notion of word similarity is not sensitive to word polysemia. By contrast, the aim of our method is to measure semantic similarity between subcategorisation contexts. This allows us to assign a polysemic word to different contextual classes of subcategorisation. This strategy is also used in the Asium system (Faure and N´edellec, 1998; Faure, 2000). 3 Subcategorisation Acquisition To evaluate the hypotheses presented above, a software package was developed to support the automatic acquisition of syntactic and semantic subcategorisation information. The learning strategy is mainly constituted by two sequential procedures. The first one aims to extract subcategorisation candidates, while the second one leads us to both identify correct subcategorisation candidates and gather them into semantic classes of subcategorisation. The two procedures will be accurately described in the remainder of the section. 3.1 Extraction of Candidates We have</context>
<context position="14758" citStr="Faure, 2000" startWordPosition="2246" endWordPosition="2247">same context-dependent semantic class. In our corpus, context (to infringe) is not only considered similar to context c¸˜a (infringement of) , but also to other contexts such as: (to respect) and (to apply) . In this section, we will specify the procedure for learning context-dependent semantic classes by comparing similarity between the previously extracted contextual word sets. This will be done in two steps: filtering and clustering. 3.2.1 Filtering As has been said in the introduction, the cooperative system Asium also extract similar subcategorisation contexts (Faure and N´edellec, 1998; Faure, 2000). This system requires the interactive participation of a language specialist in order to the contextual word sets be filtered and cleaned when they are taken as input of the clustering strategy. Such a cooperative method requires manual removal of those words that have been incorrectly tagged or analyzed from the sets. Our strategy, by contrast, attempts to automatically remove incorrect words from the contextual sets. Automatic filtering requires the following subtasks: First, each word set is associated with a list of its most similar contextual sets. Intuitively, two sets are considered as</context>
</contexts>
<marker>Faure, 2000</marker>
<rawString>David Faure. 2000. Conception de m ´ethode d’aprentissage symbolique et automatique pour l’acquisition de cadres de sous-cat´egorisation de verbes et de connaissances s´emantiques a` partir de textes : le syst`eme ASIUM. Ph.D. thesis, Universit´e Paris XI Orsay, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesc Ribas Framis</author>
</authors>
<title>On learning more appropriate selectional restrictions.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Dublin.</location>
<contexts>
<context position="2805" citStr="Framis, 1995" startWordPosition="419" endWordPosition="420">tatoes can have different kinds of stains. So, the parser is able to propose a correct analysis only if the lexicon is provided with, not only syntactic subcategorisation information, but also with information on semantic-pragmatic requirements (i.e., with selection restrictions). Other works attempt to acquire selection restrictions requiring pre-existing lexical ressources. The learning algorithm requires sample corpora to be constituted by verb-noun, noun-verb, or verb-prepnoun dependencies, where the nouns are semantically tagged by using lexical hierarchies such as WordNet (Resnik, 1997; Framis, 1995). Selection restrictions are induced by considering those dependencies associated with the same semantic tags. For instance, if verb ratify frequently appears with nouns semantically tagged as “legal documents” in the direct object position (e.g., article, law, precept, ... ), then it follows that it must select for nouns denoting legal documents. Unfortunately, if a pre-defined Unsupervised Lexical Acquisition: Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia, July 2002, pp. 34-41. Association for Computational Linguistics. set of semantic ta</context>
</contexts>
<marker>Framis, 1995</marker>
<rawString>Francesc Ribas Framis. 1995. On learning more appropriate selectional restrictions. In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gamallo</author>
<author>Alexandre Agustini</author>
<author>Gabriel P Lopes</author>
</authors>
<title>Selection restrictions acquisition from corpora.</title>
<date>2001</date>
<booktitle>In EPIA’01,</booktitle>
<pages>30--43</pages>
<publisher>LNAI, Springer-Verlag.</publisher>
<location>Porto,</location>
<contexts>
<context position="13157" citStr="Gamallo et al., 2001" startWordPosition="2008" endWordPosition="2011">verb and its real complement da lei, the two proposed attachments are odd. Hence, the four subcategorisation contexts should not be acquired. We will see how our algorithm allows us to learn subcategorisation information that will be used later to invalidate such odd attachments and propose new ones. The algorithm basically works by comparing the similarity between the word sets associated to each subcategorisation candidate. Let’s note finally that unlike many learning approaches, information on co-composition is available for the characterization of syntactic subcategorisation contexts. In (Gamallo et al., 2001b), a strategy for measuring word similarity based on the co-composition hypothesis was compared to Grefensetette’s strategy (Grefenstette, 1994). Experimental tests demonstrated that co-composition allows a finer-grained characterization of “meaningful” syntactic contexts. 3.2 Clustering Similar Contexts According to the second assumption introduced above (section 2), two subcategorisation contexts with similar word distribution should have the same extensional definition and, then, the same selection restrictions. This way, the word sets associated with two similar contexts are merged into a</context>
<context position="15718" citStr="Gamallo et al., 2001" startWordPosition="2397" endWordPosition="2400">ontrast, attempts to automatically remove incorrect words from the contextual sets. Automatic filtering requires the following subtasks: First, each word set is associated with a list of its most similar contextual sets. Intuitively, two sets are considered as similar if they share a significant number of words. Various similarity measure coefficients were tested to create lists of similar sets. The best results were achieved using a particular weighted version of the Jaccard coefficient, where words are weighted considering both their dispersion and their relative frequency for each context (Gamallo et al., 2001a). Then, once each contextual set has been compared to the other sets, we select the words shared by each pair of similar sets, i.e., we select the intersection between each pair of sets considered as similar. Since words that are not shared by two similar sets could be incorrect words, we remove them. Intersection allows us to clear words that are not semantically homogeneous. Thus, the intersection of two similar sets represents a class of co-hyponyms, [CONTXij] [CONTXi ] [CONTXj ] Figure 1: Clustering step which we call basic class. Let’s take an example. In our corpus, the most similar se</context>
<context position="17440" citStr="Gamallo et al., 2001" startWordPosition="2666" endWordPosition="2669">ms to be semantically homogeneous because it contains only co-hyponym words referring to legal documents. Once basic classes have been created, they are used by the conceptual clustering algorithm to build more general classes. 3.2.2 Conceptual Clustering We use an agglomerative (bottom-up) clustering for successively aggregating the previously created basic classes. Unlike most research on conceptual clustering, aggregation does not rely on a statistical distance between classes, but on empirically set conditions and constraints (Talavera and B´ejar, 1999). These conditions are discussed in (Gamallo et al., 2001a). Figure 1 shows two basic classes associated with two pairs of similar subcategorisation contexts. represents a pair of similar subcategorisation contexts sharing the words preceito, lei, norma (precept, law, norm, while represents another pair of similar contexts sharing the words preceito, Table 1: Class Membership of trabalho Cluster 1 contrato execuc¸˜ao exercicio prazo processo procedimento trabalho (agreement execution practice term/time process procedure work) Cluster 2 contrato exerciicio prestac¸˜ao recurso servic¸o trabalho (agreement practice installment appeal service work) Clus</context>
</contexts>
<marker>Gamallo, Agustini, Lopes, 2001</marker>
<rawString>Pablo Gamallo, Alexandre Agustini, and Gabriel P. Lopes. 2001a. Selection restrictions acquisition from corpora. In EPIA’01, pages 30–43, Porto, Portugal. LNAI, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gamallo</author>
<author>Caroline Gasperin</author>
<author>Alexandre Agustini</author>
<author>Gabriel P Lopes</author>
</authors>
<title>Syntactic-based methods for measuring word similarity.</title>
<date>2001</date>
<booktitle>In TSD-2001,</booktitle>
<pages>116--125</pages>
<publisher>Berlin:Springer Verlag.</publisher>
<contexts>
<context position="13157" citStr="Gamallo et al., 2001" startWordPosition="2008" endWordPosition="2011">verb and its real complement da lei, the two proposed attachments are odd. Hence, the four subcategorisation contexts should not be acquired. We will see how our algorithm allows us to learn subcategorisation information that will be used later to invalidate such odd attachments and propose new ones. The algorithm basically works by comparing the similarity between the word sets associated to each subcategorisation candidate. Let’s note finally that unlike many learning approaches, information on co-composition is available for the characterization of syntactic subcategorisation contexts. In (Gamallo et al., 2001b), a strategy for measuring word similarity based on the co-composition hypothesis was compared to Grefensetette’s strategy (Grefenstette, 1994). Experimental tests demonstrated that co-composition allows a finer-grained characterization of “meaningful” syntactic contexts. 3.2 Clustering Similar Contexts According to the second assumption introduced above (section 2), two subcategorisation contexts with similar word distribution should have the same extensional definition and, then, the same selection restrictions. This way, the word sets associated with two similar contexts are merged into a</context>
<context position="15718" citStr="Gamallo et al., 2001" startWordPosition="2397" endWordPosition="2400">ontrast, attempts to automatically remove incorrect words from the contextual sets. Automatic filtering requires the following subtasks: First, each word set is associated with a list of its most similar contextual sets. Intuitively, two sets are considered as similar if they share a significant number of words. Various similarity measure coefficients were tested to create lists of similar sets. The best results were achieved using a particular weighted version of the Jaccard coefficient, where words are weighted considering both their dispersion and their relative frequency for each context (Gamallo et al., 2001a). Then, once each contextual set has been compared to the other sets, we select the words shared by each pair of similar sets, i.e., we select the intersection between each pair of sets considered as similar. Since words that are not shared by two similar sets could be incorrect words, we remove them. Intersection allows us to clear words that are not semantically homogeneous. Thus, the intersection of two similar sets represents a class of co-hyponyms, [CONTXij] [CONTXi ] [CONTXj ] Figure 1: Clustering step which we call basic class. Let’s take an example. In our corpus, the most similar se</context>
<context position="17440" citStr="Gamallo et al., 2001" startWordPosition="2666" endWordPosition="2669">ms to be semantically homogeneous because it contains only co-hyponym words referring to legal documents. Once basic classes have been created, they are used by the conceptual clustering algorithm to build more general classes. 3.2.2 Conceptual Clustering We use an agglomerative (bottom-up) clustering for successively aggregating the previously created basic classes. Unlike most research on conceptual clustering, aggregation does not rely on a statistical distance between classes, but on empirically set conditions and constraints (Talavera and B´ejar, 1999). These conditions are discussed in (Gamallo et al., 2001a). Figure 1 shows two basic classes associated with two pairs of similar subcategorisation contexts. represents a pair of similar subcategorisation contexts sharing the words preceito, lei, norma (precept, law, norm, while represents another pair of similar contexts sharing the words preceito, Table 1: Class Membership of trabalho Cluster 1 contrato execuc¸˜ao exercicio prazo processo procedimento trabalho (agreement execution practice term/time process procedure work) Cluster 2 contrato exerciicio prestac¸˜ao recurso servic¸o trabalho (agreement practice installment appeal service work) Clus</context>
</contexts>
<marker>Gamallo, Gasperin, Agustini, Lopes, 2001</marker>
<rawString>Pablo Gamallo, Caroline Gasperin, Alexandre Agustini, and Gabriel P. Lopes. 2001b. Syntactic-based methods for measuring word similarity. In TSD-2001, pages 116–125. Berlin:Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers, USA.</publisher>
<contexts>
<context position="4204" citStr="Grefenstette, 1994" startWordPosition="621" endWordPosition="622">e created specifically to capture corpus dependent restrictions, there could be serious problems concerning portability to a new specific domain. By contrast, unsupervised strategies to acquire selection restrictions do not require a training corpus to be semantically annotated using pre-existing lexical hierarchies (Sekine et al., 1992; Dagan et al., 1998; Grishman and Sterling, 1994). They require only a minimum of linguistic knowledge in order to identify “meaningful” syntactic dependencies. According to the Grefenstette’s terminology, they can be classified as “knowledge-poor approaches” (Grefenstette, 1994). Semantic preferences are induced by merely using co-occurrence data, i.e., by using a similarity measure to identify words which occur in the same dependencies. It is assumed that two words are semantically similar if they appear in the same contexts and syntactic dependencies. Consider for instance that the verb ratify frequently appear with the noun organisation in the subject position. Moreover, suppose that this noun turns to be similar in a particular corpus to other nouns: e.g., secretary and council. It follows that ratify not only selects for organisation, but also for its similar wo</context>
<context position="13302" citStr="Grefenstette, 1994" startWordPosition="2028" endWordPosition="2029">ill see how our algorithm allows us to learn subcategorisation information that will be used later to invalidate such odd attachments and propose new ones. The algorithm basically works by comparing the similarity between the word sets associated to each subcategorisation candidate. Let’s note finally that unlike many learning approaches, information on co-composition is available for the characterization of syntactic subcategorisation contexts. In (Gamallo et al., 2001b), a strategy for measuring word similarity based on the co-composition hypothesis was compared to Grefensetette’s strategy (Grefenstette, 1994). Experimental tests demonstrated that co-composition allows a finer-grained characterization of “meaningful” syntactic contexts. 3.2 Clustering Similar Contexts According to the second assumption introduced above (section 2), two subcategorisation contexts with similar word distribution should have the same extensional definition and, then, the same selection restrictions. This way, the word sets associated with two similar contexts are merged into a more general set, which represents their extensional semantic preferences. Consider the two following subcategorisation contexts and the words t</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>Generalizing automatically generated selectional patterns.</title>
<date>1994</date>
<booktitle>In COLING’94.</booktitle>
<contexts>
<context position="3973" citStr="Grishman and Sterling, 1994" startWordPosition="588" endWordPosition="591">ssociation for Computational Linguistics. set of semantic tags is used to annotate the training corpus, it is not obvious that the tags available are the more appropriate for extracting domain-specific semantic restrictions. If the tags were created specifically to capture corpus dependent restrictions, there could be serious problems concerning portability to a new specific domain. By contrast, unsupervised strategies to acquire selection restrictions do not require a training corpus to be semantically annotated using pre-existing lexical hierarchies (Sekine et al., 1992; Dagan et al., 1998; Grishman and Sterling, 1994). They require only a minimum of linguistic knowledge in order to identify “meaningful” syntactic dependencies. According to the Grefenstette’s terminology, they can be classified as “knowledge-poor approaches” (Grefenstette, 1994). Semantic preferences are induced by merely using co-occurrence data, i.e., by using a similarity measure to identify words which occur in the same dependencies. It is assumed that two words are semantically similar if they appear in the same contexts and syntactic dependencies. Consider for instance that the verb ratify frequently appear with the noun organisation </context>
</contexts>
<marker>Grishman, Sterling, 1994</marker>
<rawString>Ralph Grishman and John Sterling. 1994. Generalizing automatically generated selectional patterns. In COLING’94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="27430" citStr="Hindle and Rooth, 1993" startWordPosition="4207" endWordPosition="4210">verage for NP-PP attachments and that for verbal attachments (i.e., VP-NP and VP-PP), leads us to believe that the values reached by coverage should increase as corpus size grows. Indeed, given that verbs are less frequent than nouns, verb occurrences are still very low in a corpus containing millions of word occurrences. We need larger annotated corpora to improve the learning task, in particular, concerning verb subcategorisation. 5 Future Work As we do not propose long distance attachments, our method can not be compared with other standard corpus-based approaches to attachment resolution (Hindle and Rooth, 1993; Brill and Resnik, 1994; Li and Abe, 1998). Long distance attachments only will be considered after having achieved the corrections for immediate dependencies in the first cycle of syntactic analysis. We are currently working on the specification of new analysis cycles in order to long distance attachments be solved. Consider again the phrase emanou de facto da lei. Atthe second cycle, the diagnoser proposed that the first PP de facto is not corrected attached to emanou. At the third cycle, the system will check whether the second PP da lei maybe attached to the verb. We will perform n-cycles</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Alghorith schemata and data structures in syntactic processing.</title>
<date>1980</date>
<tech>Technical report, XEROX PARK,</tech>
<pages>80--12</pages>
<location>Palo Alto, Ca., Report</location>
<contexts>
<context position="22221" citStr="Kay, 1980" startWordPosition="3427" endWordPosition="3428">lects for “de-head nouns” like fixac¸˜ao (fixac¸˜ao do abono - fixing the loan), as well as for verbs like fixar in the direct object position: fixar o abono (to fix the loan). 4.2 Attachment Resolution Algorithm The syntactic and semantic subcategorisation information provided by the lexical entries is used to check whether the subcategorisation candidates previously extracted by the parser are true attachments. The degree of efficiency in such a task may serve as a reliable evaluation for measuring the soundness of our learning strategy. We assume the use of both a traditional chart parser (Kay, 1980) and a set of simple heuristics for identifying attachment candidates. Then, in order to improve the analysis, a “diagnosis parser” (Rocio et al., 2001) receives as input the sequences of chunks proposed as attachment candidates, checks them and raises correction procedures. Consider, for instance, the expression editou o artigo (edited the article). The diagnoser reads the sequence of chunks VP(editar) and NP(artigo), and then proposes the (assistance expense pension amount remuneration subsidy additional tax value salary) conceder conter definir determinar fixar manter prever (concede compri</context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>Martin Kay. 1980. Alghorith schemata and data structures in syntactic processing. Technical report, XEROX PARK, Palo Alto, Ca., Report CSL-80-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Word clustering and disambiguation based on co-occurrence data. In</title>
<date>1998</date>
<booktitle>ColingACL’98),</booktitle>
<pages>749--755</pages>
<contexts>
<context position="27473" citStr="Li and Abe, 1998" startWordPosition="4215" endWordPosition="4218"> attachments (i.e., VP-NP and VP-PP), leads us to believe that the values reached by coverage should increase as corpus size grows. Indeed, given that verbs are less frequent than nouns, verb occurrences are still very low in a corpus containing millions of word occurrences. We need larger annotated corpora to improve the learning task, in particular, concerning verb subcategorisation. 5 Future Work As we do not propose long distance attachments, our method can not be compared with other standard corpus-based approaches to attachment resolution (Hindle and Rooth, 1993; Brill and Resnik, 1994; Li and Abe, 1998). Long distance attachments only will be considered after having achieved the corrections for immediate dependencies in the first cycle of syntactic analysis. We are currently working on the specification of new analysis cycles in order to long distance attachments be solved. Consider again the phrase emanou de facto da lei. Atthe second cycle, the diagnoser proposed that the first PP de facto is not corrected attached to emanou. At the third cycle, the system will check whether the second PP da lei maybe attached to the verb. We will perform n-cycles of attachment propositions, until no candi</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Word clustering and disambiguation based on co-occurrence data. In ColingACL’98), pages 749–755.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuno Marques</author>
</authors>
<title>Uma Metodologia para a Modelac¸˜ao Estat´istica da Subcategorizac¸ ˜ao Verbal.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Univ. Nova de Lisboa,</institution>
<location>Lisboa, Portugal.</location>
<contexts>
<context position="1304" citStr="Marques, 2000" startWordPosition="174" endWordPosition="175">Grammars project the subcategorisation information encoding in the lexicon onto syntactic structures. These grammars use accurate subcategorised lexicons to restrict potential syntactic structures. In terms of parsing development, it is broadly assumed that parsers need such information in order to reduce the number of possible analyses and, therefore, solve syntactic ambiguity. Over the last years various methods for acquiring subcategorisation information from corpora has been proposed. Some of them induce syntactic subcategorisation from tagged texts (Brent, 1993; Briscoe and Carrol, 1997; Marques, 2000). Unfortunately, syntactic information is not enough to solve structural ambiguity. Consider the following verbal phrases: (1) [peel [ the potato] [ with a knife]] (2) [peel [ [ the potato] [ with a rough stain]]] The attachment of “with PP” to both the verb “peel” in phrase (1) and to the NP “the potato” in (2) does not depend only on syntactic requirements. Indeed, it is not possible to attach the PP “with a knife” to the verb “peel” by asserting that this verb subcategorises a “with PP’. Such a subcategorisation information cannot be used to explain the analysis of phrase (2), where it is t</context>
<context position="11000" citStr="Marques, 2000" startWordPosition="1672" endWordPosition="1673">yntactic and semantic subcategorisation information. The learning strategy is mainly constituted by two sequential procedures. The first one aims to extract subcategorisation candidates, while the second one leads us to both identify correct subcategorisation candidates and gather them into semantic classes of subcategorisation. The two procedures will be accurately described in the remainder of the section. 3.1 Extraction of Candidates We have developed the following procedure for extracting those syntactic patterns that could become later true subcategorisation contexts. Raw text is tagged (Marques, 2000) and then analyzed using some potentialities of the shallow parser introduced in (Rocio et al., 2001). The parser yields a single partial syntactic description of sentences, which are analyzed as sequences of basic chunks (NP, PP, VP, ... ). Then, attachment is temporarily resolved by a simple heuristic based on right association (a chunk tend to attach to another chunk immediately to its right). Following our first assumption in section 2, we consider that the word heads of two attached chunks form a binary dependency that is likely to be split in two subcategorisation contexts. It can be eas</context>
</contexts>
<marker>Marques, 2000</marker>
<rawString>Nuno Marques. 2000. Uma Metodologia para a Modelac¸˜ao Estat´istica da Subcategorizac¸ ˜ao Verbal. Ph.D. thesis, Univ. Nova de Lisboa, Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In ACL’93,</booktitle>
<pages>183--190</pages>
<location>Ohio.</location>
<contexts>
<context position="5457" citStr="Pereira et al., 1993" startWordPosition="829" endWordPosition="832">ever, suppose that organisation also appears in expressions like the organisation of society began to be disturbed in the last decade, or they are involved in the actual organisation of things, with a significant different word meaning. In this case, the noun means a particular kind of process. It seems obvious that its similar words, secretary and council, cannot appear in such subcategorisation contexts, since they are related to the other sense of the word. Soft clusters, in which words can be members of different clusters to different degrees, might solve this problem to a certain extent (Pereira et al., 1993). We claim, however, that class membership should be modeled by boolean decisions. Since subcategorisation contexts require words in boolean terms (i.e., words are either required or not required), words are either members or not members of specific subcagorisation classes. Hence, we propose a clustering method in which a word may be gathered into different boolean clusters, each cluster representing the semantic restrictions imposed by a class of subcategorisation contexts. This paper describes an unsupervised method for acquiring information on syntactic and semantic subcategorisation from p</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In ACL’93, pages 183–190, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The Generative Lexicon.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="7222" citStr="Pustejovsky, 1995" startWordPosition="1089" endWordPosition="1090">,5 million of words belonging to the P.G.R. (Portuguese General Attorney Opinions) corpus, which is a domain-specific Portuguese corpus containing case-law documents. 2 Underlying Assumptions Our acquisition method is based on two theoretical assumptions. First, we assume a very general notion of linguistic subcategorisation. More precisely, we consider that in a “head-complement” dependency, not only the head imposes constraints on the complement, but also the complement imposes linguistic requirements on the head. Following Pustejovsky’s terminology, we call this phenomenon “cocomposition” (Pustejovsky, 1995). So, for a particular word, we attempt to learn both what kind of complements and what kind of heads it subcategorises. For instance, consider the compositional behavior of the noun republic in a domain-specific corpus. On the one hand, this word appears in the head position within dependencies such as republic of Ireland, republic of Portugal, and so on. On the other hand, it appears in the complement position in dependencies like president of the republic, government of the republic, etc. Given that there are interesting semantic regularities among the words cooccurring with republic in suc</context>
</contexts>
<marker>Pustejovsky, 1995</marker>
<rawString>James Pustejovsky. 1995. The Generative Lexicon. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In ACL-SIGLEX Workshop on Tagging with Lexical Semantics,</booktitle>
<location>Washinton DC.</location>
<contexts>
<context position="2790" citStr="Resnik, 1997" startWordPosition="417" endWordPosition="418">eeling, and potatoes can have different kinds of stains. So, the parser is able to propose a correct analysis only if the lexicon is provided with, not only syntactic subcategorisation information, but also with information on semantic-pragmatic requirements (i.e., with selection restrictions). Other works attempt to acquire selection restrictions requiring pre-existing lexical ressources. The learning algorithm requires sample corpora to be constituted by verb-noun, noun-verb, or verb-prepnoun dependencies, where the nouns are semantically tagged by using lexical hierarchies such as WordNet (Resnik, 1997; Framis, 1995). Selection restrictions are induced by considering those dependencies associated with the same semantic tags. For instance, if verb ratify frequently appears with nouns semantically tagged as “legal documents” in the direct object position (e.g., article, law, precept, ... ), then it follows that it must select for nouns denoting legal documents. Unfortunately, if a pre-defined Unsupervised Lexical Acquisition: Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia, July 2002, pp. 34-41. Association for Computational Linguistics. set</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In ACL-SIGLEX Workshop on Tagging with Lexical Semantics, Washinton DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rocio</author>
<author>E de la Clergerie</author>
<author>J G P Lopes</author>
</authors>
<title>Tabulation for multi-purpose partial parsing.</title>
<date>2001</date>
<journal>Journal of Grammars,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="11101" citStr="Rocio et al., 2001" startWordPosition="1686" endWordPosition="1689"> by two sequential procedures. The first one aims to extract subcategorisation candidates, while the second one leads us to both identify correct subcategorisation candidates and gather them into semantic classes of subcategorisation. The two procedures will be accurately described in the remainder of the section. 3.1 Extraction of Candidates We have developed the following procedure for extracting those syntactic patterns that could become later true subcategorisation contexts. Raw text is tagged (Marques, 2000) and then analyzed using some potentialities of the shallow parser introduced in (Rocio et al., 2001). The parser yields a single partial syntactic description of sentences, which are analyzed as sequences of basic chunks (NP, PP, VP, ... ). Then, attachment is temporarily resolved by a simple heuristic based on right association (a chunk tend to attach to another chunk immediately to its right). Following our first assumption in section 2, we consider that the word heads of two attached chunks form a binary dependency that is likely to be split in two subcategorisation contexts. It can be easily seen that syntactic errors may appear since the attachment heuristic does not take into account d</context>
<context position="22373" citStr="Rocio et al., 2001" startWordPosition="3449" endWordPosition="3452"> fixar o abono (to fix the loan). 4.2 Attachment Resolution Algorithm The syntactic and semantic subcategorisation information provided by the lexical entries is used to check whether the subcategorisation candidates previously extracted by the parser are true attachments. The degree of efficiency in such a task may serve as a reliable evaluation for measuring the soundness of our learning strategy. We assume the use of both a traditional chart parser (Kay, 1980) and a set of simple heuristics for identifying attachment candidates. Then, in order to improve the analysis, a “diagnosis parser” (Rocio et al., 2001) receives as input the sequences of chunks proposed as attachment candidates, checks them and raises correction procedures. Consider, for instance, the expression editou o artigo (edited the article). The diagnoser reads the sequence of chunks VP(editar) and NP(artigo), and then proposes the (assistance expense pension amount remuneration subsidy additional tax value salary) conceder conter definir determinar fixar manter prever (concede comprise define determine fix maintain foresee) emanar (emanate) alinea artigo c´odigo decreto diploma disposic¸˜ao estatuto legislac¸˜ao lei norma regulament</context>
</contexts>
<marker>Rocio, Clergerie, Lopes, 2001</marker>
<rawString>V. Rocio, E. de la Clergerie, and J.G.P. Lopes. 2001. Tabulation for multi-purpose partial parsing. Journal of Grammars, 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Jeremy Carrol</author>
<author>Sofia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Automatic learning for semantic collocation.</title>
<date>1992</date>
<booktitle>In Applied Natural Language Processing,</booktitle>
<pages>104--110</pages>
<contexts>
<context position="3923" citStr="Sekine et al., 1992" startWordPosition="580" endWordPosition="583">X), Philadelphia, July 2002, pp. 34-41. Association for Computational Linguistics. set of semantic tags is used to annotate the training corpus, it is not obvious that the tags available are the more appropriate for extracting domain-specific semantic restrictions. If the tags were created specifically to capture corpus dependent restrictions, there could be serious problems concerning portability to a new specific domain. By contrast, unsupervised strategies to acquire selection restrictions do not require a training corpus to be semantically annotated using pre-existing lexical hierarchies (Sekine et al., 1992; Dagan et al., 1998; Grishman and Sterling, 1994). They require only a minimum of linguistic knowledge in order to identify “meaningful” syntactic dependencies. According to the Grefenstette’s terminology, they can be classified as “knowledge-poor approaches” (Grefenstette, 1994). Semantic preferences are induced by merely using co-occurrence data, i.e., by using a similarity measure to identify words which occur in the same dependencies. It is assumed that two words are semantically similar if they appear in the same contexts and syntactic dependencies. Consider for instance that the verb ra</context>
</contexts>
<marker>Sekine, Carrol, Ananiadou, Tsujii, 1992</marker>
<rawString>Satoshi Sekine, Jeremy Carrol, Sofia Ananiadou, and Jun’ichi Tsujii. 1992. Automatic learning for semantic collocation. In Applied Natural Language Processing, pages 104–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Talavera</author>
<author>Javier B´ejar</author>
</authors>
<title>Integrating declarative knowledge in hierarchical clustering tasks.</title>
<date>1999</date>
<booktitle>In Intelligent Data Analysis,</booktitle>
<pages>211--222</pages>
<marker>Talavera, B´ejar, 1999</marker>
<rawString>Luis Talavera and Javier B´ejar. 1999. Integrating declarative knowledge in hierarchical clustering tasks. In Intelligent Data Analysis, pages 211–222.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>