<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.046820">
<title confidence="0.9991565">
A machine learning approach
to the automatic evaluation of machine translation
</title>
<author confidence="0.998468">
Simon Corston-Oliver, Michael Gamon and Chris Brockett
</author>
<affiliation confidence="0.964181">
Microsoft Research
</affiliation>
<address confidence="0.9447095">
One Microsoft Way
Redmond WA 98052, USA
</address>
<email confidence="0.993043">
{simonco, mgamon, chrisbkt}@microsoft.com
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975466666667">
We present a machine learning
approach to evaluating the well-
formedness of output of a machine
translation system, using classifiers that
learn to distinguish human reference
translations from machine translations.
This approach can be used to evaluate
an MT system, tracking improvements
over time; to aid in the kind of failure
analysis that can help guide system
development; and to select among
alternative output strings. The method
presented is fully automated and
independent of source language, target
language and domain.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939625">
Human evaluation of machine translation (MT)
output is an expensive process, often
prohibitively so when evaluations must be
performed quickly and frequently in order to
measure progress. This paper describes an
approach to automated evaluation designed to
facilitate the identification of areas for
investigation and improvement. It focuses on
evaluating the wellformedness of output and
does not address issues of evaluating content
transfer.
Researchers are now applying automated
evaluation in MT and natural language
generation tasks, both as system-internal
goodness metrics and for the assessment of
output. Langkilde and Knight (1998), for
example, employ n-gram metrics to select
among candidate outputs in natural language
generation, while Ringger et al. (2001) use n-
gram perplexity to compare the output of MT
systems. Su et al. (1992), Alshawi et al. (1998)
and Bangalore et al. (2000) employ string edit
distance between reference and output sentences
to gauge output quality for MT and generation.
To be useful to researchers, however,
assessment must provide linguistic information
that can guide in identifying areas where work is
required. (See Nyberg et al., 1994 for useful
discussion of this issue.)
The better the MT system, the more its
output will resemble human-generated text.
Indeed, MT might be considered a solved
problem should it ever become impossible to
distinguish automated output from human
translation. We have observed that in general
humans can easily and reliably categorize a
sentence as either machine- or human-generated.
Moreover, they can usually justify their
decision. This observation suggests that
evaluation of the wellformedness of output
sentences can be treated as a classification
problem: given a sentence, how accurately can
we predict whether it has been translated by
machine? In this paper we cast the problem of
MT evaluation as a machine learning
classification task that targets both linguistic
features and more abstract features such as n-
gram perplexity.
</bodyText>
<sectionHeader confidence="0.979826" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999927130434782">
Our corpus consists of 350,000 aligned Spanish-
English sentence pairs taken from published
computer software manuals and online help
documents. We extracted 200,000 English
sentences for building language models to
evaluate per-sentence perplexity. From the
remainder of the corpus, we extracted 100,000
aligned sentence pairs. The Spanish sentences in
this latter sample were then translated by the
Microsoft machine translation system, which
was trained on documents from this domain
(Richardson et al., 2001). This yielded a set of
200,000 English sentences, one half of which
were English reference sentences, and the other
half of which were MT output. (The Spanish
sentences were not used in building or
evaluating the classifiers). We split the 200,000
English sentences 90/10, to yield 180,000
sentences for training classifiers and 20,000
sentences that we used as held-out test data.
Training and test data were evenly divided
between reference English sentences and
Spanish-to-English translations.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.995684288888889">
The selection of features used in our
classification task was motivated by failure
analysis of system output. We were particularly
interested in those linguistic features that could
aid in qualitative analysis, as we discuss in
section 5. For each sentence we automatically
extracted 46 features by performing a syntactic
parse using the Microsoft NLPWin natural
language processing system (Heidorn, 2000) and
language modeling tools. The features extracted
fall into two broad categories:
(i) Perplexity measures were extracted using the
CMU-Cambridge Statistical Language Modeling
Toolkit (Clarkson and Rosenfeld, 1997). We
calculated two sets of values: lexicalized trigram
perplexity, with values discretized into deciles
and part of speech (POS) trigram perplexity. For
the latter we used the following sixteen POS
tags: adjective, adverb, auxiliary, punctuation,
complementizer, coordinating conjunction,
subordinating conjunction, determiner,
interjection, noun, possessor, preposition,
pronoun, quantifier, verb, and other.
(ii) Linguistic features fell into several
subcategories: branching properties of the parse;
function word density, constituent length, and
other miscellaneous features
We employed a selection of features to
provide a detailed assessment of the branching
properties of the parse tree. The linguistic
motivation behind this was twofold. First, it had
become apparent from failure analysis that MT
system output tended to favor right-branching
structures over noun compounding. Second, we
hypothesized that translation from languages
whose branching properties are radically
different from English (e.g. Japanese, or a verb-
second language like German) might pollute the
English output with non-English characteristics.
For this reason, assessment of branching
properties is a good candidate for a language-
pair independent measure. The branching
features we employed are given below. Indices
are scalar counts; other measures are normalized
for sentence length.
</bodyText>
<listItem confidence="0.917069">
➢ number of right-branching nodes across
all constituent types
➢ number of right-branching nodes for
NPs only
➢ number of left-branching nodes across
all constituent types
➢ number of left-branching nodes for NPs
only
➢ number of premodifiers across all
constituent types
➢ number of premodifiers within NPs only
➢ number of postmodifiers across all
constituent types
➢ number of postmodifiers within NPs
only
➢ branching index across all constituent
types, i.e. the number of right-branching
nodes minus number of left-branching
nodes
</listItem>
<bodyText confidence="0.98235212">
➢ branching index for NPs only
➢ branching weight index: number of
tokens covered by right-branching
nodes minus number of tokens covered
by left-branching nodes across all
categories
➢ branching weight index for NPs only
➢ modification index, i.e. the number of
premodifiers minus the number of
postmodifiers across all categories
➢ modification index for NPs only
➢ modification weight index: length in
tokens of all premodifiers minus length
in tokens of all postmodifiers across all
categories
➢ modification weight index for NPs only
➢ coordination balance, i.e. the maximal
length difference in coordinated
constituents
We considered the density of function words,
i.e. the ratio of function words to content words,
because of observed problems in WinMT
output. Pronouns received special attention
because of frequent problems detected in failure
analysis. The density features are:
</bodyText>
<listItem confidence="0.9136478">
➢ overall function word density
➢ density of determiners/quantifiers
➢ density of pronouns
➢ density of prepositions
➢ density of punctuation marks,
specifically commas and semicolons
➢ density of auxiliary verbs
➢ density of conjunctions
➢ density of different pronoun types: Wh,
1st, 2nd, and 3rd person pronouns
</listItem>
<bodyText confidence="0.978441333333333">
We also measured the following constituent
sizes:
➢ maximal and average NP length
➢ maximal and average AJP length
➢ maximal and average PP length
➢ maximal and average AVP length
➢ sentence length
On a lexical level, the presence of out of
vocabulary (OOV) words is frequently caused
by the direct transfer of source language words
for which no translation could be found. The
top-level syntactic template, i.e. the labels of the
immediate children of the root node of a
sentence, was also used, as was subject-verb
disagreement. The final five features are:
</bodyText>
<listItem confidence="0.98055675">
➢ number of OOV words
➢ the presence of a word containing a non-
English letter, i.e. an extended ASCII
character. This is a special case of the
OOV problem.
➢ label of the root node of the sentence
(declarative, imperative, question, NP,
or &amp;quot;FITTED&amp;quot; for non-spanning parses)
</listItem>
<bodyText confidence="0.707539666666667">
➢ sentence template, i.e. the labels of the
immediate children of the root node.
➢ subject-verb disagreement
</bodyText>
<sectionHeader confidence="0.993938" genericHeader="method">
4 Decision Trees
</sectionHeader>
<bodyText confidence="0.9999425">
We used a set of automated tools to construct
decision trees (Chickering et al., 1997) based on
the features extracted from the reference and
MT sentences. To avoid overfitting, we
specified that nodes in the decision tree should
not be split if they accounted for fewer than fifty
cases. In the discussion below we distinguish the
perplexity features from the linguistic features.
</bodyText>
<subsectionHeader confidence="0.9766345">
4.1 Decision trees built using all
training data
</subsectionHeader>
<bodyText confidence="0.999214615384615">
Table 1 gives the accuracy of the decision trees,
when trained on all 180,000 training sentences
and evaluated against the 20,000 held-out test
sentences. Since the training data and test data
contain an even split between reference human
translations and machine translations, the
baseline for comparison is 50.00%. As Table 1
shows, the decision trees dramatically
outperform this baseline. Using only perplexity
features or only linguistic features yields
accuracy substantially above this baseline.
Combining the two sets of features yields the
highest accuracy, 82.89%.
</bodyText>
<table confidence="0.99939025">
Features used Accuracy (%)
All features 82.89
Perplexity features only 74.73
Linguistic features only 76.51
</table>
<tableCaption confidence="0.999651">
Table 1 Accuracy of the decision trees
</tableCaption>
<bodyText confidence="0.9999515">
Notably, most of the annotated features
were selected by the decision tree tools. Two
features were found not to be predictive. The
first non-selected feature is the presence of a
word containing an extended ASCII character,
suggesting that general OOV features were
sufficient and subsume the effect of this
narrower feature. Secondly, subject-verb
disagreement was also not predictive, validating
the consistent enforcement of agreement
constraints in the natural language generation
component of the MT system. In addition, only
eight of approximately 5,200 observed sentence
templates turned out to be discriminatory.
For a different use of perplexity in
classification, see Ringger et al. (2001) who
compare the perplexity of a sentence using a
language model built solely from reference
translations to the perplexity using a language
model built solely from machine translations.
The output of such a classifier could be used as
an input feature in building decision trees.
</bodyText>
<figureCaption confidence="0.989742">
Figure 1 Accuracy with varying amounts of training data
</figureCaption>
<figure confidence="0.996007739130435">
Effect of training data size
All features Perplexity only Linguistic only
84
83
82
81
80
79
78
77
76
75
74
73
72
71
70
69
68
67
66
Training cases
Avg best accuracy
</figure>
<subsectionHeader confidence="0.997933">
4.2 Varying the amount of training data
</subsectionHeader>
<bodyText confidence="0.996830222222223">
For our experiments, we had access to several
hundred thousand sentences from the target
domain. To measure the effect of reducing the
size of the training data set on the accuracy of
the classifier, we built classifiers using samples
of the training data and evaluating against the
same held-out sample of 20,000 sentences. We
randomly extracted ten samples containing the
following numbers of sentences: {1,000, 2,000,
3,000, 4,000, 5,000, 6,000, 12,000, 25,000,
50,000, 100,000, 150,000}. Figure 1 shows the
effect of varying the size of the training data.
The data point graphed is the average accuracy
over the ten samples at a given sample size, with
error bars showing the range from the least
accurate decision tree at that sample size to the
most accurate.
As Figure 1 shows, the models built using
only perplexity features do not benefit from
additional training data. The models built using
linguistic features, however, benefit
substantially, with accuracy leveling off after
150,000 training cases. With only 2,000 training
cases, the classifiers built using all features
range in accuracy from 75.06% to 78.84%,
substantially above the baseline accuracy of
50%.
</bodyText>
<sectionHeader confidence="0.999299" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999858214285714">
As the results in section 4 show, it is possible to
build classifiers that can distinguish human
reference translations from the output of a
machine translation system with high accuracy.
We thus have an automatic mechanism that can
perform the task that humans appear to do with
ease, as noted in section 1. The best result, a
classifier with 82.89% accuracy, is achieved by
combining perplexity calculations with a set of
finer-grained linguistic features. Even with as
few as 2,000 training cases, accuracy exceeded
75%. In the discussion below we consider the
advantages and possible uses of this automatic
evaluation methodology.
</bodyText>
<subsectionHeader confidence="0.999081">
5.1 Advantages of the approach
</subsectionHeader>
<bodyText confidence="0.999990916666667">
Once an appropriate set of features has been
selected and tools to automatically extract those
features are in place, classifiers can be built and
evaluated quickly. This overcomes the two
problems associated with traditional manual
evaluation of MT systems: manual evaluation is
both costly and time-consuming. Indeed, an
automated approach is essential when dealing
with an MT system that is under constant
development in a collaborative research
environment. The output of such a system may
change from day to day, requiring frequent
feedback to monitor progress.
The methodology does not crucially rely on
any particular set of features. As an MT system
matures, more and more subtle cues might be
necessary to distinguish between human and
machine translations. Any linguistic feature that
can be reliably extracted can be proposed as a
candidate feature to the decision tree tools.
The methodology is also not sensitive to the
domain of the training texts. All that is needed
to build classifiers for a new domain is a
sufficient quantity of aligned translations.
</bodyText>
<subsectionHeader confidence="0.997741">
5.2 Possible applications of the
approach
</subsectionHeader>
<bodyText confidence="0.999924">
The classifiers can be used for evaluating a
system overall, providing feedback to aid in
system development, and in evaluating
individual sentences.
</bodyText>
<subsectionHeader confidence="0.94133">
Evaluating an MT system overall
</subsectionHeader>
<bodyText confidence="0.999972230769231">
Evaluating the accuracy of the classifier against
held-out data is equivalent to evaluating the
fluency of the MT system. As the MT system
improves, its output will become more like the
human reference translations. To measure
improvement over time, we would hold the set
of features constant and build and evaluate new
classifiers using the human reference
translations and the output of the MT system at a
given point in time. Using the same set of
features, we expect the accuracy of the
classifiers to go down over time as the MT
output becomes more like human translations.
</bodyText>
<subsectionHeader confidence="0.814451">
Feedback to aid system development
</subsectionHeader>
<bodyText confidence="0.999920140350877">
Our primary interest in evaluating an MT system
is to identify areas that require improvement.
This has been the motivation for using linguistic
features in addition to perplexity measures.
From the point of view of system development,
perplexity is a rather opaque measure. This can
be viewed as both a strength and a weakness. On
the one hand, it is difficult to tune a system with
the express goal of causing perplexity to
improve, rendering perplexity a particularly
good objective measurement. On the other hand,
given a poor perplexity score, it is not clear how
to improve a system without additional failure
analysis.
We used the DNETVIEWER tool (Heckerman
et al., 2000), a visualization tool for viewing
decision trees and Bayesian networks, to explore
the decision trees and identify problem areas in
our MT system. In one visualization, shown in
Figure 2, DNETVIEWER allows the user to adjust
a slider to see the order in which the features
were selected during the heuristic search that
guides the construction of decision trees. The
most discriminatory features are those which
cause the MT translations to look most awful, or
are characteristics of the reference translations
that ought to be emulated by the MT system. For
the coarse model shown in Figure 2, the distance
between pronouns (nPronDist) is the highest
predictor, followed by the number of second
person pronouns (n2ndPersPron), the number of
function words (nFunctionWords), and the
distance between prepositions (nPrepDist).
Using DNETVIEWER we are able to explore
the decision tree, as shown in Figure 3. Viewing
the leaf nodes in the decision tree, we see a
probability distribution over the possible states
of the target variable. In the case of the binary
classifier here, this is the probability that a
sentence will be a reference translation. In
Figure 3, the topmost leaf node shows that
p(Human translation) is low. We modified
DNETVIEWER so that double-clicking on the leaf
node would display reference translations and
MT sentences from the training data. We display
a window showing the path through the decision
tree, the probability that the sentence is a
reference translation given that path, and the
sentences from the training data identified by the
features on the path. This visualization allows
additional linguistic features that will allow us to
pinpoint problem areas in the MT system and
thereby further automate failure analysis.
the researcher to view manageable groups of
similar problem sentences with a view to
identifying classes of problems within the
groups. A goal for future research is to select
</bodyText>
<figureCaption confidence="0.998666">
Figure 2 Using the slider to view the best predictors
Figure 3 Examining sentences at a leaf node in the decision tree
Figure 4 Examining sentences at a leaf node in the decision tree
</figureCaption>
<bodyText confidence="0.999941">
Decision trees are merely one form of
classifier that could be used for the automated
evaluation of an MT system. In preliminary
experiments, the accuracy of classifiers using
support vector machines (SVMs) (Vapnik, 1998;
Platt et al., 2000) exceeded the accuracy of the
decision tree classifiers by a little less than one
percentage point using a linear kernel function,
and by a slightly greater margin using a
polynomial kernel function of degree three. We
prefer the decision tree classifiers because they
allow a researcher to explore the classification
system and focus on problem areas and
sentences. We find this method for exploring the
data more intuitive than attempting to visualize
the location of sentences in the high-
dimensional space of the corresponding SVM.
</bodyText>
<subsectionHeader confidence="0.7589">
Evaluating individual sentences
</subsectionHeader>
<bodyText confidence="0.999893375">
In addition to system evaluation and failure
analysis, classifiers could be used on a per-
sentence basis to guide the output of an MT
system by selecting among multiple candidate
strings. If no candidate is judged sufficiently
similar to a human reference translation, the
sentence could be flagged for human post-
editing.
</bodyText>
<sectionHeader confidence="0.999485" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999943941176471">
We have presented a method for evaluating the
fluency of MT, using classifiers based on
linguistic features to emulate the human ability
to distinguish MT from human translation. The
techniques we have described are system- and
language-independent. Possible applications of
our approach include system evaluation, failure
analysis to guide system development, and
selection among alternative possible outputs.
We have focused on structural aspects of a
text that can be used to evaluate fluency. A full
evaluation of MT quality would of course need
to include measurements of idiomaticity and
techniques to verify that the semantic and
pragmatic content of the source language had
been successfully transferred to the target
language.
</bodyText>
<sectionHeader confidence="0.998406" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999991888888889">
Our thanks go to Eric Ringger and Max
Chickering for programming assistance with the
tools used in building and evaluating the
decision trees, and to Mike Carlson for help in
sampling the initial datasets. Thanks also to
John Platt for helpful discussion on parameter
setting for the SVM tools, and to the members
of the MSR NLP group for feedback on the uses
of the methodology presented here.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999864120689655">
Alshawi, H., S. Bangalore, and S. Douglas. 1998.
Automatic acquisition of hierarchical transduction
models for machine translation. In Proceedings of
the 36th Annual Meeting of the Association for
Computational Linguistics, Montreal Canada, Vol.
I: 41-47.
Bangalore, S., O. Rambow, and S. Whittaker. 2000.
Evaluation Metrics for Generation. In Proceedings
of the International Conference on Natural
Language Generation (INLG 2000), Mitzpe
Ramon, Israel. 1-13.
Chickering, D. M., D. Heckerman, and C. Meek.
1997. A Bayesian approach to learning Bayesian
networks with local structure. In Geiger, D. and P.
Punadlik Shenoy (Eds.), Uncertainty in Artificial
Intelligence: Proceedings of the Thirteenth
Conference. 80-89.
Clarkson, P. and R. Rosenfeld. 1997. Statistical
Language Modeling Using the CMU-Cambridge
Toolkit. Proceedings of Eurospeech97. 2707-
2710.
Heckerman, D., D. M. Chickering, C. Meek, R.
Rounthwaite, and C. Kadie. 2000. Dependency
networks for inference, collaborative filtering and
data visualization. Journal of Machine Learning
Research 1:49-75.
Heidorn, G. E., 2000. Intelligent writing assistance.
In R. Dale, H. Moisl and H. Somers (Eds.).
Handbook of Natural Language Processing. New
York, NY. Marcel Dekker. 181-207.
Langkilde, I., and K. Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics, and
17th International Conference on Computational
Linguistics, Montreal, Canada. 704-710.
Nyberg, E. H., T. Mitamura, and J. G. Carbonnell.
1994. Evaluation Metrics for Knowledge-Based
Machine Translation. In Proceedings of the 15th
International Conference on Computational
Linguistics, Kyoto, Japan (Coling 94). 95-99.
Platt, J., N. Cristianini, J. Shawe-Taylor. 2000. Large
margin DAGs for multiclass classification. In
Advances in Neural Information Processing
Systems 12, MIT Press. 547-553.
Richardson, S., B. Dolan, A. Menezes, and J.
Pinkham. 2001. Achieving commercial-quality
translation with example-based methods.
Submitted for review.
Ringger, E., M. Corston-Oliver, and R. Moore. 2001.
Using Word-Perplexity for Automatic Evaluation
of Machine Translation. Manuscript.
Su, K., M. Wu, and J. Chang. 1992. A new
quantitative quality measure for machine
translation systems. In Proceedings of COLING-
92, Nantes, France. 433-439.
Vapnik, V. 1998. Statistical Learning Theory, Wiley-
Interscience, New York.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951396">
<title confidence="0.997523">A machine learning approach to the automatic evaluation of machine translation</title>
<author confidence="0.999683">Simon Corston-Oliver</author>
<author confidence="0.999683">Michael Gamon</author>
<author confidence="0.999683">Chris Brockett</author>
<affiliation confidence="0.999846">Microsoft Research</affiliation>
<address confidence="0.9928505">One Microsoft Way Redmond WA 98052, USA</address>
<email confidence="0.999547">simonco@microsoft.com</email>
<email confidence="0.999547">mgamon@microsoft.com</email>
<email confidence="0.999547">chrisbkt@microsoft.com</email>
<abstract confidence="0.9981045625">We present a machine learning approach to evaluating the wellformedness of output of a machine translation system, using classifiers that learn to distinguish human reference translations from machine translations. This approach can be used to evaluate an MT system, tracking improvements over time; to aid in the kind of failure analysis that can help guide system development; and to select among alternative output strings. The method presented is fully automated and independent of source language, target language and domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Automatic acquisition of hierarchical transduction models for machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, Montreal Canada,</booktitle>
<volume>Vol. I:</volume>
<pages>41--47</pages>
<contexts>
<context position="1660" citStr="Alshawi et al. (1998)" startWordPosition="238" endWordPosition="241">gned to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have observed that in gener</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 1998</marker>
<rawString>Alshawi, H., S. Bangalore, and S. Douglas. 1998. Automatic acquisition of hierarchical transduction models for machine translation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, Montreal Canada, Vol. I: 41-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
<author>S Whittaker</author>
</authors>
<title>Evaluation Metrics for Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Natural Language Generation (INLG</booktitle>
<pages>1--13</pages>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="1688" citStr="Bangalore et al. (2000)" startWordPosition="243" endWordPosition="246">ntification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have observed that in general humans can easily and rel</context>
</contexts>
<marker>Bangalore, Rambow, Whittaker, 2000</marker>
<rawString>Bangalore, S., O. Rambow, and S. Whittaker. 2000. Evaluation Metrics for Generation. In Proceedings of the International Conference on Natural Language Generation (INLG 2000), Mitzpe Ramon, Israel. 1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Chickering</author>
<author>D Heckerman</author>
<author>C Meek</author>
</authors>
<title>A Bayesian approach to learning Bayesian networks with local structure.</title>
<date>1997</date>
<booktitle>In Geiger, D. and P. Punadlik Shenoy (Eds.), Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference.</booktitle>
<pages>80--89</pages>
<contexts>
<context position="8613" citStr="Chickering et al., 1997" startWordPosition="1280" endWordPosition="1283">s of the immediate children of the root node of a sentence, was also used, as was subject-verb disagreement. The final five features are: ➢ number of OOV words ➢ the presence of a word containing a nonEnglish letter, i.e. an extended ASCII character. This is a special case of the OOV problem. ➢ label of the root node of the sentence (declarative, imperative, question, NP, or &amp;quot;FITTED&amp;quot; for non-spanning parses) ➢ sentence template, i.e. the labels of the immediate children of the root node. ➢ subject-verb disagreement 4 Decision Trees We used a set of automated tools to construct decision trees (Chickering et al., 1997) based on the features extracted from the reference and MT sentences. To avoid overfitting, we specified that nodes in the decision tree should not be split if they accounted for fewer than fifty cases. In the discussion below we distinguish the perplexity features from the linguistic features. 4.1 Decision trees built using all training data Table 1 gives the accuracy of the decision trees, when trained on all 180,000 training sentences and evaluated against the 20,000 held-out test sentences. Since the training data and test data contain an even split between reference human translations and</context>
</contexts>
<marker>Chickering, Heckerman, Meek, 1997</marker>
<rawString>Chickering, D. M., D. Heckerman, and C. Meek. 1997. A Bayesian approach to learning Bayesian networks with local structure. In Geiger, D. and P. Punadlik Shenoy (Eds.), Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference. 80-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical Language Modeling Using the CMU-Cambridge Toolkit.</title>
<date>1997</date>
<booktitle>Proceedings of Eurospeech97.</booktitle>
<pages>2707--2710</pages>
<contexts>
<context position="4457" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="654" endWordPosition="657">eatures The selection of features used in our classification task was motivated by failure analysis of system output. We were particularly interested in those linguistic features that could aid in qualitative analysis, as we discuss in section 5. For each sentence we automatically extracted 46 features by performing a syntactic parse using the Microsoft NLPWin natural language processing system (Heidorn, 2000) and language modeling tools. The features extracted fall into two broad categories: (i) Perplexity measures were extracted using the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). We calculated two sets of values: lexicalized trigram perplexity, with values discretized into deciles and part of speech (POS) trigram perplexity. For the latter we used the following sixteen POS tags: adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (ii) Linguistic features fell into several subcategories: branching properties of the parse; function word density, constituent length, and other miscellaneous features We employed a select</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Clarkson, P. and R. Rosenfeld. 1997. Statistical Language Modeling Using the CMU-Cambridge Toolkit. Proceedings of Eurospeech97. 2707-2710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Heckerman</author>
<author>D M Chickering</author>
<author>C Meek</author>
<author>R Rounthwaite</author>
<author>C Kadie</author>
</authors>
<title>Dependency networks for inference, collaborative filtering and data visualization.</title>
<date>2000</date>
<journal>Journal of Machine Learning Research</journal>
<pages>1--49</pages>
<contexts>
<context position="15362" citStr="Heckerman et al., 2000" startWordPosition="2350" endWordPosition="2353">dentify areas that require improvement. This has been the motivation for using linguistic features in addition to perplexity measures. From the point of view of system development, perplexity is a rather opaque measure. This can be viewed as both a strength and a weakness. On the one hand, it is difficult to tune a system with the express goal of causing perplexity to improve, rendering perplexity a particularly good objective measurement. On the other hand, given a poor perplexity score, it is not clear how to improve a system without additional failure analysis. We used the DNETVIEWER tool (Heckerman et al., 2000), a visualization tool for viewing decision trees and Bayesian networks, to explore the decision trees and identify problem areas in our MT system. In one visualization, shown in Figure 2, DNETVIEWER allows the user to adjust a slider to see the order in which the features were selected during the heuristic search that guides the construction of decision trees. The most discriminatory features are those which cause the MT translations to look most awful, or are characteristics of the reference translations that ought to be emulated by the MT system. For the coarse model shown in Figure 2, the </context>
</contexts>
<marker>Heckerman, Chickering, Meek, Rounthwaite, Kadie, 2000</marker>
<rawString>Heckerman, D., D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. 2000. Dependency networks for inference, collaborative filtering and data visualization. Journal of Machine Learning Research 1:49-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Intelligent writing assistance. In</title>
<date>2000</date>
<booktitle>Handbook of Natural Language Processing.</booktitle>
<pages>181--207</pages>
<publisher>Marcel Dekker.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="4241" citStr="Heidorn, 2000" startWordPosition="628" endWordPosition="629">for training classifiers and 20,000 sentences that we used as held-out test data. Training and test data were evenly divided between reference English sentences and Spanish-to-English translations. 3 Features The selection of features used in our classification task was motivated by failure analysis of system output. We were particularly interested in those linguistic features that could aid in qualitative analysis, as we discuss in section 5. For each sentence we automatically extracted 46 features by performing a syntactic parse using the Microsoft NLPWin natural language processing system (Heidorn, 2000) and language modeling tools. The features extracted fall into two broad categories: (i) Perplexity measures were extracted using the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). We calculated two sets of values: lexicalized trigram perplexity, with values discretized into deciles and part of speech (POS) trigram perplexity. For the latter we used the following sixteen POS tags: adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, qu</context>
</contexts>
<marker>Heidorn, 2000</marker>
<rawString>Heidorn, G. E., 2000. Intelligent writing assistance. In R. Dale, H. Moisl and H. Somers (Eds.). Handbook of Natural Language Processing. New York, NY. Marcel Dekker. 181-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, and 17th International Conference on Computational Linguistics,</booktitle>
<pages>704--710</pages>
<location>Montreal,</location>
<contexts>
<context position="1432" citStr="Langkilde and Knight (1998)" startWordPosition="200" endWordPosition="203">n of machine translation (MT) output is an expensive process, often prohibitively so when evaluations must be performed quickly and frequently in order to measure progress. This paper describes an approach to automated evaluation designed to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT sys</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde, I., and K. Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, and 17th International Conference on Computational Linguistics, Montreal, Canada. 704-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Nyberg</author>
<author>T Mitamura</author>
<author>J G Carbonnell</author>
</authors>
<title>Evaluation Metrics for Knowledge-Based Machine Translation.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics, Kyoto, Japan (Coling</booktitle>
<volume>94</volume>
<pages>95--99</pages>
<contexts>
<context position="1972" citStr="Nyberg et al., 1994" startWordPosition="286" endWordPosition="289"> goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have observed that in general humans can easily and reliably categorize a sentence as either machine- or human-generated. Moreover, they can usually justify their decision. This observation suggests that evaluation of the wellformedness of output sentences can be treated as a classification problem: given a sentence, how accurately can w</context>
</contexts>
<marker>Nyberg, Mitamura, Carbonnell, 1994</marker>
<rawString>Nyberg, E. H., T. Mitamura, and J. G. Carbonnell. 1994. Evaluation Metrics for Knowledge-Based Machine Translation. In Proceedings of the 15th International Conference on Computational Linguistics, Kyoto, Japan (Coling 94). 95-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>Large margin DAGs for multiclass classification.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 12,</booktitle>
<pages>547--553</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="17721" citStr="Platt et al., 2000" startWordPosition="2730" endWordPosition="2733">failure analysis. the researcher to view manageable groups of similar problem sentences with a view to identifying classes of problems within the groups. A goal for future research is to select Figure 2 Using the slider to view the best predictors Figure 3 Examining sentences at a leaf node in the decision tree Figure 4 Examining sentences at a leaf node in the decision tree Decision trees are merely one form of classifier that could be used for the automated evaluation of an MT system. In preliminary experiments, the accuracy of classifiers using support vector machines (SVMs) (Vapnik, 1998; Platt et al., 2000) exceeded the accuracy of the decision tree classifiers by a little less than one percentage point using a linear kernel function, and by a slightly greater margin using a polynomial kernel function of degree three. We prefer the decision tree classifiers because they allow a researcher to explore the classification system and focus on problem areas and sentences. We find this method for exploring the data more intuitive than attempting to visualize the location of sentences in the highdimensional space of the corresponding SVM. Evaluating individual sentences In addition to system evaluation </context>
</contexts>
<marker>Platt, Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Platt, J., N. Cristianini, J. Shawe-Taylor. 2000. Large margin DAGs for multiclass classification. In Advances in Neural Information Processing Systems 12, MIT Press. 547-553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Richardson</author>
<author>B Dolan</author>
<author>A Menezes</author>
<author>J Pinkham</author>
</authors>
<title>Achieving commercial-quality translation with example-based methods.</title>
<date>2001</date>
<note>Submitted for review.</note>
<contexts>
<context position="3326" citStr="Richardson et al., 2001" startWordPosition="489" endWordPosition="492">sification task that targets both linguistic features and more abstract features such as ngram perplexity. 2 Data Our corpus consists of 350,000 aligned SpanishEnglish sentence pairs taken from published computer software manuals and online help documents. We extracted 200,000 English sentences for building language models to evaluate per-sentence perplexity. From the remainder of the corpus, we extracted 100,000 aligned sentence pairs. The Spanish sentences in this latter sample were then translated by the Microsoft machine translation system, which was trained on documents from this domain (Richardson et al., 2001). This yielded a set of 200,000 English sentences, one half of which were English reference sentences, and the other half of which were MT output. (The Spanish sentences were not used in building or evaluating the classifiers). We split the 200,000 English sentences 90/10, to yield 180,000 sentences for training classifiers and 20,000 sentences that we used as held-out test data. Training and test data were evenly divided between reference English sentences and Spanish-to-English translations. 3 Features The selection of features used in our classification task was motivated by failure analysi</context>
</contexts>
<marker>Richardson, Dolan, Menezes, Pinkham, 2001</marker>
<rawString>Richardson, S., B. Dolan, A. Menezes, and J. Pinkham. 2001. Achieving commercial-quality translation with example-based methods. Submitted for review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ringger</author>
<author>M Corston-Oliver</author>
<author>R Moore</author>
</authors>
<title>Using Word-Perplexity for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<tech>Manuscript.</tech>
<contexts>
<context position="1562" citStr="Ringger et al. (2001)" startWordPosition="219" endWordPosition="222">uently in order to measure progress. This paper describes an approach to automated evaluation designed to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become</context>
<context position="10380" citStr="Ringger et al. (2001)" startWordPosition="1544" endWordPosition="1547">s. Two features were found not to be predictive. The first non-selected feature is the presence of a word containing an extended ASCII character, suggesting that general OOV features were sufficient and subsume the effect of this narrower feature. Secondly, subject-verb disagreement was also not predictive, validating the consistent enforcement of agreement constraints in the natural language generation component of the MT system. In addition, only eight of approximately 5,200 observed sentence templates turned out to be discriminatory. For a different use of perplexity in classification, see Ringger et al. (2001) who compare the perplexity of a sentence using a language model built solely from reference translations to the perplexity using a language model built solely from machine translations. The output of such a classifier could be used as an input feature in building decision trees. Figure 1 Accuracy with varying amounts of training data Effect of training data size All features Perplexity only Linguistic only 84 83 82 81 80 79 78 77 76 75 74 73 72 71 70 69 68 67 66 Training cases Avg best accuracy 4.2 Varying the amount of training data For our experiments, we had access to several hundred thous</context>
</contexts>
<marker>Ringger, Corston-Oliver, Moore, 2001</marker>
<rawString>Ringger, E., M. Corston-Oliver, and R. Moore. 2001. Using Word-Perplexity for Automatic Evaluation of Machine Translation. Manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Su</author>
<author>M Wu</author>
<author>J Chang</author>
</authors>
<title>A new quantitative quality measure for machine translation systems.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING92,</booktitle>
<pages>433--439</pages>
<location>Nantes,</location>
<contexts>
<context position="1637" citStr="Su et al. (1992)" startWordPosition="234" endWordPosition="237">ed evaluation designed to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have</context>
</contexts>
<marker>Su, Wu, Chang, 1992</marker>
<rawString>Su, K., M. Wu, and J. Chang. 1992. A new quantitative quality measure for machine translation systems. In Proceedings of COLING92, Nantes, France. 433-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<date>1998</date>
<booktitle>Statistical Learning Theory,</booktitle>
<location>WileyInterscience, New York.</location>
<contexts>
<context position="17700" citStr="Vapnik, 1998" startWordPosition="2728" endWordPosition="2729">ther automate failure analysis. the researcher to view manageable groups of similar problem sentences with a view to identifying classes of problems within the groups. A goal for future research is to select Figure 2 Using the slider to view the best predictors Figure 3 Examining sentences at a leaf node in the decision tree Figure 4 Examining sentences at a leaf node in the decision tree Decision trees are merely one form of classifier that could be used for the automated evaluation of an MT system. In preliminary experiments, the accuracy of classifiers using support vector machines (SVMs) (Vapnik, 1998; Platt et al., 2000) exceeded the accuracy of the decision tree classifiers by a little less than one percentage point using a linear kernel function, and by a slightly greater margin using a polynomial kernel function of degree three. We prefer the decision tree classifiers because they allow a researcher to explore the classification system and focus on problem areas and sentences. We find this method for exploring the data more intuitive than attempting to visualize the location of sentences in the highdimensional space of the corresponding SVM. Evaluating individual sentences In addition </context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vapnik, V. 1998. Statistical Learning Theory, WileyInterscience, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>