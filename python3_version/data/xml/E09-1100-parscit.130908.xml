<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.9663">
Character-Level Dependencies in Chinese: Usefulness and Learning
</title>
<author confidence="0.998476">
Hai Zhao
</author>
<affiliation confidence="0.908838">
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Avenue, Kowloon, Hong Kong, China
</affiliation>
<email confidence="0.991793">
haizhao@cityu.edu.hk
</email>
<sectionHeader confidence="0.997326" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893458333333">
We investigate the possibility of exploit-
ing character-based dependency for Chi-
nese information processing. As Chinese
text is made up of character sequences
rather than word sequences, word in Chi-
nese is not so natural a concept as in En-
glish, nor is word easy to be defined with-
out argument for such a language. There-
fore we propose a character-level depen-
dency scheme to represent primary lin-
guistic relationships within a Chinese sen-
tence. The usefulness of character depen-
dencies are verified through two special-
ized dependency parsing tasks. The first
is to handle trivial character dependencies
that are equally transformed from tradi-
tional word boundaries. The second fur-
thermore considers the case that annotated
internal character dependencies inside a
word are involved. Both of these results
from character-level dependency parsing
are positive. This study provides an alter-
native way to formularize basic character-
and word-level representation for Chinese.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903714285714">
In many human languages, word can be naturally
identified from writing. However, this is not the
case for Chinese, for Chinese is born to be written
in character1 sequence rather than word sequence,
namely, no natural separators such as blanks ex-
ist between words. As word does not appear in
a natural way as most European languages2, it
</bodyText>
<footnote confidence="0.750431285714286">
1Character here stands for various tokens occurring in
a naturally written Chinese text, including Chinese charac-
ter(hanzi), punctuation, and foreign letters. However, Chi-
nese characters often cover the most part.
2Even in European languages, a naive but necessary
method to properly define word is to list them all by hand.
Thank the first anonymous reviewer who points this fact.
</footnote>
<bodyText confidence="0.99938841025641">
brings the argument about how to determine the
word-hood in Chinese. Linguists’ views about
what is a Chinese word diverge so greatly that
multiple word segmentation standards have been
proposed for computational linguistics tasks since
the first Bakeoff (Bakeoff-1, or Bakeoff-2003)3
(Sproat and Emerson, 2003).
Up to Bakeoff-4, seven word segmentation stan-
dards have been proposed. However, this does not
effectively solve the open problem what a Chi-
nese word should exactly be but raises another is-
sue: what a segmentation standard should be se-
lected for the successive application. As word
often plays a basic role for the further language
processing, if it cannot be determined in a uni-
fied way, then all successive tasks will be affected
more or less.
Motivated by dependency representation for
syntactic parsing since (Collins, 1999) that has
been drawn more and more interests in recent
years, we suggest that character-level dependen-
cies can be adopted to alleviate this difficulty in
Chinese processing. If we regard traditional word
boundary as a linear representation for neighbored
characters, then character-level dependencies can
provide a way to represent non-linear relations be-
tween non-neighbored characters. To show that
character dependencies can be useful, we develop
a parsing scheme for the related learning task and
demonstrate its effectiveness.
The rest of the paper is organized as fol-
lows. The next section shows the drawbacks of
the current word boundary representation through
some language examples. Section 3 describes
a character-level dependency parsing scheme for
traditional word segmentation task and reports its
evaluation results. Section 4 verifies the useful-
ness of annotated character dependencies inside a
word. Section 5 looks into a few issues concern-
</bodyText>
<footnote confidence="0.998256">
3First International Chinese Word Segmentation Bakeoff,
available at http://www.sighan.org/bakeoff2003.
</footnote>
<note confidence="0.962306">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 879–887,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998252">
879
</page>
<bodyText confidence="0.975222">
ing the role of character dependencies. Section 6
concludes the paper.
</bodyText>
<sectionHeader confidence="0.867303" genericHeader="method">
2 To Segment or Not: That Is the
Question
</sectionHeader>
<bodyText confidence="0.999256428571429">
Though most words can be unambiguously de-
fined in Chinese text, some word boundaries are
not so easily determined. We show such three ex-
amples as the following.
The first example is from the MSRA segmented
corpus of Bakeoff-2 (Bakeoff-2005) (Emerson,
2005):
</bodyText>
<equation confidence="0.6201615">
�/JK�/&amp;quot;/JL—fi-9110K���/�
A/Alk/if/„
</equation>
<bodyText confidence="0.971785625">
a / piece of / “ / Beijing City Beijing Opera
OK Sodality / member / entrance / ticket / ”
As the guideline of MSRA standard requires any
organization’s full name as a word, many long
words in this form are frequently encountered.
Though this type of ‘words’ may be regarded as an
effective unit to some extent, some smaller mean-
ingful constituents can be still identified inside
them. Some researchers argue that these should
be seen as phrases rather than words. In fact, e.g.,
a machine translation system will have to segment
this type of words into some smaller units for a
proper translation.
The second example is from the PKU corpus of
Bakeoff-2,
¥I/7/Hš/Œ1t,
China / in / South Africa / embassy
(the Chinese embassy in South Africa)
This example demonstrates how researchers can
also feel inconvenient if an organization name is
segmented into pieces. Though the word ‘)c, 1t
’(embassy) is right after ‘Itlj lk’(South Africa)
in the above phrase, the embassy does not belong
to South Africa but China, and it is only located in
South Africa.
The third example is an abbreviation that makes
use of the characteristics of Chinese characters.
��/�/�/�
Week / one / three / five
(Monday, Wednesday and Friday)
This example shows that there will be in a
dilemma to perform segmentation over these char-
acters. If a segmentation position locates before
(three) or ‘K’(five), then this will make them
meaningless or losing its original meaning at least
because either of these two characters should log-
ically follow the substring ’ (week) to con-
struct the expected word ‘,Y- ’(Wednesday) or
‘�4114AK’ (Friday). Otherwise, to make all the
above five characters as a word will have to ig-
nore all these logical dependent relations among
these characters and segment it later for a proper
tackling as the above first example.
All these examples suggest that dependencies
exist between discontinuous characters, and word
boundary representation is insufficient to handle
these cases. This motivates us to introduce char-
acter dependencies.
</bodyText>
<sectionHeader confidence="0.970783" genericHeader="method">
3 Character-Level Dependency Parsing
</sectionHeader>
<bodyText confidence="0.99994075">
Character dependency is proposed as an alterna-
tive to word boundary. The idea itself is extremely
simple, character dependencies inside sequence
are annotated or formally defined in the similar
way that syntactic dependencies over words are
usually annotated.
We will initially develop a character-level de-
pendency parsing scheme in this section. Es-
pecially, we show character dependencies, even
those trivial ones that are equally transformed
from pre-defined word boundaries, can be effec-
tively captured in a parsing way.
</bodyText>
<subsectionHeader confidence="0.992347">
3.1 Formularization
</subsectionHeader>
<bodyText confidence="0.9982725">
Using a character-level dependency representa-
tion, we first show how a word segmentation task
can be transformed into a dependency parsing
problem. Since word segmentation is traditionally
formularized as an unlabeled character chunking
task since (Xue, 2003), only unlabeled dependen-
cies are concerned in the transformation. There are
many ways to transform chunks in a sequence into
dependency representation. However, for the sake
of simplicity, only well-formed and projective out-
put sequences are considered for our processing.
Borrowing the notation from (Nivre and Nils-
son, 2005), an unlabeled dependency graph is for-
mally defined as follows:
An unlabeled dependency graph for a string
of cliques (i.e., words and characters) W =
</bodyText>
<page confidence="0.997676">
880
</page>
<figureCaption confidence="0.998372">
Figure 1: Two character dependency schemes
</figureCaption>
<bodyText confidence="0.6514995">
w1...wn is an unlabeled directed graph D =
(W, A), where
</bodyText>
<listItem confidence="0.9832895">
(a) W is the set of ordered nodes, i.e. clique
tokens in the input string, ordered by a
linear precedence relation &lt;,
(b) A is a set of unlabeled arcs (wi, wj),
</listItem>
<bodyText confidence="0.969362888888889">
where wi, wj E W,
If (wi, wj) E A, wi is called the head of wj
and wj a dependent of wi. Traditionally, the no-
tation wi —* wj means (wi, wj) E A; wi —**
wj denotes the reflexive and transitive closure of
the (unlabeled) arc relation. We assume that the
designed dependency structure satisfies the fol-
lowing common constraints in existing literature
(Nivre, 2006).
</bodyText>
<listItem confidence="0.984789363636364">
(1) D is weakly connected, that is, the cor-
responding undirected graph is connected.
(CONNECTEDNESS)
(2) The graph D is acyclic, i.e., if wi —* wj then
not wj —** wi. (ACYCLICITY)
(3) There is at most one arc (wi, wj) E A, bwj E
W. (SINGLE-HEAD)
(4) An arc wi —* wk is projective iff, for every
word wj occurring between wi and wk in the
string (wi &lt; wj &lt; wk or wi &gt; wj &gt; wk),
wi —** wj . (PROJECTIVITY)
</listItem>
<bodyText confidence="0.999941625">
We say that D is well-formed iff it is acyclic and
connected, and D is projective iff every arcs in A
are projective. Note that the above four conditions
entail that the graph D is a single-rooted tree. For
an arc wi —* wj, if wi &lt; wj, then it is called right-
arc, otherwise left-arc.
Following the above four constraints and con-
sidering segmentation characteristics, we may
have two character dependency representation
schemes as shown in Figure 1 by using a series
of trivial dependencies inside or outside a word.
Note that we use arc direction to distinguish con-
nected and segmented relation among characters.
The scheme with the assistant root node before the
sequence in Figure 1 is called Scheme B, and the
other Scheme E.
</bodyText>
<subsectionHeader confidence="0.998941">
3.2 Shift-reduce Parsing
</subsectionHeader>
<bodyText confidence="0.990388545454545">
According to (McDonald and Nivre, 2007), all
data-driven models for dependency parsing that
have been proposed in recent years can be de-
scribed as either graph-based or transition-based.
Since both dependency schemes that we construct
for parsing are well-formed and projective, the lat-
ter is chosen as the parsing framework for the sake
of efficiency. In detail, a shift-reduce method is
adopted as in (Nivre, 2003).
The method is step-wise and a classifier is used
to make a parsing decision step by step. In each
step, the classifier checks a clique pair 4, namely,
TOP, the top of a stack that consists of the pro-
cessed cliques, and, INPUT, the first clique in the
unprocessed sequence, to determine if a dependent
relation should be established between them. Be-
sides two arc-building actions, a shift action and a
reduce action are also defined, as follows,
Left-arc: Add an arc from INPUT to TOP and
pop the stack.
Right-arc: Add an arc from TOP to INPUT and
push INPUT onto the stack.
Reduce: Pop TOP from the stack.
Shift: Push INPUT onto the stack.
In this work, we adopt a left-to-right arc-eager
parsing model, that means that the parser scans the
input sequence from left to right and right depen-
dents are attached to their heads as soon as possi-
ble (Hall et al., 2007). In the implementation, as
for Scheme E, all four actions are required to pass
through an input sequence. However, only three
actions, i.e., reduce action will never be used, are
needed for Scheme B.
</bodyText>
<subsectionHeader confidence="0.999752">
3.3 Learning Model and Features
</subsectionHeader>
<bodyText confidence="0.989570428571429">
While memory-based and margin-based learn-
ing approaches such as support vector machines
are popularly applied to shift-reduce parsing, we
apply maximum entropy model as the learning
model for efficient training and producing some
comparable results. Our implementation of max-
imum entropy adopts L-BFGS algorithm for pa-
rameter optimization as usual. No additional fea-
ture selection techniques are used.
With notations defined in Table 1, a feature set
as shown in Table 2 is adopted. Here, we explain
some terms in Tables 1 and 2.
4Here, clique means character or word in a sequence,
which depends on what constructs the sequence.
</bodyText>
<page confidence="0.998858">
881
</page>
<tableCaption confidence="0.998695">
Table 1: Feature Notations
</tableCaption>
<bodyText confidence="0.999965588235294">
Since we only considered unlabeled depen-
dency parsing, dprel means the arc direction from
the head, either left or right. The feature cur-
root returns the root of a partial parsing tree that
includes a specified node. The feature cnseq re-
turns a substring started from a given character. It
checks the direction of the arc that passes the given
character and collects all characters with the same
arc direction to yield an output substring until the
arc direction is changed. Note that all combina-
tional features concerned with this one can be re-
garded as word-level features.
The feature av is derived from unsupervised
segmentation as in (Zhao and Kit, 2008a), and
the accessor variety (AV) (Feng et al., 2004) is
adopted as the unsupervised segmentation crite-
rion. The AV value of a substring s is defined as
</bodyText>
<equation confidence="0.888245">
AV (s) = min{Lav(s), Rav(s)},
</equation>
<bodyText confidence="0.999965222222222">
where the left and right AV values Lav(s) and
Rav(s) are defined, respectively, as the numbers
of its distinct predecessor and successor charac-
ters. In this work, AV values for substrings are
derived from unlabeled training and test corpora
by substring counting. Multiple features are used
to represent substrings of various lengths identi-
fied by the AV criterion. Formally put, the feature
function for a n-character substring s with a score
</bodyText>
<equation confidence="0.8522925">
AV (s) is defined as
avn = t, if 2t ≤ AV (s) &lt; 2t+1, (1)
</equation>
<bodyText confidence="0.999609333333333">
where t is an integer to logarithmize the score and
taken as the feature value. For an overlap character
of several substrings, we only choose the one with
</bodyText>
<tableCaption confidence="0.590076">
Table 2: Features for Parsing
</tableCaption>
<bodyText confidence="0.976442916666667">
Basic Extension
x.char itself, its previous two and next two
characters, and all bigrams within the
five-character window. (x is s or i.)
s.h.char
s.dprel
s.rm.dprel
s_1.cnseq
s_1.cnseq+s.char
s_1.curroot.lm.cnseq
s_1.curroot.lm.cnseq+s.char
s_1.curroot.lm.cnseq+i.char
s_1.curroot.lm.cnseq+s_1.cnseq
s_1.curroot.lm.cnseq+s.char+s_1.cnseq
s_1.curroot.lm.cnseq+i.char+s_1.cnseq
s.avn+i.avn, n = 1, 2, 3, 4, 5
preact_1
preact_2
preact_2+preact_1
the greatest AV score to activate the above feature
function for that character.
The feature preactn returns the previous pars-
ing action type, and the subscript n stands for the
action order before the current action.
</bodyText>
<subsectionHeader confidence="0.981844">
3.4 Decoding
</subsectionHeader>
<bodyText confidence="0.9995545">
Without Markovian feature like preact−1, a shift-
reduce parser can scan through an input sequence
in linear time. That is, the decoding of a parsing
method for word segmentation will be extremely
fast. The time complexity of decoding will be 2L
for Scheme E, and L for Scheme B, where L is
the length of the input sequence.
However, it is somewhat complicated as Marko-
vian features are involved. Following the work of
(Duan et al., 2007), the decoding in this case is to
search a parsing action sequence with the maximal
probability.
</bodyText>
<equation confidence="0.9936865">
�5di = argmax p(di|di−1di−2...),
i
</equation>
<bodyText confidence="0.99982425">
where 5di is the object parsing action sequence,
p(di|di−1...) is the conditional probability, and di
is i-th parsing action. We use a beam search al-
gorithm as in (Ratnaparkhi, 1996) to find the ob-
ject parsing action sequence. The time complex-
ity of this beam search algorithm will be 4BL for
Scheme E and 3BL for Scheme B, where B is the
beam width.
</bodyText>
<sectionHeader confidence="0.99767" genericHeader="method">
3.5 Related Methods
</sectionHeader>
<bodyText confidence="0.9868715">
Among character-based learning techniques for
word segmentation, we may identify two main
</bodyText>
<table confidence="0.999492392857143">
Notation
Meaning
The character in the top of stack
The first character below the top of stack, etc.
The first (second) character in the
unprocessed sequence, etc.
Dependent label
Head
Leftmost child
Rightmost child
Right nearest child
Character form
’s, i.e., ‘s.dprel’ means dependent label
of character in the top of stack
Feature combination, i.e., ‘s.char+i.char’
means both s.char and i.char work as a
feature function.
s
s_1,...
i, i+1,...
dprel
h
lm
rm
rn
char
.
+
</table>
<page confidence="0.992713">
882
</page>
<bodyText confidence="0.996079555555555">
types, classification (GOH et al., 2004) and tag-
ging (Low et al., 2005). Both character classifi-
cation and tagging need to define the position of
character inside a word. Traditionally, the four
tags, b, m, e, and s stand, respectively, for the
beginning, midle, end of a word, and a single-
character as word since (Xue, 2003). The follow-
ing n-gram features from (Xue, 2003; Low et al.,
2005) are used as basic features,
</bodyText>
<listItem confidence="0.961379">
(a) Cn(n = −2,−1,0,1,2),
(b) CnCn+1(n = −2, −1, 0, 1),
(c) C−1C1,
</listItem>
<bodyText confidence="0.91984487804878">
where C stands for a character and the subscripts
for the relative order to the current character C0. In
addition, the feature av that is defined in equation
(1) is also taken as an option. avn (n=1,...,5) is
applied as feature for the current character.
While word segmentation is conducted as a
classification task, each individual character will
be simply assigned a tag with the maximal prob-
ability given by the classifier. In this case, we re-
store word boundary only according to two tags
b and s. However, the output tag sequence given
by character classification may include illegal tag
transition (e.g., m is after e.). In (Low et al., 2005),
a dynamic programming algorithm is adopted to
find a tag sequence with the maximal joint prob-
ability from all legal tag sequences. If such a dy-
namic programming decoding is adopted, then this
method for word segmentation is regarded as char-
acter tagging 5.
The time complexity of character-based classifi-
cation method for decoding is L, which is the best
result in decoding velocity. As dynamic program-
ming is applied, the time complexity will be 16L
with four tags.
Recently, conditional random fields (CRFs) be-
comes popular for word segmentation since it pro-
vides slightly better performance than maximum
entropy method does (Peng et al., 2004). How-
ever, CRFs is a structural learning tool rather than
a simple classification framework. As shift-reduce
parsing is a typical step-wise method that checks
5Someone may argue that maximum entropy Markov
model (MEMM) is truly a tagging tool. Yes, this method was
initialized by (Xue, 2003). However, our empirical results
show that MEMM never outperforms maximum entropy plus
dynamic programming decoding as (Low et al., 2005) in Chi-
nese word segmentation. We also know that the latter reports
the best results in Bakeoff-2. This is why MEMM method is
excluded from our comparison.
each character one by one, it is reasonable to com-
pare it to a classification method over characters.
</bodyText>
<subsectionHeader confidence="0.894176">
3.6 Evaluation Results
</subsectionHeader>
<tableCaption confidence="0.9013515">
Table 3: Corpus size of Bakeoff-2 in number of
words
</tableCaption>
<table confidence="0.997071333333333">
AS CityU MSRA PKU
Training(M) 5.45 1.46 2.37 1.1
Test(K) 122 41 107 104
</table>
<bodyText confidence="0.992394166666667">
The experiments in this section are performed
in all four corpora from Bakeoff-2. Corpus size
information is in Table 3.
Traditionally, word segmentation performance
is measured by F-score ( F = 2RP/(R + P) ),
where the recall (R) and precision (P) are the pro-
portions of the correctly segmented words to all
words in, respectively, the gold-standard segmen-
tation and a segmenter’s output. To compute the
word F-score, all parsing results will be restored
to word boundaries according to the direction of
output arcs.
</bodyText>
<tableCaption confidence="0.878605333333333">
Table 4: The results of parsing and classifica-
tion/tagging approaches using different feature
combinations
</tableCaption>
<table confidence="0.999830615384615">
S.a Feature AS CityU MSRA PKU
Basicb .935 .922 .950 .917
B +AVc .941 .933 .956 .927
+Prevd .937 .923 .951 .918
+AV+Prev .942 .935 .958 .929
Basic .940 .932 .957 .926
E +AV .948 .947 .964 .942
+Prev .944 .940 .962 .931
+AV+Prev .949 .951 .967 .943
n-gram/ce .933 .923 .948 .923
Cf +AV/c .942 .936 .957 .933
n-gram/dg .945 .938 .956 .936
+AV/d .950 .949 .966 .945
</table>
<tableCaption confidence="0.861001">
aScheme
bFeatures in top two blocks of Table 2.
cFive av features are added on the above basic features.
dThree Markovian features in Table 2 are added on the above
basic features.
e/c: Classification
fCharacter classification or tagging using maximum entropy
g/d: Only search in legal tag sequences.
</tableCaption>
<bodyText confidence="0.9994692">
Our comparison with existing work will be con-
ducted in closed test of Bakeoff. The rule for the
closed test is that no additional information be-
yond training corpus is allowed, while open test
of Bakeoff is without such restrict.
</bodyText>
<page confidence="0.997433">
883
</page>
<bodyText confidence="0.999974181818182">
The results with different dependency schemes
are in Table 4. As the feature preact is involved,
a beam search algorithm with width 5 is used to
decode, otherwise, a simple shift-reduce decod-
ing is used. We see that the performance given
by Scheme E is much better than that by Scheme
B. The results of character-based classification
and tagging methods are at the bottom of Table 46.
It is observed that the parsing method outperforms
classification and tagging method without Marko-
vian features or decoding throughout the whole se-
quence. As full features are used, the former and
the latter provide the similar performance.
Due to using a global model like CRFs, our pre-
vious work in (Zhao et al., 2006; Zhao and Kit,
2008c) reported the best results over the evaluated
corpora of Bakeoff-2 until now7. Though those
results are slightly better than the results here, we
still see that the results of character-level depen-
dency parsing approach (Scheme E) are compara-
ble to those state-of-the-art ones on each evaluated
corpus.
</bodyText>
<sectionHeader confidence="0.993179" genericHeader="method">
4 Character Dependencies inside a Word
</sectionHeader>
<bodyText confidence="0.999106">
We further consider exploiting annotated charac-
ter dependencies inside a word (internal depen-
dencies). A parsing task for these internal de-
pendencies incorporated with trivial external de-
pendencies 8 that are transformed from common
word boundaries are correspondingly proposed us-
ing the same parsing way as the previous section.
</bodyText>
<subsectionHeader confidence="0.999719">
4.1 Annotation of Internal Dependencies
</subsectionHeader>
<bodyText confidence="0.999767909090909">
In Subsection 3.1, we assign trivial character de-
pendencies inside a word for the parsing task of
word segmentation, i.e., each character as the head
of its predecessor or successor. These trivial for-
mally defined dependencies may be against the
syntactic or semantic senses of those characters,
as we have discussed in Section 2. Now we will
consider human annotated character dependencies
inside a word.
As such an corpus with annotated inter-
nal dependencies has not been available until
</bodyText>
<footnote confidence="0.967462444444445">
6Only the results of open track are reported in (Low et
al., 2005), while we give a comparison following closed track
rules, so, our results here are not comparable to those of (Low
et al., 2005).
7As n-gram features are used, F-scores in (Zhao et al.,
2006) are, AS:0.953, CityU:0.948, MSRA:0.974,PKU:0.952.
8We correspondingly call dependencies that mark word
boundary external dependencies that correspond to internal
dependencies.
</footnote>
<bodyText confidence="0.999271176470588">
now, we launched an annotation job based on
UPUC segmented corpus of Bakeoff-3(Bakeoff-
2006)(Levow, 2006). The training corpus is with
880K characters and test corpus 270K. However,
the essential of the annotation job is actually con-
ducted in a lexicon.
After a lexicon is extracted from CTB seg-
mented corpus, we use a top-down strategy to an-
notate internal dependencies inside these words
from the lexicon. A long word is first split
into some smaller constituents, and dependencies
among these constituents are determined, char-
acter dependencies inside each constituents are
then annotated. Some simple rules are adopted
to determine dependency relation, e.g., modifiers
are kept marking as dependants and the only
rest constituent will be marked as head at last.
Some words are hard to determine internal depen-
dency relation, such as foreign names, e.g., &amp;quot;fa7
M A -’(Portugal) and ‘ —_L� TA i W(Maradona), and
uninterrupted words (l Ri i7), e.g., 11 tV(ant)
and ‘ q ’(clover). In this case, we simply adopt
a series of linear dependencies with the last char-
acter as head to mark these words.
In the previous section, we have shown that
Scheme E is a better dependency representation
for encoding word boundaries. Thus annotated
internal dependencies are used to replace those
trivial internal dependencies in Scheme E to ob-
tain the corpus that we require. Note that now
we cannot distinguish internal and external de-
pendencies only according to the arc direction
any more, as both left- and right-arc can ap-
pear for internal character dependency represen-
tation. Thus two labeled left arcs, external and
internal, are used for the annotation disambigua-
tion. As internal dependencies are introduced,
we find that some words (about 10%) are con-
structed by two or more parallel constituent parts
according to our annotations, this not only lets
two labeled arcs insufficiently distinguish internal-
and external dependencies, but also makes pars-
ing extremely difficult, namely, a great amount
of non-projective dependencies will appear if we
directly introduce these internal dependencies.
Again, we adopt a series of linear dependencies
with the last character as head to represent in-
ternal dependencies for these words by ignor-
ing their parallel constituents. To handle the re-
mained non-projectivities, a strengthened pseudo-
projectivization technique as in (Zhao and Kit,
</bodyText>
<page confidence="0.998074">
884
</page>
<figureCaption confidence="0.738227">
Figure 2: Annotated internal dependencies (Arc
label a notes trivial external dependencies.)
</figureCaption>
<tableCaption confidence="0.56117">
Table 5: Features for internal dependency parsing
</tableCaption>
<bodyText confidence="0.534603222222222">
Basic Extension
s.char itself, its next two characters, and all bigrams
i.char within the three-character window.
its previous one and next three characters, and
all bigrams within the four-character window.
s.char+i.char
s.h.char
s.rm.dprel
s.curtree
s.curtree+s.char
s_1.curtree+s.char
s.curroot.lm.curtree
s_1.curroot.lm.curtree
s.curroot.lm.curtree+s.char
s_1.curroot.lm.curtree+s.char
s.curtree+s.curroot.lm.curtree
s_1.curtree+s_1.curroot.lm.curtree
s.curtree+s.curroot.lm.curtree+s.char
s_1.curtree+s_1.curroot.lm.curtree+s.char
s_1.curtree+s_1.curroot.lm.curtree+i.char
x.avn, n = 1, ..., 5 (x is s or i.)
s.avn+i.avn, n = 1, ..., 5
preact_1
preact_2
preact_2+preact_1
2008b) is used during parsing. An annotated ex-
ample is illustrated in Figure 2.
</bodyText>
<subsectionHeader confidence="0.999822">
4.2 Learning of Internal Dependencies
</subsectionHeader>
<bodyText confidence="0.999429448275862">
To demonstrate internal character dependencies
are helpful for further processing. A series of
similar word segmentation experiments as in Sub-
section 3.6 are performed. Note that this task is
slightly different from the previous one, as it is a
five-class parsing action classification task as left
arc has two labels to differ internal and external
dependencies. Thus a different feature set has to
be used. However, all input sequences are still pro-
jective.
Features listed in Table 5 are adopted for the
parsing task that annotated character dependencies
exist inside words. The feature curtree in Table
5 is similar to cnseq of Table 2. It first greedily
searches all connected character started from the
given one until an arc with external label is found
over some character. Then it collects all characters
that has been reached to yield an output substring
as feature value.
A comparison of classification/tagging and
parsing methods is given in Table 6. To evalu-
ate the results with word F-score, all external de-
pendencies in outputs are restored as word bound-
aries. There are three models are evaluated in Ta-
ble 6. It is shown that there is a significant perfor-
mance enhancement as annotated internal charac-
ter dependency is introduced. This positive result
shows that annotated internal character dependen-
cies are meaningful.
</bodyText>
<tableCaption confidence="0.999008">
Table 6: Comparison of different methods
</tableCaption>
<table confidence="0.927125909090909">
Approach a basic +AV +Prevb +AV+Prev
Class/Tagc .918 .935 .928 .941
Parsing/wod .921 .937 .924 .942
Parsing/w e .925 .940 .929 .945
aThe highest F-score in Bakeoff-3 is 0.933.
bAs for the tagging method, this means dynamic pro-
gramming decoding; As for the parsing method, this means
three Markovian features.
cCharacter-based classification or tagging method
dUsing trivial internal dependencies in Scheme E.
eUsing annotated internal character dependencies.
</table>
<sectionHeader confidence="0.957094" genericHeader="method">
5 Is Word Still Necessary?
</sectionHeader>
<bodyText confidence="0.999967148148148">
Note that this work is not about joint learning
of word boundaries and syntactic dependencies
such as (Luo, 2003), where a character-based tag-
ging method is used for syntactic constituent pars-
ing from unsegmented Chinese text. Instead, this
work is to explore an alternative way to repre-
sent “word-hood” in Chinese, which is based on
character-level dependencies instead of traditional
word boundaries definition.
Though considering dependencies among
words is not novel (Gao and Suzuki, 2004),
we recognize that this study is the first work
concerned with character dependency. This
study originally intends to lead us to consider an
alternative way that can play the similar role as
word boundary annotations.
In Chinese, not word but character is the actual
minimal unit for either writing or speaking. Word-
hood has been carefully defined by many means,
and this effort results in multi-standard segmented
corpora provided by a series of Bakeoff evalu-
ations. However, from the view of linguistics,
Bakeoff does not solve the problem but technically
skirts round it. As one asks what a Chinese word
is, Bakeoff just answers that we have many def-
initions and each one is fine. Instead, motivated
from the results of the previous two sections, we
</bodyText>
<page confidence="0.995046">
885
</page>
<bodyText confidence="0.993598">
ter dependencies provide a more general and nat-
ural way to reflect character relations within a se-
quence than word boundary annotations do.
</bodyText>
<note confidence="0.321889">
6 Conclusion and Future Work
</note>
<tableCaption confidence="0.7283985">
Table 7: What we have done for character depen-
dency
</tableCaption>
<bodyText confidence="0.999665928571429">
If we regard that our current work is stepping
into more and more annotated character dependen-
cies as shown in Table 7, then it is natural to ex-
tend annotated internal character dependencies to
the whole sequence without those unnatural word
boundary constraints. In this sense, internal and
external character dependency will not need be
differed any more. A full character-level depen-
dency tree is illustrated as shown in Figure 3(a)9
With the help of such a tree, we may define word
or even phrase according to what part of subtree is
picked up. Word-hood, if we still need this con-
cept, can be freely determined later as further pro-
cessing purpose requires.
</bodyText>
<figureCaption confidence="0.998265">
Figure 3: Extended character dependencies
</figureCaption>
<bodyText confidence="0.999789888888889">
Basically we only consider unlabeled depen-
dencies in this work, and dependant labels can be
emptied to do something else, e.g., Figure 3(b)
shows how to extend internal character dependen-
cies of Figure 2 to accommodate part-of-speech
tags. This extension can also be transplanted to a
full character dependency tree of Figure 3(a), then
this may leads to a character-based labeled syntac-
tic dependency tree. In brief, we see that charac-
</bodyText>
<footnote confidence="0.7798376">
9We may easily build such a corpus by embedding an-
notated internal dependencies into a word-level dependency
tree bank. As UPUC corpus of Bakeoff-3 just follows the
word segmentation convention of Chinese tree bank, we have
built such a full character-level dependency tree corpus.
</footnote>
<bodyText confidence="0.999957217391304">
In this study, we initially investigate the possibil-
ity of exploiting character dependencies for Chi-
nese. To show that character-level dependency
can be a good alternative to word boundary rep-
resentation for Chinese, we carry out a series of
parsing experiments. The techniques are devel-
oped step by step. Firstly, we show that word seg-
mentation task can be effectively re-formularized
character-level dependency parsing. The results of
a character-level dependency parser can be com-
parable with traditional methods. Secondly, we
consider annotated character dependencies inside
a word. We show that a parser can still effectively
capture both these annotated internal character de-
pendencies and trivial external dependencies that
are transformed from word boundaries. The exper-
imental results show that annotated internal depen-
dencies even bring performance enhancement and
indirectly verify the usefulness of them. Finally,
we suggest that a full annotated character depen-
dency tree can be constructed over all possible
character pairs within a given sequence, though its
usefulness needs to be explored in the future.
</bodyText>
<sectionHeader confidence="0.997364" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999703">
This work is beneficial from many sources, in-
cluding three anonymous reviewers. Especially,
the authors are grateful to two colleagues, one re-
viewer from EMNLP-2008 who gave some very
insightful comments to help us extend this work,
and Mr. SONG Yan who annotated internal depen-
dencies of top frequent 22K words extracted from
UPUC segmentation corpus. Of course, it is the
duty of the first author if there still exists anything
wrong in this work.
</bodyText>
<sectionHeader confidence="0.99931" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.962722875">
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic parsing action models for multi-lingual de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session ofEMNLP-CoNLL 2007, pages
940–946, Prague, Czech, June 28-30.
</reference>
<figure confidence="0.987586625">
Internal External
trivial trivial
annotated trivial
annotated
Our work
Section 3
Section 4
?
</figure>
<bodyText confidence="0.99752">
suggest that character dependency representation
could present a natural and unified way to allevi-
ate the drawbacks of word boundary representa-
tion that is only able to represent the relation of
neighbored characters.
</bodyText>
<page confidence="0.998368">
886
</page>
<reference confidence="0.999823778846154">
Thomas Emerson. 2005. The second international
Chinese word segmentation bakeoff. In Proceed-
ings of the Fourth SIGHAN Workshop on Chinese
Language Processing, pages 123–133, Jeju Island,
Korea, October 14-15.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics,
30(1):75–93.
Jianfeng Gao and Hisami Suzuki. 2004. Capturing
long distance dependency in language modeling: An
empirical study. In K.-Y. Su, J. Tsujii, J. H. Lee, and
O. Y. Kwong, editors, Natural Language Processing
- IJCNLP 2004, volume 3248 of Lecture Notes in
Computer Science, pages 396–405, Sanya, Hainan
Island, China, March 22-24.
Chooi-Ling GOH, Masayuki Asahara, and Yuji Mat-
sumoto. 2004. Chinese word segmentation by clas-
sification of characters. In ACL SIGHAN Workshop
2004, pages 57–64, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
Johan Hall, Jens Nilsson, Joakim Nivre,
G¨ulsen Eryiˇgit, Be´ata Megyesi, Mattias Nils-
son, and Markus Saers. 2007. Single malt or
blended? a study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 933–939,
Prague, Czech, June.
Gina-Anne Levow. 2006. The third international Chi-
nese language processing bakeoff: Word segmen-
tation and named entity recognition. In Proceed-
ings ofthe Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 108–117, Sydney, Aus-
tralia, July 22-23.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to Chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161–164, Jeju Island, Korea, October 14-15.
Xiaoqiang Luo. 2003. A maximum entropy chinese
character-based parser. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2003), pages 192 – 199,
Sapporo, Japan, July 11-12.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007), pages 122–131,
Prague, Czech, June 28-30.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL-2005), pages 99–106, Ann
Arbor, Michigan, USA, June 25-30.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT 03), pages 149–160, Nancy, France, April
23-25.
Joakim Nivre. 2006. Constraints on non-projective de-
pendency parsing. In Proceedings of 11th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-2006), pages 73–
80, Trento, Italy, April 3-7.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In COLING
2004, pages 562–568, Geneva, Switzerland, August
23-27.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Empiri-
cal Method in Natural Language Processing Confer-
ence, pages 133–142, University of Pennsylvania.
Richard Sproat and Thomas Emerson. 2003. The first
international Chinese word segmentation bakeoff.
In The Second SIGHAN Workshop on Chinese Lan-
guage Processing, pages 133–143, Sapporo, Japan.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29–48.
Hai Zhao and Chunyu Kit. 2008a. Exploiting unla-
beled text with different unsupervised segmentation
criteria for chinese word segmentation. In Research
in Computing Science, volume 33, pages 93–104.
Hai Zhao and Chunyu Kit. 2008b. Parsing syn-
tactic and semantic dependencies with two single-
stage maximum entropy models. In Twelfth Confer-
ence on Computational Natural Language Learning
(CoNLL-2008), pages 203–207, Manchester, UK,
August 16-17.
Hai Zhao and Chunyu Kit. 2008c. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named en-
tity recognition. In The Sixth SIGHAN Workshop
on Chinese Language Processing, pages 106–111,
Hyderabad, India, January 11-12.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of the 20th Asian Pacific
Conference on Language, Information and Compu-
tation, pages 87–94, Wuhan, China, November 1-3.
</reference>
<page confidence="0.998012">
887
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.588052">
<title confidence="0.998926">Character-Level Dependencies in Chinese: Usefulness and Learning</title>
<author confidence="0.999668">Hai Zhao</author>
<affiliation confidence="0.8807435">Department of Chinese, Translation and Linguistics City University of Hong Kong</affiliation>
<address confidence="0.923613">Tat Chee Avenue, Kowloon, Hong Kong, China</address>
<email confidence="0.99581">haizhao@cityu.edu.hk</email>
<abstract confidence="0.99318512">We investigate the possibility of exploiting character-based dependency for Chinese information processing. As Chinese text is made up of character sequences rather than word sequences, word in Chinese is not so natural a concept as in English, nor is word easy to be defined without argument for such a language. Therefore we propose a character-level dependency scheme to represent primary linguistic relationships within a Chinese sentence. The usefulness of character dependencies are verified through two specialized dependency parsing tasks. The first is to handle trivial character dependencies that are equally transformed from traditional word boundaries. The second furthermore considers the case that annotated internal character dependencies inside a word are involved. Both of these results from character-level dependency parsing are positive. This study provides an alternative way to formularize basic characterand word-level representation for Chinese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2772" citStr="Collins, 1999" startWordPosition="428" endWordPosition="429">asks since the first Bakeoff (Bakeoff-1, or Bakeoff-2003)3 (Sproat and Emerson, 2003). Up to Bakeoff-4, seven word segmentation standards have been proposed. However, this does not effectively solve the open problem what a Chinese word should exactly be but raises another issue: what a segmentation standard should be selected for the successive application. As word often plays a basic role for the further language processing, if it cannot be determined in a unified way, then all successive tasks will be affected more or less. Motivated by dependency representation for syntactic parsing since (Collins, 1999) that has been drawn more and more interests in recent years, we suggest that character-level dependencies can be adopted to alleviate this difficulty in Chinese processing. If we regard traditional word boundary as a linear representation for neighbored characters, then character-level dependencies can provide a way to represent non-linear relations between non-neighbored characters. To show that character dependencies can be useful, we develop a parsing scheme for the related learning task and demonstrate its effectiveness. The rest of the paper is organized as follows. The next section show</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangyu Duan</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
</authors>
<title>Probabilistic parsing action models for multi-lingual dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL</booktitle>
<pages>940--946</pages>
<location>Prague, Czech,</location>
<contexts>
<context position="14374" citStr="Duan et al., 2007" startWordPosition="2325" endWordPosition="2328">re function for that character. The feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. 3.4 Decoding Without Markovian feature like preact−1, a shiftreduce parser can scan through an input sequence in linear time. That is, the decoding of a parsing method for word segmentation will be extremely fast. The time complexity of decoding will be 2L for Scheme E, and L for Scheme B, where L is the length of the input sequence. However, it is somewhat complicated as Markovian features are involved. Following the work of (Duan et al., 2007), the decoding in this case is to search a parsing action sequence with the maximal probability. �5di = argmax p(di|di−1di−2...), i where 5di is the object parsing action sequence, p(di|di−1...) is the conditional probability, and di is i-th parsing action. We use a beam search algorithm as in (Ratnaparkhi, 1996) to find the object parsing action sequence. The time complexity of this beam search algorithm will be 4BL for Scheme E and 3BL for Scheme B, where B is the beam width. 3.5 Related Methods Among character-based learning techniques for word segmentation, we may identify two main Notatio</context>
</contexts>
<marker>Duan, Zhao, Xu, 2007</marker>
<rawString>Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic parsing action models for multi-lingual dependency parsing. In Proceedings of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, pages 940–946, Prague, Czech, June 28-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international Chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>123--133</pages>
<location>Jeju Island,</location>
<contexts>
<context position="4382" citStr="Emerson, 2005" startWordPosition="669" endWordPosition="670">ntation Bakeoff, available at http://www.sighan.org/bakeoff2003. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 879–887, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 879 ing the role of character dependencies. Section 6 concludes the paper. 2 To Segment or Not: That Is the Question Though most words can be unambiguously defined in Chinese text, some word boundaries are not so easily determined. We show such three examples as the following. The first example is from the MSRA segmented corpus of Bakeoff-2 (Bakeoff-2005) (Emerson, 2005): �/JK�/&amp;quot;/JL—fi-9110K���/� A/Alk/if/„ a / piece of / “ / Beijing City Beijing Opera OK Sodality / member / entrance / ticket / ” As the guideline of MSRA standard requires any organization’s full name as a word, many long words in this form are frequently encountered. Though this type of ‘words’ may be regarded as an effective unit to some extent, some smaller meaningful constituents can be still identified inside them. Some researchers argue that these should be seen as phrases rather than words. In fact, e.g., a machine translation system will have to segment this type of words into some sma</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international Chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 123–133, Jeju Island, Korea, October 14-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haodi Feng</author>
<author>Kang Chen</author>
<author>Xiaotie Deng</author>
<author>Weimin Zheng</author>
</authors>
<title>Accessor variety criteria for Chinese word extraction.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="12446" citStr="Feng et al., 2004" startWordPosition="2022" endWordPosition="2025"> head, either left or right. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature cnseq returns a substring started from a given character. It checks the direction of the arc that passes the given character and collects all characters with the same arc direction to yield an output substring until the arc direction is changed. Note that all combinational features concerned with this one can be regarded as word-level features. The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al., 2004) is adopted as the unsupervised segmentation criterion. The AV value of a substring s is defined as AV (s) = min{Lav(s), Rav(s)}, where the left and right AV values Lav(s) and Rav(s) are defined, respectively, as the numbers of its distinct predecessor and successor characters. In this work, AV values for substrings are derived from unlabeled training and test corpora by substring counting. Multiple features are used to represent substrings of various lengths identified by the AV criterion. Formally put, the feature function for a n-character substring s with a score AV (s) is defined as avn =</context>
</contexts>
<marker>Feng, Chen, Deng, Zheng, 2004</marker>
<rawString>Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin Zheng. 2004. Accessor variety criteria for Chinese word extraction. Computational Linguistics, 30(1):75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Hisami Suzuki</author>
</authors>
<title>Capturing long distance dependency in language modeling: An empirical study.</title>
<date>2004</date>
<booktitle>Natural Language Processing - IJCNLP 2004,</booktitle>
<volume>3248</volume>
<pages>396--405</pages>
<editor>In K.-Y. Su, J. Tsujii, J. H. Lee, and O. Y. Kwong, editors,</editor>
<location>Sanya, Hainan Island, China,</location>
<contexts>
<context position="27614" citStr="Gao and Suzuki, 2004" startWordPosition="4431" endWordPosition="4434"> dUsing trivial internal dependencies in Scheme E. eUsing annotated internal character dependencies. 5 Is Word Still Necessary? Note that this work is not about joint learning of word boundaries and syntactic dependencies such as (Luo, 2003), where a character-based tagging method is used for syntactic constituent parsing from unsegmented Chinese text. Instead, this work is to explore an alternative way to represent “word-hood” in Chinese, which is based on character-level dependencies instead of traditional word boundaries definition. Though considering dependencies among words is not novel (Gao and Suzuki, 2004), we recognize that this study is the first work concerned with character dependency. This study originally intends to lead us to consider an alternative way that can play the similar role as word boundary annotations. In Chinese, not word but character is the actual minimal unit for either writing or speaking. Wordhood has been carefully defined by many means, and this effort results in multi-standard segmented corpora provided by a series of Bakeoff evaluations. However, from the view of linguistics, Bakeoff does not solve the problem but technically skirts round it. As one asks what a Chine</context>
</contexts>
<marker>Gao, Suzuki, 2004</marker>
<rawString>Jianfeng Gao and Hisami Suzuki. 2004. Capturing long distance dependency in language modeling: An empirical study. In K.-Y. Su, J. Tsujii, J. H. Lee, and O. Y. Kwong, editors, Natural Language Processing - IJCNLP 2004, volume 3248 of Lecture Notes in Computer Science, pages 396–405, Sanya, Hainan Island, China, March 22-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chooi-Ling GOH</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chinese word segmentation by classification of characters.</title>
<date>2004</date>
<booktitle>In ACL SIGHAN Workshop</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona, Spain,</location>
<contexts>
<context position="15483" citStr="GOH et al., 2004" startWordPosition="2510" endWordPosition="2513">ed Methods Among character-based learning techniques for word segmentation, we may identify two main Notation Meaning The character in the top of stack The first character below the top of stack, etc. The first (second) character in the unprocessed sequence, etc. Dependent label Head Leftmost child Rightmost child Right nearest child Character form ’s, i.e., ‘s.dprel’ means dependent label of character in the top of stack Feature combination, i.e., ‘s.char+i.char’ means both s.char and i.char work as a feature function. s s_1,... i, i+1,... dprel h lm rm rn char . + 882 types, classification (GOH et al., 2004) and tagging (Low et al., 2005). Both character classification and tagging need to define the position of character inside a word. Traditionally, the four tags, b, m, e, and s stand, respectively, for the beginning, midle, end of a word, and a singlecharacter as word since (Xue, 2003). The following n-gram features from (Xue, 2003; Low et al., 2005) are used as basic features, (a) Cn(n = −2,−1,0,1,2), (b) CnCn+1(n = −2, −1, 0, 1), (c) C−1C1, where C stands for a character and the subscripts for the relative order to the current character C0. In addition, the feature av that is defined in equat</context>
</contexts>
<marker>GOH, Asahara, Matsumoto, 2004</marker>
<rawString>Chooi-Ling GOH, Masayuki Asahara, and Yuji Matsumoto. 2004. Chinese word segmentation by classification of characters. In ACL SIGHAN Workshop 2004, pages 57–64, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>G¨ulsen Eryiˇgit</author>
<author>Be´ata Megyesi</author>
<author>Mattias Nilsson</author>
<author>Markus Saers</author>
</authors>
<title>Single malt or blended? a study in multilingual parser optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>933--939</pages>
<location>Prague, Czech,</location>
<marker>Hall, Nilsson, Nivre, Eryiˇgit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>Johan Hall, Jens Nilsson, Joakim Nivre, G¨ulsen Eryiˇgit, Be´ata Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? a study in multilingual parser optimization. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 933–939, Prague, Czech, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
</authors>
<title>The third international Chinese language processing bakeoff: Word segmentation and named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings ofthe Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>108--117</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="22049" citStr="Levow, 2006" startWordPosition="3597" endWordPosition="3599">corpus with annotated internal dependencies has not been available until 6Only the results of open track are reported in (Low et al., 2005), while we give a comparison following closed track rules, so, our results here are not comparable to those of (Low et al., 2005). 7As n-gram features are used, F-scores in (Zhao et al., 2006) are, AS:0.953, CityU:0.948, MSRA:0.974,PKU:0.952. 8We correspondingly call dependencies that mark word boundary external dependencies that correspond to internal dependencies. now, we launched an annotation job based on UPUC segmented corpus of Bakeoff-3(Bakeoff2006)(Levow, 2006). The training corpus is with 880K characters and test corpus 270K. However, the essential of the annotation job is actually conducted in a lexicon. After a lexicon is extracted from CTB segmented corpus, we use a top-down strategy to annotate internal dependencies inside these words from the lexicon. A long word is first split into some smaller constituents, and dependencies among these constituents are determined, character dependencies inside each constituents are then annotated. Some simple rules are adopted to determine dependency relation, e.g., modifiers are kept marking as dependants a</context>
</contexts>
<marker>Levow, 2006</marker>
<rawString>Gina-Anne Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings ofthe Fifth SIGHAN Workshop on Chinese Language Processing, pages 108–117, Sydney, Australia, July 22-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<location>Jeju Island,</location>
<contexts>
<context position="15514" citStr="Low et al., 2005" startWordPosition="2517" endWordPosition="2520">d learning techniques for word segmentation, we may identify two main Notation Meaning The character in the top of stack The first character below the top of stack, etc. The first (second) character in the unprocessed sequence, etc. Dependent label Head Leftmost child Rightmost child Right nearest child Character form ’s, i.e., ‘s.dprel’ means dependent label of character in the top of stack Feature combination, i.e., ‘s.char+i.char’ means both s.char and i.char work as a feature function. s s_1,... i, i+1,... dprel h lm rm rn char . + 882 types, classification (GOH et al., 2004) and tagging (Low et al., 2005). Both character classification and tagging need to define the position of character inside a word. Traditionally, the four tags, b, m, e, and s stand, respectively, for the beginning, midle, end of a word, and a singlecharacter as word since (Xue, 2003). The following n-gram features from (Xue, 2003; Low et al., 2005) are used as basic features, (a) Cn(n = −2,−1,0,1,2), (b) CnCn+1(n = −2, −1, 0, 1), (c) C−1C1, where C stands for a character and the subscripts for the relative order to the current character C0. In addition, the feature av that is defined in equation (1) is also taken as an opt</context>
<context position="17654" citStr="Low et al., 2005" startWordPosition="2875" endWordPosition="2878">our tags. Recently, conditional random fields (CRFs) becomes popular for word segmentation since it provides slightly better performance than maximum entropy method does (Peng et al., 2004). However, CRFs is a structural learning tool rather than a simple classification framework. As shift-reduce parsing is a typical step-wise method that checks 5Someone may argue that maximum entropy Markov model (MEMM) is truly a tagging tool. Yes, this method was initialized by (Xue, 2003). However, our empirical results show that MEMM never outperforms maximum entropy plus dynamic programming decoding as (Low et al., 2005) in Chinese word segmentation. We also know that the latter reports the best results in Bakeoff-2. This is why MEMM method is excluded from our comparison. each character one by one, it is reasonable to compare it to a classification method over characters. 3.6 Evaluation Results Table 3: Corpus size of Bakeoff-2 in number of words AS CityU MSRA PKU Training(M) 5.45 1.46 2.37 1.1 Test(K) 122 41 107 104 The experiments in this section are performed in all four corpora from Bakeoff-2. Corpus size information is in Table 3. Traditionally, word segmentation performance is measured by F-score ( F =</context>
<context position="21576" citStr="Low et al., 2005" startWordPosition="3528" endWordPosition="3531">he previous section. 4.1 Annotation of Internal Dependencies In Subsection 3.1, we assign trivial character dependencies inside a word for the parsing task of word segmentation, i.e., each character as the head of its predecessor or successor. These trivial formally defined dependencies may be against the syntactic or semantic senses of those characters, as we have discussed in Section 2. Now we will consider human annotated character dependencies inside a word. As such an corpus with annotated internal dependencies has not been available until 6Only the results of open track are reported in (Low et al., 2005), while we give a comparison following closed track rules, so, our results here are not comparable to those of (Low et al., 2005). 7As n-gram features are used, F-scores in (Zhao et al., 2006) are, AS:0.953, CityU:0.948, MSRA:0.974,PKU:0.952. 8We correspondingly call dependencies that mark word boundary external dependencies that correspond to internal dependencies. now, we launched an annotation job based on UPUC segmented corpus of Bakeoff-3(Bakeoff2006)(Levow, 2006). The training corpus is with 880K characters and test corpus 270K. However, the essential of the annotation job is actually co</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to Chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161–164, Jeju Island, Korea, October 14-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>A maximum entropy chinese character-based parser.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP 2003), pages 192 – 199,</booktitle>
<pages>11--12</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="27234" citStr="Luo, 2003" startWordPosition="4377" endWordPosition="4378">nt methods Approach a basic +AV +Prevb +AV+Prev Class/Tagc .918 .935 .928 .941 Parsing/wod .921 .937 .924 .942 Parsing/w e .925 .940 .929 .945 aThe highest F-score in Bakeoff-3 is 0.933. bAs for the tagging method, this means dynamic programming decoding; As for the parsing method, this means three Markovian features. cCharacter-based classification or tagging method dUsing trivial internal dependencies in Scheme E. eUsing annotated internal character dependencies. 5 Is Word Still Necessary? Note that this work is not about joint learning of word boundaries and syntactic dependencies such as (Luo, 2003), where a character-based tagging method is used for syntactic constituent parsing from unsegmented Chinese text. Instead, this work is to explore an alternative way to represent “word-hood” in Chinese, which is based on character-level dependencies instead of traditional word boundaries definition. Though considering dependencies among words is not novel (Gao and Suzuki, 2004), we recognize that this study is the first work concerned with character dependency. This study originally intends to lead us to consider an alternative way that can play the similar role as word boundary annotations. I</context>
</contexts>
<marker>Luo, 2003</marker>
<rawString>Xiaoqiang Luo. 2003. A maximum entropy chinese character-based parser. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP 2003), pages 192 – 199, Sapporo, Japan, July 11-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>122--131</pages>
<location>Prague, Czech,</location>
<contexts>
<context position="9599" citStr="McDonald and Nivre, 2007" startWordPosition="1540" endWordPosition="1543">e graph D is a single-rooted tree. For an arc wi —* wj, if wi &lt; wj, then it is called rightarc, otherwise left-arc. Following the above four constraints and considering segmentation characteristics, we may have two character dependency representation schemes as shown in Figure 1 by using a series of trivial dependencies inside or outside a word. Note that we use arc direction to distinguish connected and segmented relation among characters. The scheme with the assistant root node before the sequence in Figure 1 is called Scheme B, and the other Scheme E. 3.2 Shift-reduce Parsing According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. Since both dependency schemes that we construct for parsing are well-formed and projective, the latter is chosen as the parsing framework for the sake of efficiency. In detail, a shift-reduce method is adopted as in (Nivre, 2003). The method is step-wise and a classifier is used to make a parsing decision step by step. In each step, the classifier checks a clique pair 4, namely, TOP, the top of a stack that consists of the processed cliques, and, I</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 122–131, Prague, Czech, June 28-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL-2005),</booktitle>
<pages>99--106</pages>
<location>Ann Arbor, Michigan, USA,</location>
<contexts>
<context position="7622" citStr="Nivre and Nilsson, 2005" startWordPosition="1176" endWordPosition="1180">in a parsing way. 3.1 Formularization Using a character-level dependency representation, we first show how a word segmentation task can be transformed into a dependency parsing problem. Since word segmentation is traditionally formularized as an unlabeled character chunking task since (Xue, 2003), only unlabeled dependencies are concerned in the transformation. There are many ways to transform chunks in a sequence into dependency representation. However, for the sake of simplicity, only well-formed and projective output sequences are considered for our processing. Borrowing the notation from (Nivre and Nilsson, 2005), an unlabeled dependency graph is formally defined as follows: An unlabeled dependency graph for a string of cliques (i.e., words and characters) W = 880 Figure 1: Two character dependency schemes w1...wn is an unlabeled directed graph D = (W, A), where (a) W is the set of ordered nodes, i.e. clique tokens in the input string, ordered by a linear precedence relation &lt;, (b) A is a set of unlabeled arcs (wi, wj), where wi, wj E W, If (wi, wj) E A, wi is called the head of wj and wj a dependent of wi. Traditionally, the notation wi —* wj means (wi, wj) E A; wi —** wj denotes the reflexive and tr</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL-2005), pages 99–106, Ann Arbor, Michigan, USA, June 25-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03),</booktitle>
<pages>149--160</pages>
<location>Nancy, France,</location>
<contexts>
<context position="9976" citStr="Nivre, 2003" startWordPosition="1602" endWordPosition="1603">ish connected and segmented relation among characters. The scheme with the assistant root node before the sequence in Figure 1 is called Scheme B, and the other Scheme E. 3.2 Shift-reduce Parsing According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. Since both dependency schemes that we construct for parsing are well-formed and projective, the latter is chosen as the parsing framework for the sake of efficiency. In detail, a shift-reduce method is adopted as in (Nivre, 2003). The method is step-wise and a classifier is used to make a parsing decision step by step. In each step, the classifier checks a clique pair 4, namely, TOP, the top of a stack that consists of the processed cliques, and, INPUT, the first clique in the unprocessed sequence, to determine if a dependent relation should be established between them. Besides two arc-building actions, a shift action and a reduce action are also defined, as follows, Left-arc: Add an arc from INPUT to TOP and pop the stack. Right-arc: Add an arc from TOP to INPUT and push INPUT onto the stack. Reduce: Pop TOP from the</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03), pages 149–160, Nancy, France, April 23-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Constraints on non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006),</booktitle>
<pages>73--80</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="8400" citStr="Nivre, 2006" startWordPosition="1321" endWordPosition="1322">wo character dependency schemes w1...wn is an unlabeled directed graph D = (W, A), where (a) W is the set of ordered nodes, i.e. clique tokens in the input string, ordered by a linear precedence relation &lt;, (b) A is a set of unlabeled arcs (wi, wj), where wi, wj E W, If (wi, wj) E A, wi is called the head of wj and wj a dependent of wi. Traditionally, the notation wi —* wj means (wi, wj) E A; wi —** wj denotes the reflexive and transitive closure of the (unlabeled) arc relation. We assume that the designed dependency structure satisfies the following common constraints in existing literature (Nivre, 2006). (1) D is weakly connected, that is, the corresponding undirected graph is connected. (CONNECTEDNESS) (2) The graph D is acyclic, i.e., if wi —* wj then not wj —** wi. (ACYCLICITY) (3) There is at most one arc (wi, wj) E A, bwj E W. (SINGLE-HEAD) (4) An arc wi —* wk is projective iff, for every word wj occurring between wi and wk in the string (wi &lt; wj &lt; wk or wi &gt; wj &gt; wk), wi —** wj . (PROJECTIVITY) We say that D is well-formed iff it is acyclic and connected, and D is projective iff every arcs in A are projective. Note that the above four conditions entail that the graph D is a single-root</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Constraints on non-projective dependency parsing. In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), pages 73– 80, Trento, Italy, April 3-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In COLING 2004,</booktitle>
<pages>562--568</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="17226" citStr="Peng et al., 2004" startWordPosition="2809" endWordPosition="2812">thm is adopted to find a tag sequence with the maximal joint probability from all legal tag sequences. If such a dynamic programming decoding is adopted, then this method for word segmentation is regarded as character tagging 5. The time complexity of character-based classification method for decoding is L, which is the best result in decoding velocity. As dynamic programming is applied, the time complexity will be 16L with four tags. Recently, conditional random fields (CRFs) becomes popular for word segmentation since it provides slightly better performance than maximum entropy method does (Peng et al., 2004). However, CRFs is a structural learning tool rather than a simple classification framework. As shift-reduce parsing is a typical step-wise method that checks 5Someone may argue that maximum entropy Markov model (MEMM) is truly a tagging tool. Yes, this method was initialized by (Xue, 2003). However, our empirical results show that MEMM never outperforms maximum entropy plus dynamic programming decoding as (Low et al., 2005) in Chinese word segmentation. We also know that the latter reports the best results in Bakeoff-2. This is why MEMM method is excluded from our comparison. each character o</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In COLING 2004, pages 562–568, Geneva, Switzerland, August 23-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Empirical Method in Natural Language Processing Conference,</booktitle>
<pages>133--142</pages>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="14688" citStr="Ratnaparkhi, 1996" startWordPosition="2378" endWordPosition="2379">ng of a parsing method for word segmentation will be extremely fast. The time complexity of decoding will be 2L for Scheme E, and L for Scheme B, where L is the length of the input sequence. However, it is somewhat complicated as Markovian features are involved. Following the work of (Duan et al., 2007), the decoding in this case is to search a parsing action sequence with the maximal probability. �5di = argmax p(di|di−1di−2...), i where 5di is the object parsing action sequence, p(di|di−1...) is the conditional probability, and di is i-th parsing action. We use a beam search algorithm as in (Ratnaparkhi, 1996) to find the object parsing action sequence. The time complexity of this beam search algorithm will be 4BL for Scheme E and 3BL for Scheme B, where B is the beam width. 3.5 Related Methods Among character-based learning techniques for word segmentation, we may identify two main Notation Meaning The character in the top of stack The first character below the top of stack, etc. The first (second) character in the unprocessed sequence, etc. Dependent label Head Leftmost child Rightmost child Right nearest child Character form ’s, i.e., ‘s.dprel’ means dependent label of character in the top of st</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proceedings of the Empirical Method in Natural Language Processing Conference, pages 133–142, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first international Chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In The Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133--143</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2243" citStr="Sproat and Emerson, 2003" startWordPosition="339" endWordPosition="342">ritten Chinese text, including Chinese character(hanzi), punctuation, and foreign letters. However, Chinese characters often cover the most part. 2Even in European languages, a naive but necessary method to properly define word is to list them all by hand. Thank the first anonymous reviewer who points this fact. brings the argument about how to determine the word-hood in Chinese. Linguists’ views about what is a Chinese word diverge so greatly that multiple word segmentation standards have been proposed for computational linguistics tasks since the first Bakeoff (Bakeoff-1, or Bakeoff-2003)3 (Sproat and Emerson, 2003). Up to Bakeoff-4, seven word segmentation standards have been proposed. However, this does not effectively solve the open problem what a Chinese word should exactly be but raises another issue: what a segmentation standard should be selected for the successive application. As word often plays a basic role for the further language processing, if it cannot be determined in a unified way, then all successive tasks will be affected more or less. Motivated by dependency representation for syntactic parsing since (Collins, 1999) that has been drawn more and more interests in recent years, we sugges</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The first international Chinese word segmentation bakeoff. In The Second SIGHAN Workshop on Chinese Language Processing, pages 133–143, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="7295" citStr="Xue, 2003" startWordPosition="1130" endWordPosition="1131"> way that syntactic dependencies over words are usually annotated. We will initially develop a character-level dependency parsing scheme in this section. Especially, we show character dependencies, even those trivial ones that are equally transformed from pre-defined word boundaries, can be effectively captured in a parsing way. 3.1 Formularization Using a character-level dependency representation, we first show how a word segmentation task can be transformed into a dependency parsing problem. Since word segmentation is traditionally formularized as an unlabeled character chunking task since (Xue, 2003), only unlabeled dependencies are concerned in the transformation. There are many ways to transform chunks in a sequence into dependency representation. However, for the sake of simplicity, only well-formed and projective output sequences are considered for our processing. Borrowing the notation from (Nivre and Nilsson, 2005), an unlabeled dependency graph is formally defined as follows: An unlabeled dependency graph for a string of cliques (i.e., words and characters) W = 880 Figure 1: Two character dependency schemes w1...wn is an unlabeled directed graph D = (W, A), where (a) W is the set o</context>
<context position="15768" citStr="Xue, 2003" startWordPosition="2563" endWordPosition="2564">t child Rightmost child Right nearest child Character form ’s, i.e., ‘s.dprel’ means dependent label of character in the top of stack Feature combination, i.e., ‘s.char+i.char’ means both s.char and i.char work as a feature function. s s_1,... i, i+1,... dprel h lm rm rn char . + 882 types, classification (GOH et al., 2004) and tagging (Low et al., 2005). Both character classification and tagging need to define the position of character inside a word. Traditionally, the four tags, b, m, e, and s stand, respectively, for the beginning, midle, end of a word, and a singlecharacter as word since (Xue, 2003). The following n-gram features from (Xue, 2003; Low et al., 2005) are used as basic features, (a) Cn(n = −2,−1,0,1,2), (b) CnCn+1(n = −2, −1, 0, 1), (c) C−1C1, where C stands for a character and the subscripts for the relative order to the current character C0. In addition, the feature av that is defined in equation (1) is also taken as an option. avn (n=1,...,5) is applied as feature for the current character. While word segmentation is conducted as a classification task, each individual character will be simply assigned a tag with the maximal probability given by the classifier. In this cas</context>
<context position="17517" citStr="Xue, 2003" startWordPosition="2857" endWordPosition="2858">ing is L, which is the best result in decoding velocity. As dynamic programming is applied, the time complexity will be 16L with four tags. Recently, conditional random fields (CRFs) becomes popular for word segmentation since it provides slightly better performance than maximum entropy method does (Peng et al., 2004). However, CRFs is a structural learning tool rather than a simple classification framework. As shift-reduce parsing is a typical step-wise method that checks 5Someone may argue that maximum entropy Markov model (MEMM) is truly a tagging tool. Yes, this method was initialized by (Xue, 2003). However, our empirical results show that MEMM never outperforms maximum entropy plus dynamic programming decoding as (Low et al., 2005) in Chinese word segmentation. We also know that the latter reports the best results in Bakeoff-2. This is why MEMM method is excluded from our comparison. each character one by one, it is reasonable to compare it to a classification method over characters. 3.6 Evaluation Results Table 3: Corpus size of Bakeoff-2 in number of words AS CityU MSRA PKU Training(M) 5.45 1.46 2.37 1.1 Test(K) 122 41 107 104 The experiments in this section are performed in all four</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Research in Computing Science,</booktitle>
<volume>33</volume>
<pages>93--104</pages>
<contexts>
<context position="12393" citStr="Zhao and Kit, 2008" startWordPosition="2013" endWordPosition="2016">dency parsing, dprel means the arc direction from the head, either left or right. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature cnseq returns a substring started from a given character. It checks the direction of the arc that passes the given character and collects all characters with the same arc direction to yield an output substring until the arc direction is changed. Note that all combinational features concerned with this one can be regarded as word-level features. The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al., 2004) is adopted as the unsupervised segmentation criterion. The AV value of a substring s is defined as AV (s) = min{Lav(s), Rav(s)}, where the left and right AV values Lav(s) and Rav(s) are defined, respectively, as the numbers of its distinct predecessor and successor characters. In this work, AV values for substrings are derived from unlabeled training and test corpora by substring counting. Multiple features are used to represent substrings of various lengths identified by the AV criterion. Formally put, the feature function for a n-characte</context>
<context position="20306" citStr="Zhao and Kit, 2008" startWordPosition="3329" endWordPosition="3332">arch algorithm with width 5 is used to decode, otherwise, a simple shift-reduce decoding is used. We see that the performance given by Scheme E is much better than that by Scheme B. The results of character-based classification and tagging methods are at the bottom of Table 46. It is observed that the parsing method outperforms classification and tagging method without Markovian features or decoding throughout the whole sequence. As full features are used, the former and the latter provide the similar performance. Due to using a global model like CRFs, our previous work in (Zhao et al., 2006; Zhao and Kit, 2008c) reported the best results over the evaluated corpora of Bakeoff-2 until now7. Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus. 4 Character Dependencies inside a Word We further consider exploiting annotated character dependencies inside a word (internal dependencies). A parsing task for these internal dependencies incorporated with trivial external dependencies 8 that are transformed from common word boundaries are cor</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008a. Exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation. In Research in Computing Science, volume 33, pages 93–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Parsing syntactic and semantic dependencies with two singlestage maximum entropy models.</title>
<date>2008</date>
<booktitle>In Twelfth Conference on Computational Natural Language Learning (CoNLL-2008),</booktitle>
<pages>203--207</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="12393" citStr="Zhao and Kit, 2008" startWordPosition="2013" endWordPosition="2016">dency parsing, dprel means the arc direction from the head, either left or right. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature cnseq returns a substring started from a given character. It checks the direction of the arc that passes the given character and collects all characters with the same arc direction to yield an output substring until the arc direction is changed. Note that all combinational features concerned with this one can be regarded as word-level features. The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al., 2004) is adopted as the unsupervised segmentation criterion. The AV value of a substring s is defined as AV (s) = min{Lav(s), Rav(s)}, where the left and right AV values Lav(s) and Rav(s) are defined, respectively, as the numbers of its distinct predecessor and successor characters. In this work, AV values for substrings are derived from unlabeled training and test corpora by substring counting. Multiple features are used to represent substrings of various lengths identified by the AV criterion. Formally put, the feature function for a n-characte</context>
<context position="20306" citStr="Zhao and Kit, 2008" startWordPosition="3329" endWordPosition="3332">arch algorithm with width 5 is used to decode, otherwise, a simple shift-reduce decoding is used. We see that the performance given by Scheme E is much better than that by Scheme B. The results of character-based classification and tagging methods are at the bottom of Table 46. It is observed that the parsing method outperforms classification and tagging method without Markovian features or decoding throughout the whole sequence. As full features are used, the former and the latter provide the similar performance. Due to using a global model like CRFs, our previous work in (Zhao et al., 2006; Zhao and Kit, 2008c) reported the best results over the evaluated corpora of Bakeoff-2 until now7. Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus. 4 Character Dependencies inside a Word We further consider exploiting annotated character dependencies inside a word (internal dependencies). A parsing task for these internal dependencies incorporated with trivial external dependencies 8 that are transformed from common word boundaries are cor</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008b. Parsing syntactic and semantic dependencies with two singlestage maximum entropy models. In Twelfth Conference on Computational Natural Language Learning (CoNLL-2008), pages 203–207, Manchester, UK, August 16-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In The Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>106--111</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="12393" citStr="Zhao and Kit, 2008" startWordPosition="2013" endWordPosition="2016">dency parsing, dprel means the arc direction from the head, either left or right. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature cnseq returns a substring started from a given character. It checks the direction of the arc that passes the given character and collects all characters with the same arc direction to yield an output substring until the arc direction is changed. Note that all combinational features concerned with this one can be regarded as word-level features. The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al., 2004) is adopted as the unsupervised segmentation criterion. The AV value of a substring s is defined as AV (s) = min{Lav(s), Rav(s)}, where the left and right AV values Lav(s) and Rav(s) are defined, respectively, as the numbers of its distinct predecessor and successor characters. In this work, AV values for substrings are derived from unlabeled training and test corpora by substring counting. Multiple features are used to represent substrings of various lengths identified by the AV criterion. Formally put, the feature function for a n-characte</context>
<context position="20306" citStr="Zhao and Kit, 2008" startWordPosition="3329" endWordPosition="3332">arch algorithm with width 5 is used to decode, otherwise, a simple shift-reduce decoding is used. We see that the performance given by Scheme E is much better than that by Scheme B. The results of character-based classification and tagging methods are at the bottom of Table 46. It is observed that the parsing method outperforms classification and tagging method without Markovian features or decoding throughout the whole sequence. As full features are used, the former and the latter provide the similar performance. Due to using a global model like CRFs, our previous work in (Zhao et al., 2006; Zhao and Kit, 2008c) reported the best results over the evaluated corpora of Bakeoff-2 until now7. Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus. 4 Character Dependencies inside a Word We further consider exploiting annotated character dependencies inside a word (internal dependencies). A parsing task for these internal dependencies incorporated with trivial external dependencies 8 that are transformed from common word boundaries are cor</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008c. Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition. In The Sixth SIGHAN Workshop on Chinese Language Processing, pages 106–111, Hyderabad, India, January 11-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Effective tag set selection in Chinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the 20th Asian Pacific Conference on Language, Information and Computation,</booktitle>
<pages>87--94</pages>
<location>Wuhan, China,</location>
<contexts>
<context position="20286" citStr="Zhao et al., 2006" startWordPosition="3325" endWordPosition="3328">involved, a beam search algorithm with width 5 is used to decode, otherwise, a simple shift-reduce decoding is used. We see that the performance given by Scheme E is much better than that by Scheme B. The results of character-based classification and tagging methods are at the bottom of Table 46. It is observed that the parsing method outperforms classification and tagging method without Markovian features or decoding throughout the whole sequence. As full features are used, the former and the latter provide the similar performance. Due to using a global model like CRFs, our previous work in (Zhao et al., 2006; Zhao and Kit, 2008c) reported the best results over the evaluated corpora of Bakeoff-2 until now7. Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus. 4 Character Dependencies inside a Word We further consider exploiting annotated character dependencies inside a word (internal dependencies). A parsing task for these internal dependencies incorporated with trivial external dependencies 8 that are transformed from common wor</context>
<context position="21768" citStr="Zhao et al., 2006" startWordPosition="3562" endWordPosition="3565">aracter as the head of its predecessor or successor. These trivial formally defined dependencies may be against the syntactic or semantic senses of those characters, as we have discussed in Section 2. Now we will consider human annotated character dependencies inside a word. As such an corpus with annotated internal dependencies has not been available until 6Only the results of open track are reported in (Low et al., 2005), while we give a comparison following closed track rules, so, our results here are not comparable to those of (Low et al., 2005). 7As n-gram features are used, F-scores in (Zhao et al., 2006) are, AS:0.953, CityU:0.948, MSRA:0.974,PKU:0.952. 8We correspondingly call dependencies that mark word boundary external dependencies that correspond to internal dependencies. now, we launched an annotation job based on UPUC segmented corpus of Bakeoff-3(Bakeoff2006)(Levow, 2006). The training corpus is with 880K characters and test corpus 270K. However, the essential of the annotation job is actually conducted in a lexicon. After a lexicon is extracted from CTB segmented corpus, we use a top-down strategy to annotate internal dependencies inside these words from the lexicon. A long word is f</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2006. Effective tag set selection in Chinese word segmentation via conditional random field modeling. In Proceedings of the 20th Asian Pacific Conference on Language, Information and Computation, pages 87–94, Wuhan, China, November 1-3.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>