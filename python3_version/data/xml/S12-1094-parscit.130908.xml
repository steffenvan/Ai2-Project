<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.041169">
<title confidence="0.9961535">
UNT: A Supervised Synergistic Approach
to Semantic Text Similarity
</title>
<author confidence="0.998828">
Carmen Banea, Samer Hassan, Michael Mohler, Rada Mihalcea
</author>
<affiliation confidence="0.999693">
University of North Texas
</affiliation>
<address confidence="0.845904">
Denton, TX, USA
</address>
<email confidence="0.998872">
{CarmenBanea,SamerHassan,MichaelMohler}@my.unt.edu, rada@cs.unt.edu
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892714285714">
This paper presents the systems that we par-
ticipated with in the Semantic Text Similar-
ity task at SEMEVAL 2012. Based on prior
research in semantic similarity and related-
ness, we combine various methods in a ma-
chine learning framework. The three varia-
tions submitted during the task evaluation pe-
riod ranked number 5, 9 and 14 among the 89
participating systems. Our evaluations show
that corpus-based methods display a more ro-
bust behavior on the training data, yet com-
bining a variety of methods allows a learning
algorithm to achieve a superior decision than
that achievable by any of the individual parts.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999696057692308">
Measures of text similarity have been used for a
long time in applications in natural language pro-
cessing and related areas. One of the earliest ap-
plications of text similarity is perhaps the vector-
space model used in information retrieval, where the
document most relevant to an input query is deter-
mined by ranking documents in a collection in re-
versed order of their similarity to the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and more recently for
extractive summarization (Salton et al., 1997), and
methods for automatic evaluation of machine trans-
lation (Papineni et al., 2002) or text summarization
(Lin and Hovy, 2003). Measures of text similarity
were also found useful for the evaluation of text co-
herence (Lapata and Barzilay, 2005).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stop-word removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is
an obvious similarity between the text segments I
own a dog and I have an animal, but most of the
current text similarity metrics will fail in identifying
any kind of connection between these texts.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al., 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al., 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus measures
such as Latent Semantic Analysis (Landauer et al.,
1997), Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007), or Salient Semantic Analysis
(Hassan and Mihalcea, 2011).
In this paper, we describe the system with which
</bodyText>
<page confidence="0.983668">
635
</page>
<note confidence="0.972756">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635–642,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999626857142857">
we participated in the SEMEVAL 2012 task on se-
mantic text similarity (Agirre et al., 2012). The sys-
tem builds upon our earlier work on corpus-based
and knowledge-based methods of text semantic sim-
ilarity (Mihalcea et al., 2006; Hassan and Mihal-
cea, 2011; Mohler et al., 2011), and combines all
these previous methods into a meta-system by us-
ing machine learning. The framework provided by
the task organizers also enabled us to perform an in-
depth analysis of the various components used in our
system, and draw conclusions concerning the role
played by the different resources, features, and al-
gorithms in building a state-of-the-art semantic text
similarity system.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999945642857143">
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al., 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al., 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
</bodyText>
<sectionHeader confidence="0.946186" genericHeader="method">
3 Semantic Textual Similarity System
</sectionHeader>
<bodyText confidence="0.999980545454546">
The system we proposed for the SEMEVAL 2012
Semantic Textual Similarity task builds upon both
knowledge- and corpus-based methods previously
described in (Mihalcea et al., 2006; Hassan and Mi-
halcea, 2011; Mohler et al., 2011). The predictions
of these independent systems, paired with additional
salient features, are leveraged by a meta-system that
employs machine learning. In this section, we will
elaborate further on the resources we use, our fea-
tures, and the components of our machine learning
system. We will start by describing the task setup.
</bodyText>
<subsectionHeader confidence="0.99861">
3.1 Task Setup
</subsectionHeader>
<bodyText confidence="0.999988529411765">
The training data released by the task organiz-
ers consists of three datasets showcasing two sen-
tences per line and a manually assigned similarity
score ranging from 0 (no relation) to 5 (semanti-
cally equivalent). The datasets1 provided are taken
from the Microsoft Research Paraphrase Corpus
(M5Rpar), the Microsoft Research Video Descrip-
tion Corpus (M5Rvid), and the WMT2008 devel-
opment dataset (Europarl section)(5MTeuroparl);
they each consist of about 750 sentence pairs with
the class distribution varying with each dataset. The
testing data contains additional sentences from the
same collections as the training data as well as
from two additional unknown sets (OnWN and
5MTnews); they range from 399 to 750 sentence
pairs. The reader may refer to (Agirre et al., 2012)
for additional information regarding this task.
</bodyText>
<subsectionHeader confidence="0.995236">
3.2 Resources
</subsectionHeader>
<bodyText confidence="0.999957090909091">
Wikipedia2 is a free on-line encyclopedia, represent-
ing the outcome of a continuous collaborative effort
of a large number of volunteer contributors. Virtu-
ally any Internet user can create or edit a Wikipedia
web page, and this “freedom of contribution” has a
positive impact on both the quantity (fast-growing
number of articles) and the quality (potential mis-
takes are quickly corrected within the collaborative
environment) of this on-line resource. The basic en-
try in Wikipedia is an article which describes an en-
tity or an event, and which, in addition to untagged
</bodyText>
<footnote confidence="0.99995525">
1http://www.cs.york.ac.uk/semeval-2012/
task6/data/uploads/datasets/train-readme.
txt
2www.wikipedia.org
</footnote>
<page confidence="0.998508">
636
</page>
<bodyText confidence="0.999953416666667">
content, also consists of hyperlinked text to other
pages within or outside of Wikipedia. These hyper-
links are meant to guide the reader to pages that pro-
vide additional information / clarifications, so that
a better understanding of the primary concept can
be achieved. The structure of Wikipedia in terms of
pages and hyperlinks is exploited directly by seman-
tic similarity methods such as ESA (Gabrilovich and
Markovitch, 2007), or SSA (Hassan and Mihalcea,
2011).
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
between basic units of meaning, or synsets. A synset
groups together senses of different words that share
a very similar meaning, which act in a particu-
lar context as synonyms. Each synset is accompa-
nied by a gloss or definition, and one or two ex-
amples illustrating usage in the given context. Un-
like a traditional thesaurus, the structure of Word-
Net is able to encode additional relationships be-
side synonymy, such as antonymy, hypernymy, hy-
ponymy, meronymy, entailment, etc., which vari-
ous knowledge-based methods use to derive seman-
tic similarity.
</bodyText>
<subsectionHeader confidence="0.973583">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999991">
Our meta-system uses several features, which can
be grouped into knowledge-based, corpus-based,
and bipartite graph matching, as described below.
The abbreviations appearing between parentheses
by each method allow for easy cross-referencing
with the evaluations provided in Table 1.
</bodyText>
<subsectionHeader confidence="0.9650695">
3.3.1 Knowledge-based Semantic Similarity
Features
</subsectionHeader>
<bodyText confidence="0.9918004375">
Following prior work from our group (Mihalcea
et al., 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for
each open-class word in one of the input texts, we
compute the maximum semantic similarity (using
the WordNet::Similarity package (Pedersen et al.,
2004)) that can be obtained by pairing it with any
open-class word in the other input text. All the
word-to-word similarity scores obtained in this way
are summed and normalized to the length of the two
input texts. We provide below a short description
for each of the similarity metrics employed by this
system3.
The shortest path (Path) similarity is determined
as:
</bodyText>
<equation confidence="0.993231333333333">
1
Simpath = (1)
length
</equation>
<bodyText confidence="0.93084">
where length is the length of the shortest path be-
tween two concepts using node-counting (including
the end nodes).
The Leacock &amp; Chodorow (Leacock and
Chodorow, 1998) (LCH) similarity is determined
as:
</bodyText>
<equation confidence="0.990082">
length
Simlch = − log (2)
2 � �
</equation>
<bodyText confidence="0.999980769230769">
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu &amp; Palmer (Wu and Palmer, 1994) (W UP)
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
</bodyText>
<equation confidence="0.9977245">
2 * depth(LCS)
Sim,,up = depth(concept1) + depth(concept2) (3)
</equation>
<bodyText confidence="0.999861666666667">
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
</bodyText>
<equation confidence="0.992256">
Sim,,, = IC(LCS) (4)
</equation>
<bodyText confidence="0.692216">
where IC is defined as:
</bodyText>
<equation confidence="0.999229">
IC(c) = − log P(c) (5)
</equation>
<bodyText confidence="0.998955333333333">
and P(c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik’s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
</bodyText>
<equation confidence="0.936134">
Simlzn
2 * IC(LCS) (6)
= IC(concept1) + IC(concept2)
</equation>
<footnote confidence="0.909543">
3We point out that the similarity metric proposed by Hirst &amp;
St. Onge was not considered due to the time constraints associ-
ated with the STS task.
</footnote>
<page confidence="0.996011">
637
</page>
<bodyText confidence="0.9987445">
We also consider the Jiang &amp; Conrath (Jiang and
Conrath, 1997) (JCN) measure of similarity:
</bodyText>
<equation confidence="0.997529333333333">
1
Simi&amp;quot; = IC(concept1) + IC(concept2) − 2 * IC(LCS)
(7)
</equation>
<bodyText confidence="0.9999735">
Each of the measures listed above is used as a fea-
ture by our meta-system.
</bodyText>
<subsectionHeader confidence="0.9832555">
3.3.2 Corpus-based Semantic Similarity
Features
</subsectionHeader>
<bodyText confidence="0.99953992">
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
stand out as different, since they rely on a concept-
space representation. In these methods, the semantic
profile of a word is expressed in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch. In the
experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia
download from October 2008, with approximately
6 million articles, and more than 9.5 million hyper-
links.
Latent Semantic Analysis (LSA) (Landauer et al.,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. Since
encyclopedic knowledge is typically organized into
concepts (or topics), each concept is further de-
scribed using definitions and examples. ESA relies
on the distribution of words inside the encyclopedic
descriptions. It builds semantic representations for
a given word using a word-document association,
where the document represents a Wikipedia article
(concept). ESA is in effect a Vector Space Model
(VSM) built using Wikipedia corpus, where vectors
represents word-articles association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction and interpretation of words as ESA,
yet it uses salient concepts gathered from encyclo-
pedic knowledge, where a “concept” represents an
unambiguous word or phrase with a concrete mean-
ing, and which affords an encyclopedic definition.
Saliency in this case is determined based on the
word being hyperlinked (either trough manual or au-
tomatic annotations) in context, implying that they
are highly relevant to the given text. SSA is an ex-
ample of Generalized Vector Space Model (GVSM),
where vectors represent word-concepts associations.
In order to determine the similarity of two text
fragments , we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail below.
Both variations were paired with the LSA, ESA,
and SSA systems resulting in six similarity scores
that were used as features by our meta-system,
namely LSAcos, LSAalign, ESAcos, ESAalign,
SSAcos, and SSAalign.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
ber of shared terms (w) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list co which holds the strongest semantic pairings
between the fragments’ terms, such that each term
can only belong to one and only one pair.
</bodyText>
<equation confidence="0.984458333333333">
|ϕ|
Sim(Ta,b) = T (w + Ei=1 coi) x (tab) (8)
a + b
</equation>
<bodyText confidence="0.999802333333333">
where w is the number of shared terms between the
text fragments and coi is the similarity score for the
ith pairing.
</bodyText>
<subsectionHeader confidence="0.944282">
3.3.3 Bipartite Graph Matching
</subsectionHeader>
<bodyText confidence="0.9985245">
In an attempt to move beyond the bag-of-words
paradigm described thus far, we attempt to compute
</bodyText>
<page confidence="0.994745">
638
</page>
<bodyText confidence="0.9869018">
a set of dependency graph alignment scores based
on previous work in automatic short-answer grading
(Mohler et al., 2011). This score, computed in two
stages, is used as a feature by our meta-system.
In the first stage, the system is provided with the
dependency graphs for each pair of sentences4. For
each node in one dependency graph, we compute a
similarity score for each node in the other depen-
dency graph based upon a set of lexical, semantic,
and syntactic features applied to both the pair of
nodes and their corresponding subgraphs (i.e. the set
of nodes reachable from a given node by following
directional governor-to-dependant links). The scor-
ing function is trained on a small set of manually
aligned graphs using the averaged perceptron algo-
rithm.
We define a total of 64 features5 to be used to train
a machine learning system to compute subgraph-
subgraph similarity. Of these, 32 are based upon the
bag-of-words semantic similarity of the subgraphs
using the metrics described in Section 3.3.1 as well
as a Wikipedia-trained LSA model. The remaining
32 features are lexico-syntactic features associated
with the parent nodes of the subgraphs and are de-
scribed in more detail in our earlier paper.
We then calculate weights associated with these
features using an averaged version of the percep-
tron algorithm (Freund and Schapire, 1999; Collins,
2002) trained on a set of 32 manually annotated
instructor/student answer pairs selected from the
short-answer grading corpus (MM2011). These
pairs contain 7303 node pairs (656 matches, 6647
non-matches). Once the weights are calculated, a
similarity score for each pair of nodes can be com-
puted by taking the dot product of the feature vector
with the weights.
In the second stage, the node similarity scores cal-
culated in the previous step are used to find an op-
timal alignment for the pair of dependency graphs.
We begin with a bipartite graph where each node
in one graph is represented by a node on the left
side of the bipartite graph and each node in the other
4We here use the output of the Stanford Dependency Parser
in collapse/propagate mode with some modifications as de-
scribed in our earlier work.
5With the exception of the four features based upon the Hirst
&amp; St.Onge similarity metric, these are equivalent to the features
used in previous work.
graph is represented by a node on the right side. The
weight associated with each edge is the score com-
puted for each node-node pair in the previous stage.
The bipartite graph is then augmented by adding
dummy nodes to both sides which are allowed to
match any node with a score of zero. An optimal
alignment between the two graphs is then computed
efficiently using the Hungarian algorithm. Note that
this results in an optimal matching, not a mapping,
so that an individual node is associated with at most
one node in the other answer. After finding the opti-
mal match, we produce four alignment-based scores
by optionally normalizing by the number of nodes
and/or weighting the node-alignments according to
the idf scores of the words.6 This results in four
alignment scores listed as graphnone, graphnorm,
graphidf, graphidfnorm.
</bodyText>
<subsectionHeader confidence="0.786271">
3.3.4 Baselines
</subsectionHeader>
<bodyText confidence="0.999916444444444">
As a baseline, we also utilize several lexical bag-
of-words approaches where each sentence is repre-
sented by a vector of tokens and the similarity of the
two sentences can be computed by finding the co-
sine of the angle between their representative vectors
using term frequency (tf) or term frequency mul-
tiplied by inverse document frequency (tf.idf)6, or
by using simple overlap between the vectors’ dimen-
sions (overlap).
</bodyText>
<subsectionHeader confidence="0.8647545">
3.4 Machine Learning
3.4.1 Algorithms
</subsectionHeader>
<bodyText confidence="0.999947923076923">
All the systems described above are used to gen-
erate a score for each training and test sample (see
Section 3.1). These scores are then aggregated per
sample, and used in a supervised learning frame-
work. We decided to use a regression model, instead
of classification, since the requirements for the task
specify that we should provide a score in the range of
0 to 5. We could have used classification paired with
bucketed ranges, yet classification does not take into
consideration the underlying ordinality of the scores
(i.e. a score of 4.5 is closer to either 4 or 5, but
farther away from 0), which is a noticeable handi-
cap in this scenario. We tried both linear and sup-
</bodyText>
<footnote confidence="0.995971">
6The document frequency scores were taken from the British
National Corpus (BNC).
</footnote>
<page confidence="0.99875">
639
</page>
<bodyText confidence="0.9999558">
port vector regression7 by performing 10 fold cross-
validation on the train data, yet the latter algorithm
consistently performs better, no matter what kernel
was chosen. Thus we decided to use support vec-
tor regression (Smola and Schoelkopf, 1998) with a
Pearson VII function-based kernel.
Due to its different learning methodology, and
since it is suited for predicting continuous classes,
our second system uses the M5P decision tree al-
gorithm (Quinlan, 1992; Wang and Witten, 1997),
which outperforms support vector regression on the
10 fold cross-validation performed on the SMTeu-
roparl train set, while providing competitive results
on the other train sets (within .01 Pearson correla-
tion).
</bodyText>
<subsectionHeader confidence="0.803615">
3.4.2 Setup
</subsectionHeader>
<bodyText confidence="0.9993065">
We submitted three system variations, namely
IndividualRegression, IndividualDecTree,
and CombinedRegression. The first word de-
scribes the training data; for individual, for the
known test sets we trained on the corresponding
train sets, while for the unknown test sets we trained
on all the train sets combined; for combined,
for each test set we trained on all the train sets
combined. The second word refers to the learning
methodology, where Regression stands for support
vector regression, and DecTree stands for M5P
decision tree.
</bodyText>
<sectionHeader confidence="0.999185" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999478214285714">
We include in Table 1 the Pearson correlations ob-
tained by comparing the predictions of each fea-
ture to the gold standard for the three train datasets.
We notice that the corpus based metrics display a
consistent performance across the three train sets,
when compared to the other methods, including
knowledge-based. Furthermore, the best alignment
strategy (align) for corpus based models outper-
forms similarity scores based on traditional cosine
similarity. It is interesting to note that simple base-
lines such as tf, tf.idf and overlap offer signifi-
cant correlations with all the train sets without ac-
cess to additional knowledge inferred by knowledge
or corpus-based methods. In the case of the bipar-
</bodyText>
<footnote confidence="0.9632025">
7Implementations provided through the Weka framework
(Hall et al., 2009).
</footnote>
<table confidence="0.999964857142857">
System MSRpar MSRvid SMTeuroparl
Path 0.49 0.62 0.50
LCH 0.48 0.49 0.45
Lesk 0.48 0.59 0.50
WUP 0.46 0.38 0.42
RES 0.47 0.55 0.48
Lin 0.49 0.54 0.48
JCN 0.49 0.63 0.51
LSAalign 0.44 0.57 0.61
LSAcos 0.37 0.74 0.56
ESAalign 0.52 0.70 0.62
ESAcos 0.30 0.71 0.53
SSAalign 0.46 0.61 0.65
SSAcos 0.22 0.63 0.39
graphnone 0.42 0.50 0.21
graphnorm 0.48 0.43 0.59
graphidf 0.16 0.67 0.16
graphidfnorm 0.08 0.60 0.19
tf.idf 0.45 0.63 0.41
tf 0.45 0.69 0.51
overlap 0.44 0.69 0.27
</table>
<tableCaption confidence="0.975923">
Table 1: Correlation of individual features for the training
sets with the gold standards
</tableCaption>
<bodyText confidence="0.999949333333333">
tite graph matching, the graphnorm variation pro-
vides the strongest correlation results across all the
datasets.
We include the evaluation results provided by the
task organizers in Table 2. They indicate that our in-
tuition in using a support vector regression strategy
was correct. While the IndividualRegression was
our strongest system on the training data, the same
ranking applies to the test data (including the addi-
tional two surprise datasets) as well, earning it the
fifth place among the 89 participating systems, with
a Pearson correlation of 0.7846.
Regarding the decision tree based learning
(IndividualDecTree), despite its more robust be-
havior on the train sets, it achieved slightly lower
outcome on the test data, at 0.7677 correlation. We
believe this happened because decision trees have a
tendency to overfit training data, as they generate a
rigid structure which is unforgiving to minor devia-
tions in the test data. Nonetheless, this second vari-
ation still ranks in the top 10% of the submitted sys-
tems.
As an alternative approach to handle unknown test
data (e.g. different distributions, genres), we opted
</bodyText>
<page confidence="0.993691">
640
</page>
<table confidence="0.99627825">
Run ALL Rank Mean RankMean MSRpar MSRvid SMTeuroparl OnWN SMTnews
IndividualRegression 0.7846 5 0.6162 13 0.5353 0.8750 0.4203 0.6715 0.4033
IndividualDecTree 0.7677 9 0.5947 25 0.5693 0.8688 0.4203 0.6491 0.2256
CombinedRegression 0.7418 14 0.6159 14 0.5032 0.8695 0.4797 0.6715 0.4033
</table>
<tableCaption confidence="0.999504">
Table 2: Evaluation results and ranking published by the task organizers
</tableCaption>
<bodyText confidence="0.999923833333333">
to also include the CombinedRegression strategy
as our third variation. This seems to have been fruit-
ful for MSRvid, SMTeuroparl, and the two sur-
prise datasets (ONWn and SMTnews). In the
case of SMTeuroparl, this expanded training set
achieves a better performance than learning from
the corresponding training set alone, gaining an im-
provement of 0.0776 correlation points. Unfortu-
nately, the variation has some losses, particularly for
the MSRpar dataset (0.0321), yet it is able to con-
sistently model and handle a wider variety of text
types.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999997208333333">
This paper describes the three system variations our
team participated with in the Semantic Text Similar-
ity task in SEMEVAL 2012. Our focus has been to
produce a synergistic approach, striving to achieve a
superior result than attainable by each system indi-
vidually. We have considered a variety of methods
for inferring semantic similarity, including knowl-
edge and corpus-based methods. These were lever-
aged in a machine-learning framework, where our
preferred learning algorithm is support vector re-
gression, due to its ability to deal with continuous
classes and to dampen the effect of noisy features,
while augmenting more robust ones. While it is al-
ways preferable to use similar test and train sets,
when information regarding the test dataset is un-
available, we show that a robust performance can
be achieved by combining all train data from dif-
ferent sources into a single set and allowing a ma-
chine learner to make predictions. Overall, it was
interesting to note that corpus-based methods main-
tain strong results on all train datasets in compari-
son to knowledge-based methods. Our three systems
ranked number 5, 9 and 14 among the 89 systems
participating in the task.
</bodyText>
<sectionHeader confidence="0.995489" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999801714285714">
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS award #1018613.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.99895" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999573242424242">
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211–257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22–29.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-02), Philadelphia, PA,
July.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277–296.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1606–1611, Hyderabad, India.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue, xx(xx).
</reference>
<page confidence="0.991076">
641
</page>
<reference confidence="0.997577896226415">
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the Fifth Conference
on Language Resources and Evaluation, volume 2,
Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227–236. John Benjamins,
Amsterdam &amp; Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, D. Laham, B. Rehder, and M. E.
Schreiner. 1997. How well can passage meaning be
derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305–332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ’86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24–26, New York, NY,
USA. ACM.
C. Y. Lin and E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 296–
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775–780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
english. Communications of the Association for Com-
puting Machinery, 38(11):39–41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics – Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311–318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024–1025.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343–348, Singapore. World Scientific.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448–453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M.E. Lesk, 1971. The SMART Retrieval
System: Experiments in Automatic Document Process-
ing, chapter Computer evaluation of indexing and text
processing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97–124.
A. J. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning,
pages 491–502, Freiburg, Germany.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133—-138, Las Cruces, New Mexico.
</reference>
<page confidence="0.997944">
642
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.707641">
<title confidence="0.9540395">UNT: A Supervised Synergistic to Semantic Text Similarity</title>
<author confidence="0.99783">Carmen Banea</author>
<author confidence="0.99783">Samer Hassan</author>
<author confidence="0.99783">Michael Mohler</author>
<author confidence="0.99783">Rada</author>
<affiliation confidence="0.8986635">University of North Denton, TX,</affiliation>
<email confidence="0.999655">rada@cs.unt.edu</email>
<abstract confidence="0.998567333333333">This paper presents the systems that we participated with in the Semantic Text Similartask at Based on prior research in semantic similarity and relatedness, we combine various methods in a machine learning framework. The three variations submitted during the task evaluation period ranked number 5, 9 and 14 among the 89 participating systems. Our evaluations show that corpus-based methods display a more robust behavior on the training data, yet combining a variety of methods allows a learning algorithm to achieve a superior decision than that achievable by any of the individual parts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Cer</author>
<author>M Diab</author>
<author>A Gonzalez</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="3707" citStr="Agirre et al., 2012" startWordPosition="560" endWordPosition="563">or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), or Salient Semantic Analysis (Hassan and Mihalcea, 2011). In this paper, we describe the system with which 635 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635–642, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics we participated in the SEMEVAL 2012 task on semantic text similarity (Agirre et al., 2012). The system builds upon our earlier work on corpus-based and knowledge-based methods of text semantic similarity (Mihalcea et al., 2006; Hassan and Mihalcea, 2011; Mohler et al., 2011), and combines all these previous methods into a meta-system by using machine learning. The framework provided by the task organizers also enabled us to perform an indepth analysis of the various components used in our system, and draw conclusions concerning the role played by the different resources, features, and algorithms in building a state-of-the-art semantic text similarity system. 2 Related Work Over the</context>
<context position="7054" citStr="Agirre et al., 2012" startWordPosition="1077" endWordPosition="1080">ing from 0 (no relation) to 5 (semantically equivalent). The datasets1 provided are taken from the Microsoft Research Paraphrase Corpus (M5Rpar), the Microsoft Research Video Description Corpus (M5Rvid), and the WMT2008 development dataset (Europarl section)(5MTeuroparl); they each consist of about 750 sentence pairs with the class distribution varying with each dataset. The testing data contains additional sentences from the same collections as the training data as well as from two additional unknown sets (OnWN and 5MTnews); they range from 399 to 750 sentence pairs. The reader may refer to (Agirre et al., 2012) for additional information regarding this task. 3.2 Resources Wikipedia2 is a free on-line encyclopedia, representing the outcome of a continuous collaborative effort of a large number of volunteer contributors. Virtually any Internet user can create or edit a Wikipedia web page, and this “freedom of contribution” has a positive impact on both the quantity (fast-growing number of articles) and the quality (potential mistakes are quickly corrected within the collaborative environment) of this on-line resource. The basic entry in Wikipedia is an article which describes an entity or an event, an</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez, 2012</marker>
<rawString>E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burgess</author>
<author>K Livesay</author>
<author>K Lund</author>
</authors>
<title>Explorations in context space: words, sentences, discourse.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="5200" citStr="Burgess et al., 1998" startWordPosition="789" endWordPosition="792">995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 3 Semantic Textual Similarity System The system we proposed for the SEMEVAL 2012 Semantic Textual Similarity task builds upon b</context>
</contexts>
<marker>Burgess, Livesay, Lund, 1998</marker>
<rawString>C. Burgess, K. Livesay, and K. Lund. 1998. Explorations in context space: words, sentences, discourse. Discourse Processes, 25(2):211–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="5077" citStr="Church and Hanks, 1990" startWordPosition="770" endWordPosition="773">wledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 3 Se</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-02),</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="17133" citStr="Collins, 2002" startWordPosition="2693" endWordPosition="2694">algorithm. We define a total of 64 features5 to be used to train a machine learning system to compute subgraphsubgraph similarity. Of these, 32 are based upon the bag-of-words semantic similarity of the subgraphs using the metrics described in Section 3.3.1 as well as a Wikipedia-trained LSA model. The remaining 32 features are lexico-syntactic features associated with the parent nodes of the subgraphs and are described in more detail in our earlier paper. We then calculate weights associated with these features using an averaged version of the perceptron algorithm (Freund and Schapire, 1999; Collins, 2002) trained on a set of 32 manually annotated instructor/student answer pairs selected from the short-answer grading corpus (MM2011). These pairs contain 7303 node pairs (656 matches, 6647 non-matches). Once the weights are calculated, a similarity score for each pair of nodes can be computed by taking the dot product of the feature vector with the weights. In the second stage, the node similarity scores calculated in the previous step are used to find an optimal alignment for the pair of dependency graphs. We begin with a bipartite graph where each node in one graph is represented by a node on t</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-02), Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--277</pages>
<contexts>
<context position="17117" citStr="Freund and Schapire, 1999" startWordPosition="2689" endWordPosition="2692">ng the averaged perceptron algorithm. We define a total of 64 features5 to be used to train a machine learning system to compute subgraphsubgraph similarity. Of these, 32 are based upon the bag-of-words semantic similarity of the subgraphs using the metrics described in Section 3.3.1 as well as a Wikipedia-trained LSA model. The remaining 32 features are lexico-syntactic features associated with the parent nodes of the subgraphs and are described in more detail in our earlier paper. We then calculate weights associated with these features using an averaged version of the perceptron algorithm (Freund and Schapire, 1999; Collins, 2002) trained on a set of 32 manually annotated instructor/student answer pairs selected from the short-answer grading corpus (MM2011). These pairs contain 7303 node pairs (656 matches, 6647 non-matches). Once the weights are calculated, a similarity score for each pair of nodes can be computed by taking the dot product of the feature vector with the weights. In the second stage, the node similarity scores calculated in the previous step are used to find an optimal alignment for the pair of dependency graphs. We begin with a bipartite graph where each node in one graph is represente</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37:277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="3335" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="505" endWordPosition="508"> and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), or Salient Semantic Analysis (Hassan and Mihalcea, 2011). In this paper, we describe the system with which 635 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635–642, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics we participated in the SEMEVAL 2012 task on semantic text similarity (Agirre et al., 2012). The system builds upon our earlier work on corpus-based and knowledge-based methods of text semantic similarity (Mihalcea et al., 2006; Hassan and Mihalcea, 2011; Mohler et al., 2011), and combines all these previous methods i</context>
<context position="4955" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="753" endWordPosition="756">e research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpusbased. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able </context>
<context position="8226" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1251" endWordPosition="1254">pedia is an article which describes an entity or an event, and which, in addition to untagged 1http://www.cs.york.ac.uk/semeval-2012/ task6/data/uploads/datasets/train-readme. txt 2www.wikipedia.org 636 content, also consists of hyperlinked text to other pages within or outside of Wikipedia. These hyperlinks are meant to guide the reader to pages that provide additional information / clarifications, so that a better understanding of the primary concept can be achieved. The structure of Wikipedia in terms of pages and hyperlinks is exploited directly by semantic similarity methods such as ESA (Gabrilovich and Markovitch, 2007), or SSA (Hassan and Mihalcea, 2011). WordNet (Miller, 1995) is a manually crafted lexical resource that maintains semantic relationships between basic units of meaning, or synsets. A synset groups together senses of different words that share a very similar meaning, which act in a particular context as synonyms. Each synset is accompanied by a gloss or definition, and one or two examples illustrating usage in the given context. Unlike a traditional thesaurus, the structure of WordNet is able to encode additional relationships beside synonymy, such as antonymy, hypernymy, hyponymy, meronymy, e</context>
<context position="13282" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2073" endWordPosition="2076">links. Latent Semantic Analysis (LSA) (Landauer et al., 1997). In LSA, term-context associations are captured by means of a dimensionality reduction operated by a singular value decomposition (SVD) on the term-by-context matrix T, where the matrix is induced from a large corpus. This reduction entails the abstraction of meaning by collapsing similar contexts and discounting noisy and irrelevant ones, hence transforming the real world term-context space into a word-latent-concept space which achieves a much deeper and concrete semantic representation of words. Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). ESA uses encyclopedic knowledge in an information retrieval framework to generate a semantic interpretation of words. Since encyclopedic knowledge is typically organized into concepts (or topics), each concept is further described using definitions and examples. ESA relies on the distribution of words inside the encyclopedic descriptions. It builds semantic representations for a given word using a word-document association, where the document represents a Wikipedia article (concept). ESA is in effect a Vector Space Model (VSM) built using Wikipedia corpus, where vectors represents word-artic</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1606–1611, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="22205" citStr="Hall et al., 2009" startWordPosition="3520" endWordPosition="3523">ice that the corpus based metrics display a consistent performance across the three train sets, when compared to the other methods, including knowledge-based. Furthermore, the best alignment strategy (align) for corpus based models outperforms similarity scores based on traditional cosine similarity. It is interesting to note that simple baselines such as tf, tf.idf and overlap offer significant correlations with all the train sets without access to additional knowledge inferred by knowledge or corpus-based methods. In the case of the bipar7Implementations provided through the Weka framework (Hall et al., 2009). System MSRpar MSRvid SMTeuroparl Path 0.49 0.62 0.50 LCH 0.48 0.49 0.45 Lesk 0.48 0.59 0.50 WUP 0.46 0.38 0.42 RES 0.47 0.55 0.48 Lin 0.49 0.54 0.48 JCN 0.49 0.63 0.51 LSAalign 0.44 0.57 0.61 LSAcos 0.37 0.74 0.56 ESAalign 0.52 0.70 0.62 ESAcos 0.30 0.71 0.53 SSAalign 0.46 0.61 0.65 SSAcos 0.22 0.63 0.39 graphnone 0.42 0.50 0.21 graphnorm 0.48 0.43 0.59 graphidf 0.16 0.67 0.16 graphidfnorm 0.08 0.60 0.19 tf.idf 0.45 0.63 0.41 tf 0.45 0.69 0.51 overlap 0.44 0.69 0.27 Table 1: Correlation of individual features for the training sets with the gold standards tite graph matching, the graphnorm va</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hassan</author>
<author>R Mihalcea</author>
</authors>
<title>Measuring semantic relatedness using salient encyclopedic concepts.</title>
<date>2011</date>
<journal>Artificial Intelligence, Special Issue, xx(xx).</journal>
<contexts>
<context position="3063" citStr="Hassan and Mihalcea, 2011" startWordPosition="468" endWordPosition="471">ity metrics will fail in identifying any kind of connection between these texts. More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), or Salient Semantic Analysis (Hassan and Mihalcea, 2011). In this paper, we describe the system with which 635 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635–642, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics we participated in the SEMEVAL 2012 task on se</context>
<context position="5016" citStr="Hassan and Mihalcea, 2011" startWordPosition="761" endWordPosition="764">sing methods that are either knowledge-based or corpusbased. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long a</context>
<context position="8262" citStr="Hassan and Mihalcea, 2011" startWordPosition="1257" endWordPosition="1260">ty or an event, and which, in addition to untagged 1http://www.cs.york.ac.uk/semeval-2012/ task6/data/uploads/datasets/train-readme. txt 2www.wikipedia.org 636 content, also consists of hyperlinked text to other pages within or outside of Wikipedia. These hyperlinks are meant to guide the reader to pages that provide additional information / clarifications, so that a better understanding of the primary concept can be achieved. The structure of Wikipedia in terms of pages and hyperlinks is exploited directly by semantic similarity methods such as ESA (Gabrilovich and Markovitch, 2007), or SSA (Hassan and Mihalcea, 2011). WordNet (Miller, 1995) is a manually crafted lexical resource that maintains semantic relationships between basic units of meaning, or synsets. A synset groups together senses of different words that share a very similar meaning, which act in a particular context as synonyms. Each synset is accompanied by a gloss or definition, and one or two examples illustrating usage in the given context. Unlike a traditional thesaurus, the structure of WordNet is able to encode additional relationships beside synonymy, such as antonymy, hypernymy, hyponymy, meronymy, entailment, etc., which various knowl</context>
<context position="13958" citStr="Hassan and Mihalcea, 2011" startWordPosition="2166" endWordPosition="2170">n retrieval framework to generate a semantic interpretation of words. Since encyclopedic knowledge is typically organized into concepts (or topics), each concept is further described using definitions and examples. ESA relies on the distribution of words inside the encyclopedic descriptions. It builds semantic representations for a given word using a word-document association, where the document represents a Wikipedia article (concept). ESA is in effect a Vector Space Model (VSM) built using Wikipedia corpus, where vectors represents word-articles association. Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011). SSA incorporates a similar semantic abstraction and interpretation of words as ESA, yet it uses salient concepts gathered from encyclopedic knowledge, where a “concept” represents an unambiguous word or phrase with a concrete meaning, and which affords an encyclopedic definition. Saliency in this case is determined based on the word being hyperlinked (either trough manual or automatic annotations) in context, implying that they are highly relevant to the given text. SSA is an example of Generalized Vector Space Model (GVSM), where vectors represent word-concepts associations. In order to det</context>
</contexts>
<marker>Hassan, Mihalcea, 2011</marker>
<rawString>S. Hassan and R. Mihalcea. 2011. Measuring semantic relatedness using salient encyclopedic concepts. Artificial Intelligence, Special Issue, xx(xx).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Islam</author>
<author>D Inkpen</author>
</authors>
<title>Second order cooccurrence PMI for determining the semantic similarity of words.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth Conference on Language Resources and Evaluation,</booktitle>
<volume>2</volume>
<location>Genoa, Italy,</location>
<contexts>
<context position="5143" citStr="Islam and Inkpen, 2006" startWordPosition="780" endWordPosition="783">lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 3 Semantic Textual Similarity System The system we proposed for the SE</context>
</contexts>
<marker>Islam, Inkpen, 2006</marker>
<rawString>A. Islam and D. Inkpen. 2006. Second order cooccurrence PMI for determining the semantic similarity of words. In Proceedings of the Fifth Conference on Language Resources and Evaluation, volume 2, Genoa, Italy, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Islam</author>
<author>D Inkpen</author>
</authors>
<title>Semantic Similarity of Short Texts.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing V,</booktitle>
<volume>309</volume>
<pages>227--236</pages>
<editor>In Nicolas Nicolov, Galia Angelova, and Ruslan Mitkov, editors,</editor>
<location>Amsterdam &amp; Philadelphia.</location>
<contexts>
<context position="3036" citStr="Islam and Inkpen, 2009" startWordPosition="464" endWordPosition="467">the current text similarity metrics will fail in identifying any kind of connection between these texts. More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), or Salient Semantic Analysis (Hassan and Mihalcea, 2011). In this paper, we describe the system with which 635 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635–642, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics we participated in </context>
</contexts>
<marker>Islam, Inkpen, 2009</marker>
<rawString>A. Islam and D. Inkpen. 2009. Semantic Similarity of Short Texts. In Nicolas Nicolov, Galia Angelova, and Ruslan Mitkov, editors, Recent Advances in Natural Language Processing V, volume 309 of Current Issues in Linguistic Theory, pages 227–236. John Benjamins, Amsterdam &amp; Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In International Conference Research on Computational Linguistics (ROCLING X),</booktitle>
<pages>9008</pages>
<contexts>
<context position="2693" citStr="Jiang and Conrath, 1997" startWordPosition="413" endWordPosition="416">uence matching, as well as various weighting and normalization factors (Salton and Buckley, 1997). While successful to a certain degree, these lexical similarity methods cannot always identify the semantic similarity of texts. For instance, there is an obvious similarity between the text segments I own a dog and I have an animal, but most of the current text similarity metrics will fail in identifying any kind of connection between these texts. More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic A</context>
<context position="11637" citStr="Jiang and Conrath, 1997" startWordPosition="1810" endWordPosition="1813"> two concepts: Sim,,, = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus. The measure introduced by Lin (Lin, 1998) (Lin) builds on Resnik’s measure of similarity, and adds a normalization factor consisting of the information content of the two input concepts: Simlzn 2 * IC(LCS) (6) = IC(concept1) + IC(concept2) 3We point out that the similarity metric proposed by Hirst &amp; St. Onge was not considered due to the time constraints associated with the STS task. 637 We also consider the Jiang &amp; Conrath (Jiang and Conrath, 1997) (JCN) measure of similarity: 1 Simi&amp;quot; = IC(concept1) + IC(concept2) − 2 * IC(LCS) (7) Each of the measures listed above is used as a feature by our meta-system. 3.3.2 Corpus-based Semantic Similarity Features While most of the corpus-based methods induce semantic profiles in a word-space, where the semantic profile of a word is expressed in terms of its cooccurrence with other words, LSA, ESA and SSA stand out as different, since they rely on a conceptspace representation. In these methods, the semantic profile of a word is expressed in terms of the implicit (LSA), explicit (ESA), or salient (</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. J. Jiang and D. W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In International Conference Research on Computational Linguistics (ROCLING X), pages 9008+, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>D Laham</author>
<author>B Rehder</author>
<author>M E Schreiner</author>
</authors>
<title>How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans.</title>
<date>1997</date>
<contexts>
<context position="3272" citStr="Landauer et al., 1997" startWordPosition="498" endWordPosition="501">ty measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), or Salient Semantic Analysis (Hassan and Mihalcea, 2011). In this paper, we describe the system with which 635 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635–642, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics we participated in the SEMEVAL 2012 task on semantic text similarity (Agirre et al., 2012). The system builds upon our earlier work on corpus-based and knowledge-based methods of text semantic similarity (Mihalcea et al., 2006; Hassan and Mihalcea, 2011; </context>
<context position="4886" citStr="Landauer et al., 1997" startWordPosition="745" endWordPosition="748"> similarity system. 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpusbased. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods</context>
<context position="12710" citStr="Landauer et al., 1997" startWordPosition="1986" endWordPosition="1989">nceptspace representation. In these methods, the semantic profile of a word is expressed in terms of the implicit (LSA), explicit (ESA), or salient (SSA) concepts. This departure from the sparse word-space to a denser, richer, and unambiguous concept-space resolves one of the fundamental problems in semantic relatedness, namely the vocabulary mismatch. In the experiments reported in this paper, all the corpusbased methods are trained on the English Wikipedia download from October 2008, with approximately 6 million articles, and more than 9.5 million hyperlinks. Latent Semantic Analysis (LSA) (Landauer et al., 1997). In LSA, term-context associations are captured by means of a dimensionality reduction operated by a singular value decomposition (SVD) on the term-by-context matrix T, where the matrix is induced from a large corpus. This reduction entails the abstraction of meaning by collapsing similar contexts and discounting noisy and irrelevant ones, hence transforming the real world term-context space into a word-latent-concept space which achieves a much deeper and concrete semantic representation of words. Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). ESA uses encyclopedic know</context>
</contexts>
<marker>Landauer, Laham, Rehder, Schreiner, 1997</marker>
<rawString>T. K. Landauer, D. Laham, B. Rehder, and M. E. Schreiner. 1997. How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>R Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence,</booktitle>
<location>Edinburgh.</location>
<contexts>
<context position="1761" citStr="Lapata and Barzilay, 2005" startWordPosition="272" endWordPosition="275">elevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). Earlier work on this task has primarily focused on simple lexical matching methods, which produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have considered stemming, stop-word removal, part-of-speech tagging, longest subsequence matching, as well as various weighting and normalization factors (Salton and Buckley, 1997). While successful to a certain degree, these lexical similarity methods cannot always identify the semantic similarity of texts. For instance, there is an obvious similarity between the text se</context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>M. Lapata and R. Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification,</title>
<date>1998</date>
<pages>305--332</pages>
<contexts>
<context position="2721" citStr="Leacock and Chodorow, 1998" startWordPosition="417" endWordPosition="420">s various weighting and normalization factors (Salton and Buckley, 1997). While successful to a certain degree, these lexical similarity methods cannot always identify the semantic similarity of texts. For instance, there is an obvious similarity between the text segments I own a dog and I have an animal, but most of the current text similarity metrics will fail in identifying any kind of connection between these texts. More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Mar</context>
<context position="10165" citStr="Leacock and Chodorow, 1998" startWordPosition="1552" endWordPosition="1555"> similarity (using the WordNet::Similarity package (Pedersen et al., 2004)) that can be obtained by pairing it with any open-class word in the other input text. All the word-to-word similarity scores obtained in this way are summed and normalized to the length of the two input texts. We provide below a short description for each of the similarity metrics employed by this system3. The shortest path (Path) similarity is determined as: 1 Simpath = (1) length where length is the length of the shortest path between two concepts using node-counting (including the end nodes). The Leacock &amp; Chodorow (Leacock and Chodorow, 1998) (LCH) similarity is determined as: length Simlch = − log (2) 2 � � where length is the length of the shortest path between two concepts using node-counting, and D is the maximum depth of the taxonomy. The Lesk (Lesk) similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu &amp; Palmer (Wu and Palmer, 1994) (W UP) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow, 1998. Combining local context and WordNet similarity for word sense identification, pages 305–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In SIGDOC ’86: Proceedings of the 5th annual international conference on Systems documentation,</booktitle>
<pages>24--26</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1430" citStr="Lesk, 1986" startWordPosition="223" endWordPosition="224">e by any of the individual parts. 1 Introduction Measures of text similarity have been used for a long time in applications in natural language processing and related areas. One of the earliest applications of text similarity is perhaps the vectorspace model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). Earlier work on this task has primarily focused on simple lexical matching methods, which produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have considered stemming, stop-word removal</context>
<context position="10569" citStr="Lesk (1986)" startWordPosition="1628" endWordPosition="1629">s determined as: 1 Simpath = (1) length where length is the length of the shortest path between two concepts using node-counting (including the end nodes). The Leacock &amp; Chodorow (Leacock and Chodorow, 1998) (LCH) similarity is determined as: length Simlch = − log (2) 2 � � where length is the length of the shortest path between two concepts using node-counting, and D is the maximum depth of the taxonomy. The Lesk (Lesk) similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu &amp; Palmer (Wu and Palmer, 1994) (W UP) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: 2 * depth(LCS) Sim,,up = depth(concept1) + depth(concept2) (3) The measure introduced by Resnik (Resnik, 1995) (RES) returns the information content (IC) of the LCS of two concepts: Sim,,, = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In SIGDOC ’86: Proceedings of the 5th annual international conference on Systems documentation, pages 24–26, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology Conference (HLT-NAACL 2003),</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1644" citStr="Lin and Hovy, 2003" startWordPosition="253" endWordPosition="256">s of text similarity is perhaps the vectorspace model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). Earlier work on this task has primarily focused on simple lexical matching methods, which produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have considered stemming, stop-word removal, part-of-speech tagging, longest subsequence matching, as well as various weighting and normalization factors (Salton and Buckley, 1997). While successful to a certain degree, these lexical similarity methods cann</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C. Y. Lin and E. H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of Human Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="2732" citStr="Lin, 1998" startWordPosition="421" endWordPosition="422">alization factors (Salton and Buckley, 1997). While successful to a certain degree, these lexical similarity methods cannot always identify the semantic similarity of texts. For instance, there is an obvious similarity between the text segments I own a dog and I have an animal, but most of the current text similarity metrics will fail in identifying any kind of connection between these texts. More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 20</context>
<context position="5242" citStr="Lin, 1998" startWordPosition="797" endWordPosition="798">ithin a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 3 Semantic Textual Similarity System The system we proposed for the SEMEVAL 2012 Semantic Textual Similarity task builds upon both knowledge- and corpus-based methods pr</context>
<context position="11225" citStr="Lin, 1998" startWordPosition="1742" endWordPosition="1743">The Wu &amp; Palmer (Wu and Palmer, 1994) (W UP) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: 2 * depth(LCS) Sim,,up = depth(concept1) + depth(concept2) (3) The measure introduced by Resnik (Resnik, 1995) (RES) returns the information content (IC) of the LCS of two concepts: Sim,,, = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus. The measure introduced by Lin (Lin, 1998) (Lin) builds on Resnik’s measure of similarity, and adds a normalization factor consisting of the information content of the two input concepts: Simlzn 2 * IC(LCS) (6) = IC(concept1) + IC(concept2) 3We point out that the similarity metric proposed by Hirst &amp; St. Onge was not considered due to the time constraints associated with the STS task. 637 We also consider the Jiang &amp; Conrath (Jiang and Conrath, 1997) (JCN) measure of similarity: 1 Simi&amp;quot; = IC(concept1) + IC(concept2) − 2 * IC(LCS) (7) Each of the measures listed above is used as a feature by our meta-system. 3.3.2 Corpus-based Semantic</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 296– 304, Madison, Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the American Association for Artificial Intelligence (AAAI</booktitle>
<pages>775--780</pages>
<location>Boston, MA, US.</location>
<contexts>
<context position="3012" citStr="Mihalcea et al., 2006" startWordPosition="460" endWordPosition="463">an animal, but most of the current text similarity metrics will fail in identifying any kind of connection between these texts. More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), or Salient Semantic Analysis (Hassan and Mihalcea, 2011). In this paper, we describe the system with which 635 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635–642, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguis</context>
<context position="5886" citStr="Mihalcea et al., 2006" startWordPosition="893" endWordPosition="896">pproaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 3 Semantic Textual Similarity System The system we proposed for the SEMEVAL 2012 Semantic Textual Similarity task builds upon both knowledge- and corpus-based methods previously described in (Mihalcea et al., 2006; Hassan and Mihalcea, 2011; Mohler et al., 2011). The predictions of these independent systems, paired with additional salient features, are leveraged by a meta-system that employs machine learning. In this section, we will elaborate further on the resources we use, our features, and the components of our machine learning system. We will start by describing the task setup. 3.1 Task Setup The training data released by the task organizers consists of three datasets showcasing two sentences per line and a manually assigned similarity score ranging from 0 (no relation) to 5 (semantically equivale</context>
<context position="9322" citStr="Mihalcea et al., 2006" startWordPosition="1417" endWordPosition="1420">ure of WordNet is able to encode additional relationships beside synonymy, such as antonymy, hypernymy, hyponymy, meronymy, entailment, etc., which various knowledge-based methods use to derive semantic similarity. 3.3 Features Our meta-system uses several features, which can be grouped into knowledge-based, corpus-based, and bipartite graph matching, as described below. The abbreviations appearing between parentheses by each method allow for easy cross-referencing with the evaluations provided in Table 1. 3.3.1 Knowledge-based Semantic Similarity Features Following prior work from our group (Mihalcea et al., 2006; Mohler and Mihalcea, 2009), we employ several WordNet-based similarity metrics for the task of sentence-level similarity. Briefly, for each open-class word in one of the input texts, we compute the maximum semantic similarity (using the WordNet::Similarity package (Pedersen et al., 2004)) that can be obtained by pairing it with any open-class word in the other input text. All the word-to-word similarity scores obtained in this way are summed and normalized to the length of the two input texts. We provide below a short description for each of the similarity metrics employed by this system3. T</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the American Association for Artificial Intelligence (AAAI 2006), pages 775–780, Boston, MA, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: a Lexical database for english.</title>
<date>1995</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="4583" citStr="Miller, 1995" startWordPosition="698" endWordPosition="699">learning. The framework provided by the task organizers also enabled us to perform an indepth analysis of the various components used in our system, and draw conclusions concerning the role played by the different resources, features, and algorithms in building a state-of-the-art semantic text similarity system. 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpusbased. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burg</context>
<context position="8286" citStr="Miller, 1995" startWordPosition="1262" endWordPosition="1263">n to untagged 1http://www.cs.york.ac.uk/semeval-2012/ task6/data/uploads/datasets/train-readme. txt 2www.wikipedia.org 636 content, also consists of hyperlinked text to other pages within or outside of Wikipedia. These hyperlinks are meant to guide the reader to pages that provide additional information / clarifications, so that a better understanding of the primary concept can be achieved. The structure of Wikipedia in terms of pages and hyperlinks is exploited directly by semantic similarity methods such as ESA (Gabrilovich and Markovitch, 2007), or SSA (Hassan and Mihalcea, 2011). WordNet (Miller, 1995) is a manually crafted lexical resource that maintains semantic relationships between basic units of meaning, or synsets. A synset groups together senses of different words that share a very similar meaning, which act in a particular context as synonyms. Each synset is accompanied by a gloss or definition, and one or two examples illustrating usage in the given context. Unlike a traditional thesaurus, the structure of WordNet is able to encode additional relationships beside synonymy, such as antonymy, hypernymy, hyponymy, meronymy, entailment, etc., which various knowledge-based methods use t</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller. 1995. WordNet: a Lexical database for english. Communications of the Association for Computing Machinery, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohler</author>
<author>R Mihalcea</author>
</authors>
<title>Text-to-text semantic similarity for automatic short answer grading.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Association for Computational Linguistics (EACL</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="9350" citStr="Mohler and Mihalcea, 2009" startWordPosition="1421" endWordPosition="1424">to encode additional relationships beside synonymy, such as antonymy, hypernymy, hyponymy, meronymy, entailment, etc., which various knowledge-based methods use to derive semantic similarity. 3.3 Features Our meta-system uses several features, which can be grouped into knowledge-based, corpus-based, and bipartite graph matching, as described below. The abbreviations appearing between parentheses by each method allow for easy cross-referencing with the evaluations provided in Table 1. 3.3.1 Knowledge-based Semantic Similarity Features Following prior work from our group (Mihalcea et al., 2006; Mohler and Mihalcea, 2009), we employ several WordNet-based similarity metrics for the task of sentence-level similarity. Briefly, for each open-class word in one of the input texts, we compute the maximum semantic similarity (using the WordNet::Similarity package (Pedersen et al., 2004)) that can be obtained by pairing it with any open-class word in the other input text. All the word-to-word similarity scores obtained in this way are summed and normalized to the length of the two input texts. We provide below a short description for each of the similarity metrics employed by this system3. The shortest path (Path) simi</context>
</contexts>
<marker>Mohler, Mihalcea, 2009</marker>
<rawString>M. Mohler and R. Mihalcea. 2009. Text-to-text semantic similarity for automatic short answer grading. In Proceedings of the European Association for Computational Linguistics (EACL 2009), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohler</author>
<author>R Bunescu</author>
<author>R Mihalcea</author>
</authors>
<title>Learning to grade short answer questions using semantic similarity measures and dependency graph alignments.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics – Human Language Technologies (ACLHLT 2011),</booktitle>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="3085" citStr="Mohler et al., 2011" startWordPosition="472" endWordPosition="475">entifying any kind of connection between these texts. More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), or Salient Semantic Analysis (Hassan and Mihalcea, 2011). In this paper, we describe the system with which 635 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635–642, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics we participated in the SEMEVAL 2012 task on semantic text similarity</context>
<context position="5935" citStr="Mohler et al., 2011" startWordPosition="902" endWordPosition="905">onsist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 3 Semantic Textual Similarity System The system we proposed for the SEMEVAL 2012 Semantic Textual Similarity task builds upon both knowledge- and corpus-based methods previously described in (Mihalcea et al., 2006; Hassan and Mihalcea, 2011; Mohler et al., 2011). The predictions of these independent systems, paired with additional salient features, are leveraged by a meta-system that employs machine learning. In this section, we will elaborate further on the resources we use, our features, and the components of our machine learning system. We will start by describing the task setup. 3.1 Task Setup The training data released by the task organizers consists of three datasets showcasing two sentences per line and a manually assigned similarity score ranging from 0 (no relation) to 5 (semantically equivalent). The datasets1 provided are taken from the Mi</context>
<context position="15888" citStr="Mohler et al., 2011" startWordPosition="2486" endWordPosition="2489">ther filter these possible combinations by creating a list co which holds the strongest semantic pairings between the fragments’ terms, such that each term can only belong to one and only one pair. |ϕ| Sim(Ta,b) = T (w + Ei=1 coi) x (tab) (8) a + b where w is the number of shared terms between the text fragments and coi is the similarity score for the ith pairing. 3.3.3 Bipartite Graph Matching In an attempt to move beyond the bag-of-words paradigm described thus far, we attempt to compute 638 a set of dependency graph alignment scores based on previous work in automatic short-answer grading (Mohler et al., 2011). This score, computed in two stages, is used as a feature by our meta-system. In the first stage, the system is provided with the dependency graphs for each pair of sentences4. For each node in one dependency graph, we compute a similarity score for each node in the other dependency graph based upon a set of lexical, semantic, and syntactic features applied to both the pair of nodes and their corresponding subgraphs (i.e. the set of nodes reachable from a given node by following directional governor-to-dependant links). The scoring function is trained on a small set of manually aligned graphs</context>
</contexts>
<marker>Mohler, Bunescu, Mihalcea, 2011</marker>
<rawString>M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learning to grade short answer questions using semantic similarity measures and dependency graph alignments. In Proceedings of the Association for Computational Linguistics – Human Language Technologies (ACLHLT 2011), Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1601" citStr="Papineni et al., 2002" startWordPosition="246" endWordPosition="249">related areas. One of the earliest applications of text similarity is perhaps the vectorspace model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). Earlier work on this task has primarily focused on simple lexical matching methods, which produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have considered stemming, stop-word removal, part-of-speech tagging, longest subsequence matching, as well as various weighting and normalization factors (Salton and Buckley, 1997). While successful to a certain de</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet:: Similarity-Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>1024--1025</pages>
<contexts>
<context position="9612" citStr="Pedersen et al., 2004" startWordPosition="1459" endWordPosition="1462">nto knowledge-based, corpus-based, and bipartite graph matching, as described below. The abbreviations appearing between parentheses by each method allow for easy cross-referencing with the evaluations provided in Table 1. 3.3.1 Knowledge-based Semantic Similarity Features Following prior work from our group (Mihalcea et al., 2006; Mohler and Mihalcea, 2009), we employ several WordNet-based similarity metrics for the task of sentence-level similarity. Briefly, for each open-class word in one of the input texts, we compute the maximum semantic similarity (using the WordNet::Similarity package (Pedersen et al., 2004)) that can be obtained by pairing it with any open-class word in the other input text. All the word-to-word similarity scores obtained in this way are summed and normalized to the length of the two input texts. We provide below a short description for each of the similarity metrics employed by this system3. The shortest path (Path) similarity is determined as: 1 Simpath = (1) length where length is the length of the shortest path between two concepts using node-counting (including the end nodes). The Leacock &amp; Chodorow (Leacock and Chodorow, 1998) (LCH) similarity is determined as: length Siml</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet:: Similarity-Measuring the Relatedness of Concepts. Proceedings of the National Conference on Artificial Intelligence, pages 1024–1025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Quinlan</author>
</authors>
<title>Learning with continuous classes.</title>
<date>1992</date>
<booktitle>In 5th Australian Joint Conference on Artificial Intelligence,</booktitle>
<pages>343--348</pages>
<publisher>World Scientific.</publisher>
<contexts>
<context position="20619" citStr="Quinlan, 1992" startWordPosition="3277" endWordPosition="3278"> noticeable handicap in this scenario. We tried both linear and sup6The document frequency scores were taken from the British National Corpus (BNC). 639 port vector regression7 by performing 10 fold crossvalidation on the train data, yet the latter algorithm consistently performs better, no matter what kernel was chosen. Thus we decided to use support vector regression (Smola and Schoelkopf, 1998) with a Pearson VII function-based kernel. Due to its different learning methodology, and since it is suited for predicting continuous classes, our second system uses the M5P decision tree algorithm (Quinlan, 1992; Wang and Witten, 1997), which outperforms support vector regression on the 10 fold cross-validation performed on the SMTeuroparl train set, while providing competitive results on the other train sets (within .01 Pearson correlation). 3.4.2 Setup We submitted three system variations, namely IndividualRegression, IndividualDecTree, and CombinedRegression. The first word describes the training data; for individual, for the known test sets we trained on the corresponding train sets, while for the unknown test sets we trained on all the train sets combined; for combined, for each test set we trai</context>
</contexts>
<marker>Quinlan, 1992</marker>
<rawString>R. J. Quinlan. 1992. Learning with continuous classes. In 5th Australian Joint Conference on Artificial Intelligence, pages 343–348, Singapore. World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy. In</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="2747" citStr="Resnik, 1995" startWordPosition="423" endWordPosition="424">actors (Salton and Buckley, 1997). While successful to a certain degree, these lexical similarity methods cannot always identify the semantic similarity of texts. For instance, there is an obvious similarity between the text segments I own a dog and I have an animal, but most of the current text similarity metrics will fail in identifying any kind of connection between these texts. More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), or Salient</context>
<context position="10956" citStr="Resnik, 1995" startWordPosition="1691" endWordPosition="1692"> depth of the taxonomy. The Lesk (Lesk) similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu &amp; Palmer (Wu and Palmer, 1994) (W UP) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: 2 * depth(LCS) Sim,,up = depth(concept1) + depth(concept2) (3) The measure introduced by Resnik (Resnik, 1995) (RES) returns the information content (IC) of the LCS of two concepts: Sim,,, = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus. The measure introduced by Lin (Lin, 1998) (Lin) builds on Resnik’s measure of similarity, and adds a normalization factor consisting of the information content of the two input concepts: Simlzn 2 * IC(LCS) (6) = IC(concept1) + IC(concept2) 3We point out that the similarity metric proposed by Hirst &amp; St. Onge was not considered due to the time constraints associated with</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevance feedback in information retrieval.</title>
<date>1971</date>
<publisher>Prentice Hall,</publisher>
<location>Ing. Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="1391" citStr="Rocchio, 1971" startWordPosition="218" endWordPosition="219">eve a superior decision than that achievable by any of the individual parts. 1 Introduction Measures of text similarity have been used for a long time in applications in natural language processing and related areas. One of the earliest applications of text similarity is perhaps the vectorspace model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). Earlier work on this task has primarily focused on simple lexical matching methods, which produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>J. Rocchio, 1971. Relevance feedback in information retrieval. Prentice Hall, Ing. Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1997</date>
<booktitle>In Readings in Information Retrieval.</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="2167" citStr="Salton and Buckley, 1997" startWordPosition="331" endWordPosition="334">c evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). Earlier work on this task has primarily focused on simple lexical matching methods, which produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have considered stemming, stop-word removal, part-of-speech tagging, longest subsequence matching, as well as various weighting and normalization factors (Salton and Buckley, 1997). While successful to a certain degree, these lexical similarity methods cannot always identify the semantic similarity of texts. For instance, there is an obvious similarity between the text segments I own a dog and I have an animal, but most of the current text similarity metrics will fail in identifying any kind of connection between these texts. More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic </context>
</contexts>
<marker>Salton, Buckley, 1997</marker>
<rawString>G. Salton and C. Buckley. 1997. Term weighting approaches in automatic text retrieval. In Readings in Information Retrieval. Morgan Kaufmann Publishers, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M E Lesk</author>
</authors>
<title>The SMART Retrieval System: Experiments in Automatic Document Processing, chapter Computer evaluation of indexing and text processing.</title>
<date>1971</date>
<publisher>Prentice Hall,</publisher>
<location>Ing. Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="1292" citStr="Salton and Lesk, 1971" startWordPosition="201" endWordPosition="204">obust behavior on the training data, yet combining a variety of methods allows a learning algorithm to achieve a superior decision than that achievable by any of the individual parts. 1 Introduction Measures of text similarity have been used for a long time in applications in natural language processing and related areas. One of the earliest applications of text similarity is perhaps the vectorspace model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). Earlier work on this task has primarily focused on simple lexical matching methods, which produce a similarity score based on the</context>
</contexts>
<marker>Salton, Lesk, 1971</marker>
<rawString>G. Salton and M.E. Lesk, 1971. The SMART Retrieval System: Experiments in Automatic Document Processing, chapter Computer evaluation of indexing and text processing. Prentice Hall, Ing. Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Singhal</author>
<author>M Mitra</author>
<author>C Buckley</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1997</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>2</volume>
<issue>32</issue>
<contexts>
<context position="1516" citStr="Salton et al., 1997" startWordPosition="233" endWordPosition="236"> have been used for a long time in applications in natural language processing and related areas. One of the earliest applications of text similarity is perhaps the vectorspace model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). Earlier work on this task has primarily focused on simple lexical matching methods, which produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have considered stemming, stop-word removal, part-of-speech tagging, longest subsequence matching, as well as various weighting a</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckley, 1997</marker>
<rawString>G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic text structuring and summarization. Information Processing and Management, 2(32).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="1446" citStr="Schutze, 1998" startWordPosition="225" endWordPosition="226">the individual parts. 1 Introduction Measures of text similarity have been used for a long time in applications in natural language processing and related areas. One of the earliest applications of text similarity is perhaps the vectorspace model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). Earlier work on this task has primarily focused on simple lexical matching methods, which produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have considered stemming, stop-word removal, part-of-speech</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>H. Schutze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola</author>
<author>B Schoelkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>1998</date>
<tech>NeuroCOLT2 Technical Report NC2-TR-1998-030.</tech>
<contexts>
<context position="20406" citStr="Smola and Schoelkopf, 1998" startWordPosition="3242" endWordPosition="3245"> used classification paired with bucketed ranges, yet classification does not take into consideration the underlying ordinality of the scores (i.e. a score of 4.5 is closer to either 4 or 5, but farther away from 0), which is a noticeable handicap in this scenario. We tried both linear and sup6The document frequency scores were taken from the British National Corpus (BNC). 639 port vector regression7 by performing 10 fold crossvalidation on the train data, yet the latter algorithm consistently performs better, no matter what kernel was chosen. Thus we decided to use support vector regression (Smola and Schoelkopf, 1998) with a Pearson VII function-based kernel. Due to its different learning methodology, and since it is suited for predicting continuous classes, our second system uses the M5P decision tree algorithm (Quinlan, 1992; Wang and Witten, 1997), which outperforms support vector regression on the 10 fold cross-validation performed on the SMTeuroparl train set, while providing competitive results on the other train sets (within .01 Pearson correlation). 3.4.2 Setup We submitted three system variations, namely IndividualRegression, IndividualDecTree, and CombinedRegression. The first word describes the </context>
</contexts>
<marker>Smola, Schoelkopf, 1998</marker>
<rawString>A. J. Smola and B. Schoelkopf. 1998. A tutorial on support vector regression. NeuroCOLT2 Technical Report NC2-TR-1998-030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="5100" citStr="Turney, 2001" startWordPosition="775" endWordPosition="776">sure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 3 Semantic Textual Similari</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>P. D. Turney. 2001. Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the 12th European Conference on Machine Learning, pages 491–502, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>I H Witten</author>
</authors>
<title>Induction of model trees for predicting continuous classes.</title>
<date>1997</date>
<booktitle>In Poster papers of the 9th European Conference on Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="20643" citStr="Wang and Witten, 1997" startWordPosition="3279" endWordPosition="3282">dicap in this scenario. We tried both linear and sup6The document frequency scores were taken from the British National Corpus (BNC). 639 port vector regression7 by performing 10 fold crossvalidation on the train data, yet the latter algorithm consistently performs better, no matter what kernel was chosen. Thus we decided to use support vector regression (Smola and Schoelkopf, 1998) with a Pearson VII function-based kernel. Due to its different learning methodology, and since it is suited for predicting continuous classes, our second system uses the M5P decision tree algorithm (Quinlan, 1992; Wang and Witten, 1997), which outperforms support vector regression on the 10 fold cross-validation performed on the SMTeuroparl train set, while providing competitive results on the other train sets (within .01 Pearson correlation). 3.4.2 Setup We submitted three system variations, namely IndividualRegression, IndividualDecTree, and CombinedRegression. The first word describes the training data; for individual, for the known test sets we trained on the corresponding train sets, while for the unknown test sets we trained on all the train sets combined; for combined, for each test set we trained on all the train set</context>
</contexts>
<marker>Wang, Witten, 1997</marker>
<rawString>Y. Wang and I. H. Witten. 1997. Induction of model trees for predicting continuous classes. In Poster papers of the 9th European Conference on Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="10652" citStr="Wu and Palmer, 1994" startWordPosition="1641" endWordPosition="1644">shortest path between two concepts using node-counting (including the end nodes). The Leacock &amp; Chodorow (Leacock and Chodorow, 1998) (LCH) similarity is determined as: length Simlch = − log (2) 2 � � where length is the length of the shortest path between two concepts using node-counting, and D is the maximum depth of the taxonomy. The Lesk (Lesk) similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu &amp; Palmer (Wu and Palmer, 1994) (W UP) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: 2 * depth(LCS) Sim,,up = depth(concept1) + depth(concept2) (3) The measure introduced by Resnik (Resnik, 1995) (RES) returns the information content (IC) of the LCS of two concepts: Sim,,, = IC(LCS) (4) where IC is defined as: IC(c) = − log P(c) (5) and P(c) is the probability of encountering an instance of concept c in a large corpus. The measure introduced by Lin (Lin, 1998) (Lin) builds on Resnik’s m</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z. Wu and M. Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133—-138, Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>