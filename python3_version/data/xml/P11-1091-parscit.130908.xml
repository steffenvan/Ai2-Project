<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000143">
<title confidence="0.999155">
A Graph Approach to Spelling Correction in Domain-Centric Search
</title>
<author confidence="0.997528">
Zhuowei Bao Benny Kimelfeld Yunyao Li
</author>
<affiliation confidence="0.8526265">
University of Pennsylvania IBM Research–Almaden IBM Research–Almaden
Philadelphia, PA 19104, USA San Jose, CA 95120, USA San Jose, CA 95120, USA
</affiliation>
<email confidence="0.999353">
zhuowei@cis.upenn.edu kimelfeld@us.ibm.com yunyaoli@us.ibm.com
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996930625">
Spelling correction for keyword-search
queries is challenging in restricted domains
such as personal email (or desktop) search,
due to the scarcity of query logs, and due to
the specialized nature of the domain. For that
task, this paper presents an algorithm that is
based on statistics from the corpus data (rather
than the query log). This algorithm, which
employs a simple graph-based approach, can
incorporate different types of data sources
with different levels of reliability (e.g., email
subject vs. email body), and can handle
complex spelling errors like splitting and
merging of words. An experimental study
shows the superiority of the algorithm over
existing alternatives in the email domain.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967826086957">
An abundance of applications require spelling cor-
rection, which (at the high level) is the following
task. The user intends to type a chunk q of text,
but types instead the chunk s that contains spelling
errors (which we discuss in detail later), due to un-
careful typing or lack of knowledge of the exact
spelling of q. The goal is to restore q, when given
s. Spelling correction has been extensively studied
in the literature, and we refer the reader to compre-
hensive summaries of prior work (Peterson, 1980;
Kukich, 1992; Jurafsky and Martin, 2000; Mitton,
2010). The focus of this paper is on the special case
where q is a search query, and where s instead of q
is submitted to a search engine (with the goal of re-
trieving documents that match the search query q).
Spelling correction for search queries is important,
because a significant portion of posed queries may
be misspelled (Cucerzan and Brill, 2004). Effective
spelling correction has a major effect on the expe-
rience and effort of the user, who is otherwise re-
quired to ensure the exact spellings of her queries.
Furthermore, it is critical when the exact spelling is
unknown (e.g., person names like Schwarzenegger).
</bodyText>
<subsectionHeader confidence="0.997855">
1.1 Spelling Errors
</subsectionHeader>
<bodyText confidence="0.999903454545454">
The more common and studied type of spelling error
is word-to-word error: a single word w is misspelled
into another single word w&apos;. The specific spelling er-
rors involved include omission of a character (e.g.,
atachment), inclusion of a redundant character
(e.g., attachement), and replacement of charac-
ters (e.g., attachemnt). The fact that w&apos; is a mis-
spelling of (and should be corrected to) w is denoted
by w&apos; → w (e.g., atachment → attachment).
Additional common spelling errors are splitting of
a word, and merging two (or more) words:
</bodyText>
<listItem confidence="0.999629">
• attach ment → attachment
• emailattachment → email attachment
</listItem>
<bodyText confidence="0.9998543125">
Part of our experiments, as well as most of our
examples, are from the domain of (personal) email
search. An email from the Enron email collec-
tion (Klimt and Yang, 2004) is shown in Figure 1.
Our running example is the following misspelling of
a search query, involving multiple types of errors.
sadeep kohli excellatach ment →
sandeep kohli excel attachment (1)
In this example, correction entails fixing sadeep,
splitting excellatach, fixing excell, merging
atach ment, and fixing atachment. Beyond the
complexity of errors, this example also illustrates
other challenges in spelling correction for search.
We need to identify not only that sadeep is mis-
spelled, but also that kohli is correctly spelled.
Just having kohli in a dictionary is not enough.
</bodyText>
<page confidence="0.972923">
905
</page>
<note confidence="0.974994">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 905–914,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
Subject: Follow-Up on Captive Generation other search applications (e.g. email search, desktop
Irom: sandeep.kohli@enron.com search, and even small-enterprise search) query logs
X-Irom: Sandeep Kohli are too scarce to provide statistical information that
X-To: Stinson Gibner@ECT, Vince J Kaminski@ECT is sufficient for effective spelling correction. Even
</note>
<bodyText confidence="0.930875787878788">
an email provider of a massive scale (such as GMail)
may need to rely on the (possibly tiny) query log of
the single user at hand, due to privacy or security
concerns; moreover, as noted earlier about kohli,
the statistics of one user may be relevant to one user,
while irrelevant to another.
The focus of this paper is on spelling correction
for search applications like the above, where query-
log analysis is impossible or undesirable (with email
search being a prominent example). Our approach
relies mainly on the corpus data (e.g., the collection
of emails of the user at hand) and external, generic
dictionaries (e.g., English). As shown in Figure 1,
the corpus data may very well contain misspelled
words (like query logs do), and such noise is a part of
the challenge. Relying on the corpus has been shown
to be successful in spelling correction for text clean-
ing (Schierle et al., 2007). Nevertheless, as we later
explain, our approach can still incorporate query-log
data as features involved in the correction, as well as
means to refine the parameters.
1.3 Contribution and Outline
As said above, our goal is to devise spelling cor-
rection that relies on the corpus. The corpus often
contains various types of information, with different
levels of reliability (e.g., n-grams from email sub-
jects and sender information, vs. those from email
bodies). The major question is how to effectively
exploit that information while addressing the vari-
ous types of spelling errors such as those discussed
in Section 1.1. The key contribution of this work is
a novel graph-based algorithm, MaxPaths, that han-
dles the different types of errors and incorporates the
corpus data in a uniform (and simple) fashion. We
describe MaxPaths in Section 2. We evaluate the
effectiveness of our algorithm via an experimental
study in Section 3. Finally, we make concluding re-
marks and discuss future directions in Section 4.
2 Spelling-Correction Algorithm
In this section, we describe our algorithm for
spelling correction. Recall that given a search query
Vince/Stinson,
Please find below two attachemnts. The Excell spreadsheet
shows some calculations... The seond attachement (Word) has
the wordings that I think we can send in to the press...
I am availabel on mobile if you have questions o clarifications...
Regards,
Sandeep.
Figure 1: Enron email (misspelled words are underlined)
For example, in kohli coupons the user may very
well mean kohls coupons if Sandeep Kohli has
nothing to do with coupons (in contrast to the store
chain Kohl’s). A similar example is the word nail,
which is a legitimate English word, but in the con-
text of email the query nail box is likely to be
a misspelling of mail box (unless nail boxes are
indeed relevant to the user’s email collection). Fi-
nally, while the word kohli is relevant to some
email users (e.g., Kohli’s colleagues), it may have
no meaning at all to other users.
1.2 Domain Knowledge
The common approach to spelling correction uti-
lizes statistical information (Kernighan et al., 1990;
Schierle et al., 2007; Mitton, 2010). As a sim-
ple example, if we want to avoid maintaining a
manually-crafted dictionary to accommodate the
wealth of new terms introduced every day (e.g.,
ipod and ipad), we may decide that atachment
is a misspelling of attachment due to both the
(relative) proximity between the words, and the
fact that attachment is significantly more pop-
ular than atachment. As another example, the
fact that the expression sandeep kohli is fre-
quent in the domain increases our confidence in
sadeep kohli —* sandeep kohli (rather than,
e.g., sadeep kohli —* sudeep kohli). One
can further note that, in email search, the fact that
Sandeep Kohli sent multiple excel attachments in-
creases our confidence in excell —* excel.
A source of statistics widely used in prior work
is the query log (Cucerzan and Brill, 2004; Ahmad
and Kondrak, 2005; Li et al., 2006a; Chen et al.,
2007; Sun et al., 2010). However, while query logs
are abundant in the context of Web search, in many
906
s of a user who intends to phrase q, the goal is to Table 1: Feature set WFD in email search
find q. Our corpus is essentially a collection D of
unstructured or semistructured documents. For ex-
ample, in email search such a document is an email
with a title, a body, one or more recipients, and so
on. As conventional in spelling correction, we de-
vise a scoring function scoreD(r  |s) that estimates
our confidence in r being the correction of s (i.e.,
that r is equal to q). Eventually, we suggest a se-
quence r from a set CD(s) of candidates, such that
scoreD(r  |s) is maximal among all the candidates
in CD(s). In this section, we describe our graph-
based approach to finding CD(s) and to determining
scoreD(r  |s).
We first give some basic notation. We fix an al-
phabet E of characters that does not include any
of the conventional whitespace characters. By E*
we denote the set of all the words, namely, fi-
nite sequences over E. A search query s is a
sequence w1, ... , wn, where each wi is a word.
For convenience, in our examples we use whites-
pace instead of comma (e.g., sandeep kohli in-
stead of sandeep, kohli). We use the Damerau-
Levenshtein edit distance (as implemented by the
Jazzy tool) as our primary edit distance between two
words r1, r2 E E*, and we denote this distance by
ed(r1, r2).
2.1 Word-Level Correction
We first handle a restriction of our problem, where
the search query is a single word w (rather than
a general sequence s of words). Moreover, we
consider only candidate suggestions that are words
(rather than sequences of words that account for the
case where w is obtained by merging keywords).
Later, we will use the solution for this restricted
problem as a basic component in our algorithm for
the general problem.
Let UD C E* be a finite universal lexicon, which
(conceptually) consists of all the words in the corpus
D. (In practice, one may want add to D words of
auxiliary sources, like English dictionary, and to fil-
ter out noisy words; we did so in the site-search do-
main that is discussed in Section 3.) The set CD(w)
of candidates is defined by
CD(w) def = {w1 U {w&apos; E UD  |ed(w, w&apos;) &lt; S1.
for some fixed number S. Note that CD(w) contains
907
Basic Features
ed(w, w0): weighted Damerau-Levenshtein edit distance
ph(w, w0): 1 if w and w0 are phonetically equal, 0 otherwise
english(w0): 1 is w0 is in English, 0 otherwise
Corpus-Based Features
logfreq(w0)): logarithm of #occurrences of w0 in the corpus
Domain-Specific Features
subject(w0): 1 if w0 is in some “Subject” field, 0 otherwise
from(w0): 1 if w0 is in some “From” field, 0 otherwise
xfrom(w0): 1 if w0 is in some “X-From” field, 0 otherwise
w even if w is misspelled; furthermore, CD(w) may
contain other misspelled words (with a small edit
distance to w) that appear in D.
We now define scoreD(w&apos;  |w). Here, our cor-
pus D is translated into a set WFD of word features,
where each feature f E WFD gives a scoring func-
tion scoref(w&apos;  |w). The function scoreD(w&apos;  |w) is
simply a linear combination of the scoref(w&apos;  |w):
�scoreD(w&apos;  |w) def = af · scoref(w&apos;  |w)
fEWFD
As a concrete example, the features of WFD we used
in the email domain are listed in Table 1; the result-
ing scoref(w&apos;|w) is in the spirit of the noisy channel
model (Kernighan et al., 1990). Note that additional
features could be used, like ones involving the stems
of w and w&apos;, and even query-log statistics (when
available). Rather than manually tuning the param-
eters af, we learned them using the well known
Support Vector Machine, abbreviated SVM (Cortes
and Vapnik, 1995), as also done by Schaback and
Li (2007) for spelling correction. We further discuss
this learning step in Section 3.
We fix a natural number k, and in the sequel we
denote by topD(w) a set of k words w&apos; E CD(w)
with the highest scoreD(w&apos;  |w). If |CD(w) |&lt; k,
then topD(w) is simply CD(w).
2.2 Query-Level Correction: MaxPaths
We now describe our algorithm, MaxPaths, for
spelling correction. The input is a (possibly mis-
spelled) search query s = s1, ... , sn. As done in
the word-level correction, the algorithm produces a
set CD(s) of suggestions and determines the values
Algorithm 1 MaxPaths of s, or there is a word w&apos; ∈ CD(w) (as defined in
Section 2.1) such that scoreD(w&apos;  |w) &gt; E for some
fixed number E. Intuitively, w is plausible if it is an
original token of s, or we have a high confidence in
our word-level suggestion to correct w (note that the
suggested correction for w can be w itself). Recall
that bsc = c1 · · · cm. A tokenization of s is a se-
quence j1, ... , jl, such that j1 = 1, jl = m + 1, and
ji &lt; ji+1 for 1 ≤ i &lt; l. The tokenization j1, ... , jl
induces the tokens bsc[j1,j�),. . . ,bsc[jl_1,jl). A tok-
enization is plausible if each of its induced tokens
is plausible. Observe that a plausible token is not
necessarily induced by any plausible tokenization;
in that case, the plausible token is useless to us.
Thus, we define a strongly plausible token, abbre-
viated sp-token, which is a token that is induced by
some plausible tokenization. As a concrete example,
for the query excellatach ment, the sp-tokens in
our implementation include excellatach, ment,
excell, and atachment.
As the first step (line 1 in Algorithm 1), we find
the sp-tokens by employing an efficient (and fairly
straightforward) dynamic-programming algorithm.
2.2.2 Correction Graph
In the next step (line 2 in Algorithm 1), we con-
struct the correction graph, which we denote by
GD(s). The construction is as follows.
We first find the set topD(w) (defined in Sec-
tion 2.1) for each sp-token w. Table 2 shows the sp-
tokens and suggestions thereon in our running exam-
ple. This example shows the actual execution of our
implementation within email search, where s is the
query sadeep kohli excellatach ment; for
clarity of presentation, we omitted a few sp-tokens
and suggested corrections. Observe that some of
the corrections in the table are actually misspelled
words (as those naturally occur in the corpus).
A node of the graph GD(s) is a pair hw, w&apos;i, where
w is an sp-token and w&apos; ∈ topD(w). Recall our
simplifying assumption that a token bsc[i,j) uniquely
identifies the indices i and j. The graph GD(s) con-
tains a (directed) edge from a node hw1, w&apos;1i to a
node hw2, w&apos;2i if w2 immediately follows w1 in bqc;
in other words, GD(s) has an edge from hw1, w&apos;1i
to hw2, w&apos;2i whenever there exist indices i, j and k,
such that w1 = bsc[i,j) and w2 = bsc[j,k). Observe
that GD(s) is a directed acyclic graph (DAG).
Input: a search query s
Output: a set CD(s) of candidate suggestions r,
ranked by scoreD(r  |s)
1: Find the strongly plausible tokens
2: Construct the correction graph
3: Find top-k full paths (with the largest weights)
4: Re-rank the paths by word correlation
scoreD(r  |s), for all r ∈ CD(s), in order to rank
CD(s). A high-level overview of MaxPaths is given
in the pseudo-code of Algorithm 1. In the rest of this
section, we will detail each of the four steps in Al-
gorithm 1. The name MaxPaths will become clear
towards the end of this section.
We use the following notation. For a word w =
c1 ···cm of m characters ci and integers i &lt; j
in {1, ... , m + 1}, we denote by w[i j) the word
ci · · · cj_1. For two words w1, w2 ∈ E*, the word
w1w2 ∈ E* is obtained by concatenating w1 and
w2. Note that for the search query s = s1, ... , sn
it holds that s1 · · · sn is a single word (in E*). We
denote the word s1 · · · sn by bsc. For example, if
s1 = sadeep and s2 = kohli, then s corresponds
to the query sadeep kohli while bsc is the word
sadeepkohli; furthermore, bsc[1,7) = sadeep.
2.2.1 Plausible Tokens
To support merging and splitting, we first iden-
tify the possible tokens of the given query s. For
example, in excellatach ment we would like to
identify excell and atach ment as tokens, since
those are indeed the tokens that the user has in mind.
Formally, suppose that bsc = c1 · · · cm. A token is
a word bsc[i,j) where 1 ≤ i &lt; j ≤ m + 1. To
simplify the presentation, we make the (often false)
assumption that a token bsc[i,j) uniquely identifies
i and j (that is, bsc[i,j) =6 bsc[i,,j,) if i =6 i&apos; or
j =6 j&apos;); in reality, we should define a token as a
triple (bsc[i,j), i, j). In principle, every token bsc[i,j)
could be viewed as a possible word that user meant
to phrase. However, such liberty would require our
algorithm to process a search space that is too large
to manage in reasonable time. Instead, we restrict to
strongly plausible tokens, which we define next.
A token w = bsc[i,j) is plausible if w is a word
</bodyText>
<figure confidence="0.98535075">
908
ment
sent
excellent
sandeep
jaideep
sandeep kohli
kohli
excellence
excell
except
excel
attachement
attachment
attached
meet
</figure>
<figureCaption confidence="0.999982">
Figure 2: The graph !9D(s)
</figureCaption>
<bodyText confidence="0.99863125">
For example, Figure 2 shows !9D(s) for the
query sadeep kohli excellatach ment, with
the sp-tokens w and the sets topD(w) being those of
Table 2. For now, the reader should ignore the node
in the grey box (containing sandeep kohli) and
its incident edges. For simplicity, in this figure we
depict each node (w, w0) by just mentioning w0; the
word w is in the first row of Table 2, above w0.
</bodyText>
<subsubsectionHeader confidence="0.770509">
2.2.3 Top-k Paths
</subsubsectionHeader>
<bodyText confidence="0.998042380952381">
Let P = (w1, w01) · � (wk, w0k) be a path
in !9D(s). We say that P is full if (w1, w01) has no
incoming edges in !9D(s), and (wk, w0k) has no out-
going edges in !9D(s). An easy observation is that,
since we consider only strongly plausible tokens, if
P is full then w1 · · · wk = Ls]; in that case, the se-
quence w01, ... , w0k is a suggestion for spelling cor-
rection, and we denote it by crc(P). As an example,
Figure 3 shows two full paths P1 and P2 in the graph
!9D(s) of Figure 2. The corrections crc(PZ), for
i = 1, 2, are jaideep kohli excellent ment
and sandeep kohli excel attachement, re-
spectively.
To obtain corrections crc(P) with high quality,
we produce a set of k full paths with the largest
weights, for some fixed k; we denote this set by
topPathsD(s). The weight of a path P, denoted
weight(P), is the sum of the weights of all the nodes
and edges in P, and we define the weights of nodes
and edges next. To find these paths, we use a well
known efficient algorithm (Eppstein, 1994).
</bodyText>
<figureCaption confidence="0.992917">
Figure 3: Full paths in the graph !9D(s) of Figure 2
</figureCaption>
<bodyText confidence="0.971466888888889">
Consider a node u = (w, w0) of !9D(s). In the
construction of !9D(s), zero or more merges of (part
of) original tokens have been applied to obtain the
token w; let #merges(w) be that number. Consider
an edge e of !9D(s) from a node u1 = (w1, w01) to
u2 = (w2, w02). In s, either w1 and w2 belong to
different words (i.e., there is a whitespace between
them) or not; in the former case define #splits(e) =
0, and in the latter #splits(e) = 1. We define:
</bodyText>
<equation confidence="0.825854">
weight(u) def = scoreD(w0  |w) + am · #merges(w)
weight(e) def = as · #splits(e)
</equation>
<bodyText confidence="0.996122875">
Note that am and as are negative, as they penalize
for merges and splits, respectively. Again, in our
implementations, we learned am and as by means
of SVM.
Recall that topPathsD(s) is the set of k full paths
(in the graph !9D(s)) with the largest weights. From
topPathsD(s) we get the set CD(s) of candidate
suggestions:
</bodyText>
<equation confidence="0.556438">
CD(s) def = {crc(P)  |P E topPathsD(s)} .
</equation>
<subsubsectionHeader confidence="0.723503">
2.2.4 Word Correlation
</subsubsectionHeader>
<bodyText confidence="0.99991975">
To compute scoreD(r|s) for r E CD(s), we incor-
porate correlation among the words of r. Intuitively,
we would like to reward a candidate with pairs of
words that are likely to co-exist in a query. For
that, we assume a (symmetric) numerical function
crl(w01, w02) that estimates the extent to which the
words w01 and w02 are correlated. As an example, in
the email domain we would like crl(kohli, excel)
to be high if Kohli sent many emails with excel at-
tachments. Our implementation of crl(w01, w02) es-
sentially employs pointwise mutual information that
has also been used in (Schierle et al., 2007), and that
</bodyText>
<figure confidence="0.9752583">
ment
P1
P2
excellent
jaideep
sandeep
kohli
kohli
excel
attachment
</figure>
<page confidence="0.989866">
909
</page>
<tableCaption confidence="0.982544">
Table 2: topD(w) for sp-tokens w
</tableCaption>
<bodyText confidence="0.983439545454545">
sadeep kohli excellatach ment excell atachment
sandeep kohli excellent ment excel attachment
jaideep excellence sent excell attached
meet except attachement
compares the number of documents (emails) con-
taining w01 and w02 separately and jointly.
Let P E topPathsD(s) be a path. We de-
note by crl(P) a function that aggregates the num-
bers crl(w01, w02) for nodes (w1, w01) and (w2, w02)
of P (where (w1, w01) and (w2, w02) are not nec-
essarily neighbors in P). Over the email domain,
our crl(P) is the minimum of the crl(w01, w02). We
define scoreD(P) = weight(P) + crl(P). To
improve the performance, in our implementation
we learned again (re-trained) all the parameters in-
volved in scoreD(P).
Finally, as the top suggestions we take crc(P)
for full paths P with highest scoreD(P). Note that
crc(P) is not necessarily injective; that is, there can
be two full paths P1 =� P2 satisfying crc(P1) =
crc(P2). Thus, in effect, scoreD(r  |s) is determined
by the best evidence of r; that is,
</bodyText>
<equation confidence="0.9818065">
scoreD(r  |s) def = max{scoreD(P)  |crc(P) = rn
P E topPathsD(s)} .
</equation>
<bodyText confidence="0.99999525">
Note that our final scoring function essentially views
P as a clique rather than a path. In principle,
we could define GD(s) in a way that we would
extract the maximal cliques directly without find-
ing topPathsD(s) first. However, we chose our
method (finding top paths first, and then re-ranking)
to avoid the inherent computational hardness in-
volved in finding maximal cliques.
</bodyText>
<subsectionHeader confidence="0.999517">
2.3 Handling Expressions
</subsectionHeader>
<bodyText confidence="0.999964857142857">
We now briefly discuss our handling of frequent n-
grams (expressions). We handle n-grams by intro-
ducing new nodes to the graph GD(s); such a new
node u is a pair (t, t0), where t is a sequence of
n consecutive sp-tokens and t0 is a n-gram. The
weight of such a node u is rewarded for consti-
tuting a frequent or important n-gram. An exam-
ple of such a node is in the grey box of Figure 2,
where sandeep kohli is a bigram. Observe that
sandeep kohli may be deemed an important bi-
gram because it occurs as a sender of an email, and
not necessarily because it is frequent.
An advantage of our approach is avoidance
of over-scoring due to conflicting n-grams. For
example, consider the query textile import
expert, and assume that both textile import
and import export (with an “o” rather than an
“e”) are frequent bigrams. If the user referred to the
bigram textile import, then expert is likely to
be correct. But if she meant for import export,
then expert is misspelled. However, only one of
these two options can hold true, and we would like
textile import export to be rewarded only
once—for the bigram import export. This is
achieved in our approach, since a full path in GD(s)
may contain either anode for textile import or
a node for import export, but it cannot contain
nodes for both of these bigrams.
Finally, we note that our algorithm is in the spirit
of that of Cucerzan and Brill (2004), with a few in-
herent differences. In essence, a node in the graph
they construct corresponds to what we denote here
as (w, w0) in the special case where w is an actual
word of the query; that is, no re-tokenization is ap-
plied. They can split a word by comparing it to a bi-
gram. However, it is not clear how they can split into
non-bigrams (without a huge index) and to handle si-
multaneous merging and splitting as in our running
example (1). Furthermore, they translate bigram in-
formation into edge weights, which implies that the
above problem of over-rewarding due to conflicting
bigrams occurs.
</bodyText>
<sectionHeader confidence="0.994684" genericHeader="introduction">
3 Experimental Study
</sectionHeader>
<bodyText confidence="0.999863333333333">
Our experimental study aims to investigate the ef-
fectiveness of our approach in various settings, as
we explain next.
</bodyText>
<subsectionHeader confidence="0.993497">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99998">
We first describe our experimental setup, and specif-
ically the datasets and general methodology.
Datasets. The focus of our experimental study is
on personal email search; later on (Section 3.6),
we will consider (and give experimental results for)
a totally different setting—site search over www.
ibm.com, which is a massive and open domain.
Our dataset (for the email domain) is obtained from
</bodyText>
<page confidence="0.991745">
910
</page>
<bodyText confidence="0.9995954">
the Enron email collection (Bekkerman et al., 2004;
Klimt and Yang, 2004). Specifically, we chose the
three users with the largest number of emails. We re-
fer to the three email collections by the last names of
their owners: Farmer, Kaminski and Kitchen. Each
user mailbox is a separate domain, with a separate
corpus D, that one can search upon. Due to the ab-
sence of real user queries, we constructed our dataset
by conducting a user study, as described next.
For each user, we randomly sampled 50 emails
and divided them into 5 disjoint sets of 10 emails
each. We gave each 10-email set to a unique hu-
man subject that was asked to phrase two search
queries for each email: one for the entire email con-
tent (general query), and the other for the From and
X-From fields (sender query). (Figure 1 shows ex-
amples of the From and X-From fields.) The latter
represents queries posed against a specific field (e.g.,
using “advanced search”). The participants were not
told about the goal of this study (i.e., spelling correc-
tion), and the collected queries have no spelling er-
rors. For generating spelling errors, we implemented
a typo generator.1 This generator extends an online
typo generator (Seobook, 2010) that produces a vari-
ety of spelling errors, including skipped letter, dou-
bled letter, reversed letter, skipped space (merge),
missed key and inserted key; in addition, our gener-
ator produces inserted space (split). When applied
to a search query, our generator adds random typos
to each word, independently, with a specified prob-
ability p that is 50% by default. For each collected
query (and for each considered value of p) we gener-
ated 5 misspelled queries, and thereby obtained 250
instances of misspelled general queries and 250 in-
stances of misspelled sender queries.
Methodology. We compared the accuracy of
MaxPaths (Section 2) with three alternatives. The
first alternative is the open-source Jazzy, which
is a widely used spelling-correction tool based on
(weighted) edit distance. The second alternative is
the spelling correction provided by Google. We
provided Jazzy with our unigram index (as a dic-
tionary). However, we were not able to do so
with Google, as we used remote access via its Java
API (Google, 2010); hence, the Google tool is un-
</bodyText>
<footnote confidence="0.926046">
1The queries and our typo generator are publicly available
athttps://dbappserv.cis.upenn.edu/spell/.
</footnote>
<bodyText confidence="0.999984375">
aware of our domain, but is rather based on its
own statistics (from the World Wide Web). The
third alternative is what we call WordWise, which
applies word-level correction (Section 2.1) to each
input query term, independently. More precisely,
WordWise is a simplified version of MaxPaths,
where we forbid splitting and merging of words (i.e.,
only the original tokens are considered), and where
we do not take correlation into account.
Our emphasis is on correcting misspelled queries,
rather than recognizing correctly spelled queries,
due to the role of spelling in a search engine: we
wish to provide the user with the correct query upon
misspelling, but there is no harm in making a sug-
gestion for correctly spelled queries, except for vi-
sual discomfort. Hence, by default accuracy means
the number of properly corrected queries (within
the top-k suggestions) divided by the number of the
misspelled queries. An exception is in Section 3.5,
where we study the accuracy on correct queries.
Since MaxPaths and WordWise involve parame-
ter learning (SVM), the results for them are consis-
tently obtained by performing 5-folder cross valida-
tion over each collection of misspelled queries.
</bodyText>
<subsectionHeader confidence="0.999433">
3.2 Fixed Error Probability
</subsectionHeader>
<bodyText confidence="0.999930772727273">
Here, we compare MaxPaths to the alternatives
when the error probability p is fixed (0.5). We con-
sider only the Kaminski dataset; the results for the
other two datasets are similar. Figure 4(a) shows the
accuracy, for general queries, of top-k suggestions
for k = 1, k = 3 and k = 10. Note that we can get
only one (top-1) suggestion from Google. As can
be seen, MaxPaths has the highest accuracy in all
cases. Moreover, the advantage of MaxPaths over
the alternatives increases as k increases, which indi-
cates potential for further improving MaxPaths.
Figure 4(b) shows the accuracy of top-k sugges-
tions for sender queries. Overall, the results are sim-
ilar to those of Figure 4(a), except that top-1 of both
WordWise and MaxPaths has a higher accuracy in
sender queries than in general queries. This is due
to the fact that the dictionaries of person names and
email addresses extracted from the X-From and
From fields, respectively, provide strong features
for the scoring function, since a sender query refers
to these two fields. In addition, the accuracy of
MaxPaths is further enhanced by exploiting the cor-
</bodyText>
<page confidence="0.987329">
911
</page>
<figure confidence="0.999465136363637">
Google Jazzy MaxPaths* MaxPaths
100%
75%
50%
25%
0%
0% 20% 40% 60% 80% 100%
(a) General queries (Kaminski)
Spelling Error Probability
(a) General queries (Farmer)
Google Jazzy WordWise MaxPaths
Top 1 Top 3 Top 10
100%
80%
60%
40%
20%
0%
80%
60%
40%
20%
0%
Top 1 Top 3 Top 10
Google Jazzy MaxPaths* MaxPaths
100%
75%
50%
25%
0%
0% 20% 40% 60% 80% 100%
(b) Sender queries (Kaminski)
Spelling Error Probability
(b) Sender queries (Farmer)
Google Jazzy WordWise MaxPaths
100%
Google Jazzy WordWise MaxPaths
100%
75%
50%
25%
0%
0% 20% 40% 60% 80% 100%
Spelling Error Probability
</figure>
<figureCaption confidence="0.980243">
(c) Varying error probability (Kaminski)
Figure 4: Accuracy for Kaminski (misspelled queries)
</figureCaption>
<bodyText confidence="0.970274">
relation between the first and last name of a person.
</bodyText>
<subsectionHeader confidence="0.997092">
3.3 Impact of Error Probability
</subsectionHeader>
<bodyText confidence="0.999998285714286">
We now study the impact of the complexity of
spelling errors on our algorithm. For that, we mea-
sure the accuracy while the error probability p varies
from 10% to 90% (with gaps of 20%). The re-
sults are in Figure 4(c). Again, we show the results
only for Kaminski, since we get similar results for
the other two datasets. As expected, in all exam-
ined methods the accuracy decreases as p increases.
Now, not only does MaxPaths outperform the alter-
natives, its decrease (as well as that of WordWise) is
the mildest—13% as p increases from 10% to 90%
(while Google and Jazzy decrease by 23% or more).
We got similar results for the sender queries (and for
each of the three users).
</bodyText>
<subsectionHeader confidence="0.998985">
3.4 Adaptiveness of Parameters
</subsectionHeader>
<bodyText confidence="0.999058">
Obtaining the labeled data needed for parameter
learning entails a nontrivial manual effort. Ideally,
we would like to learn the parameters of MaxPaths
in one domain, and use them in similar domains.
</bodyText>
<figureCaption confidence="0.99806">
Figure 5: Accuracy for Farmer (misspelled queries)
</figureCaption>
<bodyText confidence="0.999976333333333">
More specifically, our desire is to use the parame-
ters learned over one corpus (e.g., the email collec-
tion of one user) on a second corpus (e.g., the email
collection of another user), rather than learning the
parameters again over the second corpus. In this set
of experiments, we examine the feasibility of that
approach. Specifically, we consider the user Farmer
and observe the accuracy of our algorithm with two
sets of parameters: the first, denoted by MaxPaths in
Figures 5(a) and 5(b), is learned within the Farmer
dataset, and the second, denoted by MaxPaths*, is
learned within the Kaminski dataset. Figures 5(a)
and 5(b) show the accuracy of the top-1 suggestion
for general queries and sender queries, respectively,
with varying error probabilities. As can be seen,
these results mean good news—the accuracies of
MaxPaths* and MaxPaths are extremely close (their
curves are barely distinguishable, as in most cases
the difference is smaller than 1%). We repeated this
experiment for Kitchen and Kaminski, and got sim-
ilar results.
</bodyText>
<sectionHeader confidence="0.618822" genericHeader="method">
3.5 Accuracy for Correct Queries
</sectionHeader>
<bodyText confidence="0.999978">
Next, we study the accuracy on correct queries,
where the task is to recognize the given query as cor-
rect by returning it as the top suggestion. For each
of the three users, we considered the 50 + 50 (gen-
eral + sender) collected queries (having no spelling
errors), and measured the accuracy, which is the
percentage of queries that are equal to the top sug-
</bodyText>
<page confidence="0.994895">
912
</page>
<tableCaption confidence="0.995433">
Table 3: Accuracy for Correct Queries
</tableCaption>
<table confidence="0.9799725">
Dataset Google Jazzy MaxPaths
Kaminski (general) 90% 98% 94%
Kaminski (sender) 94% 98% 94%
Farmer (general) 96% 98% 96%
Farmer (sender) 96% 96% 92%
Kitchen (general) 86% 100% 92%
Kitchen (sender) 94% 100% 98%
Google Jazzy WordWise MaxPaths
</table>
<figure confidence="0.926030571428572">
Top 1 Top 3 Top 10
100%
80%
60%
40%
20%
0%
</figure>
<figureCaption confidence="0.999969">
Figure 6: Accuracy for site search
</figureCaption>
<bodyText confidence="0.999974625">
gestion. Table 3 shows the results. Since Jazzy is
based on edit distance, it almost always gives the in-
put query as the top suggestion; the misses of Jazzy
are for queries that contain a word that is not the cor-
pus. MaxPaths is fairly close to the upper bound set
by Jazzy. Google (having no access to the domain)
also performs well, partly because it returns the in-
put query if no reasonable suggestion is found.
</bodyText>
<subsectionHeader confidence="0.988072">
3.6 Applicability to Large-Scale Site Search
</subsectionHeader>
<bodyText confidence="0.999996954545455">
Up to now, our focus has been on email search,
which represents a restricted (closed) domain with
specialized knowledge (e.g., sender names). In this
part, we examine the effectiveness of our algorithm
in a totally different setting—large-scale site search
within www.ibm.com, a domain that is popular on
a world scale. There, the accuracy of Google is very
high, due to this domain’s popularity, scale, and full
accessibility on the Web. We crawled 10 million
documents in that domain to obtain the corpus. We
manually collected 1348 misspelled queries from
the log of search issued against developerWorks
(www.ibm.com/developerworks/) during a
week. To facilitate the manual collection of these
queries, we inspected each query with two or fewer
search results, after applying a random permutation
to those queries. Figure 6 shows the accuracy of
top-k suggestions. Note that the performance of
MaxPaths is very close to that of Google—only 2%
lower for top-1. For k = 3 and k = 10, MaxPaths
outperforms Jazzy and the top-1 of Google (from
which we cannot obtain top-k for k &gt; 1).
</bodyText>
<subsectionHeader confidence="0.90472">
3.7 Summary
</subsectionHeader>
<bodyText confidence="0.999971411764706">
To conclude, our experiments demonstrate various
important qualities of MaxPaths. First, it outper-
forms its alternatives, in both accuracy (Section 3.2)
and robustness to varying error complexities (Sec-
tion 3.3). Second, the parameters learned in one
domain (e.g., an email user) can be applied to sim-
ilar domains (e.g., other email users) with essen-
tially no loss in performance (Section 3.4). Third,
it is highly accurate in recognition of correct queries
(Section 3.5). Fourth, even when applied to large
(open) domains, it achieves a comparable perfor-
mance to the state-of-the-art Google spelling correc-
tion (Section 3.6). Finally, the higher performance
of MaxPaths on top-3 and top-10 corrections sug-
gests a potential for further improvement of top-1
(which is important since search engines often re-
strict their interfaces to only one suggestion).
</bodyText>
<sectionHeader confidence="0.999822" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999929652173913">
We presented the algorithm MaxPaths for spelling
correction in domain-centric search. This algo-
rithm relies primarily on corpus statistics and do-
main knowledge (rather than on query logs). It can
handle a variety of spelling errors, and can incor-
porate different levels of spelling reliability among
different parts of the corpus. Our experimental study
demonstrates the superiority of MaxPaths over ex-
isting alternatives in the domain of email search, and
indicates its effectiveness beyond that domain.
In future work, we plan to explore how to utilize
additional domain knowledge to better estimate the
correlation between words. Particularly, from avail-
able auxiliary data (Fagin et al., 2010) and tools like
information extraction (Chiticariu et al., 2010), we
can infer and utilize type information from the cor-
pus (Li et al., 2006b; Zhu et al., 2007). For instance,
if kohli is of type person, and phone is highly cor-
related with person instances, then phone is highly
correlated with kohli even if the two words do not
frequently co-occur. We also plan to explore as-
pects of corpus maintenance in dynamic (constantly
changing) domains.
</bodyText>
<page confidence="0.998483">
913
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999892044776119">
F. Ahmad and G. Kondrak. 2005. Learning a spelling
error model from search query logs. In HLT/EMNLP.
R. Bekkerman, A. Mccallum, and G. Huang. 2004. Au-
tomatic categorization of email into folders: Bench-
mark experiments on Enron and Sri Corpora. Techni-
cal report, University of Massachusetts - Amherst.
Q. Chen, M. Li, and M. Zhou. 2007. Improving
query spelling correction using Web search results. In
EMNLP-CoNLL, pages 181–189.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010. SystemT: An
algebraic approach to declarative information extrac-
tion. In ACL, pages 128–137.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine Learning, 20(3):273–297.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of Web users. In EMNLP, pages 293–300.
D. Eppstein. 1994. Finding the k shortest paths. In
FOCS, pages 154–165.
R. Fagin, B. Kimelfeld, Y. Li, S. Raghavan, and
S. Vaithyanathan. 2010. Understanding queries in a
search database system. In PODS, pages 273–284.
Google. 2010. A Java API for Google spelling check ser-
vice. http://code.google.com/p/google-api-spelling-
java/.
D. Jurafsky and J. H. Martin. 2000. Speech and
Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition. Prentice Hall PTR.
M. D. Kernighan, K. W. Church, and W. A. Gale. 1990.
A spelling correction program based on a noisy chan-
nel model. In COLING, pages 205–210.
B. Klimt and Y. Yang. 2004. Introducing the Enron cor-
pus. In CEAS.
K. Kukich. 1992. Techniques for automatically correct-
ing words in text. ACM Comput. Surv., 24(4):377–
439.
M. Li, M. Zhu, Y. Zhang, and M. Zhou. 2006a. Explor-
ing distributional similarity based models for query
spelling correction. In ACL.
Y. Li, R. Krishnamurthy, S. Vaithyanathan, and H. V. Ja-
gadish. 2006b. Getting work done on the web: sup-
porting transactional queries. In SIGIR, pages 557–
564.
R. Mitton. 2010. Fifty years of spellchecking. Wring
Systems Research, 2:1–7.
J. L. Peterson. 1980. Computer Programs for Spelling
Correction: An Experiment in Program Design, vol-
ume 96 of Lecture Notes in Computer Science.
Springer.
J. Schaback and F. Li. 2007. Multi-level feature extrac-
tion for spelling correction. In AND, pages 79–86.
M. Schierle, S. Schulz, and M. Ackermann. 2007. From
spelling correction to text cleaning - using context in-
formation. In GfKl, Studies in Classification, Data
Analysis, and Knowledge Organization, pages 397–
404.
Seobook. 2010. Keyword typo generator.
http://tools.seobook.com/spelling/keywords-
typos.cgi.
X. Sun, J. Gao, D. Micol, and C. Quirk. 2010. Learning
phrase-based spelling error models from clickthrough
data. In ACL, pages 266–274.
H. Zhu, S. Raghavan, S. Vaithyanathan, and A. L¨oser.
2007. Navigating the intranet with high precision. In
WWW, pages 491–500.
</reference>
<page confidence="0.998322">
914
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969316">
<title confidence="0.99952">A Graph Approach to Spelling Correction in Domain-Centric Search</title>
<author confidence="0.996945">Zhuowei Bao Benny Kimelfeld Yunyao Li</author>
<affiliation confidence="0.999938">University of Pennsylvania IBM Research–Almaden IBM Research–Almaden</affiliation>
<address confidence="0.996864">Philadelphia, PA 19104, USA San Jose, CA 95120, USA San Jose, CA 95120, USA</address>
<email confidence="0.999045">zhuowei@cis.upenn.edukimelfeld@us.ibm.comyunyaoli@us.ibm.com</email>
<abstract confidence="0.998619764705882">Spelling correction for keyword-search queries is challenging in restricted domains such as personal email (or desktop) search, due to the scarcity of query logs, and due to the specialized nature of the domain. For that task, this paper presents an algorithm that is based on statistics from the corpus data (rather than the query log). This algorithm, which employs a simple graph-based approach, can incorporate different types of data sources with different levels of reliability (e.g., email subject vs. email body), and can handle complex spelling errors like splitting and merging of words. An experimental study shows the superiority of the algorithm over existing alternatives in the email domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Ahmad</author>
<author>G Kondrak</author>
</authors>
<title>Learning a spelling error model from search query logs.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="8038" citStr="Ahmad and Kondrak, 2005" startWordPosition="1292" endWordPosition="1295"> a misspelling of attachment due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli —* sandeep kohli (rather than, e.g., sadeep kohli —* sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell —* excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 s of a user who intends to phrase q, the goal is to Table 1: Feature set WFD in email search find q. Our corpus is essentially a collection D of unstructured or semistructured documents. For example, in email search such a document is an email with a title, a body, one or more recipients, and so on. As conventional in spelling correction, we devise a scoring function scoreD(r |s) that estimates our confidence in r being the correction of s (i.e., that r is </context>
</contexts>
<marker>Ahmad, Kondrak, 2005</marker>
<rawString>F. Ahmad and G. Kondrak. 2005. Learning a spelling error model from search query logs. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
<author>A Mccallum</author>
<author>G Huang</author>
</authors>
<title>Automatic categorization of email into folders: Benchmark experiments on Enron and Sri Corpora.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>University of Massachusetts - Amherst.</institution>
<contexts>
<context position="23968" citStr="Bekkerman et al., 2004" startWordPosition="4196" endWordPosition="4199">s occurs. 3 Experimental Study Our experimental study aims to investigate the effectiveness of our approach in various settings, as we explain next. 3.1 Experimental Setup We first describe our experimental setup, and specifically the datasets and general methodology. Datasets. The focus of our experimental study is on personal email search; later on (Section 3.6), we will consider (and give experimental results for) a totally different setting—site search over www. ibm.com, which is a massive and open domain. Our dataset (for the email domain) is obtained from 910 the Enron email collection (Bekkerman et al., 2004; Klimt and Yang, 2004). Specifically, we chose the three users with the largest number of emails. We refer to the three email collections by the last names of their owners: Farmer, Kaminski and Kitchen. Each user mailbox is a separate domain, with a separate corpus D, that one can search upon. Due to the absence of real user queries, we constructed our dataset by conducting a user study, as described next. For each user, we randomly sampled 50 emails and divided them into 5 disjoint sets of 10 emails each. We gave each 10-email set to a unique human subject that was asked to phrase two search</context>
</contexts>
<marker>Bekkerman, Mccallum, Huang, 2004</marker>
<rawString>R. Bekkerman, A. Mccallum, and G. Huang. 2004. Automatic categorization of email into folders: Benchmark experiments on Enron and Sri Corpora. Technical report, University of Massachusetts - Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Chen</author>
<author>M Li</author>
<author>M Zhou</author>
</authors>
<title>Improving query spelling correction using Web search results. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>181--189</pages>
<contexts>
<context position="8075" citStr="Chen et al., 2007" startWordPosition="1300" endWordPosition="1303">e (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli —* sandeep kohli (rather than, e.g., sadeep kohli —* sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell —* excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 s of a user who intends to phrase q, the goal is to Table 1: Feature set WFD in email search find q. Our corpus is essentially a collection D of unstructured or semistructured documents. For example, in email search such a document is an email with a title, a body, one or more recipients, and so on. As conventional in spelling correction, we devise a scoring function scoreD(r |s) that estimates our confidence in r being the correction of s (i.e., that r is equal to q). Eventually, we suggest a</context>
</contexts>
<marker>Chen, Li, Zhou, 2007</marker>
<rawString>Q. Chen, M. Li, and M. Zhou. 2007. Improving query spelling correction using Web search results. In EMNLP-CoNLL, pages 181–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chiticariu</author>
<author>R Krishnamurthy</author>
<author>Y Li</author>
<author>S Raghavan</author>
<author>F Reiss</author>
<author>S Vaithyanathan</author>
</authors>
<title>SystemT: An algebraic approach to declarative information extraction.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>128--137</pages>
<marker>Chiticariu, Krishnamurthy, Li, Raghavan, Reiss, Vaithyanathan, 2010</marker>
<rawString>L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan, F. Reiss, and S. Vaithyanathan. 2010. SystemT: An algebraic approach to declarative information extraction. In ACL, pages 128–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="11712" citStr="Cortes and Vapnik, 1995" startWordPosition="1957" endWordPosition="1960">nction scoref(w&apos; |w). The function scoreD(w&apos; |w) is simply a linear combination of the scoref(w&apos; |w): �scoreD(w&apos; |w) def = af · scoref(w&apos; |w) fEWFD As a concrete example, the features of WFD we used in the email domain are listed in Table 1; the resulting scoref(w&apos;|w) is in the spirit of the noisy channel model (Kernighan et al., 1990). Note that additional features could be used, like ones involving the stems of w and w&apos;, and even query-log statistics (when available). Rather than manually tuning the parameters af, we learned them using the well known Support Vector Machine, abbreviated SVM (Cortes and Vapnik, 1995), as also done by Schaback and Li (2007) for spelling correction. We further discuss this learning step in Section 3. We fix a natural number k, and in the sequel we denote by topD(w) a set of k words w&apos; E CD(w) with the highest scoreD(w&apos; |w). If |CD(w) |&lt; k, then topD(w) is simply CD(w). 2.2 Query-Level Correction: MaxPaths We now describe our algorithm, MaxPaths, for spelling correction. The input is a (possibly misspelled) search query s = s1, ... , sn. As done in the word-level correction, the algorithm produces a set CD(s) of suggestions and determines the values Algorithm 1 MaxPaths of s</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine Learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>E Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of Web users.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>293--300</pages>
<contexts>
<context position="1954" citStr="Cucerzan and Brill, 2004" startWordPosition="307" endWordPosition="310">edge of the exact spelling of q. The goal is to restore q, when given s. Spelling correction has been extensively studied in the literature, and we refer the reader to comprehensive summaries of prior work (Peterson, 1980; Kukich, 1992; Jurafsky and Martin, 2000; Mitton, 2010). The focus of this paper is on the special case where q is a search query, and where s instead of q is submitted to a search engine (with the goal of retrieving documents that match the search query q). Spelling correction for search queries is important, because a significant portion of posed queries may be misspelled (Cucerzan and Brill, 2004). Effective spelling correction has a major effect on the experience and effort of the user, who is otherwise required to ensure the exact spellings of her queries. Furthermore, it is critical when the exact spelling is unknown (e.g., person names like Schwarzenegger). 1.1 Spelling Errors The more common and studied type of spelling error is word-to-word error: a single word w is misspelled into another single word w&apos;. The specific spelling errors involved include omission of a character (e.g., atachment), inclusion of a redundant character (e.g., attachement), and replacement of characters (e</context>
<context position="8013" citStr="Cucerzan and Brill, 2004" startWordPosition="1288" endWordPosition="1291">y decide that atachment is a misspelling of attachment due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli —* sandeep kohli (rather than, e.g., sadeep kohli —* sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell —* excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 s of a user who intends to phrase q, the goal is to Table 1: Feature set WFD in email search find q. Our corpus is essentially a collection D of unstructured or semistructured documents. For example, in email search such a document is an email with a title, a body, one or more recipients, and so on. As conventional in spelling correction, we devise a scoring function scoreD(r |s) that estimates our confidence in r being the correcti</context>
<context position="22757" citStr="Cucerzan and Brill (2004)" startWordPosition="3993" endWordPosition="3996"> “o” rather than an “e”) are frequent bigrams. If the user referred to the bigram textile import, then expert is likely to be correct. But if she meant for import export, then expert is misspelled. However, only one of these two options can hold true, and we would like textile import export to be rewarded only once—for the bigram import export. This is achieved in our approach, since a full path in GD(s) may contain either anode for textile import or a node for import export, but it cannot contain nodes for both of these bigrams. Finally, we note that our algorithm is in the spirit of that of Cucerzan and Brill (2004), with a few inherent differences. In essence, a node in the graph they construct corresponds to what we denote here as (w, w0) in the special case where w is an actual word of the query; that is, no re-tokenization is applied. They can split a word by comparing it to a bigram. However, it is not clear how they can split into non-bigrams (without a huge index) and to handle simultaneous merging and splitting as in our running example (1). Furthermore, they translate bigram information into edge weights, which implies that the above problem of over-rewarding due to conflicting bigrams occurs. 3</context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>S. Cucerzan and E. Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of Web users. In EMNLP, pages 293–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Eppstein</author>
</authors>
<title>Finding the k shortest paths.</title>
<date>1994</date>
<booktitle>In FOCS,</booktitle>
<pages>154--165</pages>
<contexts>
<context position="18221" citStr="Eppstein, 1994" startWordPosition="3188" endWordPosition="3189">t by crc(P). As an example, Figure 3 shows two full paths P1 and P2 in the graph !9D(s) of Figure 2. The corrections crc(PZ), for i = 1, 2, are jaideep kohli excellent ment and sandeep kohli excel attachement, respectively. To obtain corrections crc(P) with high quality, we produce a set of k full paths with the largest weights, for some fixed k; we denote this set by topPathsD(s). The weight of a path P, denoted weight(P), is the sum of the weights of all the nodes and edges in P, and we define the weights of nodes and edges next. To find these paths, we use a well known efficient algorithm (Eppstein, 1994). Figure 3: Full paths in the graph !9D(s) of Figure 2 Consider a node u = (w, w0) of !9D(s). In the construction of !9D(s), zero or more merges of (part of) original tokens have been applied to obtain the token w; let #merges(w) be that number. Consider an edge e of !9D(s) from a node u1 = (w1, w01) to u2 = (w2, w02). In s, either w1 and w2 belong to different words (i.e., there is a whitespace between them) or not; in the former case define #splits(e) = 0, and in the latter #splits(e) = 1. We define: weight(u) def = scoreD(w0 |w) + am · #merges(w) weight(e) def = as · #splits(e) Note that am</context>
</contexts>
<marker>Eppstein, 1994</marker>
<rawString>D. Eppstein. 1994. Finding the k shortest paths. In FOCS, pages 154–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fagin</author>
<author>B Kimelfeld</author>
<author>Y Li</author>
<author>S Raghavan</author>
<author>S Vaithyanathan</author>
</authors>
<title>Understanding queries in a search database system.</title>
<date>2010</date>
<booktitle>In PODS,</booktitle>
<pages>273--284</pages>
<marker>Fagin, Kimelfeld, Li, Raghavan, Vaithyanathan, 2010</marker>
<rawString>R. Fagin, B. Kimelfeld, Y. Li, S. Raghavan, and S. Vaithyanathan. 2010. Understanding queries in a search database system. In PODS, pages 273–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>A Java API for Google spelling check service.</title>
<date>2010</date>
<note>http://code.google.com/p/google-api-spellingjava/.</note>
<contexts>
<context position="26150" citStr="Google, 2010" startWordPosition="4564" endWordPosition="4565"> value of p) we generated 5 misspelled queries, and thereby obtained 250 instances of misspelled general queries and 250 instances of misspelled sender queries. Methodology. We compared the accuracy of MaxPaths (Section 2) with three alternatives. The first alternative is the open-source Jazzy, which is a widely used spelling-correction tool based on (weighted) edit distance. The second alternative is the spelling correction provided by Google. We provided Jazzy with our unigram index (as a dictionary). However, we were not able to do so with Google, as we used remote access via its Java API (Google, 2010); hence, the Google tool is un1The queries and our typo generator are publicly available athttps://dbappserv.cis.upenn.edu/spell/. aware of our domain, but is rather based on its own statistics (from the World Wide Web). The third alternative is what we call WordWise, which applies word-level correction (Section 2.1) to each input query term, independently. More precisely, WordWise is a simplified version of MaxPaths, where we forbid splitting and merging of words (i.e., only the original tokens are considered), and where we do not take correlation into account. Our emphasis is on correcting m</context>
</contexts>
<marker>Google, 2010</marker>
<rawString>Google. 2010. A Java API for Google spelling check service. http://code.google.com/p/google-api-spellingjava/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.</title>
<date>2000</date>
<publisher>Prentice Hall PTR.</publisher>
<contexts>
<context position="1591" citStr="Jurafsky and Martin, 2000" startWordPosition="243" endWordPosition="246">he algorithm over existing alternatives in the email domain. 1 Introduction An abundance of applications require spelling correction, which (at the high level) is the following task. The user intends to type a chunk q of text, but types instead the chunk s that contains spelling errors (which we discuss in detail later), due to uncareful typing or lack of knowledge of the exact spelling of q. The goal is to restore q, when given s. Spelling correction has been extensively studied in the literature, and we refer the reader to comprehensive summaries of prior work (Peterson, 1980; Kukich, 1992; Jurafsky and Martin, 2000; Mitton, 2010). The focus of this paper is on the special case where q is a search query, and where s instead of q is submitted to a search engine (with the goal of retrieving documents that match the search query q). Spelling correction for search queries is important, because a significant portion of posed queries may be misspelled (Cucerzan and Brill, 2004). Effective spelling correction has a major effect on the experience and effort of the user, who is otherwise required to ensure the exact spellings of her queries. Furthermore, it is critical when the exact spelling is unknown (e.g., pe</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>D. Jurafsky and J. H. Martin. 2000. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall PTR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Kernighan</author>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>A spelling correction program based on a noisy channel model.</title>
<date>1990</date>
<booktitle>In COLING,</booktitle>
<pages>205--210</pages>
<contexts>
<context position="7178" citStr="Kernighan et al., 1990" startWordPosition="1148" endWordPosition="1151">s the user may very well mean kohls coupons if Sandeep Kohli has nothing to do with coupons (in contrast to the store chain Kohl’s). A similar example is the word nail, which is a legitimate English word, but in the context of email the query nail box is likely to be a misspelling of mail box (unless nail boxes are indeed relevant to the user’s email collection). Finally, while the word kohli is relevant to some email users (e.g., Kohli’s colleagues), it may have no meaning at all to other users. 1.2 Domain Knowledge The common approach to spelling correction utilizes statistical information (Kernighan et al., 1990; Schierle et al., 2007; Mitton, 2010). As a simple example, if we want to avoid maintaining a manually-crafted dictionary to accommodate the wealth of new terms introduced every day (e.g., ipod and ipad), we may decide that atachment is a misspelling of attachment due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli —* sandeep kohli (rather than, e.g., sadeep kohli —* sudeep kohli). One c</context>
<context position="11425" citStr="Kernighan et al., 1990" startWordPosition="1911" endWordPosition="1914"> 0 otherwise w even if w is misspelled; furthermore, CD(w) may contain other misspelled words (with a small edit distance to w) that appear in D. We now define scoreD(w&apos; |w). Here, our corpus D is translated into a set WFD of word features, where each feature f E WFD gives a scoring function scoref(w&apos; |w). The function scoreD(w&apos; |w) is simply a linear combination of the scoref(w&apos; |w): �scoreD(w&apos; |w) def = af · scoref(w&apos; |w) fEWFD As a concrete example, the features of WFD we used in the email domain are listed in Table 1; the resulting scoref(w&apos;|w) is in the spirit of the noisy channel model (Kernighan et al., 1990). Note that additional features could be used, like ones involving the stems of w and w&apos;, and even query-log statistics (when available). Rather than manually tuning the parameters af, we learned them using the well known Support Vector Machine, abbreviated SVM (Cortes and Vapnik, 1995), as also done by Schaback and Li (2007) for spelling correction. We further discuss this learning step in Section 3. We fix a natural number k, and in the sequel we denote by topD(w) a set of k words w&apos; E CD(w) with the highest scoreD(w&apos; |w). If |CD(w) |&lt; k, then topD(w) is simply CD(w). 2.2 Query-Level Correct</context>
</contexts>
<marker>Kernighan, Church, Gale, 1990</marker>
<rawString>M. D. Kernighan, K. W. Church, and W. A. Gale. 1990. A spelling correction program based on a noisy channel model. In COLING, pages 205–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Klimt</author>
<author>Y Yang</author>
</authors>
<title>Introducing the Enron corpus.</title>
<date>2004</date>
<booktitle>In CEAS.</booktitle>
<contexts>
<context position="3018" citStr="Klimt and Yang, 2004" startWordPosition="485" endWordPosition="488">g errors involved include omission of a character (e.g., atachment), inclusion of a redundant character (e.g., attachement), and replacement of characters (e.g., attachemnt). The fact that w&apos; is a misspelling of (and should be corrected to) w is denoted by w&apos; → w (e.g., atachment → attachment). Additional common spelling errors are splitting of a word, and merging two (or more) words: • attach ment → attachment • emailattachment → email attachment Part of our experiments, as well as most of our examples, are from the domain of (personal) email search. An email from the Enron email collection (Klimt and Yang, 2004) is shown in Figure 1. Our running example is the following misspelling of a search query, involving multiple types of errors. sadeep kohli excellatach ment → sandeep kohli excel attachment (1) In this example, correction entails fixing sadeep, splitting excellatach, fixing excell, merging atach ment, and fixing atachment. Beyond the complexity of errors, this example also illustrates other challenges in spelling correction for search. We need to identify not only that sadeep is misspelled, but also that kohli is correctly spelled. Just having kohli in a dictionary is not enough. 905 Proceedin</context>
<context position="23991" citStr="Klimt and Yang, 2004" startWordPosition="4200" endWordPosition="4203"> Study Our experimental study aims to investigate the effectiveness of our approach in various settings, as we explain next. 3.1 Experimental Setup We first describe our experimental setup, and specifically the datasets and general methodology. Datasets. The focus of our experimental study is on personal email search; later on (Section 3.6), we will consider (and give experimental results for) a totally different setting—site search over www. ibm.com, which is a massive and open domain. Our dataset (for the email domain) is obtained from 910 the Enron email collection (Bekkerman et al., 2004; Klimt and Yang, 2004). Specifically, we chose the three users with the largest number of emails. We refer to the three email collections by the last names of their owners: Farmer, Kaminski and Kitchen. Each user mailbox is a separate domain, with a separate corpus D, that one can search upon. Due to the absence of real user queries, we constructed our dataset by conducting a user study, as described next. For each user, we randomly sampled 50 emails and divided them into 5 disjoint sets of 10 emails each. We gave each 10-email set to a unique human subject that was asked to phrase two search queries for each email</context>
</contexts>
<marker>Klimt, Yang, 2004</marker>
<rawString>B. Klimt and Y. Yang. 2004. Introducing the Enron corpus. In CEAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Comput. Surv.,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>439</pages>
<contexts>
<context position="1564" citStr="Kukich, 1992" startWordPosition="241" endWordPosition="242">periority of the algorithm over existing alternatives in the email domain. 1 Introduction An abundance of applications require spelling correction, which (at the high level) is the following task. The user intends to type a chunk q of text, but types instead the chunk s that contains spelling errors (which we discuss in detail later), due to uncareful typing or lack of knowledge of the exact spelling of q. The goal is to restore q, when given s. Spelling correction has been extensively studied in the literature, and we refer the reader to comprehensive summaries of prior work (Peterson, 1980; Kukich, 1992; Jurafsky and Martin, 2000; Mitton, 2010). The focus of this paper is on the special case where q is a search query, and where s instead of q is submitted to a search engine (with the goal of retrieving documents that match the search query q). Spelling correction for search queries is important, because a significant portion of posed queries may be misspelled (Cucerzan and Brill, 2004). Effective spelling correction has a major effect on the experience and effort of the user, who is otherwise required to ensure the exact spellings of her queries. Furthermore, it is critical when the exact sp</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>K. Kukich. 1992. Techniques for automatically correcting words in text. ACM Comput. Surv., 24(4):377– 439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>M Zhu</author>
<author>Y Zhang</author>
<author>M Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spelling correction.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8055" citStr="Li et al., 2006" startWordPosition="1296" endWordPosition="1299">ent due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli —* sandeep kohli (rather than, e.g., sadeep kohli —* sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell —* excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 s of a user who intends to phrase q, the goal is to Table 1: Feature set WFD in email search find q. Our corpus is essentially a collection D of unstructured or semistructured documents. For example, in email search such a document is an email with a title, a body, one or more recipients, and so on. As conventional in spelling correction, we devise a scoring function scoreD(r |s) that estimates our confidence in r being the correction of s (i.e., that r is equal to q). Even</context>
</contexts>
<marker>Li, Zhu, Zhang, Zhou, 2006</marker>
<rawString>M. Li, M. Zhu, Y. Zhang, and M. Zhou. 2006a. Exploring distributional similarity based models for query spelling correction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>R Krishnamurthy</author>
<author>S Vaithyanathan</author>
<author>H V Jagadish</author>
</authors>
<title>Getting work done on the web: supporting transactional queries.</title>
<date>2006</date>
<booktitle>In SIGIR,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="8055" citStr="Li et al., 2006" startWordPosition="1296" endWordPosition="1299">ent due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli —* sandeep kohli (rather than, e.g., sadeep kohli —* sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell —* excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 s of a user who intends to phrase q, the goal is to Table 1: Feature set WFD in email search find q. Our corpus is essentially a collection D of unstructured or semistructured documents. For example, in email search such a document is an email with a title, a body, one or more recipients, and so on. As conventional in spelling correction, we devise a scoring function scoreD(r |s) that estimates our confidence in r being the correction of s (i.e., that r is equal to q). Even</context>
</contexts>
<marker>Li, Krishnamurthy, Vaithyanathan, Jagadish, 2006</marker>
<rawString>Y. Li, R. Krishnamurthy, S. Vaithyanathan, and H. V. Jagadish. 2006b. Getting work done on the web: supporting transactional queries. In SIGIR, pages 557– 564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitton</author>
</authors>
<title>Fifty years of spellchecking.</title>
<date>2010</date>
<journal>Wring Systems Research,</journal>
<pages>2--1</pages>
<contexts>
<context position="1606" citStr="Mitton, 2010" startWordPosition="247" endWordPosition="248">alternatives in the email domain. 1 Introduction An abundance of applications require spelling correction, which (at the high level) is the following task. The user intends to type a chunk q of text, but types instead the chunk s that contains spelling errors (which we discuss in detail later), due to uncareful typing or lack of knowledge of the exact spelling of q. The goal is to restore q, when given s. Spelling correction has been extensively studied in the literature, and we refer the reader to comprehensive summaries of prior work (Peterson, 1980; Kukich, 1992; Jurafsky and Martin, 2000; Mitton, 2010). The focus of this paper is on the special case where q is a search query, and where s instead of q is submitted to a search engine (with the goal of retrieving documents that match the search query q). Spelling correction for search queries is important, because a significant portion of posed queries may be misspelled (Cucerzan and Brill, 2004). Effective spelling correction has a major effect on the experience and effort of the user, who is otherwise required to ensure the exact spellings of her queries. Furthermore, it is critical when the exact spelling is unknown (e.g., person names like</context>
<context position="7216" citStr="Mitton, 2010" startWordPosition="1156" endWordPosition="1157">Sandeep Kohli has nothing to do with coupons (in contrast to the store chain Kohl’s). A similar example is the word nail, which is a legitimate English word, but in the context of email the query nail box is likely to be a misspelling of mail box (unless nail boxes are indeed relevant to the user’s email collection). Finally, while the word kohli is relevant to some email users (e.g., Kohli’s colleagues), it may have no meaning at all to other users. 1.2 Domain Knowledge The common approach to spelling correction utilizes statistical information (Kernighan et al., 1990; Schierle et al., 2007; Mitton, 2010). As a simple example, if we want to avoid maintaining a manually-crafted dictionary to accommodate the wealth of new terms introduced every day (e.g., ipod and ipad), we may decide that atachment is a misspelling of attachment due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli —* sandeep kohli (rather than, e.g., sadeep kohli —* sudeep kohli). One can further note that, in email search,</context>
</contexts>
<marker>Mitton, 2010</marker>
<rawString>R. Mitton. 2010. Fifty years of spellchecking. Wring Systems Research, 2:1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Peterson</author>
</authors>
<title>Computer Programs for Spelling Correction: An Experiment in Program Design,</title>
<date>1980</date>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>96</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="1550" citStr="Peterson, 1980" startWordPosition="239" endWordPosition="240">udy shows the superiority of the algorithm over existing alternatives in the email domain. 1 Introduction An abundance of applications require spelling correction, which (at the high level) is the following task. The user intends to type a chunk q of text, but types instead the chunk s that contains spelling errors (which we discuss in detail later), due to uncareful typing or lack of knowledge of the exact spelling of q. The goal is to restore q, when given s. Spelling correction has been extensively studied in the literature, and we refer the reader to comprehensive summaries of prior work (Peterson, 1980; Kukich, 1992; Jurafsky and Martin, 2000; Mitton, 2010). The focus of this paper is on the special case where q is a search query, and where s instead of q is submitted to a search engine (with the goal of retrieving documents that match the search query q). Spelling correction for search queries is important, because a significant portion of posed queries may be misspelled (Cucerzan and Brill, 2004). Effective spelling correction has a major effect on the experience and effort of the user, who is otherwise required to ensure the exact spellings of her queries. Furthermore, it is critical whe</context>
</contexts>
<marker>Peterson, 1980</marker>
<rawString>J. L. Peterson. 1980. Computer Programs for Spelling Correction: An Experiment in Program Design, volume 96 of Lecture Notes in Computer Science. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schaback</author>
<author>F Li</author>
</authors>
<title>Multi-level feature extraction for spelling correction. In</title>
<date>2007</date>
<booktitle>AND,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="11752" citStr="Schaback and Li (2007)" startWordPosition="1965" endWordPosition="1968">w&apos; |w) is simply a linear combination of the scoref(w&apos; |w): �scoreD(w&apos; |w) def = af · scoref(w&apos; |w) fEWFD As a concrete example, the features of WFD we used in the email domain are listed in Table 1; the resulting scoref(w&apos;|w) is in the spirit of the noisy channel model (Kernighan et al., 1990). Note that additional features could be used, like ones involving the stems of w and w&apos;, and even query-log statistics (when available). Rather than manually tuning the parameters af, we learned them using the well known Support Vector Machine, abbreviated SVM (Cortes and Vapnik, 1995), as also done by Schaback and Li (2007) for spelling correction. We further discuss this learning step in Section 3. We fix a natural number k, and in the sequel we denote by topD(w) a set of k words w&apos; E CD(w) with the highest scoreD(w&apos; |w). If |CD(w) |&lt; k, then topD(w) is simply CD(w). 2.2 Query-Level Correction: MaxPaths We now describe our algorithm, MaxPaths, for spelling correction. The input is a (possibly misspelled) search query s = s1, ... , sn. As done in the word-level correction, the algorithm produces a set CD(s) of suggestions and determines the values Algorithm 1 MaxPaths of s, or there is a word w&apos; ∈ CD(w) (as defi</context>
</contexts>
<marker>Schaback, Li, 2007</marker>
<rawString>J. Schaback and F. Li. 2007. Multi-level feature extraction for spelling correction. In AND, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schierle</author>
<author>S Schulz</author>
<author>M Ackermann</author>
</authors>
<title>From spelling correction to text cleaning - using context information.</title>
<date>2007</date>
<booktitle>In GfKl, Studies in Classification, Data Analysis, and Knowledge Organization,</booktitle>
<pages>397--404</pages>
<contexts>
<context position="5048" citStr="Schierle et al., 2007" startWordPosition="801" endWordPosition="804">relevant to another. The focus of this paper is on spelling correction for search applications like the above, where querylog analysis is impossible or undesirable (with email search being a prominent example). Our approach relies mainly on the corpus data (e.g., the collection of emails of the user at hand) and external, generic dictionaries (e.g., English). As shown in Figure 1, the corpus data may very well contain misspelled words (like query logs do), and such noise is a part of the challenge. Relying on the corpus has been shown to be successful in spelling correction for text cleaning (Schierle et al., 2007). Nevertheless, as we later explain, our approach can still incorporate query-log data as features involved in the correction, as well as means to refine the parameters. 1.3 Contribution and Outline As said above, our goal is to devise spelling correction that relies on the corpus. The corpus often contains various types of information, with different levels of reliability (e.g., n-grams from email subjects and sender information, vs. those from email bodies). The major question is how to effectively exploit that information while addressing the various types of spelling errors such as those d</context>
<context position="7201" citStr="Schierle et al., 2007" startWordPosition="1152" endWordPosition="1155"> mean kohls coupons if Sandeep Kohli has nothing to do with coupons (in contrast to the store chain Kohl’s). A similar example is the word nail, which is a legitimate English word, but in the context of email the query nail box is likely to be a misspelling of mail box (unless nail boxes are indeed relevant to the user’s email collection). Finally, while the word kohli is relevant to some email users (e.g., Kohli’s colleagues), it may have no meaning at all to other users. 1.2 Domain Knowledge The common approach to spelling correction utilizes statistical information (Kernighan et al., 1990; Schierle et al., 2007; Mitton, 2010). As a simple example, if we want to avoid maintaining a manually-crafted dictionary to accommodate the wealth of new terms introduced every day (e.g., ipod and ipad), we may decide that atachment is a misspelling of attachment due to both the (relative) proximity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli —* sandeep kohli (rather than, e.g., sadeep kohli —* sudeep kohli). One can further note that, i</context>
<context position="19794" citStr="Schierle et al., 2007" startWordPosition="3473" endWordPosition="3476">2.2.4 Word Correlation To compute scoreD(r|s) for r E CD(s), we incorporate correlation among the words of r. Intuitively, we would like to reward a candidate with pairs of words that are likely to co-exist in a query. For that, we assume a (symmetric) numerical function crl(w01, w02) that estimates the extent to which the words w01 and w02 are correlated. As an example, in the email domain we would like crl(kohli, excel) to be high if Kohli sent many emails with excel attachments. Our implementation of crl(w01, w02) essentially employs pointwise mutual information that has also been used in (Schierle et al., 2007), and that ment P1 P2 excellent jaideep sandeep kohli kohli excel attachment 909 Table 2: topD(w) for sp-tokens w sadeep kohli excellatach ment excell atachment sandeep kohli excellent ment excel attachment jaideep excellence sent excell attached meet except attachement compares the number of documents (emails) containing w01 and w02 separately and jointly. Let P E topPathsD(s) be a path. We denote by crl(P) a function that aggregates the numbers crl(w01, w02) for nodes (w1, w01) and (w2, w02) of P (where (w1, w01) and (w2, w02) are not necessarily neighbors in P). Over the email domain, our c</context>
</contexts>
<marker>Schierle, Schulz, Ackermann, 2007</marker>
<rawString>M. Schierle, S. Schulz, and M. Ackermann. 2007. From spelling correction to text cleaning - using context information. In GfKl, Studies in Classification, Data Analysis, and Knowledge Organization, pages 397– 404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seobook</author>
</authors>
<date>2010</date>
<note>Keyword typo generator. http://tools.seobook.com/spelling/keywordstypos.cgi.</note>
<contexts>
<context position="25124" citStr="Seobook, 2010" startWordPosition="4398" endWordPosition="4399">a unique human subject that was asked to phrase two search queries for each email: one for the entire email content (general query), and the other for the From and X-From fields (sender query). (Figure 1 shows examples of the From and X-From fields.) The latter represents queries posed against a specific field (e.g., using “advanced search”). The participants were not told about the goal of this study (i.e., spelling correction), and the collected queries have no spelling errors. For generating spelling errors, we implemented a typo generator.1 This generator extends an online typo generator (Seobook, 2010) that produces a variety of spelling errors, including skipped letter, doubled letter, reversed letter, skipped space (merge), missed key and inserted key; in addition, our generator produces inserted space (split). When applied to a search query, our generator adds random typos to each word, independently, with a specified probability p that is 50% by default. For each collected query (and for each considered value of p) we generated 5 misspelled queries, and thereby obtained 250 instances of misspelled general queries and 250 instances of misspelled sender queries. Methodology. We compared t</context>
</contexts>
<marker>Seobook, 2010</marker>
<rawString>Seobook. 2010. Keyword typo generator. http://tools.seobook.com/spelling/keywordstypos.cgi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>J Gao</author>
<author>D Micol</author>
<author>C Quirk</author>
</authors>
<title>Learning phrase-based spelling error models from clickthrough data. In</title>
<date>2010</date>
<booktitle>ACL,</booktitle>
<pages>266--274</pages>
<contexts>
<context position="8094" citStr="Sun et al., 2010" startWordPosition="1304" endWordPosition="1307">ity between the words, and the fact that attachment is significantly more popular than atachment. As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in sadeep kohli —* sandeep kohli (rather than, e.g., sadeep kohli —* sudeep kohli). One can further note that, in email search, the fact that Sandeep Kohli sent multiple excel attachments increases our confidence in excell —* excel. A source of statistics widely used in prior work is the query log (Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006a; Chen et al., 2007; Sun et al., 2010). However, while query logs are abundant in the context of Web search, in many 906 s of a user who intends to phrase q, the goal is to Table 1: Feature set WFD in email search find q. Our corpus is essentially a collection D of unstructured or semistructured documents. For example, in email search such a document is an email with a title, a body, one or more recipients, and so on. As conventional in spelling correction, we devise a scoring function scoreD(r |s) that estimates our confidence in r being the correction of s (i.e., that r is equal to q). Eventually, we suggest a sequence r from a </context>
</contexts>
<marker>Sun, Gao, Micol, Quirk, 2010</marker>
<rawString>X. Sun, J. Gao, D. Micol, and C. Quirk. 2010. Learning phrase-based spelling error models from clickthrough data. In ACL, pages 266–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhu</author>
<author>S Raghavan</author>
<author>S Vaithyanathan</author>
<author>A L¨oser</author>
</authors>
<title>Navigating the intranet with high precision. In WWW,</title>
<date>2007</date>
<pages>491--500</pages>
<marker>Zhu, Raghavan, Vaithyanathan, L¨oser, 2007</marker>
<rawString>H. Zhu, S. Raghavan, S. Vaithyanathan, and A. L¨oser. 2007. Navigating the intranet with high precision. In WWW, pages 491–500.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>