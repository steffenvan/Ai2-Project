<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.963194">
Joint Inference for Bilingual Semantic Role Labeling
</title>
<author confidence="0.994404">
Tao Zhuang and Chengqing Zong
</author>
<affiliation confidence="0.9960985">
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
</affiliation>
<email confidence="0.994476">
{tzhuang, cqzong}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.996606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853">
We show that jointly performing semantic role
labeling (SRL) on bitext can improve SRL
results on both sides. In our approach, we
use monolingual SRL systems to produce ar-
gument candidates for predicates in bitext at
first. Then, we simultaneously generate SRL
results for two sides of bitext using our joint
inference model. Our model prefers the bilin-
gual SRL result that is not only reasonable on
each side of bitext, but also has more consis-
tent argument structures between two sides.
To evaluate the consistency between two argu-
ment structures, we also formulate a log-linear
model to compute the probability of aligning
two arguments. We have experimented with
our model on Chinese-English parallel Prop-
Bank data. Using our joint inference model,
F1 scores of SRL results on Chinese and En-
glish text achieve 79.53% and 77.87% respec-
tively, which are 1.52 and 1.74 points higher
than the results of baseline monolingual SRL
combination systems respectively.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990353659090909">
In recent years, there has been an increasing inter-
est in SRL on several languages. However, little
research has been done on how to effectively per-
form SRL on bitext, which has important applica-
tions including machine translation (Wu and Fung,
2009). A conventional way to perform SRL on bi-
text is performing SRL on each side of bitext sep-
arately, as has been done by Fung et al. (2007) on
Chinese-English bitext. However, it is very difficult
to obtain good SRL results on both sides of bitext
in this way. The reason is that even the state-of-
the-art SRL systems do not have very high accuracy
on both English text (M`arquez et al., 2008; Pradhan
et al., 2008; Punyakanok et al., 2008; Toutanova et
al., 2008), and Chinese text (Che et al., 2008; Xue,
2008; Li et al., 2009; Sun et al., 2009).
On the other hand, the semantic equivalence be-
tween two sides of bitext means that they should
have consistent predicate-argument structures. This
bilingual argument structure consistency can guide
us to find better SRL results. For example, in Fig-
ure 1(a), the argument structure consistency can
guide us to choose a correct SRL result on Chinese
side. Consistency between two argument structures
is reflected by sound argument alignments between
them, as shown in Figure 1(b). Previous research has
shown that bilingual constraints can be very help-
ful for parsing (Burkett and Klein, 2008; Huang et
al., 2008). In this paper, we show that the bilingual
argument structure consistency can be leveraged to
substantially improve SRL results on both sides of
bitext.
Formally, we present a joint inference model to
preform bilingual SRL. Using automatic word align-
ment on bitext, we first identify a pair of predicates
that align with each other. And we use monolin-
gual SRL systems to produce argument candidates
for each predicate. Then, our model jointly generate
SRL results for both predicates from their argument
candidates, using integer linear programming (ILP)
technique. An overview of our approach is shown in
Figure 2.
Our joint inference model consists of three com-
ponents: the source side, the target side, and the ar-
</bodyText>
<page confidence="0.985848">
304
</page>
<note confidence="0.8525795">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 304–314,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.9818945">
R1: [ A1 ] [ AM-TMP ] [ C-A1 ] [ AM-ADV ] [Pred]
R2: [ A1 ] [ AM-ADV ] [Pred]
中国 建筑 市场 近年 来 对 外 开放 步伐 进一步 加快
zhongguo jianzhu shichang jinnian lai dui wai kaifang bufa jinyibu jiakuai
In recent years the pace of opening up to the outside of China `s construction market has further accelerated
[ AM-TMP ] [ A1 ] [ A2 ] [ Pred ]
(a) Word alignment and SRL results for a Chinese-English predicate pair.
中国 建筑 市场 近年 来 对 外 开放 步伐 进一步 加快
[ A1 ] [ AM-TMP ] [ C-A1 ] [AM-ADV] [Pred]
[ AM-TMP ] [ A1 ] [ A2 ] [ Pred ]
In recent years the pace of opening up to the outside of China `s construction market has further accelerated
(b) Argument alignments for a Chinese-English predicate pair.
</figure>
<figureCaption confidence="0.998152428571428">
Figure 1: An example from Chinese-English parallel PropBank. In (a), the SRL results are generated by the state-
of-the-art monolingual SRL systems. The English SRL result is correct. But it is to more difficult to get correct
SRL result on Chinese side, because the AM-TMP argument embeds into a discontinuous A1 argument. The Chinese
SRL result in the row marked by ‘R1’ is correct and consistent with the result on English side. Whereas the result in
the row marked by ‘R2’ is incorrect and inconsistent with the result on English side, with the circles showing their
inconsistency. The argument structure consistency can guide us to choose the correct Chinese SRL result.
Figure 2: Overview of our approach.
</figureCaption>
<bodyText confidence="0.999889578947369">
gument alignment between two sides. These three
components correspond to three interrelated factors:
the quality of the SRL result on source side, the qual-
ity of the SRL result on target side, and the argu-
ment structure consistency between the SRL results
on both sides. To evaluate the consistency between
the two argument structures in our joint inference
model, we formulate a log-linear model to compute
the probability of aligning two arguments. Experi-
ments on Chinese-English parallel PropBank shows
that our model significantly outperforms monolin-
gual SRL combination systems on both Chinese and
English sides.
The rest of this paper is organized as follows: Sec-
tion 2 introduces related work. Section 3 describes
how we generate SRL candidates on each side of bi-
text. Section 4 presents our joint inference model.
Section 5 presents our experiments. And Section 6
concludes our work.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999882807692308">
Some existing work on monolingual SRL combina-
tion is related to our work. Punyakanok et al. (2004;
2008) formulated an ILP model for SRL. Koomen
et al. (2005) combined several SRL outputs using
ILP method. M`arquez et al. (2005) and Pradhan et
al. (2005) proposed combination strategies that are
not based on ILP method. Surdeanu et al. (2007)
did a complete research on a variety of combination
strategies. Zhuang and Zong (2010) proposed a min-
imum error weighting combination strategy for Chi-
nese SRL combination.
Research on SRL utilizing parallel corpus is also
related to our work. Pad´o and Lapata (2009) did
research on cross-lingual annotation projection on
English-German parallel corpus. They performed
SRL only on the English side, and then mapped
the English SRL result to German side. Fung et
al. (2007) did pioneering work on studying argu-
ment alignment on Chinese-English parallel Prop-
Bank. They performed SRL on Chinese and En-
glish sides separately. Then, given the SRL result
on both sides, they automatically induced the argu-
ment alignment between two sides.
The major difference between our work and all
existing research is that our model performs SRL in-
ference on two sides of bitext simultaneously. In our
</bodyText>
<figure confidence="0.998897529411765">
Monolingual
SRL System
Source-side
SRL
Candidates
Source-side
Predidate
Our Joint
Inference
Model
Target side
Predicate
Monolingual
SRL System
Target-side
SRL Bilingual
Candidates SRL Result
</figure>
<page confidence="0.998179">
305
</page>
<bodyText confidence="0.999909333333333">
model, we jointly consider three interrelated factors:
SRL result on the source side, SRL result on the tar-
get side, and the argument alignment between them.
</bodyText>
<sectionHeader confidence="0.96397" genericHeader="method">
3 Generating Candidates for Inference
</sectionHeader>
<subsectionHeader confidence="0.999147">
3.1 Monolingual SRL System
</subsectionHeader>
<bodyText confidence="0.999985772727273">
As shown in Figure 2, we need to use a monolin-
gual SRL system to generate candidates for our joint
inference model. We have implemented a monolin-
gual SRL system which utilize full phrase-structure
parse trees to perform SRL. In this system, the whole
SRL process is comprised of three stages: pruning,
argument identification, and argument classification.
In the pruning stage, the heuristic pruning method
in (Xue, 2008) is employed. In the argument iden-
tification stage, a number of argument locations are
identified in a sentence. In the argument classifica-
tion stage, each location identified in the previous
stage is assigned a semantic role label. Maximum
entropy classifier is employed for both the argument
identification and classification tasks. And Zhang
Le’s MaxEnt toolkit1 is used for implementation.
We use the monolingual SRL system described
above for both Chinese and English SRL tasks. For
the Chinese SRL task, the features used in this paper
are the same with those used in (Xue, 2008). For
the English SRL task, the features used are the same
with those used in (Pradhan et al., 2008).
</bodyText>
<subsectionHeader confidence="0.99906">
3.2 Output of the Monolingual SRL System
</subsectionHeader>
<bodyText confidence="0.999972230769231">
The maximum entropy classifier in our monolingual
SRL system can output classification probabilities.
We use the classification probability of the argument
classification stage as an argument’s probability. As
illustrated in Figure 3, in an individual system’s out-
put, each argument has three attributes: its location
in sentence loc, represented by the number of its first
word and last word; its semantic role label l; and its
probability p.
So each argument outputted by a system is a triple
(loc, l, p). For example, the A0 argument in Figure 3
is ((0, 2), A0, 0.94). Because these outputs are to be
combined, we call such triple a candidate.
</bodyText>
<footnote confidence="0.994253">
1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit
.html
</footnote>
<table confidence="0.9934186">
Sent: *-A Rif ^1L A)� +[A *,N X-5 *Y,-A
Args: [ A0 ] [Pred] [ A1 ]
loc: (0, 2) (4, 7)
l: A0 A1
p: 0.94 0.92
</table>
<figureCaption confidence="0.987484">
Figure 3: Three attributes of an output argument: location
loc, label l, and probability p.
</figureCaption>
<subsectionHeader confidence="0.999352">
3.3 Generating and Merging Candidates
</subsectionHeader>
<bodyText confidence="0.999772117647059">
To generate candidates for joint inference, we need
to have multiple SRL results on each side of bi-
text. Therefore, for both Chinese and English SRL
systems, we use the 3-best parse trees of Berkeley
parser (Petrov and Klein, 2007) and 1-best parse
trees of Bikel parser (Bikel, 2004) and Stanford
parser (Klein and Manning, 2003) as inputs. All the
three parsers are multilingual parsers. The second
and third best parse trees of Berkeley parser are used
for their good quality. Therefore, each monolingual
SRL system produces 5 different outputs.
Candidates from different outputs may have the
same loc and l but different p. So we merge all
candidates with the same loc and l into one by av-
eraging their probabilities. For a merged candidate
(loc, l, p), we say that p is the probability of assign-
ing l to loc.
</bodyText>
<sectionHeader confidence="0.9975" genericHeader="method">
4 Joint Inference Model
</sectionHeader>
<bodyText confidence="0.9990836">
Our model can be conceptually decomposed to three
components: the source side, the target side, and the
argument alignment. The objective function of our
joint inference model is the weighted sum of three
sub-objectives:
</bodyText>
<equation confidence="0.997046">
max O3 + λ1Ot + λ2Oa (1)
</equation>
<bodyText confidence="0.997727166666667">
where O3 and Ot represent the quality of the SRL
results on source and target sides, and Oa represents
the soundness of the argument alignment between
the SRL results on two sides, λ1, λ2 are positive
weights corresponding to the importance of Ot and
Oa respectively.
</bodyText>
<subsectionHeader confidence="0.997915">
4.1 Components of Source and Target Sides
4.1.1 Source Side Component
</subsectionHeader>
<bodyText confidence="0.999228">
The source side component aims to improve the
SRL result on source side. This is equivalent to a
</bodyText>
<page confidence="0.995717">
306
</page>
<bodyText confidence="0.941556461538462">
monolingual SRL combination problem.
For convenience, we denote the whole semantic
role label set for source language as {ls1, ls2, ... , lsLs},
in which ls1 — ls6 stand for the key argument labels
A0 — A5 respectively. Suppose there are Ns differ-
ent locations, denoted as locs1, ... , locsNs, among all
candidates on the source side. The probability of as-
signing lsj to locsi is psij. An indicator variable xij is
defined as:
xij = [locsi is assigned label lsj].
Then the source side sub-objective Os in equation
(1) is the sum of arguments’ probabilities on source
side:
</bodyText>
<equation confidence="0.992626">
s
pij — Ts)xij (2)
</equation>
<bodyText confidence="0.99450875">
where Ts is a bias to prevent including too many can-
didates in solution (Surdeanu et al., 2007).
We consider the following two linguistically mo-
tivated constraints:
</bodyText>
<listItem confidence="0.967947">
1. No duplication: There is no duplication for key
arguments: A0 — A5.
2. No overlapping: Arguments cannot overlap
with each other.
</listItem>
<bodyText confidence="0.998854">
In (Punyakanok et al., 2004), several more con-
straints are considered. According to (Surdeanu
et al., 2007), however, no significant performance
improvement can be obtained by considering more
constraints than the two above. So we do not con-
sider other constraints.
The inequalities in (3) make sure that each locsi is
assigned at most one label.
</bodyText>
<equation confidence="0.973189333333333">
b1 &lt; i &lt; Ns : XLs xij &lt; 1 (3)
j=1
i.e., PNs
</equation>
<bodyText confidence="0.94640425">
j=1 xuj = 0. A common technique in ILP
modeling to form such a constraint is to use a suf-
ficiently large auxiliary constant M. And the con-
straint is formulated as:
</bodyText>
<equation confidence="0.997353">
b1 &lt; i &lt; Ns : X XLs xuj &lt; (1— XLs xij)M (5)
uECi j=1 j=1
</equation>
<bodyText confidence="0.998252333333333">
In this case, M only needs to be larger than the num-
ber of candidates to be combined. In this paper,
M = 500 is large enough.
</bodyText>
<subsectionHeader confidence="0.688975">
4.1.2 Target Side Component
</subsectionHeader>
<bodyText confidence="0.929046461538462">
In principle, the target side component of our joint
inference model is the same with the source side
component.
The whole semantic role label set for target lan-
guage is denoted by {lt1,lt2,...,ltLt}. There are
Nt different locations, denoted as loct1, ... , loct Nt,
among all candidates in the target side. And lt1 — lt6
stand for the key argument labels A0 — A5 respec-
tively. The probability of assigning ltj to loctk is ptkj.
An indicator variable ykj is defined as:
ykj = [loctk is assigned label ltj].
Then the target side sub-objective Ot in equation (1)
is:
</bodyText>
<equation confidence="0.994645227272727">
XLs
j=1
Os =
XNs
i=1
(ptkj - Tt)ykj (6)
The constraints on target side are as follows:
Each loctk is assigned at most one label:
Lt
b1 &lt; k &lt; Nt : X ykj &lt; 1 (7)
j=1
The No duplication constraint:
XNt
k=1
Ot =
XLt
j=1
The inequalities in (4) satisfy the No duplication b1 &lt; j &lt; 6 : Nt ykj &lt; 1 (8)
constraint. X
k=1
b1 &lt; j &lt; 6 : XNs xij &lt; 1 (4)
i=1
</equation>
<bodyText confidence="0.999938666666667">
For any source side location locsi, let Ci denote
the index set of the locations that overlap with it.
Then the No overlapping constraint means that if
</bodyText>
<equation confidence="0.8538915">
locsi is assigned a label, i.e., PNs
j=1 xij = 1, then
</equation>
<bodyText confidence="0.856692">
for any u E Ci, locsu cannot be assigned any label,
The No overlapping constraint:
</bodyText>
<equation confidence="0.909583666666667">
X
b1 &lt; k &lt; Nt :
vECk
</equation>
<bodyText confidence="0.996698333333333">
In (9), Ck denotes the index set of the locations that
overlap with loctk, and the constant M is set to 500
in this paper.
</bodyText>
<equation confidence="0.985567333333333">
Lt Lt
X yvj &lt; (1— X ykj)M (9)
j=1 j=1
</equation>
<page confidence="0.99309">
307
</page>
<subsectionHeader confidence="0.98944">
4.2 Argument Alignment
</subsectionHeader>
<bodyText confidence="0.993115833333333">
The argument alignment component is the core of
our joint inference model. It gives preference to the
bilingual SRL results that have more consistent ar-
gument structures.
For a source side argument argsi = (locsi, ls) and
a target side argument argtk = (loctk, lt), let zik be
the following indicator variable:
zik = [argsi aligns with argtk].
We use pa ik to represent the probability that argsi and
argtk align with each other, i.e., paik = P(zik = 1).
We call pa ik the argument alignment probability
between argsi and argtk.
</bodyText>
<subsubsectionHeader confidence="0.734918">
4.2.1 Argument Alignment Probability Model
</subsubsectionHeader>
<bodyText confidence="0.985068666666667">
We use a log-linear model to compute the argu-
ment alignment probability pa ik between argsi and
argtk. Let (s, t) denote a bilingual sentence pair and
wa denote the word alignment on (s, t). Our log-
linear model defines a distribution on zik given the
tuple tup = (argsi , argt k, wa, s, t):
</bodyText>
<equation confidence="0.785849">
P(zik|tup) a exp(wTo(tup))
</equation>
<bodyText confidence="0.98581675">
where 0(tup) is the feature vector. With this model,
paik can be computed as pa ik = P(zik = 1|tup).
In order to study the argument alignment in cor-
pus and to provide training data for our log-linear
model, we have manually aligned the arguments in
60 files (chtb 0121.fid to chtb 0180.fid) of Chinese-
English parallel PropBank. On this data set, we get
the argument alignment matrix in Table 1.
</bodyText>
<table confidence="0.999804375">
Ch\En A0 A1 A2 A3 A4 AM* NUL
A0 492 30 4 0 0 0 46
A1 98 853 43 2 0 0 8
A2 9 57 51 1 0 47 0
A3 1 0 2 6 0 0 0
A4 0 0 2 0 3 0 0
AM* 0 2 39 0 0 895 221
NUL 53 14 27 0 0 45 0
</table>
<tableCaption confidence="0.9977075">
Table 1: The argument alignment matrix on manually
aligned corpus.
</tableCaption>
<bodyText confidence="0.999321024390244">
Each entry in Table 1 is the number of times for
which one type of Chinese argument aligns with one
type of English argument. AM* stands for all ad-
juncts types like AM-TMP, AM-LOC, etc., and NUL
means that the argument on the other side cannot be
aligned with any argument on this side. For exam-
ple, the number 46 in the A0 row and NUL column
means that Chinese A0 argument cannot be aligned
with any argument on English side for 46 times in
our manually aligned corpus.
We use the following features in our model.
Word alignment feature: If there are many word-
to-word alignments between argsi and argtk, then
it is very probable that argsi and argtk would align
with each other. We adopt the method used in (Pad´o
and Lapata, 2009) to measure the word-to-word
alignments between argsi and argtk. And the word
alignment feature is defined as same as the word
alignment-based word overlap in (Pad´o and Lapata,
2009). Note that this is a real-valued feature.
Head word alignment feature: The head word
of an argument is usually more representative than
other words. So we use whether the head words of
argsi and argtk align with each other as a binary fea-
ture. The use of this feature is inspired by the work
in (Burkett and Klein, 2008).
Semantic role labels of two arguments: From Ta-
ble 1, we can see that semantic role labels of two ar-
guments are a good indicator of whether they should
align with each other. For example, a Chinese A0
argument aligns with an English A0 argument most
of the times, and never aligns with an English AM*
argument in Table 1. Therefore, the semantic role
labels of argsi and argtk are used as a feature.
Predicate verb pair: Different predicate pairs have
different argument alignment patterns. Let’s take the
Chinese predicate M-c/zengzhang and the English
predicate grow as an example. The argument align-
ment matrix for all instances of the Chinese-English
predicate pair (zengzhang, grow) in our manually
aligned corpus is shown in Table 2.
</bodyText>
<table confidence="0.9114844">
CH \EN A0 A1 A2 AM* NUL
A0 0 16 0 0 0
A1 0 0 12 0 0
AM* 0 0 4 7 10
NUL 0 0 0 2 0
</table>
<tableCaption confidence="0.9798365">
Table 2: The argument alignment matrix for the predicate
pair (zengzhang, grow).
</tableCaption>
<bodyText confidence="0.9705885">
From Table 2 we can see that all A0 arguments of
zengzhang align with A1 arguments of grow. This
</bodyText>
<page confidence="0.995363">
308
</page>
<bodyText confidence="0.999850666666667">
is very different from the results in Table 1, where a
Chinese A0 argument tends to align with an English
A0 argument. This phenomenon shows that a pred-
icate pair can determine which types of arguments
should align with each other. Therefore, we use the
predicate pair as a feature.
</bodyText>
<subsectionHeader confidence="0.465341">
4.2.2 Argument Alignment Component
</subsectionHeader>
<bodyText confidence="0.963725">
The argument alignment sub-objective Oa in
equation (1) is the sum of argument alignment prob-
abilities:
</bodyText>
<equation confidence="0.513466">
�paik − Ta)zik (10)
</equation>
<bodyText confidence="0.9995249">
where Ta is a bias to prevent including too many
alignments in final solution, and paik is computed
using the log-linear model described in subsec-
tion 4.2.1.
Oa reflects the consistency between argument
structures on two sides of bitext. Larger Oa means
better argument alignment between two sides, thus
indicates more consistency between argument struc-
tures on two sides.
The following constraints are considered:
</bodyText>
<listItem confidence="0.987562666666667">
1. Conformity with bilingual SRL result. For
all candidates on both source and target sides, only
those that are chosen to be arguments on each side
can be aligned.
2. One-to-many alignment limit. Each argument
can not be aligned with more than 3 arguments.
3. Complete argument alignment. Each argument
on source side must be aligned with at least one ar-
gument on target side, and vice versa.
</listItem>
<bodyText confidence="0.989277">
The Conformity with bilingual SRL result con-
straint is necessary to validly integrate the bilingual
SRL result with the argument alignment. This con-
straint means that if argsi and argtk align with each
other, i.e., zik = 1, then locsi must be assigned
a label on source side, i.e., PLs
</bodyText>
<equation confidence="0.9877596">
j=1 xij = 1, and
loctk must be assigned a label on target side, i.e.,
PLt
j=1 ykj = 1. So this constraint can be represented
as:
V1 &lt; i &lt; Ns,1 &lt; k &lt; Nt : XLs xij ? zik (11)
j=1
Lt
V1 &lt; k &lt; Nt, 1 &lt; i &lt; Ns : X ykj &gt; zik (12)
j=1
</equation>
<bodyText confidence="0.9990874">
The One-to-many alignment limit constraint
comes from our observation on manually aligned
corpus. We have found that no argument aligns with
more than 3 arguments in our manually aligned cor-
pus. This constraint can be represented as:
</bodyText>
<equation confidence="0.9916212">
Nt
V1 &lt; i &lt; Ns : X zik &lt; 3 (13)
k=1
V1&lt;k&lt;Nt: XNs zik &lt; 3 (14)
i=1
</equation>
<bodyText confidence="0.99845575">
The Complete argument alignment constraint
comes from the semantic equivalence between two
sides of bitext. For each source side location locsi,
if it is assigned a label, i.e., PLs
</bodyText>
<equation confidence="0.756443666666667">
j=1 xij = 1, then it
must be aligned with some arguments on target side,
i.e., PNt
k=1 zik &gt; 1. This can be represented as:
V1 &lt; i &lt; Ns :
Similarly, each target side argument must be aligned
to at least one source side argument. This can be
represented as:
V1&lt;k&lt;Nt:
</equation>
<subsectionHeader confidence="0.8952455">
4.3 Complete Argument Alignment as a Soft
Constraint
</subsectionHeader>
<bodyText confidence="0.999981266666667">
Although the hard Complement argument alignment
constraint is ideally reasonable, in real situations this
constraint does not always hold. The manual argu-
ment alignment result shown in Table 1 indicates
that in some cases an argument cannot be aligned
with any argument on the other side (see the NUL
row and column in Table 1). Therefore, it would
be reasonable to change the hard Complement argu-
ment alignment constraint to a soft one. To do so,
we need to remove the hard Complement argument
alignment constraint and add penalty for violation of
this constraint.
If an argument does not align with any argument
on the other side, we say it aligns with NUL. And we
define the following indicator variables:
</bodyText>
<equation confidence="0.997193578947368">
zi,NUL = [argsi aligns with NUL],1 &lt; i &lt; Ns.
XNt
k=1
Oa =
XNs
i=1
Nt
X
k=1
zik �!
XLs
j=1
xij (15)
ykj (16)
XNs
i=1
zik �!
XLt
j=1
</equation>
<page confidence="0.539559">
309
</page>
<equation confidence="0.8820284">
zNUL,k = [argtk aligns with NUL],1 ≤ k ≤ Nt.
Then PNs
i=1 zi,NUL is the number of source side ar-
guments that align with NUL. And PNt
k=1 zNUL,k is
</equation>
<bodyText confidence="0.9855788">
the number of target side arguments that align with
NUL. For each argument that aligns with NUL, we
add a penalty A3 to the argument alignment sub-
objective Oa. Therefore, the sub-objective Oa in
equation (10) is changed to:
</bodyText>
<equation confidence="0.968239">
zNUL,k) (17)
</equation>
<bodyText confidence="0.987367375">
From the definition of zi,NUL, it is obvious that,
for any 1 ≤ i ≤ Ns, zi,NUL and zik(1 ≤ k ≤ Nt)
have the following relationship: If PNt
k=1 zik ≥ 1,
i.e., argsi aligns with some arguments on target side,
then zi,NUL = 0; Otherwise, zi,NUL = 1. These
relationships can be captured by the following con-
straints:
</bodyText>
<equation confidence="0.9817384">
∀1 ≤ i ≤ Ns,1 ≤ k ≤ Nt : zi,NUL ≤ 1−zik (18)
Nt
∀1 ≤ i ≤ Ns : X zik + zi,NUL ≥ 1 (19)
k=1
Similarly, for any 1 ≤ k ≤ Nt, zNUL,k and
zik(1 ≤ i ≤ Ns) observe the following constraints:
∀1 ≤ k ≤ Nt, 1 ≤ i ≤ Ns : zNUL,k ≤ 1 − zik
(20)
∀1 ≤ k ≤ Nt : XNs zik + zNUL,k ≥ 1 (21)
i=1
</equation>
<subsectionHeader confidence="0.998309">
4.4 Models Summary
</subsectionHeader>
<bodyText confidence="0.99996275">
So far, we have presented two versions of our joint
inference model. The first version treats Comple-
ment argument alignment as a hard constraint. We
will refer to this version as Joint]. The objective
function of Joint] is defined by equations (1, 2, 6,
10). And the constraints of Joint] are defined by
equations (3-5, 7-9, 11-16).
The sencond version treats Complement argument
alignment as a soft constraint. We will refer to this
version as Joint2. The objective function of Joint2
is defined by equations (1, 2, 6, 17). And the con-
straints of Joint2 are defined by equations (3-5, 7-9,
11-14, 18-21).
Our baseline models are monolingual SRL com-
bination models. We will refer to the source side
combination model as SrcCmb. The objective of Sr-
cCmb is to maximize Os, which is defined in equa-
tion (2). And the constraints of SrcCmb are defined
by equations (3-5). Similarly, we will refer to the tar-
get side combination model as TrgCmb. The objec-
tive of TrgCmb is to maximize Ot defined in equa-
tion (6). And the constraints of TrgCmb are defined
by equations (7-9). In this paper, we employ lp-
solve2 to solve all ILP models.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996688">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999951346153846">
In our experiments, we use the Xinhua News por-
tion of Chinese and English data in LDC OntoNotes
Release 3.0. This data is a Chinese-English parallel
proposition bank described in (Palmer et al., 2005).
It contains parallel proposition annotations for 325
files (chtb 0001.fid to chtb 0325.fid) from Chinese-
English parallel Treebank. The English part of this
data contains proposition annotations only for ver-
bal predicates. Therefore, we only consider verbal
predicates in this paper.
We employ the GIZA++ toolkit (Och and Ney,
2003) to perform automatic word alignment. Be-
sides the parallel PropBank data, we use additional
4,500K Chinese-English sentence pairs3 to induce
word alignments for both directions, with the default
GIZA++ settings. The alignments are symmetrized
using the intersection heuristic (Och and Ney, 2003),
which is known to produce high-precision align-
ments.
We use 80 files (chtb 0001.fid to chtb 0080.fid)
as test data, and 40 files (chtb 0081.fid to
chtb 0120.fid) as development data. Although our
joint inference model needs no training, we still
need to train a log-linear argument alignment prob-
ability model, which is used in the joint inference
model. As specified in subsection 4.2.1, the train-
</bodyText>
<footnote confidence="0.98106275">
2http://lpsolve.sourceforge.net/
3These data includes the following LDC corpus:
LDC2002E18, LDC2003E07, LDC2003E14, LDC2005T06,
LDC2004T07, LDC2000T50.
</footnote>
<equation confidence="0.5832206">
a
pik − Ta)zik
−A3( XNs zi,NUL + Nt
i=1 X
k=1
XNs
i=1
Oa =
XNt
k=1
</equation>
<page confidence="0.990882">
310
</page>
<bodyText confidence="0.996082846153846">
ing set for the argument alignment probability model
consists of 60 files (chtb 0121.fid to chtb 0180.fid)
with manual argument alignment. Unfortunately,
the quality of automatic word alignment on one-
to-many Chinese-English sentence pairs is usually
very poor. So we only include one-to-one Chinese-
English sentence pairs in all data. And not all predi-
cates in a sentence pair can be included. Only bilin-
gual predicate pairs are included. A bilingual pred-
icate pair is defined to be a pair of predicates in bi-
text which align with each other in automatic word
alignment. Table 3 shows how many sentences and
predicates are included in each data set.
</bodyText>
<table confidence="0.999326375">
Test Dev Train
Articles 1-80 81-120 121-180
Chinese Sentences 1067 578 778
English Sentences 1182 620 828
Bilingual pairs 821 448 614
Chinese Predicates 3792 2042 2572
English Predicates 2864 1647 1860
Bilingual pairs 1476 790 982
</table>
<tableCaption confidence="0.99959">
Table 3: Sentence and predicate counts.
</tableCaption>
<bodyText confidence="0.999765391304348">
Our monolingual SRL systems are trained sep-
arately. Our Chinese SRL system is trained on
640 files (chtb 0121.fid to chtb 0931.fid) in Chinese
Propbank 1.0. Because Xinhua News is a quite dif-
ferent domain from WSJ, the training set for our En-
glish SRL system includes not only Sections 02∼21
of WSJ data in English Propbank, but also 205 files
(chtb 0121.fid to chtb 0325.fid) in the English part
of parallel PropBank. For Chinese, the syntactic
parsers are trained on 640 files (chtb 0121.fid to
chtb 0931.fid) plus the broadcast news portion of
Chinese Treebank 6.0. For English, the syntactic
parsers are trained on the following data: Sections
02∼21 of WSJ data in English Treebank, 205 files
(chtb 0121.fid to chtb 0325.fid) of Xinhua News
data in OntoNotes 3.0, and the Sinorama data in
OntoNotes 3.0. We treat discontinuous and corefer-
ential arguments in accordance to the CoNLL-2005
shared task (Carreras and M`arquez, 2005). The first
part of a discontinuous argument is labeled as it is,
and the second part is labeled with a prefix “C-”.
All coreferential arguments are labeled with a prefix
“R-”.
</bodyText>
<subsectionHeader confidence="0.999819">
5.2 Tuning Parameters in Models
</subsectionHeader>
<bodyText confidence="0.999885136363637">
The models Joint], Joint2, SrcCmb, and TrgCmb
have different parameters. For each model, we have
automatically tuned its parameters on development
set using Powell’s Mothod (Brent, 1973). Powell’s
Method is a heuristic optimization algorithm that
does not require the objective function to have an ex-
plicit analytical formula. For a monolingual model
like SrcCmb or TrgCmb, our objective is to maxi-
mize the F1 score of the model’s result on develop-
ment set. But a joint model, like Joint] or Joint2,
generates SRL results on both sides of bitext. So
our objective is to maximize the sum of the two F1
scores of the model’s results for both Chinese and
English on development set. For all models, we re-
gard the parameters to be tuned as variables. Then
we optimize our objective using Powell’s Method.
The solution of this optimization is the values of pa-
rameters. To avoid finding poor local optimum, we
perform the optimization 30 times with different ini-
tial parameter values, and choose the best solution
found. The final parameter values are listed in Ta-
ble 4.
</bodyText>
<table confidence="0.9969246">
Model T3 Tt Ta A1 A2 As
SrcCmb 0.21 - - - - -
TrgCmb - 0.32 - - - -
Joint] 0.17 0.22 0.36 0.96 1.04 -
Joint2 0.15 0.26 0.42 1.02 1.21 0.15
</table>
<tableCaption confidence="0.999585">
Table 4: Parameter values in models.
</tableCaption>
<subsectionHeader confidence="0.910494">
5.3 Individual SRL Outputs’ Performance
</subsectionHeader>
<bodyText confidence="0.998993384615385">
As specified in subsection 3.3, the monoligual SRL
system uses different parse trees to generate multi-
ple SRL outputs. The performance of these outputs
on test set is shown in Table 5. In Table 5, O1-O3
are the outputs using 3-best parse trees of Berkeley
parser respectively, O4 and O5 are the outputs us-
ing the best parse trees of Stanford parser and Bikel
parser respectively.
As specified in subsection 5.1, only a small part
of English SRL training data is in the same domain
with test data. Therefore, the English SRL result in
Table 5 is not very impressive. But the Chinese SRL
result is pretty good.
</bodyText>
<page confidence="0.994914">
311
</page>
<table confidence="0.999632727272727">
Side Outputs P(%) R(%) Fl
O1 79.84 71.95 75.69
O2 78.53 70.32 74.20
Chinese O3 78.41 69.99 73.96
O4 73.21 67.13 70.04
O5 75.32 63.78 69.07
O1 77.13 70.42 73.62
O2 75.88 69.06 72.31
English O3 75.74 68.65 72.02
O4 71.57 66.11 68.73
O5 73.12 68.04 70.49
</table>
<tableCaption confidence="0.999168">
Table 5: The results of individual monolingual SRL out-
puts on test set.
</tableCaption>
<subsectionHeader confidence="0.975572">
5.4 Effects of Different Constraints
</subsectionHeader>
<bodyText confidence="0.999956764705882">
The One-to-many limit and Complete argument
alignment constraints in subsection 4.2.2 comes
from our empirical knowledge. To investigate the
effect of these two constraits, we remove them from
our joint inference models one by one, and observe
the performance variations on test set. The results
are shown in Table 6. In Table 6, ‘c2’ refers to the
One-to-many limit constraint, ‘c3’ refers to the Com-
plete argument alignment constraint, and ‘-’ means
removing. For example, ‘Joint] - c2’ means remov-
ing the constraint ‘c2’ from the model Joint]. Recall
that the only difference between Joint] and Joint2 is
that ‘c3’ is a hard constraint in Joint], but a soft con-
straint in Joint2. Therefore, ‘Joint2 - c3’ and ‘Joint2
- c2 - c3’ do not appear in Table 6, because they are
the same with ‘Joint] - c3’ and ‘Joint] - c2 - c3’
respectively.
</bodyText>
<table confidence="0.998643153846154">
Model Side P(%) R(%) Fl
Joint] Chinese 82.95 75.21 78.89
Joint] - c2 81.46 75.97 78.62
Joint] - c3 82.36 74.68 78.33
Joint] - c2 - c3 82.04 74.67 78.18
Joint2 83.35 76.04 79.53
Joint2 - c2 82.41 76.03 79.09
Joint] English 79.38 75.16 77.21
Joint] - c2 78.51 75.22 76.83
Joint] - c3 78.66 74.55 76.55
Joint] - c2 - c3 78.37 74.37 76.32
Joint2 79.64 76.18 77.87
Joint2 - c2 78.41 75.89 77.13
</table>
<tableCaption confidence="0.999618">
Table 6: Results of different joint models on test set.
</tableCaption>
<bodyText confidence="0.9999197">
From Table 6, we can see that the constraints ‘c2’
and ‘c3’ both have positive effect in our joint in-
ference model, because removing any one of them
causes performance degradation. And removing
‘c3’ from Joint] causes more performance degrada-
tion than removing ‘c2’. This means that ‘c3’ plays
a more important role than ‘c2’ in our joint inference
model. Indeed, by treating ‘c3’ as a soft constraint,
the model Joint2 has the best performance on both
sides of bitext.
</bodyText>
<subsectionHeader confidence="0.96832">
5.5 Final Results
</subsectionHeader>
<bodyText confidence="0.999903375">
We use Joint2 as our final joint inference model.
And as specified in subsection 4.4, our baselines are
monolingual SRL combination models: SrcCmb for
Chinese, and TrgCmb for English. Note that SrcCmb
and TrgCmb are basically the same as the state-of-
the-art combination model in (Surdeanu et al., 2007)
with No overlapping and No duplication constraints.
The final results on test set are shown in Table 7.
</bodyText>
<table confidence="0.9985162">
Side Model P(%) R(%) Fl
Chinese SrcCmb 82.58 73.92 78.01
Joint2 83.35 76.04 79.53
English TrgCmb 79.02 73.44 76.13
Joint2 79.64 76.18 77.87
</table>
<tableCaption confidence="0.9843735">
Table 7: Comparison between monolingual combination
model and our joint inference model on test set.
</tableCaption>
<bodyText confidence="0.995228785714286">
From Table 5 and Table 7, we can see that SrcCmb
and TrgCmb improve F1 scores over the best indi-
vidual SRL outputs by 2.32 points and 2.51 points
on Chinese and English seperately. Thus they form
strong baselines for our joint inference model. Even
so, our joint inference model still improves F1 score
over SrcCmb by 1.52 points, and over TrgCmb by
1.74 points.
From Table 7, we can see that, despite only part of
training data for English SRL system is in-domain,
our joint inference model still produces good En-
glish SRL result. And the F1 score of Chinese SRL
result reaches 79.53%, which represents the state-
of-the-art Chinese SRL performance to date.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.991873">
In this paper, we propose a joint inference model
to perform bilingual SRL. Our joint inference
model incorporates not only linguistic constraints on
</bodyText>
<page confidence="0.995449">
312
</page>
<bodyText confidence="0.999934">
source and target sides of bitext, but also the bilin-
gual argument structure consistency requirement on
bitext. Experiments on Chinese-English parallel
PropBank show that our joint inference model is
very effective for bilingual SRL. Compared to state-
of-the-art monolingual SRL combination baselines,
our joint inference model substantially improves
SRL results on both sides of bitext. In fact, the so-
lution of our joint inference model contains not only
the SRL results on bitext, but also the optimal argu-
ment alignment between two sides of bitext. This
makes our model especially suitable for application
in machine translation, which needs to obtain the ar-
gument alignment.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999945">
The research work has been partially funded by
the Natural Science Foundation of China under
Grant No. 60975053 and 60736014, the National
Key Technology R&amp;D Program under Grant No.
2006BAH03B02. We would like to thank Jiajun
Zhang for helpful discussions and the anonymous
reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.99875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999965675324676">
Daniel Bikel. 2004. Intricacies of Collins Parsing Model.
Computational Linguistics, 30(4):480-511.
Richard P. Brent. 1973. Algorithms for Minimization
without Derivatives. Prentice-Hall, Englewood Cliffs,
NJ.
David Burkett, and Dan Klein. 2008. Two Languages
are Better than One (for Syntactic Parsing). In Pro-
ceedings of EMNLP-2008, pages 877-886.
Xavier Carreras, and Llu´ıs M`arquez. 2005. Introduction
to the CoNLL-2005 shared task: semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152-164.
Wanxiang Che, Min Zhang, Ai Ti Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a Hybrid Convo-
lution Tree Kernel for Semantic Role Labeling. ACM
Transactions on Asian Language Information Process-
ing, 2008, 7(4): 1-23.
Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai
Wu. 2007. Learning Bilingual Semantic Frames:
Shallow Semantic Parsing vs. Semantic Role Projec-
tion. In Proceedings of the 11th Conference on The-
oretical and Methodological Issues in Machine Trans-
lation, pages 75-84.
Liang Huang, Wenbin Jiang, Qun Liu. 2009.
Bilingually-Constrained (Monolingual) Shift-Reduce
Parsing. In Proceedings of EMNLP-2009, pages 1222-
1231.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL-2003,
pages 423-430.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-
tau Yih. 2005. Generalized Inference with Multiple
Semantic Role Labeling Systems. In Proceedings of
CoNLL-2005 shared task, pages 181-184.
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,
and Peide Qian. 2009. Improving Nominal SRL in
Chinese Language with Verbal SRL Information and
Automatic Predicate Recognition. In Proceedings of
EMNLP-2009, pages 1280-1288.
Llu´ıs M`arquez, Xavier Carreras, Kenneth C. Litkowski,
Suzanne Stevenson. 2008. Semantic Role Labeling:
An Introduction to the Special Issue. Computational
Linguistics, 34(2):145-159.
Llu´ıs M`arquez, Mihai Surdeanu, Pere Comas, and Jordi
Turmo. 2005. A Robust Combination Strategy for
Semantic Role Labeling. In Proceedings of EMNLP-
2005, pages 644-651.
Frans J. Och, and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19-51.
Sebastian Pad´o, and Mirella Lapata. 2009. Cross-lingual
Annotation Projection of Semantic Roles. Journal of
Artificial Intelligence Research (JAIR), 36:307-340.
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jiny-
ing Chen, Benjamin Snyder. 2005. A Parallel Propo-
sition Bank II for Chinese and English. In Frontiers
in Corpus Annotation, Workshop in conjunction with
ACL-05, pages 61-67.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized parsing. In Proceedings of ACL-
2007, pages 46-54.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, James
H. Martin, and Daniel Jurafsky. 2005. Semantic Role
Labeling Using Different Syntactic Views. In Pro-
ceedings of ACL-2005, pages 581-588.
Sameer S. Pradhan, Wayne Ward, James H. Martin.
2008. Towards Robust Semantic Role Labeling. Com-
putational Linguistics, 34(2):289-310.
Vasin Punyakanok, Dan Roth, Wen-tauYih. 2008. The
Importance of Syntactic Parsing and Inference in Se-
mantic Role Labeling. Computational Linguistics,
34(2):257-287.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic Role Labeling via Integer
Linear Programming Inference. In Proceedings of
COLING-2004, pages 1346-1352.
Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.
2009. Chinese Semantic Role Labeling with Shallow
</reference>
<page confidence="0.992357">
313
</page>
<reference confidence="0.9999438">
Parsing. In Proceedings of EMNLP-2009, pages 1475-
1483.
Mihai Surdeanu, Llu´ıs M`arquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination Strategies for
Semantic Role Labeling. Journal of Artificial Intel-
ligence Research (JAIR), 29:105-151.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Seman-
tic Role Labeling. Computational Linguistics, 34(2):
145-159.
Dekai Wu, and Pascale Fung. 2009. Semantic Roles for
SMT: A Hybrid Two-Pass Model. In Proceedings of
NAACL-2009, pages 13-16.
Nianwen Xue. 2008. Labeling Chinese Predicates with
Semantic Roles. Computational Linguistics, 34(2):
225-255.
Tao Zhuang, and Chengqing Zong. 2010. A Minimum
Error Weighting Combination Strategy for Chinese Se-
mantic Role Labeling. In Proceedings of COLING-
2010, pages 1362-1370.
</reference>
<page confidence="0.999135">
314
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.830984">
<title confidence="0.997936">Joint Inference for Bilingual Semantic Role Labeling</title>
<author confidence="0.939952">Zhuang</author>
<affiliation confidence="0.9554505">National Laboratory of Pattern Institute of Automation, Chinese Academy of</affiliation>
<abstract confidence="0.997882608695652">We show that jointly performing semantic role labeling (SRL) on bitext can improve SRL results on both sides. In our approach, we use monolingual SRL systems to produce argument candidates for predicates in bitext at first. Then, we simultaneously generate SRL results for two sides of bitext using our joint inference model. Our model prefers the bilingual SRL result that is not only reasonable on each side of bitext, but also has more consistent argument structures between two sides. To evaluate the consistency between two argument structures, we also formulate a log-linear model to compute the probability of aligning two arguments. We have experimented with our model on Chinese-English parallel Prop- Bank data. Using our joint inference model, F1 scores of SRL results on Chinese and Entext achieve respecwhich are higher than the results of baseline monolingual SRL combination systems respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins Parsing Model. Computational Linguistics,</journal>
<pages>30--4</pages>
<contexts>
<context position="9845" citStr="Bikel, 2004" startWordPosition="1629" endWordPosition="1630">all such triple a candidate. 1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit .html Sent: *-A Rif ^1L A)� +[A *,N X-5 *Y,-A Args: [ A0 ] [Pred] [ A1 ] loc: (0, 2) (4, 7) l: A0 A1 p: 0.94 0.92 Figure 3: Three attributes of an output argument: location loc, label l, and probability p. 3.3 Generating and Merging Candidates To generate candidates for joint inference, we need to have multiple SRL results on each side of bitext. Therefore, for both Chinese and English SRL systems, we use the 3-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse trees of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs. All the three parsers are multilingual parsers. The second and third best parse trees of Berkeley parser are used for their good quality. Therefore, each monolingual SRL system produces 5 different outputs. Candidates from different outputs may have the same loc and l but different p. So we merge all candidates with the same loc and l into one by averaging their probabilities. For a merged candidate (loc, l, p), we say that p is the probability of assigning l to loc. 4 Joint Inference Model Our model can be conceptually decomposed to th</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel Bikel. 2004. Intricacies of Collins Parsing Model. Computational Linguistics, 30(4):480-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard P Brent</author>
</authors>
<title>Algorithms for Minimization without Derivatives.</title>
<date>1973</date>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="27174" citStr="Brent, 1973" startWordPosition="4772" endWordPosition="4773">chtb 0325.fid) of Xinhua News data in OntoNotes 3.0, and the Sinorama data in OntoNotes 3.0. We treat discontinuous and coreferential arguments in accordance to the CoNLL-2005 shared task (Carreras and M`arquez, 2005). The first part of a discontinuous argument is labeled as it is, and the second part is labeled with a prefix “C-”. All coreferential arguments are labeled with a prefix “R-”. 5.2 Tuning Parameters in Models The models Joint], Joint2, SrcCmb, and TrgCmb have different parameters. For each model, we have automatically tuned its parameters on development set using Powell’s Mothod (Brent, 1973). Powell’s Method is a heuristic optimization algorithm that does not require the objective function to have an explicit analytical formula. For a monolingual model like SrcCmb or TrgCmb, our objective is to maximize the F1 score of the model’s result on development set. But a joint model, like Joint] or Joint2, generates SRL results on both sides of bitext. So our objective is to maximize the sum of the two F1 scores of the model’s results for both Chinese and English on development set. For all models, we regard the parameters to be tuned as variables. Then we optimize our objective using Po</context>
</contexts>
<marker>Brent, 1973</marker>
<rawString>Richard P. Brent. 1973. Algorithms for Minimization without Derivatives. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two Languages are Better than One (for Syntactic Parsing).</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-2008,</booktitle>
<pages>877--886</pages>
<contexts>
<context position="2584" citStr="Burkett and Klein, 2008" startWordPosition="417" endWordPosition="420">et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous research has shown that bilingual constraints can be very helpful for parsing (Burkett and Klein, 2008; Huang et al., 2008). In this paper, we show that the bilingual argument structure consistency can be leveraged to substantially improve SRL results on both sides of bitext. Formally, we present a joint inference model to preform bilingual SRL. Using automatic word alignment on bitext, we first identify a pair of predicates that align with each other. And we use monolingual SRL systems to produce argument candidates for each predicate. Then, our model jointly generate SRL results for both predicates from their argument candidates, using integer linear programming (ILP) technique. An overview </context>
<context position="16908" citStr="Burkett and Klein, 2008" startWordPosition="2954" endWordPosition="2957"> is very probable that argsi and argtk would align with each other. We adopt the method used in (Pad´o and Lapata, 2009) to measure the word-to-word alignments between argsi and argtk. And the word alignment feature is defined as same as the word alignment-based word overlap in (Pad´o and Lapata, 2009). Note that this is a real-valued feature. Head word alignment feature: The head word of an argument is usually more representative than other words. So we use whether the head words of argsi and argtk align with each other as a binary feature. The use of this feature is inspired by the work in (Burkett and Klein, 2008). Semantic role labels of two arguments: From Table 1, we can see that semantic role labels of two arguments are a good indicator of whether they should align with each other. For example, a Chinese A0 argument aligns with an English A0 argument most of the times, and never aligns with an English AM* argument in Table 1. Therefore, the semantic role labels of argsi and argtk are used as a feature. Predicate verb pair: Different predicate pairs have different argument alignment patterns. Let’s take the Chinese predicate M-c/zengzhang and the English predicate grow as an example. The argument al</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett, and Dan Klein. 2008. Two Languages are Better than One (for Syntactic Parsing). In Proceedings of EMNLP-2008, pages 877-886.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<pages>152--164</pages>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras, and Llu´ıs M`arquez. 2005. Introduction to the CoNLL-2005 shared task: semantic role labeling. In Proceedings of CoNLL-2005, pages 152-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Min Zhang</author>
<author>Ai Ti Aw</author>
<author>Chew Lim Tan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Using a Hybrid Convolution Tree Kernel for Semantic Role Labeling.</title>
<date>2008</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>7</volume>
<issue>4</issue>
<pages>1--23</pages>
<contexts>
<context position="1945" citStr="Che et al., 2008" startWordPosition="315" endWordPosition="318">o effectively perform SRL on bitext, which has important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous research has shown that bilingual constraints can be very helpf</context>
</contexts>
<marker>Che, Zhang, Aw, Tan, Liu, Li, 2008</marker>
<rawString>Wanxiang Che, Min Zhang, Ai Ti Aw, Chew Lim Tan, Ting Liu, and Sheng Li. 2008. Using a Hybrid Convolution Tree Kernel for Semantic Role Labeling. ACM Transactions on Asian Language Information Processing, 2008, 7(4): 1-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Zhaojun Wu</author>
<author>Yongsheng Yang</author>
<author>Dekai Wu</author>
</authors>
<title>Learning Bilingual Semantic Frames: Shallow Semantic Parsing vs. Semantic Role Projection.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>75--84</pages>
<contexts>
<context position="1585" citStr="Fung et al. (2007)" startWordPosition="251" endWordPosition="254">odel, F1 scores of SRL results on Chinese and English text achieve 79.53% and 77.87% respectively, which are 1.52 and 1.74 points higher than the results of baseline monolingual SRL combination systems respectively. 1 Introduction In recent years, there has been an increasing interest in SRL on several languages. However, little research has been done on how to effectively perform SRL on bitext, which has important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can</context>
<context position="6636" citStr="Fung et al. (2007)" startWordPosition="1105" endWordPosition="1108">method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus. They performed SRL only on the English side, and then mapped the English SRL result to German side. Fung et al. (2007) did pioneering work on studying argument alignment on Chinese-English parallel PropBank. They performed SRL on Chinese and English sides separately. Then, given the SRL result on both sides, they automatically induced the argument alignment between two sides. The major difference between our work and all existing research is that our model performs SRL inference on two sides of bitext simultaneously. In our Monolingual SRL System Source-side SRL Candidates Source-side Predidate Our Joint Inference Model Target side Predicate Monolingual SRL System Target-side SRL Bilingual Candidates SRL Resu</context>
</contexts>
<marker>Fung, Wu, Yang, Wu, 2007</marker>
<rawString>Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai Wu. 2007. Learning Bilingual Semantic Frames: Shallow Semantic Parsing vs. Semantic Role Projection. In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation, pages 75-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-Constrained (Monolingual) Shift-Reduce Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP-2009,</booktitle>
<pages>1222--1231</pages>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, Qun Liu. 2009. Bilingually-Constrained (Monolingual) Shift-Reduce Parsing. In Proceedings of EMNLP-2009, pages 1222-1231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-2003,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="9891" citStr="Klein and Manning, 2003" startWordPosition="1634" endWordPosition="1637">://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit .html Sent: *-A Rif ^1L A)� +[A *,N X-5 *Y,-A Args: [ A0 ] [Pred] [ A1 ] loc: (0, 2) (4, 7) l: A0 A1 p: 0.94 0.92 Figure 3: Three attributes of an output argument: location loc, label l, and probability p. 3.3 Generating and Merging Candidates To generate candidates for joint inference, we need to have multiple SRL results on each side of bitext. Therefore, for both Chinese and English SRL systems, we use the 3-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse trees of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs. All the three parsers are multilingual parsers. The second and third best parse trees of Berkeley parser are used for their good quality. Therefore, each monolingual SRL system produces 5 different outputs. Candidates from different outputs may have the same loc and l but different p. So we merge all candidates with the same loc and l into one by averaging their probabilities. For a merged candidate (loc, l, p), we say that p is the probability of assigning l to loc. 4 Joint Inference Model Our model can be conceptually decomposed to three components: the source side, the target si</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL-2003, pages 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Koomen</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wentau Yih</author>
</authors>
<title>Generalized Inference with Multiple Semantic Role Labeling Systems.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005 shared task,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="5978" citStr="Koomen et al. (2005)" startWordPosition="1000" endWordPosition="1003">ents. Experiments on Chinese-English parallel PropBank shows that our model significantly outperforms monolingual SRL combination systems on both Chinese and English sides. The rest of this paper is organized as follows: Section 2 introduces related work. Section 3 describes how we generate SRL candidates on each side of bitext. Section 4 presents our joint inference model. Section 5 presents our experiments. And Section 6 concludes our work. 2 Related Work Some existing work on monolingual SRL combination is related to our work. Punyakanok et al. (2004; 2008) formulated an ILP model for SRL. Koomen et al. (2005) combined several SRL outputs using ILP method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus. They performed SRL only on the English side, and then mapped</context>
</contexts>
<marker>Koomen, Punyakanok, Roth, Yih, 2005</marker>
<rawString>Peter Koomen, Vasin Punyakanok, Dan Roth, and Wentau Yih. 2005. Generalized Inference with Multiple Semantic Role Labeling Systems. In Proceedings of CoNLL-2005 shared task, pages 181-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Guodong Zhou</author>
<author>Hai Zhao</author>
<author>Qiaoming Zhu</author>
<author>Peide Qian</author>
</authors>
<date>2009</date>
<booktitle>Improving Nominal SRL in Chinese Language with Verbal SRL Information and Automatic Predicate Recognition. In Proceedings of EMNLP-2009,</booktitle>
<pages>1280--1288</pages>
<contexts>
<context position="1973" citStr="Li et al., 2009" startWordPosition="321" endWordPosition="324">bitext, which has important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous research has shown that bilingual constraints can be very helpful for parsing (Burkett and </context>
</contexts>
<marker>Li, Zhou, Zhao, Zhu, Qian, 2009</marker>
<rawString>Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, and Peide Qian. 2009. Improving Nominal SRL in Chinese Language with Verbal SRL Information and Automatic Predicate Recognition. In Proceedings of EMNLP-2009, pages 1280-1288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>Xavier Carreras</author>
<author>Kenneth C Litkowski</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Semantic Role Labeling: An Introduction to the Special Issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--2</pages>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>Llu´ıs M`arquez, Xavier Carreras, Kenneth C. Litkowski, Suzanne Stevenson. 2008. Semantic Role Labeling: An Introduction to the Special Issue. Computational Linguistics, 34(2):145-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>Mihai Surdeanu</author>
<author>Pere Comas</author>
<author>Jordi Turmo</author>
</authors>
<title>A Robust Combination Strategy for Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP2005,</booktitle>
<pages>644--651</pages>
<marker>M`arquez, Surdeanu, Comas, Turmo, 2005</marker>
<rawString>Llu´ıs M`arquez, Mihai Surdeanu, Pere Comas, and Jordi Turmo. 2005. A Robust Combination Strategy for Semantic Role Labeling. In Proceedings of EMNLP2005, pages 644-651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="24009" citStr="Och and Ney, 2003" startWordPosition="4260" endWordPosition="4263">er, we employ lpsolve2 to solve all ILP models. 5 Experiments 5.1 Experimental Setup In our experiments, we use the Xinhua News portion of Chinese and English data in LDC OntoNotes Release 3.0. This data is a Chinese-English parallel proposition bank described in (Palmer et al., 2005). It contains parallel proposition annotations for 325 files (chtb 0001.fid to chtb 0325.fid) from ChineseEnglish parallel Treebank. The English part of this data contains proposition annotations only for verbal predicates. Therefore, we only consider verbal predicates in this paper. We employ the GIZA++ toolkit (Och and Ney, 2003) to perform automatic word alignment. Besides the parallel PropBank data, we use additional 4,500K Chinese-English sentence pairs3 to induce word alignments for both directions, with the default GIZA++ settings. The alignments are symmetrized using the intersection heuristic (Och and Ney, 2003), which is known to produce high-precision alignments. We use 80 files (chtb 0001.fid to chtb 0080.fid) as test data, and 40 files (chtb 0081.fid to chtb 0120.fid) as development data. Although our joint inference model needs no training, we still need to train a log-linear argument alignment probability</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Frans J. Och, and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Cross-lingual Annotation Projection of Semantic Roles.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>36--307</pages>
<marker>Pad´o, Lapata, 2009</marker>
<rawString>Sebastian Pad´o, and Mirella Lapata. 2009. Cross-lingual Annotation Projection of Semantic Roles. Journal of Artificial Intelligence Research (JAIR), 36:307-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
<author>Olga Babko-Malaya</author>
<author>Jinying Chen</author>
<author>Benjamin Snyder</author>
</authors>
<title>A Parallel Proposition Bank II for Chinese and English.</title>
<date>2005</date>
<booktitle>In Frontiers in Corpus Annotation, Workshop in conjunction with ACL-05,</booktitle>
<pages>61--67</pages>
<contexts>
<context position="23676" citStr="Palmer et al., 2005" startWordPosition="4210" endWordPosition="4213">SrcCmb is to maximize Os, which is defined in equation (2). And the constraints of SrcCmb are defined by equations (3-5). Similarly, we will refer to the target side combination model as TrgCmb. The objective of TrgCmb is to maximize Ot defined in equation (6). And the constraints of TrgCmb are defined by equations (7-9). In this paper, we employ lpsolve2 to solve all ILP models. 5 Experiments 5.1 Experimental Setup In our experiments, we use the Xinhua News portion of Chinese and English data in LDC OntoNotes Release 3.0. This data is a Chinese-English parallel proposition bank described in (Palmer et al., 2005). It contains parallel proposition annotations for 325 files (chtb 0001.fid to chtb 0325.fid) from ChineseEnglish parallel Treebank. The English part of this data contains proposition annotations only for verbal predicates. Therefore, we only consider verbal predicates in this paper. We employ the GIZA++ toolkit (Och and Ney, 2003) to perform automatic word alignment. Besides the parallel PropBank data, we use additional 4,500K Chinese-English sentence pairs3 to induce word alignments for both directions, with the default GIZA++ settings. The alignments are symmetrized using the intersection h</context>
</contexts>
<marker>Palmer, Xue, Babko-Malaya, Chen, Snyder, 2005</marker>
<rawString>Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jinying Chen, Benjamin Snyder. 2005. A Parallel Proposition Bank II for Chinese and English. In Frontiers in Corpus Annotation, Workshop in conjunction with ACL-05, pages 61-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL2007,</booktitle>
<pages>46--54</pages>
<contexts>
<context position="9792" citStr="Petrov and Klein, 2007" startWordPosition="1618" endWordPosition="1621">0, 2), A0, 0.94). Because these outputs are to be combined, we call such triple a candidate. 1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit .html Sent: *-A Rif ^1L A)� +[A *,N X-5 *Y,-A Args: [ A0 ] [Pred] [ A1 ] loc: (0, 2) (4, 7) l: A0 A1 p: 0.94 0.92 Figure 3: Three attributes of an output argument: location loc, label l, and probability p. 3.3 Generating and Merging Candidates To generate candidates for joint inference, we need to have multiple SRL results on each side of bitext. Therefore, for both Chinese and English SRL systems, we use the 3-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse trees of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs. All the three parsers are multilingual parsers. The second and third best parse trees of Berkeley parser are used for their good quality. Therefore, each monolingual SRL system produces 5 different outputs. Candidates from different outputs may have the same loc and l but different p. So we merge all candidates with the same loc and l into one by averaging their probabilities. For a merged candidate (loc, l, p), we say that p is the probability of assigning l to loc. 4 Joint Inference</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized parsing. In Proceedings of ACL2007, pages 46-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic Role Labeling Using Different Syntactic Views.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005,</booktitle>
<pages>581--588</pages>
<contexts>
<context position="6074" citStr="Pradhan et al. (2005)" startWordPosition="1016" endWordPosition="1019">rforms monolingual SRL combination systems on both Chinese and English sides. The rest of this paper is organized as follows: Section 2 introduces related work. Section 3 describes how we generate SRL candidates on each side of bitext. Section 4 presents our joint inference model. Section 5 presents our experiments. And Section 6 concludes our work. 2 Related Work Some existing work on monolingual SRL combination is related to our work. Punyakanok et al. (2004; 2008) formulated an ILP model for SRL. Koomen et al. (2005) combined several SRL outputs using ILP method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus. They performed SRL only on the English side, and then mapped the English SRL result to German side. Fung et al. (2007) did pioneering work on studying argum</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Daniel Jurafsky. 2005. Semantic Role Labeling Using Different Syntactic Views. In Proceedings of ACL-2005, pages 581-588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<date>2008</date>
<booktitle>Towards Robust Semantic Role Labeling. Computational Linguistics,</booktitle>
<pages>34--2</pages>
<contexts>
<context position="1859" citStr="Pradhan et al., 2008" startWordPosition="300" endWordPosition="303">sing interest in SRL on several languages. However, little research has been done on how to effectively perform SRL on bitext, which has important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in</context>
<context position="8572" citStr="Pradhan et al., 2008" startWordPosition="1413" endWordPosition="1416">argument locations are identified in a sentence. In the argument classification stage, each location identified in the previous stage is assigned a semantic role label. Maximum entropy classifier is employed for both the argument identification and classification tasks. And Zhang Le’s MaxEnt toolkit1 is used for implementation. We use the monolingual SRL system described above for both Chinese and English SRL tasks. For the Chinese SRL task, the features used in this paper are the same with those used in (Xue, 2008). For the English SRL task, the features used are the same with those used in (Pradhan et al., 2008). 3.2 Output of the Monolingual SRL System The maximum entropy classifier in our monolingual SRL system can output classification probabilities. We use the classification probability of the argument classification stage as an argument’s probability. As illustrated in Figure 3, in an individual system’s output, each argument has three attributes: its location in sentence loc, represented by the number of its first word and last word; its semantic role label l; and its probability p. So each argument outputted by a system is a triple (loc, l, p). For example, the A0 argument in Figure 3 is ((0, </context>
</contexts>
<marker>Pradhan, Ward, Martin, 2008</marker>
<rawString>Sameer S. Pradhan, Wayne Ward, James H. Martin. 2008. Towards Robust Semantic Role Labeling. Computational Linguistics, 34(2):289-310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tauYih</author>
</authors>
<date>2008</date>
<booktitle>The Importance of Syntactic Parsing and Inference in Semantic Role Labeling. Computational Linguistics,</booktitle>
<pages>34--2</pages>
<contexts>
<context position="1884" citStr="Punyakanok et al., 2008" startWordPosition="304" endWordPosition="307">n several languages. However, little research has been done on how to effectively perform SRL on bitext, which has important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous re</context>
</contexts>
<marker>Punyakanok, Roth, Wen-tauYih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen-tauYih. 2008. The Importance of Syntactic Parsing and Inference in Semantic Role Labeling. Computational Linguistics, 34(2):257-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Semantic Role Labeling via Integer Linear Programming Inference.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-2004,</booktitle>
<pages>1346--1352</pages>
<contexts>
<context position="5917" citStr="Punyakanok et al. (2004" startWordPosition="989" endWordPosition="992">og-linear model to compute the probability of aligning two arguments. Experiments on Chinese-English parallel PropBank shows that our model significantly outperforms monolingual SRL combination systems on both Chinese and English sides. The rest of this paper is organized as follows: Section 2 introduces related work. Section 3 describes how we generate SRL candidates on each side of bitext. Section 4 presents our joint inference model. Section 5 presents our experiments. And Section 6 concludes our work. 2 Related Work Some existing work on monolingual SRL combination is related to our work. Punyakanok et al. (2004; 2008) formulated an ILP model for SRL. Koomen et al. (2005) combined several SRL outputs using ILP method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus.</context>
<context position="12001" citStr="Punyakanok et al., 2004" startWordPosition="2000" endWordPosition="2003"> among all candidates on the source side. The probability of assigning lsj to locsi is psij. An indicator variable xij is defined as: xij = [locsi is assigned label lsj]. Then the source side sub-objective Os in equation (1) is the sum of arguments’ probabilities on source side: s pij — Ts)xij (2) where Ts is a bias to prevent including too many candidates in solution (Surdeanu et al., 2007). We consider the following two linguistically motivated constraints: 1. No duplication: There is no duplication for key arguments: A0 — A5. 2. No overlapping: Arguments cannot overlap with each other. In (Punyakanok et al., 2004), several more constraints are considered. According to (Surdeanu et al., 2007), however, no significant performance improvement can be obtained by considering more constraints than the two above. So we do not consider other constraints. The inequalities in (3) make sure that each locsi is assigned at most one label. b1 &lt; i &lt; Ns : XLs xij &lt; 1 (3) j=1 i.e., PNs j=1 xuj = 0. A common technique in ILP modeling to form such a constraint is to use a sufficiently large auxiliary constant M. And the constraint is formulated as: b1 &lt; i &lt; Ns : X XLs xuj &lt; (1— XLs xij)M (5) uECi j=1 j=1 In this case, M </context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic Role Labeling via Integer Linear Programming Inference. In Proceedings of COLING-2004, pages 1346-1352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Zhifang Sui</author>
<author>Meng Wang</author>
<author>Xin Wang</author>
</authors>
<title>Chinese Semantic Role Labeling with Shallow Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP-2009,</booktitle>
<pages>1475--1483</pages>
<contexts>
<context position="1992" citStr="Sun et al., 2009" startWordPosition="325" endWordPosition="328"> important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous research has shown that bilingual constraints can be very helpful for parsing (Burkett and Klein, 2008; Huang </context>
</contexts>
<marker>Sun, Sui, Wang, Wang, 2009</marker>
<rawString>Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang. 2009. Chinese Semantic Role Labeling with Shallow Parsing. In Proceedings of EMNLP-2009, pages 1475-1483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Llu´ıs M`arquez</author>
<author>Xavier Carreras</author>
<author>Pere R Comas</author>
</authors>
<title>Combination Strategies for Semantic Role Labeling.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>29--105</pages>
<marker>Surdeanu, M`arquez, Carreras, Comas, 2007</marker>
<rawString>Mihai Surdeanu, Llu´ıs M`arquez, Xavier Carreras, and Pere R. Comas. 2007. Combination Strategies for Semantic Role Labeling. Journal of Artificial Intelligence Research (JAIR), 29:105-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A Global Joint Model for Semantic Role Labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>145--159</pages>
<contexts>
<context position="1909" citStr="Toutanova et al., 2008" startWordPosition="308" endWordPosition="311">ver, little research has been done on how to effectively perform SRL on bitext, which has important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous research has shown that bil</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A Global Joint Model for Semantic Role Labeling. Computational Linguistics, 34(2): 145-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Semantic Roles for SMT: A Hybrid Two-Pass Model.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-2009,</booktitle>
<pages>13--16</pages>
<contexts>
<context position="1448" citStr="Wu and Fung, 2009" startWordPosition="223" endWordPosition="226">ity of aligning two arguments. We have experimented with our model on Chinese-English parallel PropBank data. Using our joint inference model, F1 scores of SRL results on Chinese and English text achieve 79.53% and 77.87% respectively, which are 1.52 and 1.74 points higher than the results of baseline monolingual SRL combination systems respectively. 1 Introduction In recent years, there has been an increasing interest in SRL on several languages. However, little research has been done on how to effectively perform SRL on bitext, which has important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between tw</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu, and Pascale Fung. 2009. Semantic Roles for SMT: A Hybrid Two-Pass Model. In Proceedings of NAACL-2009, pages 13-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Labeling Chinese Predicates with Semantic Roles.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<pages>225--255</pages>
<contexts>
<context position="1956" citStr="Xue, 2008" startWordPosition="319" endWordPosition="320">orm SRL on bitext, which has important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous research has shown that bilingual constraints can be very helpful for pars</context>
<context position="7887" citStr="Xue, 2008" startWordPosition="1302" endWordPosition="1303">nterrelated factors: SRL result on the source side, SRL result on the target side, and the argument alignment between them. 3 Generating Candidates for Inference 3.1 Monolingual SRL System As shown in Figure 2, we need to use a monolingual SRL system to generate candidates for our joint inference model. We have implemented a monolingual SRL system which utilize full phrase-structure parse trees to perform SRL. In this system, the whole SRL process is comprised of three stages: pruning, argument identification, and argument classification. In the pruning stage, the heuristic pruning method in (Xue, 2008) is employed. In the argument identification stage, a number of argument locations are identified in a sentence. In the argument classification stage, each location identified in the previous stage is assigned a semantic role label. Maximum entropy classifier is employed for both the argument identification and classification tasks. And Zhang Le’s MaxEnt toolkit1 is used for implementation. We use the monolingual SRL system described above for both Chinese and English SRL tasks. For the Chinese SRL task, the features used in this paper are the same with those used in (Xue, 2008). For the Engli</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Labeling Chinese Predicates with Semantic Roles. Computational Linguistics, 34(2): 225-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Zhuang</author>
<author>Chengqing Zong</author>
</authors>
<title>A Minimum Error Weighting Combination Strategy for Chinese Semantic Role Labeling.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING2010,</booktitle>
<pages>1362--1370</pages>
<contexts>
<context position="6250" citStr="Zhuang and Zong (2010)" startWordPosition="1044" endWordPosition="1047">ribes how we generate SRL candidates on each side of bitext. Section 4 presents our joint inference model. Section 5 presents our experiments. And Section 6 concludes our work. 2 Related Work Some existing work on monolingual SRL combination is related to our work. Punyakanok et al. (2004; 2008) formulated an ILP model for SRL. Koomen et al. (2005) combined several SRL outputs using ILP method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus. They performed SRL only on the English side, and then mapped the English SRL result to German side. Fung et al. (2007) did pioneering work on studying argument alignment on Chinese-English parallel PropBank. They performed SRL on Chinese and English sides separately. Then, given the SRL result on both sides, they automatically ind</context>
</contexts>
<marker>Zhuang, Zong, 2010</marker>
<rawString>Tao Zhuang, and Chengqing Zong. 2010. A Minimum Error Weighting Combination Strategy for Chinese Semantic Role Labeling. In Proceedings of COLING2010, pages 1362-1370.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>