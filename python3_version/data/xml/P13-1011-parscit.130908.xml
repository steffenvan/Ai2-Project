<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000862">
<title confidence="0.998357">
Recognizing Rare Social Phenomena in Conversation:
Empowerment Detection in Support Group Chatrooms
</title>
<author confidence="0.999063">
Elijah Mayfield, David Adamson, and Carolyn Penstein Ros´e
</author>
<affiliation confidence="0.874262">
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, PA 15213
</affiliation>
<email confidence="0.990513">
{emayfiel, dadamson, cprose}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997351" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99976605">
Automated annotation of social behavior
in conversation is necessary for large-scale
analysis of real-world conversational data.
Important behavioral categories, though,
are often sparse and often appear only
in specific subsections of a conversation.
This makes supervised machine learning
difficult, through a combination of noisy
features and unbalanced class distribu-
tions. We propose within-instance con-
tent selection, using cue features to selec-
tively suppress sections of text and bias-
ing the remaining representation towards
minority classes. We show the effective-
ness of this technique in automated anno-
tation of empowerment language in online
support group chatrooms. Our technique
is significantly more accurate than multi-
ple baselines, especially when prioritizing
high precision.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954293103448">
Quantitative social science research has experi-
enced a recent expansion, out of controlled set-
tings and into natural environments. With this
influx of interest comes new methodology, and
the inevitable question arises of how to move
towards testable hypotheses, using these uncon-
trolled sources of data as scientific lenses into the
real world.
The study of conversational transcripts is a key
domain in this new frontier. There are certain
social and behavioral phenomena in conversation
that cannot be easily identified through question-
naire data, self-reported surveys, or easily ex-
tracted user metadata. Examples of these social
phenomena in conversation include overt displays
of power (Prabhakaran et al., 2012) or indicators
of rapport and relationship building (Wang et al.,
2012). Manually annotating these social phenom-
ena cannot scale to large data, so researchers turn
to automated annotation of transcripts (Ros´e et al.,
2008). While machine learning is highly effec-
tive for annotation tasks with relatively balanced
labels, such as sentiment analysis (Pang and Lee,
2004), more complex social functions are often
rarer. This leads to unbalanced class label distri-
butions and a much more difficult machine learn-
ing task. Moreover, features indicative of rare so-
cial annotations tend to be drowned out in favor of
features biased towards the majority class. The net
effect is that classification algorithms tend to bias
towards the majority class, giving low accuracy for
rare class detection.
Automated annotation of social phenomena also
brings opportunities for real-world applications.
For example, real-time annotation of conversation
can power adaptive intervention in collaborative
learning settings (Rummel et al., 2008; Adamson
and Ros´e, 2012). However, with the considerable
power of automation comes great responsibility. It
is critical to avoid intervening in the case of er-
roneous annotations, as providing unnecessary or
inappropriate support in such a setting has been
shown to be harmful to group performance and so-
cial cohesion (Dillenbourg, 2002; Stahl, 2012).
We propose adaptations to existing machine
learning algorithms which improve recognition of
rare annotations in conversational text data. Our
primary contribution comes in the form of within-
instance content selection. We develop a novel al-
gorithm based on textual cues, suppressing infor-
mation which is likely to be irrelevant to an in-
stance’s class label. This allows features which
predict minority classes to gain prominence, help-
ing to sidestep the frequency of common features
pointing to a majority class label.
Additionally, we propose modifications to ex-
isting algorithms. First, we identify a new appli-
cation of logistic model trees to text data. Next,
</bodyText>
<page confidence="0.983986">
104
</page>
<note confidence="0.914092">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 104–113,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999971909090909">
we define a modification of confidence-based en-
semble voting which encourages minority class la-
beling. Using these techniques, we demonstrate a
significant improvement in classifier performance
when recognizing the language of empowerment
in support group chatrooms, a critical application
area for researchers studying conversational inter-
actions in healthcare (Uden-Kraan et al., 2009).
The remainder of this paper is structured as fol-
lows. We introduce the domain of empowerment
in support contexts, along with previous studies on
the challenges that these annotations (and similar
others) bring to machine learning. We introduce
our new technique for improving the ability to au-
tomate this annotation, along with other optimiza-
tions to the machine learning workflow which are
tailored to this skewed class balance. We present
experimental results showing that our method is
effective, and provide a detailed analysis of the be-
havior of our model and the features it uses most.
We conclude with a discussion of particularly use-
ful applications of this work.
</bodyText>
<sectionHeader confidence="0.995266" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999845666666667">
We ground this paper’s discussion of machine
learning with a real problem, turning to the an-
notation of empowerment language in chat1. The
concept of empowerment, while a prolific area
of research, lacks a broad definition across pro-
fessionals, but broadly relates to “the power to
act efficaciously to bring about desired results”
(Boehm and Staples, 2002) and “experiencing per-
sonal growth as a result of developing skills and
abilities along with a more positive self-definition”
(Staples, 1990). Participants in online support
groups feel increased empowerment (Uden-Kraan
et al., 2009; Barak et al., 2008). Quantita-
tive studies have shown the effect of empower-
ment through statistical methods such as structural
equation modeling (Vauth et al., 2007), as have
qualitative methods such as deductive transcript
analysis (Owen et al., 2008) and interview studies
(Wahlin et al., 2006).
The transition between these styles of research
has been gradual. Pioneering work has demon-
strated the ability to distinguish empowerment lan-
guage in written texts, including prompted writ-
ing samples (Pennebaker and Seagal, 1999), nar-
</bodyText>
<tableCaption confidence="0.8018998">
1Definitions of empowerment are closely related to the
notion of self-efficacy (Bandura, 1997). For simplicity, we
use the former term exclusively in this paper.
Table 1: Empowerment label distribution in our
corpus.
</tableCaption>
<table confidence="0.999106571428572">
Annotation Label # %
Self-Empowerment NA 1522 79.3
POS 202 10.5
NEG 196 10.2
Other-Empowerment NA 1560 81.3
POS 217 11.3
NEG 143 7.4
</table>
<bodyText confidence="0.99417552631579">
ratives in online forums (Hoybye et al., 2005), and
some preliminary analysis of synchronous discus-
sion (Ogura et al., 2008; Mayfield et al., 2012b).
These transitional works have used limited analy-
sis methodology; in the absence of sophisticated
natural language processing, their conclusions of-
ten rely on coarse measures, such as word counts
and proportions of annotations in a text.
Users, of course, do not express empowerment
in every thread in which they participate, which
leads to a challenge for machine learning. Threads
often focus on a single user’s experiences, in
which most participants in a chat are merely com-
mentators, if they participate at all, matching pre-
vious research on shifts in speaker salience over
time (Hassan et al., 2008). This leads to many
user threads which are annotated as not applicable
(N/A). We move to our proposed approach with
these skewed distributions in mind.
</bodyText>
<sectionHeader confidence="0.997711" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9990433125">
Our data consists of a set of chatroom conversa-
tion transcripts from the Cancer Support Commu-
nity2. Each 90-minute conversation took place in
the context of a weekly meeting in a real-time chat,
with up to 6 participants in addition to a profes-
sional therapist facilitating the discussion. In to-
tal, 2,206 conversations were collected from 2007-
2011. This data offers potentially rich insight into
coping and social support; however, annotating
such a dataset by hand would be prohibitively ex-
pensive, even when it is already transcribed.
Twenty-one of these conversations have been
annotated, as originally described and analyzed
in (Mayfield et al., 2012b)3. This data was dis-
entangled into threads based on common themes
or topics, as in prior work (Elsner and Charniak,
</bodyText>
<footnote confidence="0.994195">
2www.cancersupportcommunity.org
3All annotations were found to be adequately reliable be-
tween humans, with thread disentanglement f = 0.75 and
empowerment annotation κ &gt; 0.7.
</footnote>
<page confidence="0.9979">
105
</page>
<figureCaption confidence="0.98908">
Figure 1: An example mapping from a single thread’s chat lines (left) to the per-user, per-thread instances
used for classification in this paper (right), with example annotations for self-empowerment indicated.
</figureCaption>
<bodyText confidence="0.999911096774194">
2010; Adams and Martel, 2010). A novel per-
user, per-thread annotation was then employed
for empowerment annotation, following a coding
manual based on definitions like those in Section
2. Each user was assigned a label of positive
or negative empowerment if they exhibited such
emotions, or was left blank if they did not do so
within the context of that thread. This annotation
was performed both for their self-empowerment
as well as their attitude towards others’ situations
(other-empowerment). An example of this annota-
tion for self-empowerment is presented in Figure
1 and the distribution of labels is given in Table 1.
Most previous annotation tasks attempt to an-
notate on a per-utterance basis, such as dialogue
act tagging (Popescu-Belis, 2008), or on arbitrary
spans of text, such as in the MPQA subjectivity
corpus (Wiebe et al., 2005). However, for our task,
a per-user, per-thread annotation is more appropri-
ate, because empowerment is often indicated best
through narrative (Hoybye et al., 2005). Human
annotators are instructed to take this context into
account when annotating (Mayfield et al., 2012b).
It would therefore be nonsensical to annotate indi-
vidual lines as “embodying” empowerment. Simi-
lar arguments have been made for sentiment, espe-
cially as the field moves towards aspect-oriented
sentiment (Breck et al., 2007). Assigning labels
based on thread boundaries allows for context to
be meaningfully taken into account, without cross-
ing topic boundaries.
However, this granularity comes with a price:
the distribution of class values in these instances
is highly skewed. In our data, the vast majority of
users’ threads are marked as not applicable to em-
powerment. Perhaps more inconveniently, while
taking context into account is important for reli-
able annotation, it leads to extraneous information
in many cases. Many threads can have multiple
lines of contributions that are topically related to
an expression of empowerment (and thus belong
in the same thread), but which do not indicate any
empowerment themselves. This exacerbates the
likelihood of instances being classified as N/A.
We choose to take advantage of these attributes
of threads. We know from research in discourse
analysis that many sections of conversations are
formulaic and rote, like introductions and greet-
ings (Schegloff, 1968). We additionally know that
polarity often shifts in dialogue through the use
of discourse connectives such as conjunctions and
transitional phrases. These issues have been ad-
dressed in work in the language technologies com-
munity, most notably through the Penn Discourse
Treebank (Prasad et al., 2008); however, their ap-
plications to noisier synchronous conversation has
beenrare in computational linguistics.
With these linguistic insights in mind, we ex-
amine how we can make best use of them for
machine learning performance. While techniques
for predicting rare events (Weiss and Hirsh, 1998)
and compensating for class imbalance (Frank and
</bodyText>
<page confidence="0.991782">
106
</page>
<bodyText confidence="0.9999569">
Bouckaert, 2006), these approaches generally fo-
cus on statistical properties of large class sets with-
out taking the nature of their datasets into account.
In the next section, we propose a new algorithm
which takes advantage specifically of the linguis-
tic phenomena in the conversation-based data that
we study for empowerment detection. As such,
our algorithm is highly suited to this data and task,
with the necessary tradeoff in uncertain generality
to new domains with unrelated data.
</bodyText>
<sectionHeader confidence="0.850241" genericHeader="method">
4 Cue Discovery for Content Selection
</sectionHeader>
<bodyText confidence="0.999990857142857">
Our algorithm performs content selection by
learning a set of cue features. Each of these fea-
tures indicates some linguistic function within the
discourse which should downplay the importance
of features either before or after that discourse
marker. Our algorithm allows us to evaluate the
impact of rules against a baseline, and to itera-
tively judge each rule atop the changes made by
previous rules.
This algorithm fits into existing language tech-
nologies research which has attempted to partition
documents into sections which are more or less
relevant for classification. Many researchers have
attempted to make use of cue phrases (Hirschberg
and Litman, 1993), especially for segmentation
both in prose (Hearst, 1997) and conversation
(Galley et al., 2003). The approach of content se-
lection, meanwhile, has been explored for senti-
ment analysis (Pang and Lee, 2004), where indi-
vidual sentences may be less subjective and there-
fore less relevant to the sentiment classification
task. It is also similar conceptually to content
selection algorithms that have been used for text
summarization (Teufel and Moens, 2002) and text
generation (Sauper and Barzilay, 2009), both of
which rely on finding highly-relevant passages
within source texts.
Our work is distinct from these approaches.
While we have coarse-grained annotations of em-
powerment, there is no direct annotation of what
makes a good cue for content selection. With
our cues, we hope to take advantage of shallow
discourse structure in conversation, such as con-
trastive markers, making use of implicit structure
in the conversational domain.
</bodyText>
<subsectionHeader confidence="0.929052">
4.1 Notation
</subsectionHeader>
<bodyText confidence="0.999964294117647">
Before describing extensions to the baseline lo-
gistic regression model, we define notation. Our
data is arranged hierarchically. We assume that
we have a collection of d training documents Tr =
{D1 ... Dd}, each of which contains many train-
ing instances (in our task, an instance consists of
all lines of chat from one user in one thread). Our
total set of n instances I thus consists of instances
{I1, I2, ... In}. Each document contains lines of
chat L and each instance Ii is comprised of some
subset of those lines, Li C L.
Our feature space X = {x1, x2,... xm} con-
sists of m unigram features representing the ob-
served vocabulary used in our corpus. Each in-
stance is associated with a feature vector x¯ con-
taining values for each x E X, and each feature
x that is present in the i-th instance maintains a
“memory” of the lines in which it appeared in that
instance, Lix, where Lix C Li. Our potential out-
put labels consist of Y = {NA, NEG, POS},
though this generalizes to any nominal classifica-
tion task. Each instance I is associated with ex-
actly one y E Y for self-empowerment and one
for other-empowerment; these two labels do not
interact and our tasks are treated as independent
in this paper4. We define classifiers as functions
f(¯x → y E Y); in practice, we use logistic regres-
sion via LibLINEAR (Fan et al., 2008).
We define a content selection rule as a pairing
r = (c, t) between a cue feature c E X and a se-
lection function t E T. We created a list of possi-
ble selection functions, given a cue c, maximizing
for generality while being expressive. These are
illustrated in Figure 2 and described below:
</bodyText>
<listItem confidence="0.9966103">
• Ignore Local Future (A): Ignore all features
from the two lines after each occurrence of c.
• Ignore All Future (B): Ignore all features
occurring after the first occurrence of c.
• Ignore Local History (C): Ignore all features
in the two lines preceding each occurrence of
c.
• Ignore All History (D): Ignore all features
occurring only before the last occurrence of
c.
</listItem>
<bodyText confidence="0.99997725">
We define an ensemble member E = (R, fR) -
the ordered list of learned content selection rules
R = [r1, r2,... ] and a classifier fC trained on in-
stances transformed by those rules. Our final out-
</bodyText>
<footnote confidence="0.699483">
4Future work may examine the interaction of jointly an-
notating multiple sparse social phenomena.
</footnote>
<page confidence="0.996035">
107
</page>
<figureCaption confidence="0.7547195">
Figure 2: Effects of content selection rules, based
on a cue feature (ovals) observed at lines m and n.
</figureCaption>
<bodyText confidence="0.958008">
put of a trained model is a set of ensemble mem-
bers {E1, ... , Ek}.
</bodyText>
<subsectionHeader confidence="0.905046">
4.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.999968803921569">
Our ensemble learning follows the paradigm
of cross-validated committees (Parmanto et al.,
1996), where k ensemble members are trained by
subdividing our training data into k subfolds. For
each ensemble classifier, cue rules R are generated
on k − 1 subfolds (Trk) and evaluated on the re-
maining subfold (Tek). In practice, with 21 train-
ing documents, 7-fold cross-validation, and k = 3
ensemble members, each generation set consists
of 12 documents’ instances, while each evaluation
set contains instances from 6 documents.
Our full algorithm is presented in Algorithm
1, and is broken into component parts for clar-
ity. Algorithm 2 begins by measuring the base-
line classifier’s ability to recognize minority-class
labels. After training on Trk, we measure the
average probability assigned to the correct label
of instances in Tek, but only for instances whose
correct labels are minority classes (remember, be-
cause both Trk and Tek are drawn from the over-
all Tr, we have access to true class labels). We
choose this subset of only minority instances, as
we are not interested in optimizing to the majority
class.
We next enumerate all rules that we wish to
judge. To keep this problem tractable, we ignore
features which do not occur in at least 5% of train-
ing instances. For the remaining features, we cre-
ate a candidate rule for each possible pairing of
features and selection functions. For each of these
candidates, we test its utility by selecting content
as if it were an actual rule, then building a new
classifier (trained on the generation set) using in-
stances that have been altered in that way. In the
evaluation set, we measure the difference in prob-
ability of minority class labels being assigned cor-
rectly between the baseline and this altered space.
This measure of an individual rule’s impact is de-
scribed in Algorithm 3.
Once we have evaluated every possible rule
once, we select the top-ranked rule and ap-
ply it to the feature set. We then iteratively
progress through our now-ranked list of candi-
dates, each time treating the newly filtered dataset
as our new baseline. We search only top can-
didates for efficiency, following the fixed-width
search methodology for feature selection in very
high-dimensionality feature spaces (G¨utlein et al.,
2009). Each ensemble classifier is finally retrained
on all training data, after applying the correspond-
ing content selection rules to that data.
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="method">
5 Prediction
</sectionHeader>
<bodyText confidence="0.999957294117647">
Our prediction algorithm begins with a stan-
dard implementation of cross-validated commit-
tees (Parmanto et al., 1996), whose results are
aggregated with a confidence voting method in-
tended to favor rare labels (Erp et al., 2002).
Cross-validated committees are an ensemble tech-
nique used to subsample training data to produce
multiple hypotheses for classification. Each clas-
sifier produced by our cue-based transformation
is trained on a subset of our training data. Each
makes predictions on all test set instances, pro-
ducing a distribution of confidence across possi-
ble labels. These values serve as inputs to a voting
method to produce a final label for each instance.
Compared to other ensemble methods, cross-
validated committees as described above are a
good fit for our task, because of its unique unit of
analysis. As thread-level analysis is the set of in-
dividual participants’ turns in a conversation, we
risk overfitting if we sample from the same con-
versations for the training and testing sets. In con-
trast to standard bagging, hard sampling bound-
aries never train and test on instances drawn from
the same conversation.
To aggregate the votes from members of this en-
semble into a final prediction, we employ a variant
on Selfridge’s Pandemonium (Selfridge, 1958).
If a minority label is selected as the highest-
confidence value in any classifier in our ensem-
ble, it is selected. The majority label, by contrast,
is only selected if it is the most likely prediction
by all classifiers in our ensemble. Thus consen-
sus is required to elect the majority class, and the
strongest minority candidate is elected otherwise.
</bodyText>
<page confidence="0.994318">
108
</page>
<bodyText confidence="0.8671215">
In : generation set Trk, evaluation set Tek
Out: ensemble committee {E, ... Ek}
</bodyText>
<equation confidence="0.986648136363636">
fori=1tokdo
Rfinal +- [ ];
Xfreq +- {x E X I freq(x) E Trk &gt;
5%};
R +- Xfreq X T;
R* +- R;
repeat
Pbase +- EvaluateClassifier(Trk, Tek);
EvaluateRules(Pbase, Trk, Tek, R*);
Trk, Tek +- ApplyRule(R*[0]);
R +- R − R*[0];
Δ +- score(R*[0]);
Rfinal +- Rfinal + R*[0];
R* +- R[0... 50];
until Δ &lt; threshold;
Trfinal +- Trk U Tek;
foreach r E Rfinal do
Trfinal +- ApplyRule(Trfinal, r);
end
Train f(¯x -+ y) on Trfinal;
end
Algorithm 1: LearnSelectionCues()
</equation>
<bodyText confidence="0.999464454545455">
This approach is designed to bias the prediction
of our machine learning algorithms in favor of mi-
nority classes in a coherent manner. If there is a
plausible model that has been trained which rec-
ognizes the possibility of a rare label, it is used;
the prediction only reverts to the majority class
when no plausible minority label could be chosen.
As validation of this technique, we compare our
“minority pandemonium” approach against both
typical pandemonium and standard sum-rule con-
fidence voting (Erp et al., 2002).
</bodyText>
<subsectionHeader confidence="0.839434">
5.1 Logistic Model Stumps
</subsectionHeader>
<bodyText confidence="0.997667875">
One characteristic of highly skewed data is that,
while minority labels may be expressed in a num-
ber of different surface forms, there are many ob-
vious cases in which they do not apply. These
cases can actually be harmful to classification of
borderline cases. Features that could be given high
weight in marginal cases may be undervalued in
“low-hanging fruit” easy cases. To remove those
obvious instances, a very simple screening heuris-
tic is often enough to eliminate frequent pheno-
types of instances where the rare annotation is
not present. Prior work has sometimes screened
training data through obvious heuristic rules, espe-
In : generation set Trk, evaluation set Tek
Out: minority class probability average Pbase
Train f(¯x -+ y) on Trk;
</bodyText>
<equation confidence="0.952591545454545">
Temin
k +- {Instance I E Tek yI =� “NA”}
;
Pbase +- 0 ;
foreach Instance I E Temin
k do
Pbase +- Pbase + P(f(¯xI) = yI)
end
min
Pbase = Pbase/size(Tek )
Algorithm 2: EvaluateClassifier()
</equation>
<bodyText confidence="0.89241575">
In : Trk, Tek, rules R, base probability Pbase
Out: R sorted on each rule’s improvement
score
foreach Rule r E R do
</bodyText>
<equation confidence="0.9871525">
Trk, Tek +- ApplyRule(Trk, Tek, r);
Palter +- EvaluateClassifier(Trk, Tek);
score(r) +- Palter − Pbase;
end
Sort R on score(r) from high to low;
Algorithm 3: EvaluateRules()
</equation>
<bodyText confidence="0.999785928571429">
cially in speech recognition; for instance, training
speech recognition for words followed by a pause
separately from words followed by another word
(Franco et al., 2010), or training separate models
based on gender (Jiang et al., 1999).
We achieve this instance screening by learn-
ing logistic model tree stumps (Landwehr et al.,
2005), which allow us to quickly partition data if
there is a particularly easy heuristic that can be
learned to eliminate a large number of majority-
class labels. One challenge of this approach is
our underlying unigram feature space - tree-based
algorithms are generally poor classifiers for the
high-dimensionality, low-information features in a
lexical feature space (Han et al., 2001). To com-
pensate, we employ a smaller, denser set of binary
features for tree stump screening: instance length
thresholds and LIWC category membership.
First, we define a set of features that split based
on the number of lines an instance contains, from
1 to 10 (only a tiny fraction of instances are more
than 10 lines long). For example, a feature split-
ting on instances with lines &lt; 2 would be true
for one- and two-line instances, and false for all
others. Second, we define a feature for each cate-
gory in the Linguistic Inquiry and Word Count dic-
tionary (Tausczik and Pennebaker, 2010) - these
broad classes of words allow for more balanced
</bodyText>
<page confidence="0.998761">
109
</page>
<figureCaption confidence="0.979077">
Figure 3: Precision/recall curves for algorithms.
</figureCaption>
<bodyText confidence="0.9344915">
After 50% recall all models converge and there are
no significant differences in performance.
splits than would unigrams alone. Each category’s
feature is true if any word in that category was
used at least once in that instance.
We exhaustively sweep this feature space, and
report the most successful stump rules for each an-
notation task. In our other experiments, we report
results with and without the best rule for this pre-
processing step; we also measure its impact alone.
</bodyText>
<sectionHeader confidence="0.997365" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.998797409090909">
All experiments were performed using LightSIDE
(Mayfield and Ros´e, 2013). We use a binary uni-
gram feature space, and we perform 7-fold cross-
validation. Instances from the same chat transcript
never occur in both train and testing folds. Fur-
thermore, we assume that threads have been dis-
entangled already, and our experiments use gold
standard thread structure. While this is not a triv-
ial assumption, prior work has shown thread dis-
entanglement to be manageable (Mayfield et al.,
2012a); we consider it an acceptable simplify-
ing assumption for our experiments. We compare
our methods against baselines including a majority
baseline, a baseline logistic regression classifier
with L2 regularized features, and two common en-
semble methods, AdaBoost (Freund and Schapire,
1996) and bagging (Breiman, 1996) with logistic
regression base classifiers5.
Table 2 presents the best-performing result
from each classification method. For self-
empowerment recognition, all methods that we
introduce are significant improvements in κ, the
</bodyText>
<footnote confidence="0.8720475">
5These methods usually use weak, unstable base classi-
fiers; however, in our experiments, those performed poorly.
</footnote>
<tableCaption confidence="0.682306">
Table 2: Performance for baselines, common en-
semble algorithms, and proposed methods. Statis-
tically significant improvements over baseline are
marked (p &lt; .01, †; p &lt; .05, *; p &lt; 0.1, +).
</tableCaption>
<table confidence="0.9996492">
Self Other
Method % κ % κ
Majority 79.3 .000 81.3 .000
LR Baseline 81.0 .367 81.0 .270
LR + Boosting 78.1 .325 78.5 .275
LR + Bagging 81.2 .352 81.9 .265
LR + Committee 81.0 .367 81.0 .270
Learned Stumps 81.8* .385† 81.7 .293+
Content Selection 80.9 .389† 80.7 .282
Stumps+Selection 81.3 .406† 79.4 .254
</table>
<tableCaption confidence="0.858010333333333">
Table 3: Performance of content-selection
wrapped learners, for minority voting and two
baseline voting methods.
</tableCaption>
<table confidence="0.9989626">
Self Other
Method % κ % κ
Pandemonium 80.3 .283 81.4 .239
Averaged 80.6 .304 81.6 .251
Minority Voting 80.9† .389† 80.7 .282
</table>
<bodyText confidence="0.9973766">
measurement of agreement over chance, compared
to all baselines. While accuracy remains stable,
this is due to predictions shifting away from the
majority class and towards minority classes. Our
combined model using both logistic model tree
stumps and content selection is significantly better
than either alone (p &lt; .01). To compare the mi-
nority pandemonium voting method against base-
lines of simple pandemonium and summed confi-
dence voting, Table 3 presents the results of con-
tent selection wrappers with each voting method.
Minority voting is more effective compared to
standard confidence voting, improving κ while
modestly reducing accuracy; this is typical of a
shift towards minority class predictions.
</bodyText>
<sectionHeader confidence="0.997742" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.998237666666667">
These results show promise for our techniques,
which are able to distinguish features of rare la-
bels, previously awash in a sea of irrelevance. Fig-
ure 3 shows the impact of our rules as we tune
to different levels of recall, with a large boost in
precision when recall is not important; our model
converges with the baseline for high-recall, low-
precision tuning. This suggests that our method is
particularly suitable for tasks where confident la-
</bodyText>
<page confidence="0.999218">
110
</page>
<tableCaption confidence="0.862116">
Table 4: Cue rules commonly selected by the algo-
rithm. Average improvement over the LR baseline
</tableCaption>
<table confidence="0.979057714285714">
is also shown.
Self-Empowerment
Cue Transformation A%
and,but Ignore Local Future +5.0
have Ignore All History +4.3
! Ignore All History +4.2
me,my Ignore All History +3.4
Other-Empowerment
Cue Transformation A%
and,but Ignore Local Future +5.5
you Ignore Local History +5.2
’s Ignore Local History +4.1
that Ignore Local History +3.9
Table 5: Best decision rules for logistic model
stumps. Significant improvement (p &lt; 0.05) in-
dicated with *.
Self-Empowerment
Split Rule n An % A%
Split &lt; 1 * 0.385 +.018 81.8 +0.8
LIWC-Article 0.379 +.012 81.6 +0.6
LIWC-Swear * 0.376 +.009 81.4 +0.4
LIWC-Self * 0.376 +.009 81.5 +0.5
Other-Empowerment
Split Rule n An % A%
LIWC-You 0.293 +.023 81.7 +0.7
LIWC-Eating * 0.283 +.013 81.6 +0.6
LIWC-Negate * 0.282 +.012 82.3 +1.3
LIWC-Present 0.281 +.011 81.6 +0.6
</table>
<bodyText confidence="0.999965352941177">
beling of a few instances is more important than
labeling as many instances as possible. This is
common when tasks have a high cost or carry high
risk (for instance, providing real-time conversa-
tional supports with an agent, where inappropriate
intervention could be disruptive). Other low-recall
applications include exploration large corpora for
exemplar instances, where the most confident pre-
dictions for a given label should be presented first
for analyst use. In the rest of this section, we
examine notable within-instance and per-instance
rules selected by our methods. These rules are
summarized in Tables 4 and 5.
For both self- and other-empowerment, we find
pronoun rules that match the task (first-person and
second-person pronouns for self-Empowerment
and other-Empowerment respectively). In both
tasks, we find cue rules that suppress the context
preceding personal pronouns. These, as well as
the possessive suffix ’s, echo the per-instance ef-
fect of the Self and You splits, anticipating that
what follows such a personal reference is likely to
bear an evaluation of empowerment. Exclamation
marks may indicate strong emotion - we find many
instances where what precedes a line with an ex-
clamation is more objective, and what follows in-
cludes an assessment. Conjunctions but and and
are selected as cue rules suppressing the two lines
that follow the occurrence - suggesting, as sus-
pected, that connective discourse markers play a
role in indicating empowerment (Fraser, 1999).
The best-performing stump splits for the Self-
Empowerment annotation are Line Length &lt; 1
and the LIWC word-categories Article, Swear, and
Self. The split on line length corresponds to the
observation that longer instances provide greater
opportunity for personal narrative self-assessment
to occur (95% of single-line instances are labeled
NA). The Article category may serve as a proxy for
content length - article-less instances in our corpus
include one-line social greetings and exchanges
of contact information. Swear words may be a
cue for awareness of self-empowerment - a recent
study of women coping with illness reported that
swearing in the presence of others, but not alone,
was related to potentially harmful outcomes (Rob-
bins et al., 2011). Among other- oriented split
rules, Eating stands out as non-obvious, although
medical literature has suggested a link between
dietary behavior and empowerment attitudes in a
study of women with cancer (Pinto et al., 2002).
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999977866666667">
We have demonstrated an algorithm for improv-
ing automated classification accuracy on highly
skewed tasks for conversational data. This algo-
rithm, particularly its focus on content selection, is
rooted in the structural format of our data, which
can generalize to many tasks involving conversa-
tional data. Our experiments show that this model
significantly improves machine learning perfor-
mance. Our algorithm is taking advantage of
structural facets of discourse markers, lending ba-
sic sociolinguistic validity to its behavior. Though
we have treated each of these rarely-occurring la-
bels as independent thus far, in practice we know
that this is not the case. Joint prediction of labels
through structured modeling is an obvious next
</bodyText>
<page confidence="0.996208">
111
</page>
<bodyText confidence="0.999761357142857">
step for improving classification accuracy.
This is an important step towards large-scale
analysis of the impact of support groups on pa-
tients and caregivers. Our method can be used to
confidently highlight occurrences of rare labels in
large data sets. This has real-world implications
for professional intervention in social conversa-
tional domains, especially in scenarios where such
an intervention is likely to be associated with a
high cost or high risk. With the construction of
more accurate classifiers, we open the possibility
of automating annotation on large conversational
datasets, enabling new directions for researchers
with domain expertise.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.60192">
The research reported here was supported by Na-
tional Science Foundation grant IIS-0968485.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998735214285714">
Paige Adams and Craig Martel. 2010. Conversational
thread extraction and topic detection in text-based
chat. In Semantic Computing.
David Adamson and Carolyn Penstein Ros´e. 2012.
Coordinating multi-dimensional support in collabo-
rative conversational agents. In Proceedings of In-
telligent Tutoring Systems.
Albert Bandura. 1997. Self-Efficacy: The Exercise of
Control.
Azy Barak, Meyran Boniel-Nissim, and John Suler.
2008. Fostering empowerment in online support
groups. Computers in Human Behavior.
A Boehm and L H Staples. 2002. The functions of the
social worker in empowering: The voices of con-
sumers and professionals. Social Work.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI.
Leo Breiman. 1996. Bagging predictors. Machine
Learning.
Pierre Dillenbourg. 2002. Over-scripting cscl: The
risks of blending collaborative learning with instruc-
tional design. Three worlds of CSCL. Can we sup-
port CSCL?
Micha Elsner and Eugene Charniak. 2010. Disentan-
gling chat. Computational Linguistics.
Merijn Van Erp, Louis Vuurpijl, and Lambert
Schomaker. 2002. An overview and comparison of
voting methods for pattern recognition. In Frontiers
in Handwriting Recognition. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification.
Horacio Franco, Harry Bratt, Romain Rossier,
Venkata Rao Gadde, Elizabeth Shriberg, Victor
Abrash, and Kristin Precoda. 2010. Eduspeak: A
speech recognition and pronunciation scoring toolkit
for computer-aided language learning applications.
Language Testing.
Eibe Frank and Remco R Bouckaert. 2006. Naive
bayes for text classification with unbalanced classes.
Knowledge Discovery in Databases.
Bruce Fraser. 1999. What are discourse markers?
Journal ofpragmatics, 31(7):931–952.
Yoav Freund and Robert E Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proceed-
ings of ICML.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of ACL.
Martin G¨utlein, Eibe Frank, Mark Hall, and Andreas
Karwath. 2009. Large-scale attribute selection us-
ing wrappers. In Proceedings of IEEE CIDM.
Eui-Hong Han, George Karypis, and Vipin Kumar.
2001. Text categorization using weight adjusted
k-nearest neighbor classification. Lecture Notes in
Computer Science: Advances in Knowledge Discov-
ery and Data Mining.
Ahmed Hassan, Anthony Fader, Michael H Crespin,
Kevin M Quinn, Burt L Monroe, Michael Colaresi,
and Dragomir R Radev. 2008. Tracking the dy-
namic evolution of participant salience in a discus-
sion. In Proceedings of Coling.
Marti A Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics.
Mette Terp Hoybye, Christoffer Johansen, and Tine
Tjornhoj-Thomsen. 2005. Online interaction ef-
fects of storytelling in an internet breast cancer sup-
port group. Psycho-oncology.
Hui Jiang, Keikichi Hirose, and Qiang Huo. 1999. Ro-
bust speech recognition based on a bayesian predic-
tion approach. In IEEE Transactions on Speech and
Audio Processing.
Niels Landwehr, Mark Hall, and Eibe Frank. 2005.
Logistic model trees. Machine Learning.
Elijah Mayfield and Carolyn Penstein Ros´e. 2013.
Lightside: Open source machine learning for text.
In Handbook of Automated Essay Evaluation: Cur-
rent Applications and New Directions.
</reference>
<page confidence="0.982308">
112
</page>
<reference confidence="0.999844273584906">
Elijah Mayfield, David Adamson, and Carolyn Pen-
stein Ros´e. 2012a. Hierarchical conversation struc-
ture prediction in multi-party chat. In Proceedings
of SIGDIAL Meeting on Discourse and Dialogue.
Elijah Mayfield, Miaomiao Wen, Mitch Golant, and
Carolyn Penstein Ros´e. 2012b. Discovering habits
of effective online support group chatrooms. In
ACM Conference on Supporting Group Work.
Kanayo Ogura, Takashi Kusumi, and Asako Miura.
2008. Analysis of community development using
chat logs: A virtual support group of cancer patients.
In Proceedings of the IEEE Symposium on Universal
Communication.
Jason E. Owen, Erin O’Carroll Bantum, and Mitch
Golant. 2008. Benefits and challenges experienced
by professional facilitators of online support groups
for cancer survivors. In Psycho-Oncology.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the Association for Computational Linguistics.
Bambang Parmanto, Paul Munro, and Howard R
Doyle. 1996. Improving committee diagnosis with
resampling techniques. In Proceedings of NIPS.
James W Pennebaker and J D Seagal. 1999. Forming
a story: The health benefits of narrative. Journal of
Clinical Psychology.
Bernardine M Pinto, Nancy C Maruyama, Matthew M
Clark, Dean G Cruess, Elyse Park, and Mary
Roberts. 2002. Motivation to modify lifestyle risk
behaviors in women treated for breast cancer. In
Mayo Clinic Proceedings.
Andrei Popescu-Belis. 2008. Dimensionality of di-
alogue act tagsets: An empirical analysis of large
corpora. In Language Resources and Evaluation.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting overt display of power in
written dialogs. In Proceedings of NAACL.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of LREC.
Megan L Robbins, Elizabeth S Focella, Shelley Kasle,
Ana Mar´ıa L´opez, Karen L Weihs, and Matthias R
Mehl. 2011. Naturalistically observed swear-
ing, emotional support, and depressive symptoms
in women coping with illness. Health Psychology,
30:789.
Carolyn Penstein Ros´e, Yi-Chia Wang, Yue Cui, Jaime
Arguello, Karsten Stegmann, Armin Weinberger,
and Frank Fischer. 2008. Analyzing collabo-
rative learning processes automatically: Exploit-
ing the advances of computational linguistics in
computer-supported collaborative learning. In Inter-
national Journal of Computer Supported Collabora-
tive Learning.
Nikol Rummel, Armin Weinberger, Christof Wecker,
Frank Fischer, Anne Meier, Eleni Voyiatzaki,
George Kahrimanis, Hans Spada, Nikolaos Avouris,
and Erin Walker. 2008. New challenges in cscl:
Towards adaptive script support. In Proceedings of
ICLS.
Christina Sauper and Regina Barzilay. 2009. Auto-
matically generating wikipedia articles: A structure-
aware approach. In Proceedings of ACL.
Emanuel A Schegloff. 1968. Sequencing in conversa-
tional openings. American Anthropologist.
Oliver G Selfridge. 1958. Pandemonium: a
paradigm for learning. In Proceedings of Sympo-
sium on Mechanisation of Thought Processes, Na-
tional Physical Laboratory.
Gerry Stahl. 2012. Interaction analysis of a biology
chat. Productive multivocality.
Lee H Staples. 1990. Powerful ideas about empower-
ment. Administration in Social Work.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientic articles: Experiments with relevance and
rhetorical status. Computational Linguistics.
C F Van Uden-Kraan, C H C Drossaert, E Taal, E R
Seydel, and M A F J Van de Laar. 2009. Partici-
pation in online patient support groups endorses pa-
tients empowerment. Patient Education and Coun-
seling.
R Vauth, B Kleim, M Wirtz, and P W Corrigan. 2007.
Self-efficacy and empowerment as outcomes of self-
stigmatizing and coping in schizophrenia. Psychia-
try Research.
Ingrid Wahlin, Anna-Christina Ek, and Ewa Idvali.
2006. Patient empowerment in intensive carean in-
terview study. Intensive and Critical Care Nursing.
William Yang Wang, Samantha Finkelstein, Amy
Ogan, Alan Black, and Justine Cassell. 2012. “love
ya, jerkface:” using sparse log-linear models to build
positive (and impolite) relationships with teens. In
Proceedings of SIGDIAL.
Gary M Weiss and Haym Hirsh. 1998. Learning to
predict rare events in event sequences. In Proceed-
ings of KDD.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation.
</reference>
<page confidence="0.999318">
113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.679187">
<title confidence="0.998061">Recognizing Rare Social Phenomena in Empowerment Detection in Support Group Chatrooms</title>
<author confidence="0.985726">Elijah Mayfield</author>
<author confidence="0.985726">David Adamson</author>
<author confidence="0.985726">Carolyn Penstein</author>
<affiliation confidence="0.8437395">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.999972">5000 Forbes Ave, Pittsburgh, PA</address>
<email confidence="0.982724">dadamson,</email>
<abstract confidence="0.999351761904762">Automated annotation of social behavior in conversation is necessary for large-scale analysis of real-world conversational data. Important behavioral categories, though, are often sparse and often appear only in specific subsections of a conversation. This makes supervised machine learning difficult, through a combination of noisy features and unbalanced class distribu- We propose conusing cue features to selectively suppress sections of text and biasing the remaining representation towards minority classes. We show the effectiveness of this technique in automated annotation of empowerment language in online support group chatrooms. Our technique is significantly more accurate than multiple baselines, especially when prioritizing high precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paige Adams</author>
<author>Craig Martel</author>
</authors>
<title>Conversational thread extraction and topic detection in text-based chat.</title>
<date>2010</date>
<booktitle>In Semantic Computing.</booktitle>
<contexts>
<context position="8714" citStr="Adams and Martel, 2010" startWordPosition="1308" endWordPosition="1311">s have been annotated, as originally described and analyzed in (Mayfield et al., 2012b)3. This data was disentangled into threads based on common themes or topics, as in prior work (Elsner and Charniak, 2www.cancersupportcommunity.org 3All annotations were found to be adequately reliable between humans, with thread disentanglement f = 0.75 and empowerment annotation κ &gt; 0.7. 105 Figure 1: An example mapping from a single thread’s chat lines (left) to the per-user, per-thread instances used for classification in this paper (right), with example annotations for self-empowerment indicated. 2010; Adams and Martel, 2010). A novel peruser, per-thread annotation was then employed for empowerment annotation, following a coding manual based on definitions like those in Section 2. Each user was assigned a label of positive or negative empowerment if they exhibited such emotions, or was left blank if they did not do so within the context of that thread. This annotation was performed both for their self-empowerment as well as their attitude towards others’ situations (other-empowerment). An example of this annotation for self-empowerment is presented in Figure 1 and the distribution of labels is given in Table 1. Mo</context>
</contexts>
<marker>Adams, Martel, 2010</marker>
<rawString>Paige Adams and Craig Martel. 2010. Conversational thread extraction and topic detection in text-based chat. In Semantic Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Adamson</author>
<author>Carolyn Penstein Ros´e</author>
</authors>
<title>Coordinating multi-dimensional support in collaborative conversational agents.</title>
<date>2012</date>
<booktitle>In Proceedings of Intelligent Tutoring Systems.</booktitle>
<marker>Adamson, Ros´e, 2012</marker>
<rawString>David Adamson and Carolyn Penstein Ros´e. 2012. Coordinating multi-dimensional support in collaborative conversational agents. In Proceedings of Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bandura</author>
</authors>
<title>Self-Efficacy: The Exercise of Control.</title>
<date>1997</date>
<contexts>
<context position="6349" citStr="Bandura, 1997" startWordPosition="935" endWordPosition="936">., 2008). Quantitative studies have shown the effect of empowerment through statistical methods such as structural equation modeling (Vauth et al., 2007), as have qualitative methods such as deductive transcript analysis (Owen et al., 2008) and interview studies (Wahlin et al., 2006). The transition between these styles of research has been gradual. Pioneering work has demonstrated the ability to distinguish empowerment language in written texts, including prompted writing samples (Pennebaker and Seagal, 1999), nar1Definitions of empowerment are closely related to the notion of self-efficacy (Bandura, 1997). For simplicity, we use the former term exclusively in this paper. Table 1: Empowerment label distribution in our corpus. Annotation Label # % Self-Empowerment NA 1522 79.3 POS 202 10.5 NEG 196 10.2 Other-Empowerment NA 1560 81.3 POS 217 11.3 NEG 143 7.4 ratives in online forums (Hoybye et al., 2005), and some preliminary analysis of synchronous discussion (Ogura et al., 2008; Mayfield et al., 2012b). These transitional works have used limited analysis methodology; in the absence of sophisticated natural language processing, their conclusions often rely on coarse measures, such as word counts</context>
</contexts>
<marker>Bandura, 1997</marker>
<rawString>Albert Bandura. 1997. Self-Efficacy: The Exercise of Control.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Azy Barak</author>
<author>Meyran Boniel-Nissim</author>
<author>John Suler</author>
</authors>
<title>Fostering empowerment in online support groups. Computers in Human Behavior.</title>
<date>2008</date>
<contexts>
<context position="5743" citStr="Barak et al., 2008" startWordPosition="843" endWordPosition="846">ckground We ground this paper’s discussion of machine learning with a real problem, turning to the annotation of empowerment language in chat1. The concept of empowerment, while a prolific area of research, lacks a broad definition across professionals, but broadly relates to “the power to act efficaciously to bring about desired results” (Boehm and Staples, 2002) and “experiencing personal growth as a result of developing skills and abilities along with a more positive self-definition” (Staples, 1990). Participants in online support groups feel increased empowerment (Uden-Kraan et al., 2009; Barak et al., 2008). Quantitative studies have shown the effect of empowerment through statistical methods such as structural equation modeling (Vauth et al., 2007), as have qualitative methods such as deductive transcript analysis (Owen et al., 2008) and interview studies (Wahlin et al., 2006). The transition between these styles of research has been gradual. Pioneering work has demonstrated the ability to distinguish empowerment language in written texts, including prompted writing samples (Pennebaker and Seagal, 1999), nar1Definitions of empowerment are closely related to the notion of self-efficacy (Bandura,</context>
</contexts>
<marker>Barak, Boniel-Nissim, Suler, 2008</marker>
<rawString>Azy Barak, Meyran Boniel-Nissim, and John Suler. 2008. Fostering empowerment in online support groups. Computers in Human Behavior.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Boehm</author>
<author>L H Staples</author>
</authors>
<title>The functions of the social worker in empowering: The voices of consumers and professionals. Social Work.</title>
<date>2002</date>
<contexts>
<context position="5490" citStr="Boehm and Staples, 2002" startWordPosition="806" endWordPosition="809">ss balance. We present experimental results showing that our method is effective, and provide a detailed analysis of the behavior of our model and the features it uses most. We conclude with a discussion of particularly useful applications of this work. 2 Background We ground this paper’s discussion of machine learning with a real problem, turning to the annotation of empowerment language in chat1. The concept of empowerment, while a prolific area of research, lacks a broad definition across professionals, but broadly relates to “the power to act efficaciously to bring about desired results” (Boehm and Staples, 2002) and “experiencing personal growth as a result of developing skills and abilities along with a more positive self-definition” (Staples, 1990). Participants in online support groups feel increased empowerment (Uden-Kraan et al., 2009; Barak et al., 2008). Quantitative studies have shown the effect of empowerment through statistical methods such as structural equation modeling (Vauth et al., 2007), as have qualitative methods such as deductive transcript analysis (Owen et al., 2008) and interview studies (Wahlin et al., 2006). The transition between these styles of research has been gradual. Pio</context>
</contexts>
<marker>Boehm, Staples, 2002</marker>
<rawString>A Boehm and L H Staples. 2002. The functions of the social worker in empowering: The voices of consumers and professionals. Social Work.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="10028" citStr="Breck et al., 2007" startWordPosition="1514" endWordPosition="1517"> tagging (Popescu-Belis, 2008), or on arbitrary spans of text, such as in the MPQA subjectivity corpus (Wiebe et al., 2005). However, for our task, a per-user, per-thread annotation is more appropriate, because empowerment is often indicated best through narrative (Hoybye et al., 2005). Human annotators are instructed to take this context into account when annotating (Mayfield et al., 2012b). It would therefore be nonsensical to annotate individual lines as “embodying” empowerment. Similar arguments have been made for sentiment, especially as the field moves towards aspect-oriented sentiment (Breck et al., 2007). Assigning labels based on thread boundaries allows for context to be meaningfully taken into account, without crossing topic boundaries. However, this granularity comes with a price: the distribution of class values in these instances is highly skewed. In our data, the vast majority of users’ threads are marked as not applicable to empowerment. Perhaps more inconveniently, while taking context into account is important for reliable annotation, it leads to extraneous information in many cases. Many threads can have multiple lines of contributions that are topically related to an expression of</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="25365" citStr="Breiman, 1996" startWordPosition="4059" endWordPosition="4060">ipt never occur in both train and testing folds. Furthermore, we assume that threads have been disentangled already, and our experiments use gold standard thread structure. While this is not a trivial assumption, prior work has shown thread disentanglement to be manageable (Mayfield et al., 2012a); we consider it an acceptable simplifying assumption for our experiments. We compare our methods against baselines including a majority baseline, a baseline logistic regression classifier with L2 regularized features, and two common ensemble methods, AdaBoost (Freund and Schapire, 1996) and bagging (Breiman, 1996) with logistic regression base classifiers5. Table 2 presents the best-performing result from each classification method. For selfempowerment recognition, all methods that we introduce are significant improvements in κ, the 5These methods usually use weak, unstable base classifiers; however, in our experiments, those performed poorly. Table 2: Performance for baselines, common ensemble algorithms, and proposed methods. Statistically significant improvements over baseline are marked (p &lt; .01, †; p &lt; .05, *; p &lt; 0.1, +). Self Other Method % κ % κ Majority 79.3 .000 81.3 .000 LR Baseline 81.0 .36</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Dillenbourg</author>
</authors>
<title>Over-scripting cscl: The risks of blending collaborative learning with instructional design. Three worlds of CSCL. Can we support CSCL?</title>
<date>2002</date>
<contexts>
<context position="3190" citStr="Dillenbourg, 2002" startWordPosition="458" endWordPosition="459"> giving low accuracy for rare class detection. Automated annotation of social phenomena also brings opportunities for real-world applications. For example, real-time annotation of conversation can power adaptive intervention in collaborative learning settings (Rummel et al., 2008; Adamson and Ros´e, 2012). However, with the considerable power of automation comes great responsibility. It is critical to avoid intervening in the case of erroneous annotations, as providing unnecessary or inappropriate support in such a setting has been shown to be harmful to group performance and social cohesion (Dillenbourg, 2002; Stahl, 2012). We propose adaptations to existing machine learning algorithms which improve recognition of rare annotations in conversational text data. Our primary contribution comes in the form of withininstance content selection. We develop a novel algorithm based on textual cues, suppressing information which is likely to be irrelevant to an instance’s class label. This allows features which predict minority classes to gain prominence, helping to sidestep the frequency of common features pointing to a majority class label. Additionally, we propose modifications to existing algorithms. Fir</context>
</contexts>
<marker>Dillenbourg, 2002</marker>
<rawString>Pierre Dillenbourg. 2002. Over-scripting cscl: The risks of blending collaborative learning with instructional design. Three worlds of CSCL. Can we support CSCL?</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>Disentangling chat. Computational Linguistics.</title>
<date>2010</date>
<marker>Elsner, Charniak, 2010</marker>
<rawString>Micha Elsner and Eugene Charniak. 2010. Disentangling chat. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Merijn Van Erp</author>
<author>Louis Vuurpijl</author>
<author>Lambert Schomaker</author>
</authors>
<title>An overview and comparison of voting methods for pattern recognition. In Frontiers in Handwriting Recognition.</title>
<date>2002</date>
<publisher>IEEE.</publisher>
<marker>Van Erp, Vuurpijl, Schomaker, 2002</marker>
<rawString>Merijn Van Erp, Louis Vuurpijl, and Lambert Schomaker. 2002. An overview and comparison of voting methods for pattern recognition. In Frontiers in Handwriting Recognition. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<contexts>
<context position="15137" citStr="Fan et al., 2008" startWordPosition="2342" endWordPosition="2345">taining values for each x E X, and each feature x that is present in the i-th instance maintains a “memory” of the lines in which it appeared in that instance, Lix, where Lix C Li. Our potential output labels consist of Y = {NA, NEG, POS}, though this generalizes to any nominal classification task. Each instance I is associated with exactly one y E Y for self-empowerment and one for other-empowerment; these two labels do not interact and our tasks are treated as independent in this paper4. We define classifiers as functions f(¯x → y E Y); in practice, we use logistic regression via LibLINEAR (Fan et al., 2008). We define a content selection rule as a pairing r = (c, t) between a cue feature c E X and a selection function t E T. We created a list of possible selection functions, given a cue c, maximizing for generality while being expressive. These are illustrated in Figure 2 and described below: • Ignore Local Future (A): Ignore all features from the two lines after each occurrence of c. • Ignore All Future (B): Ignore all features occurring after the first occurrence of c. • Ignore Local History (C): Ignore all features in the two lines preceding each occurrence of c. • Ignore All History (D): Ign</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Franco</author>
<author>Harry Bratt</author>
<author>Romain Rossier</author>
<author>Venkata Rao Gadde</author>
<author>Elizabeth Shriberg</author>
<author>Victor Abrash</author>
<author>Kristin Precoda</author>
</authors>
<title>Eduspeak: A speech recognition and pronunciation scoring toolkit for computer-aided language learning applications. Language Testing.</title>
<date>2010</date>
<contexts>
<context position="22813" citStr="Franco et al., 2010" startWordPosition="3643" endWordPosition="3646">k yI =� “NA”} ; Pbase +- 0 ; foreach Instance I E Temin k do Pbase +- Pbase + P(f(¯xI) = yI) end min Pbase = Pbase/size(Tek ) Algorithm 2: EvaluateClassifier() In : Trk, Tek, rules R, base probability Pbase Out: R sorted on each rule’s improvement score foreach Rule r E R do Trk, Tek +- ApplyRule(Trk, Tek, r); Palter +- EvaluateClassifier(Trk, Tek); score(r) +- Palter − Pbase; end Sort R on score(r) from high to low; Algorithm 3: EvaluateRules() cially in speech recognition; for instance, training speech recognition for words followed by a pause separately from words followed by another word (Franco et al., 2010), or training separate models based on gender (Jiang et al., 1999). We achieve this instance screening by learning logistic model tree stumps (Landwehr et al., 2005), which allow us to quickly partition data if there is a particularly easy heuristic that can be learned to eliminate a large number of majorityclass labels. One challenge of this approach is our underlying unigram feature space - tree-based algorithms are generally poor classifiers for the high-dimensionality, low-information features in a lexical feature space (Han et al., 2001). To compensate, we employ a smaller, denser set of </context>
</contexts>
<marker>Franco, Bratt, Rossier, Gadde, Shriberg, Abrash, Precoda, 2010</marker>
<rawString>Horacio Franco, Harry Bratt, Romain Rossier, Venkata Rao Gadde, Elizabeth Shriberg, Victor Abrash, and Kristin Precoda. 2010. Eduspeak: A speech recognition and pronunciation scoring toolkit for computer-aided language learning applications. Language Testing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Remco R Bouckaert</author>
</authors>
<title>Naive bayes for text classification with unbalanced classes. Knowledge Discovery in Databases.</title>
<date>2006</date>
<marker>Frank, Bouckaert, 2006</marker>
<rawString>Eibe Frank and Remco R Bouckaert. 2006. Naive bayes for text classification with unbalanced classes. Knowledge Discovery in Databases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Fraser</author>
</authors>
<title>What are discourse markers?</title>
<date>1999</date>
<journal>Journal ofpragmatics,</journal>
<pages>31--7</pages>
<contexts>
<context position="29991" citStr="Fraser, 1999" startWordPosition="4796" endWordPosition="4797"> pronouns. These, as well as the possessive suffix ’s, echo the per-instance effect of the Self and You splits, anticipating that what follows such a personal reference is likely to bear an evaluation of empowerment. Exclamation marks may indicate strong emotion - we find many instances where what precedes a line with an exclamation is more objective, and what follows includes an assessment. Conjunctions but and and are selected as cue rules suppressing the two lines that follow the occurrence - suggesting, as suspected, that connective discourse markers play a role in indicating empowerment (Fraser, 1999). The best-performing stump splits for the SelfEmpowerment annotation are Line Length &lt; 1 and the LIWC word-categories Article, Swear, and Self. The split on line length corresponds to the observation that longer instances provide greater opportunity for personal narrative self-assessment to occur (95% of single-line instances are labeled NA). The Article category may serve as a proxy for content length - article-less instances in our corpus include one-line social greetings and exchanges of contact information. Swear words may be a cue for awareness of self-empowerment - a recent study of wom</context>
</contexts>
<marker>Fraser, 1999</marker>
<rawString>Bruce Fraser. 1999. What are discourse markers? Journal ofpragmatics, 31(7):931–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="25337" citStr="Freund and Schapire, 1996" startWordPosition="4053" endWordPosition="4056">on. Instances from the same chat transcript never occur in both train and testing folds. Furthermore, we assume that threads have been disentangled already, and our experiments use gold standard thread structure. While this is not a trivial assumption, prior work has shown thread disentanglement to be manageable (Mayfield et al., 2012a); we consider it an acceptable simplifying assumption for our experiments. We compare our methods against baselines including a majority baseline, a baseline logistic regression classifier with L2 regularized features, and two common ensemble methods, AdaBoost (Freund and Schapire, 1996) and bagging (Breiman, 1996) with logistic regression base classifiers5. Table 2 presents the best-performing result from each classification method. For selfempowerment recognition, all methods that we introduce are significant improvements in κ, the 5These methods usually use weak, unstable base classifiers; however, in our experiments, those performed poorly. Table 2: Performance for baselines, common ensemble algorithms, and proposed methods. Statistically significant improvements over baseline are marked (p &lt; .01, †; p &lt; .05, *; p &lt; 0.1, +). Self Other Method % κ % κ Majority 79.3 .000 81</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E Schapire. 1996. Experiments with a new boosting algorithm. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Eric FoslerLussier, and Hongyan Jing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin G¨utlein</author>
<author>Eibe Frank</author>
<author>Mark Hall</author>
<author>Andreas Karwath</author>
</authors>
<title>Large-scale attribute selection using wrappers.</title>
<date>2009</date>
<booktitle>In Proceedings of IEEE CIDM.</booktitle>
<marker>G¨utlein, Frank, Hall, Karwath, 2009</marker>
<rawString>Martin G¨utlein, Eibe Frank, Mark Hall, and Andreas Karwath. 2009. Large-scale attribute selection using wrappers. In Proceedings of IEEE CIDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eui-Hong Han</author>
<author>George Karypis</author>
<author>Vipin Kumar</author>
</authors>
<title>Text categorization using weight adjusted k-nearest neighbor classification.</title>
<date>2001</date>
<booktitle>Lecture Notes in Computer Science: Advances in Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="23361" citStr="Han et al., 2001" startWordPosition="3729" endWordPosition="3732"> separately from words followed by another word (Franco et al., 2010), or training separate models based on gender (Jiang et al., 1999). We achieve this instance screening by learning logistic model tree stumps (Landwehr et al., 2005), which allow us to quickly partition data if there is a particularly easy heuristic that can be learned to eliminate a large number of majorityclass labels. One challenge of this approach is our underlying unigram feature space - tree-based algorithms are generally poor classifiers for the high-dimensionality, low-information features in a lexical feature space (Han et al., 2001). To compensate, we employ a smaller, denser set of binary features for tree stump screening: instance length thresholds and LIWC category membership. First, we define a set of features that split based on the number of lines an instance contains, from 1 to 10 (only a tiny fraction of instances are more than 10 lines long). For example, a feature splitting on instances with lines &lt; 2 would be true for one- and two-line instances, and false for all others. Second, we define a feature for each category in the Linguistic Inquiry and Word Count dictionary (Tausczik and Pennebaker, 2010) - these br</context>
</contexts>
<marker>Han, Karypis, Kumar, 2001</marker>
<rawString>Eui-Hong Han, George Karypis, and Vipin Kumar. 2001. Text categorization using weight adjusted k-nearest neighbor classification. Lecture Notes in Computer Science: Advances in Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Anthony Fader</author>
<author>Michael H Crespin</author>
<author>Kevin M Quinn</author>
<author>Burt L Monroe</author>
<author>Michael Colaresi</author>
<author>Dragomir R Radev</author>
</authors>
<title>Tracking the dynamic evolution of participant salience in a discussion.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling.</booktitle>
<contexts>
<context position="7359" citStr="Hassan et al., 2008" startWordPosition="1097" endWordPosition="1100">et al., 2012b). These transitional works have used limited analysis methodology; in the absence of sophisticated natural language processing, their conclusions often rely on coarse measures, such as word counts and proportions of annotations in a text. Users, of course, do not express empowerment in every thread in which they participate, which leads to a challenge for machine learning. Threads often focus on a single user’s experiences, in which most participants in a chat are merely commentators, if they participate at all, matching previous research on shifts in speaker salience over time (Hassan et al., 2008). This leads to many user threads which are annotated as not applicable (N/A). We move to our proposed approach with these skewed distributions in mind. 3 Data Our data consists of a set of chatroom conversation transcripts from the Cancer Support Community2. Each 90-minute conversation took place in the context of a weekly meeting in a real-time chat, with up to 6 participants in addition to a professional therapist facilitating the discussion. In total, 2,206 conversations were collected from 2007- 2011. This data offers potentially rich insight into coping and social support; however, annot</context>
</contexts>
<marker>Hassan, Fader, Crespin, Quinn, Monroe, Colaresi, Radev, 2008</marker>
<rawString>Ahmed Hassan, Anthony Fader, Michael H Crespin, Kevin M Quinn, Burt L Monroe, Michael Colaresi, and Dragomir R Radev. 2008. Tracking the dynamic evolution of participant salience in a discussion. In Proceedings of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics.</title>
<date>1997</date>
<contexts>
<context position="12914" citStr="Hearst, 1997" startWordPosition="1961" endWordPosition="1962">some linguistic function within the discourse which should downplay the importance of features either before or after that discourse marker. Our algorithm allows us to evaluate the impact of rules against a baseline, and to iteratively judge each rule atop the changes made by previous rules. This algorithm fits into existing language technologies research which has attempted to partition documents into sections which are more or less relevant for classification. Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al., 2003). The approach of content selection, meanwhile, has been explored for sentiment analysis (Pang and Lee, 2004), where individual sentences may be less subjective and therefore less relevant to the sentiment classification task. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. Our work is distinct from these approaches. While we have coarse-grained anno</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A Hearst. 1997. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="12856" citStr="Hirschberg and Litman, 1993" startWordPosition="1951" endWordPosition="1954">tion by learning a set of cue features. Each of these features indicates some linguistic function within the discourse which should downplay the importance of features either before or after that discourse marker. Our algorithm allows us to evaluate the impact of rules against a baseline, and to iteratively judge each rule atop the changes made by previous rules. This algorithm fits into existing language technologies research which has attempted to partition documents into sections which are more or less relevant for classification. Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al., 2003). The approach of content selection, meanwhile, has been explored for sentiment analysis (Pang and Lee, 2004), where individual sentences may be less subjective and therefore less relevant to the sentiment classification task. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. Our work is distinc</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mette Terp Hoybye</author>
<author>Christoffer Johansen</author>
<author>Tine Tjornhoj-Thomsen</author>
</authors>
<title>Online interaction effects of storytelling in an internet breast cancer support group.</title>
<date>2005</date>
<publisher>Psycho-oncology.</publisher>
<contexts>
<context position="6651" citStr="Hoybye et al., 2005" startWordPosition="984" endWordPosition="987">tion between these styles of research has been gradual. Pioneering work has demonstrated the ability to distinguish empowerment language in written texts, including prompted writing samples (Pennebaker and Seagal, 1999), nar1Definitions of empowerment are closely related to the notion of self-efficacy (Bandura, 1997). For simplicity, we use the former term exclusively in this paper. Table 1: Empowerment label distribution in our corpus. Annotation Label # % Self-Empowerment NA 1522 79.3 POS 202 10.5 NEG 196 10.2 Other-Empowerment NA 1560 81.3 POS 217 11.3 NEG 143 7.4 ratives in online forums (Hoybye et al., 2005), and some preliminary analysis of synchronous discussion (Ogura et al., 2008; Mayfield et al., 2012b). These transitional works have used limited analysis methodology; in the absence of sophisticated natural language processing, their conclusions often rely on coarse measures, such as word counts and proportions of annotations in a text. Users, of course, do not express empowerment in every thread in which they participate, which leads to a challenge for machine learning. Threads often focus on a single user’s experiences, in which most participants in a chat are merely commentators, if they </context>
<context position="9695" citStr="Hoybye et al., 2005" startWordPosition="1464" endWordPosition="1467">ed both for their self-empowerment as well as their attitude towards others’ situations (other-empowerment). An example of this annotation for self-empowerment is presented in Figure 1 and the distribution of labels is given in Table 1. Most previous annotation tasks attempt to annotate on a per-utterance basis, such as dialogue act tagging (Popescu-Belis, 2008), or on arbitrary spans of text, such as in the MPQA subjectivity corpus (Wiebe et al., 2005). However, for our task, a per-user, per-thread annotation is more appropriate, because empowerment is often indicated best through narrative (Hoybye et al., 2005). Human annotators are instructed to take this context into account when annotating (Mayfield et al., 2012b). It would therefore be nonsensical to annotate individual lines as “embodying” empowerment. Similar arguments have been made for sentiment, especially as the field moves towards aspect-oriented sentiment (Breck et al., 2007). Assigning labels based on thread boundaries allows for context to be meaningfully taken into account, without crossing topic boundaries. However, this granularity comes with a price: the distribution of class values in these instances is highly skewed. In our data,</context>
</contexts>
<marker>Hoybye, Johansen, Tjornhoj-Thomsen, 2005</marker>
<rawString>Mette Terp Hoybye, Christoffer Johansen, and Tine Tjornhoj-Thomsen. 2005. Online interaction effects of storytelling in an internet breast cancer support group. Psycho-oncology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Jiang</author>
<author>Keikichi Hirose</author>
<author>Qiang Huo</author>
</authors>
<title>Robust speech recognition based on a bayesian prediction approach.</title>
<date>1999</date>
<booktitle>In IEEE Transactions on Speech and Audio Processing.</booktitle>
<contexts>
<context position="22879" citStr="Jiang et al., 1999" startWordPosition="3654" endWordPosition="3657">+- Pbase + P(f(¯xI) = yI) end min Pbase = Pbase/size(Tek ) Algorithm 2: EvaluateClassifier() In : Trk, Tek, rules R, base probability Pbase Out: R sorted on each rule’s improvement score foreach Rule r E R do Trk, Tek +- ApplyRule(Trk, Tek, r); Palter +- EvaluateClassifier(Trk, Tek); score(r) +- Palter − Pbase; end Sort R on score(r) from high to low; Algorithm 3: EvaluateRules() cially in speech recognition; for instance, training speech recognition for words followed by a pause separately from words followed by another word (Franco et al., 2010), or training separate models based on gender (Jiang et al., 1999). We achieve this instance screening by learning logistic model tree stumps (Landwehr et al., 2005), which allow us to quickly partition data if there is a particularly easy heuristic that can be learned to eliminate a large number of majorityclass labels. One challenge of this approach is our underlying unigram feature space - tree-based algorithms are generally poor classifiers for the high-dimensionality, low-information features in a lexical feature space (Han et al., 2001). To compensate, we employ a smaller, denser set of binary features for tree stump screening: instance length threshol</context>
</contexts>
<marker>Jiang, Hirose, Huo, 1999</marker>
<rawString>Hui Jiang, Keikichi Hirose, and Qiang Huo. 1999. Robust speech recognition based on a bayesian prediction approach. In IEEE Transactions on Speech and Audio Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niels Landwehr</author>
<author>Mark Hall</author>
<author>Eibe Frank</author>
</authors>
<title>Logistic model trees.</title>
<date>2005</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="22978" citStr="Landwehr et al., 2005" startWordPosition="3670" endWordPosition="3673">: Trk, Tek, rules R, base probability Pbase Out: R sorted on each rule’s improvement score foreach Rule r E R do Trk, Tek +- ApplyRule(Trk, Tek, r); Palter +- EvaluateClassifier(Trk, Tek); score(r) +- Palter − Pbase; end Sort R on score(r) from high to low; Algorithm 3: EvaluateRules() cially in speech recognition; for instance, training speech recognition for words followed by a pause separately from words followed by another word (Franco et al., 2010), or training separate models based on gender (Jiang et al., 1999). We achieve this instance screening by learning logistic model tree stumps (Landwehr et al., 2005), which allow us to quickly partition data if there is a particularly easy heuristic that can be learned to eliminate a large number of majorityclass labels. One challenge of this approach is our underlying unigram feature space - tree-based algorithms are generally poor classifiers for the high-dimensionality, low-information features in a lexical feature space (Han et al., 2001). To compensate, we employ a smaller, denser set of binary features for tree stump screening: instance length thresholds and LIWC category membership. First, we define a set of features that split based on the number </context>
</contexts>
<marker>Landwehr, Hall, Frank, 2005</marker>
<rawString>Niels Landwehr, Mark Hall, and Eibe Frank. 2005. Logistic model trees. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elijah Mayfield</author>
<author>Carolyn Penstein Ros´e</author>
</authors>
<title>Lightside: Open source machine learning for text.</title>
<date>2013</date>
<booktitle>In Handbook of Automated Essay Evaluation: Current Applications and</booktitle>
<location>New Directions.</location>
<marker>Mayfield, Ros´e, 2013</marker>
<rawString>Elijah Mayfield and Carolyn Penstein Ros´e. 2013. Lightside: Open source machine learning for text. In Handbook of Automated Essay Evaluation: Current Applications and New Directions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elijah Mayfield</author>
<author>David Adamson</author>
<author>Carolyn Penstein Ros´e</author>
</authors>
<title>Hierarchical conversation structure prediction in multi-party chat.</title>
<date>2012</date>
<booktitle>In Proceedings of SIGDIAL Meeting on Discourse and Dialogue.</booktitle>
<marker>Mayfield, Adamson, Ros´e, 2012</marker>
<rawString>Elijah Mayfield, David Adamson, and Carolyn Penstein Ros´e. 2012a. Hierarchical conversation structure prediction in multi-party chat. In Proceedings of SIGDIAL Meeting on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elijah Mayfield</author>
<author>Miaomiao Wen</author>
<author>Mitch Golant</author>
<author>Carolyn Penstein Ros´e</author>
</authors>
<title>Discovering habits of effective online support group chatrooms.</title>
<date>2012</date>
<booktitle>In ACM Conference on Supporting Group Work.</booktitle>
<marker>Mayfield, Wen, Golant, Ros´e, 2012</marker>
<rawString>Elijah Mayfield, Miaomiao Wen, Mitch Golant, and Carolyn Penstein Ros´e. 2012b. Discovering habits of effective online support group chatrooms. In ACM Conference on Supporting Group Work.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kanayo Ogura</author>
<author>Takashi Kusumi</author>
<author>Asako Miura</author>
</authors>
<title>Analysis of community development using chat logs: A virtual support group of cancer patients.</title>
<date>2008</date>
<booktitle>In Proceedings of the IEEE Symposium on Universal Communication.</booktitle>
<contexts>
<context position="6728" citStr="Ogura et al., 2008" startWordPosition="996" endWordPosition="999">onstrated the ability to distinguish empowerment language in written texts, including prompted writing samples (Pennebaker and Seagal, 1999), nar1Definitions of empowerment are closely related to the notion of self-efficacy (Bandura, 1997). For simplicity, we use the former term exclusively in this paper. Table 1: Empowerment label distribution in our corpus. Annotation Label # % Self-Empowerment NA 1522 79.3 POS 202 10.5 NEG 196 10.2 Other-Empowerment NA 1560 81.3 POS 217 11.3 NEG 143 7.4 ratives in online forums (Hoybye et al., 2005), and some preliminary analysis of synchronous discussion (Ogura et al., 2008; Mayfield et al., 2012b). These transitional works have used limited analysis methodology; in the absence of sophisticated natural language processing, their conclusions often rely on coarse measures, such as word counts and proportions of annotations in a text. Users, of course, do not express empowerment in every thread in which they participate, which leads to a challenge for machine learning. Threads often focus on a single user’s experiences, in which most participants in a chat are merely commentators, if they participate at all, matching previous research on shifts in speaker salience </context>
</contexts>
<marker>Ogura, Kusumi, Miura, 2008</marker>
<rawString>Kanayo Ogura, Takashi Kusumi, and Asako Miura. 2008. Analysis of community development using chat logs: A virtual support group of cancer patients. In Proceedings of the IEEE Symposium on Universal Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason E Owen</author>
<author>Erin O’Carroll Bantum</author>
<author>Mitch Golant</author>
</authors>
<title>Benefits and challenges experienced by professional facilitators of online support groups for cancer survivors. In Psycho-Oncology.</title>
<date>2008</date>
<contexts>
<context position="5975" citStr="Owen et al., 2008" startWordPosition="878" endWordPosition="881">cross professionals, but broadly relates to “the power to act efficaciously to bring about desired results” (Boehm and Staples, 2002) and “experiencing personal growth as a result of developing skills and abilities along with a more positive self-definition” (Staples, 1990). Participants in online support groups feel increased empowerment (Uden-Kraan et al., 2009; Barak et al., 2008). Quantitative studies have shown the effect of empowerment through statistical methods such as structural equation modeling (Vauth et al., 2007), as have qualitative methods such as deductive transcript analysis (Owen et al., 2008) and interview studies (Wahlin et al., 2006). The transition between these styles of research has been gradual. Pioneering work has demonstrated the ability to distinguish empowerment language in written texts, including prompted writing samples (Pennebaker and Seagal, 1999), nar1Definitions of empowerment are closely related to the notion of self-efficacy (Bandura, 1997). For simplicity, we use the former term exclusively in this paper. Table 1: Empowerment label distribution in our corpus. Annotation Label # % Self-Empowerment NA 1522 79.3 POS 202 10.5 NEG 196 10.2 Other-Empowerment NA 1560 </context>
</contexts>
<marker>Owen, Bantum, Golant, 2008</marker>
<rawString>Jason E. Owen, Erin O’Carroll Bantum, and Mitch Golant. 2008. Benefits and challenges experienced by professional facilitators of online support groups for cancer survivors. In Psycho-Oncology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2199" citStr="Pang and Lee, 2004" startWordPosition="308" endWordPosition="311">omena in conversation that cannot be easily identified through questionnaire data, self-reported surveys, or easily extracted user metadata. Examples of these social phenomena in conversation include overt displays of power (Prabhakaran et al., 2012) or indicators of rapport and relationship building (Wang et al., 2012). Manually annotating these social phenomena cannot scale to large data, so researchers turn to automated annotation of transcripts (Ros´e et al., 2008). While machine learning is highly effective for annotation tasks with relatively balanced labels, such as sentiment analysis (Pang and Lee, 2004), more complex social functions are often rarer. This leads to unbalanced class label distributions and a much more difficult machine learning task. Moreover, features indicative of rare social annotations tend to be drowned out in favor of features biased towards the majority class. The net effect is that classification algorithms tend to bias towards the majority class, giving low accuracy for rare class detection. Automated annotation of social phenomena also brings opportunities for real-world applications. For example, real-time annotation of conversation can power adaptive intervention i</context>
<context position="13062" citStr="Pang and Lee, 2004" startWordPosition="1983" endWordPosition="1986">ur algorithm allows us to evaluate the impact of rules against a baseline, and to iteratively judge each rule atop the changes made by previous rules. This algorithm fits into existing language technologies research which has attempted to partition documents into sections which are more or less relevant for classification. Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al., 2003). The approach of content selection, meanwhile, has been explored for sentiment analysis (Pang and Lee, 2004), where individual sentences may be less subjective and therefore less relevant to the sentiment classification task. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. Our work is distinct from these approaches. While we have coarse-grained annotations of empowerment, there is no direct annotation of what makes a good cue for content selection. With our cues, we hope to take advantage of sh</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bambang Parmanto</author>
<author>Paul Munro</author>
<author>Howard R Doyle</author>
</authors>
<title>Improving committee diagnosis with resampling techniques.</title>
<date>1996</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="16380" citStr="Parmanto et al., 1996" startWordPosition="2566" endWordPosition="2569">rring only before the last occurrence of c. We define an ensemble member E = (R, fR) - the ordered list of learned content selection rules R = [r1, r2,... ] and a classifier fC trained on instances transformed by those rules. Our final out4Future work may examine the interaction of jointly annotating multiple sparse social phenomena. 107 Figure 2: Effects of content selection rules, based on a cue feature (ovals) observed at lines m and n. put of a trained model is a set of ensemble members {E1, ... , Ek}. 4.2 Algorithm Our ensemble learning follows the paradigm of cross-validated committees (Parmanto et al., 1996), where k ensemble members are trained by subdividing our training data into k subfolds. For each ensemble classifier, cue rules R are generated on k − 1 subfolds (Trk) and evaluated on the remaining subfold (Tek). In practice, with 21 training documents, 7-fold cross-validation, and k = 3 ensemble members, each generation set consists of 12 documents’ instances, while each evaluation set contains instances from 6 documents. Our full algorithm is presented in Algorithm 1, and is broken into component parts for clarity. Algorithm 2 begins by measuring the baseline classifier’s ability to recogn</context>
<context position="18823" citStr="Parmanto et al., 1996" startWordPosition="2969" endWordPosition="2972"> and apply it to the feature set. We then iteratively progress through our now-ranked list of candidates, each time treating the newly filtered dataset as our new baseline. We search only top candidates for efficiency, following the fixed-width search methodology for feature selection in very high-dimensionality feature spaces (G¨utlein et al., 2009). Each ensemble classifier is finally retrained on all training data, after applying the corresponding content selection rules to that data. 5 Prediction Our prediction algorithm begins with a standard implementation of cross-validated committees (Parmanto et al., 1996), whose results are aggregated with a confidence voting method intended to favor rare labels (Erp et al., 2002). Cross-validated committees are an ensemble technique used to subsample training data to produce multiple hypotheses for classification. Each classifier produced by our cue-based transformation is trained on a subset of our training data. Each makes predictions on all test set instances, producing a distribution of confidence across possible labels. These values serve as inputs to a voting method to produce a final label for each instance. Compared to other ensemble methods, crossval</context>
</contexts>
<marker>Parmanto, Munro, Doyle, 1996</marker>
<rawString>Bambang Parmanto, Paul Munro, and Howard R Doyle. 1996. Improving committee diagnosis with resampling techniques. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>J D Seagal</author>
</authors>
<title>Forming a story: The health benefits of narrative.</title>
<date>1999</date>
<journal>Journal of Clinical Psychology.</journal>
<contexts>
<context position="6250" citStr="Pennebaker and Seagal, 1999" startWordPosition="919" endWordPosition="922">es, 1990). Participants in online support groups feel increased empowerment (Uden-Kraan et al., 2009; Barak et al., 2008). Quantitative studies have shown the effect of empowerment through statistical methods such as structural equation modeling (Vauth et al., 2007), as have qualitative methods such as deductive transcript analysis (Owen et al., 2008) and interview studies (Wahlin et al., 2006). The transition between these styles of research has been gradual. Pioneering work has demonstrated the ability to distinguish empowerment language in written texts, including prompted writing samples (Pennebaker and Seagal, 1999), nar1Definitions of empowerment are closely related to the notion of self-efficacy (Bandura, 1997). For simplicity, we use the former term exclusively in this paper. Table 1: Empowerment label distribution in our corpus. Annotation Label # % Self-Empowerment NA 1522 79.3 POS 202 10.5 NEG 196 10.2 Other-Empowerment NA 1560 81.3 POS 217 11.3 NEG 143 7.4 ratives in online forums (Hoybye et al., 2005), and some preliminary analysis of synchronous discussion (Ogura et al., 2008; Mayfield et al., 2012b). These transitional works have used limited analysis methodology; in the absence of sophisticate</context>
</contexts>
<marker>Pennebaker, Seagal, 1999</marker>
<rawString>James W Pennebaker and J D Seagal. 1999. Forming a story: The health benefits of narrative. Journal of Clinical Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardine M Pinto</author>
<author>Nancy C Maruyama</author>
<author>Matthew M Clark</author>
<author>Dean G Cruess</author>
<author>Elyse Park</author>
<author>Mary Roberts</author>
</authors>
<title>Motivation to modify lifestyle risk behaviors in women treated for breast cancer. In Mayo Clinic Proceedings.</title>
<date>2002</date>
<contexts>
<context position="30968" citStr="Pinto et al., 2002" startWordPosition="4945" endWordPosition="4948">category may serve as a proxy for content length - article-less instances in our corpus include one-line social greetings and exchanges of contact information. Swear words may be a cue for awareness of self-empowerment - a recent study of women coping with illness reported that swearing in the presence of others, but not alone, was related to potentially harmful outcomes (Robbins et al., 2011). Among other- oriented split rules, Eating stands out as non-obvious, although medical literature has suggested a link between dietary behavior and empowerment attitudes in a study of women with cancer (Pinto et al., 2002). 8 Conclusion We have demonstrated an algorithm for improving automated classification accuracy on highly skewed tasks for conversational data. This algorithm, particularly its focus on content selection, is rooted in the structural format of our data, which can generalize to many tasks involving conversational data. Our experiments show that this model significantly improves machine learning performance. Our algorithm is taking advantage of structural facets of discourse markers, lending basic sociolinguistic validity to its behavior. Though we have treated each of these rarely-occurring lab</context>
</contexts>
<marker>Pinto, Maruyama, Clark, Cruess, Park, Roberts, 2002</marker>
<rawString>Bernardine M Pinto, Nancy C Maruyama, Matthew M Clark, Dean G Cruess, Elyse Park, and Mary Roberts. 2002. Motivation to modify lifestyle risk behaviors in women treated for breast cancer. In Mayo Clinic Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Dimensionality of dialogue act tagsets: An empirical analysis of large corpora.</title>
<date>2008</date>
<booktitle>In Language Resources and Evaluation.</booktitle>
<contexts>
<context position="9439" citStr="Popescu-Belis, 2008" startWordPosition="1425" endWordPosition="1426"> manual based on definitions like those in Section 2. Each user was assigned a label of positive or negative empowerment if they exhibited such emotions, or was left blank if they did not do so within the context of that thread. This annotation was performed both for their self-empowerment as well as their attitude towards others’ situations (other-empowerment). An example of this annotation for self-empowerment is presented in Figure 1 and the distribution of labels is given in Table 1. Most previous annotation tasks attempt to annotate on a per-utterance basis, such as dialogue act tagging (Popescu-Belis, 2008), or on arbitrary spans of text, such as in the MPQA subjectivity corpus (Wiebe et al., 2005). However, for our task, a per-user, per-thread annotation is more appropriate, because empowerment is often indicated best through narrative (Hoybye et al., 2005). Human annotators are instructed to take this context into account when annotating (Mayfield et al., 2012b). It would therefore be nonsensical to annotate individual lines as “embodying” empowerment. Similar arguments have been made for sentiment, especially as the field moves towards aspect-oriented sentiment (Breck et al., 2007). Assigning</context>
</contexts>
<marker>Popescu-Belis, 2008</marker>
<rawString>Andrei Popescu-Belis. 2008. Dimensionality of dialogue act tagsets: An empirical analysis of large corpora. In Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Predicting overt display of power in written dialogs.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1830" citStr="Prabhakaran et al., 2012" startWordPosition="252" endWordPosition="255">ettings and into natural environments. With this influx of interest comes new methodology, and the inevitable question arises of how to move towards testable hypotheses, using these uncontrolled sources of data as scientific lenses into the real world. The study of conversational transcripts is a key domain in this new frontier. There are certain social and behavioral phenomena in conversation that cannot be easily identified through questionnaire data, self-reported surveys, or easily extracted user metadata. Examples of these social phenomena in conversation include overt displays of power (Prabhakaran et al., 2012) or indicators of rapport and relationship building (Wang et al., 2012). Manually annotating these social phenomena cannot scale to large data, so researchers turn to automated annotation of transcripts (Ros´e et al., 2008). While machine learning is highly effective for annotation tasks with relatively balanced labels, such as sentiment analysis (Pang and Lee, 2004), more complex social functions are often rarer. This leads to unbalanced class label distributions and a much more difficult machine learning task. Moreover, features indicative of rare social annotations tend to be drowned out in</context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2012. Predicting overt display of power in written dialogs. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="11318" citStr="Prasad et al., 2008" startWordPosition="1712" endWordPosition="1715">dicate any empowerment themselves. This exacerbates the likelihood of instances being classified as N/A. We choose to take advantage of these attributes of threads. We know from research in discourse analysis that many sections of conversations are formulaic and rote, like introductions and greetings (Schegloff, 1968). We additionally know that polarity often shifts in dialogue through the use of discourse connectives such as conjunctions and transitional phrases. These issues have been addressed in work in the language technologies community, most notably through the Penn Discourse Treebank (Prasad et al., 2008); however, their applications to noisier synchronous conversation has beenrare in computational linguistics. With these linguistic insights in mind, we examine how we can make best use of them for machine learning performance. While techniques for predicting rare events (Weiss and Hirsh, 1998) and compensating for class imbalance (Frank and 106 Bouckaert, 2006), these approaches generally focus on statistical properties of large class sets without taking the nature of their datasets into account. In the next section, we propose a new algorithm which takes advantage specifically of the linguist</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The penn discourse treebank 2.0. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megan L Robbins</author>
<author>Elizabeth S Focella</author>
<author>Shelley Kasle</author>
<author>Ana Mar´ıa L´opez</author>
<author>Karen L Weihs</author>
<author>Matthias R Mehl</author>
</authors>
<title>Naturalistically observed swearing, emotional support, and depressive symptoms in women coping with illness.</title>
<date>2011</date>
<pages>30--789</pages>
<publisher>Health Psychology,</publisher>
<marker>Robbins, Focella, Kasle, L´opez, Weihs, Mehl, 2011</marker>
<rawString>Megan L Robbins, Elizabeth S Focella, Shelley Kasle, Ana Mar´ıa L´opez, Karen L Weihs, and Matthias R Mehl. 2011. Naturalistically observed swearing, emotional support, and depressive symptoms in women coping with illness. Health Psychology, 30:789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Penstein Ros´e</author>
<author>Yi-Chia Wang</author>
<author>Yue Cui</author>
<author>Jaime Arguello</author>
<author>Karsten Stegmann</author>
<author>Armin Weinberger</author>
<author>Frank Fischer</author>
</authors>
<title>Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computer-supported collaborative learning.</title>
<date>2008</date>
<journal>In International Journal of Computer Supported Collaborative Learning.</journal>
<marker>Ros´e, Wang, Cui, Arguello, Stegmann, Weinberger, Fischer, 2008</marker>
<rawString>Carolyn Penstein Ros´e, Yi-Chia Wang, Yue Cui, Jaime Arguello, Karsten Stegmann, Armin Weinberger, and Frank Fischer. 2008. Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computer-supported collaborative learning. In International Journal of Computer Supported Collaborative Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikol Rummel</author>
<author>Armin Weinberger</author>
<author>Christof Wecker</author>
<author>Frank Fischer</author>
<author>Anne Meier</author>
<author>Eleni Voyiatzaki</author>
<author>George Kahrimanis</author>
<author>Hans Spada</author>
<author>Nikolaos Avouris</author>
<author>Erin Walker</author>
</authors>
<title>New challenges in cscl: Towards adaptive script support.</title>
<date>2008</date>
<booktitle>In Proceedings of ICLS.</booktitle>
<contexts>
<context position="2853" citStr="Rummel et al., 2008" startWordPosition="404" endWordPosition="407"> often rarer. This leads to unbalanced class label distributions and a much more difficult machine learning task. Moreover, features indicative of rare social annotations tend to be drowned out in favor of features biased towards the majority class. The net effect is that classification algorithms tend to bias towards the majority class, giving low accuracy for rare class detection. Automated annotation of social phenomena also brings opportunities for real-world applications. For example, real-time annotation of conversation can power adaptive intervention in collaborative learning settings (Rummel et al., 2008; Adamson and Ros´e, 2012). However, with the considerable power of automation comes great responsibility. It is critical to avoid intervening in the case of erroneous annotations, as providing unnecessary or inappropriate support in such a setting has been shown to be harmful to group performance and social cohesion (Dillenbourg, 2002; Stahl, 2012). We propose adaptations to existing machine learning algorithms which improve recognition of rare annotations in conversational text data. Our primary contribution comes in the form of withininstance content selection. We develop a novel algorithm </context>
</contexts>
<marker>Rummel, Weinberger, Wecker, Fischer, Meier, Voyiatzaki, Kahrimanis, Spada, Avouris, Walker, 2008</marker>
<rawString>Nikol Rummel, Armin Weinberger, Christof Wecker, Frank Fischer, Anne Meier, Eleni Voyiatzaki, George Kahrimanis, Hans Spada, Nikolaos Avouris, and Erin Walker. 2008. New challenges in cscl: Towards adaptive script support. In Proceedings of ICLS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatically generating wikipedia articles: A structureaware approach.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13359" citStr="Sauper and Barzilay, 2009" startWordPosition="2028" endWordPosition="2031"> less relevant for classification. Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al., 2003). The approach of content selection, meanwhile, has been explored for sentiment analysis (Pang and Lee, 2004), where individual sentences may be less subjective and therefore less relevant to the sentiment classification task. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. Our work is distinct from these approaches. While we have coarse-grained annotations of empowerment, there is no direct annotation of what makes a good cue for content selection. With our cues, we hope to take advantage of shallow discourse structure in conversation, such as contrastive markers, making use of implicit structure in the conversational domain. 4.1 Notation Before describing extensions to the baseline logistic regression model, we define notation. Our data is arranged hierarchically. We assume that we ha</context>
</contexts>
<marker>Sauper, Barzilay, 2009</marker>
<rawString>Christina Sauper and Regina Barzilay. 2009. Automatically generating wikipedia articles: A structureaware approach. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
</authors>
<title>Sequencing in conversational openings.</title>
<date>1968</date>
<journal>American Anthropologist.</journal>
<contexts>
<context position="11017" citStr="Schegloff, 1968" startWordPosition="1668" endWordPosition="1669">tly, while taking context into account is important for reliable annotation, it leads to extraneous information in many cases. Many threads can have multiple lines of contributions that are topically related to an expression of empowerment (and thus belong in the same thread), but which do not indicate any empowerment themselves. This exacerbates the likelihood of instances being classified as N/A. We choose to take advantage of these attributes of threads. We know from research in discourse analysis that many sections of conversations are formulaic and rote, like introductions and greetings (Schegloff, 1968). We additionally know that polarity often shifts in dialogue through the use of discourse connectives such as conjunctions and transitional phrases. These issues have been addressed in work in the language technologies community, most notably through the Penn Discourse Treebank (Prasad et al., 2008); however, their applications to noisier synchronous conversation has beenrare in computational linguistics. With these linguistic insights in mind, we examine how we can make best use of them for machine learning performance. While techniques for predicting rare events (Weiss and Hirsh, 1998) and </context>
</contexts>
<marker>Schegloff, 1968</marker>
<rawString>Emanuel A Schegloff. 1968. Sequencing in conversational openings. American Anthropologist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver G Selfridge</author>
</authors>
<title>Pandemonium: a paradigm for learning.</title>
<date>1958</date>
<booktitle>In Proceedings of Symposium on Mechanisation of Thought Processes,</booktitle>
<institution>National Physical Laboratory.</institution>
<contexts>
<context position="19982" citStr="Selfridge, 1958" startWordPosition="3160" endWordPosition="3161">ch instance. Compared to other ensemble methods, crossvalidated committees as described above are a good fit for our task, because of its unique unit of analysis. As thread-level analysis is the set of individual participants’ turns in a conversation, we risk overfitting if we sample from the same conversations for the training and testing sets. In contrast to standard bagging, hard sampling boundaries never train and test on instances drawn from the same conversation. To aggregate the votes from members of this ensemble into a final prediction, we employ a variant on Selfridge’s Pandemonium (Selfridge, 1958). If a minority label is selected as the highestconfidence value in any classifier in our ensemble, it is selected. The majority label, by contrast, is only selected if it is the most likely prediction by all classifiers in our ensemble. Thus consensus is required to elect the majority class, and the strongest minority candidate is elected otherwise. 108 In : generation set Trk, evaluation set Tek Out: ensemble committee {E, ... Ek} fori=1tokdo Rfinal +- [ ]; Xfreq +- {x E X I freq(x) E Trk &gt; 5%}; R +- Xfreq X T; R* +- R; repeat Pbase +- EvaluateClassifier(Trk, Tek); EvaluateRules(Pbase, Trk, </context>
</contexts>
<marker>Selfridge, 1958</marker>
<rawString>Oliver G Selfridge. 1958. Pandemonium: a paradigm for learning. In Proceedings of Symposium on Mechanisation of Thought Processes, National Physical Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerry Stahl</author>
</authors>
<title>Interaction analysis of a biology chat. Productive multivocality.</title>
<date>2012</date>
<contexts>
<context position="3204" citStr="Stahl, 2012" startWordPosition="460" endWordPosition="461">y for rare class detection. Automated annotation of social phenomena also brings opportunities for real-world applications. For example, real-time annotation of conversation can power adaptive intervention in collaborative learning settings (Rummel et al., 2008; Adamson and Ros´e, 2012). However, with the considerable power of automation comes great responsibility. It is critical to avoid intervening in the case of erroneous annotations, as providing unnecessary or inappropriate support in such a setting has been shown to be harmful to group performance and social cohesion (Dillenbourg, 2002; Stahl, 2012). We propose adaptations to existing machine learning algorithms which improve recognition of rare annotations in conversational text data. Our primary contribution comes in the form of withininstance content selection. We develop a novel algorithm based on textual cues, suppressing information which is likely to be irrelevant to an instance’s class label. This allows features which predict minority classes to gain prominence, helping to sidestep the frequency of common features pointing to a majority class label. Additionally, we propose modifications to existing algorithms. First, we identif</context>
</contexts>
<marker>Stahl, 2012</marker>
<rawString>Gerry Stahl. 2012. Interaction analysis of a biology chat. Productive multivocality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee H Staples</author>
</authors>
<title>Powerful ideas about empowerment. Administration in Social Work.</title>
<date>1990</date>
<contexts>
<context position="5631" citStr="Staples, 1990" startWordPosition="829" endWordPosition="830">features it uses most. We conclude with a discussion of particularly useful applications of this work. 2 Background We ground this paper’s discussion of machine learning with a real problem, turning to the annotation of empowerment language in chat1. The concept of empowerment, while a prolific area of research, lacks a broad definition across professionals, but broadly relates to “the power to act efficaciously to bring about desired results” (Boehm and Staples, 2002) and “experiencing personal growth as a result of developing skills and abilities along with a more positive self-definition” (Staples, 1990). Participants in online support groups feel increased empowerment (Uden-Kraan et al., 2009; Barak et al., 2008). Quantitative studies have shown the effect of empowerment through statistical methods such as structural equation modeling (Vauth et al., 2007), as have qualitative methods such as deductive transcript analysis (Owen et al., 2008) and interview studies (Wahlin et al., 2006). The transition between these styles of research has been gradual. Pioneering work has demonstrated the ability to distinguish empowerment language in written texts, including prompted writing samples (Pennebake</context>
</contexts>
<marker>Staples, 1990</marker>
<rawString>Lee H Staples. 1990. Powerful ideas about empowerment. Administration in Social Work.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>The psychological meaning of words: Liwc and computerized text analysis methods.</title>
<date>2010</date>
<journal>Journal of Language and Social Psychology.</journal>
<contexts>
<context position="23950" citStr="Tausczik and Pennebaker, 2010" startWordPosition="3833" endWordPosition="3836"> lexical feature space (Han et al., 2001). To compensate, we employ a smaller, denser set of binary features for tree stump screening: instance length thresholds and LIWC category membership. First, we define a set of features that split based on the number of lines an instance contains, from 1 to 10 (only a tiny fraction of instances are more than 10 lines long). For example, a feature splitting on instances with lines &lt; 2 would be true for one- and two-line instances, and false for all others. Second, we define a feature for each category in the Linguistic Inquiry and Word Count dictionary (Tausczik and Pennebaker, 2010) - these broad classes of words allow for more balanced 109 Figure 3: Precision/recall curves for algorithms. After 50% recall all models converge and there are no significant differences in performance. splits than would unigrams alone. Each category’s feature is true if any word in that category was used at least once in that instance. We exhaustively sweep this feature space, and report the most successful stump rules for each annotation task. In our other experiments, we report results with and without the best rule for this preprocessing step; we also measure its impact alone. 6 Experimen</context>
</contexts>
<marker>Tausczik, Pennebaker, 2010</marker>
<rawString>Yla R Tausczik and James W Pennebaker. 2010. The psychological meaning of words: Liwc and computerized text analysis methods. Journal of Language and Social Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientic articles: Experiments with relevance and rhetorical status. Computational Linguistics.</title>
<date>2002</date>
<contexts>
<context position="13311" citStr="Teufel and Moens, 2002" startWordPosition="2021" endWordPosition="2024">ion documents into sections which are more or less relevant for classification. Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al., 2003). The approach of content selection, meanwhile, has been explored for sentiment analysis (Pang and Lee, 2004), where individual sentences may be less subjective and therefore less relevant to the sentiment classification task. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. Our work is distinct from these approaches. While we have coarse-grained annotations of empowerment, there is no direct annotation of what makes a good cue for content selection. With our cues, we hope to take advantage of shallow discourse structure in conversation, such as contrastive markers, making use of implicit structure in the conversational domain. 4.1 Notation Before describing extensions to the baseline logistic regression model, we define notation. Our data </context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientic articles: Experiments with relevance and rhetorical status. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Van Uden-Kraan</author>
<author>C H C Drossaert</author>
<author>E Taal</author>
</authors>
<title>Participation in online patient support groups endorses patients empowerment. Patient Education and Counseling.</title>
<date>2009</date>
<journal>E R Seydel, and M A F J Van</journal>
<marker>Van Uden-Kraan, Drossaert, Taal, 2009</marker>
<rawString>C F Van Uden-Kraan, C H C Drossaert, E Taal, E R Seydel, and M A F J Van de Laar. 2009. Participation in online patient support groups endorses patients empowerment. Patient Education and Counseling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Vauth</author>
<author>B Kleim</author>
<author>M Wirtz</author>
<author>P W Corrigan</author>
</authors>
<title>Self-efficacy and empowerment as outcomes of selfstigmatizing and coping in schizophrenia. Psychiatry Research.</title>
<date>2007</date>
<contexts>
<context position="5888" citStr="Vauth et al., 2007" startWordPosition="865" endWordPosition="868">he concept of empowerment, while a prolific area of research, lacks a broad definition across professionals, but broadly relates to “the power to act efficaciously to bring about desired results” (Boehm and Staples, 2002) and “experiencing personal growth as a result of developing skills and abilities along with a more positive self-definition” (Staples, 1990). Participants in online support groups feel increased empowerment (Uden-Kraan et al., 2009; Barak et al., 2008). Quantitative studies have shown the effect of empowerment through statistical methods such as structural equation modeling (Vauth et al., 2007), as have qualitative methods such as deductive transcript analysis (Owen et al., 2008) and interview studies (Wahlin et al., 2006). The transition between these styles of research has been gradual. Pioneering work has demonstrated the ability to distinguish empowerment language in written texts, including prompted writing samples (Pennebaker and Seagal, 1999), nar1Definitions of empowerment are closely related to the notion of self-efficacy (Bandura, 1997). For simplicity, we use the former term exclusively in this paper. Table 1: Empowerment label distribution in our corpus. Annotation Label</context>
</contexts>
<marker>Vauth, Kleim, Wirtz, Corrigan, 2007</marker>
<rawString>R Vauth, B Kleim, M Wirtz, and P W Corrigan. 2007. Self-efficacy and empowerment as outcomes of selfstigmatizing and coping in schizophrenia. Psychiatry Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Wahlin</author>
<author>Anna-Christina Ek</author>
<author>Ewa Idvali</author>
</authors>
<title>Patient empowerment in intensive carean interview study. Intensive and Critical Care Nursing.</title>
<date>2006</date>
<contexts>
<context position="6019" citStr="Wahlin et al., 2006" startWordPosition="885" endWordPosition="888">o “the power to act efficaciously to bring about desired results” (Boehm and Staples, 2002) and “experiencing personal growth as a result of developing skills and abilities along with a more positive self-definition” (Staples, 1990). Participants in online support groups feel increased empowerment (Uden-Kraan et al., 2009; Barak et al., 2008). Quantitative studies have shown the effect of empowerment through statistical methods such as structural equation modeling (Vauth et al., 2007), as have qualitative methods such as deductive transcript analysis (Owen et al., 2008) and interview studies (Wahlin et al., 2006). The transition between these styles of research has been gradual. Pioneering work has demonstrated the ability to distinguish empowerment language in written texts, including prompted writing samples (Pennebaker and Seagal, 1999), nar1Definitions of empowerment are closely related to the notion of self-efficacy (Bandura, 1997). For simplicity, we use the former term exclusively in this paper. Table 1: Empowerment label distribution in our corpus. Annotation Label # % Self-Empowerment NA 1522 79.3 POS 202 10.5 NEG 196 10.2 Other-Empowerment NA 1560 81.3 POS 217 11.3 NEG 143 7.4 ratives in onl</context>
</contexts>
<marker>Wahlin, Ek, Idvali, 2006</marker>
<rawString>Ingrid Wahlin, Anna-Christina Ek, and Ewa Idvali. 2006. Patient empowerment in intensive carean interview study. Intensive and Critical Care Nursing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Samantha Finkelstein</author>
<author>Amy Ogan</author>
<author>Alan Black</author>
<author>Justine Cassell</author>
</authors>
<title>love ya, jerkface:” using sparse log-linear models to build positive (and impolite) relationships with teens.</title>
<date>2012</date>
<booktitle>In Proceedings of SIGDIAL.</booktitle>
<contexts>
<context position="1901" citStr="Wang et al., 2012" startWordPosition="263" endWordPosition="266">methodology, and the inevitable question arises of how to move towards testable hypotheses, using these uncontrolled sources of data as scientific lenses into the real world. The study of conversational transcripts is a key domain in this new frontier. There are certain social and behavioral phenomena in conversation that cannot be easily identified through questionnaire data, self-reported surveys, or easily extracted user metadata. Examples of these social phenomena in conversation include overt displays of power (Prabhakaran et al., 2012) or indicators of rapport and relationship building (Wang et al., 2012). Manually annotating these social phenomena cannot scale to large data, so researchers turn to automated annotation of transcripts (Ros´e et al., 2008). While machine learning is highly effective for annotation tasks with relatively balanced labels, such as sentiment analysis (Pang and Lee, 2004), more complex social functions are often rarer. This leads to unbalanced class label distributions and a much more difficult machine learning task. Moreover, features indicative of rare social annotations tend to be drowned out in favor of features biased towards the majority class. The net effect is</context>
</contexts>
<marker>Wang, Finkelstein, Ogan, Black, Cassell, 2012</marker>
<rawString>William Yang Wang, Samantha Finkelstein, Amy Ogan, Alan Black, and Justine Cassell. 2012. “love ya, jerkface:” using sparse log-linear models to build positive (and impolite) relationships with teens. In Proceedings of SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary M Weiss</author>
<author>Haym Hirsh</author>
</authors>
<title>Learning to predict rare events in event sequences.</title>
<date>1998</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="11612" citStr="Weiss and Hirsh, 1998" startWordPosition="1756" endWordPosition="1759">greetings (Schegloff, 1968). We additionally know that polarity often shifts in dialogue through the use of discourse connectives such as conjunctions and transitional phrases. These issues have been addressed in work in the language technologies community, most notably through the Penn Discourse Treebank (Prasad et al., 2008); however, their applications to noisier synchronous conversation has beenrare in computational linguistics. With these linguistic insights in mind, we examine how we can make best use of them for machine learning performance. While techniques for predicting rare events (Weiss and Hirsh, 1998) and compensating for class imbalance (Frank and 106 Bouckaert, 2006), these approaches generally focus on statistical properties of large class sets without taking the nature of their datasets into account. In the next section, we propose a new algorithm which takes advantage specifically of the linguistic phenomena in the conversation-based data that we study for empowerment detection. As such, our algorithm is highly suited to this data and task, with the necessary tradeoff in uncertain generality to new domains with unrelated data. 4 Cue Discovery for Content Selection Our algorithm perfor</context>
</contexts>
<marker>Weiss, Hirsh, 1998</marker>
<rawString>Gary M Weiss and Haym Hirsh. 1998. Learning to predict rare events in event sequences. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation.</title>
<date>2005</date>
<contexts>
<context position="9532" citStr="Wiebe et al., 2005" startWordPosition="1440" endWordPosition="1443">ve or negative empowerment if they exhibited such emotions, or was left blank if they did not do so within the context of that thread. This annotation was performed both for their self-empowerment as well as their attitude towards others’ situations (other-empowerment). An example of this annotation for self-empowerment is presented in Figure 1 and the distribution of labels is given in Table 1. Most previous annotation tasks attempt to annotate on a per-utterance basis, such as dialogue act tagging (Popescu-Belis, 2008), or on arbitrary spans of text, such as in the MPQA subjectivity corpus (Wiebe et al., 2005). However, for our task, a per-user, per-thread annotation is more appropriate, because empowerment is often indicated best through narrative (Hoybye et al., 2005). Human annotators are instructed to take this context into account when annotating (Mayfield et al., 2012b). It would therefore be nonsensical to annotate individual lines as “embodying” empowerment. Similar arguments have been made for sentiment, especially as the field moves towards aspect-oriented sentiment (Breck et al., 2007). Assigning labels based on thread boundaries allows for context to be meaningfully taken into account, </context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>