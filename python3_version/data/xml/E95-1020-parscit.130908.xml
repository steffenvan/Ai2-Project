<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016609">
<title confidence="0.9823">
Distributional Part-of-Speech Tagging
</title>
<author confidence="0.905432">
Hinrich Schiitze
</author>
<affiliation confidence="0.798085">
CSLI, Ventura Hall
</affiliation>
<address confidence="0.910565">
Stanford, CA 94305-4115 , USA
</address>
<email confidence="0.833053">
email: schuetze@csli.stanford.edu
URL: ftp://csli.stanford.edu/pub/prosit/DisPosTag.ps
</email>
<sectionHeader confidence="0.988161" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899833333333">
This paper presents an algorithm for tag-
ging words whose part-of-speech proper-
ties are unknown. Unlike previous work,
the algorithm categorizes word tokens in
context instead of word types. The algo-
rithm is evaluated on the Brown Corpus.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911259259259">
Since online text becomes available in ever increas-
ing volumes and an ever increasing number of lan-
guages, there is a growing need for robust pro-
cessing techniques that can analyze text without
expensive and time-consuming adaptation to new
domains and genres. This need motivates research
on fully automatic text processing that may rely
on general principles of linguistics and computa-
tion, but does not depend on knowledge about
individual words.
In this paper, we describe an experiment on
fully automatic derivation of the knowledge nec-
essary for part-of-speech tagging. Part-of-speech
tagging is of interest for a number of applications,
for example access to text data bases (Kupiec,
1993), robust parsing (Abney, 1991), and general
parsing (deMarcken, 1990; Charniak et al., 1994).
The goal is to find an unsupervised method for
tagging that relies on general distributional prop-
erties of text, properties that are invariant across
languages and sublanguages. While the proposed
algorithm is not successful for all grammatical cat-
egories, it does show that fully automatic tagging
is possible when demands on accuracy are modest.
The following sections discuss related work, de-
scribe the learning procedure and evaluate it on
the Brown Corpus (Francis and KuCera, 1982).
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999941574468085">
The simplest part-of-speech taggers are bigram
or trigram models (Church, 1989; Charniak et
al., 1993). They require a relatively large tagged
training text. Transformation-based tagging as
introduced by Brill (1993) also requires a hand-
tagged text for training. No pretagged text is nec-
essary for Hidden Markov Models (Jelinek, 1985;
Cutting et al., 1991; Kupiec, 1992). Still, a lexi-
con is needed that specifies the possible parts of
speech for every word. Brill and Marcus (1992a)
have shown that the effort necessary to construct
the part-of-speech lexicon can be considerably re-
duced by combining learning procedures and a
partial part-of-speech categorization elicited from
an informant.
The present paper is concerned with tagging
languages and sublanguages for which no a priori
knowledge about grammatical categories is avail-
able, a situation that occurs often in practice
(Brill and Marcus, 1992a).
Several researchers have worked on learning
grammatical properties of words. Elman (1990)
trains a connectionist net to predict words, a pro-
cess that generates internal representations that
reflect grammatical category. Brill et al. (1990)
try to infer grammatical category from bi-
gram statistics. Finch and Chater (1992) and
Finch (1993) use vector models in which words are
clustered according to the similarity of their close
neighbors in a corpus. Kneser and Ney (1993)
present a probabilistic model for entropy maxi-
mization that also relies on the immediate neigh-
bors of words in a corpus. Biber (1993) ap-
plies factor analysis to collocations of two target
words (&amp;quot;certain&amp;quot; and &amp;quot;right&amp;quot;) with their immedi-
ate neighbors.
What these approaches have in common is that
they classify words instead of individual occur-
rences. Given the widespread part-of-speech am-
biguity of words this is problematic.&apos; How should
a word like &amp;quot;plant&amp;quot; be categorized if it has uses
both as a verb and as a noun? How can a cate-
gorization be considered meaningful if the infini-
tive marker &amp;quot;to&amp;quot; is not distinguished from the ho-
mophonous preposition?
In a previous paper (Schiltze, 1993), we trained
a neural network to disambiguate part-of-speech
</bodyText>
<footnote confidence="0.64081425">
Although Biber (1993) classifies collocations,
these can also be ambiguous. For example, &amp;quot;for cer-
tain&amp;quot; has both senses of &amp;quot;certain&amp;quot;: &amp;quot;particular&amp;quot; and
&amp;quot;sure&amp;quot;.
</footnote>
<page confidence="0.99335">
141
</page>
<note confidence="0.361431">
word side nearest neighbors
</note>
<tableCaption confidence="0.823783444444444">
onto left
onto right
seemed left
seemed right
into toward away off together against beside around down
reduce among regarding against towards plus toward using unlike
appeared might would remained had became could must should
seem seems wanted want going meant tried expect likely
Table 1: Words with most similar left and right neighbors for &amp;quot;onto&amp;quot; and &amp;quot;seemed&amp;quot;.
</tableCaption>
<bodyText confidence="0.999356545454545">
using context; however, no information about the
word that is to be categorized was used. This
scheme fails for cases like &amp;quot;The soldiers rarely
come home.&amp;quot; vs. &amp;quot;The soldiers will come home.&amp;quot;
where the context is identical and information
about the lexical item in question (&amp;quot;rarely&amp;quot; vs.
&amp;quot;will&amp;quot;) is needed in combination with context for
correct classification. In this paper, we will com-
pare two tagging algorithms, one based on clas-
sifying word types, and one based on classifying
words-plus-context.
</bodyText>
<sectionHeader confidence="0.994142" genericHeader="method">
3 Tag induction
</sectionHeader>
<bodyText confidence="0.999965555555556">
We start by constructing representations of the
syntactic behavior of a word. with respect to its
left and right context. Our working hypothe-
sis is that syntactic behavior is reflected in co-
occurrence patterns. Therefore, we will measure
the similarity between two words with respect to
their syntactic behavior to, say, their left side by
the degree to which they share the same neighbors
on the left. If the counts of neighbors are assem-
bled into a vector (with one dimension for each
neighbor), the cosine can be employed to measure
similarity. It will assign a value close to 1.0 if two
words share many neighbors, and 0.0 if they share
none. We refer to the vector of left neighbors of
a word as its left context vector, and to the vec-
tor of right neighbors as its right context vector.
The unreduced context vectors in the experiment
described here have 250 entries, corresponding to
the 250 most frequent words in the Brown corpus.
This basic idea of measuring distributional sim-
ilarity in terms of shared neighbors must be mod-
ified because of the sparseness of the data. Con-
sider two infrequent adjectives that happen to
modify different nouns in the corpus. Their right
similarity according to the cosine measure would
be zero. This is clearly undesirable. But even with
high-frequency words, the simple vector model can
yield misleading similarity measurements. A case
in point is &amp;quot;a&amp;quot; vs. &amp;quot;an&amp;quot;. These two articles do not
share any right neighbors since the former is only
used before consonants and the latter only before
vowels. Yet intuitively, they are similar with re-
spect to their right syntactic context despite the
lack of common right neighbors.
Our solution to these problems is the applica-
tion of a singular value decomposition. We can
represent the left vectors of all words in the cor-
pus as a matrix C with n rows, one for each word
whose left neighbors are to be represented, and k
columns, one for each of the possible neighbors.
SVD can be used to approximate the row and col-
umn vectors of C in a low-dimensional space. In
more detail, SVD decomposes a matrix C, the ma-
trix of left vectors in our case, into three matrices
To, So, and Do such that:
</bodyText>
<equation confidence="0.94208">
C = ToSoVo
</equation>
<bodyText confidence="0.999937348837209">
So is a diagonal k-by-k matrix that contains the
singular values of C in descending order. The ith
singular value can be interpreted as indicating the
strength of the ith principal component of C. To
and Do are orthonormal matrices that approxi-
mate the rows and columns of C, respectively. By
restricting the matrices To, So, and Do to their
first m &lt; k columns (= principal components)
one obtains the matrices T, S, and D. Their prod-
uct C is the best least square approximation of C
by a matrix of rank m: C = TSD&apos;. We chose
m = 50 (reduction to a 50-dimensional space) for
the SVD&apos;s described in this paper.
SVD addresses the problems of generalization
and sparseness because broad and stable general-
izations are represented on dimensions with large
values which will be retained in the dimensionality
reduction. In contrast, dimensions corresponding
to small singular values represent idiosyncrasies,
like the phonological constraint on the usage of
&amp;quot;an&amp;quot; vs. &amp;quot;a&amp;quot;, and will be dropped. We also gain
efficiency since we can manipulate smaller vectors,
reduced to 50 dimensions. We used SVDPACK
to compute the singular value decompositions de-
scribed in this paper (Berry, 1992).
Table 1 shows the nearest neighbors of two
words (ordered according to closeness to the head
word) after the dimensionality reduction. Neigh-
bors with highest similarity according to both
left and right context are listed. One can see
clear differences between the nearest neighbors in
the two spaces. The right-context neighbors of
&amp;quot;onto&amp;quot; contain verbs because both prepositions
and verbs govern noun phrases to their right.
The left-context neighborhood of &amp;quot;onto&amp;quot; reflects
the fact that prepositional phrases are used in
the same position as adverbs like &amp;quot;away&amp;quot; and
&amp;quot;together&amp;quot;, thus making their left context sim-
ilar. For &amp;quot;seemed&amp;quot;, left-context neighbors are
words that have similar types of noun phrases in
subject position (mainly auxiliaries). The right-
context neighbors all take &amp;quot;to&amp;quot;-infinitives as com-
plements. An adjective like &amp;quot;likely&amp;quot; is very sim-
</bodyText>
<page confidence="0.99062">
142
</page>
<bodyText confidence="0.999656692307692">
ilar to &amp;quot;seemed&amp;quot; in this respect although its left
context is quite different from that of &amp;quot;seemed&amp;quot;.
Similarly, the generalization that prepositions and
transitive verbs are very similar if not identical
in the way they govern noun phrases would be
lost if &amp;quot;left&amp;quot; and &amp;quot;right&amp;quot; properties of words were
lumped together in one representation. These ex-
amples demonstrate the importance of represent-
ing generalizations about left and right context
separately.
The left and right context vectors are the basis
for four different tag induction experiments, which
are described in detail below:
</bodyText>
<listItem confidence="0.996961857142857">
• induction based on word type only
• induction based on word type and context
• induction based on word type and context,
restricted to &amp;quot;natural&amp;quot; contexts
• induction based on word type and context,
using generalized left and right context vec-
tors
</listItem>
<subsectionHeader confidence="0.996024">
3.1 Induction based on word type only
</subsectionHeader>
<bodyText confidence="0.99981275">
The two context vectors of a word characterize the
distribution of neighboring words to its left and
right. The concatenation of left and right context
vector can therefore serve as a representation of a
word&apos;s distributional behavior (Finch and Chater,
1992; Schiitze, 1993). We formed such concate-
nated vectors for all 47,025 words (surface forms)
in the Brown corpus. Here, we use the raw 250-
dimensional context vectors and apply the SVD
to the 47,025-by-500 matrix (47,025 words with
two 250-dimensional context vectors each). We
obtained 47,025 50-dimensional reduced vectors
from the SVD and clustered them into 200 classes
using the fast clustering algorithm Buckshot (Cut-
ting et al., 1992) (group average agglomeration ap-
plied to a sample). This classification constitutes
the baseline performance for distributional part-
of-speech tagging. All occurrences of a word are
assigned to one class. As pointed out above, such
a procedure is problematic for ambiguous words.
</bodyText>
<subsectionHeader confidence="0.9343935">
3.2 Induction based on word type and
context
</subsectionHeader>
<bodyText confidence="0.998053">
In order to exploit contextual information in the
classification of a token, we simply use context
vectors of the two words occurring next to the
token. An occurrence of word w is represented by
a concatenation of four context vectors:
</bodyText>
<listItem confidence="0.9711218">
• The right context vector of the preceding
word.
• The left context vector of w.
• The right context vector of w.
• The left context vector of the following word.
</listItem>
<bodyText confidence="0.999510516129032">
The motivation is that a word&apos;s syntactic role
depends both on the syntactic properties of its
neighbors and on its own potential for entering
into syntactic relationships with these neighbors.
The only properties of context that we consider
are the right-context vector of the preceding word
and the left-context vector of the following word
because they seem to represent the contextual in-
formation most important for the categorization
of w. For example, for the disambiguation of
&amp;quot;work&amp;quot; in &amp;quot;her work seemed to be important&amp;quot;,
only the fact that &amp;quot;seemed&amp;quot; expects noun phrases
to its left is important, the right context vector of
&amp;quot;seemed&amp;quot; does not contribute to disambiguation.
That only the immediate neighbors are crucial for
categorization is clearly a simplification, but as
the results presented below show it seems to work
surprisingly well.
Again, an SVD is applied to address the prob-
lems of sparseness and generalization. We ran-
domly selected 20,000 word triplets from the cor-
pus and formed concatenations of four context
vectors as described above. The singular value de-
composition of the resulting 20,000-by-1,000 ma-
trix defines a mapping from the 1,000-dimensional
space of concatenated context vectors to a 50-
dimensional reduced space. Our tag set was then
induced by clustering the reduced vectors of the
20,000 selected occurrences into 200 classes. Each
of the 200 tags is defined by the centroid of the cor-
responding class (the sum of its members). Dis-
tributional tagging of an occurrence of a word
w proceeds then by retrieving the four relevant
context vectors (right context vector of previous
word, left context vector of following word, both
context vectors of w) concatenating them to one
1000-component vector, mapping this vector to 50
dimensions, computing the correlations with the
200 cluster centroids and, finally, assigning the oc-
currence to the closest cluster. This procedure was
applied to all tokens of the Brown corpus.
We will see below that this method of distribu-
tional tagging, although partially successful, fails
for many tokens whose neighbors are punctuation
marks. The context vectors of punctuation marks
contribute little information about syntactic cate-
gorization since there are no grammatical depen-
dencies between words and punctuation marks, in
contrast to strong dependencies between neigh-
boring words.
For this reason, a second induction on the ba-
sis of word type and context was performed, but
only for those tokens with informative contexts.
Tokens next to punctuation marks and tokens
with rare words as neighbors were not included.
Contexts with rare words (less than ten occur-
rences) were also excluded for similar reasons: If
a word only occurs nine or fewer times its left
and right context vectors capture little informa-
tion for syntactic categorization. In the experi-
ment, 20,000 natural contexts were randomly se-
lected, processed by the SVD and clustered into
</bodyText>
<page confidence="0.990936">
143
</page>
<table confidence="0.998563777777778">
tag description Penn Treebank tags tag description Penn Treebank tags
ADN adnominal modifier ADN&apos; $ POS possessive marker POS
CC conjunction CC PRP pronoun PRP
CD cardinal CD RB adverbial RB RP RBR RBS
DT determiner DT PDT PRP$ TO infinitive marker TO
IN preposition IN VB infinitive VB
ING &amp;quot;-ing&amp;quot; forms VBG VBD inflected verb form VBD VBZ VBP
MD modal MD VBN predicative VBN PRD&amp;quot;
nominal NNP(S) NN(S) WDT wh-word WPM WRB WDT
</table>
<tableCaption confidence="0.999744">
Table 2: Evaluation tag set. Structural tags derived from parse trees are marked with *.
</tableCaption>
<bodyText confidence="0.6984135">
200 classes. The classification was then applied to
all natural contexts of the Brown corpus.
</bodyText>
<subsectionHeader confidence="0.975986">
3.3 Generalized context vectors
</subsectionHeader>
<bodyText confidence="0.999977557377049">
The context vectors used so far only capture infor-
mation about distributional interactions with the
250 most frequent words. Intuitively, it should be
possible to gain accuracy in tag induction by us-
ing information from more words. One way to do
this is to let the right context vector record which
classes of left context vectors occur to the right of
a word. The rationale is that words with similar
left context characterize words to their right in a
similar way. For example, &amp;quot;seemed&amp;quot; and &amp;quot;would&amp;quot;
have similar left contexts, and they characterize
the right contexts of &amp;quot;he&amp;quot; and &amp;quot;the firefighter&amp;quot;
as potentially containing an inflected verb form.
Rather than having separate entries in its right
context vector for &amp;quot;seemed&amp;quot;, &amp;quot;would&amp;quot;, and &amp;quot;likes&amp;quot;,
a word like &amp;quot;he&amp;quot; can now be characterized by a
generalized entry for &amp;quot;inflected verb form occurs
frequently to my right&amp;quot;.
This proposal was implemented by applying a
singular value decomposition to the 47025-by-250
matrix of left context vectors and clustering the
resulting context vectors into 250 classes. A gen-
eralized right context vector v for word w was
then formed by counting how often words from
these 250 classes occurred to the right of w. En-
try v, counts the number of times that a word
from class i occurs to the right of w in the cor-
pus (as opposed to the number of times that the
word with frequency rank i occurs to the right of
w). Generalized left context vectors were derived
by an analogous procedure using word-based right
context vectors. Note that the information about
left and right is kept separate in this computation.
This differs from previous approaches (Finch and
Chater, 1992; Schiitze, 1993) in which left and
right context vectors of a word are always used
in one concatenated vector. There are arguably
fewer different types of right syntactic contexts
than types of syntactic categories. For example,
transitive verbs and prepositions belong to differ-
ent syntactic categories, but their right contexts
are virtually identical in that they require a noun
phrase. This generalization could not be exploited
if left and right context were not treated sepa-
rately.
Another argument for the two-step derivation
is that many words don&apos;t have any of the 250
most frequent words as their left or right neighbor.
Hence, their vector would be zero in the word-
based scheme. The class-based scheme makes it
more likely that meaningful representations are
formed for all words in the vocabulary.
The generalized context vectors were input to
the tag induction procedure described above for
word-based context vectors: 20,000 word triplets
were selected from the corpus, encoded as 1,000-
dimensional vectors (consisting of four generalized
context vectors), decomposed by a singular value
decomposition and clustered into 200 classes. The
resulting classification was applied to all tokens in
the Brown corpus.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999728125">
The results of the four experiments were evalu-
ated by forming 16 classes of tags from the Penn
Treebank as shown in Table 2. Preliminary ex-
periments showed that distributional methods dis-
tinguish adnominal and predicative uses of adjec-
tives (e.g. &amp;quot;the black cat&amp;quot; vs. &amp;quot;the cat is black&amp;quot;).
Therefore the tag &amp;quot;ADN&amp;quot; was introduced for uses
of adjectives, nouns, and participles as adnominal
modifiers. The tag &amp;quot;PRD&amp;quot; stands for predicative
uses of adjectives. The Penn Treebank parses of
the Brown corpus were used to determine whether
a token functions as an adnominal modifier. Punc-
tuation marks, special symbols, interjections, for-
eign words and tags with fewer than 100 instances
were excluded from the evaluation.
Tables 3 and 4 present results for word type-
based induction and induction based on word type
and context. For each tag t, the table lists the
frequency of t in the corpus (&amp;quot;frequency&amp;quot; )2, the
number of induced tags io, , i1, that were as-
signed to it (&amp;quot;# classes&amp;quot;); the number of times an
occurrence of t was correctly labeled as belong-
ing to one of io, , it (&amp;quot;correct&amp;quot;); the num-
ber of times that a token of a different tag t&apos; was
</bodyText>
<footnote confidence="0.9826395">
2The small difference in overall frequency in the
tables is due to the fact that some word-based context
vectors consist entirely of zeros. There were about a
hundred word triplets whose four context vectors did
not have non-zero entries and could not be assigned a
cluster.
</footnote>
<page confidence="0.992958">
144
</page>
<table confidence="0.999955388888889">
tag frequency # classes correct incorrect precision recall F
ADN 108586 34 38282 19528 0.66 0.35 0.46
CC 36808 0 0 0 0.00 0.00 0.00
CD 15085 4 3376 1431 0.70 0.22 0.34
DT 129626 2 125540 31783 0.80 0.97 0.87
IN 132079 3 118726 75829 0.61 0.90 0.73
ING 14753 5 2111 1016 0.68 0.14 0.24
MD 13498 2 13383 13016 0.51 0.99 0.67
N 231434 98 193838 79652 0.71 0.84 0.77
POS 5086 1 4641 1213 0.79 0.91 0.85
PRP 47686 3 43839 21723 0.67 0.92 0.77
RB 54525 7 35364 56505 0.38 0.65 0.48
TO 25196 0 0 0 0.00 0.00 0.00
VB 35342 8 29138 17945 0.62 0.82 0.71
VBD 80058 12 36653 3855 0.90 0.46 0.61
VBN 41146 21 7773 8841 0.47 0.19 0.27
WDT 14093 0 0 0 0.00 0.00 0.00
avg. 0.53 0.52 0.49
</table>
<tableCaption confidence="0.999856">
Table 3: Precision and recall for induction based on word type.
</tableCaption>
<bodyText confidence="0.9602555875">
miscategorized as being an instance of io, ii, . . . , il
(&amp;quot;incorrect&amp;quot;); and precision and recall of the cate-
gorization of t. Precision is the number of correct
tokens divided by the sum of correct and incorrect
tokens. Recall is the number of correct tokens di-
vided by the total number of tokens of t (in the
first column). The last column gives van Rijs-
bergen&apos;s F measure which computes an aggregate
score from precision and recall: (van Rijsbergen,
1979) F = 1 . We chose a = 0.5 to give
equal weight to precision and recall.
It is clear from the tables that incorporating
context improves performance considerably. The
F score increases for all tags except CD, with an
average improvement of more than 0.20. The tag
CD is probably better thought of as describing a
word class. There is a wide range of heterogeneous
syntactic functions of cardinals in particular con-
texts: quantificational and adnominal uses, bare
NP&apos;s (&amp;quot;is one of&amp;quot;), dates and ages (&amp;quot;Jan 1&amp;quot;, &amp;quot;gave
his age as 25&amp;quot;), and enumerations. In this light, it
is not surprising that the word-type method does
better on cardinals.
Table 5 shows that performance for generalized
context vectors is better than for word-based con-
text vectors (0.74 vs. 0.72). However, since the
number of tags with better and worse performance
is about the same (7 and 5), one cannot con-
clude with certainty that generalized context vec-
tors induce tags of higher quality. Apparently, the
250 most frequent words capture most of the rel-
evant distributional information so that the addi-
tional information from less frequent words avail-
able from generalized vectors only has a small ef-
fect.
Table 6 looks at results for &amp;quot;natural&amp;quot; contexts,
i.e. those not containing punctuation marks and
rare words. Performance is consistently better
than for the evaluation on all contexts, indicating
that the low quality of the distributional informa-
tion about punctuation marks and rare words is a
difficulty for successful tag induction.
Even for &amp;quot;natural&amp;quot; contexts, performance varies
considerably. It is fairly good for prepositions, de-
terminers, pronouns, conjunctions, the infinitive
marker, modals, and the possessive marker. Tag
induction fails for cardinals (for the reasons men-
tioned above) and for &amp;quot;-ing&amp;quot; forms. Present par-
ticiples and gerunds are difficult because they ex-
hibit both verbal and nominal properties and oc-
cur in a wide variety of different contexts whereas
other parts of speech have a few typical and fre-
quent contexts.
It may seem worrying that some of the tags are
assigned a high number of clusters (e.g., 49 for
N, 36 for ADN). A closer look reveals that many
clusters embody finer distinctions. Some exam-
ples: Nouns in cluster 0 are heads of larger noun
phrases, whereas the nouns in cluster 1 are full-
fledged NPs. The members of classes 29 and 111
function as subjects. Class 49 consists of proper
nouns. However, there are many pairs or triples
of clusters that should be collapsed into one on
linguistic grounds. They were separated on distri-
butional criteria that don&apos;t have linguistic corre-
lates.
An analysis of the divergence between our clas-
sification and the manually assigned tags revealed
three main sources of errors: rare words and rare
syntactic phenomena, indistinguishable distribu-
tion, and non-local dependencies.
Rare words are difficult because of lack of dis-
tributional evidence. For example, &amp;quot;ties&amp;quot; is used
as a verb only 2 times (out of 15 occurrences in
the corpus). Both occurrences are miscategorized,
since its context vectors do not provide enough
evidence for the verbal use. Rare syntactic con-
structions pose a related problem: There are not
enough instances to justify the creation of a sepa-
rate cluster. For example, verbs taking bare in-
</bodyText>
<page confidence="0.995788">
145
</page>
<table confidence="0.999957444444445">
tag frequency # classes correct incorrect precision recall F
ADN 108532 42 87128 24743 0.78 0.80 0.79
CC 36808 2 28671 1501 0.95 0.78 0.86
CD 15084 1 747 809 0.48 0.05 0.09
DT 129626 6 119534 6178 0.95 0.92 0.94
IN 132079 11 125554 25316 0.83 0.95 0.89
ING 14753 4 3096 4876 0.39 0.21 0.27
MD 13498 2 12983 936 0.93 0.96 0.95
N 231424 68 207822 51695 0.80 0.90 0.85
POS 5086 2 4623 533 0.90 0.91 0.90
PRP 47686 7 44946 12759 0.78 0.94 0.85
RB 54524 16 31184 17403 0.64 0.57 0.60
TO 25196 1 23291 61 1.00 0.92 0.96
VB 35342 8 29392 6152 0.83 0.83 0.83
VBD 80058 17 64150 8663 0.88 0.80 0.84
VBN 41145 11 25578 11972 0.68 0.62 0.65
WDT 14093 2 1621 1017 0.61 0.12 0.19
avg. 0.78 0.71 0.72
</table>
<tableCaption confidence="0.996807">
Table 4: Precision and recall for induction based on word type and context.
</tableCaption>
<table confidence="0.991384777777778">
tag frequency I # classes correct incorrect precision recall I F
ADN 108586 50 91893 26790 0.77 0.85 0.81
CC 36808 4 34127 6430 0.84 0.93 0.88
CD 15085 3 3707 1530 0.71 0.25 0.36
DT 129626 10 120968 5780 0.95 0.93 0.94
IN 132079 8 123516 22070 0.85 0.94 0.89
ING 14753 2 3798 7161 0.35 0.26 0.30
MD 13498 3 13175 1059 0.93 0.98 0.95
N 231434 70 201890 33206 0.86 0.87 0.87
POS 5086 2 4932 1636 0.75 0.97 0.85
PRP 47686 5 37535 9221 0.80 0.79 0.79
RB 54524 9 29892 18398 0.62 0.55 0.58
TO 25196 1 25181 27 1.00 1.00 1.00
VB 35342 7 28879 6560 0.81 0.82 0.82
VBD 80058 15 66457 12079 0.85 0.83 0.84
VBN 41145 10 26960 17356 0.61 0.66 0.63
WDT 14093 1 2223 563 0.80 0.16 0.26
avg. 0.78 0.73 0.74
</table>
<tableCaption confidence="0.994977">
Table 5: Precision and recall for induction based on generalized context vectors.
</tableCaption>
<table confidence="0.999935166666667">
tag frequency # classes correct incorrect precision recall F
ADN 63771 36 54398 12203 0.82 0.85 0.83
CC 16148 4 15657 1798 0.90 0.97 0.93
CD 7011 1 1857 918 0.67 0.26 0.38
DT 87914 9 82206 2664 0.97 0.94 0.95
IN 91950 9 86793 6842 0.93 0.94 0.94
ING 7268 2 1243 1412 0.47 0.17 0.25
MD 11244 3 10363 476 0.96 0.92 0.94
N 111368 49 100105 14452 0.87 0.90 0.89
POS 3202 1 2912 255 0.92 0.91 0.91
PRP 23946 7 22877 4062 0.85 0.96 0.90
RB 32331 16 21037 9922 0.68 0.65 0.66
TO 19859 2 19537 53 1.00 0.98 0.99
VB 26714 11 24036 4119 0.85 0.90 0.88
VBD 56540 33 51016 8488 0.86 0.90 0.88
VBN 24804 14 18889 7448 0.72 0.76 0.74
WDT 8329 3 3691 670 0.85 0.44 0.58
avg. 0.83 0.78 0.79
</table>
<tableCaption confidence="0.999008">
Table 6: Precision and recall for induction for natural contexts.
</tableCaption>
<page confidence="0.998125">
146
</page>
<bodyText confidence="0.999975689655172">
finitives were classified as adverbs since this is
too rare a phenomenon to provide strong distri-
butional evidence (&amp;quot;we do not DARE speak of&amp;quot;,
&amp;quot;legislation could HELP remove&amp;quot;).
The case of the tags &amp;quot;VBN&amp;quot; and &amp;quot;PRD&amp;quot; (past
participles and predicative adjectives) demon-
strates the difficulties of word classes with indis-
tinguishable distributions. There are hardly any
distributional clues for distinguishing &amp;quot;VBN&amp;quot; and
&amp;quot;PRD&amp;quot; since both are mainly used as comple-
ments of &amp;quot;to be&amp;quot; .3 A common tag class was cre-
ated for &amp;quot;VBN&amp;quot; and &amp;quot;PRD&amp;quot; to show that they
are reasonably well distinguished from other parts
of speech, even if not from each other. Semantic
understanding is necessary to distinguish between
the states described by phrases of the form &amp;quot;to be
adjective&amp;quot; and the processes described by phrases
of the form &amp;quot;to be past participle&amp;quot;.
Finally, the method fails if there are no local
dependencies that could be used for categoriza-
tion and only non-local dependencies are informa-
tive. For example, the adverb in &amp;quot;Mc*N. Hester,
CURRENTLY Dean of...&amp;quot; and the conjunction
in &amp;quot;to add that, IF United States policies ... &amp;quot;
have similar immediate neighbors (comma, NP).
The decision to consider only immediate neighbors
is responsible for this type of error since taking
a wider context into account would disambiguate
the parts of speech in question.
</bodyText>
<sectionHeader confidence="0.999808" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.989580666666667">
There are three avenues of future research we are
interested in pursuing. First, we are planning to
apply the algorithm to an as yet untagged lan-
guage. Languages with a rich morphology may
be more difficult than English since with fewer to-
kens per type, there is less data on which to base
a categorization decision.
Secondly, the error analysis suggests that con-
sidering non-local dependencies would improve re-
sults. Categories that can be induced well (those
characterized by local dependencies) could be in-
put into procedures that learn phrase structure
(e.g. (Brill and Marcus, 1992b; Finch, 1993)).
These phrase constraints could then be incorpo-
rated into the distributional tagger to characterize
non-local dependencies.
Finally, our procedure induces a &amp;quot;hard&amp;quot; part-of-
speech classification of occurrences in context, i.e.,
each occurrence is assigned to only one category.
It is by no means generally accepted that such
a classification is linguistically adequate. There
is both synchronic (Ross, 1972) and diachronic
(Tabor, 1994) evidence suggesting that words and
their uses can inherit properties from several pro-
totypical syntactic categories. For example, &amp;quot;fun&amp;quot;
3Because of phrases like &amp;quot;I had sweet potatoes&amp;quot;,
forms of &amp;quot;have&amp;quot; cannot serve as a reliable discrimina-
tor either.
in &amp;quot;It&apos;s a fun thing to do.&amp;quot; has properties of both a
noun and an adjective (superlative &amp;quot;funnest&amp;quot; pos-
sible). We are planning to explore &amp;quot;soft&amp;quot; classifi-
cation algorithms that can account for these phe-
nomena.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999985538461539">
In this paper, we have attempted to construct an
algorithm for fully automatic distributional tag-
ging, using unannotated corpora as the sole source
of information. The main innovation is that the
algorithm is able to deal with part-of-speech am-
biguity, a pervasive phenomenon in natural lan-
guage that was unaccounted for in previous work
on learning categories from corpora. The method
was systematically evaluated on the Brown cor-
pus. Even if no automatic procedure can rival the
accuracy of human tagging, we hope that the al-
gorithm will facilitate the initial tagging of texts
in new languages and sublanguages.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998196">
I am grateful for helpful comments to Steve Finch,
Jan Pedersen and two anonymous reviewers (from
ACL and EACL). I&apos;m also indebted to Michael
Berry for SVDPACK and to the Penn Treebank
Project for the parsed Brown corpus.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998592703703704">
Steven Abney. 1991. Parsing by chunks. In
Berwick, Abney, and Tenny, editors, Principle-
Based Parsing. Kluwer Academic Publishers.
Michael W. Berry. 1992. Large-scale sparse
singular value computations. The Interna-
tional Journal of Supercomputer Applications,
6(1):13-49.
Douglas Biber. 1993. Co-occurrence patterns
among collocations: A tool for corpus-based
lexical knowledge acquisition. Computational
Linguistics, 19(3):531-538.
Eric Brill and Mitch Marcus. 1992a. Tagging
an unfamiliar text with minimal human super-
vision. In Robert Goldman, editor, Working
Notes of the AAAI Fall Symposium on Proba-
bilistic Approaches to Natural Language. AAAI
Press.
Eric Brill and Mitchell Marcus. 1992b. Au-
tomatically acquiring phrase structure using
distributional analysis. In Proceedings of the
DARPA workshop &amp;quot;Speech and Natural Lan-
guage&amp;quot;, pages 155-159.
Eric Brill, David Magerman, Mitch Marcus, and
Beatrice Santorini. 1990. Deducing linguistic
structure from the statistics of large corpora. In
Proceedings of the DARPA Speech and Natural
Language Workshop, pages 275-282.
</reference>
<page confidence="0.981686">
147
</page>
<reference confidence="0.99870112">
Eric Brill. 1993. Automatic grammar induction
and parsing free text: A transformation-based
approach. In Proceedings of ACL 31, Columbus
OH.
Eugene Charniak, Curtis Hendrickson, Neil Ja-
cobson, and Mike Perkowitz. 1993. Equations
for part-of-speech tagging. In Proceedings of the
Eleventh National Conference on Artificial In-
telligence, pages 784-789.
Eugene Charniak, Glenn Carroll, John Adcock,
Anthony Cassandra, Yoshihiko Gotoh, Jeremy
Katz, Michael Littman, and John McCann.
1994. Taggers for parsers. Technical Report
CS-94-06, Brown University.
Kenneth W. Church. 1989. A stochastic parts
program and noun phrase parser for unre-
stricted text. In Proceedings of ICASSP-89,
Glasgow, Scotland.
Doug Cutting, Julian Kupiec, Jan Pedersen, and
Penelope Sibun. 1991. A practical part-
of-speech tagger. In The 3rd Conference on
Applied Natural Language Processing, Trento,
Italy.
Douglas R. Cutting, Jan 0. &apos;Pedersen, David
Karger, and John W. Tukey. 1992. Scat-
ter/gather: A cluster-based approach to brows-
ing large document collections. In Proceedings
of SIGIR &apos;92, pages 318-329.
C. G. deMarcken. 1990. Parsing the LOB corpus.
In Proceedings of the 28th Annual Meeting of
the Association for Computational Linguistics,
pages 243-259.
Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive Science, 14:179-211.
Steven Finch and Nick Chater. 1992. Bootstrap-
ping syntactic categories using statistical meth-
ods. In Walter Daelemans and David Powers,
editors, Background and Experiments in Ma-
chine Learning of Natural Language, pages 229-
235, Tilburg University. Institute for Language
Technology and Al.
Steven Paul Finch. 1993. Finding Structure in
Language. Ph.D. thesis, University of Edin-
burgh.
W.N. Francis and F. KuCera. 1982. Frequency
Analysis of English Usage. Houghton Mifflin,
Boston.
F. Jelinek. 1985. Robust part-of-speech tagging
using a hidden markov model. Technical report,
IBM, T.J. Watson Research Center.
Reinhard Kneser and Hermann Ney. 1993. Form-
ing word classes by statistical clustering for sta-
tistical language modelling. In Reinhard KOhler
and Burghard B. Rieger, editors, Contribu-
tions to Quantitative Linguistics, pages 221-
226. Kluwer Academic Publishers, Dordrecht,
The Netherlands.
Julian Kupiec. 1992. Robust part-of-speech tag-
ging using a hidden markov model. Computer
Speech and Language, 6:225-242.
Julian Kupiec. 1993. Murax: A robust linguistic
approach for question answering using an on-
line encyclopedia. In Proceedings of SIGIR &apos;93,
pages 181-190.
John R. Ross. 1972. The category squish: End-
station Hauptwort. In Papers from the Eighth
Regional Meeting. Chicago Linguistic Society.
Hinrich Schiitze. 1993. Part-of-speech induction
from scratch. In Proceedings of ACL 31, pages
251-258, Columbus OH.
Whitney Tabor. 1994. Syntactic Innovation: A
Connectionist Model. Ph.D. thesis, Stanford
University.
C. J. van Rijsbergen. 1979. Information Re-
trieval. Butterworths, London. Second Edition.
</reference>
<page confidence="0.996847">
148
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550779">
<title confidence="0.8239085">Distributional Part-of-Speech Tagging Hinrich Schiitze</title>
<address confidence="0.927132">CSLI, Ventura Hall Stanford, CA 94305-4115 , USA</address>
<email confidence="0.999791">schuetze@csli.stanford.edu</email>
<abstract confidence="0.986844571428572">This paper presents an algorithm for tagging words whose part-of-speech properties are unknown. Unlike previous work, algorithm categorizes tokens in of types. algorithm is evaluated on the Brown Corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<editor>In Berwick, Abney, and Tenny, editors, PrincipleBased Parsing.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1179" citStr="Abney, 1991" startWordPosition="169" endWordPosition="170">s a growing need for robust processing techniques that can analyze text without expensive and time-consuming adaptation to new domains and genres. This need motivates research on fully automatic text processing that may rely on general principles of linguistics and computation, but does not depend on knowledge about individual words. In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al., 1994). The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech tagg</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing by chunks. In Berwick, Abney, and Tenny, editors, PrincipleBased Parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
</authors>
<title>Large-scale sparse singular value computations.</title>
<date>1992</date>
<journal>The International Journal of Supercomputer Applications,</journal>
<pages>6--1</pages>
<contexts>
<context position="8315" citStr="Berry, 1992" startWordPosition="1343" endWordPosition="1344">pace) for the SVD&apos;s described in this paper. SVD addresses the problems of generalization and sparseness because broad and stable generalizations are represented on dimensions with large values which will be retained in the dimensionality reduction. In contrast, dimensions corresponding to small singular values represent idiosyncrasies, like the phonological constraint on the usage of &amp;quot;an&amp;quot; vs. &amp;quot;a&amp;quot;, and will be dropped. We also gain efficiency since we can manipulate smaller vectors, reduced to 50 dimensions. We used SVDPACK to compute the singular value decompositions described in this paper (Berry, 1992). Table 1 shows the nearest neighbors of two words (ordered according to closeness to the head word) after the dimensionality reduction. Neighbors with highest similarity according to both left and right context are listed. One can see clear differences between the nearest neighbors in the two spaces. The right-context neighbors of &amp;quot;onto&amp;quot; contain verbs because both prepositions and verbs govern noun phrases to their right. The left-context neighborhood of &amp;quot;onto&amp;quot; reflects the fact that prepositional phrases are used in the same position as adverbs like &amp;quot;away&amp;quot; and &amp;quot;together&amp;quot;, thus making their l</context>
</contexts>
<marker>Berry, 1992</marker>
<rawString>Michael W. Berry. 1992. Large-scale sparse singular value computations. The International Journal of Supercomputer Applications, 6(1):13-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Co-occurrence patterns among collocations: A tool for corpus-based lexical knowledge acquisition.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--3</pages>
<contexts>
<context position="3263" citStr="Biber (1993)" startWordPosition="491" endWordPosition="492">2a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (&amp;quot;certain&amp;quot; and &amp;quot;right&amp;quot;) with their immediate neighbors. What these approaches have in common is that they classify words instead of individual occurrences. Given the widespread part-of-speech ambiguity of words this is problematic.&apos; How should a word like &amp;quot;plant&amp;quot; be categorized if it has uses both as a verb and as a noun? How can a categorization be considered meaningful if the infinitive marker &amp;quot;to&amp;quot; is not distinguished from the homophonous preposition? In a previous paper (Schiltze, 1993), we trained a neural network to disambiguat</context>
</contexts>
<marker>Biber, 1993</marker>
<rawString>Douglas Biber. 1993. Co-occurrence patterns among collocations: A tool for corpus-based lexical knowledge acquisition. Computational Linguistics, 19(3):531-538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Mitch Marcus</author>
</authors>
<title>Tagging an unfamiliar text with minimal human supervision.</title>
<date>1992</date>
<booktitle>Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language.</booktitle>
<editor>In Robert Goldman, editor,</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="2228" citStr="Brill and Marcus (1992" startWordPosition="331" endWordPosition="334">ections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal rep</context>
<context position="28054" citStr="Brill and Marcus, 1992" startWordPosition="4691" endWordPosition="4694">f speech in question. 5 Future Work There are three avenues of future research we are interested in pursuing. First, we are planning to apply the algorithm to an as yet untagged language. Languages with a rich morphology may be more difficult than English since with fewer tokens per type, there is less data on which to base a categorization decision. Secondly, the error analysis suggests that considering non-local dependencies would improve results. Categories that can be induced well (those characterized by local dependencies) could be input into procedures that learn phrase structure (e.g. (Brill and Marcus, 1992b; Finch, 1993)). These phrase constraints could then be incorporated into the distributional tagger to characterize non-local dependencies. Finally, our procedure induces a &amp;quot;hard&amp;quot; part-ofspeech classification of occurrences in context, i.e., each occurrence is assigned to only one category. It is by no means generally accepted that such a classification is linguistically adequate. There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. For example, &amp;quot;fun&amp;quot; 3Because of </context>
</contexts>
<marker>Brill, Marcus, 1992</marker>
<rawString>Eric Brill and Mitch Marcus. 1992a. Tagging an unfamiliar text with minimal human supervision. In Robert Goldman, editor, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Mitchell Marcus</author>
</authors>
<title>Automatically acquiring phrase structure using distributional analysis.</title>
<date>1992</date>
<booktitle>In Proceedings of the DARPA workshop &amp;quot;Speech and Natural Language&amp;quot;,</booktitle>
<pages>155--159</pages>
<contexts>
<context position="2228" citStr="Brill and Marcus (1992" startWordPosition="331" endWordPosition="334">ections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal rep</context>
<context position="28054" citStr="Brill and Marcus, 1992" startWordPosition="4691" endWordPosition="4694">f speech in question. 5 Future Work There are three avenues of future research we are interested in pursuing. First, we are planning to apply the algorithm to an as yet untagged language. Languages with a rich morphology may be more difficult than English since with fewer tokens per type, there is less data on which to base a categorization decision. Secondly, the error analysis suggests that considering non-local dependencies would improve results. Categories that can be induced well (those characterized by local dependencies) could be input into procedures that learn phrase structure (e.g. (Brill and Marcus, 1992b; Finch, 1993)). These phrase constraints could then be incorporated into the distributional tagger to characterize non-local dependencies. Finally, our procedure induces a &amp;quot;hard&amp;quot; part-ofspeech classification of occurrences in context, i.e., each occurrence is assigned to only one category. It is by no means generally accepted that such a classification is linguistically adequate. There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. For example, &amp;quot;fun&amp;quot; 3Because of </context>
</contexts>
<marker>Brill, Marcus, 1992</marker>
<rawString>Eric Brill and Mitchell Marcus. 1992b. Automatically acquiring phrase structure using distributional analysis. In Proceedings of the DARPA workshop &amp;quot;Speech and Natural Language&amp;quot;, pages 155-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>David Magerman</author>
<author>Mitch Marcus</author>
<author>Beatrice Santorini</author>
</authors>
<title>Deducing linguistic structure from the statistics of large corpora.</title>
<date>1990</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>275--282</pages>
<contexts>
<context position="2895" citStr="Brill et al. (1990)" startWordPosition="427" endWordPosition="430">ruct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (&amp;quot;certain&amp;quot; and &amp;quot;right&amp;quot;) with their immediate neighbors. What these approaches have in common is that they classify words instead of individual occurrences. Given the wides</context>
</contexts>
<marker>Brill, Magerman, Marcus, Santorini, 1990</marker>
<rawString>Eric Brill, David Magerman, Mitch Marcus, and Beatrice Santorini. 1990. Deducing linguistic structure from the statistics of large corpora. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 275-282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: A transformation-based approach.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL 31,</booktitle>
<location>Columbus OH.</location>
<contexts>
<context position="1963" citStr="Brill (1993)" startWordPosition="287" endWordPosition="288">ext, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical c</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Eric Brill. 1993. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of ACL 31, Columbus OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Curtis Hendrickson</author>
<author>Neil Jacobson</author>
<author>Mike Perkowitz</author>
</authors>
<title>Equations for part-of-speech tagging.</title>
<date>1993</date>
<booktitle>In Proceedings of the Eleventh National Conference on Artificial Intelligence,</booktitle>
<pages>784--789</pages>
<contexts>
<context position="1849" citStr="Charniak et al., 1993" startWordPosition="270" endWordPosition="273"> et al., 1994). The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The pres</context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowitz, 1993</marker>
<rawString>Eugene Charniak, Curtis Hendrickson, Neil Jacobson, and Mike Perkowitz. 1993. Equations for part-of-speech tagging. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 784-789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Glenn Carroll</author>
<author>John Adcock</author>
<author>Anthony Cassandra</author>
<author>Yoshihiko Gotoh</author>
<author>Jeremy Katz</author>
<author>Michael Littman</author>
<author>John McCann</author>
</authors>
<title>Taggers for parsers.</title>
<date>1994</date>
<tech>Technical Report CS-94-06,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="1241" citStr="Charniak et al., 1994" startWordPosition="176" endWordPosition="179">at can analyze text without expensive and time-consuming adaptation to new domains and genres. This need motivates research on fully automatic text processing that may rely on general principles of linguistics and computation, but does not depend on knowledge about individual words. In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al., 1994). The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al</context>
</contexts>
<marker>Charniak, Carroll, Adcock, Cassandra, Gotoh, Katz, Littman, McCann, 1994</marker>
<rawString>Eugene Charniak, Glenn Carroll, John Adcock, Anthony Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael Littman, and John McCann. 1994. Taggers for parsers. Technical Report CS-94-06, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1989</date>
<booktitle>In Proceedings of ICASSP-89,</booktitle>
<location>Glasgow, Scotland.</location>
<contexts>
<context position="1825" citStr="Church, 1989" startWordPosition="268" endWordPosition="269">1990; Charniak et al., 1994). The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited fro</context>
</contexts>
<marker>Church, 1989</marker>
<rawString>Kenneth W. Church. 1989. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of ICASSP-89, Glasgow, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A practical partof-speech tagger.</title>
<date>1991</date>
<booktitle>In The 3rd Conference on Applied Natural Language Processing,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="2102" citStr="Cutting et al., 1991" startWordPosition="309" endWordPosition="312">tical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning gr</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1991</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1991. A practical partof-speech tagger. In The 3rd Conference on Applied Natural Language Processing, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Karger &apos;Pedersen</author>
<author>John W Tukey</author>
</authors>
<title>Scatter/gather: A cluster-based approach to browsing large document collections.</title>
<date>1992</date>
<booktitle>In Proceedings of SIGIR &apos;92,</booktitle>
<pages>318--329</pages>
<marker>&apos;Pedersen, Tukey, 1992</marker>
<rawString>Douglas R. Cutting, Jan 0. &apos;Pedersen, David Karger, and John W. Tukey. 1992. Scatter/gather: A cluster-based approach to browsing large document collections. In Proceedings of SIGIR &apos;92, pages 318-329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G deMarcken</author>
</authors>
<title>Parsing the LOB corpus.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>243--259</pages>
<contexts>
<context position="1217" citStr="deMarcken, 1990" startWordPosition="174" endWordPosition="175">ing techniques that can analyze text without expensive and time-consuming adaptation to new domains and genres. This need motivates research on fully automatic text processing that may rely on general principles of linguistics and computation, but does not depend on knowledge about individual words. In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al., 1994). The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Chur</context>
</contexts>
<marker>deMarcken, 1990</marker>
<rawString>C. G. deMarcken. 1990. Parsing the LOB corpus. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 243-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<pages>14--179</pages>
<contexts>
<context position="2745" citStr="Elman (1990)" startWordPosition="407" endWordPosition="408">on is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (&amp;quot;certain&amp;quot; and &amp;quot;right</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L. Elman. 1990. Finding structure in time. Cognitive Science, 14:179-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Finch</author>
<author>Nick Chater</author>
</authors>
<title>Bootstrapping syntactic categories using statistical methods.</title>
<date>1992</date>
<booktitle>In Walter Daelemans and</booktitle>
<pages>229--235</pages>
<editor>David Powers, editors,</editor>
<institution>Tilburg University. Institute for Language Technology and Al.</institution>
<contexts>
<context position="2977" citStr="Finch and Chater (1992)" startWordPosition="440" endWordPosition="443">ing procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (&amp;quot;certain&amp;quot; and &amp;quot;right&amp;quot;) with their immediate neighbors. What these approaches have in common is that they classify words instead of individual occurrences. Given the widespread part-of-speech ambiguity of words this is problematic.&apos; How should a word li</context>
<context position="10308" citStr="Finch and Chater, 1992" startWordPosition="1655" endWordPosition="1658">ur different tag induction experiments, which are described in detail below: • induction based on word type only • induction based on word type and context • induction based on word type and context, restricted to &amp;quot;natural&amp;quot; contexts • induction based on word type and context, using generalized left and right context vectors 3.1 Induction based on word type only The two context vectors of a word characterize the distribution of neighboring words to its left and right. The concatenation of left and right context vector can therefore serve as a representation of a word&apos;s distributional behavior (Finch and Chater, 1992; Schiitze, 1993). We formed such concatenated vectors for all 47,025 words (surface forms) in the Brown corpus. Here, we use the raw 250- dimensional context vectors and apply the SVD to the 47,025-by-500 matrix (47,025 words with two 250-dimensional context vectors each). We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot (Cutting et al., 1992) (group average agglomeration applied to a sample). This classification constitutes the baseline performance for distributional partof-speech tagging. All occu</context>
<context position="16703" citStr="Finch and Chater, 1992" startWordPosition="2701" endWordPosition="2704">tors into 250 classes. A generalized right context vector v for word w was then formed by counting how often words from these 250 classes occurred to the right of w. Entry v, counts the number of times that a word from class i occurs to the right of w in the corpus (as opposed to the number of times that the word with frequency rank i occurs to the right of w). Generalized left context vectors were derived by an analogous procedure using word-based right context vectors. Note that the information about left and right is kept separate in this computation. This differs from previous approaches (Finch and Chater, 1992; Schiitze, 1993) in which left and right context vectors of a word are always used in one concatenated vector. There are arguably fewer different types of right syntactic contexts than types of syntactic categories. For example, transitive verbs and prepositions belong to different syntactic categories, but their right contexts are virtually identical in that they require a noun phrase. This generalization could not be exploited if left and right context were not treated separately. Another argument for the two-step derivation is that many words don&apos;t have any of the 250 most frequent words a</context>
</contexts>
<marker>Finch, Chater, 1992</marker>
<rawString>Steven Finch and Nick Chater. 1992. Bootstrapping syntactic categories using statistical methods. In Walter Daelemans and David Powers, editors, Background and Experiments in Machine Learning of Natural Language, pages 229-235, Tilburg University. Institute for Language Technology and Al.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Paul Finch</author>
</authors>
<title>Finding Structure in Language.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="2994" citStr="Finch (1993)" startWordPosition="445" endWordPosition="446"> part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (&amp;quot;certain&amp;quot; and &amp;quot;right&amp;quot;) with their immediate neighbors. What these approaches have in common is that they classify words instead of individual occurrences. Given the widespread part-of-speech ambiguity of words this is problematic.&apos; How should a word like &amp;quot;plant&amp;quot; be cat</context>
<context position="28069" citStr="Finch, 1993" startWordPosition="4695" endWordPosition="4696">uture Work There are three avenues of future research we are interested in pursuing. First, we are planning to apply the algorithm to an as yet untagged language. Languages with a rich morphology may be more difficult than English since with fewer tokens per type, there is less data on which to base a categorization decision. Secondly, the error analysis suggests that considering non-local dependencies would improve results. Categories that can be induced well (those characterized by local dependencies) could be input into procedures that learn phrase structure (e.g. (Brill and Marcus, 1992b; Finch, 1993)). These phrase constraints could then be incorporated into the distributional tagger to characterize non-local dependencies. Finally, our procedure induces a &amp;quot;hard&amp;quot; part-ofspeech classification of occurrences in context, i.e., each occurrence is assigned to only one category. It is by no means generally accepted that such a classification is linguistically adequate. There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. For example, &amp;quot;fun&amp;quot; 3Because of phrases like &amp;quot;I</context>
</contexts>
<marker>Finch, 1993</marker>
<rawString>Steven Paul Finch. 1993. Finding Structure in Language. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>F KuCera</author>
</authors>
<title>Frequency Analysis of English Usage.</title>
<date>1982</date>
<location>Houghton Mifflin, Boston.</location>
<contexts>
<context position="1730" citStr="Francis and KuCera, 1982" startWordPosition="252" endWordPosition="255">ple access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al., 1994). The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably re</context>
</contexts>
<marker>Francis, KuCera, 1982</marker>
<rawString>W.N. Francis and F. KuCera. 1982. Frequency Analysis of English Usage. Houghton Mifflin, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Robust part-of-speech tagging using a hidden markov model.</title>
<date>1985</date>
<tech>Technical report, IBM,</tech>
<institution>T.J. Watson Research Center.</institution>
<contexts>
<context position="2080" citStr="Jelinek, 1985" startWordPosition="307" endWordPosition="308"> for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have</context>
</contexts>
<marker>Jelinek, 1985</marker>
<rawString>F. Jelinek. 1985. Robust part-of-speech tagging using a hidden markov model. Technical report, IBM, T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Forming word classes by statistical clustering for statistical language modelling.</title>
<date>1993</date>
<booktitle>Contributions to Quantitative Linguistics,</booktitle>
<pages>221--226</pages>
<editor>In Reinhard KOhler and Burghard B. Rieger, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, The Netherlands.</location>
<contexts>
<context position="3129" citStr="Kneser and Ney (1993)" startWordPosition="466" endWordPosition="469">ges for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (&amp;quot;certain&amp;quot; and &amp;quot;right&amp;quot;) with their immediate neighbors. What these approaches have in common is that they classify words instead of individual occurrences. Given the widespread part-of-speech ambiguity of words this is problematic.&apos; How should a word like &amp;quot;plant&amp;quot; be categorized if it has uses both as a verb and as a noun? How can a categorization be considered meaningful if the infinitive marker &amp;quot;to&amp;quot; i</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1993. Forming word classes by statistical clustering for statistical language modelling. In Reinhard KOhler and Burghard B. Rieger, editors, Contributions to Quantitative Linguistics, pages 221-226. Kluwer Academic Publishers, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--225</pages>
<contexts>
<context position="2117" citStr="Kupiec, 1992" startWordPosition="313" endWordPosition="314">oes show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993). They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is necessary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992). Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant. The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a). Several researchers have worked on learning grammatical prope</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Julian Kupiec. 1992. Robust part-of-speech tagging using a hidden markov model. Computer Speech and Language, 6:225-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Murax: A robust linguistic approach for question answering using an online encyclopedia.</title>
<date>1993</date>
<booktitle>In Proceedings of SIGIR &apos;93,</booktitle>
<pages>181--190</pages>
<contexts>
<context position="1149" citStr="Kupiec, 1993" startWordPosition="165" endWordPosition="166">ng number of languages, there is a growing need for robust processing techniques that can analyze text without expensive and time-consuming adaptation to new domains and genres. This need motivates research on fully automatic text processing that may rely on general principles of linguistics and computation, but does not depend on knowledge about individual words. In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993), robust parsing (Abney, 1991), and general parsing (deMarcken, 1990; Charniak et al., 1994). The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest. The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and KuCera, 1982). 2 Related Work Th</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Julian Kupiec. 1993. Murax: A robust linguistic approach for question answering using an online encyclopedia. In Proceedings of SIGIR &apos;93, pages 181-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Ross</author>
</authors>
<title>The category squish: Endstation Hauptwort.</title>
<date>1972</date>
<booktitle>In Papers from the Eighth Regional Meeting.</booktitle>
<publisher>Chicago Linguistic Society.</publisher>
<contexts>
<context position="28476" citStr="Ross, 1972" startWordPosition="4753" endWordPosition="4754">uld improve results. Categories that can be induced well (those characterized by local dependencies) could be input into procedures that learn phrase structure (e.g. (Brill and Marcus, 1992b; Finch, 1993)). These phrase constraints could then be incorporated into the distributional tagger to characterize non-local dependencies. Finally, our procedure induces a &amp;quot;hard&amp;quot; part-ofspeech classification of occurrences in context, i.e., each occurrence is assigned to only one category. It is by no means generally accepted that such a classification is linguistically adequate. There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. For example, &amp;quot;fun&amp;quot; 3Because of phrases like &amp;quot;I had sweet potatoes&amp;quot;, forms of &amp;quot;have&amp;quot; cannot serve as a reliable discriminator either. in &amp;quot;It&apos;s a fun thing to do.&amp;quot; has properties of both a noun and an adjective (superlative &amp;quot;funnest&amp;quot; possible). We are planning to explore &amp;quot;soft&amp;quot; classification algorithms that can account for these phenomena. 6 Conclusion In this paper, we have attempted to construct an algorithm for fully automatic distributional taggi</context>
</contexts>
<marker>Ross, 1972</marker>
<rawString>John R. Ross. 1972. The category squish: Endstation Hauptwort. In Papers from the Eighth Regional Meeting. Chicago Linguistic Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Part-of-speech induction from scratch.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL 31,</booktitle>
<pages>251--258</pages>
<location>Columbus OH.</location>
<contexts>
<context position="10325" citStr="Schiitze, 1993" startWordPosition="1659" endWordPosition="1660">on experiments, which are described in detail below: • induction based on word type only • induction based on word type and context • induction based on word type and context, restricted to &amp;quot;natural&amp;quot; contexts • induction based on word type and context, using generalized left and right context vectors 3.1 Induction based on word type only The two context vectors of a word characterize the distribution of neighboring words to its left and right. The concatenation of left and right context vector can therefore serve as a representation of a word&apos;s distributional behavior (Finch and Chater, 1992; Schiitze, 1993). We formed such concatenated vectors for all 47,025 words (surface forms) in the Brown corpus. Here, we use the raw 250- dimensional context vectors and apply the SVD to the 47,025-by-500 matrix (47,025 words with two 250-dimensional context vectors each). We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot (Cutting et al., 1992) (group average agglomeration applied to a sample). This classification constitutes the baseline performance for distributional partof-speech tagging. All occurrences of a word</context>
<context position="16720" citStr="Schiitze, 1993" startWordPosition="2705" endWordPosition="2706"> generalized right context vector v for word w was then formed by counting how often words from these 250 classes occurred to the right of w. Entry v, counts the number of times that a word from class i occurs to the right of w in the corpus (as opposed to the number of times that the word with frequency rank i occurs to the right of w). Generalized left context vectors were derived by an analogous procedure using word-based right context vectors. Note that the information about left and right is kept separate in this computation. This differs from previous approaches (Finch and Chater, 1992; Schiitze, 1993) in which left and right context vectors of a word are always used in one concatenated vector. There are arguably fewer different types of right syntactic contexts than types of syntactic categories. For example, transitive verbs and prepositions belong to different syntactic categories, but their right contexts are virtually identical in that they require a noun phrase. This generalization could not be exploited if left and right context were not treated separately. Another argument for the two-step derivation is that many words don&apos;t have any of the 250 most frequent words as their left or r</context>
</contexts>
<marker>Schiitze, 1993</marker>
<rawString>Hinrich Schiitze. 1993. Part-of-speech induction from scratch. In Proceedings of ACL 31, pages 251-258, Columbus OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Whitney Tabor</author>
</authors>
<title>Syntactic Innovation: A Connectionist Model.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="28505" citStr="Tabor, 1994" startWordPosition="4757" endWordPosition="4758">ies that can be induced well (those characterized by local dependencies) could be input into procedures that learn phrase structure (e.g. (Brill and Marcus, 1992b; Finch, 1993)). These phrase constraints could then be incorporated into the distributional tagger to characterize non-local dependencies. Finally, our procedure induces a &amp;quot;hard&amp;quot; part-ofspeech classification of occurrences in context, i.e., each occurrence is assigned to only one category. It is by no means generally accepted that such a classification is linguistically adequate. There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. For example, &amp;quot;fun&amp;quot; 3Because of phrases like &amp;quot;I had sweet potatoes&amp;quot;, forms of &amp;quot;have&amp;quot; cannot serve as a reliable discriminator either. in &amp;quot;It&apos;s a fun thing to do.&amp;quot; has properties of both a noun and an adjective (superlative &amp;quot;funnest&amp;quot; possible). We are planning to explore &amp;quot;soft&amp;quot; classification algorithms that can account for these phenomena. 6 Conclusion In this paper, we have attempted to construct an algorithm for fully automatic distributional tagging, using unannotated corpora</context>
</contexts>
<marker>Tabor, 1994</marker>
<rawString>Whitney Tabor. 1994. Syntactic Innovation: A Connectionist Model. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London.</location>
<note>Second Edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Butterworths, London. Second Edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>