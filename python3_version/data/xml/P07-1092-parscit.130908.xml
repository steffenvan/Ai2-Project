<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000854">
<title confidence="0.960409">
Machine Translation by Triangulation:
Making Effective Use of Multi-Parallel Corpora
</title>
<author confidence="0.995825">
Trevor Cohn and Mirella Lapata
</author>
<affiliation confidence="0.997324">
Human Computer Research Centre, School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.997763">
{tcohn,mlap}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.994728" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999612117647059">
Current phrase-based SMT systems perform
poorly when using small training sets. This
is a consequence of unreliable translation es-
timates and low coverage over source and
target phrases. This paper presents a method
which alleviates this problem by exploit-
ing multiple translations of the same source
phrase. Central to our approach is triangula-
tion, the process of translating from a source
to a target language via an intermediate third
language. This allows the use of a much
wider range of parallel corpora for train-
ing, and can be combined with a standard
phrase-table using conventional smoothing
methods. Experimental results demonstrate
BLEU improvements for triangulated mod-
els over a standard phrase-based system.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999860982142858">
Statistical machine translation (Brown et al., 1993)
has seen many improvements in recent years, most
notably the transition from word- to phrase-based
models (Koehn et al., 2003). Modern SMT sys-
tems are capable of producing high quality transla-
tions when provided with large quantities of training
data. With only a small training sample, the trans-
lation output is often inferior to the output from us-
ing larger corpora because the translation algorithm
must rely on more sparse estimates of phrase fre-
quencies and must also ‘back-off’ to smaller sized
phrases. This often leads to poor choices of target
phrases and reduces the coherence of the output. Un-
fortunately, parallel corpora are not readily available
in large quantities, except for a small subset of the
world’s languages (see Resnik and Smith (2003) for
discussion), therefore limiting the potential use of
current SMT systems.
In this paper we provide a means for obtaining
more reliable translation frequency estimates from
small datasets. We make use of multi-parallel cor-
pora (sentence aligned parallel texts over three or
more languages). Such corpora are often created
by international organisations, the United Nations
(UN) being a prime example. They present a chal-
lenge for current SMT systems due to their rela-
tively moderate size and domain variability (exam-
ples of UN texts include policy documents, proceed-
ings of meetings, letters, etc.). Our method translates
each target phrase, t, first to an intermediate lan-
guage, i, and then into the source language, s. We
call this two-stage translation process triangulation
(Kay, 1997). We present a probabilistic formulation
through which we can estimate the desired phrase
translation distribution (phrase-table) by marginali-
sation, p(s|t) _ Ei p(s, i|t).
As with conventional smoothing methods (Koehn
et al., 2003; Foster et al., 2006), triangulation in-
creases the robustness of phrase translation esti-
mates. In contrast to smoothing, our method allevi-
ates data sparseness by exploring additional multi-
parallel data rather than adjusting the probabilities of
existing data. Importantly, triangulation provides us
with separately estimated phrase-tables which could
be further smoothed to provide more reliable dis-
tributions. Moreover, the triangulated phrase-tables
can be easily combined with the standard source-
target phrase-table, thereby improving the coverage
over unseen source phrases.
As an example, consider Figure 1 which shows
the coverage of unigrams and larger n-gram phrases
when using a standard source target phrase-table, a
triangulated phrase-table with one (it) and nine lan-
guages (all), and a combination of standard and tri-
angulated phrase-tables (all+standard). The phrases
were harvested from a small French-English bitext
</bodyText>
<page confidence="0.957003">
728
</page>
<note confidence="0.925731">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 728–735,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999920727272727">
and evaluated against a test set. Although very few
small phrases are unknown, the majority of larger
phrases are unseen. The Italian and all results show
that triangulation alone can provide similar or im-
proved coverage compared to the standard source-
target model; further improvement is achieved by
combining the triangulated and standard models
(all+standard). These models and datasets will be
described in detail in Section 3.
We also demonstrate that triangulation can be
used on its own, that is without a source-target dis-
tribution, and still yield acceptable translation out-
put. This is particularly heartening, as it provides a
means of translating between the many “low den-
sity” language pairs for which we don’t yet have a
source-target bitext. This allows SMT to be applied
to a much larger set of language pairs than was pre-
viously possible.
In the following section we provide an overview
of related work. Section 3 introduces a generative
formulation of triangulation. We present our evalua-
tion framework in Section 4 and results in Section 5.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99980716">
The idea of using multiple source languages for
improving the translation quality of the target lan-
guage dates back at least to Kay (1997), who ob-
served that ambiguities in translating from one lan-
guage onto another may be resolved if a transla-
tion into some third language is available. Systems
which have used this notion of triangulation typi-
cally create several candidate sentential target trans-
lations for source sentences via different languages.
A single translation is then selected by finding the
candidate that yields the best overall score (Och and
Ney, 2001; Utiyama and Isahara, 2007) or by co-
training (Callison-Burch and Osborne, 2003). This
ties in with recent work on ensemble combinations
of SMT systems, which have used alignment tech-
niques (Matusov et al., 2006) or simple heuristics
(Eisele, 2005) to guide target sentence selection and
generation. Beyond SMT, the use of an intermediate
language as a translation aid has also found appli-
cation in cross-lingual information retrieval (Gollins
and Sanderson, 2001).
Callison-Burch et al. (2006) propose the use of
paraphrases as a means of dealing with unseen
source phrases. Their method acquires paraphrases
by identifying candidate phrases in the source lan-
</bodyText>
<figure confidence="0.760748">
1 2 3 4 5 6
phrase length
</figure>
<figureCaption confidence="0.998725666666667">
Figure 1: Coverage of fr --+ en test phrases using a 10,000 sen-
tence bitext. The standard model is shown alongside triangu-
lated models using one (Italian) or nine other languages (all).
</figureCaption>
<bodyText confidence="0.99986328125">
guage, translating them into multiple target lan-
guages, and then back to the source. Unknown
source phrases are substituted by the back-translated
paraphrases and translation proceeds on the para-
phrases.
In line with previous work, we exploit multi-
ple source corpora to alleviate data sparseness and
increase translation coverage. However, we differ
in several important respects. Our method oper-
ates over phrases rather than sentences. We propose
a generative formulation which treats triangulation
not as a post-processing step but as part of the trans-
lation model itself. The induced phrase-table entries
are fed directly into the decoder, thus avoiding the
additional inefficiencies of merging the output of
several translation systems.
Although related to Callison-Burch et al. (2006)
our method is conceptually simpler and more gen-
eral. Phrase-table entries are created via multiple
source languages without the intermediate step of
paraphrase extraction, thereby reducing the expo-
sure to compounding errors. Our phrase-tables may
well contain paraphrases but these are naturally in-
duced as part of our model, without extra processing
effort. Furthermore, we improve the translation esti-
mates for both seen and unseen phrase-table entries,
whereas Callison-Burch et al. concentrate solely on
unknown phrases. In contrast to Utiyama and Isa-
hara (2007), we employ a large number of inter-
mediate languages and demonstrate how triangu-
lated phrase-tables can be combined with standard
phrase-tables to improve translation output.
</bodyText>
<figure confidence="0.965747666666667">
standard
Italian
all
all + standard
proportion of test events in phrase table
0.005 0.01 0.02 0.05 0.1 0.2 0.5 1
</figure>
<page confidence="0.703545">
729
</page>
<bodyText confidence="0.414818">
une patate delicate une patate chaud une question delicate
</bodyText>
<figureCaption confidence="0.9000325">
Figure 2: Triangulation between English (source) and French (target), showing three phrases in Dutch, Danish and Portuguese,
respectively. Arrows denote phrases aligned in a language pair and also the generative translation process.
</figureCaption>
<figure confidence="0.777405">
source
a hot potato
intermediate
een hete aardappel en varm kartoffel uma batata quente
target
</figure>
<sectionHeader confidence="0.944649" genericHeader="method">
3 Triangulation
</sectionHeader>
<bodyText confidence="0.999870387096774">
We start with a motivating example before formalis-
ing the mechanics of triangulation. Consider trans-
lating the English phrase a hot potato1 into French,
as shown in Figure 2. In our corpus this English
phrase occurs only three times. Due to errors in
the word alignment the phrase was not included in
the English-French phrase-table. Triangulation first
translates hot potato into a set of intermediate lan-
guages (Dutch, Danish and Portuguese are shown in
the figure), and then these phrases are further trans-
lated into the target language (French). In the ex-
ample, four different target phrases are obtained, all
of which are useful phrase-table entries. We argue
that the redundancy introduced by a large suite of
other languages can correct for errors in the word
alignments and also provide greater generalisation,
since the translation distribution is estimated from a
richer set of data-points. For example, instances of
the Danish en varm kartoffel may be used to trans-
late several English phrases, not only a hot potato.
In general we expect that a wider range of pos-
sible translations are found for any source phrase,
simply due to the extra layer of indirection. So, if a
source phrase tends to align with two different tar-
get phrases, then we would also expect it to align
with two phrases in the ‘intermediate’ language.
These intermediate phrases should then each align
with two target phrases, yielding up to four target
phrases. Consequently, triangulation will often pro-
duce more varied translation distributions than the
standard source-target approach.
</bodyText>
<subsectionHeader confidence="0.996061">
3.1 Formalisation
</subsectionHeader>
<bodyText confidence="0.9999838">
We now formalise triangulation as a generative
probabilistic process operating independently on
phrase pairs. We start with the conditional distri-
bution over three languages, p(s, i|t), where the ar-
guments denote phrases in the source, intermediate
</bodyText>
<footnote confidence="0.774014">
1An idiom meaning a situation for which no one wants to
claim responsibility.
</footnote>
<bodyText confidence="0.99795775">
and target language, respectively. From this distri-
bution, we can find the desired conditional over the
source-target pair by marginalising out the interme-
diate phrases:2
</bodyText>
<equation confidence="0.999432333333333">
p(s|t) = � p(s|i, t)p(i|t)
�
≈ p(s|i)p(i|t) (1)
</equation>
<bodyText confidence="0.999936724137931">
where (1) imposes a simplifying conditional inde-
pendence assumption: the intermediate phrase fully
represents the information (semantics, syntax, etc.)
in the source phrase, rendering the target phrase re-
dundant in p(s|i, t).
Equation (1) requires that all phrases in the
intermediate-target bitext must also be found in the
source-intermediate bitext, such that p(s|i) is de-
fined. Clearly this will often not be the case. In these
situations we could back-off to another distribution
(by discarding part, or all, of the conditioning con-
text), however we take a more pragmatic approach
and ignore the missing phrases. This problem of
missing contexts is uncommon in multi-parallel cor-
pora, but is more common when the two bitexts are
drawn from different sources.
While triangulation is intuitively appealing, it
may suffer from a few problems. Firstly, as with any
SMT approach, the translation estimates are based
on noisy automatic word alignments. This leads to
many errors and omissions in the phrase-table. With
a standard source-target phrase-table these errors are
only encountered once, however with triangulation
they are encountered twice, and therefore the errors
will compound. This leads to more noisy estimates
than in the source-target phrase-table.
Secondly, the increased exposure to noise means
that triangulation will omit a greater proportion of
large or rare phrases than the standard method. An
</bodyText>
<footnote confidence="0.9870655">
2Equation (1) is used with the source and target arguments
reversed to give p(t s).
</footnote>
<page confidence="0.994348">
730
</page>
<bodyText confidence="0.998863533333333">
alignment error in either of the source-intermediate
or intermediate-target bitexts can prevent the extrac-
tion of a source-target phrase pair. This effect can be
seen in Figure 1, where the coverage of the Italian
triangulated phrase-table is worse than the standard
source-target model, despite the two models using
the same sized bitexts. As we explain in the next
section, these problems can be ameliorated by us-
ing the triangulated phrase-table in conjunction with
a standard phrase-table.
Finally, another potential problem stems from the
independence assumption in (1), which may be an
oversimplification and lead to a loss of information.
The experiments in Section 5 show that this effect is
only mild.
</bodyText>
<subsectionHeader confidence="0.999945">
3.2 Merging the phrase-tables
</subsectionHeader>
<bodyText confidence="0.9999528">
Once induced, the triangulated phrase-table can be
usefully combined with the standard source-target
phrase-table. The simplest approach is to use linear
interpolation to combine the two (or more) distribu-
tions, as follows:
</bodyText>
<equation confidence="0.9644005">
p(s, t) = � Ajpj(s,t) (2)
j
</equation>
<bodyText confidence="0.999757388888889">
where each joint distribution, pj, has a non-negative
weight, Aj, and the weights sum to one. The joint
distribution for triangulated phrase-tables is defined
in an analogous way to Equation (1). We expect
that the standard phrase-table should be allocated
a higher weight than triangulated phrase-tables, as
it will be less noisy. The joint distribution is now
conditionalised to yield p(s|t) and p(t|s), which are
both used as features in the decoder. Note that the re-
sulting conditional distribution will be drawn solely
from one input distribution when the conditioning
context is unseen in the remaining distributions. This
may lead to an over-reliance on unreliable distribu-
tions, which can be ameliorated by smoothing (e.g.,
Foster et al. (2006)).
As an alternative to linear interpolation, we also
employ a weighted product for phrase-table combi-
nation:
</bodyText>
<equation confidence="0.8249005">
p(s|t) a � pj(s|t)λj (3)
j
</equation>
<bodyText confidence="0.999938222222222">
This has the same form used for log-linear training
of SMT decoders (Och, 2003), which allows us to
treat each distribution as a feature, and learn the mix-
ing weights automatically. Note that we must indi-
vidually smooth the component distributions in (3)
to stop zeros from propagating. For this we use
Simple Good-Turing smoothing (Gale and Samp-
son, 1995) for each distribution, which provides es-
timates for zero count events.
</bodyText>
<sectionHeader confidence="0.995313" genericHeader="method">
4 Experimental Design
</sectionHeader>
<bodyText confidence="0.989025136363637">
Corpora We used the Europarl corpus (Koehn,
2005) for experimentation. This corpus consists of
about 700,000 sentences of parliamentary proceed-
ings from the European Union in eleven European
languages. We present results on the full corpus for a
range of language pairs. In addition, we have created
smaller parallel corpora by sub-sampling 10,000
sentence bitexts for each language pair. These cor-
pora are likely to have minimal overlap — about
1.5% of the sentences will be shared between each
pair. However, the phrasal overlap is much greater
(10 to 20%), which allows for triangulation using
these common phrases. This training setting was
chosen to simulate translating to or from a “low
density” language, where only a few small indepen-
dently sourced parallel corpora are available. These
bitexts were used for direct translation and triangula-
tion. All experimental results were evaluated on the
ACL/WMT 20053 set of 2,000 sentences, and are
reported in BLEU percentage-points.
Decoding Pharaoh (Koehn, 2003), a beam-
search decoder, was used to maximise:
</bodyText>
<equation confidence="0.984887">
�T� = arg max
T j fj(T, S)λj (4)
</equation>
<bodyText confidence="0.9984412">
where T and S denote a target and source sentence
respectively. The parameters, Aj, were trained using
minimum error rate training (Och, 2003) to max-
imise the BLEU score (Papineni et al., 2002) on
a 150 sentence development set. We used a stan-
dard set of features, comprising a 4-gram language
model, distance based distortion model, forward
and backward translation probabilities, forward and
backward lexical translation scores and the phrase-
and word-counts. The translation models and lex-
ical scores were estimated on the training corpus
which was automatically aligned using Giza++ (Och
et al., 1999) in both directions between source and
target and symmetrised using the growing heuristic
(Koehn et al., 2003).
</bodyText>
<footnote confidence="0.988796">
3For details see http://www.statmt.org/wpt05/
mt-shared-task.
</footnote>
<page confidence="0.994403">
731
</page>
<bodyText confidence="0.999054384615385">
Lexical weights The lexical translation score is
used for smoothing the phrase-table translation esti-
mate. This represents the translation probability of a
phrase when it is decomposed into a series of inde-
pendent word-for-word translation steps (Koehn et
al., 2003), and has proven a very effective feature
(Zens and Ney, 2004; Foster et al., 2006). Pharaoh’s
lexical weights require access to word-alignments;
calculating these alignments between the source and
target words in a phrase would prove difficult for
a triangulated model. Therefore we use a modified
lexical score, corresponding to the maximum IBM
model 1 score for the phrase pair:
</bodyText>
<equation confidence="0.99149075">
1 � p(tk|sak) (5)
lex(t|s) = Z max
�
k
</equation>
<bodyText confidence="0.9999873">
where the maximisation4 ranges over all one-to-
many alignments and Z normalises the score by the
number of possible alignments.
The lexical probability is obtained by interpo-
lating a relative frequency estimate on the source-
target bitext with estimates from triangulation, in
the same manner used for phrase translations in (1)
and (2). The addition of the lexical probability fea-
ture yielded a substantial gain of up to two BLEU
points over a basic feature set.
</bodyText>
<sectionHeader confidence="0.996294" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999577461538462">
The evaluation of our method was motivated by
three questions: (1) How do different training re-
quirements affect the performance of the triangu-
lated models presented in this paper? We expect
performance gains with triangulation on small and
moderate datasets. (2) Is machine translation out-
put influenced by the choice of the intermediate lan-
guage/s? Here, we would like to evaluate whether
the number and choice of intermediate languages
matters. (3) What is the quality of the triangulated
phrase-table? In particular, we are interested in the
resulting distribution and whether it is sufficiently
distinct from the standard phrase-table.
</bodyText>
<subsectionHeader confidence="0.995844">
5.1 Training requirements
</subsectionHeader>
<bodyText confidence="0.999732666666667">
Before reporting our results, we briefly discuss the
specific choice of model for our experiments. As
mentioned in Section 3, our method combines the
</bodyText>
<footnote confidence="0.9480335">
4The maximisation in (5) can be replaced with a sum with
similar experimental results.
</footnote>
<table confidence="0.998476666666667">
standard interp +indic separate
en de 12.03 12.66 12.95 12.25
fr en 23.02 24.63 23.86 23.43
</table>
<tableCaption confidence="0.798438">
Table 1: Different feature sets used with the 10K training
corpora, using a single language (es) for triangulation. The
columns refer to standard, uniform interpolation, interpolation
</tableCaption>
<bodyText confidence="0.987105454545454">
with 0-1 indicator features, and separate phrase-tables, respec-
tively.
triangulated phrase-table with the standard source-
target one. This is desired in order to compensate for
the noise incurred by the triangulation process. We
used two combination methods, namely linear inter-
polation (see (2)) and a weighted geometric mean
(see (3)).
Table 1 reports the results for two translation tasks
when triangulating with a single language (es) us-
ing three different feature sets, each with different
translation features. The interpolation model uses
uniform linear interpolation to merge the standard
and triangulated phrase-tables. Non-uniform mix-
tures did not provide consistent gains, although,
as expected, biasing towards the standard phrase-
table was more effective than against. The indicator
model uses the same interpolated distribution along
with a series of 0-1 indicator features to identify the
source of each event, i.e., if each (s, t) pair is present
in phrase-table j. We also tried per-context features
with similar results. The separate model has a sepa-
rate feature for each phrase-table.
All three feature sets improve over the standard
source-target system, while the interpolated features
provided the best overall performance. The rela-
tively poorer performance of the separate model
is perhaps surprising, as it is able to differentially
weight the component distributions; this is probably
due to MERT not properly handling the larger fea-
ture sets. In all subsequent experiments we report
results using linear interpolation.
As a proof of concept, we first assessed the ef-
fect of triangulation on corpora consisting of 10,000
sentence bitexts. We expect triangulation to de-
liver performance gains on small corpora, since a
large number of phrase-table entries will be un-
seen. In Table 2 each entry shows the BLEU score
when using the standard phrase-table and the ab-
solute improvement when using triangulation. Here
we have used three languages for triangulation
(it U {de, en, es, fr}\{s, t}). The source-target lan-
guages were chosen so as to mirror the evaluation
setup of NAACL/WMT. The translation tasks range
</bodyText>
<page confidence="0.993289">
732
</page>
<table confidence="0.999098555555555">
s ↓ t → de en es fr
de - 17.58 16.84 18.06
- +1.20 +1.99 +1.94
en 12.45 - 23.83 24.05
+1.22 - +1.04 +1.48
es 12.31 23.83 - 32.69
+2.24 +1.35 - +0.85
fr 11.76 23.02 31.22 -
+2.41 +2.24 +1.30 -
</table>
<tableCaption confidence="0.989453333333333">
Table 2: BLEU improvements over the standard phrase-table
(top) when interpolating with three triangulated phrase-tables
(bottom) on the small training sample.
</tableCaption>
<bodyText confidence="0.999823108108108">
from easy (es → fr) to very hard (de → en). In all
cases triangulation resulted in an improvement in
translation quality, with the highest gains observed
for the most difficult tasks (to and from German).
For these tasks the standard systems have poor cov-
erage (due in part to the sizeable vocabulary of Ger-
man phrases) and therefore the gain can be largely
explained by the additional coverage afforded by the
triangulated phrase-tables.
To test whether triangulation can also improve
performance of larger corpora we ran six separate
translation tasks on the full Europarl corpus. The
results are presented in Table 3, for a single trian-
gulation language used alone (triang) or uniformly
interpolated with the standard phrase-table (interp).
These results show that triangulation can produce
high quality translations on its own, which is note-
worthy, as it allows for SMT between a much larger
set of language pairs. Using triangulation in con-
junction with the standard phrase-table improved
over the standard system in most instances, and
only degraded performance once. The improvement
is largest for the German tasks which can be ex-
plained by triangulation providing better robustness
to noisy alignments (which are often quite poor for
German) and better estimates of low-count events.
The difficulty of aligning German with the other lan-
guages is apparent from the Giza++ perplexity: the
final Model 4 perplexities for German are quite high,
as much as double the perplexity for more easily
aligned language pairs (e.g., Spanish-French).
Figure 3 shows the effect of triangulation on dif-
ferent sized corpora for the language pair fr → en.
It presents learning curves for the standard system
and a triangulated system using one language (es).
As can be seen, gains from triangulation only di-
minish slightly for larger training corpora, and that
</bodyText>
<table confidence="0.963865285714286">
task standard interm triang interp
de → en 23.85 es 23.48 24.36
en → de 17.24 es 16.28 17.42
es → en 30.48 fr 29.06 30.52
en → es 29.09 fr 28.19 29.09
fr → en 29.66 es 29.59 30.36
en → fr 30.07 es 28.94 29.62
</table>
<tableCaption confidence="0.987065">
Table 3: Results on the full training set showing triangulation
with a single language, both alone (triang) and alongside a stan-
dard model (interp).
</tableCaption>
<figure confidence="0.5932105">
10K 40K 160K 700K
size of training bitext(s)
</figure>
<figureCaption confidence="0.965748">
Figure 3: Learning curve forfr --+ en translation for the standard
source-target model and a triangulated model using Spanish as
an intermediate language.
</figureCaption>
<bodyText confidence="0.99954175">
the purely triangulated models have very competi-
tive performance. The gain from interpolation with
a triangulated model is roughly equivalent to having
twice as much training data.
Finally, notice that triangulation may benefit
when the sentences in each bitext are drawn from the
same source, in that there are no unseen ‘intermedi-
ate’ phrases, and therefore (1) can be easily evalu-
ated. We investigate this by examining the robust-
ness of our method in the face of disjoint bitexts.
The concepts contained in each bitext will be more
varied, potentially leading to better coverage of the
target language. In lieu of a study on different do-
main bitexts which we plan for the future, we bi-
sected the Europarl corpus for fr → en, triangulat-
ing with Spanish. The triangulated models were pre-
sented with fr-es and es-en bitexts drawn from either
the same half of the corpus or from different halves,
resulting in scores of 28.37 and 28.13, respectively.5
These results indicate that triangulation is effective
</bodyText>
<footnote confidence="0.891532">
5The baseline source-target system on one half has a score
of 28.85.
</footnote>
<figure confidence="0.991373111111111">
BLEU score
22 24 26 28 30
●
●
●
●standard
triang
interp
●
</figure>
<page confidence="0.64786">
733
</page>
<figure confidence="0.711633">
triang interp
</figure>
<figureCaption confidence="0.983602">
Figure 4: Comparison of different triangulation languages for
fr --+ en translation, relative to the standard model (10K training
sample). The bar for fi has been truncated to fit on the graph.
</figureCaption>
<bodyText confidence="0.9965235">
for disjoint bitexts, although ideally we would test
this with independently sourced parallel texts.
</bodyText>
<subsectionHeader confidence="0.998653">
5.2 The choice of intermediate languages
</subsectionHeader>
<bodyText confidence="0.999511310344828">
The previous experiments used an ad-hoc choice
of ‘intermediate’ language/s for triangulation, and
we now examine which languages are most effec-
tive. Figure 4 shows the efficacy of the remaining
nine languages when translating fr —* en. Minimum
error-rate training was not used for this experiment,
or the next shown in Figure 5, in order to highlight
the effect of the changing translation estimates. Ro-
mance languages (es, it, pt) give the best results,
both on their own and when used together with the
standard phrase-table (using uniform interpolation);
Germanic languages (de, nl, da, sv) are a distant sec-
ond, with the less related Greek and Finnish the least
useful. Interpolation yields an improvement for all
‘intermediate’ languages, even Finnish, which has a
very low score when used alone.
The same experiment was repeated for en —* de
translation with similar trends, except that the
Germanic languages out-scored the Romance lan-
guages. These findings suggest that ‘intermediate’
languages which exhibit a high degree of similarity
with the source or target language are desirable. We
conjecture that this is a consequence of better auto-
matic word alignments and a generally easier trans-
lation task, as well as a better preservation of infor-
mation between aligned phrases.
Using a single language for triangulation clearly
improves performance, but can we realise further
improvements by using additional languages? Fig-
</bodyText>
<figure confidence="0.419779">
# intermediate languages
</figure>
<figureCaption confidence="0.99967825">
Figure 5: Increasing the number of intermediate languages used
for triangulation increases performance for fr --+ en (10K train-
ing sample). The dashed line shows the BLEU score for the
standard phrase-table.
</figureCaption>
<bodyText confidence="0.999991818181818">
ure 5 shows the performance profile for fr —* en
when adding languages in a fixed order. The lan-
guages were ordered by family, with Romance be-
fore Germanic before Greek and Finnish. Each ad-
dition results in an increase in performance, even for
the final languages, from which we expect little in-
formation. The purely triangulated (triang) and in-
terpolated scores (interp) are converging, suggesting
that the source-target bitext is redundant given suf-
ficient triangulated data. We obtained similar results
for en —* de.
</bodyText>
<subsectionHeader confidence="0.975308">
5.3 Evaluating the quality of the phrase-table
</subsectionHeader>
<bodyText confidence="0.999986842105263">
Our experimental results so far have shown that
triangulation is not a mere approximation of the
source-target phrase-table, but that it extracts addi-
tional useful translation information. We now as-
sess the phrase-table quality more directly. Com-
parative statistics of a standard and a triangulated
phrase-table are given in Table 4. The coverage over
source and target phrases is much higher in the stan-
dard table than the triangulated tables, which reflects
the reduced ability of triangulation to extract large
phrases — despite the large increase in the num-
ber of events. The table also shows the overlapping
probability mass which measures the sum of prob-
ability in one table for which the events are present
in the other. This shows that the majority of mass
is shared by both tables (as joint distributions), al-
though there are significant differences. The Jensen-
Shannon divergence is perhaps more appropriate for
the comparison, giving a relatively high divergence
</bodyText>
<figure confidence="0.99885364">
19 20 21 22 23 24 25
da
de
el
es
fi (−14.26)
it
nl
pt
sv
da
de
el
es
fi
it
nl
pt
sv
1 2 3 4 5 6 7 8 9
22 23 24 25 26
triang
interp
BLEU score
BLEU score
</figure>
<page confidence="0.974072">
734
</page>
<table confidence="0.9862972">
standard triang
source phrases (M) 8 2.5
target phrases (M) 7 2.5
events (M) 12 70
overlapping mass 0.646 0.750
</table>
<tableCaption confidence="0.991046">
Table 4: Comparative statistics of the standard triangulated table
on fr --+ en using the full training set and Spanish as an inter-
mediate language.
</tableCaption>
<bodyText confidence="0.999779333333333">
of 0.3937. This augurs well for the combination of
standard and triangulated phrase-tables, where di-
versity is valued. The decoding results (shown in
Table 3 for fr —* en) indicate that the two meth-
ods have similar efficacy, and that their interpolated
combination provides the best overall performance.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.976612366666667">
In this paper we have presented a novel method for
obtaining more reliable translation estimates from
small datasets. The key premise of our work is that
multi-parallel data can be usefully exploited for im-
proving the coverage and quality of phrase-based
SMT. Our triangulation method translates from a
source to a target via one or many intermediate lan-
guages. We present a generative formulation of this
process and show how it can be used together with
the entries of a standard source-target phrase-table.
We observe large performance gains when trans-
lating with triangulated models trained on small
datasets. Furthermore, when combined with a stan-
dard phrase-table, our models also yield perfor-
mance improvements on larger datasets. Our exper-
iments revealed that triangulation benefits from a
large set of intermediate languages and that perfor-
mance is increased when languages of the same fam-
ily to the source or target are used as intermediates.
We have just scratched the surface of the possi-
bilities for the framework discussed here. Important
future directions lie in combining triangulation with
richer means of conventional smoothing and using
triangulation to translate between low-density lan-
guage pairs.
Acknowledgements The authors acknowledge
the support of EPSRC (grants GR/T04540/01 and
GR/T04557/01). Special thanks to Markus Becker, Chris
Callison-Burch, David Talbot and Miles Osborne for their
helpful comments.
</bodyText>
<sectionHeader confidence="0.965757" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999876551724138">
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, R. L. Mercer. 1993.
The mathematics of statistical machine translation: Parame-
ter estimation. Computational Linguistics, 19(2):263–311.
C. Callison-Burch, M. Osborne. 2003. Bootstrapping parallel
corpora. In Proceedings of the NAACL Workshop on Build-
ing and Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, Edmonton, Canada.
C. Callison-Burch, P. Koehn, M. Osborne. 2006. Improved sta-
tistical machine translation using paraphrases. In Proceed-
ings of the HLT/NAACL, 17–24, New York, NY.
A. Eisele. 2005. First steps towards multi-engine machine
translation. In Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts, 155–158, Ann Arbor, MI.
G. Foster, R. Kuhn, H. Johnson. 2006. Phrase-table smooth-
ing for statistical machine translation. In Proceedings of the
EMNLP, 53–61, Sydney, Australia.
W. A. Gale, G. Sampson. 1995. Good-turing frequency esti-
mation without tears. Journal of Quantitative Linguistics,
2(3):217–237.
T. Gollins, M. Sanderson. 2001. Improving cross language
retrieval with triangulated translation. In Proceedings of the
SIGIR, 90–95, New Orleans, LA.
M. Kay. 1997. The proper place of men and machines in lan-
guage translation. Machine Translation, 12(1–2):3–23.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of the HLT/NAACL, 48–
54, Edomonton, Canada.
P. Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, Uni-
versity of Southern California, Los Angeles, California.
P. Koehn. 2005. Europarl: A parallel corpus for evaluation of
machine translation. In Proceedings of MT Summit, Phuket,
Thailand.
E. Matusov, N. Ueffing, H. Ney. 2006. Computing consesus
translation from multiple machine translation systems us-
ing enhanced hypotheses alignment. In Proceedings of the
EACL, 33–40, Trento, Italy.
F. J. Och, H. Ney. 2001. Statistical multi-source translation. In
Proceedings of the MT Summit, 253–258, Santiago de Com-
postela, Spain.
F. J. Och, C. Tillmann, H. Ney. 1999. Improved alignment
models for statistical machine translation. In Proceedings of
the EMNLP and VLC, 20–28, University of Maryland, Col-
lege Park, MD.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proceedings of the ACL, 160–167, Sap-
poro, Japan.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. BLEU: A
method for automatic evaluation of machine translation. In
Proceedings of the ACL, 311–318, Philadelphia, PA.
P. Resnik, N. A. Smith. 2003. The Web as a parallel corpus.
Computational Linguistics, 29(3):349–380.
M. Utiyama, H. Isahara. 2007. A comparison of pivot methods
for phrase-based statistical machine translation. In Proceed-
ings of the HLT/NAACL, 484–491, Rochester, NY.
R. Zens, H. Ney. 2004. Improvements in phrase-based statisti-
cal machine translation. In D. M. Susan Dumais, S. Roukos,
eds., Proceedings of the HLT/NAACL, 257–264, Boston,
MA.
</reference>
<page confidence="0.998574">
735
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.733405">
<title confidence="0.9884585">Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora</title>
<author confidence="0.790855">Cohn Lapata</author>
<affiliation confidence="0.994207">Human Computer Research Centre, School of Informatics University of Edinburgh</affiliation>
<abstract confidence="0.996559055555556">Current phrase-based SMT systems perform poorly when using small training sets. This is a consequence of unreliable translation estimates and low coverage over source and target phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source Central to our approach is triangulathe process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1020" citStr="Brown et al., 1993" startWordPosition="143" endWordPosition="146">arget phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system. 1 Introduction Statistical machine translation (Brown et al., 1993) has seen many improvements in recent years, most notably the transition from word- to phrase-based models (Koehn et al., 2003). Modern SMT systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also ‘back-off’ to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the outpu</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>M Osborne</author>
</authors>
<title>Bootstrapping parallel corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the NAACL Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5653" citStr="Callison-Burch and Osborne, 2003" startWordPosition="860" endWordPosition="863">g multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan1 2 3 4 5 6 phrase l</context>
</contexts>
<marker>Callison-Burch, Osborne, 2003</marker>
<rawString>C. Callison-Burch, M. Osborne. 2003. Bootstrapping parallel corpora. In Proceedings of the NAACL Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>M Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT/NAACL,</booktitle>
<pages>17--24</pages>
<location>New York, NY.</location>
<contexts>
<context position="6067" citStr="Callison-Burch et al. (2006)" startWordPosition="923" endWordPosition="926">ferent languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan1 2 3 4 5 6 phrase length Figure 1: Coverage of fr --+ en test phrases using a 10,000 sentence bitext. The standard model is shown alongside triangulated models using one (Italian) or nine other languages (all). guage, translating them into multiple target languages, and then back to the source. Unknown source phrases are substituted by the back-translated paraphrases and translation proceeds on the paraphrases. In line with previ</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>C. Callison-Burch, P. Koehn, M. Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of the HLT/NAACL, 17–24, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Eisele</author>
</authors>
<title>First steps towards multi-engine machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts, 155–158,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="5821" citStr="Eisele, 2005" startWordPosition="889" endWordPosition="890">nto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan1 2 3 4 5 6 phrase length Figure 1: Coverage of fr --+ en test phrases using a 10,000 sentence bitext. The standard model is shown alongside triangulated models using one (Italian) or nine</context>
</contexts>
<marker>Eisele, 2005</marker>
<rawString>A. Eisele. 2005. First steps towards multi-engine machine translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, 155–158, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>R Kuhn</author>
<author>H Johnson</author>
</authors>
<title>Phrase-table smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<location>53–61, Sydney, Australia.</location>
<contexts>
<context position="2827" citStr="Foster et al., 2006" startWordPosition="427" endWordPosition="430">nge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters, etc.). Our method translates each target phrase, t, first to an intermediate language, i, and then into the source language, s. We call this two-stage translation process triangulation (Kay, 1997). We present a probabilistic formulation through which we can estimate the desired phrase translation distribution (phrase-table) by marginalisation, p(s|t) _ Ei p(s, i|t). As with conventional smoothing methods (Koehn et al., 2003; Foster et al., 2006), triangulation increases the robustness of phrase translation estimates. In contrast to smoothing, our method alleviates data sparseness by exploring additional multiparallel data rather than adjusting the probabilities of existing data. Importantly, triangulation provides us with separately estimated phrase-tables which could be further smoothed to provide more reliable distributions. Moreover, the triangulated phrase-tables can be easily combined with the standard sourcetarget phrase-table, thereby improving the coverage over unseen source phrases. As an example, consider Figure 1 which sho</context>
<context position="13882" citStr="Foster et al. (2006)" startWordPosition="2138" endWordPosition="2141">d phrase-tables is defined in an analogous way to Equation (1). We expect that the standard phrase-table should be allocated a higher weight than triangulated phrase-tables, as it will be less noisy. The joint distribution is now conditionalised to yield p(s|t) and p(t|s), which are both used as features in the decoder. Note that the resulting conditional distribution will be drawn solely from one input distribution when the conditioning context is unseen in the remaining distributions. This may lead to an over-reliance on unreliable distributions, which can be ameliorated by smoothing (e.g., Foster et al. (2006)). As an alternative to linear interpolation, we also employ a weighted product for phrase-table combination: p(s|t) a � pj(s|t)λj (3) j This has the same form used for log-linear training of SMT decoders (Och, 2003), which allows us to treat each distribution as a feature, and learn the mixing weights automatically. Note that we must individually smooth the component distributions in (3) to stop zeros from propagating. For this we use Simple Good-Turing smoothing (Gale and Sampson, 1995) for each distribution, which provides estimates for zero count events. 4 Experimental Design Corpora We us</context>
<context position="16694" citStr="Foster et al., 2006" startWordPosition="2582" endWordPosition="2585">imated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). 3For details see http://www.statmt.org/wpt05/ mt-shared-task. 731 Lexical weights The lexical translation score is used for smoothing the phrase-table translation estimate. This represents the translation probability of a phrase when it is decomposed into a series of independent word-for-word translation steps (Koehn et al., 2003), and has proven a very effective feature (Zens and Ney, 2004; Foster et al., 2006). Pharaoh’s lexical weights require access to word-alignments; calculating these alignments between the source and target words in a phrase would prove difficult for a triangulated model. Therefore we use a modified lexical score, corresponding to the maximum IBM model 1 score for the phrase pair: 1 � p(tk|sak) (5) lex(t|s) = Z max � k where the maximisation4 ranges over all one-tomany alignments and Z normalises the score by the number of possible alignments. The lexical probability is obtained by interpolating a relative frequency estimate on the sourcetarget bitext with estimates from trian</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>G. Foster, R. Kuhn, H. Johnson. 2006. Phrase-table smoothing for statistical machine translation. In Proceedings of the EMNLP, 53–61, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>G Sampson</author>
</authors>
<title>Good-turing frequency estimation without tears.</title>
<date>1995</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="14375" citStr="Gale and Sampson, 1995" startWordPosition="2219" endWordPosition="2223">. This may lead to an over-reliance on unreliable distributions, which can be ameliorated by smoothing (e.g., Foster et al. (2006)). As an alternative to linear interpolation, we also employ a weighted product for phrase-table combination: p(s|t) a � pj(s|t)λj (3) j This has the same form used for log-linear training of SMT decoders (Och, 2003), which allows us to treat each distribution as a feature, and learn the mixing weights automatically. Note that we must individually smooth the component distributions in (3) to stop zeros from propagating. For this we use Simple Good-Turing smoothing (Gale and Sampson, 1995) for each distribution, which provides estimates for zero count events. 4 Experimental Design Corpora We used the Europarl corpus (Koehn, 2005) for experimentation. This corpus consists of about 700,000 sentences of parliamentary proceedings from the European Union in eleven European languages. We present results on the full corpus for a range of language pairs. In addition, we have created smaller parallel corpora by sub-sampling 10,000 sentence bitexts for each language pair. These corpora are likely to have minimal overlap — about 1.5% of the sentences will be shared between each pair. Howe</context>
</contexts>
<marker>Gale, Sampson, 1995</marker>
<rawString>W. A. Gale, G. Sampson. 1995. Good-turing frequency estimation without tears. Journal of Quantitative Linguistics, 2(3):217–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Gollins</author>
<author>M Sanderson</author>
</authors>
<title>Improving cross language retrieval with triangulated translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the SIGIR,</booktitle>
<pages>90--95</pages>
<location>New Orleans, LA.</location>
<contexts>
<context position="6037" citStr="Gollins and Sanderson, 2001" startWordPosition="919" endWordPosition="922">s for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan1 2 3 4 5 6 phrase length Figure 1: Coverage of fr --+ en test phrases using a 10,000 sentence bitext. The standard model is shown alongside triangulated models using one (Italian) or nine other languages (all). guage, translating them into multiple target languages, and then back to the source. Unknown source phrases are substituted by the back-translated paraphrases and translation proceeds on the p</context>
</contexts>
<marker>Gollins, Sanderson, 2001</marker>
<rawString>T. Gollins, M. Sanderson. 2001. Improving cross language retrieval with triangulated translation. In Proceedings of the SIGIR, 90–95, New Orleans, LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>The proper place of men and machines in language translation.</title>
<date>1997</date>
<booktitle>Machine Translation,</booktitle>
<pages>12--1</pages>
<contexts>
<context position="2574" citStr="Kay, 1997" startWordPosition="392" endWordPosition="393">datasets. We make use of multi-parallel corpora (sentence aligned parallel texts over three or more languages). Such corpora are often created by international organisations, the United Nations (UN) being a prime example. They present a challenge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters, etc.). Our method translates each target phrase, t, first to an intermediate language, i, and then into the source language, s. We call this two-stage translation process triangulation (Kay, 1997). We present a probabilistic formulation through which we can estimate the desired phrase translation distribution (phrase-table) by marginalisation, p(s|t) _ Ei p(s, i|t). As with conventional smoothing methods (Koehn et al., 2003; Foster et al., 2006), triangulation increases the robustness of phrase translation estimates. In contrast to smoothing, our method alleviates data sparseness by exploring additional multiparallel data rather than adjusting the probabilities of existing data. Importantly, triangulation provides us with separately estimated phrase-tables which could be further smooth</context>
<context position="5142" citStr="Kay (1997)" startWordPosition="781" endWordPosition="782">larly heartening, as it provides a means of translating between the many “low density” language pairs for which we don’t yet have a source-target bitext. This allows SMT to be applied to a much larger set of language pairs than was previously possible. In the following section we provide an overview of related work. Section 3 introduces a generative formulation of triangulation. We present our evaluation framework in Section 4 and results in Section 5. 2 Related Work The idea of using multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used </context>
</contexts>
<marker>Kay, 1997</marker>
<rawString>M. Kay. 1997. The proper place of men and machines in language translation. Machine Translation, 12(1–2):3–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL, 48– 54,</booktitle>
<location>Edomonton, Canada.</location>
<contexts>
<context position="1147" citStr="Koehn et al., 2003" startWordPosition="163" endWordPosition="166">e phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system. 1 Introduction Statistical machine translation (Brown et al., 1993) has seen many improvements in recent years, most notably the transition from word- to phrase-based models (Koehn et al., 2003). Modern SMT systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also ‘back-off’ to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the output. Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world’s lang</context>
<context position="2805" citStr="Koehn et al., 2003" startWordPosition="423" endWordPosition="426">hey present a challenge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters, etc.). Our method translates each target phrase, t, first to an intermediate language, i, and then into the source language, s. We call this two-stage translation process triangulation (Kay, 1997). We present a probabilistic formulation through which we can estimate the desired phrase translation distribution (phrase-table) by marginalisation, p(s|t) _ Ei p(s, i|t). As with conventional smoothing methods (Koehn et al., 2003; Foster et al., 2006), triangulation increases the robustness of phrase translation estimates. In contrast to smoothing, our method alleviates data sparseness by exploring additional multiparallel data rather than adjusting the probabilities of existing data. Importantly, triangulation provides us with separately estimated phrase-tables which could be further smoothed to provide more reliable distributions. Moreover, the triangulated phrase-tables can be easily combined with the standard sourcetarget phrase-table, thereby improving the coverage over unseen source phrases. As an example, consi</context>
<context position="16277" citStr="Koehn et al., 2003" startWordPosition="2521" endWordPosition="2524">d using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). 3For details see http://www.statmt.org/wpt05/ mt-shared-task. 731 Lexical weights The lexical translation score is used for smoothing the phrase-table translation estimate. This represents the translation probability of a phrase when it is decomposed into a series of independent word-for-word translation steps (Koehn et al., 2003), and has proven a very effective feature (Zens and Ney, 2004; Foster et al., 2006). Pharaoh’s lexical weights require access to word-alignments; calculating these alignments between the source and target words in a phrase would prove difficult for a triangulated mo</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrasebased translation. In Proceedings of the HLT/NAACL, 48– 54, Edomonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Noun Phrase Translation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern</institution>
<location>California, Los Angeles, California.</location>
<contexts>
<context position="15484" citStr="Koehn, 2003" startWordPosition="2394" endWordPosition="2395">are likely to have minimal overlap — about 1.5% of the sentences will be shared between each pair. However, the phrasal overlap is much greater (10 to 20%), which allows for triangulation using these common phrases. This training setting was chosen to simulate translating to or from a “low density” language, where only a few small independently sourced parallel corpora are available. These bitexts were used for direct translation and triangulation. All experimental results were evaluated on the ACL/WMT 20053 set of 2,000 sentences, and are reported in BLEU percentage-points. Decoding Pharaoh (Koehn, 2003), a beamsearch decoder, was used to maximise: �T� = arg max T j fj(T, S)λj (4) where T and S denote a target and source sentence respectively. The parameters, Aj, were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on </context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>P. Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, University of Southern California, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of MT Summit,</booktitle>
<location>Phuket, Thailand.</location>
<contexts>
<context position="14518" citStr="Koehn, 2005" startWordPosition="2244" endWordPosition="2245">linear interpolation, we also employ a weighted product for phrase-table combination: p(s|t) a � pj(s|t)λj (3) j This has the same form used for log-linear training of SMT decoders (Och, 2003), which allows us to treat each distribution as a feature, and learn the mixing weights automatically. Note that we must individually smooth the component distributions in (3) to stop zeros from propagating. For this we use Simple Good-Turing smoothing (Gale and Sampson, 1995) for each distribution, which provides estimates for zero count events. 4 Experimental Design Corpora We used the Europarl corpus (Koehn, 2005) for experimentation. This corpus consists of about 700,000 sentences of parliamentary proceedings from the European Union in eleven European languages. We present results on the full corpus for a range of language pairs. In addition, we have created smaller parallel corpora by sub-sampling 10,000 sentence bitexts for each language pair. These corpora are likely to have minimal overlap — about 1.5% of the sentences will be shared between each pair. However, the phrasal overlap is much greater (10 to 20%), which allows for triangulation using these common phrases. This training setting was chos</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for evaluation of machine translation. In Proceedings of MT Summit, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>Computing consesus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>33--40</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="5785" citStr="Matusov et al., 2006" startWordPosition="882" endWordPosition="885">biguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan1 2 3 4 5 6 phrase length Figure 1: Coverage of fr --+ en test phrases using a 10,000 sentence bitext. The standard model is shown alongside triangulate</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>E. Matusov, N. Ueffing, H. Ney. 2006. Computing consesus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proceedings of the EACL, 33–40, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Statistical multi-source translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the MT Summit, 253–258, Santiago de Compostela,</booktitle>
<contexts>
<context position="5573" citStr="Och and Ney, 2001" startWordPosition="848" endWordPosition="851">tion 4 and results in Section 5. 2 Related Work The idea of using multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires pa</context>
</contexts>
<marker>Och, Ney, 2001</marker>
<rawString>F. J. Och, H. Ney. 2001. Statistical multi-source translation. In Proceedings of the MT Summit, 253–258, Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the EMNLP and VLC,</booktitle>
<institution>University of Maryland, College Park, MD.</institution>
<contexts>
<context position="16167" citStr="Och et al., 1999" startWordPosition="2504" endWordPosition="2507">(T, S)λj (4) where T and S denote a target and source sentence respectively. The parameters, Aj, were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). 3For details see http://www.statmt.org/wpt05/ mt-shared-task. 731 Lexical weights The lexical translation score is used for smoothing the phrase-table translation estimate. This represents the translation probability of a phrase when it is decomposed into a series of independent word-for-word translation steps (Koehn et al., 2003), and has proven a very effective feature (Zens and Ney, 2004; Foster et al., 2006). Pharaoh’s lexical weights require access to word-alignments; calculatin</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F. J. Och, C. Tillmann, H. Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the EMNLP and VLC, 20–28, University of Maryland, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="14098" citStr="Och, 2003" startWordPosition="2176" endWordPosition="2177">now conditionalised to yield p(s|t) and p(t|s), which are both used as features in the decoder. Note that the resulting conditional distribution will be drawn solely from one input distribution when the conditioning context is unseen in the remaining distributions. This may lead to an over-reliance on unreliable distributions, which can be ameliorated by smoothing (e.g., Foster et al. (2006)). As an alternative to linear interpolation, we also employ a weighted product for phrase-table combination: p(s|t) a � pj(s|t)λj (3) j This has the same form used for log-linear training of SMT decoders (Och, 2003), which allows us to treat each distribution as a feature, and learn the mixing weights automatically. Note that we must individually smooth the component distributions in (3) to stop zeros from propagating. For this we use Simple Good-Turing smoothing (Gale and Sampson, 1995) for each distribution, which provides estimates for zero count events. 4 Experimental Design Corpora We used the Europarl corpus (Koehn, 2005) for experimentation. This corpus consists of about 700,000 sentences of parliamentary proceedings from the European Union in eleven European languages. We present results on the f</context>
<context position="15705" citStr="Och, 2003" startWordPosition="2434" endWordPosition="2435">aining setting was chosen to simulate translating to or from a “low density” language, where only a few small independently sourced parallel corpora are available. These bitexts were used for direct translation and triangulation. All experimental results were evaluated on the ACL/WMT 20053 set of 2,000 sentences, and are reported in BLEU percentage-points. Decoding Pharaoh (Koehn, 2003), a beamsearch decoder, was used to maximise: �T� = arg max T j fj(T, S)λj (4) where T and S denote a target and source sentence respectively. The parameters, Aj, were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). 3For details see http://ww</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the ACL, 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="15756" citStr="Papineni et al., 2002" startWordPosition="2442" endWordPosition="2445">ranslating to or from a “low density” language, where only a few small independently sourced parallel corpora are available. These bitexts were used for direct translation and triangulation. All experimental results were evaluated on the ACL/WMT 20053 set of 2,000 sentences, and are reported in BLEU percentage-points. Decoding Pharaoh (Koehn, 2003), a beamsearch decoder, was used to maximise: �T� = arg max T j fj(T, S)λj (4) where T and S denote a target and source sentence respectively. The parameters, Aj, were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). 3For details see http://www.statmt.org/wpt05/ mt-shared-task. 731 Lexical wei</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the ACL, 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>N A Smith</author>
</authors>
<title>The Web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="1781" citStr="Resnik and Smith (2003)" startWordPosition="267" endWordPosition="270">T systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also ‘back-off’ to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the output. Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world’s languages (see Resnik and Smith (2003) for discussion), therefore limiting the potential use of current SMT systems. In this paper we provide a means for obtaining more reliable translation frequency estimates from small datasets. We make use of multi-parallel corpora (sentence aligned parallel texts over three or more languages). Such corpora are often created by international organisations, the United Nations (UN) being a prime example. They present a challenge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters, etc</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>P. Resnik, N. A. Smith. 2003. The Web as a parallel corpus. Computational Linguistics, 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Utiyama</author>
<author>H Isahara</author>
</authors>
<title>A comparison of pivot methods for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the HLT/NAACL,</booktitle>
<pages>484--491</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="5601" citStr="Utiyama and Isahara, 2007" startWordPosition="852" endWordPosition="855">in Section 5. 2 Related Work The idea of using multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying can</context>
<context position="7801" citStr="Utiyama and Isahara (2007)" startWordPosition="1189" endWordPosition="1193">n systems. Although related to Callison-Burch et al. (2006) our method is conceptually simpler and more general. Phrase-table entries are created via multiple source languages without the intermediate step of paraphrase extraction, thereby reducing the exposure to compounding errors. Our phrase-tables may well contain paraphrases but these are naturally induced as part of our model, without extra processing effort. Furthermore, we improve the translation estimates for both seen and unseen phrase-table entries, whereas Callison-Burch et al. concentrate solely on unknown phrases. In contrast to Utiyama and Isahara (2007), we employ a large number of intermediate languages and demonstrate how triangulated phrase-tables can be combined with standard phrase-tables to improve translation output. standard Italian all all + standard proportion of test events in phrase table 0.005 0.01 0.02 0.05 0.1 0.2 0.5 1 729 une patate delicate une patate chaud une question delicate Figure 2: Triangulation between English (source) and French (target), showing three phrases in Dutch, Danish and Portuguese, respectively. Arrows denote phrases aligned in a language pair and also the generative translation process. source a hot pot</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>M. Utiyama, H. Isahara. 2007. A comparison of pivot methods for phrase-based statistical machine translation. In Proceedings of the HLT/NAACL, 484–491, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>Proceedings of the HLT/NAACL,</booktitle>
<pages>257--264</pages>
<editor>In D. M. Susan Dumais, S. Roukos, eds.,</editor>
<location>Boston, MA.</location>
<contexts>
<context position="16672" citStr="Zens and Ney, 2004" startWordPosition="2578" endWordPosition="2581">ical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). 3For details see http://www.statmt.org/wpt05/ mt-shared-task. 731 Lexical weights The lexical translation score is used for smoothing the phrase-table translation estimate. This represents the translation probability of a phrase when it is decomposed into a series of independent word-for-word translation steps (Koehn et al., 2003), and has proven a very effective feature (Zens and Ney, 2004; Foster et al., 2006). Pharaoh’s lexical weights require access to word-alignments; calculating these alignments between the source and target words in a phrase would prove difficult for a triangulated model. Therefore we use a modified lexical score, corresponding to the maximum IBM model 1 score for the phrase pair: 1 � p(tk|sak) (5) lex(t|s) = Z max � k where the maximisation4 ranges over all one-tomany alignments and Z normalises the score by the number of possible alignments. The lexical probability is obtained by interpolating a relative frequency estimate on the sourcetarget bitext wit</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens, H. Ney. 2004. Improvements in phrase-based statistical machine translation. In D. M. Susan Dumais, S. Roukos, eds., Proceedings of the HLT/NAACL, 257–264, Boston, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>