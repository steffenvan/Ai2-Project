<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.751336">
Joint Parsing and Alignment with Weakly Synchronized Grammars
</title>
<author confidence="0.90179">
David Burkett John Blitzer Dan Klein
</author>
<affiliation confidence="0.946997">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.997568">
{dburkett,blitzer,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.993808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995094904761905">
Syntactic machine translation systems extract
rules from bilingual, word-aligned, syntacti-
cally parsed text, but current systems for pars-
ing and word alignment are at best cascaded
and at worst totally independent of one an-
other. This work presents a unified joint model
for simultaneous parsing and word alignment.
To flexibly model syntactic divergence, we de-
velop a discriminative log-linear model over
two parse trees and an ITG derivation which
is encouraged but not forced to synchronize
with the parses. Our model gives absolute
improvements of 3.3 Fl for English pars-
ing, 2.1 Fl for Chinese parsing, and 5.5 Fl
for word alignment over each task’s indepen-
dent baseline, giving the best reported results
for both Chinese-English word alignment and
joint parsing on the parallel portion of the Chi-
nese treebank. We also show an improvement
of 1.2 BLEU in downstream MT evaluation
over basic HMM alignments.
</bodyText>
<sectionHeader confidence="0.998117" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999897645833334">
Current syntactic machine translation (MT) sys-
tems build synchronous context free grammars from
aligned syntactic fragments (Galley et al., 2004;
Zollmann et al., 2006). Extracting such grammars
requires that bilingual word alignments and mono-
lingual syntactic parses be compatible. Because of
this, much recent work in both word alignment and
parsing has focused on changing aligners to make
use of syntactic information (DeNero and Klein,
2007; May and Knight, 2007; Fossum et al., 2008)
or changing parsers to make use of word align-
ments (Smith and Smith, 2004; Burkett and Klein,
2008; Snyder et al., 2009). In the first case, how-
ever, parsers do not exploit bilingual information.
In the second, word alignment is performed with a
model that does not exploit syntactic information.
This work presents a single, joint model for parsing
and word alignment that allows both pieces to influ-
ence one another simultaneously.
While building a joint model seems intuitive,
there is no easy way to characterize how word align-
ments and syntactic parses should relate to each
other in general. In the ideal situation, each pair
of sentences in a bilingual corpus could be syntacti-
cally parsed using a synchronous context-free gram-
mar. Of course, real translations are almost always
at least partially syntactically divergent. Therefore,
it is unreasonable to expect perfect matches of any
kind between the two sides’ syntactic trees, much
less expect that those matches be well explained at
a word level. Indeed, it is sometimes the case that
large pieces of a sentence pair are completely asyn-
chronous and can only be explained monolingually.
Our model exploits synchronization where pos-
sible to perform more accurately on both word
alignment and parsing, but also allows indepen-
dent models to dictate pieces of parse trees and
word alignments when synchronization is impossi-
ble. This notion of “weak synchronization” is pa-
rameterized and estimated from data to maximize
the likelihood of the correct parses and word align-
ments. Weak synchronization is closely related to
the quasi-synchronous models of Smith and Eis-
ner (2006; 2009) and the bilingual parse reranking
model of Burkett and Klein (2008), but those models
assume that the word alignment of a sentence pair is
known and fixed.
To simultaneously model both parses and align-
</bodyText>
<note confidence="0.820555333333333">
127
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 127–135,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999898827586207">
ments, our model loosely couples three separate
combinatorial structures: monolingual trees in the
source and target languages, and a synchronous ITG
alignment that links the two languages (but is not
constrained to match linguistic syntax). The model
has no hard constraints on how these three struc-
tures must align, but instead contains a set of “syn-
chronization” features that are used to propagate
influence between the three component grammars.
The presence of synchronization features couples
the parses and alignments, but makes exact inference
in the model intractable; we show how to use a vari-
ational mean field approximation, both for comput-
ing approximate feature expectations during train-
ing, and for performing approximate joint inference
at test time.
We train our joint model on the parallel, gold
word-aligned portion of the Chinese treebank.
When evaluated on parsing and word alignment, this
model significantly improves over independently-
trained baselines: the monolingual parser of Petrov
and Klein (2007) and the discriminative word
aligner of Haghighi et al. (2009). It also improves
over the discriminative, bilingual parsing model
of Burkett and Klein (2008), yielding the highest
joint parsing F1 numbers on this data set. Finally,
our model improves word alignment in the context
of translation, leading to a 1.2 BLEU increase over
using HMM word alignments.
</bodyText>
<sectionHeader confidence="0.635895" genericHeader="introduction">
2 Joint Parsing and Alignment
</sectionHeader>
<bodyText confidence="0.993148125">
Given a source-language sentence, s, and a target-
language sentence, s&apos;, we wish to predict a source
tree t, a target tree t&apos;, and some kind of alignment
a between them. These structures are illustrated in
Figure 1.
To facilitate these predictions, we define a condi-
tional distribution P(t, a, t&apos;|s, s&apos;). We begin with a
generic conditional exponential form:
</bodyText>
<equation confidence="0.957393">
P(t, a, t&apos;|s, s&apos;) a exp BTO(t, a, t&apos;, s, s&apos;) (1)
</equation>
<bodyText confidence="0.999728647058824">
Unfortunately, a generic model of this form is in-
tractable, because we cannot efficiently sum over
all triples (t, a, t&apos;) without some assumptions about
how the features 0(t, a, t&apos;, s, s&apos;) decompose.
One natural solution is to restrict our candidate
triples to those given by a synchronous context free
grammar (SCFG) (Shieber and Schabes, 1990). Fig-
ure 1(a) gives a simple example of generation from
a log-linearly parameterized synchronous grammar,
together with its features. With the SCFG restric-
tion, we can sum over the necessary structures using
the 0(n6) bitext inside-outside algorithm, making
P(t, a, t&apos;|s, s&apos;) relatively efficient to compute expec-
tations under.
Unfortunately, an SCFG requires that all the con-
stituents of each tree, from the root down to the
words, are generated perfectly in tandem. The re-
sulting inability to model any level of syntactic di-
vergence prevents accurate modeling of the individ-
ual monolingual trees. We will consider the run-
ning example from Figure 2 throughout the paper.
Here, for instance, the verb phrase established in
such places as Quanzhou, Zhangzhou, etc. in En-
glish does not correspond to any single node in the
Chinese tree. A synchronous grammar has no choice
but to analyze this sentence incorrectly, either by ig-
noring this verb phrase in English or postulating an
incorrect Chinese constituent that corresponds to it.
Therefore, instead of requiring strict synchroniza-
tion, our model treats the two monolingual trees and
the alignment as separate objects that can vary arbi-
trarily. However, the model rewards synchronization
appropriately when the alignment brings the trees
into correspondence.
</bodyText>
<sectionHeader confidence="0.957046" genericHeader="method">
3 Weakly Synchronized Grammars
</sectionHeader>
<bodyText confidence="0.9997538">
We propose a joint model which still gives probabil-
ities on triples (t, a, t&apos;). However, instead of using
SCFG rules to synchronously enforce the tree con-
straints on t and t&apos;, we only require that each of t
and t&apos; be well-formed under separate monolingual
CFGs.
In order to permit efficient enumeration of all pos-
sible alignments a, we also restrict a to the set of
unlabeled ITG bitrees (Wu, 1997), though again we
do not require that a relate to t or t&apos; in any particular
way. Although this assumption does limit the space
of possible word-level alignments, for the domain
we consider (Chinese-English word alignment), the
reduced space still contains almost all empirically
observed alignments (Haghighi et al., 2009).1 For
</bodyText>
<figure confidence="0.970261125">
1See Section 8.1 for some new terminal productions re-
quired to make this true for the parallel Chinese treebank.
128
Features
0 (IP, s) � (b0, s, sÕ)
0 (NP, s) � (br, s, sÕ)
0 (VP, s) � (b2, s, sÕ)
0 (S, sÕ)
0 (NP, sÕ)
0 (AP, sÕ)
0 (VP, sÕ)
�� (IP, b0)
�� (b0, S)
�� (br, NP)
��� (IP, b0, S)
(b) Asynchronous Rule
Features
0( (IP, b0, S), s, s’ )
0( (NP, b1, NP), s, s’ )
0( (VP, b2, VP), s, s’ )
S
NP VP
b0
b1
b2
NP
IP
VP
(a) Synchronous Rule
S
NP
IP
VP
Parsing
NP AP VP
b0
br
b2
Alignment
Synchronization
</figure>
<figureCaption confidence="0.992320666666667">
Figure 1: Source trees, t (right), alignments, a (grid), and target trees, t&apos; (top), and feature decompositions for syn-
chronous (a) and weakly synchronous (b) grammars. Features always condition on bispans and/or anchored syntactic
productions, but weakly synchronous grammars permit more general decompositions.
</figureCaption>
<bodyText confidence="0.999988">
example, in Figure 2, the word alignment is ITG-
derivable, and each of the colored rectangles is a bi-
span in that derivation.
There are no additional constraints beyond the
independent, internal structural constraints on t, a,
and t0. This decoupling permits derivations like that
in Figure 1(b), where the top-level syntactic nodes
align, but their children are allowed to diverge. With
the three structures separated, our first model is a
completely factored decomposition of (1).
Formally, we represent a source tree t as a set of
nodes {n}, each node representing a labeled span.
Likewise, a target tree t0 is a set of nodes {n0}.2 We
represent alignments a as sets of bispans {b}, indi-
cated by rectangles in Figure 1.3 Using this notation,
the initial model has the following form:
</bodyText>
<equation confidence="0.97469975">
⎡P(t, a, t0|s, s0) a exp ⎣X θ&gt;φF(n, s)+
n∈t
X θ&gt;φA(b, s, s0)+ X
b∈a n0∈t0
</equation>
<bodyText confidence="0.998608382352941">
Here φF(n, s) indicates a vector of source node fea-
tures, φE(n0, s0) is a vector of target node features,
and φA(b, s, s0) is a vector of alignment bispan fea-
tures. Of course, this model is completely asyn-
2For expositional clarity, we describe n and n0 as labeled
spans only. However, in general, features that depend on n or
n0 are permitted to depend on the entire rule, and do in our final
system.
3Alignments a link arbitrary spans of s and s0 (including
non-constituents and individual words). We discuss the relation
to word-level alignments in Section 4.
chronous so far, and fails to couple the trees and
alignments at all. To permit soft constraints between
the three structures we are modeling, we add a set of
synchronization features.
For n E t and b E a, we say that n ✄ b if n and b
both map onto the same span of s. We define b ✁ n0
analogously for n0 E t0. We now consider three
different types of synchronization features. Source-
alignment synchronization features φ✄(n, b) are ex-
tracted whenever n ✄ b. Similarly, target-alignment
features φ✁(b, n0) are extracted if b ✁ n0. These
features capture phenomena like that of bispan b7
in Figure 2. Here the Chinese noun �-ft synchronizes
with the ITG derivation, but the English projection
of b7 is a distituent. Finally, we extract source-target
features φN(n, b, n0) whenever n✄b✁n0. These fea-
tures capture complete bispan synchrony (as in bi-
span b8) and can be expressed over triples (n, b, n0)
which happen to align, allowing us to reward syn-
chrony, but not requiring it. All of these licensing
conditions are illustrated in Figure 1(b).
With these features added, the final form of the
model is:
</bodyText>
<equation confidence="0.975517333333333">
⎡P(t, a, t0|s, s0) a exp ⎣X θ&gt;φF(n, s)+
n∈t
X θ&gt;φA(b, s, s0)+ X θ&gt;φE(n0,s0)+
b∈a n0∈t0
X θ&gt;φ✄(n, b)+ X θ&gt;φ✁(b, n0)+
n✄b b✁n0
n✄b✁n0
X ⎤θ&gt;φ.(n, b, n0) ⎦
⎤ (2)
θ&gt;φE(n0, s0) ⎦
(3)
129
</equation>
<bodyText confidence="0.9999586">
We emphasize that because of the synchronization
features, this final form does not admit any known
efficient dynamic programming for the exact com-
putation of expectations. We will therefore turn to a
variational inference method in Section 6.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999255571428572">
With the model’s locality structure defined, we
just need to specify the actual feature function,
φ. We divide the features into three types: pars-
ing features (φF(n, s) and φ,,(n&apos;, s&apos;)), alignment
features (φA(b, s, s&apos;)) and synchronization features
(φp(n, b), φa(b, n&apos;), and φ./(n, b, n&apos;)). We detail
each of these in turn here.
</bodyText>
<subsectionHeader confidence="0.998945">
4.1 Parsing
</subsectionHeader>
<bodyText confidence="0.999918166666667">
The monolingual parsing features we use are sim-
ply parsing model scores under the parser of Petrov
and Klein (2007). While that parser uses heavily re-
fined PCFGs with rule probabilities defined at the
refined symbol level, we interact with its posterior
distribution via posterior marginal probabilities over
unrefined symbols. In particular, to each unrefined
anchored production iAj —* iBkCj, we associate a
single feature whose value is the marginal quantity
log P(iBkCj|iAj, s) under the monolingual parser.
These scores are the same as the variational rule
scores of Matsuzaki et al. (2005).4
</bodyText>
<subsectionHeader confidence="0.996513">
4.2 Alignment
</subsectionHeader>
<bodyText confidence="0.9974814">
We begin with the same set of alignment features
as Haghighi et al. (2009), which are defined only for
terminal bispans. In addition, we include features on
nonterminal bispans, including a bias feature, fea-
tures that measure the difference in size between
the source and target spans, features that measure
the difference in relative sentence position between
the source and target spans, and features that mea-
sure the density of word-to-word alignment poste-
riors under a separate unsupervised word alignment
model.
4Of course the structure of our model permits any of the
additional rule-factored monolingual parsing features that have
been described in the parsing literature, but in the present work
we focus on the contributions of joint modeling.
</bodyText>
<subsectionHeader confidence="0.998123">
4.3 Synchronization
</subsectionHeader>
<bodyText confidence="0.999937111111111">
Our synchronization features are indicators for the
syntactic types of the participating nodes. We de-
termine types at both a coarse (more collapsed
than Treebank symbols) and fine (Treebank sym-
bol) level. At the coarse level, we distinguish be-
tween phrasal nodes (e.g. S, NP), synthetic nodes
introduced in the process of binarizing the grammar
(e.g. S&apos;, NP&apos;), and part-of-speech nodes (e.g. NN,
VBZ). At the fine level, we distinguish all nodes
by their exact label. We use coarse and fine types
for both partially synchronized (source-alignment or
target-alignment) features and completely synchro-
nized (source-alignment-target) features. The inset
of Figure 2 shows some sample features. Of course,
we could devise even more sophisticated features by
using the input text itself. As we shall see, however,
our model gives significant improvements with these
simple features alone.
</bodyText>
<sectionHeader confidence="0.994217" genericHeader="method">
5 Learning
</sectionHeader>
<bodyText confidence="0.994543727272727">
We learn the parameters of our model on the paral-
lel portion of the Chinese treebank. Although our
model assigns probabilities to entire synchronous
derivations of sentences, the parallel Chinese tree-
bank gives alignments only at the word level (1 by
1 bispans in Figure 2). This means that our align-
ment variable a is not fully observed. Because of
this, given a particular word alignment w, we max-
imize the marginal probability of the set of deriva-
tions A(w) that are consistent with w (Haghighi et
al., 2009).5
</bodyText>
<equation confidence="0.886716">
G(θ)=log � P(ti, a, t&apos;i|si, s&apos;i)
aEA(wi)
</equation>
<bodyText confidence="0.999693">
We maximize this objective using standard gradient
methods (Nocedal and Wright, 1999). As with fully
visible log-linear models, the gradient for the ith sen-
tence pair with respect to θ is a difference of feature
expectations:
</bodyText>
<equation confidence="0.999521">
VG(θ) =E (� �) [φ(t a t&apos; si,&apos;)]
P a ti,wi,ti,si,si 27 , i,2 i
[φ(t, a, t&apos;, si, s&apos; i)] (4)
− EP(t,a,t�Jsi,s� i)
</equation>
<bodyText confidence="0.963646">
5We also learn from non-ITG alignments by maximizing the
marginal probability of the set of minimum-recall error align-
ments in the same way as Haghighi et al. (2009)
</bodyText>
<figure confidence="0.978550476190477">
130
Sample Synchronization Features
S
VP
VP
VBN
PP
���( e b8e ) = COARSESOURCETARGET asa , asa 1
FINESOURCETARGET , 1
VBD
were established in such places as Quanzhou Zhangzhou etc.
0&gt;( ) = COARSESOURCEALIGN
, b7 s 1
FINESOURCEALIGN 1
NP
NP
PP
IN
NP
JJ NNS
IN
P
PP
NP
NP
NN
VV
AS VP
NP
VP
ba
b7
b8
在
泉州
漳州
等
地
了
...
NP
...
</figure>
<figureCaption confidence="0.728831666666667">
Figure 2: An example of a Chinese-English sentence pair with parses, word alignments, and a subset of the full optimal
ITG derivation, including one totally unsynchronized bispan (b4), one partially synchronized bispan (b7), and and fully
synchronized bispan (b8). The inset provides some examples of active synchronization features (see Section 4.3) on
these bispans. On this example, the monolingual English parser erroneously attached the lower PP to the VP headed by
established, and the non-syntactic ITG word aligner misaligned 4 to such instead of to etc. Our joint model corrected
both of these mistakes because it was rewarded for the synchronization of the two NPs joined by b8.
</figureCaption>
<bodyText confidence="0.999957666666667">
We cannot efficiently compute the model expecta-
tions in this equation exactly. Therefore we turn next
to an approximate inference method.
</bodyText>
<sectionHeader confidence="0.979696" genericHeader="method">
6 Mean Field Inference
</sectionHeader>
<bodyText confidence="0.9283838">
Instead of computing the model expectations from
(4), we compute the expectations for each sentence
pair with respect to a simpler, fully factored distri-
bution Q(t, a, t&apos;) = q(t)q(a)q(t&apos;). Rewriting Q in
log-linear form, we have:
</bodyText>
<equation confidence="0.396085">
2
Q(t, a, t&apos;) oc exp 41On + 1: Ob + 1 On0
nEt bEa n0Et0
</equation>
<bodyText confidence="0.999674333333333">
Here, the On, Ob and On0 are variational parameters
which we set to best approximate our weakly syn-
chronized model from (3):
</bodyText>
<equation confidence="0.996025">
~ ~
O* = argmin KL Q�||PB(t, a, t&apos;|s, s&apos;)
�
</equation>
<bodyText confidence="0.95785">
Once we have found Q, we compute an approximate
gradient by replacing the model expectations with
</bodyText>
<equation confidence="0.9077476">
expectations under Q:
i)�
EQ(a|wi) ��(ti, a, t&apos; i, si, s&apos;
��(t, a, t&apos;, si, s&apos; i)�
− EQ(t,a,t0|si,s0 i)
</equation>
<bodyText confidence="0.999946105263158">
Now, we will briefly describe how we compute Q.
First, note that the parameters O of Q factor along
individual source nodes, target nodes, and bispans.
The combination of the KL objective and our par-
ticular factored form of Q make our inference pro-
cedure a structured mean field algorithm (Saul and
Jordan, 1996). Structured mean field techniques are
well-studied in graphical models, and our adaptation
in this section to multiple grammars follows stan-
dard techniques (see e.g. Wainwright and Jordan,
2008).
Rather than derive the mean field updates forO,
we describe the algorithm (shown in Figure 3) pro-
cedurally. Similar to block Gibbs sampling, we it-
eratively optimize each component (source parse,
target parse, and alignment) of the model in turn,
conditioned on the others. Where block Gibbs sam-
pling conditions on fixed trees or ITG derivations,
our mean field algorithm maintains uncertainty in
</bodyText>
<page confidence="0.721696">
131
</page>
<figureCaption confidence="0.997588333333333">
Figure 3: Structured mean field inference for the weakly
synchronized model. I(n E t) is an indicator value for
the presence of node n in source tree t.
</figureCaption>
<bodyText confidence="0.9998115">
the form of monolingual parse forests or ITG forests.
The key components to this uncertainty are the
expected counts of particular source nodes, target
nodes, and bispans under the mean field distribution:
</bodyText>
<equation confidence="0.981065142857143">
/-
fin=1: qp(t)I(n E t)
t
1: An0 = qp(t&apos;)I(n&apos; E t&apos;)
t0
1: Ab =
a
</equation>
<bodyText confidence="0.999784">
Since dynamic programs exist for summing over
each of the individual factors, these expectations can
be computed in polynomial time.
</bodyText>
<subsectionHeader confidence="0.997022">
6.1 Pruning
</subsectionHeader>
<bodyText confidence="0.999986696969697">
Although we can approximate the expectations from
(4) in polynomial time using our mean field distribu-
tion, in practice we must still prune the ITG forests
and monolingual parse forests to allow tractable in-
ference. We prune our ITG forests using the same
basic idea as Haghighi et al. (2009), but we em-
ploy a technique that allows us to be more aggres-
sive. Where Haghighi et al. (2009) pruned bispans
based on how many unsupervised HMM alignments
were violated, we first train a maximum-matching
word aligner (Taskar et al., 2005) using our super-
vised data set, which has only half the precision er-
rors of the unsupervised HMM. We then prune ev-
ery bispan which violates at least three alignments
from the maximum-matching aligner. When com-
pared to pruning the bitext forest of our model with
Haghighi et al. (2009)’s HMM technique, this new
technique allows us to maintain the same level of ac-
curacy while cutting the number of bispans in half.
In addition to pruning the bitext forests, we also
prune the syntactic parse forests using the mono-
lingual parsing model scores. For each unrefined
anchored production iAj —* iBkCj, we com-
pute the marginal probability P(iAj,i Bk,k Cj|s) un-
der the monolingual parser (these are equivalent to
the maxrule scores from Petrov and Klein 2007). We
only include productions where this probability is
greater than 10−20. Note that at training time, we are
not guaranteed that the gold trees will be included
in the pruned forest. Because of this, we replace the
gold trees ti, t&apos;i with oracle trees from the pruned for-
est, which can be found efficiently using a variant of
the inside algorithm (Huang, 2008).
</bodyText>
<sectionHeader confidence="0.946393" genericHeader="method">
7 Testing
</sectionHeader>
<bodyText confidence="0.999948">
Once the model has been trained, we still need to
determine how to use it to predict parses and word
alignments for our test sentence pairs. Ideally, given
the sentence pair (s, s&apos;), we would find:
</bodyText>
<equation confidence="0.997345">
(t*, w*, t&apos;*) = argmax P(t, w, t&apos;|s, s&apos;)
t,w,t0
1: =argmax
t,w,t0 aEA(w) P(t, a, t&apos;|s, s&apos;)
</equation>
<bodyText confidence="0.999754333333333">
Of course, this is also intractable, so we once again
resort to our mean field approximation. This yields
the approximate solution:
</bodyText>
<equation confidence="0.976764">
1: (t*, w*, t&apos;*) = argmax
t,w,t0 aEA(w)
</equation>
<bodyText confidence="0.996267">
However, recall that Q incorporates the model’s mu-
tual constraint into the variational parameters, which
</bodyText>
<figure confidence="0.881222821428571">
Input: sentence pair (s, s0)
parameter vector θ
Output: variational parameters ψ
1. Initialize
ψ0n ←θ&gt;φF(n, s)
ψ0
b ←θ&gt;φA(b, s, s0)
ψ0n0 ←θ&gt;φE(n0, s0)
µ0n ← Et qψ0(t)I(n ∈ t), etc for µ0b, µ0n0
2. While not converged, for each n, n0, b in
the monolingual and ITG charts
ψn θ&gt; (φF(n, s) + Eb,n✄b µib
En,n✄b En0,b✁n0 µi−1
n µi−1
n0 φ./(n, b, n0) J
µb ← Ea qψ(a)I(b ∈ a) (bitext inside-outside)
updates for ψin0, µin0 analogous to ψin, µin
3. Return variational parameters ψ
1φ✄(n, b)+
i−1 i−1 ( 0)J
Eb,n✄b En0,b✁n0 µb µn0 φ./ n, b, n
µi n ← Et qψ(t)I(n ∈ t) (inside-outside,)/,
ψib ← θ&gt; (φA(b, s, s0) + En,n✄b µin 1Y&apos;D(n, b)+
En0,b✁n0 µi−1
n0 φ✁(b, n0)+
qp(a)I(b E a)
Q(t, a, t&apos;)
132
</figure>
<bodyText confidence="0.999876428571429">
factor into q(t), q(a), and q(t&apos;). This allows us to
simplify further, and find the maximum a posteriori
assignments under the variational distribution. The
trees can be found quickly using the Viterbi inside
algorithm on their respective qs. However, the sum
for computing w* under q is still intractable.
As we cannot find the maximum probability word
alignment, we provide two alternative approaches
for finding w*. The first is to just find the Viterbi
ITG derivation a* = argmaxa q(a) and then set w*
to contain exactly the 1x1 bispans in a*. The second
method, posterior thresholding, is to compute poste-
rior marginal probabilities under q for each 1x1 cell
beginning at position i, j in the word alignment grid:
</bodyText>
<equation confidence="0.824698">
m(i, j) = � q(a)I((i, i + 1, j, j + 1) E a)
a
</equation>
<bodyText confidence="0.999371333333333">
We then include w(i, j) in w* if m(w(i, j)) &gt; T,
where T is a threshold chosen to trade off precision
and recall. For our experiments, we found that the
Viterbi alignment was uniformly worse than poste-
rior thresholding. All the results from the next sec-
tion use the threshold T = 0.25.
</bodyText>
<sectionHeader confidence="0.996836" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<bodyText confidence="0.999998466666667">
We trained and tested our model on the translated
portion of the Chinese treebank (Bies et al., 2007),
which includes hand annotated Chinese and English
parses and word alignments. We separated the data
into three sets: train, dev, and test, according to the
standard Chinese treebank split. To speed up train-
ing, we only used training sentences of length G 50
words, which left us with 1974 of 2261 sentences.
We measured the results in two ways. First, we
directly measured F1 for English parsing, Chinese
parsing, and word alignment on a held out section of
the hand annotated corpus used to train the model.
Next, we further evaluated the quality of the word
alignments produced by our model by using them as
input for a machine translation system.
</bodyText>
<subsectionHeader confidence="0.979807">
8.1 Dataset-specific ITG Terminals
</subsectionHeader>
<bodyText confidence="0.9995144">
The Chinese treebank gold word alignments include
significantly more many-to-many word alignments
than those used by Haghighi et al. (2009). We are
able to produce some of these many-to-many align-
ments by including new many-to-many terminals in
</bodyText>
<figure confidence="0.894624">
(b) 2x3 (c) Gapped 2x3
</figure>
<figureCaption confidence="0.9842975">
Figure 4: Examples of phrasal alignments that can be rep-
resented by our new ITG terminal bispans.
</figureCaption>
<bodyText confidence="0.99984125">
our ITG word aligner, as shown in Figure 4. Our
terminal productions sometimes capture non-literal
translation like both sides or in recent years. They
also can allow us to capture particular, systematic
changes in the annotation standard. For example,
the gapped pattern from Figure 4 captures the stan-
dard that English word the is always aligned to the
Chinese head noun in a noun phrase. We featurize
these non-terminals with features similar to those
of Haghighi et al. (2009), and all of the alignment
results we report in Section 8.2 (both joint and ITG)
employ these features.
</bodyText>
<subsectionHeader confidence="0.99521">
8.2 Parsing and Word Alignment
</subsectionHeader>
<bodyText confidence="0.980487518518518">
To compute features that depend on external models,
we needed to train an unsupervised word aligner and
monolingual English and Chinese parsers. The un-
supervised word aligner was a pair of jointly trained
HMMs (Liang et al., 2006), trained on the FBIS cor-
pus. We used the Berkeley Parser (Petrov and Klein,
2007) for both monolingual parsers, with the Chi-
nese parser trained on the full Chinese treebank, and
the English parser trained on a concatenation of the
Penn WSJ corpus (Marcus et al., 1993) and the En-
glish side of train.6
We compare our parsing results to the mono-
lingual parsing models and to the English-Chinese
bilingual reranker of Burkett and Klein (2008),
trained on the same dataset. The results are in
Table 1. For word alignment, we compare to
6To avoid overlap in the data used to train the monolingual
parsers and the joint model, at training time, we used a separate
version of the Chinese parser, trained only on articles 400-1151
(omitting articles in train). For English parsing, we deemed it
insufficient to entirely omit the Chinese treebank data from the
monolingual parser’s training set, as otherwise the monolingual
parser would be trained entirely on out-of-domain data. There-
fore, at training time we used two separate English parsers: to
compute model scores for the first half of train, we used a parser
trained on a concatenation of the WSJ corpus and the second
half of train, and vice versa for the remaining sentences.
</bodyText>
<figure confidence="0.935714875">
全
国 国
年 近年 近年
来 来
�
岸
(a) 2x2
133
</figure>
<table confidence="0.9791418">
Test Results
Ch F1 Eng F1 Tot F1
Monolingual 83.6 81.2 82.5
Reranker 86.0 83.8 84.9
Joint 85.7 84.5 85.1
</table>
<tableCaption confidence="0.995572">
Table 1: Parsing results. Our joint model has the highest
reported Fl for English-Chinese bilingual parsing.
</tableCaption>
<table confidence="0.999827">
Test Results
Precision Recall AER F1
HMM 86.0 58.4 30.0 69.5
ITG 86.8 73.4 20.2 79.5
Joint 85.5 84.6 14.9 85.0
</table>
<tableCaption confidence="0.922424">
Table 2: Word alignment results. Our joint model has the
highest reported Fl for English-Chinese word alignment.
</tableCaption>
<bodyText confidence="0.997642857142857">
the baseline unsupervised HMM word aligner and
to the English-Chinese ITG-based word aligner
of Haghighi et al. (2009). The results are in Table 2.
As can be seen, our model makes substantial im-
provements over the independent models. For pars-
ing, we improve absolute F1 over the monolingual
parsers by 2.1 in Chinese, and by 3.3 in English.
For word alignment, we improve absolute F1 by 5.5
over the non-syntactic ITG word aligner. In addi-
tion, our English parsing results are better than those
of the Burkett and Klein (2008) bilingual reranker,
the current top-performing English-Chinese bilin-
gual parser, despite ours using a much simpler set
of synchronization features.
</bodyText>
<subsectionHeader confidence="0.987438">
8.3 Machine Translation
</subsectionHeader>
<bodyText confidence="0.99919">
We further tested our alignments by using them to
train the Joshua machine translation system (Li and
Khudanpur, 2008). Table 3 describes the results of
our experiments. For all of the systems, we tuned
</bodyText>
<table confidence="0.99541225">
Rules Tune Test
HMM 1.1M 29.0 29.4
ITG 1.5M 29.9 30.41
Joint 1.5M 29.6 30.6
</table>
<tableCaption confidence="0.9479635">
Table 3: Tune and test BLEU results for machine transla-
tion systems built with different alignment tools. † indi-
cates a statistically significant difference between a sys-
tem’s test performance and the one above it.
</tableCaption>
<bodyText confidence="0.999981111111111">
on 1000 sentences of the NIST 2004 and 2005 ma-
chine translation evaluations, and tested on 400 sen-
tences of the NIST 2006 MT evaluation. Our train-
ing set consisted of 250k sentences of newswire dis-
tributed with the GALE project, all of which were
sub-sampled to have high Ngram overlap with the
tune and test sets. All of our sentences were of
length at most 40 words. When building the trans-
lation grammars, we used Joshua’s default “tight”
phrase extraction option. We ran MERT for 4 itera-
tions, optimizing 20 weight vectors per iteration on
a 200-best list.
Table 3 gives the results. On the test set, we also
ran the approximate randomization test suggested by
Riezler and Maxwell (2005). We found that our joint
parsing and alignment system significantly outper-
formed the HMM aligner, but the improvement over
the ITG aligner was not statistically significant.
</bodyText>
<sectionHeader confidence="0.995516" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.9999704375">
The quality of statistical machine translation mod-
els depends crucially on the quality of word align-
ments and syntactic parses for the bilingual training
corpus. Our work presented the first joint model
for parsing and alignment, demonstrating that we
can improve results on both of these tasks, as well
as on downstream machine translation, by allowing
parsers and word aligners to simultaneously inform
one another. Crucial to this improved performance
is a notion of weak synchronization, which allows
our model to learn when pieces of a grammar are
synchronized and when they are not. Although ex-
act inference in the weakly synchronized model is
intractable, we developed a mean field approximate
inference scheme based on monolingual and bitext
parsing, allowing for efficient inference.
</bodyText>
<sectionHeader confidence="0.997275" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999912111111111">
We thank Adam Pauls and John DeNero for their
help in running machine translation experiments.
We also thank the three anonymous reviewers for
their helpful comments on an earlier draft of this
paper. This project is funded in part by NSF
grants 0915265 and 0643742, an NSF graduate re-
search fellowship, the CIA under grant HM1582-09-
1-0021, and BBN under DARPA contract HR0011-
06-C-0022.
</bodyText>
<page confidence="0.900631">
134
</page>
<sectionHeader confidence="0.997874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801123287671">
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese translation treebank v 1.0.
Web download. LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In EMNLP.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntax-
based statistical machine translation. In ACL MT
Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In HLT-
NAACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Takuya Matsuzaki, Yusuki Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Jon May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In EMNLP.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Workshop on Intrinsic and Extrinsic Eval-
uation Methods for MT and Summarization, ACL.
Lawrence Saul and Michael Jordan. 1996. Exploit-
ing tractable substructures in intractable networks. In
NIPS.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In ACL.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In HLT-NAACL.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilin-
gual parsing with factored estimation: using English
to parse Korean. In EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In ACL.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In EMNLP.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Varia-
tional Inference. Now Publishers Inc., Hanover, MA,
USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-AKA syntax aug-
mented machine translation system for IWSLT-06. In
IWSLT.
</reference>
<page confidence="0.944239">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960791">
<title confidence="0.999892">Joint Parsing and Alignment with Weakly Synchronized Grammars</title>
<author confidence="0.999995">David Burkett John Blitzer Dan Klein</author>
<affiliation confidence="0.9999165">Computer Science Division University of California,</affiliation>
<abstract confidence="0.998023454545455">Syntactic machine translation systems extract rules from bilingual, word-aligned, syntactically parsed text, but current systems for parsing and word alignment are at best cascaded and at worst totally independent of one another. This work presents a unified joint model for simultaneous parsing and word alignment. To flexibly model syntactic divergence, we develop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses. Our model gives absolute of 3.3 English pars- 2.1 Chinese parsing, and 5.5 for word alignment over each task’s independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Martha Palmer</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
</authors>
<title>English Chinese translation treebank v 1.0. Web download.</title>
<date>2007</date>
<pages>2007--02</pages>
<contexts>
<context position="22742" citStr="Bies et al., 2007" startWordPosition="3820" endWordPosition="3823">The second method, posterior thresholding, is to compute posterior marginal probabilities under q for each 1x1 cell beginning at position i, j in the word alignment grid: m(i, j) = � q(a)I((i, i + 1, j, j + 1) E a) a We then include w(i, j) in w* if m(w(i, j)) &gt; T, where T is a threshold chosen to trade off precision and recall. For our experiments, we found that the Viterbi alignment was uniformly worse than posterior thresholding. All the results from the next section use the threshold T = 0.25. 8 Experiments We trained and tested our model on the translated portion of the Chinese treebank (Bies et al., 2007), which includes hand annotated Chinese and English parses and word alignments. We separated the data into three sets: train, dev, and test, according to the standard Chinese treebank split. To speed up training, we only used training sentences of length G 50 words, which left us with 1974 of 2261 sentences. We measured the results in two ways. First, we directly measured F1 for English parsing, Chinese parsing, and word alignment on a held out section of the hand annotated corpus used to train the model. Next, we further evaluated the quality of the word alignments produced by our model by us</context>
</contexts>
<marker>Bies, Palmer, Mott, Warner, 2007</marker>
<rawString>Ann Bies, Martha Palmer, Justin Mott, and Colin Warner. 2007. English Chinese translation treebank v 1.0. Web download. LDC2007T02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1725" citStr="Burkett and Klein, 2008" startWordPosition="259" endWordPosition="262">sic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to each other in general. In the ideal situation, each pair of sentences in a bilingual corpus could be syntactically parsed u</context>
<context position="3333" citStr="Burkett and Klein (2008)" startWordPosition="517" endWordPosition="520"> completely asynchronous and can only be explained monolingually. Our model exploits synchronization where possible to perform more accurately on both word alignment and parsing, but also allows independent models to dictate pieces of parse trees and word alignments when synchronization is impossible. This notion of “weak synchronization” is parameterized and estimated from data to maximize the likelihood of the correct parses and word alignments. Weak synchronization is closely related to the quasi-synchronous models of Smith and Eisner (2006; 2009) and the bilingual parse reranking model of Burkett and Klein (2008), but those models assume that the word alignment of a sentence pair is known and fixed. To simultaneously model both parses and align127 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 127–135, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics ments, our model loosely couples three separate combinatorial structures: monolingual trees in the source and target languages, and a synchronous ITG alignment that links the two languages (but is not constrained to match linguistic syntax). The model has no hard </context>
<context position="4852" citStr="Burkett and Klein (2008)" startWordPosition="747" endWordPosition="750">model intractable; we show how to use a variational mean field approximation, both for computing approximate feature expectations during training, and for performing approximate joint inference at test time. We train our joint model on the parallel, gold word-aligned portion of the Chinese treebank. When evaluated on parsing and word alignment, this model significantly improves over independentlytrained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al. (2009). It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. Finally, our model improves word alignment in the context of translation, leading to a 1.2 BLEU increase over using HMM word alignments. 2 Joint Parsing and Alignment Given a source-language sentence, s, and a targetlanguage sentence, s&apos;, we wish to predict a source tree t, a target tree t&apos;, and some kind of alignment a between them. These structures are illustrated in Figure 1. To facilitate these predictions, we define a conditional distribution P(t, a, t&apos;|s, s&apos;). We begin with a generic conditional exponential form: P(t, a, t</context>
<context position="25080" citStr="Burkett and Klein (2008)" startWordPosition="4210" endWordPosition="4213">dels, we needed to train an unsupervised word aligner and monolingual English and Chinese parsers. The unsupervised word aligner was a pair of jointly trained HMMs (Liang et al., 2006), trained on the FBIS corpus. We used the Berkeley Parser (Petrov and Klein, 2007) for both monolingual parsers, with the Chinese parser trained on the full Chinese treebank, and the English parser trained on a concatenation of the Penn WSJ corpus (Marcus et al., 1993) and the English side of train.6 We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. The results are in Table 1. For word alignment, we compare to 6To avoid overlap in the data used to train the monolingual parsers and the joint model, at training time, we used a separate version of the Chinese parser, trained only on articles 400-1151 (omitting articles in train). For English parsing, we deemed it insufficient to entirely omit the Chinese treebank data from the monolingual parser’s training set, as otherwise the monolingual parser would be trained entirely on out-of-domain data. Therefore, at training time we used two separate English parsers: t</context>
<context position="26866" citStr="Burkett and Klein (2008)" startWordPosition="4519" endWordPosition="4522">lignment results. Our joint model has the highest reported Fl for English-Chinese word alignment. the baseline unsupervised HMM word aligner and to the English-Chinese ITG-based word aligner of Haghighi et al. (2009). The results are in Table 2. As can be seen, our model makes substantial improvements over the independent models. For parsing, we improve absolute F1 over the monolingual parsers by 2.1 in Chinese, and by 3.3 in English. For word alignment, we improve absolute F1 by 5.5 over the non-syntactic ITG word aligner. In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinese bilingual parser, despite ours using a much simpler set of synchronization features. 8.3 Machine Translation We further tested our alignments by using them to train the Joshua machine translation system (Li and Khudanpur, 2008). Table 3 describes the results of our experiments. For all of the systems, we tuned Rules Tune Test HMM 1.1M 29.0 29.4 ITG 1.5M 29.9 30.41 Joint 1.5M 29.6 30.6 Table 3: Tune and test BLEU results for machine translation systems built with different alignment tools. † indicates a statistically significant di</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1582" citStr="DeNero and Klein, 2007" startWordPosition="233" endWordPosition="236">and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic pars</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using syntax to improve word alignment for syntaxbased statistical machine translation.</title>
<date>2008</date>
<booktitle>In ACL MT Workshop.</booktitle>
<contexts>
<context position="1626" citStr="Fossum et al., 2008" startWordPosition="241" endWordPosition="244">he Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to each other in general. I</context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using syntax to improve word alignment for syntaxbased statistical machine translation. In ACL MT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="1281" citStr="Galley et al., 2004" startWordPosition="187" endWordPosition="190"> encouraged but not forced to synchronize with the parses. Our model gives absolute improvements of 3.3 Fl for English parsing, 2.1 Fl for Chinese parsing, and 5.5 Fl for word alignment over each task’s independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model th</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4757" citStr="Haghighi et al. (2009)" startWordPosition="733" endWordPosition="736">synchronization features couples the parses and alignments, but makes exact inference in the model intractable; we show how to use a variational mean field approximation, both for computing approximate feature expectations during training, and for performing approximate joint inference at test time. We train our joint model on the parallel, gold word-aligned portion of the Chinese treebank. When evaluated on parsing and word alignment, this model significantly improves over independentlytrained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al. (2009). It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. Finally, our model improves word alignment in the context of translation, leading to a 1.2 BLEU increase over using HMM word alignments. 2 Joint Parsing and Alignment Given a source-language sentence, s, and a targetlanguage sentence, s&apos;, we wish to predict a source tree t, a target tree t&apos;, and some kind of alignment a between them. These structures are illustrated in Figure 1. To facilitate these predictions, we define a conditional </context>
<context position="7898" citStr="Haghighi et al., 2009" startWordPosition="1245" endWordPosition="1248">f using SCFG rules to synchronously enforce the tree constraints on t and t&apos;, we only require that each of t and t&apos; be well-formed under separate monolingual CFGs. In order to permit efficient enumeration of all possible alignments a, we also restrict a to the set of unlabeled ITG bitrees (Wu, 1997), though again we do not require that a relate to t or t&apos; in any particular way. Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al., 2009).1 For 1See Section 8.1 for some new terminal productions required to make this true for the parallel Chinese treebank. 128 Features 0 (IP, s) � (b0, s, sÕ) 0 (NP, s) � (br, s, sÕ) 0 (VP, s) � (b2, s, sÕ) 0 (S, sÕ) 0 (NP, sÕ) 0 (AP, sÕ) 0 (VP, sÕ) �� (IP, b0) �� (b0, S) �� (br, NP) ��� (IP, b0, S) (b) Asynchronous Rule Features 0( (IP, b0, S), s, s’ ) 0( (NP, b1, NP), s, s’ ) 0( (VP, b2, VP), s, s’ ) S NP VP b0 b1 b2 NP IP VP (a) Synchronous Rule S NP IP VP Parsing NP AP VP b0 br b2 Alignment Synchronization Figure 1: Source trees, t (right), alignments, a (grid), and target trees, t&apos; (top), a</context>
<context position="12713" citStr="Haghighi et al. (2009)" startWordPosition="2096" endWordPosition="2099"> scores under the parser of Petrov and Klein (2007). While that parser uses heavily refined PCFGs with rule probabilities defined at the refined symbol level, we interact with its posterior distribution via posterior marginal probabilities over unrefined symbols. In particular, to each unrefined anchored production iAj —* iBkCj, we associate a single feature whose value is the marginal quantity log P(iBkCj|iAj, s) under the monolingual parser. These scores are the same as the variational rule scores of Matsuzaki et al. (2005).4 4.2 Alignment We begin with the same set of alignment features as Haghighi et al. (2009), which are defined only for terminal bispans. In addition, we include features on nonterminal bispans, including a bias feature, features that measure the difference in size between the source and target spans, features that measure the difference in relative sentence position between the source and target spans, and features that measure the density of word-to-word alignment posteriors under a separate unsupervised word alignment model. 4Of course the structure of our model permits any of the additional rule-factored monolingual parsing features that have been described in the parsing litera</context>
<context position="14818" citStr="Haghighi et al., 2009" startWordPosition="2428" endWordPosition="2431">s we shall see, however, our model gives significant improvements with these simple features alone. 5 Learning We learn the parameters of our model on the parallel portion of the Chinese treebank. Although our model assigns probabilities to entire synchronous derivations of sentences, the parallel Chinese treebank gives alignments only at the word level (1 by 1 bispans in Figure 2). This means that our alignment variable a is not fully observed. Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A(w) that are consistent with w (Haghighi et al., 2009).5 G(θ)=log � P(ti, a, t&apos;i|si, s&apos;i) aEA(wi) We maximize this objective using standard gradient methods (Nocedal and Wright, 1999). As with fully visible log-linear models, the gradient for the ith sentence pair with respect to θ is a difference of feature expectations: VG(θ) =E (� �) [φ(t a t&apos; si,&apos;)] P a ti,wi,ti,si,si 27 , i,2 i [φ(t, a, t&apos;, si, s&apos; i)] (4) − EP(t,a,t�Jsi,s� i) 5We also learn from non-ITG alignments by maximizing the marginal probability of the set of minimum-recall error alignments in the same way as Haghighi et al. (2009) 130 Sample Synchronization Features S VP VP VBN PP ��</context>
<context position="18963" citStr="Haghighi et al. (2009)" startWordPosition="3146" endWordPosition="3149">tainty are the expected counts of particular source nodes, target nodes, and bispans under the mean field distribution: /- fin=1: qp(t)I(n E t) t 1: An0 = qp(t&apos;)I(n&apos; E t&apos;) t0 1: Ab = a Since dynamic programs exist for summing over each of the individual factors, these expectations can be computed in polynomial time. 6.1 Pruning Although we can approximate the expectations from (4) in polynomial time using our mean field distribution, in practice we must still prune the ITG forests and monolingual parse forests to allow tractable inference. We prune our ITG forests using the same basic idea as Haghighi et al. (2009), but we employ a technique that allows us to be more aggressive. Where Haghighi et al. (2009) pruned bispans based on how many unsupervised HMM alignments were violated, we first train a maximum-matching word aligner (Taskar et al., 2005) using our supervised data set, which has only half the precision errors of the unsupervised HMM. We then prune every bispan which violates at least three alignments from the maximum-matching aligner. When compared to pruning the bitext forest of our model with Haghighi et al. (2009)’s HMM technique, this new technique allows us to maintain the same level of </context>
<context position="23568" citStr="Haghighi et al. (2009)" startWordPosition="3955" endWordPosition="3958">raining, we only used training sentences of length G 50 words, which left us with 1974 of 2261 sentences. We measured the results in two ways. First, we directly measured F1 for English parsing, Chinese parsing, and word alignment on a held out section of the hand annotated corpus used to train the model. Next, we further evaluated the quality of the word alignments produced by our model by using them as input for a machine translation system. 8.1 Dataset-specific ITG Terminals The Chinese treebank gold word alignments include significantly more many-to-many word alignments than those used by Haghighi et al. (2009). We are able to produce some of these many-to-many alignments by including new many-to-many terminals in (b) 2x3 (c) Gapped 2x3 Figure 4: Examples of phrasal alignments that can be represented by our new ITG terminal bispans. our ITG word aligner, as shown in Figure 4. Our terminal productions sometimes capture non-literal translation like both sides or in recent years. They also can allow us to capture particular, systematic changes in the annotation standard. For example, the gapped pattern from Figure 4 captures the standard that English word the is always aligned to the Chinese head noun </context>
<context position="26458" citStr="Haghighi et al. (2009)" startWordPosition="4447" endWordPosition="4450">sa for the remaining sentences. 全 国 国 年 近年 近年 来 来 � 岸 (a) 2x2 133 Test Results Ch F1 Eng F1 Tot F1 Monolingual 83.6 81.2 82.5 Reranker 86.0 83.8 84.9 Joint 85.7 84.5 85.1 Table 1: Parsing results. Our joint model has the highest reported Fl for English-Chinese bilingual parsing. Test Results Precision Recall AER F1 HMM 86.0 58.4 30.0 69.5 ITG 86.8 73.4 20.2 79.5 Joint 85.5 84.6 14.9 85.0 Table 2: Word alignment results. Our joint model has the highest reported Fl for English-Chinese word alignment. the baseline unsupervised HMM word aligner and to the English-Chinese ITG-based word aligner of Haghighi et al. (2009). The results are in Table 2. As can be seen, our model makes substantial improvements over the independent models. For parsing, we improve absolute F1 over the monolingual parsers by 2.1 in Chinese, and by 3.3 in English. For word alignment, we improve absolute F1 by 5.5 over the non-syntactic ITG word aligner. In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinese bilingual parser, despite ours using a much simpler set of synchronization features. 8.3 Machine Translation We further tested o</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="20316" citStr="Huang, 2008" startWordPosition="3380" endWordPosition="3381">ing the monolingual parsing model scores. For each unrefined anchored production iAj —* iBkCj, we compute the marginal probability P(iAj,i Bk,k Cj|s) under the monolingual parser (these are equivalent to the maxrule scores from Petrov and Klein 2007). We only include productions where this probability is greater than 10−20. Note that at training time, we are not guaranteed that the gold trees will be included in the pruned forest. Because of this, we replace the gold trees ti, t&apos;i with oracle trees from the pruned forest, which can be found efficiently using a variant of the inside algorithm (Huang, 2008). 7 Testing Once the model has been trained, we still need to determine how to use it to predict parses and word alignments for our test sentence pairs. Ideally, given the sentence pair (s, s&apos;), we would find: (t*, w*, t&apos;*) = argmax P(t, w, t&apos;|s, s&apos;) t,w,t0 1: =argmax t,w,t0 aEA(w) P(t, a, t&apos;|s, s&apos;) Of course, this is also intractable, so we once again resort to our mean field approximation. This yields the approximate solution: 1: (t*, w*, t&apos;*) = argmax t,w,t0 aEA(w) However, recall that Q incorporates the model’s mutual constraint into the variational parameters, which Input: sentence pair (</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In ACL SSST.</booktitle>
<contexts>
<context position="27157" citStr="Li and Khudanpur, 2008" startWordPosition="4560" endWordPosition="4563">provements over the independent models. For parsing, we improve absolute F1 over the monolingual parsers by 2.1 in Chinese, and by 3.3 in English. For word alignment, we improve absolute F1 by 5.5 over the non-syntactic ITG word aligner. In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinese bilingual parser, despite ours using a much simpler set of synchronization features. 8.3 Machine Translation We further tested our alignments by using them to train the Joshua machine translation system (Li and Khudanpur, 2008). Table 3 describes the results of our experiments. For all of the systems, we tuned Rules Tune Test HMM 1.1M 29.0 29.4 ITG 1.5M 29.9 30.41 Joint 1.5M 29.6 30.6 Table 3: Tune and test BLEU results for machine translation systems built with different alignment tools. † indicates a statistically significant difference between a system’s test performance and the one above it. on 1000 sentences of the NIST 2004 and 2005 machine translation evaluations, and tested on 400 sentences of the NIST 2006 MT evaluation. Our training set consisted of 250k sentences of newswire distributed with the GALE proj</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In ACL SSST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="24640" citStr="Liang et al., 2006" startWordPosition="4134" endWordPosition="4137">ion standard. For example, the gapped pattern from Figure 4 captures the standard that English word the is always aligned to the Chinese head noun in a noun phrase. We featurize these non-terminals with features similar to those of Haghighi et al. (2009), and all of the alignment results we report in Section 8.2 (both joint and ITG) employ these features. 8.2 Parsing and Word Alignment To compute features that depend on external models, we needed to train an unsupervised word aligner and monolingual English and Chinese parsers. The unsupervised word aligner was a pair of jointly trained HMMs (Liang et al., 2006), trained on the FBIS corpus. We used the Berkeley Parser (Petrov and Klein, 2007) for both monolingual parsers, with the Chinese parser trained on the full Chinese treebank, and the English parser trained on a concatenation of the Penn WSJ corpus (Marcus et al., 1993) and the English side of train.6 We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. The results are in Table 1. For word alignment, we compare to 6To avoid overlap in the data used to train the monolingual parsers</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="24909" citStr="Marcus et al., 1993" startWordPosition="4181" endWordPosition="4184">he alignment results we report in Section 8.2 (both joint and ITG) employ these features. 8.2 Parsing and Word Alignment To compute features that depend on external models, we needed to train an unsupervised word aligner and monolingual English and Chinese parsers. The unsupervised word aligner was a pair of jointly trained HMMs (Liang et al., 2006), trained on the FBIS corpus. We used the Berkeley Parser (Petrov and Klein, 2007) for both monolingual parsers, with the Chinese parser trained on the full Chinese treebank, and the English parser trained on a concatenation of the Penn WSJ corpus (Marcus et al., 1993) and the English side of train.6 We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. The results are in Table 1. For word alignment, we compare to 6To avoid overlap in the data used to train the monolingual parsers and the joint model, at training time, we used a separate version of the Chinese parser, trained only on articles 400-1151 (omitting articles in train). For English parsing, we deemed it insufficient to entirely omit the Chinese treebank data from the monolingual pars</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuki Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="12622" citStr="Matsuzaki et al. (2005)" startWordPosition="2080" endWordPosition="2083">e in turn here. 4.1 Parsing The monolingual parsing features we use are simply parsing model scores under the parser of Petrov and Klein (2007). While that parser uses heavily refined PCFGs with rule probabilities defined at the refined symbol level, we interact with its posterior distribution via posterior marginal probabilities over unrefined symbols. In particular, to each unrefined anchored production iAj —* iBkCj, we associate a single feature whose value is the marginal quantity log P(iBkCj|iAj, s) under the monolingual parser. These scores are the same as the variational rule scores of Matsuzaki et al. (2005).4 4.2 Alignment We begin with the same set of alignment features as Haghighi et al. (2009), which are defined only for terminal bispans. In addition, we include features on nonterminal bispans, including a bias feature, features that measure the difference in size between the source and target spans, features that measure the difference in relative sentence position between the source and target spans, and features that measure the density of word-to-word alignment posteriors under a separate unsupervised word alignment model. 4Of course the structure of our model permits any of the additiona</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuki Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon May</author>
<author>Kevin Knight</author>
</authors>
<title>Syntactic realignment models for machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1604" citStr="May and Knight, 2007" startWordPosition="237" endWordPosition="240"> parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to ea</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>Jon May and Kevin Knight. 2007. Syntactic realignment models for machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen J Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>1999</date>
<publisher>Springer.</publisher>
<contexts>
<context position="14947" citStr="Nocedal and Wright, 1999" startWordPosition="2447" endWordPosition="2450">rameters of our model on the parallel portion of the Chinese treebank. Although our model assigns probabilities to entire synchronous derivations of sentences, the parallel Chinese treebank gives alignments only at the word level (1 by 1 bispans in Figure 2). This means that our alignment variable a is not fully observed. Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A(w) that are consistent with w (Haghighi et al., 2009).5 G(θ)=log � P(ti, a, t&apos;i|si, s&apos;i) aEA(wi) We maximize this objective using standard gradient methods (Nocedal and Wright, 1999). As with fully visible log-linear models, the gradient for the ith sentence pair with respect to θ is a difference of feature expectations: VG(θ) =E (� �) [φ(t a t&apos; si,&apos;)] P a ti,wi,ti,si,si 27 , i,2 i [φ(t, a, t&apos;, si, s&apos; i)] (4) − EP(t,a,t�Jsi,s� i) 5We also learn from non-ITG alignments by maximizing the marginal probability of the set of minimum-recall error alignments in the same way as Haghighi et al. (2009) 130 Sample Synchronization Features S VP VP VBN PP ���( e b8e ) = COARSESOURCETARGET asa , asa 1 FINESOURCETARGET , 1 VBD were established in such places as Quanzhou Zhangzhou etc. 0</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Jorge Nocedal and Stephen J. Wright. 1999. Numerical Optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="4695" citStr="Petrov and Klein (2007)" startWordPosition="723" endWordPosition="726">nfluence between the three component grammars. The presence of synchronization features couples the parses and alignments, but makes exact inference in the model intractable; we show how to use a variational mean field approximation, both for computing approximate feature expectations during training, and for performing approximate joint inference at test time. We train our joint model on the parallel, gold word-aligned portion of the Chinese treebank. When evaluated on parsing and word alignment, this model significantly improves over independentlytrained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al. (2009). It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. Finally, our model improves word alignment in the context of translation, leading to a 1.2 BLEU increase over using HMM word alignments. 2 Joint Parsing and Alignment Given a source-language sentence, s, and a targetlanguage sentence, s&apos;, we wish to predict a source tree t, a target tree t&apos;, and some kind of alignment a between them. These structures are illustrated in Figur</context>
<context position="12142" citStr="Petrov and Klein (2007)" startWordPosition="2007" endWordPosition="2010"> any known efficient dynamic programming for the exact computation of expectations. We will therefore turn to a variational inference method in Section 6. 4 Features With the model’s locality structure defined, we just need to specify the actual feature function, φ. We divide the features into three types: parsing features (φF(n, s) and φ,,(n&apos;, s&apos;)), alignment features (φA(b, s, s&apos;)) and synchronization features (φp(n, b), φa(b, n&apos;), and φ./(n, b, n&apos;)). We detail each of these in turn here. 4.1 Parsing The monolingual parsing features we use are simply parsing model scores under the parser of Petrov and Klein (2007). While that parser uses heavily refined PCFGs with rule probabilities defined at the refined symbol level, we interact with its posterior distribution via posterior marginal probabilities over unrefined symbols. In particular, to each unrefined anchored production iAj —* iBkCj, we associate a single feature whose value is the marginal quantity log P(iBkCj|iAj, s) under the monolingual parser. These scores are the same as the variational rule scores of Matsuzaki et al. (2005).4 4.2 Alignment We begin with the same set of alignment features as Haghighi et al. (2009), which are defined only for </context>
<context position="19954" citStr="Petrov and Klein 2007" startWordPosition="3315" endWordPosition="3318"> violates at least three alignments from the maximum-matching aligner. When compared to pruning the bitext forest of our model with Haghighi et al. (2009)’s HMM technique, this new technique allows us to maintain the same level of accuracy while cutting the number of bispans in half. In addition to pruning the bitext forests, we also prune the syntactic parse forests using the monolingual parsing model scores. For each unrefined anchored production iAj —* iBkCj, we compute the marginal probability P(iAj,i Bk,k Cj|s) under the monolingual parser (these are equivalent to the maxrule scores from Petrov and Klein 2007). We only include productions where this probability is greater than 10−20. Note that at training time, we are not guaranteed that the gold trees will be included in the pruned forest. Because of this, we replace the gold trees ti, t&apos;i with oracle trees from the pruned forest, which can be found efficiently using a variant of the inside algorithm (Huang, 2008). 7 Testing Once the model has been trained, we still need to determine how to use it to predict parses and word alignments for our test sentence pairs. Ideally, given the sentence pair (s, s&apos;), we would find: (t*, w*, t&apos;*) = argmax P(t, </context>
<context position="24722" citStr="Petrov and Klein, 2007" startWordPosition="4149" endWordPosition="4152">ard that English word the is always aligned to the Chinese head noun in a noun phrase. We featurize these non-terminals with features similar to those of Haghighi et al. (2009), and all of the alignment results we report in Section 8.2 (both joint and ITG) employ these features. 8.2 Parsing and Word Alignment To compute features that depend on external models, we needed to train an unsupervised word aligner and monolingual English and Chinese parsers. The unsupervised word aligner was a pair of jointly trained HMMs (Liang et al., 2006), trained on the FBIS corpus. We used the Berkeley Parser (Petrov and Klein, 2007) for both monolingual parsers, with the Chinese parser trained on the full Chinese treebank, and the English parser trained on a concatenation of the Penn WSJ corpus (Marcus et al., 1993) and the English side of train.6 We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. The results are in Table 1. For word alignment, we compare to 6To avoid overlap in the data used to train the monolingual parsers and the joint model, at training time, we used a separate version of the Chinese </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Methods for MT and Summarization, ACL.</booktitle>
<contexts>
<context position="28224" citStr="Riezler and Maxwell (2005)" startWordPosition="4746" endWordPosition="4749">valuations, and tested on 400 sentences of the NIST 2006 MT evaluation. Our training set consisted of 250k sentences of newswire distributed with the GALE project, all of which were sub-sampled to have high Ngram overlap with the tune and test sets. All of our sentences were of length at most 40 words. When building the translation grammars, we used Joshua’s default “tight” phrase extraction option. We ran MERT for 4 iterations, optimizing 20 weight vectors per iteration on a 200-best list. Table 3 gives the results. On the test set, we also ran the approximate randomization test suggested by Riezler and Maxwell (2005). We found that our joint parsing and alignment system significantly outperformed the HMM aligner, but the improvement over the ITG aligner was not statistically significant. 9 Conclusion The quality of statistical machine translation models depends crucially on the quality of word alignments and syntactic parses for the bilingual training corpus. Our work presented the first joint model for parsing and alignment, demonstrating that we can improve results on both of these tasks, as well as on downstream machine translation, by allowing parsers and word aligners to simultaneously inform one ano</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Workshop on Intrinsic and Extrinsic Evaluation Methods for MT and Summarization, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Saul</author>
<author>Michael Jordan</author>
</authors>
<title>Exploiting tractable substructures in intractable networks.</title>
<date>1996</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="17505" citStr="Saul and Jordan, 1996" startWordPosition="2905" endWordPosition="2908"> set to best approximate our weakly synchronized model from (3): ~ ~ O* = argmin KL Q�||PB(t, a, t&apos;|s, s&apos;) � Once we have found Q, we compute an approximate gradient by replacing the model expectations with expectations under Q: i)� EQ(a|wi) ��(ti, a, t&apos; i, si, s&apos; ��(t, a, t&apos;, si, s&apos; i)� − EQ(t,a,t0|si,s0 i) Now, we will briefly describe how we compute Q. First, note that the parameters O of Q factor along individual source nodes, target nodes, and bispans. The combination of the KL objective and our particular factored form of Q make our inference procedure a structured mean field algorithm (Saul and Jordan, 1996). Structured mean field techniques are well-studied in graphical models, and our adaptation in this section to multiple grammars follows standard techniques (see e.g. Wainwright and Jordan, 2008). Rather than derive the mean field updates forO, we describe the algorithm (shown in Figure 3) procedurally. Similar to block Gibbs sampling, we iteratively optimize each component (source parse, target parse, and alignment) of the model in turn, conditioned on the others. Where block Gibbs sampling conditions on fixed trees or ITG derivations, our mean field algorithm maintains uncertainty in 131 Fig</context>
</contexts>
<marker>Saul, Jordan, 1996</marker>
<rawString>Lawrence Saul and Michael Jordan. 1996. Exploiting tractable substructures in intractable networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Synchronous tree-adjoining grammars.</title>
<date>1990</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5837" citStr="Shieber and Schabes, 1990" startWordPosition="914" endWordPosition="917">and some kind of alignment a between them. These structures are illustrated in Figure 1. To facilitate these predictions, we define a conditional distribution P(t, a, t&apos;|s, s&apos;). We begin with a generic conditional exponential form: P(t, a, t&apos;|s, s&apos;) a exp BTO(t, a, t&apos;, s, s&apos;) (1) Unfortunately, a generic model of this form is intractable, because we cannot efficiently sum over all triples (t, a, t&apos;) without some assumptions about how the features 0(t, a, t&apos;, s, s&apos;) decompose. One natural solution is to restrict our candidate triples to those given by a synchronous context free grammar (SCFG) (Shieber and Schabes, 1990). Figure 1(a) gives a simple example of generation from a log-linearly parameterized synchronous grammar, together with its features. With the SCFG restriction, we can sum over the necessary structures using the 0(n6) bitext inside-outside algorithm, making P(t, a, t&apos;|s, s&apos;) relatively efficient to compute expectations under. Unfortunately, an SCFG requires that all the constituents of each tree, from the root down to the words, are generated perfectly in tandem. The resulting inability to model any level of syntactic divergence prevents accurate modeling of the individual monolingual trees. W</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Stuart M. Shieber and Yves Schabes. 1990. Synchronous tree-adjoining grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="3258" citStr="Smith and Eisner (2006" startWordPosition="504" endWordPosition="508">Indeed, it is sometimes the case that large pieces of a sentence pair are completely asynchronous and can only be explained monolingually. Our model exploits synchronization where possible to perform more accurately on both word alignment and parsing, but also allows independent models to dictate pieces of parse trees and word alignments when synchronization is impossible. This notion of “weak synchronization” is parameterized and estimated from data to maximize the likelihood of the correct parses and word alignments. Weak synchronization is closely related to the quasi-synchronous models of Smith and Eisner (2006; 2009) and the bilingual parse reranking model of Burkett and Klein (2008), but those models assume that the word alignment of a sentence pair is known and fixed. To simultaneously model both parses and align127 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 127–135, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics ments, our model loosely couples three separate combinatorial structures: monolingual trees in the source and target languages, and a synchronous ITG alignment that links the two languages </context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<marker>Smith, Eisner, 2009</marker>
<rawString>David A. Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: using English to parse Korean.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1700" citStr="Smith and Smith, 2004" startWordPosition="255" endWordPosition="258">m MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to each other in general. In the ideal situation, each pair of sentences in a bilingual corpus could </context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: using English to parse Korean. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1747" citStr="Snyder et al., 2009" startWordPosition="263" endWordPosition="266">roduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit syntactic information. This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously. While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to each other in general. In the ideal situation, each pair of sentences in a bilingual corpus could be syntactically parsed using a synchronous con</context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="19202" citStr="Taskar et al., 2005" startWordPosition="3187" endWordPosition="3190">e individual factors, these expectations can be computed in polynomial time. 6.1 Pruning Although we can approximate the expectations from (4) in polynomial time using our mean field distribution, in practice we must still prune the ITG forests and monolingual parse forests to allow tractable inference. We prune our ITG forests using the same basic idea as Haghighi et al. (2009), but we employ a technique that allows us to be more aggressive. Where Haghighi et al. (2009) pruned bispans based on how many unsupervised HMM alignments were violated, we first train a maximum-matching word aligner (Taskar et al., 2005) using our supervised data set, which has only half the precision errors of the unsupervised HMM. We then prune every bispan which violates at least three alignments from the maximum-matching aligner. When compared to pruning the bitext forest of our model with Haghighi et al. (2009)’s HMM technique, this new technique allows us to maintain the same level of accuracy while cutting the number of bispans in half. In addition to pruning the bitext forests, we also prune the syntactic parse forests using the monolingual parsing model scores. For each unrefined anchored production iAj —* iBkCj, we </context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Wainwright</author>
<author>Michael I Jordan</author>
</authors>
<title>Graphical Models, Exponential Families, and Variational Inference.</title>
<date>2008</date>
<publisher>Now Publishers Inc.,</publisher>
<location>Hanover, MA, USA.</location>
<contexts>
<context position="17700" citStr="Wainwright and Jordan, 2008" startWordPosition="2933" endWordPosition="2936">expectations with expectations under Q: i)� EQ(a|wi) ��(ti, a, t&apos; i, si, s&apos; ��(t, a, t&apos;, si, s&apos; i)� − EQ(t,a,t0|si,s0 i) Now, we will briefly describe how we compute Q. First, note that the parameters O of Q factor along individual source nodes, target nodes, and bispans. The combination of the KL objective and our particular factored form of Q make our inference procedure a structured mean field algorithm (Saul and Jordan, 1996). Structured mean field techniques are well-studied in graphical models, and our adaptation in this section to multiple grammars follows standard techniques (see e.g. Wainwright and Jordan, 2008). Rather than derive the mean field updates forO, we describe the algorithm (shown in Figure 3) procedurally. Similar to block Gibbs sampling, we iteratively optimize each component (source parse, target parse, and alignment) of the model in turn, conditioned on the others. Where block Gibbs sampling conditions on fixed trees or ITG derivations, our mean field algorithm maintains uncertainty in 131 Figure 3: Structured mean field inference for the weakly synchronized model. I(n E t) is an indicator value for the presence of node n in source tree t. the form of monolingual parse forests or ITG </context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>Martin J Wainwright and Michael I Jordan. 2008. Graphical Models, Exponential Families, and Variational Inference. Now Publishers Inc., Hanover, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="7576" citStr="Wu, 1997" startWordPosition="1197" endWordPosition="1198"> and the alignment as separate objects that can vary arbitrarily. However, the model rewards synchronization appropriately when the alignment brings the trees into correspondence. 3 Weakly Synchronized Grammars We propose a joint model which still gives probabilities on triples (t, a, t&apos;). However, instead of using SCFG rules to synchronously enforce the tree constraints on t and t&apos;, we only require that each of t and t&apos; be well-formed under separate monolingual CFGs. In order to permit efficient enumeration of all possible alignments a, we also restrict a to the set of unlabeled ITG bitrees (Wu, 1997), though again we do not require that a relate to t or t&apos; in any particular way. Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al., 2009).1 For 1See Section 8.1 for some new terminal productions required to make this true for the parallel Chinese treebank. 128 Features 0 (IP, s) � (b0, s, sÕ) 0 (NP, s) � (br, s, sÕ) 0 (VP, s) � (b2, s, sÕ) 0 (S, sÕ) 0 (NP, sÕ) 0 (AP, sÕ) 0 (VP, sÕ) �� (IP, b0) �� (b0, S) �� (br,</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>The CMU-AKA syntax augmented machine translation system for IWSLT-06.</title>
<date>2006</date>
<booktitle>In IWSLT.</booktitle>
<contexts>
<context position="1305" citStr="Zollmann et al., 2006" startWordPosition="191" endWordPosition="194">orced to synchronize with the parses. Our model gives absolute improvements of 3.3 Fl for English parsing, 2.1 Fl for Chinese parsing, and 5.5 Fl for word alignment over each task’s independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chinese treebank. We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments. 1 Introduction Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006). Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible. Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information (DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008) or changing parsers to make use of word alignments (Smith and Smith, 2004; Burkett and Klein, 2008; Snyder et al., 2009). In the first case, however, parsers do not exploit bilingual information. In the second, word alignment is performed with a model that does not exploit synt</context>
</contexts>
<marker>Zollmann, Venugopal, Vogel, Waibel, 2006</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, Stephan Vogel, and Alex Waibel. 2006. The CMU-AKA syntax augmented machine translation system for IWSLT-06. In IWSLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>