<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.932743">
Stacking Dependency Parsers
</title>
<author confidence="0.993685">
Andr´e F. T. Martins*† Dipanjan Das* Noah A. Smith* Eric P. Xing*
</author>
<affiliation confidence="0.9445755">
*School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, Lisboa, Portugal
</affiliation>
<email confidence="0.997658">
{afm,dipanjan,nasmith,epxing}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995006" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999348">
We explore a stacked framework for learn-
ing to predict dependency structures for natu-
ral language sentences. A typical approach in
graph-based dependency parsing has been to
assume a factorized model, where local fea-
tures are used but a global function is opti-
mized (McDonald et al., 2005b). Recently
Nivre and McDonald (2008) used the output
of one dependency parser to provide features
for another. We show that this is an example
of stacked learning, in which a second pre-
dictor is trained to improve the performance
of the first. Further, we argue that this tech-
nique is a novel way of approximating rich
non-local features in the second parser, with-
out sacrificing efficient, model-optimal pre-
diction. Experiments on twelve languages
show that stacking transition-based and graph-
based parsers improves performance over ex-
isting state-of-the-art dependency parsers.
</bodyText>
<sectionHeader confidence="0.998128" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.947754069767442">
In this paper we address a representation-efficiency
tradeoff in statistical natural language processing
through the use of stacked learning (Wolpert,
1992). This tradeoff is exemplified in dependency
parsing, illustrated in Fig. 1, on which we focus in
this paper:
• Exact algorithms for dependency parsing (Eis-
ner and Satta, 1999; McDonald et al., 2005b)
are tractable only when the model makes very
strong, linguistically unsupportable independence
assumptions, such as “arc factorization” for non-
projective dependency parsing (McDonald and
Satta, 2007).
• Feature-rich parsers must resort to search or
greediness, (Ratnaparkhi et al., 1994; Sagae and
Lavie, 2005; Hall et al., 2006), so that parsing
solutions are inexact and learned models may be
subject to certain kinds of bias (Lafferty et al.,
2001).
A solution that leverages the complementary
strengths of these two approaches—described in de-
tail by McDonald and Nivre (2007)—was recently
and successfully explored by Nivre and McDonald
(2008). Our contribution begins by reinterpreting
and generalizing their parser combination scheme as
a stacking of parsers.
We give a new theoretical motivation for stacking
parsers, in terms of extending a parsing model’s fea-
ture space. Specifically, we view stacked learning as
a way of approximating non-local features in a lin-
ear model, rather than making empirically dubious
independence (McDonald et al., 2005b) or structural
assumptions (e.g., projectivity, Eisner, 1996), using
search approximations (Sagae and Lavie, 2005; Hall
et al., 2006; McDonald and Pereira, 2006), solving a
(generally NP-hard) integer linear program (Riedel
and Clarke, 2006), or adding latent variables (Titov
and Henderson, 2007). Notably, we introduce the
use of very rich non-local approximate features in
one parser, through the output of another parser.
Related approaches are the belief propagation algo-
rithm of Smith and Eisner (2008), and the “trading
of structure for features” explored by Liang et al.
</bodyText>
<note confidence="0.831977666666667">
157
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9999735">
Figure 1: A projective dependency arse (top), and a non-
Figure 2: Nonprojective dependency graph
</figureCaption>
<bodyText confidence="0.868534">
projective dependency parse (bottom) for two English
those that assume each dependency decsion is in
sentences; examples from McDonald and Satta (2007).
ddt modl th lbl ttl ntint
</bodyText>
<figure confidence="0.46658025">
p
(2008).
els a
tht d b u d
</figure>
<note confidence="0.814429125">
Teis paper focuses on dependencydparsing,ewhich
hasobecomerwidelyaused in0relation extraction (Cu-
lotta and Sorensen, 2004), machine translation
205a). Edgefactod mode have man computa
of the graph (Paskn 200 McDonld et al
(Ding and Palmer, 2005), question answering (Wang
tional benefits, most nobly that inference fo non
2005a) Edgefactored models have many computa
</note>
<bodyText confidence="0.905565238095238">
et al., 2007), and many other NLP applications.
prctive dpdy gph an be ied
tionl benefis most notaby tha infence for non
polymil (MDold t 5b) he p
We show that stackig methods outprform the ap-
projective dependency graphs can be achieved in
pbl i teatig h dped i
proximate “second-order” parser of McDonald and
polynomal time (McDoald et al 2005b) The pri-
dd i tht it i ot lii ti
Pereia (2006) on twelve languages a can be used
mary problem in treating each dependency as in-
Nonlocal information such as arty (or valency)
within that approximato to chieve even better re-
depedent is that it is not a realistic assumption
and neighbouring dependencies can be crucial o
sults. These results re similar in prit t (Nivre and
Non-local informaton, such as arity (or valency)
obaining high parsing accuracies (Klein and Man-
McDoald, 008), but with the following novel con-
and neighbouring dependencis, cn be crucial to
</bodyText>
<note confidence="0.679834">
ning 2002; McDonald and Pereira 2006) How-
tribuios:
</note>
<bodyText confidence="0.409901666666667">
obtain
ever, in
y y
</bodyText>
<listItem confidence="0.6805655">
• a stacking interpretation,
eer i th data-drien
resentations over the input
evioly ben known t have e
</listItem>
<subsectionHeader confidence="0.888066">
2.1 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.916115642857143">
arng via the EM algorithm –
reviously been known to have exact non-projective
Dependency syntax is a lightweight syntactic rep-
plementations
mplmtations.
resentation that models a sentence as a graph where
We then switch focus to models that account for
the words are vertices and syntactic relationships are
n We the switch focus to models that account for
nn, pir ity d negh
onlocal informaton, i partculararity and neigh
urig par disi F ystm th mdl
directed edges (arcs) connecting heads to their argu-
ments and modifiers.
</bodyText>
<equation confidence="0.618721">
ug ps dcisio
tit w gi rdti fom th Hilt
gh bl
ont rcn f h m
g p
a gaph pe suggstg tht psing pob
</equation>
<bodyText confidence="0.972604535714286">
isintractable n this cse For eghbouing
Dependency parsing is often viewed computa-
tionally as a structure prdiction problem: for each
rse deciions we extend the work of McDonald
m i inbl i hi c F nighburing
input sentence x, wit � words, exponentally many
d Pereira (2006) and show that modeling vertica
se dciin e d h rk of MDld
candidate ependncy trees � � �(x) are possible in
ighbourhoods makes parsing inractabe in addi-
d Pi (2006) d sh tha odelin ertil
on to modlng horizntal neighbourhoods. A cn-
eihbuhd k
principle. We denote each tree by its set of vertices
ing itctbl i ddi
on to modelingand directed arcs,uys= (Uy, Ay). Aelegal depen-
dencyntreejhasvn + 1evertices,aeach corresponding to
r any moel asumptions weaker ta those made
xact nonprojectiv depedency pasing is tractable
one word plus a “wall” symbol, $, assumed to be the
the edgefacored models.
or any model assumptions weaker than those made
hidden root of the sentence. In a valid dependency
y tree, each vertexdexcept the root has exactly one par-
ctive case, arcs cannot cross when
depictedeonWone side of the sentence; in the non-
projective case, thisvconstraint isanotrimposed (see
endency
</bodyText>
<equation confidence="0.528637666666667">
Fig. 1).
03; Niv
; ,
2.1.1 Graph-based vs.atransition-based models MDld t l
l N D
5a)and non-projectve parsing systems (Nive
Most recent work on dependency parsing can be
t l 2005b) Th oh an oft be
ndNilsson 2005; Hll and N´ov´ak 2005; McDon-
</equation>
<bodyText confidence="0.95884893939394">
assified into two broad ategories In the first cat
categorized as graph-based or transition-based. In
ory ar those methods ht employ approximate
graph-based parsing, deendency trees are scored
d et al 2005b) Thes approaches an ofen be
fernce typically through he use of linear ime
by factoring the tree into its arcs, and parsing is
assfied into two boad categrie In te firt cat-
gory ae those mthods at employ pproximate
ift-educ parsing algorithms Yamada and Mat-
performed by searching for the highest scoring tree
moto, 2003; Nivre and Scholz, 2004; Nivr and
(Eiser 1996; McDnald et al., 2005b). Transition-
ference, typicaly through he use of linea time
hift-ruce pag algorithms (Yamada an Mat-
based parsers model the sequnce of decisions of
lsson, 2005). n the second category are those
umoto, 2003; Nivre and Scholz, 2004; Nivre and
t employ exhaustiv inference algorithms, uu-
a shift-reduce parser, given previous decisions and
current state, ad paring is perfed by greedily
ilsson, 2005). I he second ategory ar thoe
y by making song independence assumptions, as
coosing the highest scoig transition out of each
the case for edge-factoed modls (Paskin, 200;
at employ exhaustive nference algorithms, uu-
cDonald etal., 2005a; McDonald et al., 2005b).
ly by making stro ndependence assumptions as
successive parsing state or by searching for the best
ecently thee havealso be prposals for xhau
sequence of transitions (Ratnaparkhi et al., 1994;
the case for edge-factore models (Paskn, 200
Yamada and Matsumoto, 2003; Nivre et al., 2004;
</bodyText>
<equation confidence="0.438006">
e mo wea
cDonald et a, 2005a; McDonald et al., 2005)
</equation>
<bodyText confidence="0.9760122">
e dgefaore up
on ldng bh pproximae thds (McD
ecently there have also been proposas for ex
Sagae and Lavie, 2005; Hall et al., 2006).
d nd Pei 2006) d x
methods that weaken the edgefactored assump
hds hgh
Both approaches most commonly use linear
els to assign scores to arcseor decisions, so
score is a dot-product of a feature vector f
learned weight vector w.
rabd gor (wa6).
sysems notable exceptions include te work
In sum, these two lines of research use different
gama bed odel he has be lim
approximations to achieve tracabiliy. Transition-
Wang and Harper (2004). Theoretical studes of
ork n piial ym f pjctive ps
based approaches solve equence of local prob-
te include the work of Neuhaus and Boker (1997)
g ytms tbl epi incld th wok
owing that th rcogniton probem for a in-
lems in sequence, sacrificing global optimality guar-
fWdHr(2004)Theotilstdiof
antees and possibly expressive power (Abney et al.,
t ild th k of Neh nd B¨ker (1997
howing at the recogntion probem for a mini
1999). Graph-based methods perform global in-
ference using score factorizations that correspond
to strong independence assumptions (discussed in
• a ricr fetu set that includes non-lcl fatures
partially adverted by incorporating rich feature p-
The goal of this work is to further our current
shown here to improve erformnc), and
esentaons over the input (McDonald e al., 20
nderstanding of the computational natue of
pjec pag g
• a variety of stackng archtectures.
The goal of this work is to furt
m
if hi h ddi i
Usg ack with rich featurs, we obtai resuts
projective parsing algorithms for both learning and
investigating and extendng the edge-factored model
comptitiv ith Nivre an McDonald (2008) while
inference within the data-driven setting We start by
of McDonald et al. (2005b). In particular, we ap-
preserving the fast quaratic parsing im of arc-
invesigaing and extending the edge-factored model
peal to the Matrix Tree Theorem for multidigraphs
factored spanning tree algorithms.
of McDonald et al. (2005b). In p
to desgn polynmiatime algori
The paper is oganizd as follows. We discuss re-
peal to the Matrix Tree Theorem for multi-digraphs
ing h h po fution edg ep
to design polynomial-tim algorithms for calculat-
ios ove ll pssibl dpedy gaph r given
lated prior work on dependency parsing and stacking
in §2. Our modelais givenuint§3. Annovel analysisaof
stacking inrlinear models isegiven in §4.hExperiments decod
are presentedTin §5.
, y gg g,
2 Background and Related Work
nd inference problems including min-r
y
We briefly review work on the NLP task of depen-
els syntctic language modeling and unsuprvised
dency parsing and the machine learning framework
known as stacked learning.
</bodyText>
<figure confidence="0.943039625">
(r
u
ent. In the proje
h
mod-
that a
and a
158
</figure>
<bodyText confidence="0.996619333333333">
§2.1.2). Recently, Nivre and McDonald (2008) pro-
posed combining a graph-based and a transition-
based parser and have shown a significant improve-
ment for several languages by letting one of the
parsers “guide” the other. Our stacked formalism
(to be described in §3) generalizes this approach.
</bodyText>
<subsectionHeader confidence="0.658533">
2.1.2 Arc factorization
</subsectionHeader>
<bodyText confidence="0.999876111111111">
In the successful graph-based method of McDon-
ald et al. (2005b), an arc factorization independence
assumption is used to ensure tractability. This as-
sumption forbids any feature that depends on two
or more arcs, permitting only “arc-factored” features
(i.e. features that depend only on a single candidate
arc a E Ay and on the input sequence x). This in-
duces a decomposition of the feature vector f(x, y)
as:
</bodyText>
<equation confidence="0.949492">
f(x, y) = � fa(x).
aEAy
</equation>
<bodyText confidence="0.999939590909091">
Parsing amounts to solving arg maxyEY(x)
wTf(x, y), where w is a weight vector. With
a projectivity constraint and arc factorization, the
parsing problem can be solved in cubic time by
dynamic programming (Eisner, 1996), and with a
weaker “tree” constraint (permitting nonprojective
parses) and arc factorization, a quadratic-time
algorithm exists (Chu and Liu, 1965; Edmonds,
1967), as shown by McDonald et al. (2005b). In
the projective case, the arc-factored assumption can
be weakened in certain ways while maintaining
polynomial parser runtime (Eisner and Satta, 1999),
but not in the nonprojective case (McDonald and
Satta, 2007), where finding the highest-scoring tree
becomes NP-hard.
McDonald and Pereira (2006) adopted an approx-
imation based on O(n3) projective parsing followed
by rearrangement to permit crossing arcs, achieving
higher performance. In §3 we adopt a framework
that maintains O(n2) runtime (still exploiting the
Chu-Liu-Edmonds algorithm) while approximating
non arc-factored features.
</bodyText>
<subsectionHeader confidence="0.999439">
2.2 Stacked Learning
</subsectionHeader>
<bodyText confidence="0.999947275862069">
Stacked generalization was first proposed by
Wolpert (1992) and Breiman (1996) for regression.
The idea is to include two “levels” of predictors. The
first level, “level 0,” includes one or more predictors
g1, ... , gK : Rd _* R; each receives input x E Rd
and outputs a prediction gk(x). The second level,
“level 1,” consists of a single function h : Rd+K _*
R that takes as input (x, g1(x),... gK(x)) and out-
puts a final prediction y� = h(x, g1(x),... gK(x)).
The predictor, then, combines an ensemble (the gk)
with a meta-predictor (h).
Training is done as follows: the training data are
split into L partitions, and L instances of the level
0 predictor are trained in a “leave-one-out” basis.
Then, an augmented dataset is formed by letting
each instance output predictions for the partition that
was left out. Finally, each level 0 predictor is trained
using the original dataset, and the level 1 predictor
is trained on the augmented dataset, simulating the
test-time setting when h is applied to a new instance
x concatenated with (gk(x))k.
This framework has also been applied to classifi-
cation, for example with structured data. Some ap-
plications (including here) use only one classifier at
level 0; recent work includes sequence labeling (Co-
hen and de Carvalho, 2005) and inference in condi-
tional random fields (Kou and Cohen, 2007). Stack-
ing is also intuitively related to transformation-based
learning (Brill, 1993).
</bodyText>
<sectionHeader confidence="0.973257" genericHeader="method">
3 Stacked Dependency Parsing
</sectionHeader>
<bodyText confidence="0.9999585">
We next describe how to use stacked learning for
efficient, rich-featured dependency parsing.
</bodyText>
<subsectionHeader confidence="0.999704">
3.1 Architecture
</subsectionHeader>
<bodyText confidence="0.998573875">
The architecture consists of two levels. At level 0
we include a single dependency parser. At runtime,
this “level 0 parser” g processes an input sentence x
and outputs the set of predicted edges that make up
its estimation of the dependency tree, y0 = g(x). At
level 1, we apply a dependency parser—in this work,
always a graph-based dependency parser—that uses
basic factored features plus new ones from the edges
predicted by the level 0 parser. The final parser pre-
dicts parse trees as h(x, g(x)), so that the total run-
time is additive in calculating h(·) and g(·).
The stacking framework is agnostic about the
form of g and h and the methods used to learn them
from data. In this work we use two well-known,
publicly available dependency parsers, MSTParser
(McDonald et al., 2005b),1 which implements ex-
</bodyText>
<footnote confidence="0.889567">
1http://sourceforge.net/projects/mstparser
</footnote>
<page confidence="0.816649">
159
</page>
<bodyText confidence="0.983741375">
act first-order arc-factored nonprojective parsing
(§2.1.2) and approximate second-order nonprojec-
tive parsing, and MaltParser (Nivre et al., 2006),
which is a state-of-the-art transition-based parser.2
We do not alter the training algorithms used in prior
work for learning these two parsers from data. Us-
ing the existing parsers as starting points, we will
combine them in a variety of ways.
</bodyText>
<subsectionHeader confidence="0.996922">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.998428">
Regardless of our choices for the specific parsers and
learning algorithms at level 0 and level 1, training is
done as sketched in §2.2. Let D be a set of training
examples {(xi, yi)}i.
</bodyText>
<listItem confidence="0.995127416666667">
1. Split training data D into L partitions
D1,...,DL.
2. Train L instances of the level 0 parser in the fol-
lowing way: the l-th instance, gi, is trained on
D_1 = D \ Di. Then use gi to output predic-
tions for the (unseen) partition Di. At the end,
an augmented dataset D˜ = UL �=1 ˜D� is built, so
that D˜ = {(xi, g(xi), yi)}i.
3. Train the level 0 parser g on the original training
data D.
4. Train the level 1 parser h on the augmented train-
ing data ˜D.
</listItem>
<bodyText confidence="0.999210666666667">
The runtime of this algorithm is O(LT0+T1), where
T0 and T1 are the individual runtimes required for
training level 0 and level 1 alone, respectively.
</bodyText>
<sectionHeader confidence="0.98592" genericHeader="method">
4 Two Views of Stacked Parsing
</sectionHeader>
<bodyText confidence="0.9999095">
We next describe two motivations for stacking
parsers: as a way of augmenting the features of a
graph-based dependency parser or as a way to ap-
proximate higher-order models.
</bodyText>
<subsectionHeader confidence="0.996966">
4.1 Adding Input Features
</subsectionHeader>
<bodyText confidence="0.993206666666667">
Suppose that the level 1 classifier is an arc-factored
graph-based parser. The feature vectors will take the
form3
</bodyText>
<equation confidence="0.973496">
f(x, y) = f1(x, y) ^ f2(x, ˆy0, y)
�= f1,a(x) ^ f2,a(x,g(x)),
aEAy
</equation>
<footnote confidence="0.556675">
2http://w3.msi.vxu.se/˜jha/maltparser
3We use — to denote vector concatenation.
</footnote>
<bodyText confidence="0.999218052631579">
where f1(x,y) = EaEAy f1,a(x) are regu-
lar arc-factored features, and f2(x, ˆy0, y) =
EaEAy f2,a(x, g(x)) are the stacked features. An
example of a stacked feature is a binary feature
f2,a(x, g(x)) that fires if and only if the arc a was
predicted by g, i.e., if a E Ag(x); such a feature was
used by Nivre and McDonald (2008).
It is difficult in general to decide whether the in-
clusion of such a feature yields a better parser, since
features strongly correlate with each other. How-
ever, a popular heuristic for feature selection con-
sists of measuring the information gain provided by
each individual feature. In this case, we may obtain
a closed-form expression for the information gain
that f2,a(x, g(x)) provides about the existence or not
of the arc a in the actual dependency tree y. Let A
and A&apos; be binary random variables associated with
the events a E Ay and a&apos; E Ag(x), respectively. We
have:
</bodyText>
<equation confidence="0.99097775">
p(a, a&apos;)
p(a, a&apos;) log2 p(a)p(a&apos;)
= H(A&apos;) − � p(a)H(A&apos;|A = a).
aE{0,1}
</equation>
<bodyText confidence="0.942251857142857">
Assuming, for simplicity, that at level 0 the prob-
ability of false positives equals the probability of
false negatives (i.e., Perr °= p(a&apos; = 0|a = 1) =
p(a&apos; = 1|a = 0)), and that the probability of
true positives equals the probability of true negatives
(1 − Perr = p(a&apos; = 0|a = 0) = p(a&apos; = 1|a = 1)),
the expression above reduces to:
</bodyText>
<equation confidence="0.954363">
I(A; A&apos;) = H(A&apos;) + Perr log2 Perr
+ (1 − Perr)log2(1 − Perr)
= H(A&apos;) − Herr,
</equation>
<bodyText confidence="0.999791625">
where Herr denotes the entropy of the probability of
error on each arc’s prediction by the level 0 classi-
fier. If Perr G 0.5 (i.e. if the level 0 classifier is
better than random), then the information gain pro-
vided by this simple stacked feature increases with
(a) the accuracy of the level 0 classifier, and (b) the
entropy H(A&apos;) of the distribution associated with its
arc predictions.
</bodyText>
<subsectionHeader confidence="0.998751">
4.2 Approximating Non-factored Features
</subsectionHeader>
<bodyText confidence="0.99965">
Another way of interpreting the stacking framework
is as a means to approximate a higher order model,
</bodyText>
<equation confidence="0.5916062">
�
I(A;A&apos;) =
a,a&apos;E{0,1}
160
;
</equation>
<bodyText confidence="0.99968075">
such as one that is not arc-factored, by using stacked
features that make use of the predicted structure
around a candidate arc. Consider a second-order
model where the features decompose by arc and by
</bodyText>
<equation confidence="0.9034704">
arc pair:
⎛
X X
f(x, y) = ⎝fa1(x) ^ fa1,a2(x)
a1∈Ay a2∈Ay
</equation>
<bodyText confidence="0.9990746">
Exact parsing under such model, with arbitrary
second-order features, is intractable (McDonald and
Satta, 2007). Let us now consider a stacked model
in which the level 0 predictor outputs a parse ˆy. At
level 1, we use arc-factored features that may be
</bodyText>
<equation confidence="0.8916802">
written as
⎛ ⎞
X
⎝fa1(x) ^ fa1,a2(x) ⎠;
a2∈Aˆy
</equation>
<bodyText confidence="0.931671">
this model differs from the previous one only by re-
placing Ay by Aˆy in the index set of the second sum-
mation. Since yˆ is given, this makes the latter model
arc-factored, and therefore, tractable. We can now
view ˜f(x, y) as an approximation of f(x, y); indeed,
we can bound the score approximation error,
</bodyText>
<equation confidence="0.9555655">
��� ,
Δs(x, y) = ��� ˜w&gt;˜f(x, y) − w&gt;f(x, y)
</equation>
<bodyText confidence="0.9422578">
where w˜ and w stand respectively for the parameters
learned for the stacked model and those that would
be learned for the (intractable) exact second order
model. We can bound Δs(x, y) by spliting it into
two terms: Δs(x, y) =
</bodyText>
<equation confidence="0.995711285714286">
~~~( w˜ − w)&gt;˜f(x, y) + w&gt;(˜f(x, y) − f(x, y)) ���
��
�
≤ �( w˜ −w)&gt;˜f(x, y) +�w&gt;(˜f(x, y) − f(x, y))
����
 |{z }  |{z }
oΔstr(x,y) oΔsdec(x,y)
</equation>
<bodyText confidence="0.999653">
where we introduced the terms Δstr and Δsdec that
reflect the portion of the score approximation error
that are due to training error (i.e., different parame-
terizations of the exact and approximate models) and
decoding error (same parameterizations, but differ-
ent feature vectors). Using H¨older’s inequality, the
former term can be bounded as:
</bodyText>
<equation confidence="0.998635166666667">
���( w˜ − w)&gt;˜f(x, y) ���
Δstr(x, y) =
≤
k w˜− wk1 · k˜f(x,y)k∞
≤
k w˜− wk1 ;
</equation>
<bodyText confidence="0.998479875">
where k.k1 and k.k∞ denote the `1-norm and sup-
norm, respectively, and the last inequality holds
when the features are binary (so that k ˜f(x,y)k∞ ≤
1). The proper way to bound the term k w˜ − wk1
depends on the training algorithm. As for the de-
coding error term, it can bounded for a given weight
vector w, sentence x, candidate tree y, and level 0
prediction ˆy. Decomposing the weighted vector as
</bodyText>
<equation confidence="0.98000525">
w = w1 ^ w2, w2 being the sub-vector associ-
ated with the second-order features, we have respec-
tively: Δsdec(x, y) =
���w&gt;(˜f(x, y) − f(x, y)) ���
⎛ ⎞
X X
w&gt; ⎝ X ������
fa1,a2(x) − fa1,a2(x) ⎠
2
a2∈Aˆy a2∈Ay
�����
&gt;
w2 fa1,a2(x)
X |AˆyΔAy |· max �����
≤ a2∈AˆyΔAy ~w&gt;2 fa1,a2(x)
a1∈Ay
</equation>
<bodyText confidence="0.999698333333333">
where AˆyΔAy °_ (Aˆy − Ay) ∪ (Ay − Aˆy) denotes
the symmetric difference of the sets Aˆy and Ay,
which has cardinality 2L(y, ˆy), i.e., twice the Ham-
ming distance between the sequences of heads that
characterize y and the predicted parse ˆy. Using
H¨older’s inequality, we have both
</bodyText>
<equation confidence="0.897906">
w&gt;2 fa1,a2(x) ���≤ kw2k1 · kfa1,a2(x)k∞
w&gt;2 fa1,a2(x)��� ≤ kw2k∞ · kfa1,a2(x)k1.
</equation>
<bodyText confidence="0.9918446">
Assuming that all features are binary valued, we
have that kfa1,a2(x)k∞ ≤ 1 and that kfa1,a2(x)k1 ≤
Nf,2, where Nf,2 denotes the maximum number of
active second order features for any possible pair of
arcs (a1, a2). Therefore:
</bodyText>
<equation confidence="0.94122">
Δsdec(x, y) ≤ 2nL(y, ˆy) min{kw2k1, Nf,2·kw2k∞},
</equation>
<bodyText confidence="0.999748666666667">
where n is the sentence length. Although this bound
can be loose, it suggests (intuitively) that the score
approximation degrades as the predicted tree yˆ gets
farther away from the true tree y (in Hamming dis-
tance). It also degrades with the magnitude of
weights associated with the second-order features,
</bodyText>
<figure confidence="0.952610142857143">
X=
a1∈Ay
&gt;
a2∈A ˆyΔAy w2 fa1,a2(x)
a
m
x
2L(y, ˆy)
.
˜f(x, y) = X
a1∈Ay
������
a1∈Ay
≤ X X
a1∈Ay a2∈AˆyΔAy
���
��
and �
161
II
Name Description
</figure>
<table confidence="0.991247923076923">
PredEdge Indicates whether the candidate edge
was present, and what was its label.
Sibling Lemma, POS, link label, distance and
direction of attachment of the previous
and and next predicted siblings
GrandParents Lemma, POS, link label, distance and
direction of attachment of the grandpar-
ent of the current modifier
PredHead Predicted head of the candidate modifier
(if PredEdge=0)
AllChildren Sequence of POS and link labels of all
the predicted children of the candidate
head
</table>
<tableCaption confidence="0.986938">
Table 1: Feature sets derived from the level 0 parser.
</tableCaption>
<table confidence="0.937994142857143">
Subset Description
A PredEdge
B PredEdge+Sibling
C PredEdge+Sibling+GrandParents
D PredEdge+Sibling+GrandParents+PredHead
E PredEdge+Sibling+GrandParents+PredHead+
AllChildren
</table>
<tableCaption confidence="0.9901375">
Table 2: Combinations of features enumerated in Table 1
used for stacking. A is a replication of (Nivre and Mc-
Donald, 2008), except for the modifications described in
footnote 4.
</tableCaption>
<note confidence="0.922718">
162 McDonald et al., 2005b; McDonald and Pereira,
</note>
<bodyText confidence="0.999928666666667">
which suggests that a separate regularization of the
first-order and stacked features might be beneficial
in a stacking framework.
As a side note, if we set each component of
the weight vector to one, we obtain a bound
on the `1-norm of the feature vector difference,
</bodyText>
<equation confidence="0.993979333333333">
� �
��
��f(x, y) − f(x, y) 1 G 2nL(y, �y)Nf,2.
</equation>
<bodyText confidence="0.997820428571429">
MSTParser and the tran
sition-based MaltParser.
forms parsing an
d labeling jointly. We adapted this
system to first perform unlabeled parsing, then la-
bel the arcs using alog-linear classifier with access
to the full unlabeled parse (McDonald et al., 2005a;
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998211">
In the following experiments we demonstrate the ef-
fectiveness of stacking parsers. As noted in §3.1, we
make use of two component parsers, the graph-based
</bodyText>
<subsectionHeader confidence="0.993263">
5.1 Implementation and Experimental Details
</subsectionHeader>
<bodyText confidence="0.971079607142857">
The publicly available version of MSTParser per-
2006). In stacking experiments, the arc labels from
the level 0 parser are also used as a
In the following subsections, we refer to our mod-
ification of the MSTParser as
(the arc-
factored version) and
(the second-order
arc-pair-factored version). All our experiments use
the non-projective version of this parser. We refer to
the MaltParser as Malt.
We report experiments on twelve languages from
the CoNLL-X shared task (Buchholz and Marsi,
2006).5 All experiments are evaluated using the
labeled attachment score (LAS), using the default
settings.6 Statistical significance is measured us-
ing Dan
parsing evaluation com-
parator with 10,000
The additional fea-
tures used in the level 1 parser are enumerated in
Table 1 and their various subsets are depicted in Ta-
ble 2. The PredEdge features are exactly the six fea-
tures used by Nivre and McDonald (2008) in their
MSTMalt parser; therefore, feature set A is a repli-
cation of this parser except for modifications noted
in footnote 4. In all our experiments, the number of
part
</bodyText>
<equation confidence="0.6136266">
feature.4
MST1O
MST2O
Bikel’s randomized
iterations.7
</equation>
<bodyText confidence="0.272459">
itions used to create D is L = 2.
</bodyText>
<subsectionHeader confidence="0.599324">
5.2 Experiment:
</subsectionHeader>
<bodyText confidence="0.78550625">
Our first experiment stacks the highly accurate
parser with itself. At level 0, the parser
uses only the standard features (§5.1), and at level 1,
these are augmented by vari
</bodyText>
<equation confidence="0.8876785">
MST2O+ MST2O
MST2O
</equation>
<bodyText confidence="0.993824333333333">
ous subsets of features
of x along with the output of the level 0 parser, g(x)
(Table 2). The results are shown in Table 3. While
we see improvements over the single-parser baseline
made other modifications to MSTParser, implement-
ing many of the successes described by (McDonald et al.,
2006). Our version of the code is publicly available at
//www.ark.cs.cmu.edu/MSTParserStacked. The
modifications included an approximation to lemmas for datasets
without lemmas (three-character prefixes), and replacing mor-
phology/word and morphology/lemma features with morphol-
ogy/POS features.
CoNLL-X shared task actually involves thirteen lan-
guages; our experiments do not include Czech (the largest
dataset), due to time constraints. Therefore, the average results
plotted in the last rows of Tables 3, 4, and 5 are not directly
comparable with previously published averages over thirteen
languages.
</bodyText>
<footnote confidence="0.6673612">
4We
http:
5The
6http://nextens.uvt.nl/˜conll/software.html7http://www.cis.upenn.edu/˜dbikel/software.
html
</footnote>
<table confidence="0.999683076923077">
Arabic 67.88 66.91 67.41 67.68 67.37 68.02
Bulgarian 87.31 87.39 87.03 87.61 87.57 87.55
Chinese 87.57 87.16 87.24 87.48 87.42 87.48
Danish 85.27 85.39 85.61 85.57 85.43 85.57
Dutch 79.99 79.79 79.79 79.83 80.17 80.13
German 87.44 86.92 87.32 87.32 87.26 87.04
Japanese 90.93 91.41 91.21 91.35 91.11 91.19
Portuguese 87.12 87.26 86.88 87.02 87.04 86.98
Slovene 74.02 74.30 74.30 74.00 74.14 73.94
Spanish 82.43 82.17 82.35 82.81 82.53 82.75
Swedish 82.87 82.99 82.95 82.51 83.01 82.69
Turkish 60.11 59.47 59.25 59.47 59.45 59.31
Average 81.08 80.93 80.94 81.05 81.04 81.05
</table>
<tableCaption confidence="0.976044666666667">
Table 3: Results of stacking MST20 with itself at both level 0 and level 1. Column 2 enumerates LAS for MST20.
Columns 3–6 enumerate results for four different stacked feature subsets. Bold indicates best results for a particular
language.
</tableCaption>
<bodyText confidence="0.999984">
for nine languages, the improvements are small (less
than 0.5%). One of the biggest concerns about this
model is the fact that it stacks two predictors that
are very similar in nature: both are graph-based and
share the features f1,,,(x). It has been pointed out by
Breiman (1996), among others, that the success of
ensemble methods like stacked learning strongly de-
pends on how uncorrelated the individual decisions
made by each predictor are from the others’ deci-
sions.8 This experiment provides further evidence
for the claim.
</bodyText>
<subsectionHeader confidence="0.810134">
5.3 Experiment: Malt + MST2O
</subsectionHeader>
<bodyText confidence="0.990647526315789">
We next use MaltParser at level 0 and the second-
order arc-pair-factored MST2O at level 1. This
extends the experiments of Nivre and McDonald
(2008), replicated in our feature subset A.
Table 4 enumerates the results. Note that the
best-performing stacked configuration for each and
every language outperforms MST 2O, corroborat-
ing results reported by Nivre and McDonald (2008).
The best performing stacked configuration outper-
forms Malt as well, except for Japanese and Turk-
ish. Further, our non-arc-factored features largely
outperform subset A, except on Bulgarian, Chinese,
8This claim has a parallel in the cotraining method (Blum
and Mitchell, 1998), whose performance is bounded by the de-
gree of independence between the two feature sets.
and Japanese. On average, the best feature config-
uration is E, which is statistically significant over
Malt and MST2O with p &lt; 0.0001, and over fea-
ture subset A with p &lt; 0.01.
</bodyText>
<subsectionHeader confidence="0.414731">
5.4 Experiment: Malt + MST 1O
</subsectionHeader>
<bodyText confidence="0.99988147368421">
Finally, we consider stacking MaltParser with the
first-order, arc-factored MSTParser. We view this
approach as perhaps the most promising, since it is
an exact parsing method with the quadratic runtime
complexity of MST 1O.
Table 5 enumerates the results. For all twelve
languages, some stacked configuration outperforms
MST1O and also, surprisingly, MST2O, the sec-
ond order model. This provides empirical evi-
dence that using rich features from MaltParser at
level 0, a stacked level 1 first-order MSTParser can
outperform the second-order MSTParser.9 In only
two cases (Japanese and Turkish), the MaltParser
slightly outperforms the stacked parser.
On average, feature configuration D performs
the best, and is statistically significant over Malt,
MST 1O, and MST 2O with p &lt; 0.0001, and over
feature subset A with p &lt; 0.05. Encouragingly, this
configuration is barely outperformed by configura-
</bodyText>
<table confidence="0.943368875">
9Recall that MST20 uses approximate search, as opposed
to stacking, which uses approximate features.
163
Arabic 66.71 67.88 68.56 69.12 68.64 68.34 68.92
Bulgarian 87.41 87.31 88.99 88.89 88.89 88.93 88.91
Chinese 86.92 87.57 88.41 88.31 88.29 88.13 88.41
Danish 84.77 85.27 86.45 86.67 86.79 86.13 86.71
Dutch 78.59 79.99 80.75 81.47 81.47 81.51 81.29
German 85.82 87.44 88.16 88.50 88.56 88.68 88.38
Japanese 91.65 90.93 91.63 91.43 91.59 91.61 91.49
Portuguese 87.60 87.12 88.00 88.24 88.30 88.18 88.22
Slovene 70.30 74.02 76.62 76.00 76.60 76.18 76.72
Spanish 81.29 82.43 83.09 83.73 83.47 83.21 83.43
Swedish 84.58 82.87 84.92 84.60 84.80 85.16 84.88
Turkish 65.68 60.11 64.35 64.51 64.51 65.07 65.21
Average 80.94 81.08 82.52 82.58 82.65 82.59 82.71
</table>
<tableCaption confidence="0.970880333333333">
Table 4: Results of stacking Malt and MST20 at level 0 and level 1, respectively. Columns 2–4 enumerate LAS for
Malt, MST20 and Malt + MST2o as in Nivre and McDonald (2008). Columns 5–8 enumerate results for four other
stacked feature configurations. Bold indicates best result for a language.
</tableCaption>
<bodyText confidence="0.999437571428571">
tion A of Malt + MST20 (see Table 4), the dif-
ference being statistically insignificant (p &gt; 0.05).
This shows that stacking Malt with the exact, arc-
factored MST 1O bridges the difference between the
individual MST 1O and MST20 models, by approx-
imating higher order features, but maintaining an
O(n2) runtime and finding the model-optimal parse.
</bodyText>
<subsectionHeader confidence="0.99368">
5.5 Disagreement as a Confidence Measure
</subsectionHeader>
<bodyText confidence="0.999928388888889">
In pipelines or semisupervised settings, it is use-
ful when a parser can provide a confidence measure
alongside its predicted parse tree. Because stacked
predictors use ensembles with observable outputs,
differences among those outputs may be used to es-
timate confidence in the final output. In stacked de-
pendency parsing, this can be done (for example) by
measuring the Hamming distance between the out-
puts of the level 0 and 1 parsers, L(g(x), h(x)). In-
deed, the bound derived in §4.2 suggests that the
second-order approximation degrades for candidate
parses y that are Hamming-far from g(x); therefore,
if L(g(x), h(x)) is large, the best score s(x, h(x))
may well be “biased” due to misleading neighbor-
ing information provided by the level 0 parser.
We illustrate this point with an empirical analysis
of the level 0/1 disagreement for the set of exper-
iments described in §5.3; namely, we compare the
</bodyText>
<figure confidence="0.941389">
0 2 4 6 8 10
L(g(x),h(x))
</figure>
<figureCaption confidence="0.998665">
Figure 2: Accuracy as a function of token disagreement
</figureCaption>
<bodyText confidence="0.8793546">
between level 0 and level 1. The x-axis is the Hamming
distance L(g(x), h(x)), i.e., the number of tokens where
level 0 and level 1 disagree. The y-axis is the accuracy
averaged over sentences that have the specified Hamming
distance, both for level 0 and level 1.
</bodyText>
<figure confidence="0.931438">
Sentence Averaged Accuracy
0.65
1
0.95
0.9
0.85
0.8
0.75
0.7
Level 0
Level 1
Level 0 (Overall)
Level 1 (Overall)
164
</figure>
<table confidence="0.995287230769231">
Arabic 66.71 66.81 67.88 68.40 68.50 68.20 68.42 68.68
Bulgarian 87.41 86.65 87.31 88.55 88.67 88.75 88.71 88.79
Chinese 86.92 86.60 87.57 87.67 87.73 87.83 87.67 87.61
Danish 84.77 84.87 85.27 86.59 86.27 86.21 86.35 86.15
Dutch 78.59 78.95 79.99 80.53 81.51 80.71 81.61 81.37
German 85.82 86.26 87.44 88.18 88.30 88.20 88.36 88.42
Japanese 91.65 91.01 90.93 91.55 91.53 91.51 91.43 91.57
Portuguese 87.60 86.28 87.12 88.16 88.26 88.46 88.26 88.36
Slovene 70.30 73.96 74.02 75.84 75.64 75.42 75.96 75.64
Spanish 81.29 81.07 82.43 82.61 83.13 83.13 83.09 82.99
Swedish 84.58 81.88 82.87 84.86 84.62 84.64 84.82 84.76
Turkish 65.68 59.63 60.11 64.49 64.97 64.47 64.63 64.61
Average 80.94 80.33 81.08 82.28 82.42 82.29 82.44 82.41
</table>
<tableCaption confidence="0.941408333333333">
Table 5: Results of stacking Malt and MST1O at level 0 and level 1, respectively. Columns 2–4 enumerate LAS for
Malt, MST1O and MST20. Columns 5–9 enumerate results for five different stacked feature configurations. Bold
indicates the best result for a language.
</tableCaption>
<bodyText confidence="0.99990625">
level 0 and level 1 predictions under the best overall
configuration (configuration E of Malt + MST2O).
Figure 2 depicts accuracy as a function of level 0-
level 1 disagreement (in number of tokens), aver-
aged over all datasets.
We can see that performance degrades steeply
when the disagreement between levels 0 and 1 in-
creases in the range 0–4, and then behaves more ir-
regularly but keeping the same trend. This suggests
that the Hamming distance L(g(x), h(x)) is infor-
mative about parser performance and may be used
as a confidence measure.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999945647058824">
In this work, we made use of stacked learning to im-
prove dependency parsing. We considered an archi-
tecture with two layers, where the output of a stan-
dard parser in the first level provides new features
for a parser in the subsequent level. During learning,
the second parser learns to correct mistakes made by
the first one. The novelty of our approach is in the
exploitation of higher-order predicted edges to simu-
late non-local features in the second parser. We pro-
vided a novel interpretation of stacking as feature
approximation, and our experimental results show
rich-featured stacked parsers outperforming state-
of-the-art single-layer and ensemble parsers. No-
tably, using a simple arc-factored parser at level 1,
we obtain an exact O(n2) stacked parser that outper-
forms earlier approximate methods (McDonald and
Pereira, 2006).
</bodyText>
<sectionHeader confidence="0.997137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999821181818182">
The authors thank the anonymous reviewers for
helpful comments, Vitor Carvalho, William Cohen,
and David Smith for interesting discussions, and
Ryan McDonald and Joakim Nivre for providing
us their code and preprocessed datasets. A.M. was
supported by a grant from FCT through the CMU-
Portugal Program and the Information and Com-
munications Technologies Institute (ICTI) at CMU.
N.S. was supported by NSF IIS-0713265 and an
IBM faculty award. E.X. was supported by NSF
DBI-0546594, DBI-0640543, and IIS-0713379.
</bodyText>
<sectionHeader confidence="0.994219" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998480625">
S. P. Abney, D. A. McAllester, and F. Pereira. 1999. Re-
lating probabilistic grammars and automata. In Pro-
ceedings of ACL.
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of COLT.
L. Breiman. 1996. Stacked regressions. Machine Learn-
ing, 24:49.
</reference>
<page confidence="0.599172">
165
</page>
<reference confidence="0.999950701149425">
E. Brill. 1993. A Corpus-Based Approach to Language
Learning. Ph.D. thesis, University of Pennsylvania.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proceedings
of CoNLL.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396–
1400.
W. W. Cohen and V. Rocha de Carvalho. 2005. Stacked
sequential learning. In Proceedings of IJCAI.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mar. In Proceedings of ACL.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233–
240.
J. Eisner and G. Satta. 1999. Efficient parsing for bilex-
ical context-free grammars and head automaton gram-
mars. In Proceedings of ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
classifiers for deterministic dependency parsing. In
Proceedings of ACL.
Z. Kou and W. W. Cohen. 2007. Stacked graphical mod-
els for efficient inference in Markov random fields. In
Proceedings of SDM.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
P. Liang, H. Daum´e, and D. Klein. 2008. Structure com-
pilation: trading structure for features. In Proceedings
of ICML.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL.
R. T. McDonald and F. C. N. Pereira. 2006. Online learn-
ing of approximate dependency parsing algorithms. In
Proceedings of EACL.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Ha-
jic. 2005b. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proceedings of HLT-
EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proceedings CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-HLT.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL.
J. Nivre, J. Hall, J. Nilsson, G. Eryiˇgit, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
CoNLL.
A. Ratnaparkhi, S. Roukos, and R. T. Ward. 1994. A
maximum entropy model for parsing. In Proceedings
of ICSLP.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proceedings of EMNLP.
K. Sagae and A. Lavie. 2005. A classifier-based parser
with linear run-time complexity. In Proceedings of
IWPT.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proceedings of EMNLP.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of
IWPT.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proceedings of EMNLP-CoNLL.
D. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5(2):241–260.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of IWPT.
</reference>
<page confidence="0.943668">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.567580">
<title confidence="0.999934">Stacking Dependency Parsers</title>
<author confidence="0.980728">F T Dipanjan Noah A Eric P</author>
<affiliation confidence="0.7672825">of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, de Instituto Superior T´ecnico, Lisboa,</affiliation>
<abstract confidence="0.997993285714286">We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich features the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S P Abney</author>
<author>D A McAllester</author>
<author>F Pereira</author>
</authors>
<title>Relating probabilistic grammars and automata.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Abney, McAllester, Pereira, 1999</marker>
<rawString>S. P. Abney, D. A. McAllester, and F. Pereira. 1999. Relating probabilistic grammars and automata. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="28904" citStr="Blum and Mitchell, 1998" startWordPosition="4757" endWordPosition="4760">l 0 and the secondorder arc-pair-factored MST2O at level 1. This extends the experiments of Nivre and McDonald (2008), replicated in our feature subset A. Table 4 enumerates the results. Note that the best-performing stacked configuration for each and every language outperforms MST 2O, corroborating results reported by Nivre and McDonald (2008). The best performing stacked configuration outperforms Malt as well, except for Japanese and Turkish. Further, our non-arc-factored features largely outperform subset A, except on Bulgarian, Chinese, 8This claim has a parallel in the cotraining method (Blum and Mitchell, 1998), whose performance is bounded by the degree of independence between the two feature sets. and Japanese. On average, the best feature configuration is E, which is statistically significant over Malt and MST2O with p &lt; 0.0001, and over feature subset A with p &lt; 0.01. 5.4 Experiment: Malt + MST 1O Finally, we consider stacking MaltParser with the first-order, arc-factored MSTParser. We view this approach as perhaps the most promising, since it is an exact parsing method with the quadratic runtime complexity of MST 1O. Table 5 enumerates the results. For all twelve languages, some stacked configu</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Stacked regressions.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<pages>24--49</pages>
<contexts>
<context position="13328" citStr="Breiman (1996)" startWordPosition="2142" endWordPosition="2143">n ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features. 2.2 Stacked Learning Stacked generalization was first proposed by Wolpert (1992) and Breiman (1996) for regression. The idea is to include two “levels” of predictors. The first level, “level 0,” includes one or more predictors g1, ... , gK : Rd _* R; each receives input x E Rd and outputs a prediction gk(x). The second level, “level 1,” consists of a single function h : Rd+K _* R that takes as input (x, g1(x),... gK(x)) and outputs a final prediction y� = h(x, g1(x),... gK(x)). The predictor, then, combines an ensemble (the gk) with a meta-predictor (h). Training is done as follows: the training data are split into L partitions, and L instances of the level 0 predictor are trained in a “lea</context>
<context position="27971" citStr="Breiman (1996)" startWordPosition="4616" endWordPosition="4617">82.69 Turkish 60.11 59.47 59.25 59.47 59.45 59.31 Average 81.08 80.93 80.94 81.05 81.04 81.05 Table 3: Results of stacking MST20 with itself at both level 0 and level 1. Column 2 enumerates LAS for MST20. Columns 3–6 enumerate results for four different stacked feature subsets. Bold indicates best results for a particular language. for nine languages, the improvements are small (less than 0.5%). One of the biggest concerns about this model is the fact that it stacks two predictors that are very similar in nature: both are graph-based and share the features f1,,,(x). It has been pointed out by Breiman (1996), among others, that the success of ensemble methods like stacked learning strongly depends on how uncorrelated the individual decisions made by each predictor are from the others’ decisions.8 This experiment provides further evidence for the claim. 5.3 Experiment: Malt + MST2O We next use MaltParser at level 0 and the secondorder arc-pair-factored MST2O at level 1. This extends the experiments of Nivre and McDonald (2008), replicated in our feature subset A. Table 4 enumerates the results. Note that the best-performing stacked configuration for each and every language outperforms MST 2O, corr</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Stacked regressions. Machine Learning, 24:49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A Corpus-Based Approach to Language Learning.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="14677" citStr="Brill, 1993" startWordPosition="2370" endWordPosition="2371">Finally, each level 0 predictor is trained using the original dataset, and the level 1 predictor is trained on the augmented dataset, simulating the test-time setting when h is applied to a new instance x concatenated with (gk(x))k. This framework has also been applied to classification, for example with structured data. Some applications (including here) use only one classifier at level 0; recent work includes sequence labeling (Cohen and de Carvalho, 2005) and inference in conditional random fields (Kou and Cohen, 2007). Stacking is also intuitively related to transformation-based learning (Brill, 1993). 3 Stacked Dependency Parsing We next describe how to use stacked learning for efficient, rich-featured dependency parsing. 3.1 Architecture The architecture consists of two levels. At level 0 we include a single dependency parser. At runtime, this “level 0 parser” g processes an input sentence x and outputs the set of predicted edges that make up its estimation of the dependency tree, y0 = g(x). At level 1, we apply a dependency parser—in this work, always a graph-based dependency parser—that uses basic factored features plus new ones from the edges predicted by the level 0 parser. The final</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>E. Brill. 1993. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="25005" citStr="Buchholz and Marsi, 2006" startWordPosition="4153" endWordPosition="4156">tacking parsers. As noted in §3.1, we make use of two component parsers, the graph-based 5.1 Implementation and Experimental Details The publicly available version of MSTParser per2006). In stacking experiments, the arc labels from the level 0 parser are also used as a In the following subsections, we refer to our modification of the MSTParser as (the arcfactored version) and (the second-order arc-pair-factored version). All our experiments use the non-projective version of this parser. We refer to the MaltParser as Malt. We report experiments on twelve languages from the CoNLL-X shared task (Buchholz and Marsi, 2006).5 All experiments are evaluated using the labeled attachment score (LAS), using the default settings.6 Statistical significance is measured using Dan parsing evaluation comparator with 10,000 The additional features used in the level 1 parser are enumerated in Table 1 and their various subsets are depicted in Table 2. The PredEdge features are exactly the six features used by Nivre and McDonald (2008) in their MSTMalt parser; therefore, feature set A is a replication of this parser except for modifications noted in footnote 4. In all our experiments, the number of part feature.4 MST1O MST2O B</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<volume>14</volume>
<pages>1400</pages>
<contexts>
<context position="12582" citStr="Chu and Liu, 1965" startWordPosition="2035" endWordPosition="2038">depends on two or more arcs, permitting only “arc-factored” features (i.e. features that depend only on a single candidate arc a E Ay and on the input sequence x). This induces a decomposition of the feature vector f(x, y) as: f(x, y) = � fa(x). aEAy Parsing amounts to solving arg maxyEY(x) wTf(x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2) runtime (still exploiting the Chu-Liu-Edmonds algorithm) </context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y. J. Chu and T. H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396– 1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
<author>V Rocha de Carvalho</author>
</authors>
<title>Stacked sequential learning.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<marker>Cohen, de Carvalho, 2005</marker>
<rawString>W. W. Cohen and V. Rocha de Carvalho. 2005. Stacked sequential learning. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3791" citStr="Culotta and Sorensen, 2004" startWordPosition="555" endWordPosition="559">plored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, Honolulu, October 2008.c�2008 Association for Computational Linguistics Figure 1: A projective dependency arse (top), and a nonFigure 2: Nonprojective dependency graph projective dependency parse (bottom) for two English those that assume each dependency decsion is in sentences; examples from McDonald and Satta (2007). ddt modl th lbl ttl ntint p (2008). els a tht d b u d Teis paper focuses on dependencydparsing,ewhich hasobecomerwidelyaused in0relation extraction (Culotta and Sorensen, 2004), machine translation 205a). Edgefactod mode have man computa of the graph (Paskn 200 McDonld et al (Ding and Palmer, 2005), question answering (Wang tional benefits, most nobly that inference fo non 2005a) Edgefactored models have many computa et al., 2007), and many other NLP applications. prctive dpdy gph an be ied tionl benefis most notaby tha infence for non polymil (MDold t 5b) he p We show that stackig methods outprform the approjective dependency graphs can be achieved in pbl i teatig h dped i proximate “second-order” parser of McDonald and polynomal time (McDoald et al 2005b) The prid</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>A. Culotta and J. Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ding</author>
<author>M Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammar.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3914" citStr="Ding and Palmer, 2005" startWordPosition="576" endWordPosition="579">, Honolulu, October 2008.c�2008 Association for Computational Linguistics Figure 1: A projective dependency arse (top), and a nonFigure 2: Nonprojective dependency graph projective dependency parse (bottom) for two English those that assume each dependency decsion is in sentences; examples from McDonald and Satta (2007). ddt modl th lbl ttl ntint p (2008). els a tht d b u d Teis paper focuses on dependencydparsing,ewhich hasobecomerwidelyaused in0relation extraction (Culotta and Sorensen, 2004), machine translation 205a). Edgefactod mode have man computa of the graph (Paskn 200 McDonld et al (Ding and Palmer, 2005), question answering (Wang tional benefits, most nobly that inference fo non 2005a) Edgefactored models have many computa et al., 2007), and many other NLP applications. prctive dpdy gph an be ied tionl benefis most notaby tha infence for non polymil (MDold t 5b) he p We show that stackig methods outprform the approjective dependency graphs can be achieved in pbl i teatig h dped i proximate “second-order” parser of McDonald and polynomal time (McDoald et al 2005b) The pridd i tht it i ot lii ti Pereia (2006) on twelve languages a can be used mary problem in treating each dependency as inNonloc</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Y. Ding and M. Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammar. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards,</journal>
<volume>71</volume>
<pages>240</pages>
<contexts>
<context position="12598" citStr="Edmonds, 1967" startWordPosition="2039" endWordPosition="2040">ore arcs, permitting only “arc-factored” features (i.e. features that depend only on a single candidate arc a E Ay and on the input sequence x). This induces a decomposition of the feature vector f(x, y) as: f(x, y) = � fa(x). aEAy Parsing amounts to solving arg maxyEY(x) wTf(x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximat</context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>J. Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards, 71B:233– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1523" citStr="Eisner and Satta, 1999" startWordPosition="220" endWordPosition="224">oximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (</context>
<context position="12789" citStr="Eisner and Satta, 1999" startWordPosition="2066" endWordPosition="2069">eature vector f(x, y) as: f(x, y) = � fa(x). aEAy Parsing amounts to solving arg maxyEY(x) wTf(x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features. 2.2 Stacked Learning Stacked generalization was first proposed by Wolpert (1992) and Breiman (1996) for regression. The idea is to include two “levels” of predi</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2669" citStr="Eisner, 1996" startWordPosition="392" endWordPosition="393">of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Pr</context>
<context position="12435" citStr="Eisner, 1996" startWordPosition="2017" endWordPosition="2018">McDonald et al. (2005b), an arc factorization independence assumption is used to ensure tractability. This assumption forbids any feature that depends on two or more arcs, permitting only “arc-factored” features (i.e. features that depend only on a single candidate arc a E Ay and on the input sequence x). This induces a decomposition of the feature vector f(x, y) as: f(x, y) = � fa(x). aEAy Parsing amounts to solving arg maxyEY(x) wTf(x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit cro</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hall</author>
<author>J Nivre</author>
<author>J Nilsson</author>
</authors>
<title>Discriminative classifiers for deterministic dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1878" citStr="Hall et al., 2006" startWordPosition="272" endWordPosition="275">tistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way o</context>
<context position="8879" citStr="Hall et al., 2006" startWordPosition="1420" endWordPosition="1423">ut of each the case for edge-factoed modls (Paskin, 200; at employ exhaustive nference algorithms, uucDonald etal., 2005a; McDonald et al., 2005b). ly by making stro ndependence assumptions as successive parsing state or by searching for the best ecently thee havealso be prposals for xhau sequence of transitions (Ratnaparkhi et al., 1994; the case for edge-factore models (Paskn, 200 Yamada and Matsumoto, 2003; Nivre et al., 2004; e mo wea cDonald et a, 2005a; McDonald et al., 2005) e dgefaore up on ldng bh pproximae thds (McD ecently there have also been proposas for ex Sagae and Lavie, 2005; Hall et al., 2006). d nd Pei 2006) d x methods that weaken the edgefactored assump hds hgh Both approaches most commonly use linear els to assign scores to arcseor decisions, so score is a dot-product of a feature vector f learned weight vector w. rabd gor (wa6). sysems notable exceptions include te work In sum, these two lines of research use different gama bed odel he has be lim approximations to achieve tracabiliy. TransitionWang and Harper (2004). Theoretical studes of ork n piial ym f pjctive ps based approaches solve equence of local probte include the work of Neuhaus and Boker (1997) g ytms tbl epi incld</context>
</contexts>
<marker>Hall, Nivre, Nilsson, 2006</marker>
<rawString>J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative classifiers for deterministic dependency parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kou</author>
<author>W W Cohen</author>
</authors>
<title>Stacked graphical models for efficient inference in Markov random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of SDM.</booktitle>
<contexts>
<context position="14592" citStr="Kou and Cohen, 2007" startWordPosition="2357" endWordPosition="2360">t is formed by letting each instance output predictions for the partition that was left out. Finally, each level 0 predictor is trained using the original dataset, and the level 1 predictor is trained on the augmented dataset, simulating the test-time setting when h is applied to a new instance x concatenated with (gk(x))k. This framework has also been applied to classification, for example with structured data. Some applications (including here) use only one classifier at level 0; recent work includes sequence labeling (Cohen and de Carvalho, 2005) and inference in conditional random fields (Kou and Cohen, 2007). Stacking is also intuitively related to transformation-based learning (Brill, 1993). 3 Stacked Dependency Parsing We next describe how to use stacked learning for efficient, rich-featured dependency parsing. 3.1 Architecture The architecture consists of two levels. At level 0 we include a single dependency parser. At runtime, this “level 0 parser” g processes an input sentence x and outputs the set of predicted edges that make up its estimation of the dependency tree, y0 = g(x). At level 1, we apply a dependency parser—in this work, always a graph-based dependency parser—that uses basic fact</context>
</contexts>
<marker>Kou, Cohen, 2007</marker>
<rawString>Z. Kou and W. W. Cohen. 2007. Stacked graphical models for efficient inference in Markov random fields. In Proceedings of SDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="2000" citStr="Lafferty et al., 2001" startWordPosition="293" endWordPosition="296"> in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al.</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>H Daum´e</author>
<author>D Klein</author>
</authors>
<title>Structure compilation: trading structure for features.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<marker>Liang, Daum´e, Klein, 2008</marker>
<rawString>P. Liang, H. Daum´e, and D. Klein. 2008. Structure compilation: trading structure for features. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2128" citStr="McDonald and Nivre (2007)" startWordPosition="312" endWordPosition="315">sner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall </context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>R. McDonald and J. Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F C N Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="2769" citStr="McDonald and Pereira, 2006" startWordPosition="405" endWordPosition="408">and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, Honolulu, October 2008.c�2008 Association for Computational Linguistics Fig</context>
<context position="4980" citStr="McDonald and Pereira 2006" startWordPosition="757" endWordPosition="760">l time (McDoald et al 2005b) The pridd i tht it i ot lii ti Pereia (2006) on twelve languages a can be used mary problem in treating each dependency as inNonlocal information such as arty (or valency) within that approximato to chieve even better redepedent is that it is not a realistic assumption and neighbouring dependencies can be crucial o sults. These results re similar in prit t (Nivre and Non-local informaton, such as arity (or valency) obaining high parsing accuracies (Klein and ManMcDoald, 008), but with the following novel conand neighbouring dependencis, cn be crucial to ning 2002; McDonald and Pereira 2006) Howtribuios: obtain ever, in y y • a stacking interpretation, eer i th data-drien resentations over the input evioly ben known t have e 2.1 Dependency Parsing arng via the EM algorithm – reviously been known to have exact non-projective Dependency syntax is a lightweight syntactic repplementations mplmtations. resentation that models a sentence as a graph where We then switch focus to models that account for the words are vertices and syntactic relationships are n We the switch focus to models that account for nn, pir ity d negh onlocal informaton, i partculararity and neigh urig par disi F y</context>
<context position="12936" citStr="McDonald and Pereira (2006)" startWordPosition="2087" endWordPosition="2090">tivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features. 2.2 Stacked Learning Stacked generalization was first proposed by Wolpert (1992) and Breiman (1996) for regression. The idea is to include two “levels” of predictors. The first level, “level 0,” includes one or more predictors g1, ... , gK : Rd _* R; each receives input x E Rd and outputs a prediction gk(x</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. T. McDonald and F. C. N. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>G Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="1748" citStr="McDonald and Satta, 2007" startWordPosition="251" endWordPosition="254">ce over existing state-of-the-art dependency parsers. 1 Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivat</context>
<context position="3613" citStr="McDonald and Satta (2007)" startWordPosition="528" endWordPosition="531">parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, Honolulu, October 2008.c�2008 Association for Computational Linguistics Figure 1: A projective dependency arse (top), and a nonFigure 2: Nonprojective dependency graph projective dependency parse (bottom) for two English those that assume each dependency decsion is in sentences; examples from McDonald and Satta (2007). ddt modl th lbl ttl ntint p (2008). els a tht d b u d Teis paper focuses on dependencydparsing,ewhich hasobecomerwidelyaused in0relation extraction (Culotta and Sorensen, 2004), machine translation 205a). Edgefactod mode have man computa of the graph (Paskn 200 McDonld et al (Ding and Palmer, 2005), question answering (Wang tional benefits, most nobly that inference fo non 2005a) Edgefactored models have many computa et al., 2007), and many other NLP applications. prctive dpdy gph an be ied tionl benefis most notaby tha infence for non polymil (MDold t 5b) he p We show that stackig methods o</context>
<context position="12851" citStr="McDonald and Satta, 2007" startWordPosition="2076" endWordPosition="2079">ounts to solving arg maxyEY(x) wTf(x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features. 2.2 Stacked Learning Stacked generalization was first proposed by Wolpert (1992) and Breiman (1996) for regression. The idea is to include two “levels” of predictors. The first level, “level 0,” includes one or more predic</context>
<context position="19681" citStr="McDonald and Satta, 2007" startWordPosition="3239" endWordPosition="3242">and (b) the entropy H(A&apos;) of the distribution associated with its arc predictions. 4.2 Approximating Non-factored Features Another way of interpreting the stacking framework is as a means to approximate a higher order model, � I(A;A&apos;) = a,a&apos;E{0,1} 160 ; such as one that is not arc-factored, by using stacked features that make use of the predicted structure around a candidate arc. Consider a second-order model where the features decompose by arc and by arc pair: ⎛ X X f(x, y) = ⎝fa1(x) ^ fa1,a2(x) a1∈Ay a2∈Ay Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007). Let us now consider a stacked model in which the level 0 predictor outputs a parse ˆy. At level 1, we use arc-factored features that may be written as ⎛ ⎞ X ⎝fa1(x) ^ fa1,a2(x) ⎠; a2∈Aˆy this model differs from the previous one only by replacing Ay by Aˆy in the index set of the second summation. Since yˆ is given, this makes the latter model arc-factored, and therefore, tractable. We can now view ˜f(x, y) as an approximation of f(x, y); indeed, we can bound the score approximation error, ��� , Δs(x, y) = ��� ˜w&gt;˜f(x, y) − w&gt;f(x, y) where w˜ and w stand respectively for the parameters learne</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>R. McDonald and G. Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1546" citStr="McDonald et al., 2005" startWordPosition="225" endWordPosition="228"> features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and </context>
<context position="8405" citStr="McDonald et al., 2005" startWordPosition="1339" endWordPosition="1342">hrough he use of linea time hift-ruce pag algorithms (Yamada an Matbased parsers model the sequnce of decisions of lsson, 2005). n the second category are those umoto, 2003; Nivre and Scholz, 2004; Nivre and t employ exhaustiv inference algorithms, uua shift-reduce parser, given previous decisions and current state, ad paring is perfed by greedily ilsson, 2005). I he second ategory ar thoe y by making song independence assumptions, as coosing the highest scoig transition out of each the case for edge-factoed modls (Paskin, 200; at employ exhaustive nference algorithms, uucDonald etal., 2005a; McDonald et al., 2005b). ly by making stro ndependence assumptions as successive parsing state or by searching for the best ecently thee havealso be prposals for xhau sequence of transitions (Ratnaparkhi et al., 1994; the case for edge-factore models (Paskn, 200 Yamada and Matsumoto, 2003; Nivre et al., 2004; e mo wea cDonald et a, 2005a; McDonald et al., 2005) e dgefaore up on ldng bh pproximae thds (McD ecently there have also been proposas for ex Sagae and Lavie, 2005; Hall et al., 2006). d nd Pei 2006) d x methods that weaken the edgefactored assump hds hgh Both approaches most commonly use linear els to assig</context>
<context position="10511" citStr="McDonald et al. (2005" startWordPosition="1697" endWordPosition="1700">et that includes non-lcl fatures partially adverted by incorporating rich feature pThe goal of this work is to further our current shown here to improve erformnc), and esentaons over the input (McDonald e al., 20 nderstanding of the computational natue of pjec pag g • a variety of stackng archtectures. The goal of this work is to furt m if hi h ddi i Usg ack with rich featurs, we obtai resuts projective parsing algorithms for both learning and investigating and extendng the edge-factored model comptitiv ith Nivre an McDonald (2008) while inference within the data-driven setting We start by of McDonald et al. (2005b). In particular, we appreserving the fast quaratic parsing im of arcinvesigaing and extending the edge-factored model peal to the Matrix Tree Theorem for multidigraphs factored spanning tree algorithms. of McDonald et al. (2005b). In p to desgn polynmiatime algori The paper is oganizd as follows. We discuss repeal to the Matrix Tree Theorem for multi-digraphs ing h h po fution edg ep to design polynomial-tim algorithms for calculatios ove ll pssibl dpedy gaph r given lated prior work on dependency parsing and stacking in §2. Our modelais givenuint§3. Annovel analysisaof stacking inrlinear mo</context>
<context position="11843" citStr="McDonald et al. (2005" startWordPosition="1915" endWordPosition="1919">e problems including min-r y We briefly review work on the NLP task of depenels syntctic language modeling and unsuprvised dency parsing and the machine learning framework known as stacked learning. (r u ent. In the proje h modthat a and a 158 §2.1.2). Recently, Nivre and McDonald (2008) proposed combining a graph-based and a transitionbased parser and have shown a significant improvement for several languages by letting one of the parsers “guide” the other. Our stacked formalism (to be described in §3) generalizes this approach. 2.1.2 Arc factorization In the successful graph-based method of McDonald et al. (2005b), an arc factorization independence assumption is used to ensure tractability. This assumption forbids any feature that depends on two or more arcs, permitting only “arc-factored” features (i.e. features that depend only on a single candidate arc a E Ay and on the input sequence x). This induces a decomposition of the feature vector f(x, y) as: f(x, y) = � fa(x). aEAy Parsing amounts to solving arg maxyEY(x) wTf(x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and wi</context>
<context position="15603" citStr="McDonald et al., 2005" startWordPosition="2524" endWordPosition="2527">tputs the set of predicted edges that make up its estimation of the dependency tree, y0 = g(x). At level 1, we apply a dependency parser—in this work, always a graph-based dependency parser—that uses basic factored features plus new ones from the edges predicted by the level 0 parser. The final parser predicts parse trees as h(x, g(x)), so that the total runtime is additive in calculating h(·) and g(·). The stacking framework is agnostic about the form of g and h and the methods used to learn them from data. In this work we use two well-known, publicly available dependency parsers, MSTParser (McDonald et al., 2005b),1 which implements ex1http://sourceforge.net/projects/mstparser 159 act first-order arc-factored nonprojective parsing (§2.1.2) and approximate second-order nonprojective parsing, and MaltParser (Nivre et al., 2006), which is a state-of-the-art transition-based parser.2 We do not alter the training algorithms used in prior work for learning these two parsers from data. Using the existing parsers as starting points, we will combine them in a variety of ways. 3.2 Training Regardless of our choices for the specific parsers and learning algorithms at level 0 and level 1, training is done as ske</context>
<context position="23702" citStr="McDonald et al., 2005" startWordPosition="3941" endWordPosition="3944">of the current modifier PredHead Predicted head of the candidate modifier (if PredEdge=0) AllChildren Sequence of POS and link labels of all the predicted children of the candidate head Table 1: Feature sets derived from the level 0 parser. Subset Description A PredEdge B PredEdge+Sibling C PredEdge+Sibling+GrandParents D PredEdge+Sibling+GrandParents+PredHead E PredEdge+Sibling+GrandParents+PredHead+ AllChildren Table 2: Combinations of features enumerated in Table 1 used for stacking. A is a replication of (Nivre and McDonald, 2008), except for the modifications described in footnote 4. 162 McDonald et al., 2005b; McDonald and Pereira, which suggests that a separate regularization of the first-order and stacked features might be beneficial in a stacking framework. As a side note, if we set each component of the weight vector to one, we obtain a bound on the `1-norm of the feature vector difference, � � �� ��f(x, y) − f(x, y) 1 G 2nL(y, �y)Nf,2. MSTParser and the tran sition-based MaltParser. forms parsing an d labeling jointly. We adapted this system to first perform unlabeled parsing, then label the arcs using alog-linear classifier with access to the full unlabeled parse (McDonald et al., 2005a; 5 </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP.</booktitle>
<contexts>
<context position="1546" citStr="McDonald et al., 2005" startWordPosition="225" endWordPosition="228"> features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and </context>
<context position="8405" citStr="McDonald et al., 2005" startWordPosition="1339" endWordPosition="1342">hrough he use of linea time hift-ruce pag algorithms (Yamada an Matbased parsers model the sequnce of decisions of lsson, 2005). n the second category are those umoto, 2003; Nivre and Scholz, 2004; Nivre and t employ exhaustiv inference algorithms, uua shift-reduce parser, given previous decisions and current state, ad paring is perfed by greedily ilsson, 2005). I he second ategory ar thoe y by making song independence assumptions, as coosing the highest scoig transition out of each the case for edge-factoed modls (Paskin, 200; at employ exhaustive nference algorithms, uucDonald etal., 2005a; McDonald et al., 2005b). ly by making stro ndependence assumptions as successive parsing state or by searching for the best ecently thee havealso be prposals for xhau sequence of transitions (Ratnaparkhi et al., 1994; the case for edge-factore models (Paskn, 200 Yamada and Matsumoto, 2003; Nivre et al., 2004; e mo wea cDonald et a, 2005a; McDonald et al., 2005) e dgefaore up on ldng bh pproximae thds (McD ecently there have also been proposas for ex Sagae and Lavie, 2005; Hall et al., 2006). d nd Pei 2006) d x methods that weaken the edgefactored assump hds hgh Both approaches most commonly use linear els to assig</context>
<context position="10511" citStr="McDonald et al. (2005" startWordPosition="1697" endWordPosition="1700">et that includes non-lcl fatures partially adverted by incorporating rich feature pThe goal of this work is to further our current shown here to improve erformnc), and esentaons over the input (McDonald e al., 20 nderstanding of the computational natue of pjec pag g • a variety of stackng archtectures. The goal of this work is to furt m if hi h ddi i Usg ack with rich featurs, we obtai resuts projective parsing algorithms for both learning and investigating and extendng the edge-factored model comptitiv ith Nivre an McDonald (2008) while inference within the data-driven setting We start by of McDonald et al. (2005b). In particular, we appreserving the fast quaratic parsing im of arcinvesigaing and extending the edge-factored model peal to the Matrix Tree Theorem for multidigraphs factored spanning tree algorithms. of McDonald et al. (2005b). In p to desgn polynmiatime algori The paper is oganizd as follows. We discuss repeal to the Matrix Tree Theorem for multi-digraphs ing h h po fution edg ep to design polynomial-tim algorithms for calculatios ove ll pssibl dpedy gaph r given lated prior work on dependency parsing and stacking in §2. Our modelais givenuint§3. Annovel analysisaof stacking inrlinear mo</context>
<context position="11843" citStr="McDonald et al. (2005" startWordPosition="1915" endWordPosition="1919">e problems including min-r y We briefly review work on the NLP task of depenels syntctic language modeling and unsuprvised dency parsing and the machine learning framework known as stacked learning. (r u ent. In the proje h modthat a and a 158 §2.1.2). Recently, Nivre and McDonald (2008) proposed combining a graph-based and a transitionbased parser and have shown a significant improvement for several languages by letting one of the parsers “guide” the other. Our stacked formalism (to be described in §3) generalizes this approach. 2.1.2 Arc factorization In the successful graph-based method of McDonald et al. (2005b), an arc factorization independence assumption is used to ensure tractability. This assumption forbids any feature that depends on two or more arcs, permitting only “arc-factored” features (i.e. features that depend only on a single candidate arc a E Ay and on the input sequence x). This induces a decomposition of the feature vector f(x, y) as: f(x, y) = � fa(x). aEAy Parsing amounts to solving arg maxyEY(x) wTf(x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and wi</context>
<context position="15603" citStr="McDonald et al., 2005" startWordPosition="2524" endWordPosition="2527">tputs the set of predicted edges that make up its estimation of the dependency tree, y0 = g(x). At level 1, we apply a dependency parser—in this work, always a graph-based dependency parser—that uses basic factored features plus new ones from the edges predicted by the level 0 parser. The final parser predicts parse trees as h(x, g(x)), so that the total runtime is additive in calculating h(·) and g(·). The stacking framework is agnostic about the form of g and h and the methods used to learn them from data. In this work we use two well-known, publicly available dependency parsers, MSTParser (McDonald et al., 2005b),1 which implements ex1http://sourceforge.net/projects/mstparser 159 act first-order arc-factored nonprojective parsing (§2.1.2) and approximate second-order nonprojective parsing, and MaltParser (Nivre et al., 2006), which is a state-of-the-art transition-based parser.2 We do not alter the training algorithms used in prior work for learning these two parsers from data. Using the existing parsers as starting points, we will combine them in a variety of ways. 3.2 Training Regardless of our choices for the specific parsers and learning algorithms at level 0 and level 1, training is done as ske</context>
<context position="23702" citStr="McDonald et al., 2005" startWordPosition="3941" endWordPosition="3944">of the current modifier PredHead Predicted head of the candidate modifier (if PredEdge=0) AllChildren Sequence of POS and link labels of all the predicted children of the candidate head Table 1: Feature sets derived from the level 0 parser. Subset Description A PredEdge B PredEdge+Sibling C PredEdge+Sibling+GrandParents D PredEdge+Sibling+GrandParents+PredHead E PredEdge+Sibling+GrandParents+PredHead+ AllChildren Table 2: Combinations of features enumerated in Table 1 used for stacking. A is a replication of (Nivre and McDonald, 2008), except for the modifications described in footnote 4. 162 McDonald et al., 2005b; McDonald and Pereira, which suggests that a separate regularization of the first-order and stacked features might be beneficial in a stacking framework. As a side note, if we set each component of the weight vector to one, we obtain a bound on the `1-norm of the feature vector difference, � � �� ��f(x, y) − f(x, y) 1 G 2nL(y, �y)Nf,2. MSTParser and the tran sition-based MaltParser. forms parsing an d labeling jointly. We adapted this system to first perform unlabeled parsing, then label the arcs using alog-linear classifier with access to the full unlabeled parse (McDonald et al., 2005a; 5 </context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings CoNLL.</booktitle>
<contexts>
<context position="26172" citStr="McDonald et al., 2006" startWordPosition="4350" endWordPosition="4353">experiments, the number of part feature.4 MST1O MST2O Bikel’s randomized iterations.7 itions used to create D is L = 2. 5.2 Experiment: Our first experiment stacks the highly accurate parser with itself. At level 0, the parser uses only the standard features (§5.1), and at level 1, these are augmented by vari MST2O+ MST2O MST2O ous subsets of features of x along with the output of the level 0 parser, g(x) (Table 2). The results are shown in Table 3. While we see improvements over the single-parser baseline made other modifications to MSTParser, implementing many of the successes described by (McDonald et al., 2006). Our version of the code is publicly available at //www.ark.cs.cmu.edu/MSTParserStacked. The modifications included an approximation to lemmas for datasets without lemmas (three-character prefixes), and replacing morphology/word and morphology/lemma features with morphology/POS features. CoNLL-X shared task actually involves thirteen languages; our experiments do not include Czech (the largest dataset), due to time constraints. Therefore, the average results plotted in the last rows of Tables 3, 4, and 5 are not directly comparable with previously published averages over thirteen languages. 4</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating graphbased and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="633" citStr="Nivre and McDonald (2008)" startWordPosition="85" endWordPosition="88"> Dependency Parsers Andr´e F. T. Martins*† Dipanjan Das* Noah A. Smith* Eric P. Xing* *School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA †Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 Introduction In this paper we address a representation</context>
<context position="2196" citStr="Nivre and McDonald (2008)" startWordPosition="321" endWordPosition="324">n the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-h</context>
<context position="11510" citStr="Nivre and McDonald (2008)" startWordPosition="1862" endWordPosition="1865"> fution edg ep to design polynomial-tim algorithms for calculatios ove ll pssibl dpedy gaph r given lated prior work on dependency parsing and stacking in §2. Our modelais givenuint§3. Annovel analysisaof stacking inrlinear models isegiven in §4.hExperiments decod are presentedTin §5. , y gg g, 2 Background and Related Work nd inference problems including min-r y We briefly review work on the NLP task of depenels syntctic language modeling and unsuprvised dency parsing and the machine learning framework known as stacked learning. (r u ent. In the proje h modthat a and a 158 §2.1.2). Recently, Nivre and McDonald (2008) proposed combining a graph-based and a transitionbased parser and have shown a significant improvement for several languages by letting one of the parsers “guide” the other. Our stacked formalism (to be described in §3) generalizes this approach. 2.1.2 Arc factorization In the successful graph-based method of McDonald et al. (2005b), an arc factorization independence assumption is used to ensure tractability. This assumption forbids any feature that depends on two or more arcs, permitting only “arc-factored” features (i.e. features that depend only on a single candidate arc a E Ay and on the </context>
<context position="17691" citStr="Nivre and McDonald (2008)" startWordPosition="2884" endWordPosition="2887">der models. 4.1 Adding Input Features Suppose that the level 1 classifier is an arc-factored graph-based parser. The feature vectors will take the form3 f(x, y) = f1(x, y) ^ f2(x, ˆy0, y) �= f1,a(x) ^ f2,a(x,g(x)), aEAy 2http://w3.msi.vxu.se/˜jha/maltparser 3We use — to denote vector concatenation. where f1(x,y) = EaEAy f1,a(x) are regular arc-factored features, and f2(x, ˆy0, y) = EaEAy f2,a(x, g(x)) are the stacked features. An example of a stacked feature is a binary feature f2,a(x, g(x)) that fires if and only if the arc a was predicted by g, i.e., if a E Ag(x); such a feature was used by Nivre and McDonald (2008). It is difficult in general to decide whether the inclusion of such a feature yields a better parser, since features strongly correlate with each other. However, a popular heuristic for feature selection consists of measuring the information gain provided by each individual feature. In this case, we may obtain a closed-form expression for the information gain that f2,a(x, g(x)) provides about the existence or not of the arc a in the actual dependency tree y. Let A and A&apos; be binary random variables associated with the events a E Ay and a&apos; E Ag(x), respectively. We have: p(a, a&apos;) p(a, a&apos;) log2 </context>
<context position="23621" citStr="Nivre and McDonald, 2008" startWordPosition="3927" endWordPosition="3931">ents Lemma, POS, link label, distance and direction of attachment of the grandparent of the current modifier PredHead Predicted head of the candidate modifier (if PredEdge=0) AllChildren Sequence of POS and link labels of all the predicted children of the candidate head Table 1: Feature sets derived from the level 0 parser. Subset Description A PredEdge B PredEdge+Sibling C PredEdge+Sibling+GrandParents D PredEdge+Sibling+GrandParents+PredHead E PredEdge+Sibling+GrandParents+PredHead+ AllChildren Table 2: Combinations of features enumerated in Table 1 used for stacking. A is a replication of (Nivre and McDonald, 2008), except for the modifications described in footnote 4. 162 McDonald et al., 2005b; McDonald and Pereira, which suggests that a separate regularization of the first-order and stacked features might be beneficial in a stacking framework. As a side note, if we set each component of the weight vector to one, we obtain a bound on the `1-norm of the feature vector difference, � � �� ��f(x, y) − f(x, y) 1 G 2nL(y, �y)Nf,2. MSTParser and the tran sition-based MaltParser. forms parsing an d labeling jointly. We adapted this system to first perform unlabeled parsing, then label the arcs using alog-line</context>
<context position="25410" citStr="Nivre and McDonald (2008)" startWordPosition="4220" endWordPosition="4223">-factored version). All our experiments use the non-projective version of this parser. We refer to the MaltParser as Malt. We report experiments on twelve languages from the CoNLL-X shared task (Buchholz and Marsi, 2006).5 All experiments are evaluated using the labeled attachment score (LAS), using the default settings.6 Statistical significance is measured using Dan parsing evaluation comparator with 10,000 The additional features used in the level 1 parser are enumerated in Table 1 and their various subsets are depicted in Table 2. The PredEdge features are exactly the six features used by Nivre and McDonald (2008) in their MSTMalt parser; therefore, feature set A is a replication of this parser except for modifications noted in footnote 4. In all our experiments, the number of part feature.4 MST1O MST2O Bikel’s randomized iterations.7 itions used to create D is L = 2. 5.2 Experiment: Our first experiment stacks the highly accurate parser with itself. At level 0, the parser uses only the standard features (§5.1), and at level 1, these are augmented by vari MST2O+ MST2O MST2O ous subsets of features of x along with the output of the level 0 parser, g(x) (Table 2). The results are shown in Table 3. While </context>
<context position="28397" citStr="Nivre and McDonald (2008)" startWordPosition="4682" endWordPosition="4685">st concerns about this model is the fact that it stacks two predictors that are very similar in nature: both are graph-based and share the features f1,,,(x). It has been pointed out by Breiman (1996), among others, that the success of ensemble methods like stacked learning strongly depends on how uncorrelated the individual decisions made by each predictor are from the others’ decisions.8 This experiment provides further evidence for the claim. 5.3 Experiment: Malt + MST2O We next use MaltParser at level 0 and the secondorder arc-pair-factored MST2O at level 1. This extends the experiments of Nivre and McDonald (2008), replicated in our feature subset A. Table 4 enumerates the results. Note that the best-performing stacked configuration for each and every language outperforms MST 2O, corroborating results reported by Nivre and McDonald (2008). The best performing stacked configuration outperforms Malt as well, except for Japanese and Turkish. Further, our non-arc-factored features largely outperform subset A, except on Bulgarian, Chinese, 8This claim has a parallel in the cotraining method (Blum and Mitchell, 1998), whose performance is bounded by the degree of independence between the two feature sets. an</context>
<context position="31025" citStr="Nivre and McDonald (2008)" startWordPosition="5098" endWordPosition="5101">1 Dutch 78.59 79.99 80.75 81.47 81.47 81.51 81.29 German 85.82 87.44 88.16 88.50 88.56 88.68 88.38 Japanese 91.65 90.93 91.63 91.43 91.59 91.61 91.49 Portuguese 87.60 87.12 88.00 88.24 88.30 88.18 88.22 Slovene 70.30 74.02 76.62 76.00 76.60 76.18 76.72 Spanish 81.29 82.43 83.09 83.73 83.47 83.21 83.43 Swedish 84.58 82.87 84.92 84.60 84.80 85.16 84.88 Turkish 65.68 60.11 64.35 64.51 64.51 65.07 65.21 Average 80.94 81.08 82.52 82.58 82.65 82.59 82.71 Table 4: Results of stacking Malt and MST20 at level 0 and level 1, respectively. Columns 2–4 enumerate LAS for Malt, MST20 and Malt + MST2o as in Nivre and McDonald (2008). Columns 5–8 enumerate results for four other stacked feature configurations. Bold indicates best result for a language. tion A of Malt + MST20 (see Table 4), the difference being statistically insignificant (p &gt; 0.05). This shows that stacking Malt with the exact, arcfactored MST 1O bridges the difference between the individual MST 1O and MST20 models, by approximating higher order features, but maintaining an O(n2) runtime and finding the model-optimal parse. 5.5 Disagreement as a Confidence Measure In pipelines or semisupervised settings, it is useful when a parser can provide a confidence</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating graphbased and transition-based dependency parsers. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="8693" citStr="Nivre et al., 2004" startWordPosition="1384" endWordPosition="1387">cisions and current state, ad paring is perfed by greedily ilsson, 2005). I he second ategory ar thoe y by making song independence assumptions, as coosing the highest scoig transition out of each the case for edge-factoed modls (Paskin, 200; at employ exhaustive nference algorithms, uucDonald etal., 2005a; McDonald et al., 2005b). ly by making stro ndependence assumptions as successive parsing state or by searching for the best ecently thee havealso be prposals for xhau sequence of transitions (Ratnaparkhi et al., 1994; the case for edge-factore models (Paskn, 200 Yamada and Matsumoto, 2003; Nivre et al., 2004; e mo wea cDonald et a, 2005a; McDonald et al., 2005) e dgefaore up on ldng bh pproximae thds (McD ecently there have also been proposas for ex Sagae and Lavie, 2005; Hall et al., 2006). d nd Pei 2006) d x methods that weaken the edgefactored assump hds hgh Both approaches most commonly use linear els to assign scores to arcseor decisions, so score is a dot-product of a feature vector f learned weight vector w. rabd gor (wa6). sysems notable exceptions include te work In sum, these two lines of research use different gama bed odel he has be lim approximations to achieve tracabiliy. Transition</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryiˇgit</author>
<author>S Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Nivre, Hall, Nilsson, Eryiˇgit, Marinov, 2006</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryiˇgit, and S. Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>A maximum entropy model for parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of ICSLP.</booktitle>
<contexts>
<context position="1835" citStr="Ratnaparkhi et al., 1994" startWordPosition="264" endWordPosition="267">dress a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specif</context>
<context position="8600" citStr="Ratnaparkhi et al., 1994" startWordPosition="1369" endWordPosition="1372">2004; Nivre and t employ exhaustiv inference algorithms, uua shift-reduce parser, given previous decisions and current state, ad paring is perfed by greedily ilsson, 2005). I he second ategory ar thoe y by making song independence assumptions, as coosing the highest scoig transition out of each the case for edge-factoed modls (Paskin, 200; at employ exhaustive nference algorithms, uucDonald etal., 2005a; McDonald et al., 2005b). ly by making stro ndependence assumptions as successive parsing state or by searching for the best ecently thee havealso be prposals for xhau sequence of transitions (Ratnaparkhi et al., 1994; the case for edge-factore models (Paskn, 200 Yamada and Matsumoto, 2003; Nivre et al., 2004; e mo wea cDonald et a, 2005a; McDonald et al., 2005) e dgefaore up on ldng bh pproximae thds (McD ecently there have also been proposas for ex Sagae and Lavie, 2005; Hall et al., 2006). d nd Pei 2006) d x methods that weaken the edgefactored assump hds hgh Both approaches most commonly use linear els to assign scores to arcseor decisions, so score is a dot-product of a feature vector f learned weight vector w. rabd gor (wa6). sysems notable exceptions include te work In sum, these two lines of resear</context>
</contexts>
<marker>Ratnaparkhi, Roukos, Ward, 1994</marker>
<rawString>A. Ratnaparkhi, S. Roukos, and R. T. Ward. 1994. A maximum entropy model for parsing. In Proceedings of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2849" citStr="Riedel and Clarke, 2006" startWordPosition="416" endWordPosition="419">reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, Honolulu, October 2008.c�2008 Association for Computational Linguistics Figure 1: A projective dependency arse (top), and a nonFigure 2: Nonprojective depe</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>S. Riedel and J. Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="1858" citStr="Sagae and Lavie, 2005" startWordPosition="268" endWordPosition="271">iciency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked</context>
<context position="8859" citStr="Sagae and Lavie, 2005" startWordPosition="1416" endWordPosition="1419">hest scoig transition out of each the case for edge-factoed modls (Paskin, 200; at employ exhaustive nference algorithms, uucDonald etal., 2005a; McDonald et al., 2005b). ly by making stro ndependence assumptions as successive parsing state or by searching for the best ecently thee havealso be prposals for xhau sequence of transitions (Ratnaparkhi et al., 1994; the case for edge-factore models (Paskn, 200 Yamada and Matsumoto, 2003; Nivre et al., 2004; e mo wea cDonald et a, 2005a; McDonald et al., 2005) e dgefaore up on ldng bh pproximae thds (McD ecently there have also been proposas for ex Sagae and Lavie, 2005; Hall et al., 2006). d nd Pei 2006) d x methods that weaken the edgefactored assump hds hgh Both approaches most commonly use linear els to assign scores to arcseor decisions, so score is a dot-product of a feature vector f learned weight vector w. rabd gor (wa6). sysems notable exceptions include te work In sum, these two lines of research use different gama bed odel he has be lim approximations to achieve tracabiliy. TransitionWang and Harper (2004). Theoretical studes of ork n piial ym f pjctive ps based approaches solve equence of local probte include the work of Neuhaus and Boker (1997) </context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>K. Sagae and A. Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3116" citStr="Smith and Eisner (2008)" startWordPosition="457" endWordPosition="460"> non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, Honolulu, October 2008.c�2008 Association for Computational Linguistics Figure 1: A projective dependency arse (top), and a nonFigure 2: Nonprojective dependency graph projective dependency parse (bottom) for two English those that assume each dependency decsion is in sentences; examples from McDonald and Satta (2007). ddt modl th lbl ttl ntint p (2008). els a tht d b u d Teis paper focuses on dependencydparsing,ewhich</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D. A. Smith and J. Eisner. 2008. Dependency parsing by belief propagation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>A latent variable model for generative dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="2905" citStr="Titov and Henderson, 2007" startWordPosition="424" endWordPosition="427">on scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, Honolulu, October 2008.c�2008 Association for Computational Linguistics Figure 1: A projective dependency arse (top), and a nonFigure 2: Nonprojective dependency graph projective dependency parse (bottom) for tw</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>I. Titov and J. Henderson. 2007. A latent variable model for generative dependency parsing. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>N A Smith</author>
<author>T Mitamura</author>
</authors>
<title>What is the Jeopardy model? A quasi-synchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? A quasi-synchronous grammar for QA. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural Networks,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="1348" citStr="Wolpert, 1992" startWordPosition="194" endWordPosition="195">ample of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subj</context>
<context position="13309" citStr="Wolpert (1992)" startWordPosition="2139" endWordPosition="2140"> weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features. 2.2 Stacked Learning Stacked generalization was first proposed by Wolpert (1992) and Breiman (1996) for regression. The idea is to include two “levels” of predictors. The first level, “level 0,” includes one or more predictors g1, ... , gK : Rd _* R; each receives input x E Rd and outputs a prediction gk(x). The second level, “level 1,” consists of a single function h : Rd+K _* R that takes as input (x, g1(x),... gK(x)) and outputs a final prediction y� = h(x, g1(x),... gK(x)). The predictor, then, combines an ensemble (the gk) with a meta-predictor (h). Training is done as follows: the training data are split into L partitions, and L instances of the level 0 predictor ar</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>D. Wolpert. 1992. Stacked generalization. Neural Networks, 5(2):241–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="8673" citStr="Yamada and Matsumoto, 2003" startWordPosition="1380" endWordPosition="1383">ce parser, given previous decisions and current state, ad paring is perfed by greedily ilsson, 2005). I he second ategory ar thoe y by making song independence assumptions, as coosing the highest scoig transition out of each the case for edge-factoed modls (Paskin, 200; at employ exhaustive nference algorithms, uucDonald etal., 2005a; McDonald et al., 2005b). ly by making stro ndependence assumptions as successive parsing state or by searching for the best ecently thee havealso be prposals for xhau sequence of transitions (Ratnaparkhi et al., 1994; the case for edge-factore models (Paskn, 200 Yamada and Matsumoto, 2003; Nivre et al., 2004; e mo wea cDonald et a, 2005a; McDonald et al., 2005) e dgefaore up on ldng bh pproximae thds (McD ecently there have also been proposas for ex Sagae and Lavie, 2005; Hall et al., 2006). d nd Pei 2006) d x methods that weaken the edgefactored assump hds hgh Both approaches most commonly use linear els to assign scores to arcseor decisions, so score is a dot-product of a feature vector f learned weight vector w. rabd gor (wa6). sysems notable exceptions include te work In sum, these two lines of research use different gama bed odel he has be lim approximations to achieve tr</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>