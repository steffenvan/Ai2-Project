<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.993588">
Taxonomy Induction Using Hierarchical Random Graphs
</title>
<author confidence="0.9955">
Trevor Fountain and Mirella Lapata
</author>
<affiliation confidence="0.999529">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.99381">
10 Crichton Street, Edinburgh EH8 9AB
</address>
<email confidence="0.997528">
t.fountain@sms.ed.ac.uk, mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999569823529412">
This paper presents a novel approach for in-
ducing lexical taxonomies automatically from
text. We recast the learning problem as
that of inferring a hierarchy from a graph
whose nodes represent taxonomic terms and
edges their degree of relatedness. Our model
takes this graph representation as input and
fits a taxonomy to it via combination of a
maximum likelihood approach with a Monte
Carlo Sampling algorithm. Essentially, the
method works by sampling hierarchical struc-
tures with probability proportional to the like-
lihood with which they produce the input
graph. We use our model to infer a taxonomy
over 541 nouns and show that it outperforms
popular flat and hierarchical clustering algo-
rithms.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966884615385">
The semantic knowledge encoded in lexical re-
sources such as WordNet (Fellbaum, 1998) has been
proven beneficial for several applications including
question answering (Harabgiu et al., 2003), doc-
ument classification (Hung et al., 2004), and tex-
tual entailment (Geffet and Dagan, 2005). As the
effort involved in creating such resources manu-
ally is prohibitive (cost, consistency and coverage
are often cited problems) and has to be repeated
for new languages or domains, recent years have
seen increased interest in automatic taxonomy in-
duction. The task has assumed several guises, such
as term extraction — finding the concepts of the
taxonomy (Kozareva et al., 2008; Navigli et al.,
2011), term relation discovery — learning whether
any two terms stand in an semantic relation such as
IS-A, or PART-OF (Hearst, 1992; Berland and Char-
niak, 1999), and taxonomy construction —- creat-
ing the taxonomy proper by organizing its terms hi-
erarchically (Kozareva and Hovy, 2010; Navigli et
al., 2011). Previous work has also focused on the
complementary task of augmenting an existing tax-
onomy with missing information (Snow et al., 2006;
Yang and Callan, 2009).
In this paper we propose an unsupervised ap-
proach to taxonomy induction. Given a corpus and
a set of terms, our algorithm jointly induces their re-
lations and their taxonomic organization. We view
taxonomy learning as an instance of the problem
of inferring a hierarchy from a network or graph.
We create this graph from unstructured text simply
by drawing an edge between distributionally sim-
ilar terms. Next, we fit a Hierarchical Random
Graph model (HRG; Clauset et al. (2008)) to the
observed graph data based on maximum likelihood
methods and Markov chain Monte Carlo sampling.
The model essentially works by sampling hierarchi-
cal structures with probability proportional to the
likelihood with which they produce the input graph.
This is advantageous as it allows us to consider the
ensemble of random graphs that are statistically sim-
ilar to the original graph, and through this to de-
rive a consensus hierarchical structure from the en-
semble of sampled models. The approach differs
crucially from hierarchical clustering in that it ex-
plicitly acknowledges that most real-world networks
have many plausible hierarchical representations of
roughly equal likelihood and does not seek a sin-
gle hierarchical representation for a given network.
This feature also bodes well with the nature of lexi-
cal taxonomies: there is no uniquely correct taxon-
omy for a set of terms, rather different taxonomies
</bodyText>
<page confidence="0.661280333333333">
466
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 466–476,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999812117647059">
are likely to be appropriate for different tasks and
different taxonomization criteria.
Our contributions in this paper are three-fold: we
adapt the HRG model to the taxonomy induction
task and show that its performance is superior to al-
ternative methods based on either flat or hierarchi-
cal clustering; we analyze the requirements of the
algorithm with respect to the input graph and the
semantic representation of its nodes; and introduce
new ways of evaluating the fit of an automatically
induced taxonomy against a gold-standard. In the
following section we provide an overview of related
work. Next, we describe our HRG model in more
detail (Section 3) and present the resources and eval-
uation methodology used in our experiments (Sec-
tion 4). We conclude the paper by presenting and
discussing our results (Sections 4.1–4.4).
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999885935897436">
The bulk of previous work has focused on term re-
lation discovery following essentially two method-
ological paradigms, pattern-based bootstrapping and
clustering. The former approach (Hearst, 1992;
Roark and Charniak, 1998; Berland and Charniak,
1999; Girju et al., 2003; Etzioni et al., 2005;
Kozareva et al., 2008) utilizes a few hand-crafted
seed patterns representative of taxonomic relations
(e.g., IS-A, PART-OF, SIBLING) to extract instances
from corpora. These instances are then used to ex-
tract new patterns which are in turn used to find new
instances and so on. Clustering-based approaches
have been mostly employed to discover IS-A and
SIBLING relations (Lin, 1998; Caraballo, 1999; Pan-
tel and Ravichandran, 2004). A common assump-
tion is that words are related if they occur in similar
contexts and thus clustering algorithms group words
together if they share contextual features. Most of
these algorithms aim at inducing flat clusters rather
than taxonomies, with the exception of Brown et al.
(1992) whose method induces binary trees.
Contrary to the plethora of algorithms developed
for relation discovery, methods dedicated to taxon-
omy learning have been few and far between. Cara-
ballo (1999) was the first to induce a taxonomy
from a corpus using a combination of clustering and
pattern-based methods. Specifically, nouns are orga-
nized into a tree using a bottom-up clustering algo-
rithm and internal nodes of the resulting tree are la-
beled with hypernyms from the nouns clustered un-
derneath using patterns such as “B is a kind of A”.
Kozareva et al. (2008) and Navigli et al. (2011)
both develop systems that create taxonomies end-
to-end, i.e., discover the terms, their relations, and
how these are hierarchically organized. The two ap-
proaches are conceptually similar: they both use the
web and pattern-based methods for finding domain-
specific terms. Additionally, in both approaches the
acquired knowledge is represented as a graph from
which a taxonomy is induced using task-specific al-
gorithms such as graph pruning, edge weighting,
and so on.
Our work also addresses taxonomy learning, how-
ever, without the term discovery step — we assume
we are given the terms for which to create a taxon-
omy. Similarly to Kozareva et al. (2008) and Nav-
igli et al. (2011), our model operates over a graph
whose nodes represent terms and edges their rela-
tionships. We construct this graph from a corpus
simply by taking account of the distributional sim-
ilarity of the terms in question. Our taxonomy in-
duction algorithm is conceptually simpler and more
general; it fits a taxonomy to the observed network
data using the tools of statistical inference, combin-
ing a maximum likelihood approach with a Monte
Carlo Sampling algorithm. The technique allows us
to sample hierarchical random graphs with probabil-
ity proportional to the likelihood that they generate
the observed network. The induction algorithm can
operate over any kind of (undirected) graph, and thus
does not have to be tuned specifically for different
inputs. We should also point out that our formula-
tion of the inference problem utilizes very little cor-
pus external knowledge other than the set of input
terms, and could thus be easily applied to domains
or languages where lexical resources are scarce.
The Hierarchical Random Graph model (Clauset
et al., 2008) has been applied to construct hierarchi-
cal decompositions from three sets of network data:
a bacterial metabolic network; a food-web among
grassland species; and the network of associations
among terrorist cells. The only language-related ap-
plication we are aware of concerns word sense in-
duction. Klapaftis and Manandhar (2010) create a
graph of contexts for a polysemous target word and
use the HRG to organize them hierarchically, under
the assumption that different tree heights correspond
to different levels of sense granularity.
</bodyText>
<page confidence="0.999474">
467
</page>
<figureCaption confidence="0.984381">
Figure 1: Flow of information through the Hierarchical Random Graph algorithm. From a semantic net-
</figureCaption>
<bodyText confidence="0.6857778">
work (1a), the model constructs a binary tree (1b). Edges in the semantic network are then used to compute
the θ parameters for internal nodes in the tree; the maximum-likelihood-estimated θ parameter for an internal
node indicates the density of edges between its children. This tree is then resampled using the θ parameters
(1b) until the MCMC process converges, at which point it can be collapsed into a n-ary hierarchy (1c). The
same collapsing process can be also used to identify a flat clustering (1d).
</bodyText>
<figure confidence="0.999970173913044">
(a) Input graph
(b) Binary tree
(c) Hierarchy
(d) Clusters
0.11
F
C
A
D E
B
1.00
1.00 C
A B
E F
0.50
1.00
D
C A B
E F
D
A B
C D
E F
</figure>
<figureCaption confidence="0.959973">
Figure 2: Any internal node with subtrees A, B and
C can be permuted to one of two possible alter-
nate configurations. Shaded nodes represent internal
nodes which are unmodified by such permutation.
</figureCaption>
<sectionHeader confidence="0.964966" genericHeader="method">
3 The Hierarchical Random Graph Model
</sectionHeader>
<bodyText confidence="0.998414923076923">
A HRG consists of a binary tree and a set of likeli-
hood parameters, and operates on input organized
into a semantic network, an undirected graph in
which nodes represent terms and edges between
nodes indicate a relationship between pairs of terms
(Figure 1a). From this representation, the model
constructs a binary tree whose leaves correspond
to nodes in the semantic network (Figure 1b); the
model then employs a simple Markov chain Monte
Carlo (MCMC) process in order to explore the space
of possible binary trees and derives a consensus hi-
erarchical structure from the ensemble of sampled
models (Figure 1c).
</bodyText>
<subsectionHeader confidence="0.999856">
3.1 Representing a Hierarchical Structure
</subsectionHeader>
<bodyText confidence="0.999597625">
Formally, we denote a semantic network S = (V,E),
where V = {v1,v2 ...vn} is the set of vertices, one
per term, and E is the set of edges between terms
in which Ea,b indicates the presence of an edge be-
tween va and vb.
Given a network S, we construct a binary tree D
whose n leaves correspond to V and whose n − 1
internal nodes denote a hierarchy over V. Because
the leaves remain constant for a given S, we define
D as the set of internal nodes D = {D1,D2 ...Dn}
and associate each edge Ea,b ∈ E with an internal
node Di being the lowest common parent of a,b ∈ V.
The core assumption underlying the HRG model is
that edges in S have a non-uniform and independent
probability of existing. Each possible edge Ea,b ∈ E
exists with a probability θi, where θi is associated
with the corresponding internal node Di.
For a given internal node Di, let Li and Ri be the
number of leaves in Di’s left and right subtrees, re-
spectively; let Ei be the number of edges in E asso-
ciated with Di (colloquially, the number of edges in
S between leaves in Di’s left and right subtrees). For
each Di ∈ D, we can estimate the maximum likeli-
hood for the corresponding θi as θi = Ei . The like-
</bodyText>
<equation confidence="0.91625075">
LiRi
lihood L(D,θ|S) of a HRG over a given semantic
network S is then given by:
(θi)Ei(1−θi)LiRi−Ei (1)
</equation>
<subsectionHeader confidence="0.998006">
3.2 Markov Chain Monte Carlo Sampling
</subsectionHeader>
<bodyText confidence="0.99990825">
Given a representation for a HRG H (D,θ) and a
method for estimating the likelihood of a given D
and θ, we can focus on obtaining the binary tree D
which best fits (or most plausibly explains) a given
semantic network. Because the space of possible bi-
nary trees over V is super-exponential with respect
to |V|, we employ a MCMC process to sample from
the space of binary trees. During each iteration of
</bodyText>
<figure confidence="0.94019525">
A C
B C A B A
B
C
</figure>
<equation confidence="0.65937475">
L(D,θ|S) =
n−1
∏
i=1
</equation>
<page confidence="0.979705">
468
</page>
<listItem confidence="0.745877333333333">
Algorithm 1: MCMC Sampling
1 Compute the likelihood L(D,θ) of the current
binary tree.
2 Pick a random internal node Di E D.
3 Randomly permute Di according to Figure 2.
4 Compute the likelihood ˆL(D,θ) of the modified
binary tree.
5 if ˆL(D,θ) &gt; L(D,θ) then
6 accept the transition;
</listItem>
<sectionHeader confidence="0.422098" genericHeader="method">
7 else
</sectionHeader>
<bodyText confidence="0.4947445">
8 accept with probability ˆL(D,θ)/L(D,θ)
(i.e., standard Metropolis acceptance).
</bodyText>
<sectionHeader confidence="0.4914445" genericHeader="method">
9 end
10 Repeat;
</sectionHeader>
<bodyText confidence="0.999939625">
this process we randomly select a node within the
tree and permute it according to Figure 2. If this
permutation improves the overall likelihood of the
dendrogram we accept it as a transition, otherwise it
is accepted with a probability proportional to the de-
gree to which it decreases the overall likelihood (i.e.
standard Metropolis acceptance). This procedure is
described in more detail in Algorithm 1.
</bodyText>
<subsectionHeader confidence="0.99963">
3.3 Consensus Hierarchy
</subsectionHeader>
<bodyText confidence="0.999902333333333">
Once the MCMC process has converged, the model
is left with a binary tree over the terms from the input
semantic network. As in standard hierarchical clus-
tering, however, this imposes an arbitrary structure
which may or may not correspond to the observed
data — the tree at convergence will be similar to an
ideal tree given the graph, but may not be the most
plausible structure. Indeed, for taxonomy induction
it is quite unlikely that a binary tree will provide the
most appropriate categorization.
To avoid encoding such bias we employ a
model averaging technique to produce a consen-
sus hierarchy. For a set of binary trees sam-
pled after convergence, we first identify the set
of possible clusters encoded in the tree, e.g., the
binary tree in Figure 1b encodes the clusters
{AB,ABC,EF,D,DEF,ABCDEF}. As in Clauset et
al. (2008), each cluster instance is then weighted
according to the likelihood of the originating HRG
(Equation 1); we then sum the weights for each dis-
tinct cluster across all resampled trees and discard
those whose aggregate weight is lower than 50% of
the total observed weight. The remaining clusters
are then used to reconstruct a hierarchy in which
</bodyText>
<figure confidence="0.584921125">
Algorithm 2: Flat Clusters
1 Let Dk be the root node of D.
2 if θk &gt; θ¯ then
3 output the leaves of the subtree rooted at Dk
as a cluster
4 else
5 repeat 2 with left and right children of Dk.
6 end
</figure>
<bodyText confidence="0.994993666666667">
each subtree appears in the majority of trees ob-
served after the sampling process has reached con-
vergence, hence the term consensus hierarchy.
</bodyText>
<subsectionHeader confidence="0.993693">
3.4 Obtaining Flat Clusters
</subsectionHeader>
<bodyText confidence="0.9999605">
For evaluation purposes we may want to compare
the groupings created by the HRG to a simpler non-
hierarchical clustering algorithm (see Section 4 for
details). We thus defined a method of converting the
tree produced by the HRG into a flat (hard) clus-
tering. This can be done in a relatively straightfor-
ward, principled fashion using the HRG’s θ parame-
ters. For a given H(D,θ) we identify internal nodes
whose θk likelihood is greater than the mean likeli-
hood and who possess no parent node whose θk like-
lihood is also greater than the mean. Each such node
is the root of a densely-connected subtree; each such
subtree is then assumed to represent a single discrete
cluster of related items, where θ¯ = mean(θ) (illus-
trated in Figure 1c). This procedure is explained in
greater detail in Algorithm 2.
</bodyText>
<sectionHeader confidence="0.998705" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999874176470588">
Data We evaluated our taxonomy induction algo-
rithm using McRae et al.’s (2005) dataset which
consists of for 541 basic level nouns (e.g., DOG
and TABLE). Each noun is associated with features
(e.g., has-legs, is-flat, and made-of-wood for TABLE)
collected from human participants in multiple stud-
ies over several years. The original norming study
does not include class labels for these nouns, how-
ever, we were able to exploit a clustering provided
by Fountain and Lapata (2010), in which a set of on-
line participants annotated each of the McRae et al.
nouns with basic category labels.
The nouns and their class labels were further tax-
onomized using WordNet (Fellbaum, 1998). Specif-
ically, we first identified the full hypernym path in
WordNet for each noun in McRae et al.’s (2005)
dataset, e.g., APPLE &gt; PLANT STRUCTURE &gt; NAT-
</bodyText>
<page confidence="0.998452">
469
</page>
<bodyText confidence="0.9993275625">
URAL OBJECT &gt; PHYSICAL OBJECT &gt; ENTITY (a
total of 493 concepts appear in both). These hyper-
nym paths were then combined to yield a full tax-
onomy over McRae et al.’s nouns; internal nodes
having only a single child were recursively removed
to produce a final, compact taxonomy1 containing
186 semantic classes (e.g., ANIMALS, WEAPONS,
FRUITS) organized into varying levels of granular-
ity (e.g., SONGBIRDS &gt; BIRDS &gt;ANIMALS).
Evaluation measures Evaluation of taxonomi-
cally organized information is notoriously hard (see
Hovy (2002) for an extensive discussion on this
topic). This is due to the nature of the task which is
inherently subjective and application specific (e.g., a
dolphin can be a Mammal to a biologist, but a Fish
to a fisherman or someone visiting an aquarium).
Nevertheless, we assessed the taxonomies produced
by the HRG against the WordNet-like taxonomy de-
scribed above using two measures, one that sim-
ply evaluates the grouping of the nouns into classes
without taking account of their position in the taxon-
omy and one which evaluates the taxonomy directly.
To evaluate a flat clustering into classes we use
the F-score measure introduced in the SemEval 2007
task (Agirre and Soroa, 2007); it is the harmonic
mean of precision and recall defined as the number
of correct members of a cluster divided by the num-
ber of items in the cluster and the number of items
in the gold-standard class, respectively. Although
informative, evaluation based solely on F-score puts
the HRG model at a comparative disadvantage as the
task of taxonomy induction is significantly more dif-
ficult than simple clustering. To overcome this dis-
advantage we propose an automatic method of eval-
uating taxonomies directly by first computing the
walk distance between pairs of terms that share a
gold-standard category label within a gold-standard
and a candidate taxonomy, and then computing the
pairwise correlation between distances in each tree
(Lapointe, 1995). This captures the intuition that
a ‘good’ hierarchy is one in which items appearing
near one another in the gold taxonomy also appear
near one another in the induced one. It is also con-
ceptually similar to the task-based IS-A evaluation
(Snow et al., 2006) which has been traditionally used
to evaluate taxonomies.
Formally, let G = {g0,1,g0,2 ...gn,n−1}, where ga,b
indicates the walk distance between terms a and b
</bodyText>
<footnote confidence="0.994115">
1The taxonomy and flat cluster labels are available from
http://homepages.inf.ed.ac.uk/s0897549/data.
</footnote>
<bodyText confidence="0.9997868">
in the gold standard hierarchy. Similarly, let C =
{c0,1,c0,2 ...cn,n−1}, where ca,b is the distance be-
tween a and b in the candidate hierarchy. The tree-
height correlation between G and C is then given
by Spearman’s p correlation coefficient between the
two sets. All tree-height correlations reported in
our experiments were computed using the WordNet-
based gold-standard taxonomy over McRae et al.’s
(2005) nouns.
Baselines We compared the HRG output against
three baselines. The first is Chinese Whispers (CW;
Biemann (2006)), a randomized graph-clustering al-
gorithm which like the HRG also takes as input a
graph with weighted edges. It produces a hard (flat)
clustering over the nodes in the graph, where the
number of clusters is determined automatically. Our
second baseline is Brown et al.’s (1992) agglomer-
ative clustering algorithm that induces a mapping
from word types to classes. It starts with K classes
for the K most frequent word types and then pro-
ceeds by alternately adding the next most frequent
word to the class set and merging the two classes
which result in the least decrease in the mutual in-
formation between class bigrams. The result is a
class hierarchy with word types at the leaves. Ad-
ditionally, we compare against standard agglomer-
ative clustering (Sokal and Michener, 1958) which
produces a binary dendrogram in a bottom-up fash-
ion by recursively identifying concepts or clusters
with the highest pairwise similarity.
In the following, we present our taxonomy induc-
tion experiments (Sections 4.1–4.3). Since HRGs
provide a means of inducing a hierarchy over a
graph-based representation, which may be con-
structed in an arbitrary fashion, our experiments
were designed to investigate how the topology and
quality of the input graph influences the algorithm’s
performance. We thus report results when the se-
mantic network is created from data sources of vary-
ing quality and granularity.
</bodyText>
<subsectionHeader confidence="0.8592725">
4.1 Experiment 1: Taxonomy Induction from
Feature Norms
</subsectionHeader>
<bodyText confidence="0.999886857142857">
Method We first considered the case where the in-
put graph is of high semantic quality and constructed
a semantic network from the feature norms collected
by McRae et al. (2005). Each noun was represented
as a vector with dimensions corresponding to the
possible features generated by participants of the
norming study; the value of a term along a dimen-
</bodyText>
<page confidence="0.990736">
470
</page>
<table confidence="0.9963185">
Method F-score Tree Correlation
HRG 0.507 0.168
CW 0.464 —
Agglo 0.352 0.137
</table>
<tableCaption confidence="0.987189">
Table 1: Cluster F-score and tree-height correla-
</tableCaption>
<bodyText confidence="0.970337918918919">
tion evaluation; a semantic network constructed over
McRae et al.’s (2005) nouns and features is given as
input to the algorithms.
sion was taken to be the frequency with which par-
ticipants generated the corresponding feature when
given the term. For each pair of terms an edge was
added to the semantic network if the cosine similar-
ity between their vector representations exceeded a
fixed threshold T (set to 0.15).
The resulting network was then provided as in-
put to the HRG, which was resampled until con-
vergence. The binary tree at convergence was col-
lapsed into a hierarchy over clusters using the pro-
cedure described in Section 3.4; this hierarchy was
evaluated by computing the cluster F-score between
its constituent clusters and those of a gold-standard
(human-produced) clustering. The resulting consen-
sus hierarchy was evaluated by computing the tree-
height correlation between it and the gold-standard
(WordNet-derived) hierarchy.
Results Our results are summarized in Table 1.
We only give the tree correlation for the HRG and
agglomerative methods (Agglo) as CW does not in-
duce a hierarchical clustering. In addition, we do
not compare against Brown et al. (1992) as the in-
put to this algorithm is not vector-based. When
evaluated using F-score, the HRG algorithm pro-
duces better quality clusters compared to CW, in
addition to being able to organize them hierarchi-
cally. It also outperforms agglomerative clustering
by a large margin. A similar pattern emerges when
the HRG and Agglo are evaluated on tree correla-
tion. The taxonomies produced by the HRG are a
better fit against the WordNet-based gold standard;
the difference in performance is statistically signif-
icant (p &lt; 0.01) using a t-test (Cohen and Cohen.,
1983).
</bodyText>
<subsectionHeader confidence="0.973302">
4.2 Experiment 2: Taxonomy Induction from
the British National Corpus
</subsectionHeader>
<bodyText confidence="0.999782981132075">
Method The results of Experiment 1 can be con-
sidered as an upper bound of what can be achieved
by the HRG when the input graph is constructed
from highly accurate semantic information. Feature
norms provide detailed knowledge about meaning
which would be very difficult if not close to impos-
sible to obtain from a corpus. Nevertheless, it is
interesting to explore how well we can induce tax-
onomies using a lower quality semantic network.
We therefore constructed a network based on co-
occurrence statistics computed from the British Na-
tional Corpus (BNC, 2007) and provided the result-
ing semantic network as input to the HRG, CW,
and Agglo models; additionally, we employed the
algorithm of Brown et al. (1992) to induce a hier-
archy over the target terms directly from the cor-
pus. Unfortunately, this algorithm requires the num-
ber of desired output clusters to be specified in ad-
vance; in all trials this parameter was set to the num-
ber of clusters in the gold-standard clustering (41),
thus providing the Brown-induced clusterings with a
slight oracle advantage.
Again, nouns were represented as vectors in se-
mantic space. We used a context window of five
words on either side of the target word and 5,000
vector components corresponding to the most fre-
quent non-stopwords in the BNC. Raw frequency
counts were transformed using pointwise mutual in-
formation (PMI). An edge was added to the seman-
tic network between a pair of nouns if their simi-
larity exceeded a predefined threshold (the same as
in Experiment 1). The similarity of two nouns was
defined as the cosine distance between their corre-
sponding vectors.
The HRG algorithm was used to produce a tax-
onomy from this network and was also compared
against Brown et al. (1992). The latter induces a hi-
erarchy from a corpus directly, without the interme-
diate graph representation. All resulting taxonomies
were evaluated against gold standard flat and hierar-
chical clusterings, again as in Experiment 1.
Results Results are shown in Table 2. With regard
to flat clustering (the F-score column in the table),
the HRG has a slight advantage against CW, and
Brown et al.’s (1992) algorithm (Brown). However,
differences in performance are not statistically sig-
nificant. Agglomerative clustering is the worst per-
forming method leading to a decrease in F-score of
approximately 1.5. With regard to tree correlation,
the output of the HGRG is comparable to Brown
(the difference between the two is not statistically
significant). Both algorithms are significantly better
(p &lt; 0.01) than Agglo.
</bodyText>
<page confidence="0.994927">
471
</page>
<figure confidence="0.999296">
(a) s = 0.0 (b) s = 0.5 (c) s = 1.0
</figure>
<figureCaption confidence="0.989520333333333">
Figure 3: The original semantic network as derived from the BNC (a) and the same network re-weighted
using a flat clustering produced by CW (b). As s approaches 1.0 the network exhibits an increasingly strong
small-world property, eventually reconstructing the input clustering only (c).
</figureCaption>
<subsectionHeader confidence="0.394727">
Method F-score Tree Correlation
</subsectionHeader>
<bodyText confidence="0.910840862068966">
HRG 0.276 0.104
CW 0.274 —
Brown 0.258 0.124
Agglo 0.122 0.077
Table 2: Cluster F-score and tree-height correla-
tion evaluation for taxonomies inferred over McRae
et al.’s (2005) nouns; all algorithms are run on the
BNC.
Performance of the HRG is better when the se-
mantic network is based on feature norms (compare
Tables 1 and 2), both in terms of tree-height correla-
tion and F-score. This suggests that the algorithm is
highly dependent on the quality of the semantic net-
work used as input. In particular, HRGs are known
to be more appropriate for so-called small-world
networks, graphs composed of densely-connected
subgraphs with relatively sparse connections be-
tween (Klapaftis and Manandhar, 2010). Indeed, in-
spection of the semantic network produced from the
BNC (see Figure 3a) shows that our corpus-derived
graph is emphatically not a small-world graph, yet
the HRG is able to recover some taxonomic infor-
mation from such a densely-connected network.
In the following experiments we first assess the
difficulty of the taxonomy induction task to get a
feel of how well the algorithms are performing in
comparison to humans and then investigate ways of
rendering the BNC-based graph more similar to a
small-world network.
</bodyText>
<subsectionHeader confidence="0.99818">
4.3 Experiment 3: Human Upper Bound
</subsectionHeader>
<bodyText confidence="0.999975387096774">
Method The previous experiments evaluated the
performance of the HRG against a gold-standard
hierarchy derived from WordNet. For any set of
concepts there will exist multiple valid taxonomies,
each representing an accurate if differing organiza-
tion of identical concepts using different criteria;
for the set of concepts used in Experiments 1–2 the
WordNet hierarchy represents merely one of many
valid hierarchies. Noting this, it is interesting to ex-
plore how well the hierarchies output by the model
fit within the set of possible, valid taxonomies over
a given set of concepts.
We thus conducted an experiment in which hu-
man participants were asked to organize words into
arbitrary hierarchies. To render the task feasible,
they were given a small subset of 12 words rather
than the full set of 541 nouns over which the HRG
operates. We first selected a sub-hierarchy of the
WordNet tree (‘living things’) along with its subtrees
(e.g., ‘animals’, ‘plants’), and chose target concepts
from within these trees in order to produce a tax-
onomy in which some items were differentiated at
a high level (e.g., ‘python’ vs. ‘dog’) and others at
a fine-grained level (e.g., ‘lion’ vs ‘tiger’). The ex-
periment was conducted using Amazon Mechanical
Turk2, and involved 41 participants, all self-reported
native English speakers. No guidelines as to what
features participants were to use when organizing
these concepts were provided. Participants were pre-
sented with a web-based, graphical, mouse-driven
interface for constructing a taxonomy over the cho-
</bodyText>
<footnote confidence="0.993413">
2http://mturk.com
</footnote>
<page confidence="0.989726">
472
</page>
<table confidence="0.998516">
Method Tree Correlation Min Max Std
HRG 0.412 -0.039 0.799 0.166
Brown 0.181 0.006 0.510 0.121
Agglo 0.274 -0.056 0.603 0.121
Agreement 0.511 -0.109 1.000 0.267
</table>
<tableCaption confidence="0.679187857142857">
Table 3: Model performance on a subset of the
target words used in Experiments 1–2, applied to
a subset of the semantic network used in Experi-
ment 2. Instead of a WordNet-derived hierarchy,
models were evaluated against hierarchies manually
produced by participants in an online study. Tree
correlation values are means; we also report the min-
</tableCaption>
<bodyText confidence="0.966181678571429">
imum (Min), maximum (Max), and standard devia-
tion (Std) of the mean.
sen set of concepts.
To evaluate the HRG, along with the baselines
from Experiment 2, against the resulting hierarchies
we constructed a semantic network over the subset
of concepts using similarities derived from the BNC;
this network was a subgraph of that used in Exper-
iment 2. We compute inter-annotator agreement as
the mean pairwise tree-height correlation between
the hierarchies our participants produced. We also
report for each model the mean tree-height corre-
lation between the hierarchy it produced and those
created by human annotators.
Results As shown in Table 3, participants achieve
a mean pairwise tree correlation of 0.511. This in-
dicates that there is a fair amount of agreement with
respect to the taxonomic organization of the words
in question. The HRG comes close achieving a mean
tree correlation of 0.412, followed by Agglo, and
Brown. In general, we observe that the HRG man-
ages to produce hierarchies that resemble those gen-
erated by humans to a larger extent than competing
algorithms. The results in Table 3 also hint at the
fact that the taxonomy induction task is relatively
hard as participants do not achieve perfect agree-
ment despite the fact that they are asked to taxon-
omize only 12 words.
</bodyText>
<subsectionHeader confidence="0.9852435">
4.4 Experiment 4: Taxonomy Induction from a
Small-world Network
</subsectionHeader>
<bodyText confidence="0.99472575">
Method In Experiment 2 we hypothesized that a
small-world input graph would be more advanta-
geous for the HRG. In order to explore this further,
we imposed something of a small-world structure on
</bodyText>
<table confidence="0.999733">
Method F-score Tree Correlation
HRG 0.276 0.104
HRG + CW 0.291 0.161
HRG + Brown 0.255 0.173
</table>
<tableCaption confidence="0.82935475">
Table 4: Cluster F-score and tree-height correlation
evaluation for taxonomies inferred by the HRG us-
ing semantic network derived from the BNC and re-
weighted using CW and Brown.
</tableCaption>
<bodyText confidence="0.9932885625">
the BNC semantic network, using a combination of
the baseline clustering methods evaluated in Exper-
iment 2. Specifically, we first obtain a (flat) cluster-
ing using either CW or Brown, which we then use
to re-weight the BNC graph given as input to the
HRG.3 Note that, as the clustering algorithms used
are unsupervised this procedure does not introduce
any outside supervision into the overall taxonomy
induction task.
The modified weight WA,B between a pair of
terms A,B was computed according to Equation (2),
where s indicates the proportion of edge weight
drawn from the clustering, WA,B is the edge weight
in the original (BNC) semantic network, and CA,B is
a binary value indicating that A and B belong to the
same cluster (i.e., CA,B = 1 if A and B share a cluster;
</bodyText>
<equation confidence="0.9824135">
CA,B = 0 otherwise).
WA,B = (1− s)WA,B + sCA,B (2)
</equation>
<bodyText confidence="0.999971222222222">
The value of the s parameter was tuned empirically
on held-out development data and set to s = 0.4 for
both CW and Brown algorithms. Each re-weighted
network was then used as input to an HRG, and
the resulting taxonomies were evaluated in the same
manner as in Experiments 1 and 2.
Results Table 4 shows results for cluster F-score
and tree-height correlation for the HRG when us-
ing a graph derived from the BNC without any
modifications, and two re-weighted versions using
the CW and Brown clustering algorithms, respec-
tively. As can been seen, re-weighting improves
tree-height correlation substantially: HRG with CW
and Brown is significantly better than HRG on its
own (p &lt; 0.05). In the case of CW, cluster F-score
also yields a slight improvement. Interestingly,
the tree-height correlations obtained with CW and
Brown are comparable to those attained by the HRG
</bodyText>
<footnote confidence="0.9808895">
3We omit agglomerative clustering as it performed poorly
on the BNC, see Table 2.
</footnote>
<page confidence="0.997541">
473
</page>
<figure confidence="0.448856">
jacket sweater tie vest bra camisole nylons
</figure>
<figureCaption confidence="0.99822325">
Figure 4: An excerpt from a hierarchy induced by the HRG, using the BNC semantic network with Brown
re-weighting. The HRG does not provide category labels for internal nodes of the hierarchy, but subtrees
within this excerpt correspond roughly to (0) TEXTILES, (1) CLOTHING, (2) GENDERED CLOTHING, (3)
MEN’S CLOTHING, and (4) WOMEN’S CLOTHING.
</figureCaption>
<figure confidence="0.990154166666667">
0
1
bed cushion pillow sofa
bow jeans mittens veil blouse coat gown pants trousers leotards dress swimsuit shawl scarf
2
3 4
</figure>
<bodyText confidence="0.96084">
when using the human-produced feature norms (dif-
ferences in correlations are not statistically signifi-
cant). An excerpt of a HRG-induced taxonomy is
shown in Figure 4.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.985971338709678">
In this paper we have presented a novel method for
automatically inducing lexical taxonomies based on
Hierarchical Random Graphs. The approach is con-
ceptually simple, taking a graph representation as
input and fitting a taxonomy via combination of a
maximum likelihood approach with a Monte Carlo
Sampling algorithm. Importantly, the approach does
not operate on corpora directly, instead it relies on
an abstract, interim representation (a semantic net-
work) which we argue is advantageous, as it allows
to easily encode additional information in the input.
Furthermore, the model presented here is largely
parameter-free, as both the input graph and the in-
ferred taxonomy are derived empirically in an unsu-
pervised manner (minimal tuning is required when
graph re-weighting is employed, the parameter s).
Our experiments have shown that both the input
semantic network and the representation of its nodes
influence the quality of the induced taxonomy. Rep-
resenting the terms of the taxonomy as vectors in a
human-produced feature space yields more coherent
semantic classes compared to a corpus-based vector
representation (see the F-score in Tables 1 and 4).
This is not surprising, as feature norms provide
more detailed and accurate knowledge about seman-
tic representations than often noisy and approxi-
mate corpus-based distributions.4 It maybe possi-
4Note that as multiple participants are required to create a
representation for each word, norming studies typically involve
ble to obtain better performance when considering
more elaborate representations. We have only ex-
perimented with a simple semantic space, however
variants that utilize syntactic information (e.g., Pad´o
and Lapata (2007)) may be more appropriate for the
taxonomy induction task. Our experiments have also
shown that the topology of the input semantic net-
work is critical for the success of the HRG. In partic-
ular edge re-weighting plays an important role and
generally improves performance. We have adopted
a simple method based on flat clustering; it may be
interesting to compare how this fares with more in-
volved weighting schemes such as those described
in Navigli et al. (2011). Finally, we have shown that
naive participants are able to perform the taxonomy
induction task relatively reliably and that the HRG
approximates human performance on a small-scale
experiment. We have evaluated model output using
F-score and tree-height correlation which we argue
are complementary and allow to assess hierarchical
clustering more rigorously.
Avenues for future work are many and varied. Be-
sides exploring the performance of our algorithm on
more specialized domains (e.g., mathematics or ge-
ography) we would also like to create an incremen-
tal version that augments an existing taxonomy with
missing information. Additionally, the taxonomies
inferred with the HRG do not currently admit term
ambiguity which we could remedy by modifying our
technique for constructing a consensus hierarchy to
reflect the sampled frequency of observed subtrees.
a small number of items, consequently limiting the scope of any
computational model based on normed data.
</bodyText>
<page confidence="0.998639">
474
</page>
<sectionHeader confidence="0.996134" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999259780952381">
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
pages 7–12, Prague, Czech Republic, June.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, pages 57–64, College Park, Maryland.
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to natu-
ral language processing problems. In Proceedings of
TextGraphs: the 1st Workshop on Graph Based Meth-
ods for Natural Language Processing, pages 73–80,
New York City.
BNC. 2007. The British National Corpus, version 3
(BNC XML Edition). Distributed by Oxford Univer-
sity Computing Services on behalf of the BNC Con-
sortium.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467–479.
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 120–126,
College Park, Maryland.
Aaron Clauset, Christopher Moore, and M. E. J. New-
man. 2008. Hierarchical structure and the prediction
of missing links in networks. Nature, 453:98–101,
February.
J. Cohen and P. Cohen. 1983. Applied Multiple Regres-
sion/Correlation Analysis for the Behavioral Sciences.
Erlbaum, Hillsdale, NJ.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91–134.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Communi-
cation). The MIT Press.
Trevor Fountain and Mirella Lapata. 2010. Meaning rep-
resentation in natural language categorization. In Stel-
lan Ohlsson and Richard Catrambone, editors, Pro-
ceedings of the 31st Annual Conference of the Cogni-
tive Science Society, pages 1916–1921, Portland, Ore-
gon. Cognitive Science Society.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 107–114,
Ann Arbor, Michigan.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 80–87, Edmonton,
Canada.
Sanda M. Harabgiu, Steven J. Maiorano, and Marius A.
Pas¸ca. 2003. Open-doman textual question answering
techniques. Natural Language Engineering, 9(3):1–
38.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics, pages
539–545, Nantes, France.
Eduard Hovy. 2002. Comparing sets of semantic rela-
tionships in ontologies. In Rebecca Green, Carol A.
Bean, and Sun Hyon Myaeng, editors, The Seman-
tics of Relationships: An Interdisciplinary Perspec-
tive, pages 91–110. Kluwer Academic Publishers, The
Netherlands.
Chihli Hung, Stefan Wermter, and Peter Smith. 2004.
Hybrid neural document clustering using guided self-
organization and wordnet. IEEE Intelligent Systems,
19(2):68–77.
Ioannis Klapaftis and Suresh Manandhar. 2010. Word
sense induction and disambiguation using hierarchical
random graphs. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 745–755, Cambridge, MA.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1482–1491, Uppsala, Sweden, July.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048–1056, Columbus, Ohio, June.
Franc¸ois-Joseph Lapointe. 1995. Comparison tests for
dendrograms: A comparative evaluation. Journal of
Classification 12:265-282, 12:265–282.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics, Volume 2, pages 768–774, Mon-
treal, Quebec, Canada.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature production
norms for a large set of living and non-living things.
</reference>
<page confidence="0.988518">
475
</page>
<reference confidence="0.997168810810811">
Behavioral Research Methods Instruments &amp; Comput-
ers, 37(4):547–559.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexi-
cal taxonomies from scratch. In Proceedings of the
22nd International Joint Conference on Artificial In-
telligence, pages 1872–1877, Barcelona, Spain.
Sebastian Pad´o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161–199.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Main Proceedings, pages 321–328,
Boston, Massachusetts.
Brian Roark and Eugene Charniak. 1998. Noun-
phrase co-occurrence statistics for semi-automatic se-
mantic lexicon construction. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, Volume 2, pages 1110–
1116, Montreal, Quebec.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 801–808, Sydney, Australia.
Robert Sokal and Charles Michener. 1958. A statistical
method for evaluating systematic relationships. Uni-
versity of Kansas Science Bulletin, 38:1409–1438.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 271–279, Suntec, Singapore.
</reference>
<page confidence="0.999104">
476
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.594095">
<title confidence="0.992233">Taxonomy Induction Using Hierarchical Random Graphs</title>
<author confidence="0.733174">Fountain</author>
<affiliation confidence="0.9968165">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.985481">10 Crichton Street, Edinburgh EH8</address>
<email confidence="0.998196">t.fountain@sms.ed.ac.uk,mlap@inf.ed.ac.uk</email>
<abstract confidence="0.990094166666667">This paper presents a novel approach for inducing lexical taxonomies automatically from text. We recast the learning problem as that of inferring a hierarchy from a graph whose nodes represent taxonomic terms and edges their degree of relatedness. Our model takes this graph representation as input and fits a taxonomy to it via combination of a maximum likelihood approach with a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17103" citStr="Agirre and Soroa, 2007" startWordPosition="2844" endWordPosition="2847">e to the nature of the task which is inherently subjective and application specific (e.g., a dolphin can be a Mammal to a biologist, but a Fish to a fisherman or someone visiting an aquarium). Nevertheless, we assessed the taxonomies produced by the HRG against the WordNet-like taxonomy described above using two measures, one that simply evaluates the grouping of the nouns into classes without taking account of their position in the taxonomy and one which evaluates the taxonomy directly. To evaluate a flat clustering into classes we use the F-score measure introduced in the SemEval 2007 task (Agirre and Soroa, 2007); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. Although informative, evaluation based solely on F-score puts the HRG model at a comparative disadvantage as the task of taxonomy induction is significantly more difficult than simple clustering. To overcome this disadvantage we propose an automatic method of evaluating taxonomies directly by first computing the walk distance between pairs of terms that share a gold-standard catego</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 7–12, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>57--64</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="1837" citStr="Berland and Charniak, 1999" startWordPosition="275" endWordPosition="279">g et al., 2004), and textual entailment (Geffet and Dagan, 2005). As the effort involved in creating such resources manually is prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creating the taxonomy proper by organizing its terms hierarchically (Kozareva and Hovy, 2010; Navigli et al., 2011). Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). In this paper we propose an unsupervised approach to taxonomy induction. Given a corpus and a set of terms, our algorithm jointly induces their relations and their taxonomic organization. We view taxonomy learning as an instance of the problem of inferring a hierarchy from a network or grap</context>
<context position="4867" citStr="Berland and Charniak, 1999" startWordPosition="749" endWordPosition="752">fit of an automatically induced taxonomy against a gold-standard. In the following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS-A, PART-OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS-A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words </context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 57–64, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems.</title>
<date>2006</date>
<booktitle>In Proceedings of TextGraphs: the 1st Workshop on Graph Based Methods for Natural Language Processing,</booktitle>
<pages>73--80</pages>
<location>New York City.</location>
<contexts>
<context position="18899" citStr="Biemann (2006)" startWordPosition="3128" endWordPosition="3129">and flat cluster labels are available from http://homepages.inf.ed.ac.uk/s0897549/data. in the gold standard hierarchy. Similarly, let C = {c0,1,c0,2 ...cn,n−1}, where ca,b is the distance between a and b in the candidate hierarchy. The treeheight correlation between G and C is then given by Spearman’s p correlation coefficient between the two sets. All tree-height correlations reported in our experiments were computed using the WordNetbased gold-standard taxonomy over McRae et al.’s (2005) nouns. Baselines We compared the HRG output against three baselines. The first is Chinese Whispers (CW; Biemann (2006)), a randomized graph-clustering algorithm which like the HRG also takes as input a graph with weighted edges. It produces a hard (flat) clustering over the nodes in the graph, where the number of clusters is determined automatically. Our second baseline is Brown et al.’s (1992) agglomerative clustering algorithm that induces a mapping from word types to classes. It starts with K classes for the K most frequent word types and then proceeds by alternately adding the next most frequent word to the class set and merging the two classes which result in the least decrease in the mutual information </context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of TextGraphs: the 1st Workshop on Graph Based Methods for Natural Language Processing, pages 73–80, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BNC</author>
</authors>
<title>The British National Corpus,</title>
<date>2007</date>
<booktitle>version 3 (BNC XML Edition). Distributed by Oxford University Computing Services on behalf of the BNC Consortium.</booktitle>
<contexts>
<context position="23195" citStr="BNC, 2007" startWordPosition="3828" endWordPosition="3829">xonomy Induction from the British National Corpus Method The results of Experiment 1 can be considered as an upper bound of what can be achieved by the HRG when the input graph is constructed from highly accurate semantic information. Feature norms provide detailed knowledge about meaning which would be very difficult if not close to impossible to obtain from a corpus. Nevertheless, it is interesting to explore how well we can induce taxonomies using a lower quality semantic network. We therefore constructed a network based on cooccurrence statistics computed from the British National Corpus (BNC, 2007) and provided the resulting semantic network as input to the HRG, CW, and Agglo models; additionally, we employed the algorithm of Brown et al. (1992) to induce a hierarchy over the target terms directly from the corpus. Unfortunately, this algorithm requires the number of desired output clusters to be specified in advance; in all trials this parameter was set to the number of clusters in the gold-standard clustering (41), thus providing the Brown-induced clusterings with a slight oracle advantage. Again, nouns were represented as vectors in semantic space. We used a context window of five wor</context>
</contexts>
<marker>BNC, 2007</marker>
<rawString>BNC. 2007. The British National Corpus, version 3 (BNC XML Edition). Distributed by Oxford University Computing Services on behalf of the BNC Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V de Souza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza, Jenifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>120--126</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="5310" citStr="Caraballo, 1999" startWordPosition="820" endWordPosition="821">ssentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS-A, PART-OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS-A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if they share contextual features. Most of these algorithms aim at inducing flat clusters rather than taxonomies, with the exception of Brown et al. (1992) whose method induces binary trees. Contrary to the plethora of algorithms developed for relation discovery, methods dedicated to taxonomy learning have been few and far between. Caraballo (1999) was the first to induce a taxonomy from a corpus using a combination of clustering </context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Sharon A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 120–126, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Christopher Moore</author>
<author>M E J Newman</author>
</authors>
<title>Hierarchical structure and the prediction of missing links in networks.</title>
<date>2008</date>
<journal>Nature,</journal>
<pages>453--98</pages>
<contexts>
<context position="2624" citStr="Clauset et al. (2008)" startWordPosition="405" endWordPosition="408">also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). In this paper we propose an unsupervised approach to taxonomy induction. Given a corpus and a set of terms, our algorithm jointly induces their relations and their taxonomic organization. We view taxonomy learning as an instance of the problem of inferring a hierarchy from a network or graph. We create this graph from unstructured text simply by drawing an edge between distributionally similar terms. Next, we fit a Hierarchical Random Graph model (HRG; Clauset et al. (2008)) to the observed graph data based on maximum likelihood methods and Markov chain Monte Carlo sampling. The model essentially works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. This is advantageous as it allows us to consider the ensemble of random graphs that are statistically similar to the original graph, and through this to derive a consensus hierarchical structure from the ensemble of sampled models. The approach differs crucially from hierarchical clustering in that it explicitly acknowledges that most real-w</context>
<context position="7951" citStr="Clauset et al., 2008" startWordPosition="1250" endWordPosition="1253">rlo Sampling algorithm. The technique allows us to sample hierarchical random graphs with probability proportional to the likelihood that they generate the observed network. The induction algorithm can operate over any kind of (undirected) graph, and thus does not have to be tuned specifically for different inputs. We should also point out that our formulation of the inference problem utilizes very little corpus external knowledge other than the set of input terms, and could thus be easily applied to domains or languages where lexical resources are scarce. The Hierarchical Random Graph model (Clauset et al., 2008) has been applied to construct hierarchical decompositions from three sets of network data: a bacterial metabolic network; a food-web among grassland species; and the network of associations among terrorist cells. The only language-related application we are aware of concerns word sense induction. Klapaftis and Manandhar (2010) create a graph of contexts for a polysemous target word and use the HRG to organize them hierarchically, under the assumption that different tree heights correspond to different levels of sense granularity. 467 Figure 1: Flow of information through the Hierarchical Rand</context>
<context position="13536" citStr="Clauset et al. (2008)" startWordPosition="2236" endWordPosition="2239">h may or may not correspond to the observed data — the tree at convergence will be similar to an ideal tree given the graph, but may not be the most plausible structure. Indeed, for taxonomy induction it is quite unlikely that a binary tree will provide the most appropriate categorization. To avoid encoding such bias we employ a model averaging technique to produce a consensus hierarchy. For a set of binary trees sampled after convergence, we first identify the set of possible clusters encoded in the tree, e.g., the binary tree in Figure 1b encodes the clusters {AB,ABC,EF,D,DEF,ABCDEF}. As in Clauset et al. (2008), each cluster instance is then weighted according to the likelihood of the originating HRG (Equation 1); we then sum the weights for each distinct cluster across all resampled trees and discard those whose aggregate weight is lower than 50% of the total observed weight. The remaining clusters are then used to reconstruct a hierarchy in which Algorithm 2: Flat Clusters 1 Let Dk be the root node of D. 2 if θk &gt; θ¯ then 3 output the leaves of the subtree rooted at Dk as a cluster 4 else 5 repeat 2 with left and right children of Dk. 6 end each subtree appears in the majority of trees observed af</context>
</contexts>
<marker>Clauset, Moore, Newman, 2008</marker>
<rawString>Aaron Clauset, Christopher Moore, and M. E. J. Newman. 2008. Hierarchical structure and the prediction of missing links in networks. Nature, 453:98–101, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
<author>P Cohen</author>
</authors>
<title>Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Erlbaum,</title>
<date>1983</date>
<location>Hillsdale, NJ.</location>
<marker>Cohen, Cohen, 1983</marker>
<rawString>J. Cohen and P. Cohen. 1983. Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="4909" citStr="Etzioni et al., 2005" startWordPosition="757" endWordPosition="760"> a gold-standard. In the following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS-A, PART-OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS-A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if they share contextual features</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1075" citStr="Fellbaum, 1998" startWordPosition="157" endWordPosition="158">resent taxonomic terms and edges their degree of relatedness. Our model takes this graph representation as input and fits a taxonomy to it via combination of a maximum likelihood approach with a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms. 1 Introduction The semantic knowledge encoded in lexical resources such as WordNet (Fellbaum, 1998) has been proven beneficial for several applications including question answering (Harabgiu et al., 2003), document classification (Hung et al., 2004), and textual entailment (Geffet and Dagan, 2005). As the effort involved in creating such resources manually is prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al.,</context>
<context position="15738" citStr="Fellbaum, 1998" startWordPosition="2621" endWordPosition="2622">ae et al.’s (2005) dataset which consists of for 541 basic level nouns (e.g., DOG and TABLE). Each noun is associated with features (e.g., has-legs, is-flat, and made-of-wood for TABLE) collected from human participants in multiple studies over several years. The original norming study does not include class labels for these nouns, however, we were able to exploit a clustering provided by Fountain and Lapata (2010), in which a set of online participants annotated each of the McRae et al. nouns with basic category labels. The nouns and their class labels were further taxonomized using WordNet (Fellbaum, 1998). Specifically, we first identified the full hypernym path in WordNet for each noun in McRae et al.’s (2005) dataset, e.g., APPLE &gt; PLANT STRUCTURE &gt; NAT469 URAL OBJECT &gt; PHYSICAL OBJECT &gt; ENTITY (a total of 493 concepts appear in both). These hypernym paths were then combined to yield a full taxonomy over McRae et al.’s nouns; internal nodes having only a single child were recursively removed to produce a final, compact taxonomy1 containing 186 semantic classes (e.g., ANIMALS, WEAPONS, FRUITS) organized into varying levels of granularity (e.g., SONGBIRDS &gt; BIRDS &gt;ANIMALS). Evaluation measures</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Fountain</author>
<author>Mirella Lapata</author>
</authors>
<title>Meaning representation in natural language categorization.</title>
<date>2010</date>
<booktitle>In Stellan Ohlsson</booktitle>
<pages>1916--1921</pages>
<editor>and Richard Catrambone, editors,</editor>
<publisher>Cognitive Science Society.</publisher>
<location>Portland, Oregon.</location>
<contexts>
<context position="15541" citStr="Fountain and Lapata (2010)" startWordPosition="2585" endWordPosition="2588">cluster of related items, where θ¯ = mean(θ) (illustrated in Figure 1c). This procedure is explained in greater detail in Algorithm 2. 4 Evaluation Data We evaluated our taxonomy induction algorithm using McRae et al.’s (2005) dataset which consists of for 541 basic level nouns (e.g., DOG and TABLE). Each noun is associated with features (e.g., has-legs, is-flat, and made-of-wood for TABLE) collected from human participants in multiple studies over several years. The original norming study does not include class labels for these nouns, however, we were able to exploit a clustering provided by Fountain and Lapata (2010), in which a set of online participants annotated each of the McRae et al. nouns with basic category labels. The nouns and their class labels were further taxonomized using WordNet (Fellbaum, 1998). Specifically, we first identified the full hypernym path in WordNet for each noun in McRae et al.’s (2005) dataset, e.g., APPLE &gt; PLANT STRUCTURE &gt; NAT469 URAL OBJECT &gt; PHYSICAL OBJECT &gt; ENTITY (a total of 493 concepts appear in both). These hypernym paths were then combined to yield a full taxonomy over McRae et al.’s nouns; internal nodes having only a single child were recursively removed to pro</context>
</contexts>
<marker>Fountain, Lapata, 2010</marker>
<rawString>Trevor Fountain and Mirella Lapata. 2010. Meaning representation in natural language categorization. In Stellan Ohlsson and Richard Catrambone, editors, Proceedings of the 31st Annual Conference of the Cognitive Science Society, pages 1916–1921, Portland, Oregon. Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>107--114</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1274" citStr="Geffet and Dagan, 2005" startWordPosition="184" endWordPosition="187">h a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms. 1 Introduction The semantic knowledge encoded in lexical resources such as WordNet (Fellbaum, 1998) has been proven beneficial for several applications including question answering (Harabgiu et al., 2003), document classification (Hung et al., 2004), and textual entailment (Geffet and Dagan, 2005). As the effort involved in creating such resources manually is prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creati</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Maayan Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 107–114, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4887" citStr="Girju et al., 2003" startWordPosition="753" endWordPosition="756">ced taxonomy against a gold-standard. In the following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS-A, PART-OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS-A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if they sha</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 80–87, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabgiu</author>
<author>Steven J Maiorano</author>
<author>Marius A Pas¸ca</author>
</authors>
<title>Open-doman textual question answering techniques.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>3</issue>
<pages>38</pages>
<marker>Harabgiu, Maiorano, Pas¸ca, 2003</marker>
<rawString>Sanda M. Harabgiu, Steven J. Maiorano, and Marius A. Pas¸ca. 2003. Open-doman textual question answering techniques. Natural Language Engineering, 9(3):1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="1808" citStr="Hearst, 1992" startWordPosition="273" endWordPosition="274">ification (Hung et al., 2004), and textual entailment (Geffet and Dagan, 2005). As the effort involved in creating such resources manually is prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creating the taxonomy proper by organizing its terms hierarchically (Kozareva and Hovy, 2010; Navigli et al., 2011). Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). In this paper we propose an unsupervised approach to taxonomy induction. Given a corpus and a set of terms, our algorithm jointly induces their relations and their taxonomic organization. We view taxonomy learning as an instance of the problem of inferring a hie</context>
<context position="4813" citStr="Hearst, 1992" startWordPosition="743" endWordPosition="744">nd introduce new ways of evaluating the fit of an automatically induced taxonomy against a gold-standard. In the following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS-A, PART-OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS-A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in simila</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
</authors>
<title>Comparing sets of semantic relationships in ontologies.</title>
<date>2002</date>
<booktitle>The Semantics of Relationships: An Interdisciplinary Perspective,</booktitle>
<pages>91--110</pages>
<editor>In Rebecca Green, Carol A. Bean, and Sun Hyon Myaeng, editors,</editor>
<publisher>Kluwer Academic Publishers, The Netherlands.</publisher>
<contexts>
<context position="16425" citStr="Hovy (2002)" startWordPosition="2732" endWordPosition="2733">ch noun in McRae et al.’s (2005) dataset, e.g., APPLE &gt; PLANT STRUCTURE &gt; NAT469 URAL OBJECT &gt; PHYSICAL OBJECT &gt; ENTITY (a total of 493 concepts appear in both). These hypernym paths were then combined to yield a full taxonomy over McRae et al.’s nouns; internal nodes having only a single child were recursively removed to produce a final, compact taxonomy1 containing 186 semantic classes (e.g., ANIMALS, WEAPONS, FRUITS) organized into varying levels of granularity (e.g., SONGBIRDS &gt; BIRDS &gt;ANIMALS). Evaluation measures Evaluation of taxonomically organized information is notoriously hard (see Hovy (2002) for an extensive discussion on this topic). This is due to the nature of the task which is inherently subjective and application specific (e.g., a dolphin can be a Mammal to a biologist, but a Fish to a fisherman or someone visiting an aquarium). Nevertheless, we assessed the taxonomies produced by the HRG against the WordNet-like taxonomy described above using two measures, one that simply evaluates the grouping of the nouns into classes without taking account of their position in the taxonomy and one which evaluates the taxonomy directly. To evaluate a flat clustering into classes we use th</context>
</contexts>
<marker>Hovy, 2002</marker>
<rawString>Eduard Hovy. 2002. Comparing sets of semantic relationships in ontologies. In Rebecca Green, Carol A. Bean, and Sun Hyon Myaeng, editors, The Semantics of Relationships: An Interdisciplinary Perspective, pages 91–110. Kluwer Academic Publishers, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chihli Hung</author>
<author>Stefan Wermter</author>
<author>Peter Smith</author>
</authors>
<title>Hybrid neural document clustering using guided selforganization and wordnet.</title>
<date>2004</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1225" citStr="Hung et al., 2004" startWordPosition="176" endWordPosition="179">ination of a maximum likelihood approach with a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms. 1 Introduction The semantic knowledge encoded in lexical resources such as WordNet (Fellbaum, 1998) has been proven beneficial for several applications including question answering (Harabgiu et al., 2003), document classification (Hung et al., 2004), and textual entailment (Geffet and Dagan, 2005). As the effort involved in creating such resources manually is prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF (Hearst, 1992; Berland and Cha</context>
</contexts>
<marker>Hung, Wermter, Smith, 2004</marker>
<rawString>Chihli Hung, Stefan Wermter, and Peter Smith. 2004. Hybrid neural document clustering using guided selforganization and wordnet. IEEE Intelligent Systems, 19(2):68–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Klapaftis</author>
<author>Suresh Manandhar</author>
</authors>
<title>Word sense induction and disambiguation using hierarchical random graphs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>745--755</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="8280" citStr="Klapaftis and Manandhar (2010)" startWordPosition="1299" endWordPosition="1302">should also point out that our formulation of the inference problem utilizes very little corpus external knowledge other than the set of input terms, and could thus be easily applied to domains or languages where lexical resources are scarce. The Hierarchical Random Graph model (Clauset et al., 2008) has been applied to construct hierarchical decompositions from three sets of network data: a bacterial metabolic network; a food-web among grassland species; and the network of associations among terrorist cells. The only language-related application we are aware of concerns word sense induction. Klapaftis and Manandhar (2010) create a graph of contexts for a polysemous target word and use the HRG to organize them hierarchically, under the assumption that different tree heights correspond to different levels of sense granularity. 467 Figure 1: Flow of information through the Hierarchical Random Graph algorithm. From a semantic network (1a), the model constructs a binary tree (1b). Edges in the semantic network are then used to compute the θ parameters for internal nodes in the tree; the maximum-likelihood-estimated θ parameter for an internal node indicates the density of edges between its children. This tree is th</context>
<context position="26239" citStr="Klapaftis and Manandhar, 2010" startWordPosition="4328" endWordPosition="4331">Cluster F-score and tree-height correlation evaluation for taxonomies inferred over McRae et al.’s (2005) nouns; all algorithms are run on the BNC. Performance of the HRG is better when the semantic network is based on feature norms (compare Tables 1 and 2), both in terms of tree-height correlation and F-score. This suggests that the algorithm is highly dependent on the quality of the semantic network used as input. In particular, HRGs are known to be more appropriate for so-called small-world networks, graphs composed of densely-connected subgraphs with relatively sparse connections between (Klapaftis and Manandhar, 2010). Indeed, inspection of the semantic network produced from the BNC (see Figure 3a) shows that our corpus-derived graph is emphatically not a small-world graph, yet the HRG is able to recover some taxonomic information from such a densely-connected network. In the following experiments we first assess the difficulty of the taxonomy induction task to get a feel of how well the algorithms are performing in comparison to humans and then investigate ways of rendering the BNC-based graph more similar to a small-world network. 4.3 Experiment 3: Human Upper Bound Method The previous experiments evalua</context>
</contexts>
<marker>Klapaftis, Manandhar, 2010</marker>
<rawString>Ioannis Klapaftis and Suresh Manandhar. 2010. Word sense induction and disambiguation using hierarchical random graphs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 745–755, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning arguments and supertypes of semantic relations using recursive patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1482--1491</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1960" citStr="Kozareva and Hovy, 2010" startWordPosition="295" endWordPosition="298"> prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creating the taxonomy proper by organizing its terms hierarchically (Kozareva and Hovy, 2010; Navigli et al., 2011). Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). In this paper we propose an unsupervised approach to taxonomy induction. Given a corpus and a set of terms, our algorithm jointly induces their relations and their taxonomic organization. We view taxonomy learning as an instance of the problem of inferring a hierarchy from a network or graph. We create this graph from unstructured text simply by drawing an edge between distributionally similar terms. Next, we f</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010. Learning arguments and supertypes of semantic relations using recursive patterns. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1482–1491, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1048--1056</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1658" citStr="Kozareva et al., 2008" startWordPosition="246" endWordPosition="249">urces such as WordNet (Fellbaum, 1998) has been proven beneficial for several applications including question answering (Harabgiu et al., 2003), document classification (Hung et al., 2004), and textual entailment (Geffet and Dagan, 2005). As the effort involved in creating such resources manually is prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creating the taxonomy proper by organizing its terms hierarchically (Kozareva and Hovy, 2010; Navigli et al., 2011). Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). In this paper we propose an unsupervised approach to taxonomy induction. Given a corpus and a set of terms, our </context>
<context position="4933" citStr="Kozareva et al., 2008" startWordPosition="761" endWordPosition="764">he following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS-A, PART-OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS-A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if they share contextual features. Most of these algorith</context>
<context position="6191" citStr="Kozareva et al. (2008)" startWordPosition="964" endWordPosition="967">her than taxonomies, with the exception of Brown et al. (1992) whose method induces binary trees. Contrary to the plethora of algorithms developed for relation discovery, methods dedicated to taxonomy learning have been few and far between. Caraballo (1999) was the first to induce a taxonomy from a corpus using a combination of clustering and pattern-based methods. Specifically, nouns are organized into a tree using a bottom-up clustering algorithm and internal nodes of the resulting tree are labeled with hypernyms from the nouns clustered underneath using patterns such as “B is a kind of A”. Kozareva et al. (2008) and Navigli et al. (2011) both develop systems that create taxonomies endto-end, i.e., discover the terms, their relations, and how these are hierarchically organized. The two approaches are conceptually similar: they both use the web and pattern-based methods for finding domainspecific terms. Additionally, in both approaches the acquired knowledge is represented as a graph from which a taxonomy is induced using task-specific algorithms such as graph pruning, edge weighting, and so on. Our work also addresses taxonomy learning, however, without the term discovery step — we assume we are given</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings of ACL-08: HLT, pages 1048–1056, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois-Joseph Lapointe</author>
</authors>
<title>Comparison tests for dendrograms: A comparative evaluation.</title>
<date>1995</date>
<journal>Journal of Classification</journal>
<pages>12--265</pages>
<contexts>
<context position="17852" citStr="Lapointe, 1995" startWordPosition="2965" endWordPosition="2966"> in the cluster and the number of items in the gold-standard class, respectively. Although informative, evaluation based solely on F-score puts the HRG model at a comparative disadvantage as the task of taxonomy induction is significantly more difficult than simple clustering. To overcome this disadvantage we propose an automatic method of evaluating taxonomies directly by first computing the walk distance between pairs of terms that share a gold-standard category label within a gold-standard and a candidate taxonomy, and then computing the pairwise correlation between distances in each tree (Lapointe, 1995). This captures the intuition that a ‘good’ hierarchy is one in which items appearing near one another in the gold taxonomy also appear near one another in the induced one. It is also conceptually similar to the task-based IS-A evaluation (Snow et al., 2006) which has been traditionally used to evaluate taxonomies. Formally, let G = {g0,1,g0,2 ...gn,n−1}, where ga,b indicates the walk distance between terms a and b 1The taxonomy and flat cluster labels are available from http://homepages.inf.ed.ac.uk/s0897549/data. in the gold standard hierarchy. Similarly, let C = {c0,1,c0,2 ...cn,n−1}, where</context>
</contexts>
<marker>Lapointe, 1995</marker>
<rawString>Franc¸ois-Joseph Lapointe. 1995. Comparison tests for dendrograms: A comparative evaluation. Journal of Classification 12:265-282, 12:265–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="5293" citStr="Lin, 1998" startWordPosition="818" endWordPosition="819">following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS-A, PART-OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS-A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if they share contextual features. Most of these algorithms aim at inducing flat clusters rather than taxonomies, with the exception of Brown et al. (1992) whose method induces binary trees. Contrary to the plethora of algorithms developed for relation discovery, methods dedicated to taxonomy learning have been few and far between. Caraballo (1999) was the first to induce a taxonomy from a corpus using a combinati</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2, pages 768–774, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>George S Cree</author>
<author>Mark S Seidenberg</author>
<author>Chris McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and non-living things.</title>
<date>2005</date>
<journal>Behavioral Research Methods Instruments &amp; Computers,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="20522" citStr="McRae et al. (2005)" startWordPosition="3390" endWordPosition="3393">3). Since HRGs provide a means of inducing a hierarchy over a graph-based representation, which may be constructed in an arbitrary fashion, our experiments were designed to investigate how the topology and quality of the input graph influences the algorithm’s performance. We thus report results when the semantic network is created from data sources of varying quality and granularity. 4.1 Experiment 1: Taxonomy Induction from Feature Norms Method We first considered the case where the input graph is of high semantic quality and constructed a semantic network from the feature norms collected by McRae et al. (2005). Each noun was represented as a vector with dimensions corresponding to the possible features generated by participants of the norming study; the value of a term along a dimen470 Method F-score Tree Correlation HRG 0.507 0.168 CW 0.464 — Agglo 0.352 0.137 Table 1: Cluster F-score and tree-height correlation evaluation; a semantic network constructed over McRae et al.’s (2005) nouns and features is given as input to the algorithms. sion was taken to be the frequency with which participants generated the corresponding feature when given the term. For each pair of terms an edge was added to the </context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>Ken McRae, George S. Cree, Mark S. Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and non-living things. Behavioral Research Methods Instruments &amp; Computers, 37(4):547–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
</authors>
<title>A graph-based algorithm for inducing lexical taxonomies from scratch.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22nd International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1872--1877</pages>
<location>Barcelona,</location>
<contexts>
<context position="1681" citStr="Navigli et al., 2011" startWordPosition="250" endWordPosition="253">Fellbaum, 1998) has been proven beneficial for several applications including question answering (Harabgiu et al., 2003), document classification (Hung et al., 2004), and textual entailment (Geffet and Dagan, 2005). As the effort involved in creating such resources manually is prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creating the taxonomy proper by organizing its terms hierarchically (Kozareva and Hovy, 2010; Navigli et al., 2011). Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). In this paper we propose an unsupervised approach to taxonomy induction. Given a corpus and a set of terms, our algorithm jointly induc</context>
<context position="6217" citStr="Navigli et al. (2011)" startWordPosition="969" endWordPosition="972">he exception of Brown et al. (1992) whose method induces binary trees. Contrary to the plethora of algorithms developed for relation discovery, methods dedicated to taxonomy learning have been few and far between. Caraballo (1999) was the first to induce a taxonomy from a corpus using a combination of clustering and pattern-based methods. Specifically, nouns are organized into a tree using a bottom-up clustering algorithm and internal nodes of the resulting tree are labeled with hypernyms from the nouns clustered underneath using patterns such as “B is a kind of A”. Kozareva et al. (2008) and Navigli et al. (2011) both develop systems that create taxonomies endto-end, i.e., discover the terms, their relations, and how these are hierarchically organized. The two approaches are conceptually similar: they both use the web and pattern-based methods for finding domainspecific terms. Additionally, in both approaches the acquired knowledge is represented as a graph from which a taxonomy is induced using task-specific algorithms such as graph pruning, edge weighting, and so on. Our work also addresses taxonomy learning, however, without the term discovery step — we assume we are given the terms for which to cr</context>
<context position="35335" citStr="Navigli et al. (2011)" startWordPosition="5801" endWordPosition="5804">ate representations. We have only experimented with a simple semantic space, however variants that utilize syntactic information (e.g., Pad´o and Lapata (2007)) may be more appropriate for the taxonomy induction task. Our experiments have also shown that the topology of the input semantic network is critical for the success of the HRG. In particular edge re-weighting plays an important role and generally improves performance. We have adopted a simple method based on flat clustering; it may be interesting to compare how this fares with more involved weighting schemes such as those described in Navigli et al. (2011). Finally, we have shown that naive participants are able to perform the taxonomy induction task relatively reliably and that the HRG approximates human performance on a small-scale experiment. We have evaluated model output using F-score and tree-height correlation which we argue are complementary and allow to assess hierarchical clustering more rigorously. Avenues for future work are many and varied. Besides exploring the performance of our algorithm on more specialized domains (e.g., mathematics or geography) we would also like to create an incremental version that augments an existing taxo</context>
</contexts>
<marker>Navigli, Velardi, Faralli, 2011</marker>
<rawString>Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011. A graph-based algorithm for inducing lexical taxonomies from scratch. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence, pages 1872–1877, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLTNAACL 2004: Main Proceedings,</booktitle>
<pages>321--328</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="5342" citStr="Pantel and Ravichandran, 2004" startWordPosition="822" endWordPosition="826">thodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS-A, PART-OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS-A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if they share contextual features. Most of these algorithms aim at inducing flat clusters rather than taxonomies, with the exception of Brown et al. (1992) whose method induces binary trees. Contrary to the plethora of algorithms developed for relation discovery, methods dedicated to taxonomy learning have been few and far between. Caraballo (1999) was the first to induce a taxonomy from a corpus using a combination of clustering and pattern-based methods. Speci</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLTNAACL 2004: Main Proceedings, pages 321–328, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Eugene Charniak</author>
</authors>
<title>Nounphrase co-occurrence statistics for semi-automatic semantic lexicon construction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>1110--1116</pages>
<location>Montreal, Quebec.</location>
<contexts>
<context position="4839" citStr="Roark and Charniak, 1998" startWordPosition="745" endWordPosition="748">ew ways of evaluating the fit of an automatically induced taxonomy against a gold-standard. In the following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS-A, PART-OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS-A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus cluste</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>Brian Roark and Eugene Charniak. 1998. Nounphrase co-occurrence statistics for semi-automatic semantic lexicon construction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2, pages 1110– 1116, Montreal, Quebec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>801--808</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2120" citStr="Snow et al., 2006" startWordPosition="321" endWordPosition="324">in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creating the taxonomy proper by organizing its terms hierarchically (Kozareva and Hovy, 2010; Navigli et al., 2011). Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). In this paper we propose an unsupervised approach to taxonomy induction. Given a corpus and a set of terms, our algorithm jointly induces their relations and their taxonomic organization. We view taxonomy learning as an instance of the problem of inferring a hierarchy from a network or graph. We create this graph from unstructured text simply by drawing an edge between distributionally similar terms. Next, we fit a Hierarchical Random Graph model (HRG; Clauset et al. (2008)) to the observed graph data based on maximum likelihood methods and Markov chain Monte Carlo sa</context>
<context position="18110" citStr="Snow et al., 2006" startWordPosition="3008" endWordPosition="3011">han simple clustering. To overcome this disadvantage we propose an automatic method of evaluating taxonomies directly by first computing the walk distance between pairs of terms that share a gold-standard category label within a gold-standard and a candidate taxonomy, and then computing the pairwise correlation between distances in each tree (Lapointe, 1995). This captures the intuition that a ‘good’ hierarchy is one in which items appearing near one another in the gold taxonomy also appear near one another in the induced one. It is also conceptually similar to the task-based IS-A evaluation (Snow et al., 2006) which has been traditionally used to evaluate taxonomies. Formally, let G = {g0,1,g0,2 ...gn,n−1}, where ga,b indicates the walk distance between terms a and b 1The taxonomy and flat cluster labels are available from http://homepages.inf.ed.ac.uk/s0897549/data. in the gold standard hierarchy. Similarly, let C = {c0,1,c0,2 ...cn,n−1}, where ca,b is the distance between a and b in the candidate hierarchy. The treeheight correlation between G and C is then given by Spearman’s p correlation coefficient between the two sets. All tree-height correlations reported in our experiments were computed us</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 801–808, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Sokal</author>
<author>Charles Michener</author>
</authors>
<title>A statistical method for evaluating systematic relationships.</title>
<date>1958</date>
<pages>38--1409</pages>
<institution>University of Kansas Science Bulletin,</institution>
<contexts>
<context position="19678" citStr="Sokal and Michener, 1958" startWordPosition="3257" endWordPosition="3260">he nodes in the graph, where the number of clusters is determined automatically. Our second baseline is Brown et al.’s (1992) agglomerative clustering algorithm that induces a mapping from word types to classes. It starts with K classes for the K most frequent word types and then proceeds by alternately adding the next most frequent word to the class set and merging the two classes which result in the least decrease in the mutual information between class bigrams. The result is a class hierarchy with word types at the leaves. Additionally, we compare against standard agglomerative clustering (Sokal and Michener, 1958) which produces a binary dendrogram in a bottom-up fashion by recursively identifying concepts or clusters with the highest pairwise similarity. In the following, we present our taxonomy induction experiments (Sections 4.1–4.3). Since HRGs provide a means of inducing a hierarchy over a graph-based representation, which may be constructed in an arbitrary fashion, our experiments were designed to investigate how the topology and quality of the input graph influences the algorithm’s performance. We thus report results when the semantic network is created from data sources of varying quality and g</context>
</contexts>
<marker>Sokal, Michener, 1958</marker>
<rawString>Robert Sokal and Charles Michener. 1958. A statistical method for evaluating systematic relationships. University of Kansas Science Bulletin, 38:1409–1438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Jamie Callan</author>
</authors>
<title>A metric-based framework for automatic taxonomy induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>271--279</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="2144" citStr="Yang and Callan, 2009" startWordPosition="325" endWordPosition="328">my induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as IS-A, or PART-OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creating the taxonomy proper by organizing its terms hierarchically (Kozareva and Hovy, 2010; Navigli et al., 2011). Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). In this paper we propose an unsupervised approach to taxonomy induction. Given a corpus and a set of terms, our algorithm jointly induces their relations and their taxonomic organization. We view taxonomy learning as an instance of the problem of inferring a hierarchy from a network or graph. We create this graph from unstructured text simply by drawing an edge between distributionally similar terms. Next, we fit a Hierarchical Random Graph model (HRG; Clauset et al. (2008)) to the observed graph data based on maximum likelihood methods and Markov chain Monte Carlo sampling. The model essent</context>
</contexts>
<marker>Yang, Callan, 2009</marker>
<rawString>Hui Yang and Jamie Callan. 2009. A metric-based framework for automatic taxonomy induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 271–279, Suntec, Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>