<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000287">
<title confidence="0.987564">
Clinical Information Retrieval using Document and PICO Structure
</title>
<author confidence="0.959977">
Florian Boudin and Jian-Yun Nie
</author>
<affiliation confidence="0.798176">
DIRO, Universit´e de Montr´eal
</affiliation>
<address confidence="0.8152215">
CP. 6128, succursale Centre-ville
Montr´eal, H3C 3J7 Qu´ebec, Canada
</address>
<email confidence="0.990311">
{boudinfl,nie}@iro.umontreal.ca
</email>
<author confidence="0.988761">
Martin Dawes
</author>
<affiliation confidence="0.979755">
Department of Family Medicine
</affiliation>
<address confidence="0.8223575">
McGill University, 515 Pine Ave W
Montr´eal, H2W 1S4 Qu´ebec, Canada
</address>
<email confidence="0.993168">
martin.dawes@mcgill.ca
</email>
<sectionHeader confidence="0.993647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998946466666667">
In evidence-based medicine, clinical questions
involve four aspects: Patient/Problem (P), In-
tervention (I), Comparison (C) and Outcome
(O), known as PICO elements. In this pa-
per we present a method that extends the lan-
guage modeling approach to incorporate both
document structure and PICO query formu-
lation. We present an analysis of the distri-
bution of PICO elements in medical abstracts
that motivates the use of a location-based
weighting strategy. In experiments carried out
on a collection of 1.5 million abstracts, the
method was found to lead to an improvement
of roughly 60% in MAP and 70% in P@10 as
compared to state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984891865384616">
As the volume of published medical literature con-
tinues to grow exponentially, there is more and more
research for physicians to assess and evaluate and
less time to do so. Evidence-based medicine (EBM)
(Sackett et al., 1996) is a widely accepted paradigm
in medical practice that relies on evidence from
patient-centered clinical research to make decisions.
Taking an evidence-based approach to searching
means doing a systematic search of all the available
literature, individually critically appraising each re-
search study and then applying the findings in clini-
cal practice. However, this is a time consuming ac-
tivity. One way to facilitate searching for a precise
answer is to formulate a well-focused and structured
question (Schardt et al., 2007).
Physicians are educated to formulate their clinical
questions according to several well defined aspects
in EBM: Patient/Problem (P), Intervention (I),
Comparison (C) and Outcome (O), which are called
PICO elements. In many documents in medical lit-
erature (e.g. MEDLINE), one can find the elements
of the PICO structure, but rarely explicitly anno-
tated (Dawes et al., 2007). To identify documents
corresponding to a patient’s state, physicians also
construct their queries according to the PICO struc-
ture. For example, in the question “In children with
pain and fever how does paracetamol compared
with ibuprofen affect levels of pain and fever?” one
can identify the following PICO elements:
Patient/Problem: children/pain and fever
Intervention: paracetamol
Comparison: ibuprofen
Outcome: levels ofpain and fever
Very little work, if any, has been carried out on the
use of these elements in the Information Retrieval
(IR) process. There are several reasons for that. It
is not easy to identify PICO elements in documents,
as well as in the question if these are not explicitly
separated in it. Several studies have been performed
on identifying PICO elements in abstracts (Demner-
Fushman and Lin, 2007; Hansen et al., 2008; Chung,
2009). However, all of them are reporting coarse-
grain (sentence-level) tagging methods that have not
yet been shown to be sufficient for the purpose of
IR. Moreover, there is currently no standard test col-
lection of questions in PICO structure available for
evaluation. On the other hand, the most critical as-
pect in IR is term weighting. One of the purpose
of tagging PICO elements is to assign appropriate
weights to these elements during the retrieval pro-
cess. From this perspective, a semantic tagging of
PICO elements may be a task that goes well beyond
</bodyText>
<page confidence="0.961657">
822
</page>
<note confidence="0.753402">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 822–830,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999808090909091">
that is required for IR. It may be sufficient to have
a method that assigns appropriate weights to ele-
ments rather than recognizing their semantic roles.
In this paper, we will propose an approach to deter-
mine term weights according to document structure.
This method will be compared to that using tagging
of PICO elements.
In this paper, we first report an attempt to manu-
ally annotate the PICO elements in documents by
physicians and use them as training data to build
an automatic tagging tool. It turns out that there
is a high disagreement rate between human anno-
tators. The utilization of the automatic tagging tool
in an IR experiment shows only a small gain in re-
trieval effectiveness. We therefore propose an alter-
native to PICO element detection that uses the struc-
tural information of documents. This solution turns
out to be robust and effective. The alternative ap-
proach is motivated by a strong trend that we ob-
serve in the distribution of PICO elements in docu-
ments. We then make use of both PICO query and
document structure to extend the classical language
modeling approach to IR. Specifically, we investi-
gate how each element of a PICO query should be
weighted and how a location-based weighting strat-
egy can be used to emphasize the most informative
parts (i.e. containing the most PICO elements) of
documents.
The paper is organized as follows. We first briefly
review the previous work, followed by a description
of the method we propose. Next, we present our
experiments and results. Lastly, we conclude with a
discussion and directions for future work.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999891333333334">
There have been only a few studies trying to use
PICO elements in the retrieval process. (Demner-
Fushman and Lin, 2007) is one of the few such stud-
ies. The method they describe consists in re-ranking
an initial list of retrieved citations. To this end, the
relevance of a document is scored by the use of de-
tected PICO elements, among other things. Several
other studies aimed to build a Question-Answering
system for clinical questions (Demner-Fushman and
Lin, 2006; Andrenucci, 2008). But again, the focus
has been set on the post-retrieval step, while the doc-
ument retrieval step only uses a standard approach.
In this paper, we argue that IR has much to gain by
using PICO elements.
The task of identifying PICO elements has how-
ever gain more attention. In their paper, (Demner-
Fushman and Lin, 2007) presented a method that
uses either manually crafted pattern-matching rules
or a combination of basic classifiers to detect PICO
elements in medical abstracts. Prior to that, biomed-
ical concepts are labelled by Metamap (Aronson,
2001) while relations between these concepts are
extracted with SemRep (Rindflesch and Fiszman,
2003). Recently, supervised classification using
Support Vector Machines (SVM) was proposed by
(Hansen et al., 2008) to extract the number of trial
participants. In a later study, (Chung, 2009) ex-
tended this work to other elements using Conditional
Random Fields. Although these studies are report-
ing interesting results, they are limited in several as-
pects. First, many are restricted to some segments
of the medical documents (e.g. Method section)
(Chung, 2009), and in most cases, the test collection
is very small (a few hundreds abstracts). Second, the
precision and granularity of these methods have not
yet been shown to be sufficient for the purpose of IR.
The structural information provided by markup
languages (e.g. XML) has been successfully used
to improve the IR effectiveness (INEX, 2002 2009).
For such documents, the structure information can
be used to emphasize some particular parts of the
document. Thereby, a given word should not have
the same importance depending on its position in the
document structure.
Taking into account the structure can be done ei-
ther at the step of querying or at the step of index-
ing. One way to integrate the structure at querying
is to adapt query languages (Fuhr and Großjohann,
2001). These approaches follow the assumption that
the user knows where the most relevant information
is located. However, (Kamps et al., 2005) showed
that it is preferable to use structure as a search hint,
and not as a strict search requirement
The second approach consists in integrating the
document structure at the indexing step by introduc-
ing a structure weighting scheme (Wilkinson, 1994).
In such a scheme, the weight assigned to a word is
not only based on its frequency but also on its posi-
tion in the document. The structure of a document
can be defined in terms of tags (e.g. title, section),
</bodyText>
<page confidence="0.997932">
823
</page>
<bodyText confidence="0.998604333333333">
each of those having a weight chosen either empiri-
cally or automatically by the use of optimizing tech-
niques such as genetic algorithms (Trotman, 2005).
</bodyText>
<sectionHeader confidence="0.954815" genericHeader="method">
3 Using PICO elements in retrieval
</sectionHeader>
<bodyText confidence="0.9998886">
In this section, we present an experiment on the
manual annotation of PICO elements. We then de-
scribe an approach to detect these elements in doc-
uments and give some results on the use of these
tagged elements in the retrieval process.
</bodyText>
<subsectionHeader confidence="0.999813">
3.1 Manual annotation of PICO elements
</subsectionHeader>
<bodyText confidence="0.9999908">
We asked medical professionals to manually anno-
tate the PICO elements in a small collection of ab-
stracts from PubMed1. The instructions given to
the annotators were fairly simple. They were asked
to precisely annotate all PICO elements in abstracts
with no restriction about the size of the elements (i.e.
they could be words, phrases or sentences). More
than 50 abstracts were manually annotated this way
by at least two different annotators. Two annotations
by two annotators are considered to agree if they
share some words (i.e. they overlap). We computed
the well known Cohen’s kappa measure as well as an
ad-hoc measure called loose. The latter uses PICO
elements as units and estimates the proportion of el-
ements that have been annotated by both raters.
</bodyText>
<table confidence="0.673267">
Measure P-element I/C-element O-element
kappa 0.687 0.539 0.523
loose 0.363 0.136 0.140
</table>
<tableCaption confidence="0.981406">
Table 1: Agreement measures computed for each ele-
ment. Cohen’s kappa and loose agreement are presented.
</tableCaption>
<bodyText confidence="0.999956111111111">
We can observe that there is a very low agree-
ment rate between human annotators. The loose
measure indicates that less than 15% of the I, C and
O elements have been marked by both annotators.
This fact shows that such human annotations can be
hardly used to develop an automatic tagging tool for
PICO elements, which requires consistent training
data. We therefore try to develop a coarser-grained
tagging method.
</bodyText>
<footnote confidence="0.942459">
1www.pubmed.gov, PubMed is a service of the US Na-
tional Library of Medicine that includes over 19 million cita-
tions from MEDLINE and other life science journals.
</footnote>
<subsectionHeader confidence="0.998779">
3.2 Automatic detection of PICO elements
</subsectionHeader>
<bodyText confidence="0.999803125">
Similarly to previous work, we propose a sentence-
level detection method. The identification of PICO
elements can be seen as a classification task. Even
for a coarser-grain classification task, we are still
lack of annotated data. One solution is to use the
structural information embedded in some medical
abstracts for which the authors have clearly stated
distinctive sentence headings. Some recent ab-
stracts in PubMed do contain explicit headings such
as “PATIENTS”, “SAMPLE” or “OUTCOMES”,
that can be used to locate sentences correspond-
ing to PICO elements. Using that information, we
extracted three sets of abstracts: Patient/Problem
(14 279 abstracts), Intervention/Comparison (9 095)
and Outcome (2 394).
Tagging each document goes through a three steps
process. First, the document is segmented into plain
sentences. Then each sentence is converted into a
feature vector using statistical (e.g. position, length)
and knowledge-based features (e.g. MeSH semantic
type). Knowledge-based features were derived ei-
ther from manually crafted cue-words/verbs lists or
semantic types within the MeSH ontology2. Finally,
each vector is submitted to multiple classifiers, one
for each element, allowing to label the correspond-
ing sentence. We use several algorithms imple-
mented in the Weka toolkit3: decision trees, SVM,
multi-layer perceptron and Naive Bayes. Combin-
ing multiple classifiers using a weighted linear com-
bination of their prediction scores achieves the best
results with a f-measure score of 86.3% for P, 67%
for I/C and 56.6% for O in 10-fold cross-validation.
</bodyText>
<subsectionHeader confidence="0.99993">
3.3 Use of detected elements in IR
</subsectionHeader>
<bodyText confidence="0.9998982">
We use language modeling approach to IR in this
work. The idea is that a document is a good match to
a query if its language model is likely to generate the
query (Ponte and Croft, 1998). It is one of the state-
of-the-art approaches in current IR research. Most
language modeling work in IR use unigram lan-
guage models –also called bags-of-words models–
assuming that there is no structure in queries or doc-
uments. A typical way to score a document d as
relevant to a query q is to use the Kullback-Leibler
</bodyText>
<footnote confidence="0.998853">
2www.nlm.nih.gov/mesh/
3www.cs.waikato.ac.nz/ml/index.html
</footnote>
<page confidence="0.996729">
824
</page>
<bodyText confidence="0.936519">
divergence between their respective LMs:
</bodyText>
<equation confidence="0.953697333333333">
�score(q, d) = P(w  |Mq) · log P(w  |Md) (1)
wEq
a −KL(Mq  ||Md)
</equation>
<bodyText confidence="0.9996666">
where Mq is the LM of the query and Md the LM of
the document. P(w  |Mρ) estimates the probability
of the word w given the language model Mρ. The
most direct way to estimate these models is to use
Maximum Likelihood estimation over the words:
</bodyText>
<equation confidence="0.994016">
count(w, ρ)
P(w  |Mρ) =
 |ρ |
</equation>
<bodyText confidence="0.996730083333333">
where ρ is the observed document, count(w, ρ) the
number of times the word w occurs in ρ and  |ρ  |the
length of the document. Bayesian smoothing using
Dirichlet priors is then applied to the maximum like-
lihood estimator to compensate for data sparseness.
We propose an approach that extend the basic
LM approach to take into consideration the PICO
element annotation. We assume that each ele-
ment in the document has a different importance
weight. Four more LMs are created, one for each
elements. Given ωe the weight of the PICO element
e, P(w  |Md) in equation 1 is re-defined as:
</bodyText>
<equation confidence="0.9788325">
P1(w  |Md) a P(w  |Md) + � ωe · P(w  |Me)
eE[P,I,C,O1
</equation>
<bodyText confidence="0.999989523809524">
The right hand of the above equation is not a prob-
ability function. We could use a normalization to
transform it. However, for the purpose of document
ranking, this will not make any difference. There-
fore, we will keep the un-normalized value.
We performed an extensive series of experiments
using this model on the test collection described in
Section 5. The results are shown in Table 2. It turns
out that the best improvement we were able to obtain
is very small (0.5% of MAP increase). There may
be several reasons for that. First, the accuracy of
the automatic document tagging may be insufficient.
Second, even if elements are correctly identified in
documents, if queries are treated as bags-of-words
then any PICO element can match with any identi-
cal word in the query, whether it describe the same
element or not. However, we also tested a naive ap-
proach that matches the PICO elements in queries
with the corresponding elements in documents. But
this approach quickly turns out to be too restrictive
and leads to bad results.
</bodyText>
<table confidence="0.75021925">
Weighted elements
Measure
P I / C O Best†
MAP increase 0.0% −0.2% −0.1% +0.5%
</table>
<tableCaption confidence="0.995841">
Table 2: Results using the PICO elements automatically
detected in documents (†: wP = 0.5, wI = 0.2).
</tableCaption>
<bodyText confidence="0.999873428571429">
As we can see, this approach only brings limited
improvement in retrieval effectiveness. This rises
the question of the usability of such tagging method
in its current performance state. We will see in the
next section an alternative solution to this problem
that relies on the distribution of PICO elements in
documents.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="method">
4 Method
</sectionHeader>
<subsectionHeader confidence="0.999864">
4.1 Distribution of PICO elements
</subsectionHeader>
<bodyText confidence="0.9998045">
PICO elements are not evenly distributed in medical
documents, which often follow some implicit writ-
ing convention. An intuitive method is to weight
higher a segment that is more probable to con-
tain PICO elements. The distribution of PICO el-
ements is likely to correlate to the position within
the document. This intuition has been used in most
of the supervised PICO detection methods which
use location-based features. There has been sev-
eral studies that cover the PICO extraction problem.
However, as far as we know, none of them analyses
and uses the positional ditribution of these elements
within the documents for the purpose of IR. Biomed-
ical abstracts can be typically represented by four or-
dered rhetorical categories which are Introduction,
Methods, Results and Discussion (IMRAD) (Sollaci
and Pereira, 2004). The reason is found in the need
for speed when reviewing literature, as this format
allows readers to pick those parts of particular in-
terest. Besides, many scientific journals explicitly
recommended this ordered structure.
The PICO dispersion is highly correlated to these
rhetorical categories as some elements are more
likely to occur in certain categories. For example,
outcomes are more likely to appear in Results and/or
Discussion parts. One could also expect to infer the
</bodyText>
<page confidence="0.968174">
825
</page>
<bodyText confidence="0.983753870967742">
Parts of the abstracts
role played by PICO elements in a clinical study. For
example, the drug pioglitazone has not the same role
in a clinical study if it appears as the main interven-
tion (likely to occur in all parts) or as a comparative
treatment (Methods and/or Results parts).
Instead of analysing the dispersion of PICO ele-
ments into the four IMRAD categories, we choose to
the use automatically splitted parts. There are sev-
eral reasons for that. First, the IMRAD categories
are not explicitely marked in abstracts. An auto-
matic tagging of these would surely result in some
errors. Second, using a low granularity approach
would provide more precise statistics. Furthermore,
if one would use the dispersion of elements as a cri-
terion to estimate how important each part is, an au-
tomatic partition would be a good choice because of
its repeatability and ease to implement.
We divided each manually annotated abstract into
10 parts of equal length (P1 being the begining and
P10 the ending) and computed statistics on the num-
ber of elements than occur in each of these parts.
The Figure 1 shows the proportion of elements for
each part. We can observe that PICO elements are
not evenly distributed throughout the abstracts. Uni-
versally accepted rules that govern medical writing
styles would be the first reason for that. It is clear
that the beginning and ending parts of abstracts do
contain most of the PICO elements. This gives us a
clear indication on which parts should be enhanced
when searching for these elements.
</bodyText>
<equation confidence="0.979825666666667">
P1
P2
P3
P4
P5
P6
P7
P8
P9
P10
8 9 10 11 12 13
% of PICO elements
</equation>
<figureCaption confidence="0.99791">
Figure 1: Proportion of PICO elements computed for
each different part of abstracts.
</figureCaption>
<bodyText confidence="0.999655636363636">
Therefore, there may be several levels of granu-
larity when using the PICO framework in IR. One
can identify each PICO element in the document,
whether it is described by a word, a phrase or a com-
plete sentence. One can also use a coarser-grain
approach, estimating from the distribution across
documents the probability that each part contains a
PICO element. As attempts to precisely locate PICO
elements have shown that this task is particularly
difficult, we propose to get rid this issue by using
the second method.
</bodyText>
<subsectionHeader confidence="0.995809">
4.2 Model definitions
</subsectionHeader>
<bodyText confidence="0.961833227272727">
We propose three different models that extend the
classical language modeling approach. The first uses
the structural information of documents, the second
takes advantage of the PICO query structure while
the third simply combine the first two models.
Model-1
Attempts to precisely locate PICO elements in doc-
uments have shown that this task is particularly dif-
ficult. We propose to get around this issue by intro-
ducing structural markers to convey document struc-
ture and use them as a means of providing location
information. Accordingly, each document is repre-
sented as a series of successive parts. To integrate
document structure into the ranking function, we es-
timate a series of probabilities that constraints the
word counts to a specific part instead of the entire
document. Each document d is then ranked by a
weighted linear interpolation. Intuitively, the weight
of a part should depend on how much information is
conveyed by its words. Given γp the weight of the
part p E [TITLE, P1 · · · P101, P(w I Md) in equation
1 is re-defined as:
</bodyText>
<equation confidence="0.912786666666667">
P2(w  |Md) a P(w  |Md)+ Y_ γp·P(w E p I Md)
pEd
Model-2
</equation>
<bodyText confidence="0.999972">
The PICO formulation of queries provides informa-
tion about the role of each query word. One idea
is to use this structural decomposition to thoroughly
balance elements in the ranking function. For exam-
ple, the weight given to the drug fluoxetine should be
different depending on whether it refers to the inter-
vention or comparison concept. The same goes for
obesity which can be a problem or an outcome. To
</bodyText>
<page confidence="0.996801">
826
</page>
<bodyText confidence="0.997853">
integrate this in the ranking function, we define a pa-
rameter be that represents the weight given to query
words belonging to the element e E [P, I, C, O].
</bodyText>
<equation confidence="0.9594436">
f(w, e) = 1 if w E e, 0 otherwise. We re-defined
P(w  |Md) in equation 1 as:
P3(w  |Mq) a P(w  |Mq)+ Y_ be·f(w,e)·P(w  |Mq)
eE[P,I,C,O]
Model-1+2
</equation>
<bodyText confidence="0.998756">
This is the combination of the two previously de-
scribed models. We re-defined the scoring function
as:
</bodyText>
<equation confidence="0.9466265">
Y_ score(q, d) = P3(w  |Mq) · log P2(w  |Md)
wEq
</equation>
<sectionHeader confidence="0.998466" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999952333333333">
In this section, we describe the details of our exper-
imental protocol. We then present the results ob-
tained with the three proposed models.
</bodyText>
<subsectionHeader confidence="0.845611">
Experimental settings
</subsectionHeader>
<bodyText confidence="0.999989181818182">
We gathered a collection of nearly 1.5 million ab-
stracts from PubMed with the following require-
ments: with abstract, humans subjects, in english
and selecting the following publication types: RCT,
reviews, clinical trials, letters, practice guidelines,
editorials and meta-analysis. Prior to the index con-
struction, each abstract is automatically divided into
10 parts of equal length, abstracts containing less
than 10 words are discarded. The following fields
are then marked: TITLE, P1, P2, ... P10 with P1 be-
ing the begining of the document and P10 the end-
ing.
Unfortunately, there is no standard test collection
appropriate for testing the use of PICO in IR and
we had to manually create one. For queries, we use
the Cochrane systematic reviews4 on 10 clinical
questions about different aspects of “diabetes”.
These reviews contain the best available infor-
mation about an healthcare intervention and are
designed to facilitate the choices that doctors face
in health care. All the documents in the “Included
studies” section are judged to be relevant for the
</bodyText>
<footnote confidence="0.865195">
4www.cochrane.org/reviews/
</footnote>
<bodyText confidence="0.999032714285714">
question. These included studies are selected by
the reviewers (authors of the review article) and
judged to be highly related to the clinical question.
In our experiments, we consider these documents
as relevant ones. From the 10 selected questions,
professors in family medicine have formulated a set
of 52 queries, each of which was manually anno-
tated according to the PICO structure. The resulting
testing corpus is composed of 52 queries (average
length of 14.7 words) and 378 relevant documents.
Below are some of the alternative formulations of
queries for the question “Pioglitazone for type 2
diabetes mellitus”:
In patients with type 2 diabetes (P)  |does pioglita-
zone (I)  |compared to placebo (C)  |reduce stroke
and myocardial infarction (O)
Inpatients with type 2 diabetes who have a high risk
of macrovascular events (P)  |does pioglitazone (I) |
compared to placebo (C)  |reduce mortality (O)
We use cross-validation to determine reasonable
weights and avoid over-fitting. We have divided the
queries into two groups of 26 queries: Qa and Qb.
The best parameters found for Qa are used to test
on Qb, and vice versa. In our experiments, we use
the KL divergence ranking (equation 1) as baseline.
The following evaluation measures are considered
relevant:
Precision at n (P@n). Precision computed on only
the n topmost retrieved documents.
Mean Average Precision (MAP). Average of preci-
sions computed at the point of each relevant docu-
ment in the ranked list of retrieved documents.
MAP is a popular measure that gives a global
quality score of the entire ranked list of retrieved
documents. In the case of clinical searches, one
could also imagine this scenario: a search performed
by a physician who does not have the time to look
into large sets of results, but for whom it is impor-
tant to have relevant results in the top 10. In such
case, P@10 is also an appropriate measure.
Student’s t-test is performed to determine statis-
tical significance. The Lemur Toolkit5 was used for
</bodyText>
<footnote confidence="0.995073">
5www.lemurproject.org
</footnote>
<page confidence="0.996199">
827
</page>
<bodyText confidence="0.9991884">
all retrieval tasks. Experiments were performed with
an “out-of-the-box” version of Lemur, using its tok-
enization algorithm and porter stemmer. The Dirich-
let prior smoothing parameter was set to its default
value µ = 2500.
</bodyText>
<subsectionHeader confidence="0.622518">
Experiments with model-1
</subsectionHeader>
<bodyText confidence="0.999984555555556">
We first investigated whether assigning a weight to
each part of the document can improve the retrieval
accuracy. It is however difficult to determine a set
of reasonable values for all the parts together, as the
value of one part will affect those of the others. In
this study, we perform a two pass tuning. First, we
consider the yp weights to be independent. By doing
so, searching for the optimal weight distribution can
be seen as tuning the weight of each part separately.
When searching the optimal weight of a part, the
weight for other parts is assigned 0. Second, these
approximations of the optimum values are used as
initial weights prior to the second pass. The final
weight distribution is obtained by searching for the
best weight combination around the initial values.
The Figure 2 shows the optimal weight distri-
butions along with the best relative MAP increase
for each part. A noticeable improvement is ob-
tained by increasing the weights associated to the ti-
tle/introduction and conclusion of documents. This
is consistent with the results observed on the dis-
tribution of PICO elements in abstracts. Boosting
middle parts of documents seems to have no impact
at all. We can see that the two yp weight distribu-
tions (1-pass and 2-pass) are very close.
Performance measures obtained by model-1 are
presented in Table 3. With 1-pass tuning, we ob-
serve a MAP score increase of 37.5% and a P@10
increase of 64.1%. After the second pass, scores are
lower with 35% and 60.5% for MAP and P@10 re-
spectively. This result indicates that there is possibly
overfitting when we perform the two pass parameter
tuning. It could also be caused by the limited num-
ber of query in our test collection. However, we can
determine reasonable weights by tuning each part
weight separately.
</bodyText>
<subsectionHeader confidence="0.733615">
Experiments with model-2
</subsectionHeader>
<bodyText confidence="0.999534666666667">
We have seen that a large improvement could come
from weighting each part accordingly. In a second
series of experiments, we try to assign a different
</bodyText>
<figureCaption confidence="0.9992775">
Figure 2: Best MAP increase for each part p (bar charts),
corresponding 1 and 2-pass yp weights are also given.
</figureCaption>
<bodyText confidence="0.999898766666667">
weight to each PICO element in queries. A grid
search was used to find the optimal be weights com-
bination. The results are shown in Table 3.
We observe a MAP score increase of 22.5% and
an increase of 11% in P@10. Though the use of
a PICO weighting scheme increases the retrieval
accuracy, there is clearly much to gain by using
the document structure. The optimal [bp, bi, b,, bo]
weights distribution is [0.3, 1.2, 0, 0.1] for Q and
[0.2,1, 0, 0.2] for Qb. That means that the most im-
portant words in queries belong to the Intervention
element. This supports the manual search strategy
proposed by (Weinfeld and Finkelstein, 2005), in
which they suggested that I and P elements should
be used first to construct queries, and only if too
many results are obtained that other elements should
be considered.
It is interesting to see that query words belonging
to the Comparison element have to be considered
as the least important part of a query. Even more
so because they are in the same semantic group as
the Intervention words. A reason for that could be
the use of vague words such as “no-intervention” or
“placebo”. The methodology employed to construct
the queries is also responsible. Indeed, physicians
have focused on producing alternative formulations
of 10 general clinical questions by predominantly
modifying the one of the PICO elements. As a re-
sult, some of them do share the same vague Com-
parison words.
</bodyText>
<figure confidence="0.993630074074074">
30
Q26A
1-pass
2-pass
20
10
30
20
10
0 Title P1 P2 P3 P4 P5 P6 P7 P8 P9 P10
Different part of the documents
MAP increase (%)
1.0
0.8
0.6
0.4
0.2
1.0
0.8
0.6
0.4
0.2
0.0
Weight parameter γp
Q26B
1-pass
2-pass
</figure>
<page confidence="0.985566">
828
</page>
<table confidence="0.999702142857143">
Experiments MAP P@10
Qb--�Qa Qa--�Qb % Avg. Qb--�Qa Qa--�Qb % Avg.
Baseline 0.118 0.131 0.219 0.239
Model-1 / 1pass 0.165 0.176 +37.5%t 0.377 0.373 +64.1%t
Model-1 / 2pass 0.165 0.170 +35.0%t 0.354 0.381 +60.5%t
Model-2 0.149 0.168 +22.5%t 0.250 0.258 +11.0%
Model-1+2 0.198 0.202 +61.5%t 0.385 0.392 +70.0%t
</table>
<tableCaption confidence="0.842925666666667">
Table 3: Cross-validation (train test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2-
pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged
between Qa and Qb). (1: t.test &lt; 0.01)
</tableCaption>
<subsectionHeader confidence="0.33154">
Experiments with model-1+2
</subsectionHeader>
<bodyText confidence="0.999919538461538">
We have seen that both the use of a location-based
weighting and a PICO-structure weighting scheme
increase the retrieval accuracy. In this last series of
experiments, we analyse the results of their com-
bination. We can observe that fusing model-1 and
model-2 allows us to obtain the best retrieval ac-
curacy with a MAP score increase of 61.5% and a
P@10 increase of 70.0%. It is a large improvement
over the baseline as it means that instead of about
two relevant documents in the top 10, our system
can retrieve nearly four. These results confirm that
both PICO framework and document structure can
be very helpful for the IR process.
</bodyText>
<sectionHeader confidence="0.999359" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999997024390244">
We presented a language modeling approach that in-
tegrates document and PICO structure for the pur-
pose of clinical IR. A straightforward idea is to de-
tect PICO elements in documents and use the ele-
ments in the retrieval process. However, this ap-
proach does not work well because of the diffi-
culty to arrive at a consistent tagging of these ele-
ments. Instead, we propose a less demanding ap-
proach which assigns different weights to different
parts of a document.
We first analysed the distribution of PICO el-
ements in a manually annotated abstracts collec-
tion. The observed results led us to believe that a
location-based weighting scheme can be used in-
stead of a PICO detection approach. We then ex-
plored whether this strategy can be used as an in-
dicator to refine document relevance. We also pro-
posed a model to integrate the PICO information
provided in queries and investigated how each el-
ement should be balanced in the ranking function.
On a data set composed of 1.5 million abstracts ex-
tracted from PubMed, our method obtains an in-
crease of 61.5% for MAP and 70% for P@10 over
the classical language modeling approach.
This work can be much improved in the future.
For example, the location-based weighting method
can be improved in order to model a different weight
distribution for each PICO element. As the distri-
bution in abstracts is not the same among PICO el-
ements, it is expected that differentiated weighting
schemes could result in better retrieval effectiveness.
In a similar perspective, we are continuing our ef-
forts to construct a larger manually annotated col-
lection of abstracts. It will be thereafter conceiv-
able to use this data to infer the structural weighting
schemes or to train a more precise PICO detection
method. The focused evaluation described in this
paper is a first step. Although the queries are limited
to diabetes, this does not affect the general PICO
structure in queries. We plan to extend the coverage
of queries to other topics in the future.
</bodyText>
<sectionHeader confidence="0.997555" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.971847">
The work described in this paper was funded by
the Social Sciences and Humanities Research Coun-
cil (SSHRC). The authors would like to thank Dr.
Ann McKibbon, Dr. Dina Demner-Fushman, Lorie
Kloda, Laura Shea, Lucas Baire and Lixin Shi for
their contribution in the project.
</bodyText>
<page confidence="0.998075">
829
</page>
<sectionHeader confidence="0.995862" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901109375">
A. Andrenucci. 2008. Automated Question-Answering
Techniques and the Medical Domain. In International
Conference on Health Informatics, volume 2, pages
207–212.
A.R. Aronson. 2001. Effective Mapping of Biomedical
Text to the UMLS Metathesaurus: The MetaMap Pro-
gram. In AMIA Symposium.
G. Chung. 2009. Sentence retrieval for abstracts of ran-
domized controlled trials. BMC Medical Informatics
and Decision Making, 9(1):10.
M. Dawes, P. Pluye, L. Shea, R. Grad, A. Green-
berg, and J.Y. Nie. 2007. The identification of
clinically important elements within medical jour-
nal abstracts: Patient-Population-Problem, Exposure-
Intervention, Comparison, Outcome, Duration and
Results (PECODR). Informatics in Primary care,
15(1):9–16.
D. Demner-Fushman and J. Lin. 2006. Answer extrac-
tion, semantic clustering, and extractive summariza-
tion for clinical question answering. In ACL.
D. Demner-Fushman and J. Lin. 2007. Answering
clinical questions with knowledge-based and statistical
techniques. Computational Linguistics, 33(1):63–103.
N. Fuhr and K. Großjohann. 2001. XIRQL: A query
language for information retrieval in XML documents.
In SIGIR, pages 172–180.
M.J. Hansen, N.O. Rasmussen, and G. Chung. 2008. A
method of extracting the number of trial participants
from abstracts describing randomized controlled trials.
Journal of Telemedicine and Telecare, 14(7):354–358.
INEX. 2002-2009. Proceedings of the INitiative for the
Evaluation of XML Retrieval (INEX) workshop.
J. Kamps, M. Marx, M. de Rijke, and B. Sigurbj¨ornsson.
2005. Structured queries in XML retrieval. In CIKM,
pages 4–11.
J.M. Ponte and W.B. Croft. 1998. A language model-
ing approach to information retrieval. In SIGIR, pages
275–281.
T.C. Rindflesch and M. Fiszman. 2003. The interac-
tion of domain knowledge and linguistic structure in
natural language processing: interpreting hypernymic
propositions in biomedical text. Journal of Biomedical
Informatics, 36(6):462–477.
D.L. Sackett, W. Rosenberg, J.A. Gray, R.B. Haynes, and
W.S. Richardson. 1996. Evidence based medicine:
what it is and what it isn’t. British medical journal,
312(7023):71.
C. Schardt, M. Adams, T. Owens, S. Keitz, and
P. Fontelo. 2007. Utilization of the PICO frame-
work to improve searching PubMed for clinical ques-
tions. BMC Medical Informatics and Decision Mak-
ing, 7(1):16.
L.B. Sollaci and M.G. Pereira. 2004. The introduction,
methods, results, and discussion (IMRAD) structure:
a fifty-year survey. Journal of the Medical Library
Association, 92(3):364.
A. Trotman. 2005. Choosing document structure
weights. Information Processing and Management,
41(2):243–264.
J.M. Weinfeld and K. Finkelstein. 2005. How to answer
your clinical questions more efficiently. Family prac-
tice management, 12(7):37.
R. Wilkinson. 1994. Effective retrieval of structured doc-
uments. In SIGIR, pages 311–317.
</reference>
<page confidence="0.997769">
830
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.070437">
<title confidence="0.999392">Clinical Information Retrieval using Document and PICO Structure</title>
<author confidence="0.972954">Florian Boudin</author>
<author confidence="0.972954">Jian-Yun</author>
<affiliation confidence="0.583978">DIRO, Universit´e de</affiliation>
<address confidence="0.4505005">CP. 6128, succursale Montr´eal, H3C 3J7 Qu´ebec,</address>
<author confidence="0.700903">Martin</author>
<affiliation confidence="0.816472">Department of Family McGill University, 515 Pine Ave</affiliation>
<address confidence="0.674249">Montr´eal, H2W 1S4 Qu´ebec,</address>
<email confidence="0.99596">martin.dawes@mcgill.ca</email>
<abstract confidence="0.9955358125">In evidence-based medicine, clinical questions involve four aspects: Patient/Problem (P), Intervention (I), Comparison (C) and Outcome (O), known as PICO elements. In this paper we present a method that extends the language modeling approach to incorporate both document structure and PICO query formulation. We present an analysis of the distribution of PICO elements in medical abstracts that motivates the use of a location-based weighting strategy. In experiments carried out on a collection of 1.5 million abstracts, the method was found to lead to an improvement of roughly 60% in MAP and 70% in P@10 as compared to state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Andrenucci</author>
</authors>
<title>Automated Question-Answering Techniques and the Medical Domain.</title>
<date>2008</date>
<booktitle>In International Conference on Health Informatics,</booktitle>
<volume>2</volume>
<pages>207--212</pages>
<contexts>
<context position="5861" citStr="Andrenucci, 2008" startWordPosition="933" endWordPosition="934">. Next, we present our experiments and results. Lastly, we conclude with a discussion and directions for future work. 2 Related work There have been only a few studies trying to use PICO elements in the retrieval process. (DemnerFushman and Lin, 2007) is one of the few such studies. The method they describe consists in re-ranking an initial list of retrieved citations. To this end, the relevance of a document is scored by the use of detected PICO elements, among other things. Several other studies aimed to build a Question-Answering system for clinical questions (Demner-Fushman and Lin, 2006; Andrenucci, 2008). But again, the focus has been set on the post-retrieval step, while the document retrieval step only uses a standard approach. In this paper, we argue that IR has much to gain by using PICO elements. The task of identifying PICO elements has however gain more attention. In their paper, (DemnerFushman and Lin, 2007) presented a method that uses either manually crafted pattern-matching rules or a combination of basic classifiers to detect PICO elements in medical abstracts. Prior to that, biomedical concepts are labelled by Metamap (Aronson, 2001) while relations between these concepts are ext</context>
</contexts>
<marker>Andrenucci, 2008</marker>
<rawString>A. Andrenucci. 2008. Automated Question-Answering Techniques and the Medical Domain. In International Conference on Health Informatics, volume 2, pages 207–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Aronson</author>
</authors>
<title>Effective Mapping of Biomedical Text to the UMLS Metathesaurus: The MetaMap Program.</title>
<date>2001</date>
<booktitle>In AMIA Symposium.</booktitle>
<contexts>
<context position="6414" citStr="Aronson, 2001" startWordPosition="1025" endWordPosition="1026">al questions (Demner-Fushman and Lin, 2006; Andrenucci, 2008). But again, the focus has been set on the post-retrieval step, while the document retrieval step only uses a standard approach. In this paper, we argue that IR has much to gain by using PICO elements. The task of identifying PICO elements has however gain more attention. In their paper, (DemnerFushman and Lin, 2007) presented a method that uses either manually crafted pattern-matching rules or a combination of basic classifiers to detect PICO elements in medical abstracts. Prior to that, biomedical concepts are labelled by Metamap (Aronson, 2001) while relations between these concepts are extracted with SemRep (Rindflesch and Fiszman, 2003). Recently, supervised classification using Support Vector Machines (SVM) was proposed by (Hansen et al., 2008) to extract the number of trial participants. In a later study, (Chung, 2009) extended this work to other elements using Conditional Random Fields. Although these studies are reporting interesting results, they are limited in several aspects. First, many are restricted to some segments of the medical documents (e.g. Method section) (Chung, 2009), and in most cases, the test collection is ve</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>A.R. Aronson. 2001. Effective Mapping of Biomedical Text to the UMLS Metathesaurus: The MetaMap Program. In AMIA Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Chung</author>
</authors>
<title>Sentence retrieval for abstracts of randomized controlled trials.</title>
<date>2009</date>
<journal>BMC Medical Informatics and Decision Making,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="3028" citStr="Chung, 2009" startWordPosition="459" endWordPosition="460"> of pain and fever?” one can identify the following PICO elements: Patient/Problem: children/pain and fever Intervention: paracetamol Comparison: ibuprofen Outcome: levels ofpain and fever Very little work, if any, has been carried out on the use of these elements in the Information Retrieval (IR) process. There are several reasons for that. It is not easy to identify PICO elements in documents, as well as in the question if these are not explicitly separated in it. Several studies have been performed on identifying PICO elements in abstracts (DemnerFushman and Lin, 2007; Hansen et al., 2008; Chung, 2009). However, all of them are reporting coarsegrain (sentence-level) tagging methods that have not yet been shown to be sufficient for the purpose of IR. Moreover, there is currently no standard test collection of questions in PICO structure available for evaluation. On the other hand, the most critical aspect in IR is term weighting. One of the purpose of tagging PICO elements is to assign appropriate weights to these elements during the retrieval process. From this perspective, a semantic tagging of PICO elements may be a task that goes well beyond 822 Human Language Technologies: The 2010 Annu</context>
<context position="6698" citStr="Chung, 2009" startWordPosition="1066" endWordPosition="1067"> PICO elements has however gain more attention. In their paper, (DemnerFushman and Lin, 2007) presented a method that uses either manually crafted pattern-matching rules or a combination of basic classifiers to detect PICO elements in medical abstracts. Prior to that, biomedical concepts are labelled by Metamap (Aronson, 2001) while relations between these concepts are extracted with SemRep (Rindflesch and Fiszman, 2003). Recently, supervised classification using Support Vector Machines (SVM) was proposed by (Hansen et al., 2008) to extract the number of trial participants. In a later study, (Chung, 2009) extended this work to other elements using Conditional Random Fields. Although these studies are reporting interesting results, they are limited in several aspects. First, many are restricted to some segments of the medical documents (e.g. Method section) (Chung, 2009), and in most cases, the test collection is very small (a few hundreds abstracts). Second, the precision and granularity of these methods have not yet been shown to be sufficient for the purpose of IR. The structural information provided by markup languages (e.g. XML) has been successfully used to improve the IR effectiveness (I</context>
</contexts>
<marker>Chung, 2009</marker>
<rawString>G. Chung. 2009. Sentence retrieval for abstracts of randomized controlled trials. BMC Medical Informatics and Decision Making, 9(1):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dawes</author>
<author>P Pluye</author>
<author>L Shea</author>
<author>R Grad</author>
<author>A Greenberg</author>
<author>J Y Nie</author>
</authors>
<title>The identification of clinically important elements within medical journal abstracts: Patient-Population-Problem,</title>
<date>2007</date>
<booktitle>ExposureIntervention, Comparison, Outcome, Duration and Results (PECODR). Informatics in Primary care,</booktitle>
<pages>15--1</pages>
<contexts>
<context position="2163" citStr="Dawes et al., 2007" startWordPosition="322" endWordPosition="325">ch research study and then applying the findings in clinical practice. However, this is a time consuming activity. One way to facilitate searching for a precise answer is to formulate a well-focused and structured question (Schardt et al., 2007). Physicians are educated to formulate their clinical questions according to several well defined aspects in EBM: Patient/Problem (P), Intervention (I), Comparison (C) and Outcome (O), which are called PICO elements. In many documents in medical literature (e.g. MEDLINE), one can find the elements of the PICO structure, but rarely explicitly annotated (Dawes et al., 2007). To identify documents corresponding to a patient’s state, physicians also construct their queries according to the PICO structure. For example, in the question “In children with pain and fever how does paracetamol compared with ibuprofen affect levels of pain and fever?” one can identify the following PICO elements: Patient/Problem: children/pain and fever Intervention: paracetamol Comparison: ibuprofen Outcome: levels ofpain and fever Very little work, if any, has been carried out on the use of these elements in the Information Retrieval (IR) process. There are several reasons for that. It </context>
</contexts>
<marker>Dawes, Pluye, Shea, Grad, Greenberg, Nie, 2007</marker>
<rawString>M. Dawes, P. Pluye, L. Shea, R. Grad, A. Greenberg, and J.Y. Nie. 2007. The identification of clinically important elements within medical journal abstracts: Patient-Population-Problem, ExposureIntervention, Comparison, Outcome, Duration and Results (PECODR). Informatics in Primary care, 15(1):9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Demner-Fushman</author>
<author>J Lin</author>
</authors>
<title>Answer extraction, semantic clustering, and extractive summarization for clinical question answering.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5842" citStr="Demner-Fushman and Lin, 2006" startWordPosition="929" endWordPosition="932">ption of the method we propose. Next, we present our experiments and results. Lastly, we conclude with a discussion and directions for future work. 2 Related work There have been only a few studies trying to use PICO elements in the retrieval process. (DemnerFushman and Lin, 2007) is one of the few such studies. The method they describe consists in re-ranking an initial list of retrieved citations. To this end, the relevance of a document is scored by the use of detected PICO elements, among other things. Several other studies aimed to build a Question-Answering system for clinical questions (Demner-Fushman and Lin, 2006; Andrenucci, 2008). But again, the focus has been set on the post-retrieval step, while the document retrieval step only uses a standard approach. In this paper, we argue that IR has much to gain by using PICO elements. The task of identifying PICO elements has however gain more attention. In their paper, (DemnerFushman and Lin, 2007) presented a method that uses either manually crafted pattern-matching rules or a combination of basic classifiers to detect PICO elements in medical abstracts. Prior to that, biomedical concepts are labelled by Metamap (Aronson, 2001) while relations between the</context>
</contexts>
<marker>Demner-Fushman, Lin, 2006</marker>
<rawString>D. Demner-Fushman and J. Lin. 2006. Answer extraction, semantic clustering, and extractive summarization for clinical question answering. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Demner-Fushman</author>
<author>J Lin</author>
</authors>
<title>Answering clinical questions with knowledge-based and statistical techniques.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<marker>Demner-Fushman, Lin, 2007</marker>
<rawString>D. Demner-Fushman and J. Lin. 2007. Answering clinical questions with knowledge-based and statistical techniques. Computational Linguistics, 33(1):63–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuhr</author>
<author>K Großjohann</author>
</authors>
<title>XIRQL: A query language for information retrieval in XML documents.</title>
<date>2001</date>
<booktitle>In SIGIR,</booktitle>
<pages>172--180</pages>
<contexts>
<context position="7743" citStr="Fuhr and Großjohann, 2001" startWordPosition="1236" endWordPosition="1239">t been shown to be sufficient for the purpose of IR. The structural information provided by markup languages (e.g. XML) has been successfully used to improve the IR effectiveness (INEX, 2002 2009). For such documents, the structure information can be used to emphasize some particular parts of the document. Thereby, a given word should not have the same importance depending on its position in the document structure. Taking into account the structure can be done either at the step of querying or at the step of indexing. One way to integrate the structure at querying is to adapt query languages (Fuhr and Großjohann, 2001). These approaches follow the assumption that the user knows where the most relevant information is located. However, (Kamps et al., 2005) showed that it is preferable to use structure as a search hint, and not as a strict search requirement The second approach consists in integrating the document structure at the indexing step by introducing a structure weighting scheme (Wilkinson, 1994). In such a scheme, the weight assigned to a word is not only based on its frequency but also on its position in the document. The structure of a document can be defined in terms of tags (e.g. title, section),</context>
</contexts>
<marker>Fuhr, Großjohann, 2001</marker>
<rawString>N. Fuhr and K. Großjohann. 2001. XIRQL: A query language for information retrieval in XML documents. In SIGIR, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Hansen</author>
<author>N O Rasmussen</author>
<author>G Chung</author>
</authors>
<title>A method of extracting the number of trial participants from abstracts describing randomized controlled trials.</title>
<date>2008</date>
<journal>Journal of Telemedicine and Telecare,</journal>
<volume>14</volume>
<issue>7</issue>
<contexts>
<context position="3014" citStr="Hansen et al., 2008" startWordPosition="455" endWordPosition="458">uprofen affect levels of pain and fever?” one can identify the following PICO elements: Patient/Problem: children/pain and fever Intervention: paracetamol Comparison: ibuprofen Outcome: levels ofpain and fever Very little work, if any, has been carried out on the use of these elements in the Information Retrieval (IR) process. There are several reasons for that. It is not easy to identify PICO elements in documents, as well as in the question if these are not explicitly separated in it. Several studies have been performed on identifying PICO elements in abstracts (DemnerFushman and Lin, 2007; Hansen et al., 2008; Chung, 2009). However, all of them are reporting coarsegrain (sentence-level) tagging methods that have not yet been shown to be sufficient for the purpose of IR. Moreover, there is currently no standard test collection of questions in PICO structure available for evaluation. On the other hand, the most critical aspect in IR is term weighting. One of the purpose of tagging PICO elements is to assign appropriate weights to these elements during the retrieval process. From this perspective, a semantic tagging of PICO elements may be a task that goes well beyond 822 Human Language Technologies:</context>
<context position="6621" citStr="Hansen et al., 2008" startWordPosition="1051" endWordPosition="1054">er, we argue that IR has much to gain by using PICO elements. The task of identifying PICO elements has however gain more attention. In their paper, (DemnerFushman and Lin, 2007) presented a method that uses either manually crafted pattern-matching rules or a combination of basic classifiers to detect PICO elements in medical abstracts. Prior to that, biomedical concepts are labelled by Metamap (Aronson, 2001) while relations between these concepts are extracted with SemRep (Rindflesch and Fiszman, 2003). Recently, supervised classification using Support Vector Machines (SVM) was proposed by (Hansen et al., 2008) to extract the number of trial participants. In a later study, (Chung, 2009) extended this work to other elements using Conditional Random Fields. Although these studies are reporting interesting results, they are limited in several aspects. First, many are restricted to some segments of the medical documents (e.g. Method section) (Chung, 2009), and in most cases, the test collection is very small (a few hundreds abstracts). Second, the precision and granularity of these methods have not yet been shown to be sufficient for the purpose of IR. The structural information provided by markup langu</context>
</contexts>
<marker>Hansen, Rasmussen, Chung, 2008</marker>
<rawString>M.J. Hansen, N.O. Rasmussen, and G. Chung. 2008. A method of extracting the number of trial participants from abstracts describing randomized controlled trials. Journal of Telemedicine and Telecare, 14(7):354–358.</rawString>
</citation>
<citation valid="false">
<booktitle>Proceedings of the INitiative for the Evaluation of XML Retrieval (INEX) workshop.</booktitle>
<marker></marker>
<rawString>INEX. 2002-2009. Proceedings of the INitiative for the Evaluation of XML Retrieval (INEX) workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kamps</author>
<author>M Marx</author>
<author>M de Rijke</author>
<author>B Sigurbj¨ornsson</author>
</authors>
<date>2005</date>
<booktitle>Structured queries in XML retrieval. In CIKM,</booktitle>
<pages>4--11</pages>
<marker>Kamps, Marx, de Rijke, Sigurbj¨ornsson, 2005</marker>
<rawString>J. Kamps, M. Marx, M. de Rijke, and B. Sigurbj¨ornsson. 2005. Structured queries in XML retrieval. In CIKM, pages 4–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In SIGIR,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="12178" citStr="Ponte and Croft, 1998" startWordPosition="1956" endWordPosition="1959">for each element, allowing to label the corresponding sentence. We use several algorithms implemented in the Weka toolkit3: decision trees, SVM, multi-layer perceptron and Naive Bayes. Combining multiple classifiers using a weighted linear combination of their prediction scores achieves the best results with a f-measure score of 86.3% for P, 67% for I/C and 56.6% for O in 10-fold cross-validation. 3.3 Use of detected elements in IR We use language modeling approach to IR in this work. The idea is that a document is a good match to a query if its language model is likely to generate the query (Ponte and Croft, 1998). It is one of the stateof-the-art approaches in current IR research. Most language modeling work in IR use unigram language models –also called bags-of-words models– assuming that there is no structure in queries or documents. A typical way to score a document d as relevant to a query q is to use the Kullback-Leibler 2www.nlm.nih.gov/mesh/ 3www.cs.waikato.ac.nz/ml/index.html 824 divergence between their respective LMs: �score(q, d) = P(w |Mq) · log P(w |Md) (1) wEq a −KL(Mq ||Md) where Mq is the LM of the query and Md the LM of the document. P(w |Mρ) estimates the probability of the word w gi</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>J.M. Ponte and W.B. Croft. 1998. A language modeling approach to information retrieval. In SIGIR, pages 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Rindflesch</author>
<author>M Fiszman</author>
</authors>
<title>The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text.</title>
<date>2003</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>36</volume>
<issue>6</issue>
<contexts>
<context position="6510" citStr="Rindflesch and Fiszman, 2003" startWordPosition="1036" endWordPosition="1039">us has been set on the post-retrieval step, while the document retrieval step only uses a standard approach. In this paper, we argue that IR has much to gain by using PICO elements. The task of identifying PICO elements has however gain more attention. In their paper, (DemnerFushman and Lin, 2007) presented a method that uses either manually crafted pattern-matching rules or a combination of basic classifiers to detect PICO elements in medical abstracts. Prior to that, biomedical concepts are labelled by Metamap (Aronson, 2001) while relations between these concepts are extracted with SemRep (Rindflesch and Fiszman, 2003). Recently, supervised classification using Support Vector Machines (SVM) was proposed by (Hansen et al., 2008) to extract the number of trial participants. In a later study, (Chung, 2009) extended this work to other elements using Conditional Random Fields. Although these studies are reporting interesting results, they are limited in several aspects. First, many are restricted to some segments of the medical documents (e.g. Method section) (Chung, 2009), and in most cases, the test collection is very small (a few hundreds abstracts). Second, the precision and granularity of these methods have</context>
</contexts>
<marker>Rindflesch, Fiszman, 2003</marker>
<rawString>T.C. Rindflesch and M. Fiszman. 2003. The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text. Journal of Biomedical Informatics, 36(6):462–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Sackett</author>
<author>W Rosenberg</author>
<author>J A Gray</author>
<author>R B Haynes</author>
<author>W S Richardson</author>
</authors>
<title>Evidence based medicine: what it is and what it isn’t. British medical journal,</title>
<date>1996</date>
<pages>312--7023</pages>
<contexts>
<context position="1261" citStr="Sackett et al., 1996" startWordPosition="185" endWordPosition="188">ent structure and PICO query formulation. We present an analysis of the distribution of PICO elements in medical abstracts that motivates the use of a location-based weighting strategy. In experiments carried out on a collection of 1.5 million abstracts, the method was found to lead to an improvement of roughly 60% in MAP and 70% in P@10 as compared to state-of-the-art methods. 1 Introduction As the volume of published medical literature continues to grow exponentially, there is more and more research for physicians to assess and evaluate and less time to do so. Evidence-based medicine (EBM) (Sackett et al., 1996) is a widely accepted paradigm in medical practice that relies on evidence from patient-centered clinical research to make decisions. Taking an evidence-based approach to searching means doing a systematic search of all the available literature, individually critically appraising each research study and then applying the findings in clinical practice. However, this is a time consuming activity. One way to facilitate searching for a precise answer is to formulate a well-focused and structured question (Schardt et al., 2007). Physicians are educated to formulate their clinical questions accordin</context>
</contexts>
<marker>Sackett, Rosenberg, Gray, Haynes, Richardson, 1996</marker>
<rawString>D.L. Sackett, W. Rosenberg, J.A. Gray, R.B. Haynes, and W.S. Richardson. 1996. Evidence based medicine: what it is and what it isn’t. British medical journal, 312(7023):71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Schardt</author>
<author>M Adams</author>
<author>T Owens</author>
<author>S Keitz</author>
<author>P Fontelo</author>
</authors>
<title>Utilization of the PICO framework to improve searching PubMed for clinical questions.</title>
<date>2007</date>
<journal>BMC Medical Informatics and Decision Making,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="1789" citStr="Schardt et al., 2007" startWordPosition="265" endWordPosition="268">ess and evaluate and less time to do so. Evidence-based medicine (EBM) (Sackett et al., 1996) is a widely accepted paradigm in medical practice that relies on evidence from patient-centered clinical research to make decisions. Taking an evidence-based approach to searching means doing a systematic search of all the available literature, individually critically appraising each research study and then applying the findings in clinical practice. However, this is a time consuming activity. One way to facilitate searching for a precise answer is to formulate a well-focused and structured question (Schardt et al., 2007). Physicians are educated to formulate their clinical questions according to several well defined aspects in EBM: Patient/Problem (P), Intervention (I), Comparison (C) and Outcome (O), which are called PICO elements. In many documents in medical literature (e.g. MEDLINE), one can find the elements of the PICO structure, but rarely explicitly annotated (Dawes et al., 2007). To identify documents corresponding to a patient’s state, physicians also construct their queries according to the PICO structure. For example, in the question “In children with pain and fever how does paracetamol compared w</context>
</contexts>
<marker>Schardt, Adams, Owens, Keitz, Fontelo, 2007</marker>
<rawString>C. Schardt, M. Adams, T. Owens, S. Keitz, and P. Fontelo. 2007. Utilization of the PICO framework to improve searching PubMed for clinical questions. BMC Medical Informatics and Decision Making, 7(1):16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L B Sollaci</author>
<author>M G Pereira</author>
</authors>
<title>The introduction, methods, results, and discussion (IMRAD) structure: a fifty-year survey.</title>
<date>2004</date>
<journal>Journal of the Medical Library Association,</journal>
<volume>92</volume>
<issue>3</issue>
<contexts>
<context position="15965" citStr="Sollaci and Pereira, 2004" startWordPosition="2602" endWordPosition="2605">ntain PICO elements. The distribution of PICO elements is likely to correlate to the position within the document. This intuition has been used in most of the supervised PICO detection methods which use location-based features. There has been several studies that cover the PICO extraction problem. However, as far as we know, none of them analyses and uses the positional ditribution of these elements within the documents for the purpose of IR. Biomedical abstracts can be typically represented by four ordered rhetorical categories which are Introduction, Methods, Results and Discussion (IMRAD) (Sollaci and Pereira, 2004). The reason is found in the need for speed when reviewing literature, as this format allows readers to pick those parts of particular interest. Besides, many scientific journals explicitly recommended this ordered structure. The PICO dispersion is highly correlated to these rhetorical categories as some elements are more likely to occur in certain categories. For example, outcomes are more likely to appear in Results and/or Discussion parts. One could also expect to infer the 825 Parts of the abstracts role played by PICO elements in a clinical study. For example, the drug pioglitazone has no</context>
</contexts>
<marker>Sollaci, Pereira, 2004</marker>
<rawString>L.B. Sollaci and M.G. Pereira. 2004. The introduction, methods, results, and discussion (IMRAD) structure: a fifty-year survey. Journal of the Medical Library Association, 92(3):364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Trotman</author>
</authors>
<title>Choosing document structure weights.</title>
<date>2005</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>41--2</pages>
<contexts>
<context position="8499" citStr="Trotman, 2005" startWordPosition="1367" endWordPosition="1368">wed that it is preferable to use structure as a search hint, and not as a strict search requirement The second approach consists in integrating the document structure at the indexing step by introducing a structure weighting scheme (Wilkinson, 1994). In such a scheme, the weight assigned to a word is not only based on its frequency but also on its position in the document. The structure of a document can be defined in terms of tags (e.g. title, section), 823 each of those having a weight chosen either empirically or automatically by the use of optimizing techniques such as genetic algorithms (Trotman, 2005). 3 Using PICO elements in retrieval In this section, we present an experiment on the manual annotation of PICO elements. We then describe an approach to detect these elements in documents and give some results on the use of these tagged elements in the retrieval process. 3.1 Manual annotation of PICO elements We asked medical professionals to manually annotate the PICO elements in a small collection of abstracts from PubMed1. The instructions given to the annotators were fairly simple. They were asked to precisely annotate all PICO elements in abstracts with no restriction about the size of t</context>
</contexts>
<marker>Trotman, 2005</marker>
<rawString>A. Trotman. 2005. Choosing document structure weights. Information Processing and Management, 41(2):243–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Weinfeld</author>
<author>K Finkelstein</author>
</authors>
<title>How to answer your clinical questions more efficiently. Family practice management,</title>
<date>2005</date>
<pages>12--7</pages>
<contexts>
<context position="26871" citStr="Weinfeld and Finkelstein, 2005" startWordPosition="4443" endWordPosition="4446">eight to each PICO element in queries. A grid search was used to find the optimal be weights combination. The results are shown in Table 3. We observe a MAP score increase of 22.5% and an increase of 11% in P@10. Though the use of a PICO weighting scheme increases the retrieval accuracy, there is clearly much to gain by using the document structure. The optimal [bp, bi, b,, bo] weights distribution is [0.3, 1.2, 0, 0.1] for Q and [0.2,1, 0, 0.2] for Qb. That means that the most important words in queries belong to the Intervention element. This supports the manual search strategy proposed by (Weinfeld and Finkelstein, 2005), in which they suggested that I and P elements should be used first to construct queries, and only if too many results are obtained that other elements should be considered. It is interesting to see that query words belonging to the Comparison element have to be considered as the least important part of a query. Even more so because they are in the same semantic group as the Intervention words. A reason for that could be the use of vague words such as “no-intervention” or “placebo”. The methodology employed to construct the queries is also responsible. Indeed, physicians have focused on produ</context>
</contexts>
<marker>Weinfeld, Finkelstein, 2005</marker>
<rawString>J.M. Weinfeld and K. Finkelstein. 2005. How to answer your clinical questions more efficiently. Family practice management, 12(7):37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilkinson</author>
</authors>
<title>Effective retrieval of structured documents.</title>
<date>1994</date>
<booktitle>In SIGIR,</booktitle>
<pages>311--317</pages>
<contexts>
<context position="8134" citStr="Wilkinson, 1994" startWordPosition="1300" endWordPosition="1301">cument structure. Taking into account the structure can be done either at the step of querying or at the step of indexing. One way to integrate the structure at querying is to adapt query languages (Fuhr and Großjohann, 2001). These approaches follow the assumption that the user knows where the most relevant information is located. However, (Kamps et al., 2005) showed that it is preferable to use structure as a search hint, and not as a strict search requirement The second approach consists in integrating the document structure at the indexing step by introducing a structure weighting scheme (Wilkinson, 1994). In such a scheme, the weight assigned to a word is not only based on its frequency but also on its position in the document. The structure of a document can be defined in terms of tags (e.g. title, section), 823 each of those having a weight chosen either empirically or automatically by the use of optimizing techniques such as genetic algorithms (Trotman, 2005). 3 Using PICO elements in retrieval In this section, we present an experiment on the manual annotation of PICO elements. We then describe an approach to detect these elements in documents and give some results on the use of these tagg</context>
</contexts>
<marker>Wilkinson, 1994</marker>
<rawString>R. Wilkinson. 1994. Effective retrieval of structured documents. In SIGIR, pages 311–317.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>