<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.098315">
<title confidence="0.966793">
DIRECTL: a Language-Independent Approach to Transliteration
</title>
<author confidence="0.997084">
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou, Kenneth Dwyer, Grzegorz Kondrak
</author>
<affiliation confidence="0.9984965">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.90884">
Edmonton, AB, T6G 2E8, Canada
</address>
<email confidence="0.999317">
{sj,abhargava,qdou,dwyer,kondrak}@cs.ualberta.ca
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952090909091">
We present DIRECTL: an online discrimi-
native sequence prediction model that em-
ploys a many-to-many alignment between
target and source. Our system incorpo-
rates input segmentation, target charac-
ter prediction, and sequence modeling in
a unified dynamic programming frame-
work. Experimental results suggest that
DIRECTL is able to independently dis-
cover many of the language-specific reg-
ularities in the training data.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999501689655172">
In the transliteration task, it seems intuitively im-
portant to take into consideration the specifics of
the languages in question. Of particular impor-
tance is the relative character length of the source
and target names, which vary widely depending on
whether languages employ alphabetic, syllabic, or
ideographic scripts. On the other hand, faced with
the reality of thousands of potential language pairs
that involve transliteration, the idea of a language-
independent approach is highly attractive.
In this paper, we present DIRECTL: a translit-
eration system that, in principle, can be applied to
any language pair. DIRECTL treats the transliter-
ation task as a sequence prediction problem: given
an input sequence of characters in the source lan-
guage, it produces the most likely sequence of
characters in the target language. In Section 2,
we discuss the alignment of character substrings
in the source and target languages. Our transcrip-
tion model, described in Section 3, is based on
an online discriminative training algorithm that
makes it possible to efficiently learn the weights
of a large number of features. In Section 4, we
provide details of alternative approaches that in-
corporate language-specific information. Finally,
in Section 5 and 6, we compare the experimental
results of DIRECTL with its variants that incor-
porate language-specific pre-processing, phonetic
alignment, and manual data correction.
</bodyText>
<sectionHeader confidence="0.98967" genericHeader="method">
2 Transliteration alignment
</sectionHeader>
<bodyText confidence="0.999968148148148">
In the transliteration task, training data consist of
word pairs that map source language words to
words in the target language. The matching be-
tween character substrings in the source word and
target word is not explicitly provided. These hid-
den relationships are generally known as align-
ments. In this section, we describe an EM-based
many-to-many alignment algorithm employed by
DIRECTL. In Section 4, we discuss an alternative
phonetic alignment method.
We apply an unsupervised many-to-many align-
ment algorithm (Jiampojamarn et al., 2007) to the
transliteration task. The algorithm follows the ex-
pectation maximization (EM) paradigm. In the
expectation step shown in Algorithm 1, partial
counts γ of the possible substring alignments are
collected from each word pair (xT, yV ) in the
training data; T and V represent the lengths of
words x and y, respectively. The forward prob-
ability α is estimated by summing the probabili-
ties of all possible sequences of substring pairings
from left to right. The FORWARD-M2M procedure
is similar to lines 5 through 12 of Algorithm 1, ex-
cept that it uses Equation 1 on line 8, Equation 2
on line 12, and initializes α0,0 := 1. Likewise, the
backward probability Q is estimated by summing
the probabilities from right to left.
</bodyText>
<equation confidence="0.991941">
t
αt,v += S(xt−i+1, E)αt−i,v (1)
αt,v += S(xtt−i+1,yvv−j+1)αt−i,v−j (2)
</equation>
<bodyText confidence="0.9987988">
The maxX and maxY variables specify the
maximum length of substrings that are permitted
when creating alignments. Also, for flexibility, we
allow a substring in the source word to be aligned
with a “null” letter (E) in the target word.
</bodyText>
<page confidence="0.990818">
28
</page>
<note confidence="0.978562">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 28–31,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<equation confidence="0.986927470588235">
Algorithm 1: Expectation-M2M alignment
Input: xT, yV , maxX, maxY, γ
Output: γ
1 α := FORWARD-M2M (xT , yV , maxX, maxY )
2 β := BACKWARD-M2M (xT, yV , maxX, maxY)
3 if (αT,V = 0) then
4 return
5 fort = 0 ... T , v = 0 ... V do
6 if (t &gt; 0) then
7 for i = 1... maxX st t − i &gt; 0 do
t αt−i,vδ(xt—i+1,ǫ)βt,v
8 γ(xt−i+1, ǫ) + = αt
9 if (v &gt; 0 ∧ t &gt; 0) then
10 for i = 1... maxX st t − i &gt; 0 do
11 for j = 1 ... maxY st v − j &gt; 0 do
t v αt−i,v−j δ(xt−i+1t,yv−vj+1)βt,v
12γ(xt−i+1,yv−j+1) += αT,V
</equation>
<bodyText confidence="0.999975714285714">
In the maximization step, we normalize the par-
tial counts γ to the alignment probability δ using
the conditional probability distribution. The EM
steps are repeated until the alignment probability
δ converges. Finally, the most likely alignment for
each word pair in the training data is computed
with the standard Viterbi algorithm.
</bodyText>
<sectionHeader confidence="0.991476" genericHeader="method">
3 Discriminative training
</sectionHeader>
<bodyText confidence="0.999986407407408">
We adapt the online discriminative training frame-
work described in (Jiampojamarn et al., 2008) to
the transliteration task. Once the training data has
been aligned, we can hypothesize that the ith let-
ter substring xi E x in a source language word
is transliterated into the ith substring yi E y in
the target language word. Each word pair is rep-
resented as a feature vector Φ(x, y). Our feature
vector consists of (1) n-gram context features, (2)
HMM-like transition features, and (3) linear-chain
features. The n-gram context features relate the
letter evidence that surrounds each letter xi to its
output yi. We include all n-grams that fit within
a context window of size c. The c value is deter-
mined using a development set. The HMM-like
transition features express the cohesion of the out-
put y in the target language. We make a first order
Markov assumption, so that these features are bi-
grams of the form (yi−1, yi). The linear-chain fea-
tures are identical to the context features, except
that yi is replaced with a bi-gram (yi−1, yi).
Algorithm 2 trains a linear model in this fea-
ture space. The procedure makes k passes over
the aligned training data. During each iteration,
the model produces the n most likely output words
ˆYj in the target language for each input word xj
in the source language, based on the current pa-
</bodyText>
<construct confidence="0.420292">
Algorithm 2: Online discriminative training
</construct>
<bodyText confidence="0.675812333333333">
Input: Data f(x1, y1), (x2, y2), ... , (xm, ym)},
number of iterations k, size of n-best list n
Output: Learned weights ψ
</bodyText>
<equation confidence="0.713646666666667">
1 ψ := 0~
2 for k iterations do
3 for j = 1 ... m do
4 kj = fkj1,..., kjn} = arg maxy[ψ · ,D(xj,y)]
5 update ψ according to kj and yj
6 return ψ
</equation>
<bodyText confidence="0.96863452631579">
rameters ψ. The values of k and n are deter-
mined using a development set. The model param-
eters are updated according to the correct output
yj and the predicted n-best outputs ˆYj, to make
the model prefer the correct output over the in-
correct ones. Specifically, the feature weight vec-
tor ψ is updated by using MIRA, the Margin In-
fused Relaxed Algorithm (Crammer and Singer,
2003). MIRA modifies the current weight vector
ψo by finding the smallest changes such that the
new weight vector ψn separates the correct and in-
correct outputs by a margin of at least ℓ(y, ˆy), the
loss for a wrong prediction. We define this loss to
be 0 if yˆ = y; otherwise it is 1 + d, where d is
the Levenshtein distance between y and ˆy. The
update operation is stated as a quadratic program-
ming problem in Equation 3. We utilize a function
from the SVMlight package (Joachims, 1999) to
solve this optimization problem.
</bodyText>
<equation confidence="0.976345666666667">
minψn 11 ψn − ψ.
subject to Vˆy E Y : (3)
ψn &apos; (Φ(x, y) − Φ(x, ˆy)) ? ℓ(y, ˆy)
</equation>
<bodyText confidence="0.99978775">
The arg max operation is performed by an exact
search algorithm based on a phrasal decoder (Zens
and Ney, 2004). This decoder simultaneously
finds the l most likely substrings of letters x that
generate the most probable output y, given the
feature weight vector ψ and the input word xT .
The search algorithm is based on the following dy-
namic programming recurrence:
</bodyText>
<equation confidence="0.9915165">
Q(0, $) = 0
Q(t, p) = max
p′,p, {ψ &apos; φ(xtt′+1,p′,p) + Q(t′,p′)}
t−maxX≤t′&lt;t
Q(T +1,$) = max {ψ &apos; φ($,p′,$) + Q(T,p′)}
p′
</equation>
<bodyText confidence="0.99924425">
To find the n-best predicted outputs, the table
Q records the top n scores for each output sub-
string that has the suffix p substring and is gen-
erated by the input letter substring xt1; here, p′ is
</bodyText>
<page confidence="0.993553">
29
</page>
<bodyText confidence="0.999674833333333">
a sub-output generated during the previous step.
The notation φ(xt′+1, p′, p) is a convenient way
to describe the components of our feature vector
4b(x, y). The n-best predicted outputs Y can be
discovered by backtracking from the end of the ta-
ble, which is denoted by Q(T + 1, �).
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="method">
4 Beyond DIRECTL
</sectionHeader>
<subsectionHeader confidence="0.997788">
4.1 Intermediate phonetic representation
</subsectionHeader>
<bodyText confidence="0.999995962962963">
We experimented with converting the original Chi-
nese characters to Pinyin as an intermediate repre-
sentation. Pinyin is the most commonly known
Romanization system for Standard Mandarin. Its
alphabet contains the same 26 letters as English.
Each Chinese character can be transcribed pho-
netically into Pinyin. Many resources for Pinyin
conversion are available online.1 A small percent-
age of Chinese characters have multiple pronunci-
ations represented by different Pinyin representa-
tions. For those characters (about 30 characters in
the transliteration data), we manually selected the
pronunciations that are normally used for names.
This preprocessing step significantly reduces the
size of target symbols from 370 distinct Chinese
characters to 26 Pinyin symbols which enables our
system to produce better alignments.
In order to verify whether the addition of
language-specific knowledge can improve the
overall accuracy, we also designed intermediate
representations for Russian and Japanese. We
focused on symbols that modify the neighbor-
ing characters without producing phonetic output
themselves: the two yer characters in Russian,
and the long vowel and sokuon signs in Japanese.
Those were combined with the neighboring char-
acters, creating new “super-characters.”
</bodyText>
<subsectionHeader confidence="0.968052">
4.2 Phonetic alignment with ALINE
</subsectionHeader>
<bodyText confidence="0.999857833333333">
ALINE (Kondrak, 2000) is an algorithm that
performs phonetically-informed alignment of two
strings of phonemes. Since our task requires
the alignment of characters representing different
writing scripts, we need to first replace every char-
acter with a phoneme that is the most likely to be
produced by that character.
We applied slightly different methods to the
test languages. In converting the Cyrillic script
into phonemes, we take advantage of the fact
that the Russian orthography is largely phonemic,
which makes it a relatively straightforward task.
</bodyText>
<footnote confidence="0.997785">
1For example, http://www.chinesetopinyin.com/
</footnote>
<bodyText confidence="0.999890285714286">
In Japanese, we replace each Katakana character
with one or two phonemes using standard tran-
scription tables. For the Latin script, we simply
treat every letter as an IPA symbol (International
Phonetic Association, 1999). The IPA contains a
subset of 26 letter symbols that tend to correspond
to the usual phonetic value that the letter repre-
sents in the Latin script. The Chinese characters
are first converted to Pinyin, which is then handled
in the same way as the Latin script.
Similar solutions could be engineered for other
scripts. We observed that the transcriptions do not
need to be very precise in order for ALINE to pro-
duce high quality alignments.
</bodyText>
<subsectionHeader confidence="0.998529">
4.3 System combination
</subsectionHeader>
<bodyText confidence="0.999939153846154">
The combination of predictions produced by sys-
tems based on different principles may lead to im-
proved prediction accuracy. We adopt the follow-
ing combination algorithm. First, we rank the in-
dividual systems according to their top-1 accuracy
on the development set. To obtain the top-1 pre-
diction for each input word, we use simple voting,
with ties broken according to the ranking of the
systems. We generalize this approach to handle n-
best lists by first ordering the candidate translitera-
tions according to the highest rank assigned by any
of the systems, and then similarly breaking ties by
voting and system ranking.
</bodyText>
<sectionHeader confidence="0.997277" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99990985">
In the context of the NEWS 2009 Machine
Transliteration Shared Task (Li et al., 2009), we
tested our system on six data sets: from English to
Chinese (EnCh) (Li et al., 2004), Hindi (EnHi),
Russian (EnRu) (Kumaran and Kellner, 2007),
Japanese Katakana (EnJa), and Korean Hangul
(EnKo); and from Japanese Name to Japanese
Kanji (JnJk)2. We optimized the models’ param-
eters by training on the training portion of the
provided data and measuring performance on the
development portion. For the final testing, we
trained the models on all the available labeled data
(training plus development data). For each data
set, we converted any uppercase letters to lower-
case. Our system outputs the top 10 candidate an-
swers for each input word.
Table 1 reports the performance of our system
on the development and final test sets, measured
in terms of top-1 word accuracy (ACC). For cer-
tain language pairs, we tested variants of the base
</bodyText>
<footnote confidence="0.979762">
2http://www.cjk.org/
</footnote>
<page confidence="0.991633">
30
</page>
<table confidence="0.999597941176471">
Task Model Dev Test
EnCh DIRECTL 72.4 71.7
INT(M2M) 73.9 73.4
INT(ALINE) 73.8 73.2
COMBINED 74.8 74.6
EnHi DIRECTL 41.4 49.8
DIRECTL+MC 42.3 50.9
EnJa DIRECTL 49.9 50.0
INT(M2M)∗ 49.6 49.2
INT(ALINE) 48.3 51.0
COMBINED∗ 50.6 50.5
EnKo DIRECTL 36.7 38.7
EnRu DIRECTL 80.2 61.3
INT(M2M) 80.3 60.8
INT(ALINE) 80.0 60.7
COMBINED∗ 80.3 60.8
JnJk DIRECTL 53.5 56.0
</table>
<tableCaption confidence="0.999523">
Table 1: Top-1 word accuracy on the development
</tableCaption>
<bodyText confidence="0.980831551724138">
and test sets. The asterisk denotes the results ob-
tained after the test reference sets were released.
system described in Section 4. DIRECTL refers
to our language-independent model, which uses
many-to-many alignments. The INT abbreviation
denotes the models operating on the language-
specific intermediate representations described in
Section 4.1. The alignment algorithm (ALINE or
M2M) is given in brackets.
In the EnHi set, many names consisted of mul-
tiple words: we assumed a one-to-one correspon-
dence between consecutive English words and
consecutive Hindi words. In Table 1, the results in
the first row (DIRECTL) were obtained with an au-
tomatic cleanup script that replaced hyphens with
spaces, deleted the remaining punctuation and nu-
merical symbols, and removed 43 transliteration
pairs with a disagreement between the number of
source and target words. The results in the sec-
ond row (DIRECTL+MC) were obtained when the
cases with a disagreement were individually ex-
amined and corrected by a Hindi speaker.
We did not incorporate any external resources
into the models presented in Table 1. In order
to emphasize the performance of our language-
independent approach, we consistently used the
DIRECTL model for generating our “standard”
runs on all six language pairs, regardless of its rel-
ative performance on the development sets.
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999940545454546">
DIRECTL, our language-independent approach to
transliteration achieves excellent results, espe-
cially on the EnCh, EnRu, and EnHi data sets,
which represent a wide range of language pairs
and writing scripts. Both the many-to-many
and phonetic alignment algorithms produce high-
quality alignments. The former can be applied di-
rectly to the training data without the need for an
intermediate representation, while the latter does
not require any training. Surprisingly, incorpo-
ration of language-specific intermediate represen-
tations does not consistently improve the perfor-
mance of our system, which indicates that DI-
RECTL may be able to discover the structures im-
plicit in the training data without additional guid-
ance. The EnHi results suggest that manual clean-
ing of noisy data can yield noticeable gains in ac-
curacy. On the other hand, a simple method of
combining predictions from different systems pro-
duced clear improvement on the EnCh set, but
mixed results on two other sets. More research on
this issue is warranted.
</bodyText>
<sectionHeader confidence="0.999508" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99247325">
This research was supported by the Alberta Inge-
nuity, Informatics Circle of Research Excellence
(iCORE), and Natural Sciences of Engineering
Research Council of Canada (NSERC).
</bodyText>
<sectionHeader confidence="0.999241" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998635575757576">
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal ofMachine Learning Research, 3:951–991.
International Phonetic Association. 1999. Handbook
of the International Phonetic Association. Cam-
bridge University Press.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In Proc. HLT-NAACL, pages 372–379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL, pages 905–913.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in kernel methods:
support vector learning, pages 169–184. MIT Press.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proc. NAACL,
pages 288–295.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. SI-
GIR, pages 721–722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source channel model for machine transliteration. In
Proc. ACL, pages 159–166.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proc. ACL-
IJCNLP Named Entities Workshop.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proc. HLT-NAACL, pages 257–264.
</reference>
<page confidence="0.999912">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805966">
<title confidence="0.999528">a Language-Independent Approach to Transliteration</title>
<author confidence="0.997632">Sittichai Jiampojamarn</author>
<author confidence="0.997632">Aditya Bhargava</author>
<author confidence="0.997632">Qing Dou</author>
<author confidence="0.997632">Kenneth Dwyer</author>
<author confidence="0.997632">Grzegorz</author>
<affiliation confidence="0.997901">Department of Computing University of</affiliation>
<address confidence="0.835091">Edmonton, AB, T6G 2E8,</address>
<abstract confidence="0.9973245">present an online discriminative sequence prediction model that employs a many-to-many alignment between target and source. Our system incorporates input segmentation, target character prediction, and sequence modeling in a unified dynamic programming framework. Experimental results suggest that is able to independently discover many of the language-specific regularities in the training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="6751" citStr="Crammer and Singer, 2003" startWordPosition="1134" endWordPosition="1137"> (x2, y2), ... , (xm, ym)}, number of iterations k, size of n-best list n Output: Learned weights ψ 1 ψ := 0~ 2 for k iterations do 3 for j = 1 ... m do 4 kj = fkj1,..., kjn} = arg maxy[ψ · ,D(xj,y)] 5 update ψ according to kj and yj 6 return ψ rameters ψ. The values of k and n are determined using a development set. The model parameters are updated according to the correct output yj and the predicted n-best outputs ˆYj, to make the model prefer the correct output over the incorrect ones. Specifically, the feature weight vector ψ is updated by using MIRA, the Margin Infused Relaxed Algorithm (Crammer and Singer, 2003). MIRA modifies the current weight vector ψo by finding the smallest changes such that the new weight vector ψn separates the correct and incorrect outputs by a margin of at least ℓ(y, ˆy), the loss for a wrong prediction. We define this loss to be 0 if yˆ = y; otherwise it is 1 + d, where d is the Levenshtein distance between y and ˆy. The update operation is stated as a quadratic programming problem in Equation 3. We utilize a function from the SVMlight package (Joachims, 1999) to solve this optimization problem. minψn 11 ψn − ψ. subject to Vˆy E Y : (3) ψn &apos; (Φ(x, y) − Φ(x, ˆy)) ? ℓ(y, ˆy) </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal ofMachine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<title>Association.</title>
<date>1999</date>
<publisher>Cambridge University Press.</publisher>
<institution>International Phonetic</institution>
<marker>1999</marker>
<rawString>International Phonetic Association. 1999. Handbook of the International Phonetic Association. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
<author>Tarek Sherif</author>
</authors>
<title>Applying many-to-many alignments and Hidden Markov Models to letter-to-phoneme conversion.</title>
<date>2007</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>372--379</pages>
<contexts>
<context position="2707" citStr="Jiampojamarn et al., 2007" startWordPosition="392" endWordPosition="395">ssing, phonetic alignment, and manual data correction. 2 Transliteration alignment In the transliteration task, training data consist of word pairs that map source language words to words in the target language. The matching between character substrings in the source word and target word is not explicitly provided. These hidden relationships are generally known as alignments. In this section, we describe an EM-based many-to-many alignment algorithm employed by DIRECTL. In Section 4, we discuss an alternative phonetic alignment method. We apply an unsupervised many-to-many alignment algorithm (Jiampojamarn et al., 2007) to the transliteration task. The algorithm follows the expectation maximization (EM) paradigm. In the expectation step shown in Algorithm 1, partial counts γ of the possible substring alignments are collected from each word pair (xT, yV ) in the training data; T and V represent the lengths of words x and y, respectively. The forward probability α is estimated by summing the probabilities of all possible sequences of substring pairings from left to right. The FORWARD-M2M procedure is similar to lines 5 through 12 of Algorithm 1, except that it uses Equation 1 on line 8, Equation 2 on line 12, </context>
</contexts>
<marker>Jiampojamarn, Kondrak, Sherif, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applying many-to-many alignments and Hidden Markov Models to letter-to-phoneme conversion. In Proc. HLT-NAACL, pages 372–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Colin Cherry</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Joint processing and discriminative training for letter-to-phoneme conversion.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>905--913</pages>
<contexts>
<context position="4824" citStr="Jiampojamarn et al., 2008" startWordPosition="782" endWordPosition="785"> + = αt 9 if (v &gt; 0 ∧ t &gt; 0) then 10 for i = 1... maxX st t − i &gt; 0 do 11 for j = 1 ... maxY st v − j &gt; 0 do t v αt−i,v−j δ(xt−i+1t,yv−vj+1)βt,v 12γ(xt−i+1,yv−j+1) += αT,V In the maximization step, we normalize the partial counts γ to the alignment probability δ using the conditional probability distribution. The EM steps are repeated until the alignment probability δ converges. Finally, the most likely alignment for each word pair in the training data is computed with the standard Viterbi algorithm. 3 Discriminative training We adapt the online discriminative training framework described in (Jiampojamarn et al., 2008) to the transliteration task. Once the training data has been aligned, we can hypothesize that the ith letter substring xi E x in a source language word is transliterated into the ith substring yi E y in the target language word. Each word pair is represented as a feature vector Φ(x, y). Our feature vector consists of (1) n-gram context features, (2) HMM-like transition features, and (3) linear-chain features. The n-gram context features relate the letter evidence that surrounds each letter xi to its output yi. We include all n-grams that fit within a context window of size c. The c value is d</context>
</contexts>
<marker>Jiampojamarn, Cherry, Kondrak, 2008</marker>
<rawString>Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2008. Joint processing and discriminative training for letter-to-phoneme conversion. In Proc. ACL, pages 905–913.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical. Advances in kernel methods: support vector learning,</title>
<date>1999</date>
<pages>169--184</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7235" citStr="Joachims, 1999" startWordPosition="1227" endWordPosition="1228">Specifically, the feature weight vector ψ is updated by using MIRA, the Margin Infused Relaxed Algorithm (Crammer and Singer, 2003). MIRA modifies the current weight vector ψo by finding the smallest changes such that the new weight vector ψn separates the correct and incorrect outputs by a margin of at least ℓ(y, ˆy), the loss for a wrong prediction. We define this loss to be 0 if yˆ = y; otherwise it is 1 + d, where d is the Levenshtein distance between y and ˆy. The update operation is stated as a quadratic programming problem in Equation 3. We utilize a function from the SVMlight package (Joachims, 1999) to solve this optimization problem. minψn 11 ψn − ψ. subject to Vˆy E Y : (3) ψn &apos; (Φ(x, y) − Φ(x, ˆy)) ? ℓ(y, ˆy) The arg max operation is performed by an exact search algorithm based on a phrasal decoder (Zens and Ney, 2004). This decoder simultaneously finds the l most likely substrings of letters x that generate the most probable output y, given the feature weight vector ψ and the input word xT . The search algorithm is based on the following dynamic programming recurrence: Q(0, $) = 0 Q(t, p) = max p′,p, {ψ &apos; φ(xtt′+1,p′,p) + Q(t′,p′)} t−maxX≤t′&lt;t Q(T +1,$) = max {ψ &apos; φ($,p′,$) + Q(T,p′)</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. Advances in kernel methods: support vector learning, pages 169–184. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>A new algorithm for the alignment of phonetic sequences.</title>
<date>2000</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>288--295</pages>
<contexts>
<context position="9708" citStr="Kondrak, 2000" startWordPosition="1631" endWordPosition="1632"> distinct Chinese characters to 26 Pinyin symbols which enables our system to produce better alignments. In order to verify whether the addition of language-specific knowledge can improve the overall accuracy, we also designed intermediate representations for Russian and Japanese. We focused on symbols that modify the neighboring characters without producing phonetic output themselves: the two yer characters in Russian, and the long vowel and sokuon signs in Japanese. Those were combined with the neighboring characters, creating new “super-characters.” 4.2 Phonetic alignment with ALINE ALINE (Kondrak, 2000) is an algorithm that performs phonetically-informed alignment of two strings of phonemes. Since our task requires the alignment of characters representing different writing scripts, we need to first replace every character with a phoneme that is the most likely to be produced by that character. We applied slightly different methods to the test languages. In converting the Cyrillic script into phonemes, we take advantage of the fact that the Russian orthography is largely phonemic, which makes it a relatively straightforward task. 1For example, http://www.chinesetopinyin.com/ In Japanese, we r</context>
</contexts>
<marker>Kondrak, 2000</marker>
<rawString>Grzegorz Kondrak. 2000. A new algorithm for the alignment of phonetic sequences. In Proc. NAACL, pages 288–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>721--722</pages>
<contexts>
<context position="11841" citStr="Kumaran and Kellner, 2007" startWordPosition="1973" endWordPosition="1976"> on the development set. To obtain the top-1 prediction for each input word, we use simple voting, with ties broken according to the ranking of the systems. We generalize this approach to handle nbest lists by first ordering the candidate transliterations according to the highest rank assigned by any of the systems, and then similarly breaking ties by voting and system ranking. 5 Evaluation In the context of the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), we tested our system on six data sets: from English to Chinese (EnCh) (Li et al., 2004), Hindi (EnHi), Russian (EnRu) (Kumaran and Kellner, 2007), Japanese Katakana (EnJa), and Korean Hangul (EnKo); and from Japanese Name to Japanese Kanji (JnJk)2. We optimized the models’ parameters by training on the training portion of the provided data and measuring performance on the development portion. For the final testing, we trained the models on all the available labeled data (training plus development data). For each data set, we converted any uppercase letters to lowercase. Our system outputs the top 10 candidate answers for each input word. Table 1 reports the performance of our system on the development and final test sets, measured in t</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration. In Proc. SIGIR, pages 721–722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="11783" citStr="Li et al., 2004" startWordPosition="1965" endWordPosition="1968">vidual systems according to their top-1 accuracy on the development set. To obtain the top-1 prediction for each input word, we use simple voting, with ties broken according to the ranking of the systems. We generalize this approach to handle nbest lists by first ordering the candidate transliterations according to the highest rank assigned by any of the systems, and then similarly breaking ties by voting and system ranking. 5 Evaluation In the context of the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), we tested our system on six data sets: from English to Chinese (EnCh) (Li et al., 2004), Hindi (EnHi), Russian (EnRu) (Kumaran and Kellner, 2007), Japanese Katakana (EnJa), and Korean Hangul (EnKo); and from Japanese Name to Japanese Kanji (JnJk)2. We optimized the models’ parameters by training on the training portion of the provided data and measuring performance on the development portion. For the final testing, we trained the models on all the available labeled data (training plus development data). For each data set, we converted any uppercase letters to lowercase. Our system outputs the top 10 candidate answers for each input word. Table 1 reports the performance of our sy</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source channel model for machine transliteration. In Proc. ACL, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<title>machine transliteration shared task.</title>
<date>2009</date>
<journal>Whitepaper of NEWS</journal>
<booktitle>In Proc. ACLIJCNLP Named Entities Workshop.</booktitle>
<contexts>
<context position="11694" citStr="Li et al., 2009" startWordPosition="1948" endWordPosition="1951">rediction accuracy. We adopt the following combination algorithm. First, we rank the individual systems according to their top-1 accuracy on the development set. To obtain the top-1 prediction for each input word, we use simple voting, with ties broken according to the ranking of the systems. We generalize this approach to handle nbest lists by first ordering the candidate transliterations according to the highest rank assigned by any of the systems, and then similarly breaking ties by voting and system ranking. 5 Evaluation In the context of the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), we tested our system on six data sets: from English to Chinese (EnCh) (Li et al., 2004), Hindi (EnHi), Russian (EnRu) (Kumaran and Kellner, 2007), Japanese Katakana (EnJa), and Korean Hangul (EnKo); and from Japanese Name to Japanese Kanji (JnJk)2. We optimized the models’ parameters by training on the training portion of the provided data and measuring performance on the development portion. For the final testing, we trained the models on all the available labeled data (training plus development data). For each data set, we converted any uppercase letters to lowercase. Our system outputs th</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009. Whitepaper of NEWS 2009 machine transliteration shared task. In Proc. ACLIJCNLP Named Entities Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>257--264</pages>
<contexts>
<context position="7462" citStr="Zens and Ney, 2004" startWordPosition="1272" endWordPosition="1275">ew weight vector ψn separates the correct and incorrect outputs by a margin of at least ℓ(y, ˆy), the loss for a wrong prediction. We define this loss to be 0 if yˆ = y; otherwise it is 1 + d, where d is the Levenshtein distance between y and ˆy. The update operation is stated as a quadratic programming problem in Equation 3. We utilize a function from the SVMlight package (Joachims, 1999) to solve this optimization problem. minψn 11 ψn − ψ. subject to Vˆy E Y : (3) ψn &apos; (Φ(x, y) − Φ(x, ˆy)) ? ℓ(y, ˆy) The arg max operation is performed by an exact search algorithm based on a phrasal decoder (Zens and Ney, 2004). This decoder simultaneously finds the l most likely substrings of letters x that generate the most probable output y, given the feature weight vector ψ and the input word xT . The search algorithm is based on the following dynamic programming recurrence: Q(0, $) = 0 Q(t, p) = max p′,p, {ψ &apos; φ(xtt′+1,p′,p) + Q(t′,p′)} t−maxX≤t′&lt;t Q(T +1,$) = max {ψ &apos; φ($,p′,$) + Q(T,p′)} p′ To find the n-best predicted outputs, the table Q records the top n scores for each output substring that has the suffix p substring and is generated by the input letter substring xt1; here, p′ is 29 a sub-output generated</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In Proc. HLT-NAACL, pages 257–264.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>