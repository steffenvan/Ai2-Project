<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.948339666666667">
Relationship between Utterances and “Enthusiasm”
in Non-task-oriented Conversational Dialogue
Ryoko TOKUHISA Ryuta TERASHIMA
</title>
<author confidence="0.948748">
Toyota Central R&amp;D Labs., INC. Toyota Central R&amp;D Labs., INC.
</author>
<affiliation confidence="0.978568">
Nagakute Aichi JAPAN Nagakute Aichi JAPAN
</affiliation>
<email confidence="0.998621">
tokuhisa@mosk.tytlabs.co.jp ryuta@mosk.tytlabs.co.jp
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966538461539">
The goal of this paper is to show how
to accomplish a more enjoyable and en-
thusiastic dialogue through the analysis
of human-to-human conversational dia-
logues. We first created a conversational
dialogue corpus annotated with two types
of tags: one type indicates the particu-
lar aspects of the utterance itself, while
the other indicates the degree of enthusi-
asm. We then investigated the relationship
between these tags. Our results indicate
that affective and cooperative utterances
are significant to enthusiastic dialogue.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999782363636364">
For a non-task-oriented conversational dialogue
system (e.g. home robots), we should strive for
a dialogue strategy that is both enjoyable and
enthusiastic, as well as efficient. Many studies
have been conducted on efficient dialogue strate-
gies (Walker et al., 1998; Litman et al., 2000; Ko-
matani et al., 2002), but it is not clear how to ac-
complish a more ”human-like enthusiasm” for a
conversational dialogue. The goal of this paper is
to show the types of utterances that contribute to
enthusiasm in conversational dialogues.
</bodyText>
<sectionHeader confidence="0.990578" genericHeader="method">
2 Corpus Annotation
</sectionHeader>
<bodyText confidence="0.999954">
We created a conversational corpus annotated with
two types of tags: one type indicates particular
aspects of the utterance itself, while the other in-
dicates the degree of enthusiasm in the dialogue.
This section describes our corpus and tagging
scheme in detail.
</bodyText>
<subsectionHeader confidence="0.975535">
2.1 Corpus Collection
</subsectionHeader>
<bodyText confidence="0.993880428571428">
As a result of previous works, several conversa-
tional dialogue corpora have been collected with
various settings (Graff and Bird, 2000; TSENG,
2001). The largest conversational dialogue cor-
pus is the Switchboard Corpus, which consists of
about 2400 conversational English dialogues be-
tween two unfamiliar speakers over the telephone
on one of 70 topics (e.g. pets, family life, educa-
tion, gun control, etc.).
Our corpus was collected from face-to-face in-
teraction between two unfamiliar speakers. The
reasons were 1) face-to-face interaction increases
the number of enthusiastic utterances, relative to
limited conversational channel interaction such as
over the telephone; 2) the interaction between un-
familiar speakers reduces the enthusiasm resulting
from unobserved reasons during the recording; 3)
the exchange in a twoparty dialogue will be sim-
pler than that of a multiparty dialogue.
We created a corpus containing ten conversa-
tional dialogues that were spoken by an operator
(thirties, female) and one of ten subjects (twenties
to sixties, equal numbers of males and females).
Before beginning the recording session, the sub-
ject chose three cards from fifteen cards on the fol-
lowing topics:
Food, Travel, Sport, Hobbies, Movies, Prizes,
TV Programs, Family, Books, School, Music,
Pets, Shopping, Recent Purchases, Celebrities
Straying from the selected topic was permitted,
because these topic cards were only ever intended
as a prompt to start the dialogue. Thus, we col-
lected ten dialogues, each about 20 minutes long.
For convenience, in this paper, we refer to the op-
erator as speaker1, and the subject as speaker2.
</bodyText>
<page confidence="0.976097">
161
</page>
<note confidence="0.755286">
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 161–167,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.8380185">
2.2 Annotation of DAs and RRs
2.2.1 Definition of tagging scheme
</subsectionHeader>
<bodyText confidence="0.996584617021277">
Dialogue Acts (DAs) and Rhetorical Relations
(RRs) are well-known tagging schemes for anno-
tating an utterance or a sentence. DAs are tags that
pertain to the function of an utterance itself, while
RRs indicate the relationship between sentences or
utterances. We adopted both tags to allow us to an-
alyze the aspects of utterances in various ways, but
adapted them slightly for our particular needs.
The DA annotations were based on SWBD-
DAMSL and MRDA (Jurafsky et al., 1997;
Dhillon et al., 2004). The SWBD-DAMSL is
the DA tagset for labeling a conversational dia-
logue. The Switchboard Corpus mentioned above
was annotated with SWBD-DAMSL. On the other
hand, MRDA is the DA tagset for labeling the
dialogue of a meeting between multiple partici-
pants. Table 1 shows the correspondence between
SWBD-DAMSL/MRDA and our DAs1. We de-
scribe some of the major adaptations below.
The tags pertaining to questions: In SWBD-
DAMSL and MRDA, the tags pertaining to ques-
tions were classified by the type of their form
(e.g. Wh-question). We re-categorized them into
request and confirm in terms of the ”act” for
Japanese.
The tags pertaining to responses: We subdivided
Accept and Reject into objective responses (ac-
cept,denial) and subjective responses (agree, dis-
agree).
The emotional tags: We added tags that indicate
the expression of admiration and interest.
The overlap tags with the RRs definition: We
did not use any tags (e.g. Summary), that over-
lapped the RR definition.
Consequently, we defined 47 DAs for analyzing a
conversational dialogue.
The RR annotations were based on the rhetor-
ical relation defined in Rhetorical Structure The-
ory (RST) (Mann and Thompson, 1988; Stent and
Allen, 2000). Our RR definition was based only
on informational level relation defined in RST be-
cause we annotated the intentional level with DAs.
Table 2 shows the correspondence between the in-
formational relation of RST and our RRs. We de-
scribe some of the major adaptations below.
Subdivide evaluation: The evaluation reflects the
degree of enthusiasm in the dialogue, so we di-
</bodyText>
<footnote confidence="0.895219">
1The tags listed in italics are based on SWBD-DAMSL
while those in boldface are based on MRDA.
</footnote>
<tableCaption confidence="0.983485">
Table 1: Dialogue Act Definition
</tableCaption>
<table confidence="0.99935475">
SWBD- Our DAs Definition
DAMSL/MRDA
Statement non inform objective inform non opin-
opinion fact ion
Statement opin- inform subjective inform opinion
ion element
Wh-Question request objective request non opin-
Yes-No- fact ion
question
Open-
Question
Or-Question
request agreement request agreement
opinion
confirm objective confirm non opin-
fact ion
confirm agreement confirm agreement
opinion
Accept accept accept non opinion
agree accept opinion
Reject denial denial non opinion
disagree denial opinion
not marked express admiration inform admiration
Summary DEL. (mark as RR)
</table>
<tableCaption confidence="0.718814">
Table 2: Rhetorical Relation Definition
</tableCaption>
<table confidence="0.9943195">
Mann’s RST Our RRs definition
Evaluation evaluation U2 is a positive evaluation
(positive) about U1
evaluation U2 is a negative evalua-
(negative) tion about U1
evaluation U2 is neutral evaluation
(neutral) about U1
Volitional volitional U2 is a volitional action,
cause cause-effect and U1 cause U2
Volitional re-
sult
No Definition addition U2 consists of a part of U1
</table>
<bodyText confidence="0.996221611111111">
vided the Evaluation into three types of evaluation
(positive/negative/neutral).
Integrate the causal relations: We use a di-
rected graph representation for RR annotations, so
that we integrate Non-volitional cause and Non-
volitional result into non-volitional cause-effect,
and Volitional cause and Volitional result into vo-
litional cause-effect.
Add addition relation: The RRs initially repre-
sent the structure of the written text, segmented
into clause-like units. Therefore, they do not cover
those cases in which one clause is uttered by one
speaker, but communicatively completed by an-
other. So, we added an addition to our RRs. The
following is an example of addition.
speaker A: the lunch in our company cafeteria
speaker B: is good value for money
We defined 16 RRs as a result of these adaptations.
</bodyText>
<page confidence="0.975136">
162
</page>
<figure confidence="0.994639857142857">
Context: The father of speaker2 likes watching movies, and so established a home theater system in their living room.
2:speaker1 [apposition]
[elaboration]
3:speaker2
4:speaker1
5:speaker2 [elaboration]
6:speaker1
1:speaker2 [addition]
[volitional
cause-effect]
elaboration]
[evaluation
(neutral)]
elaboration]
that s why my family really loves movies these days
we sometimes watch many more &lt;accept&gt;&lt;inform objective fact&gt;
I suppose you &lt;signal understanding&gt;
I suppose it s nice to watch them in your home
without interruptions, right?
you watch them one after another, don t you?
about 2 or 3 movies per week
so many? &lt;signal understanding&gt;&lt;exclamation&gt;&lt;confirm objective fact&gt;
&lt;accept&gt;&lt;inform objective fact&gt;
&lt;signal understanding&gt;
&lt;confirm objective fact&gt;
&lt;signal understanding&gt;&lt;confirm
agreement&gt;&lt;confirm objective fact&gt;
&lt;inform objective fact&gt;
</figure>
<figureCaption confidence="0.99999">
Figure 1: Example of Dialogue annotated with DAs and RRs (Originally in Japanese)
</figureCaption>
<subsectionHeader confidence="0.963829">
2.2.2 Annotation of DAs and RRs
</subsectionHeader>
<bodyText confidence="0.985589178571428">
DAs and RRs are annotated using the MMAX2
Annotation Tool 2 (Muller and Strube, 2003). Fig-
ure 1 shows an example of our corpus annotated
with DAs and RRs. The O symbol in Figure 1
indicates a DA, while the [ ] symbol indicates an
RR. Below, we describe our annotation process for
DAs and RRs.
Step 1. Utterance Segmentation: All the utter-
ances in the dialogue are segmented into DA seg-
ments, each of which we define as an utterance.
In Figure 1, the utterance is surrounded with a
square. In this step, we also eliminated backchan-
nels from the exchange.
Step 2. Annotation of DAs: DAs are annotated
to all utterances. In those cases in which one DA
alone cannot represent an utterance, two or more
DAs are used (see Figure 1 line 2).
Step 3. Annotation of Adjacency Pairs: Adja-
cency pairs (APs) are labeled. An AP consists of
two utterances where each part is produced by a
different speaker. In Figure 1, the solid and dotted
lines correspond to links between the APs.
Step 4. Annotation of RRs: RRs on APs are la-
beled. A solid line indicates an AP that is labeled
with RRs, while a dotted line indicates an AP that
is not labeled with RRs. If a single RR cannot
represent the type of the relationship, two or more
RRs are used.
</bodyText>
<subsectionHeader confidence="0.999849">
2.3 Annotation of Enthusiasm
</subsectionHeader>
<bodyText confidence="0.907156">
2.3.1 Related Work on Annotating the degree
of enthusiasm
Wrede et al. annotated Involvement to the ICSI
Meeting Recorder Corpus (Wrede and Shriberg,
</bodyText>
<footnote confidence="0.948763333333333">
2This supports multilevel annotation and the creation
of a relationship between utterances. http://www.eml-
research.de/english/research/nlp/down-load/mmax.php
</footnote>
<figureCaption confidence="0.998825">
Figure 2: Rating the score of the enthusiasm
</figureCaption>
<bodyText confidence="0.999799176470588">
2003b; Wrede and Shriberg, 2003a). In their
method, a rater judges involvement (agreement,
disagreement, other) or Not especially involved or
Don’t Know, by listening to each utterance with-
out the context of the dialogue. In the exper-
iment, nine raters provided ratings on 45 utter-
ances. Inter-rater agreement between Involved and
Not especially involved yielded a Kappa of κ=.59
(p&lt;.01), but 13 of the 45 utterances (28.9%) were
rated as Don’t Know by at least one of the raters.
For automatic detection, it is certainly effective to
rate Involvement without context. However, the re-
sults indicate that it is quite difficult to recognize
Involvement from a single utterance. Moreover,
the fluctuation of Involvement can not be recog-
nized by this method because Involvement is cate-
gorized into five categories only.
</bodyText>
<subsectionHeader confidence="0.62244">
2.3.2 Our Method of Annotating Enthusiasm
</subsectionHeader>
<bodyText confidence="0.990451555555556">
In this section, we propose a method for eval-
uating the degree of enthusiasm. We describe the
process for evaluating the degree of enthusiasm.
Step 1. Rating the score of enthusiasm for POD
A rater estimates a score of the enthusiasm
corresponding to the part of dialogue (POD),
which is a series of five utterances. As men-
tioned above, the backchannels are not re-
garded as utterances. In Figure 2, S� denotes
</bodyText>
<figure confidence="0.997817043478261">
utterance
...backchannel
...
speaker1
speaker2
POD
S
. . .
...
Score of enthusiasm
Part Of Dialogue
Ui-4 Ui-3 Ui-2 Ui-1 Ui Ui+1 Ui+2 Ui+3 Ui+4
Dialogue
PODi+2
PODi+1
PODi
PODi-1
PODi-2
Si+2
Si+1
Si
Si-1
Si-2
</figure>
<page confidence="0.941768">
163
</page>
<bodyText confidence="0.7787315">
the score for the enthusiasm of PODi. The
value of the score can be from 10 to 90.
</bodyText>
<note confidence="0.25797725">
90 ... Extreme
70 ... Moderate
50 ... Neutral
30 ... Low
</note>
<sectionHeader confidence="0.409368" genericHeader="method">
10 ... No
</sectionHeader>
<bodyText confidence="0.8823005">
When rating the score, a rater must obey the
following four rules.
</bodyText>
<listItem confidence="0.998704875">
1. Listen to each POD more than three
times.
2. Perform estimation based on the entire
POD and not just part of the POD.
3. Be sure that own ratings represented a
consistent continuum.
4. Estimate as participants, not as side-
participants.
</listItem>
<bodyText confidence="0.999806333333334">
We did not give any definitions or examples
to rate the enthusiasm, a rater estimated a
score based on their subjective determination.
Step 2. Calculate the score of enthusiasm for an
utterance
The score of enthusiasm for an utterance Ui
is given by the average of the scores of the
PODs that contain utterance Ui.
Step 3. Calculate the degree of enthusiasm for an
utterance and an adjacency pair
In this paper, we deal with all the degrees of
enthusiasm as a normalized score, which we
call Enthusiasm, because different raters may
have different absolute levels of enthusiasm.
Then, Enthusiasm for Ui is given as follows:
</bodyText>
<equation confidence="0.996813666666667">
V (Ui) − V (U)
E(Ui) =
Q
</equation>
<bodyText confidence="0.999485666666667">
where
n denotes the number of utterances in the di-
alogue.
In addition, Enthusiasm for APi is given by
the average of Enthusiasms of the utterances
where are APi.
</bodyText>
<equation confidence="0.995259">
1
E(APi) = �{E(Uj) + E(Uk)} (3)
</equation>
<bodyText confidence="0.760524">
Uj and Uk denote the utterances in APi.
</bodyText>
<sectionHeader confidence="0.928935" genericHeader="method">
3 Estimation of Annotated Corpus
</sectionHeader>
<subsectionHeader confidence="0.999533">
3.1 Reliability of DAs and RRs
</subsectionHeader>
<bodyText confidence="0.999094272727273">
We examined the inter-annotator reliability for
two annotators3 for DAs, RRs and APs, using four
dialogues mentioned above. Before the start of the
investigation, one annotator segmented a dialogue
into utterances. The number of segmented utter-
ances was 697. The annotaters annotated them as
described in steps 2 to 4 of Section 2.2.2.
DAs annotation: We can not apply the Kappa
statistics since it cannot be applied to multiple tag
annotations. We then apply formula 4 to examine
the reliability.
</bodyText>
<figure confidence="0.429062">
a (Agreed DAs) x 2 x 100 4
g Total of DAs annotated by A1 and A2 ( )
</figure>
<bodyText confidence="0.984204">
The result of agreement was 1542 DAs (65.5%)
from a total of 2355 DAs. The major reasons for
the disagreement were as follows.
</bodyText>
<listItem confidence="0.999888666666667">
• Disagreement of subjective/objective ... 124(15.3%)
• Disagreement of request/confirm ... 112(13.8%)
• Disagreement of partial/whole ... 72(8.9%)
</listItem>
<bodyText confidence="0.997082125">
Building APs: We examined the agreement of
building APs between utterances. The result of
agreement was 536 APs (85.2%) from the total of
the 629 APs that were built by the annotators. This
result shows that the building of APs is reliable.
RRs annotation: We also examined the agree-
ment of RRs annotation. We applied formula 5
to this examination.
</bodyText>
<equation confidence="0.990158692307692">
i+2
1 �
V (Ui) =
j=i−2
5
Sj (1)
(2)
1 n V (Ui) (Agreed RRs) x 2
V (U) = n i=1 ag. _ Total of RRs annotated by A1andA2 x100 (5)
{V (Ui) − V (U)}2
n
i=1
Q =
</equation>
<bodyText confidence="0.7381135">
d 1
n
As a result, we found agreement for 576 RRs
(59.6%) out of a total of 967 RRs.
</bodyText>
<footnote confidence="0.708157">
3We refer to these annotators as A1 and A2. A1 is one of
the authors of this paper.
</footnote>
<page confidence="0.998871">
164
</page>
<tableCaption confidence="0.9669585">
Table 3: Correlation between random rating and
sequential rating
</tableCaption>
<table confidence="0.94399">
correlation coefficient
speaker1 speaker2
twenties,female 0.833 0.881
twenties,male 0.971 0.950
sixties,female 0.972 0.973
sixties,male 0.971 0.958
</table>
<figure confidence="0.9537735">
3
2
1
0
-1
-2
-3
time
</figure>
<figureCaption confidence="0.946477">
Figure 3: Enthusiasm of dialogue of speaker1 and
speaker2(thirties,female)
</figureCaption>
<subsectionHeader confidence="0.7858995">
3.2 Estimation Context Influence on the
rating of Enthusiasm
</subsectionHeader>
<bodyText confidence="0.999968866666667">
In order to examine the influence of the context on
the rating of Enthusiasm, one rater noted Enthusi-
asm under two conditions: 1) Listening to PODs
randomly, and 2) Listening to PODs sequentially
as dialogue. Table 3 shows the correlation be-
tween the random rating and the sequential rating.
The correlation coefficient was calculated for the
Enthusiasm of each of the two participants. The
”speaker1” shows the correlation of the Enthusi-
asm rated as speaker1, and ”speaker2” shows the
correlation of the Enthusiasm rated as speaker2.
This was found to be approximately 0.9 in both
cases. These results show that Enthusiasm can be
estimated stably and that the context has little in-
fluence.
</bodyText>
<sectionHeader confidence="0.8073545" genericHeader="method">
4 Relationship between DAs/RRs and
Enthusiasm
</sectionHeader>
<bodyText confidence="0.998674689655173">
We investigated the relationship between
DAs/RRs and Enthusiasm, using four dia-
logues. The DAs/RRs corpus annotated by A1
was used in this analysis because A1 is one
of the authors of this paper and has a better
knowledge of the DAs and RRs tagging scheme
than A2. The Enthusiasm corpus annotated by
R3 was used because we found that R4 rated
Enthusiasm based on non-subjective reasons:
after the examination of the rating, R4 said that
speaker1 spoke enthusiastically but that it seemed
unnatural because speaker1 had to manage the
recording of the dialogue, which appears in the
results as speaker1’s Enthusiasm as annotated by
R4 as a notable difference (see Figure 3).
Figure 4 and 5 show the ratio of the frequency
of DAs and RRs in each of the levels of Enthu-
siasm over a range of 0.5. If DAs and RRs were
evenly annotated for any level of Enthusiasm, the
graph will be completely even. However, the
graph shows the right side as being higher if the
DAs and RRs increase as Enthusiasm increases.
Conversely, the graph shows the left side as being
higher if the DAs and RRs fall as Enthusiasm in-
creases. The number in Figure 4 and 5 indicates
the average Enthusiasm for each DA and RR. If
the average is positive, it means that the frequency
of the DAs and RRs is high in that part in which
Enthusiasm is positive. In contrast, if the average
is negative, it means that the frequency of the DAs
and RRs is high in that part in which Enthusiasm
is negative.
We determined the following two points about
the tendency of the DAs frequency.
Tendency of subjective and objective DAs: The
ratio of the frequency of those DAs related to sub-
jective elements tends to increase as Enthusiasm
increases (see *1 in Figure 4). In contrast, the ra-
tio of the frequency of those DAs pertaining to ob-
jective matters tends to decrease (see *2 in Figure
4) or equilibrate as Enthusiasm increases (see *3
in Figure 4) . We can thus conclude that those ex-
changes related to subjective elements increases in
the enthusiastic dialogue, but those related to ob-
jective elements decrease or equilibrate.
Tendency of affective DAs: The ratio of the fre-
quency of those DAs related to the affective con-
tents tends to increase as Enthusiasm increases
(see *4 in Figure 4). However, express admiration,
which is also related to affective contents, tends to
decrease (see *5 in Figure 4). We then analyzed
several instances of admiration. As a result, we
found that the prosodic characteristic of admira-
tion utterance will cause this tendency.
Furthermore, we noted the following two points
about the tendency of the RRs frequency.
Tendency of additional utterances: The ratio of
the frequency of addition, which completes the
</bodyText>
<equation confidence="0.861126">
R3(speaker1) R3(speaker2)
R4(speaker1) R4(speaker2)
Enthusiasm
</equation>
<page confidence="0.895266">
165
</page>
<figure confidence="0.999783115942029">
Frequency(ratio)
-2.5~-2.0 -2.0~-1.5 -1.5~-1.0 -1.0~-0.5 -0.5~0.0 0.0~0.5 0.5~1.0 1.0~1.5 1.5~2.0
0.3
0.03 -0.12
-0.03
0.25
-0.11
0.30 0.12
0.2
-0.07 0.04 0.29 0.11 -0.62 -0.06 -0.04 0.03 0.27 0.70 0.14
1.07
0.15
0.1
0.05
0
signal
understanding
N accept
agree
neutral
*2
request
repetition
confirm
other&apos;s
sbjective
&apos; denial
*
inform
objective fact
confirm
objective fact
inform
subjective
* element
5* 1*
show humor
*3
show interest
exclamation
request fact
*1
*1
*1
*4
confirm
agreement
express
admiration
signal partial
no
understanding
sympathy
*5
*3
*2
*2
*1
*4 *4
0.7
Frequency(ratio)
0.6
0.5
0.4
0.3
0.2
0.1
0
Dialogue Act
</figure>
<figureCaption confidence="0.998166">
Figure 4: Frequency of DAs per Enthusiasm
</figureCaption>
<figure confidence="0.996729111111111">
-0.04
-0.27
-0A7 0.05
0.05 -0.02 -0.09
0.23 -0.01
0.08 0.20 -0.18
*6
*7
Rhetorical Relation
</figure>
<figureCaption confidence="0.998616">
Figure 5: Frequency of RRs per Enthusiasm
</figureCaption>
<figure confidence="0.978486181818182">
-2.5~-2.0 -2.0~-1.5 -1.5~-1.0 -1.0~-0.5 -0.5~0 0~0.5 0.5~1.0 1.0~1.5 1.5~2.0
elaboration
apposition
non volitional
cause-effect
interpretation
volitional cause-
effect
instance
antithesis
evaluation
(positive)
summary
evaluation
(neutral)
circumstance
addition
Context:Mother of speaker2 does not cook dinner when the
father is out.
1 speaker1: but if he s there then she
2 speaker2: cooks a really delicious dinner
3 speaker1: wow
</figure>
<figureCaption confidence="0.999919">
Figure 6: Example of addition
</figureCaption>
<bodyText confidence="0.999680666666667">
other participant’s utterance, tends to increase as
Enthusiasm increases (see *6 in Figure 5). Figure
6 shows a dialogue example. There are addition
relations between lines 1 and 2. This shows that
the participant makes an utterance cooperatively
by completing the other’s utterances in enthusias-
tic dialogues. Such cooperative utterance is a sig-
nificant component of enthusiastic dialogues.
Tendency of positive evaluation: The ratio of the
frequency of positive evaluation tends to increase
at lower Enthusiasm and higher Enthusiasm (see
*7 in Figure 5). We analyzed some instances of
</bodyText>
<subsectionHeader confidence="0.618343">
Context:About a hamster and its exercise instrument.
</subsectionHeader>
<construct confidence="0.274324181818182">
1 speaker2: two hamsters run together in their exercise wheel
2 speaker2: they run up and down and side by side
3 speaker1: but surely they can t they run together if they aren t
getting along very well?
4 speaker2: exactly
5 speaker2: one gets carried along if it stops when the other
continues to run
6 speaker1: is it? does it lean forward?
7 speaker2: yes
8 speaker2: sometimes it falls out
9 speaker1: that s so cute
</construct>
<figureCaption confidence="0.999782">
Figure 7: Example of positive evaluation
</figureCaption>
<bodyText confidence="0.999882285714286">
positive evaluation, we then found that the speaker
tries to arouse the dialogue by an utterance of
positive evaluation at lower Enthusiasm, and the
speaker summarizes the previous discourse with a
positive evaluation at higher Enthusiasm. Figure
7 shows an example of positive evaluation in the
enthusiastic dialogue. In this case, speaker1 ex-
</bodyText>
<page confidence="0.995441">
166
</page>
<bodyText confidence="0.99940975">
presses positive evaluation on line 9 about the el-
ement on line 8. The utterance on line 9 also has
the function of expressing an overall positive eval-
uation of the previous discourse.
</bodyText>
<sectionHeader confidence="0.995768" genericHeader="conclusions">
5 Conclusion and Future Research
</sectionHeader>
<bodyText confidence="0.999998583333333">
We analyzed the relationship between utterances
and the degree of enthusiasm in human-to-human
conversational dialogue. We first created a conver-
sational dialogue corpus annotated with two types
of tags: DAs/RRs and Enthusiasm. The DA and
RR tagging scheme was adapted from the defini-
tion given in a previous work, and an Enthusiasm
tagging scheme is proposed. Our method of rating
Enthusiasm enables the observation of the fluctu-
ation of Enthusiasm, which enables the detailed
analysis of the relationship between utterances and
Enthusiasm. The result of the analysis shows the
frequency of objective and subjective utterances
related to the level of Enthusiasm. We also found
that affective and cooperative utterances are sig-
nificant in an enthusiastic dialogue.
In this paper, we only analyzed the relationship
between DAs/RRs and Enthusiasm, but we expect
the non-linguistic-feature related with Enthusiasm
so that we would analyze the relationship in future
research. And, we try to achieve more reliable an-
notation by reviewing our tagging scheme. Fur-
thermore, we would apply the results of the analy-
sis to our conversational dialogue system.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999835674418605">
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and
Elizabeth Shriberg. 2004. Meeting Recorder
Project: Dialog Act Labeling Guide. ICSI Techni-
cal Report, (TR-04-002).
David Graff and Steven Bird. 2000. Many Uses, Many
Annotations for Large Speech Corpora: Switch-
board and TDT as Case Studies. LREC2000.
Dan Jurafsky, Liz Shriberg, and Debra Biasca.
1997. Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual.
www.dcs.shef.ac.uk/nlp/amities/files/bib/ics-tr-97-
02.pdf.
Kazunori Komatani, Tatsuya Kawahara, Ryosuke Ito,
and Hiroshi Okuno. 2002. Efficient Dialogue Strat-
egy to Find Users’ Intended Items from Information
Query Results. In Proceedings of the COLING.
Diane Litman, Satinder Singh, Michael Kearns, and
Marilyn Walker. 2000. NJFun: A Reinforcement
Learning Spoken Dialogue System. In Proceedings
of the ANLP/NAACL.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243–281.
Christoph Muller and Michael Strube. 2003. Multi-
Level Annotation in MMAX. In Proceedings of the
4th SIGdial Workshop on Discourse and Dialogue.
Amanda Stent and James Allen. 2000. Annotating Ar-
gumentation Acts in Spoken Dialog. Technical Re-
port 740.
Shu-Chuan TSENG. 2001. Toward a Large Sponta-
neous Mandarin Dialogue Corpus. In Proceedings
of the 2nd SIGdial Workshop on Discourse and Dia-
logue.
Marilyn A. Walker, Jeanne C. Fromer, and Shrikanth
Narayanan. 1998. Learning Optimal Dialogue
Strategies: A Case Study of a Spoken Dialogue
Agent for Email. In Proceedings of COLING/ACL.
Britta Wrede and Elizabeth Shriberg. 2003a. Spotting
”Hot Spots” in Meetings: Human Judgements and
Prosodic Cues. Eurospeech-03, pages 2805–2808.
Britta Wrede and Elizabeth Shriberg. 2003b. The Re-
lationship between Dialogue Acts and Hot Spots in
Meetings. IEEE ASRU Workshop.
</reference>
<page confidence="0.997757">
167
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.343494">
<title confidence="0.9983175">Relationship between Utterances and “Enthusiasm” in Non-task-oriented Conversational Dialogue</title>
<author confidence="0.864053">Ryoko TOKUHISA Ryuta TERASHIMA</author>
<affiliation confidence="0.888896">Toyota Central R&amp;D Labs., INC. Toyota Central R&amp;D Labs., INC.</affiliation>
<address confidence="0.505601">Nagakute Aichi JAPAN Nagakute Aichi</address>
<email confidence="0.798468">tokuhisa@mosk.tytlabs.co.jpryuta@mosk.tytlabs.co.jp</email>
<abstract confidence="0.999616642857143">The goal of this paper is to show how to accomplish a more enjoyable and enthusiastic dialogue through the analysis of human-to-human conversational dialogues. We first created a conversational dialogue corpus annotated with two types of tags: one type indicates the particular aspects of the utterance itself, while the other indicates the degree of enthusiasm. We then investigated the relationship between these tags. Our results indicate that affective and cooperative utterances are significant to enthusiastic dialogue.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rajdip Dhillon</author>
<author>Sonali Bhagat</author>
<author>Hannah Carvey</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Meeting Recorder Project: Dialog Act Labeling Guide.</title>
<date>2004</date>
<tech>ICSI Technical Report, (TR-04-002).</tech>
<contexts>
<context position="4010" citStr="Dhillon et al., 2004" startWordPosition="608" endWordPosition="611">July 2006. c�2006 Association for Computational Linguistics 2.2 Annotation of DAs and RRs 2.2.1 Definition of tagging scheme Dialogue Acts (DAs) and Rhetorical Relations (RRs) are well-known tagging schemes for annotating an utterance or a sentence. DAs are tags that pertain to the function of an utterance itself, while RRs indicate the relationship between sentences or utterances. We adopted both tags to allow us to analyze the aspects of utterances in various ways, but adapted them slightly for our particular needs. The DA annotations were based on SWBDDAMSL and MRDA (Jurafsky et al., 1997; Dhillon et al., 2004). The SWBD-DAMSL is the DA tagset for labeling a conversational dialogue. The Switchboard Corpus mentioned above was annotated with SWBD-DAMSL. On the other hand, MRDA is the DA tagset for labeling the dialogue of a meeting between multiple participants. Table 1 shows the correspondence between SWBD-DAMSL/MRDA and our DAs1. We describe some of the major adaptations below. The tags pertaining to questions: In SWBDDAMSL and MRDA, the tags pertaining to questions were classified by the type of their form (e.g. Wh-question). We re-categorized them into request and confirm in terms of the ”act” for</context>
</contexts>
<marker>Dhillon, Bhagat, Carvey, Shriberg, 2004</marker>
<rawString>Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and Elizabeth Shriberg. 2004. Meeting Recorder Project: Dialog Act Labeling Guide. ICSI Technical Report, (TR-04-002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Steven Bird</author>
</authors>
<title>Many Uses, Many Annotations for Large Speech Corpora: Switchboard and TDT as Case Studies.</title>
<date>2000</date>
<contexts>
<context position="1801" citStr="Graff and Bird, 2000" startWordPosition="264" endWordPosition="267"> a more ”human-like enthusiasm” for a conversational dialogue. The goal of this paper is to show the types of utterances that contribute to enthusiasm in conversational dialogues. 2 Corpus Annotation We created a conversational corpus annotated with two types of tags: one type indicates particular aspects of the utterance itself, while the other indicates the degree of enthusiasm in the dialogue. This section describes our corpus and tagging scheme in detail. 2.1 Corpus Collection As a result of previous works, several conversational dialogue corpora have been collected with various settings (Graff and Bird, 2000; TSENG, 2001). The largest conversational dialogue corpus is the Switchboard Corpus, which consists of about 2400 conversational English dialogues between two unfamiliar speakers over the telephone on one of 70 topics (e.g. pets, family life, education, gun control, etc.). Our corpus was collected from face-to-face interaction between two unfamiliar speakers. The reasons were 1) face-to-face interaction increases the number of enthusiastic utterances, relative to limited conversational channel interaction such as over the telephone; 2) the interaction between unfamiliar speakers reduces the e</context>
</contexts>
<marker>Graff, Bird, 2000</marker>
<rawString>David Graff and Steven Bird. 2000. Many Uses, Many Annotations for Large Speech Corpora: Switchboard and TDT as Case Studies. LREC2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>Liz Shriberg</author>
<author>Debra Biasca</author>
</authors>
<date>1997</date>
<journal>Switchboard SWBD-DAMSL ShallowDiscourse-Function Annotation Coders Manual.</journal>
<pages>97--02</pages>
<contexts>
<context position="3987" citStr="Jurafsky et al., 1997" startWordPosition="604" endWordPosition="607">pages 161–167, Sydney, July 2006. c�2006 Association for Computational Linguistics 2.2 Annotation of DAs and RRs 2.2.1 Definition of tagging scheme Dialogue Acts (DAs) and Rhetorical Relations (RRs) are well-known tagging schemes for annotating an utterance or a sentence. DAs are tags that pertain to the function of an utterance itself, while RRs indicate the relationship between sentences or utterances. We adopted both tags to allow us to analyze the aspects of utterances in various ways, but adapted them slightly for our particular needs. The DA annotations were based on SWBDDAMSL and MRDA (Jurafsky et al., 1997; Dhillon et al., 2004). The SWBD-DAMSL is the DA tagset for labeling a conversational dialogue. The Switchboard Corpus mentioned above was annotated with SWBD-DAMSL. On the other hand, MRDA is the DA tagset for labeling the dialogue of a meeting between multiple participants. Table 1 shows the correspondence between SWBD-DAMSL/MRDA and our DAs1. We describe some of the major adaptations below. The tags pertaining to questions: In SWBDDAMSL and MRDA, the tags pertaining to questions were classified by the type of their form (e.g. Wh-question). We re-categorized them into request and confirm in</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997. Switchboard SWBD-DAMSL ShallowDiscourse-Function Annotation Coders Manual. www.dcs.shef.ac.uk/nlp/amities/files/bib/ics-tr-97-02.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Tatsuya Kawahara</author>
<author>Ryosuke Ito</author>
<author>Hiroshi Okuno</author>
</authors>
<title>Efficient Dialogue Strategy to Find Users’ Intended Items from Information Query Results.</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<contexts>
<context position="1142" citStr="Komatani et al., 2002" startWordPosition="158" endWordPosition="162">types of tags: one type indicates the particular aspects of the utterance itself, while the other indicates the degree of enthusiasm. We then investigated the relationship between these tags. Our results indicate that affective and cooperative utterances are significant to enthusiastic dialogue. 1 Introduction For a non-task-oriented conversational dialogue system (e.g. home robots), we should strive for a dialogue strategy that is both enjoyable and enthusiastic, as well as efficient. Many studies have been conducted on efficient dialogue strategies (Walker et al., 1998; Litman et al., 2000; Komatani et al., 2002), but it is not clear how to accomplish a more ”human-like enthusiasm” for a conversational dialogue. The goal of this paper is to show the types of utterances that contribute to enthusiasm in conversational dialogues. 2 Corpus Annotation We created a conversational corpus annotated with two types of tags: one type indicates particular aspects of the utterance itself, while the other indicates the degree of enthusiasm in the dialogue. This section describes our corpus and tagging scheme in detail. 2.1 Corpus Collection As a result of previous works, several conversational dialogue corpora have</context>
</contexts>
<marker>Komatani, Kawahara, Ito, Okuno, 2002</marker>
<rawString>Kazunori Komatani, Tatsuya Kawahara, Ryosuke Ito, and Hiroshi Okuno. 2002. Efficient Dialogue Strategy to Find Users’ Intended Items from Information Query Results. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Satinder Singh</author>
<author>Michael Kearns</author>
<author>Marilyn Walker</author>
</authors>
<title>NJFun: A Reinforcement Learning Spoken Dialogue System.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL.</booktitle>
<contexts>
<context position="1118" citStr="Litman et al., 2000" startWordPosition="154" endWordPosition="157">s annotated with two types of tags: one type indicates the particular aspects of the utterance itself, while the other indicates the degree of enthusiasm. We then investigated the relationship between these tags. Our results indicate that affective and cooperative utterances are significant to enthusiastic dialogue. 1 Introduction For a non-task-oriented conversational dialogue system (e.g. home robots), we should strive for a dialogue strategy that is both enjoyable and enthusiastic, as well as efficient. Many studies have been conducted on efficient dialogue strategies (Walker et al., 1998; Litman et al., 2000; Komatani et al., 2002), but it is not clear how to accomplish a more ”human-like enthusiasm” for a conversational dialogue. The goal of this paper is to show the types of utterances that contribute to enthusiasm in conversational dialogues. 2 Corpus Annotation We created a conversational corpus annotated with two types of tags: one type indicates particular aspects of the utterance itself, while the other indicates the degree of enthusiasm in the dialogue. This section describes our corpus and tagging scheme in detail. 2.1 Corpus Collection As a result of previous works, several conversation</context>
</contexts>
<marker>Litman, Singh, Kearns, Walker, 2000</marker>
<rawString>Diane Litman, Satinder Singh, Michael Kearns, and Marilyn Walker. 2000. NJFun: A Reinforcement Learning Spoken Dialogue System. In Proceedings of the ANLP/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a Functional Theory of Text Organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="5179" citStr="Mann and Thompson, 1988" startWordPosition="795" endWordPosition="798">hem into request and confirm in terms of the ”act” for Japanese. The tags pertaining to responses: We subdivided Accept and Reject into objective responses (accept,denial) and subjective responses (agree, disagree). The emotional tags: We added tags that indicate the expression of admiration and interest. The overlap tags with the RRs definition: We did not use any tags (e.g. Summary), that overlapped the RR definition. Consequently, we defined 47 DAs for analyzing a conversational dialogue. The RR annotations were based on the rhetorical relation defined in Rhetorical Structure Theory (RST) (Mann and Thompson, 1988; Stent and Allen, 2000). Our RR definition was based only on informational level relation defined in RST because we annotated the intentional level with DAs. Table 2 shows the correspondence between the informational relation of RST and our RRs. We describe some of the major adaptations below. Subdivide evaluation: The evaluation reflects the degree of enthusiasm in the dialogue, so we di1The tags listed in italics are based on SWBD-DAMSL while those in boldface are based on MRDA. Table 1: Dialogue Act Definition SWBD- Our DAs Definition DAMSL/MRDA Statement non inform objective inform non op</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Muller</author>
<author>Michael Strube</author>
</authors>
<title>MultiLevel Annotation in MMAX.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="8540" citStr="Muller and Strube, 2003" startWordPosition="1297" endWordPosition="1300">suppose you &lt;signal understanding&gt; I suppose it s nice to watch them in your home without interruptions, right? you watch them one after another, don t you? about 2 or 3 movies per week so many? &lt;signal understanding&gt;&lt;exclamation&gt;&lt;confirm objective fact&gt; &lt;accept&gt;&lt;inform objective fact&gt; &lt;signal understanding&gt; &lt;confirm objective fact&gt; &lt;signal understanding&gt;&lt;confirm agreement&gt;&lt;confirm objective fact&gt; &lt;inform objective fact&gt; Figure 1: Example of Dialogue annotated with DAs and RRs (Originally in Japanese) 2.2.2 Annotation of DAs and RRs DAs and RRs are annotated using the MMAX2 Annotation Tool 2 (Muller and Strube, 2003). Figure 1 shows an example of our corpus annotated with DAs and RRs. The O symbol in Figure 1 indicates a DA, while the [ ] symbol indicates an RR. Below, we describe our annotation process for DAs and RRs. Step 1. Utterance Segmentation: All the utterances in the dialogue are segmented into DA segments, each of which we define as an utterance. In Figure 1, the utterance is surrounded with a square. In this step, we also eliminated backchannels from the exchange. Step 2. Annotation of DAs: DAs are annotated to all utterances. In those cases in which one DA alone cannot represent an utterance,</context>
</contexts>
<marker>Muller, Strube, 2003</marker>
<rawString>Christoph Muller and Michael Strube. 2003. MultiLevel Annotation in MMAX. In Proceedings of the 4th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amanda Stent</author>
<author>James Allen</author>
</authors>
<title>Annotating Argumentation Acts in Spoken Dialog.</title>
<date>2000</date>
<tech>Technical Report 740.</tech>
<contexts>
<context position="5203" citStr="Stent and Allen, 2000" startWordPosition="799" endWordPosition="802">irm in terms of the ”act” for Japanese. The tags pertaining to responses: We subdivided Accept and Reject into objective responses (accept,denial) and subjective responses (agree, disagree). The emotional tags: We added tags that indicate the expression of admiration and interest. The overlap tags with the RRs definition: We did not use any tags (e.g. Summary), that overlapped the RR definition. Consequently, we defined 47 DAs for analyzing a conversational dialogue. The RR annotations were based on the rhetorical relation defined in Rhetorical Structure Theory (RST) (Mann and Thompson, 1988; Stent and Allen, 2000). Our RR definition was based only on informational level relation defined in RST because we annotated the intentional level with DAs. Table 2 shows the correspondence between the informational relation of RST and our RRs. We describe some of the major adaptations below. Subdivide evaluation: The evaluation reflects the degree of enthusiasm in the dialogue, so we di1The tags listed in italics are based on SWBD-DAMSL while those in boldface are based on MRDA. Table 1: Dialogue Act Definition SWBD- Our DAs Definition DAMSL/MRDA Statement non inform objective inform non opinopinion fact ion State</context>
</contexts>
<marker>Stent, Allen, 2000</marker>
<rawString>Amanda Stent and James Allen. 2000. Annotating Argumentation Acts in Spoken Dialog. Technical Report 740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu-Chuan TSENG</author>
</authors>
<title>Toward a Large Spontaneous Mandarin Dialogue Corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="1815" citStr="TSENG, 2001" startWordPosition="268" endWordPosition="269">thusiasm” for a conversational dialogue. The goal of this paper is to show the types of utterances that contribute to enthusiasm in conversational dialogues. 2 Corpus Annotation We created a conversational corpus annotated with two types of tags: one type indicates particular aspects of the utterance itself, while the other indicates the degree of enthusiasm in the dialogue. This section describes our corpus and tagging scheme in detail. 2.1 Corpus Collection As a result of previous works, several conversational dialogue corpora have been collected with various settings (Graff and Bird, 2000; TSENG, 2001). The largest conversational dialogue corpus is the Switchboard Corpus, which consists of about 2400 conversational English dialogues between two unfamiliar speakers over the telephone on one of 70 topics (e.g. pets, family life, education, gun control, etc.). Our corpus was collected from face-to-face interaction between two unfamiliar speakers. The reasons were 1) face-to-face interaction increases the number of enthusiastic utterances, relative to limited conversational channel interaction such as over the telephone; 2) the interaction between unfamiliar speakers reduces the enthusiasm resu</context>
</contexts>
<marker>TSENG, 2001</marker>
<rawString>Shu-Chuan TSENG. 2001. Toward a Large Spontaneous Mandarin Dialogue Corpus. In Proceedings of the 2nd SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Jeanne C Fromer</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="1097" citStr="Walker et al., 1998" startWordPosition="150" endWordPosition="153">tional dialogue corpus annotated with two types of tags: one type indicates the particular aspects of the utterance itself, while the other indicates the degree of enthusiasm. We then investigated the relationship between these tags. Our results indicate that affective and cooperative utterances are significant to enthusiastic dialogue. 1 Introduction For a non-task-oriented conversational dialogue system (e.g. home robots), we should strive for a dialogue strategy that is both enjoyable and enthusiastic, as well as efficient. Many studies have been conducted on efficient dialogue strategies (Walker et al., 1998; Litman et al., 2000; Komatani et al., 2002), but it is not clear how to accomplish a more ”human-like enthusiasm” for a conversational dialogue. The goal of this paper is to show the types of utterances that contribute to enthusiasm in conversational dialogues. 2 Corpus Annotation We created a conversational corpus annotated with two types of tags: one type indicates particular aspects of the utterance itself, while the other indicates the degree of enthusiasm in the dialogue. This section describes our corpus and tagging scheme in detail. 2.1 Corpus Collection As a result of previous works,</context>
</contexts>
<marker>Walker, Fromer, Narayanan, 1998</marker>
<rawString>Marilyn A. Walker, Jeanne C. Fromer, and Shrikanth Narayanan. 1998. Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Britta Wrede</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Spotting ”Hot Spots” in Meetings: Human Judgements and Prosodic Cues.</title>
<date>2003</date>
<booktitle>Eurospeech-03,</booktitle>
<pages>2805--2808</pages>
<contexts>
<context position="10098" citStr="Wrede and Shriberg, 2003" startWordPosition="1565" endWordPosition="1568">. A solid line indicates an AP that is labeled with RRs, while a dotted line indicates an AP that is not labeled with RRs. If a single RR cannot represent the type of the relationship, two or more RRs are used. 2.3 Annotation of Enthusiasm 2.3.1 Related Work on Annotating the degree of enthusiasm Wrede et al. annotated Involvement to the ICSI Meeting Recorder Corpus (Wrede and Shriberg, 2This supports multilevel annotation and the creation of a relationship between utterances. http://www.emlresearch.de/english/research/nlp/down-load/mmax.php Figure 2: Rating the score of the enthusiasm 2003b; Wrede and Shriberg, 2003a). In their method, a rater judges involvement (agreement, disagreement, other) or Not especially involved or Don’t Know, by listening to each utterance without the context of the dialogue. In the experiment, nine raters provided ratings on 45 utterances. Inter-rater agreement between Involved and Not especially involved yielded a Kappa of κ=.59 (p&lt;.01), but 13 of the 45 utterances (28.9%) were rated as Don’t Know by at least one of the raters. For automatic detection, it is certainly effective to rate Involvement without context. However, the results indicate that it is quite difficult to re</context>
</contexts>
<marker>Wrede, Shriberg, 2003</marker>
<rawString>Britta Wrede and Elizabeth Shriberg. 2003a. Spotting ”Hot Spots” in Meetings: Human Judgements and Prosodic Cues. Eurospeech-03, pages 2805–2808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Britta Wrede</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>The Relationship between Dialogue Acts and Hot Spots in Meetings.</title>
<date>2003</date>
<journal>IEEE ASRU Workshop.</journal>
<contexts>
<context position="10098" citStr="Wrede and Shriberg, 2003" startWordPosition="1565" endWordPosition="1568">. A solid line indicates an AP that is labeled with RRs, while a dotted line indicates an AP that is not labeled with RRs. If a single RR cannot represent the type of the relationship, two or more RRs are used. 2.3 Annotation of Enthusiasm 2.3.1 Related Work on Annotating the degree of enthusiasm Wrede et al. annotated Involvement to the ICSI Meeting Recorder Corpus (Wrede and Shriberg, 2This supports multilevel annotation and the creation of a relationship between utterances. http://www.emlresearch.de/english/research/nlp/down-load/mmax.php Figure 2: Rating the score of the enthusiasm 2003b; Wrede and Shriberg, 2003a). In their method, a rater judges involvement (agreement, disagreement, other) or Not especially involved or Don’t Know, by listening to each utterance without the context of the dialogue. In the experiment, nine raters provided ratings on 45 utterances. Inter-rater agreement between Involved and Not especially involved yielded a Kappa of κ=.59 (p&lt;.01), but 13 of the 45 utterances (28.9%) were rated as Don’t Know by at least one of the raters. For automatic detection, it is certainly effective to rate Involvement without context. However, the results indicate that it is quite difficult to re</context>
</contexts>
<marker>Wrede, Shriberg, 2003</marker>
<rawString>Britta Wrede and Elizabeth Shriberg. 2003b. The Relationship between Dialogue Acts and Hot Spots in Meetings. IEEE ASRU Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>