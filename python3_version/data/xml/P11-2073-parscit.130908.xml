<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012149">
<title confidence="0.996107">
Improving Decoding Generalization for Tree-to-String Translation
</title>
<author confidence="0.992749">
Jingbo Zhu Tong Xiao
</author>
<affiliation confidence="0.8615465">
Natural Language Processing Laboratory Natural Language Processing Laboratory
Northeastern University, Shenyang, China Northeastern University, Shenyang, China
</affiliation>
<email confidence="0.997681">
zhujingbo@mail.neu.edu.cn xiaotong@mail.neu.edu.cn
</email>
<sectionHeader confidence="0.995613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999575">
To address the parse error issue for tree-to-
string translation, this paper proposes a
similarity-based decoding generation (SDG)
solution by reconstructing similar source
parse trees for decoding at the decoding
time instead of taking multiple source parse
trees as input for decoding. Experiments on
Chinese-English translation demonstrated
that our approach can achieve a significant
improvement over the standard method,
and has little impact on decoding speed in
practice. Our approach is very easy to im-
plement, and can be applied to other para-
digms such as tree-to-tree models.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999644888888889">
Among linguistically syntax-based statistical ma-
chine translation (SMT) approaches, the tree-to-
string model (Huang et al. 2006; Liu et al. 2006) is
the simplest and fastest, in which parse trees on
source side are used for grammar extraction and
decoding. Formally, given a source (e.g., Chinese)
string c and its auto-parsed tree T1-best, the goal of
typical tree-to-string SMT is to find a target (e.g.,
English) string e* by the following equation as
</bodyText>
<equation confidence="0.843209">
= arg max Pr(  |, 1
e c T
e
</equation>
<bodyText confidence="0.999597266666667">
where Pr(e|c,T1-best) is the probability that e is the
translation of the given source string c and its T1-best.
A typical tree-to-string decoder aims to search for
the best derivation among all consistent derivations
that convert source tree into a target-language
string. We call this set of consistent derivations the
tree-to-string search space. Each derivation in the
search space respects the source parse tree.
Parsing errors on source parse trees would cause
negative effects on tree-to-string translation due to
decoding on incorrect source parse trees. To ad-
dress the parse error issue in tree-to-string transla-
tion, a natural solution is to use n-best parse trees
instead of 1-best parse tree as input for decoding,
which can be expressed by
</bodyText>
<equation confidence="0.9904975">
e = argmaxPr(  |,
e c T − ) (2)
n best
e
</equation>
<bodyText confidence="0.99983465">
where &lt;Tn-best&gt; denotes a set of n-best parse trees
of c produced by a state-of-the-art syntactic parser.
A simple alternative (Xiao et al. 2010) to generate
&lt;Tn-best&gt; is to utilize multiple parsers, which can
improve the diversity among source parse trees in
&lt;Tn-best&gt;. In this solution, the most representative
work is the forest-based translation method (Mi et
al. 2008; Mi and Huang 2008; Zhang et al. 2009)
in which a packed forest (forest for short) structure
is used to effectively represent &lt;Tn-best&gt; for decod-
ing. Forest-based approaches can increase the tree-
to-string search space for decoding, but face a non-
trivial problem of high decoding time complexity
in practice.
In this paper, we propose a new solution by re-
constructing new similar source parse trees for de-
coding, referred to as similarity-based decoding
generation (SDG), which is expressed as
where &lt;Tsim&gt; denotes a set of similar parse trees of
T1-best that are dynamically reconstructed at the de-
</bodyText>
<figure confidence="0.981787842105263">
e
max Pr(  |, {
e c T 1
e
arg
(3)
,
−
})
best
Tsim
e = arg max Pr(e
c, T )
1 −best
*
*
e
best ) (1)
*
</figure>
<page confidence="0.958281">
418
</page>
<note confidence="0.592864">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 418–423,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999903555555556">
coding time. Roughly speaking, &lt;Tn-best&gt; is a sub-
set of {T1-best, &lt;Tsim&gt;}. Along this line of thinking,
Equation (2) can be considered as a special case of
Equation (3).
In our SDG solution, given a source parse tree
T1-best, the key is how to generate its &lt;Tsim&gt; at the
decoding time. In practice, it is almost intractable
to directly reconstructing &lt;Tsim&gt; in advance as in-
put for decoding due to too high computation com-
plexity. To address this crucial challenge, this
paper presents a simple and effective technique
based on similarity-based matching constraints to
construct new similar source parse trees for decod-
ing at the decoding time. Our SDG approach can
explicitly increase the tree-to-string search space
for decoding without changing any grammar ex-
traction and pruning settings, and has little impact
on decoding speed in practice.
</bodyText>
<sectionHeader confidence="0.991233" genericHeader="method">
2 Tree-to-String Derivation
</sectionHeader>
<bodyText confidence="0.99820378125">
We choose the tree-to-string paradigm in our study
because this is the simplest and fastest among syn-
tax-based models, and has been shown to be one of
the state-of-the-art syntax-based models. Typically,
by using the GHKM algorithm (Galley et al. 2004),
translation rules are learned from word-aligned
bilingual texts whose source side has been parsed
by using a syntactic parser. Each rule consists of a
syntax tree in the source language having some
words (terminals) or variables (nonterminals) at
leaves, and sequence words or variables in the tar-
get language. With the help of these learned trans-
lation rules, the goal of tree-to-string decoding is to
search for the best derivation that converts the
source tree into a target-language string. A deriva-
tion is a sequence of translation steps (i.e., the use
of translation rules).
Figure 1 shows an example derivation d that
performs translation over a Chinese source parse
tree, and how this process works. In the first step,
we can apply rule r1 at the root node that matches a
subtree {IP[1] (NP[2] VP[3])}. The corresponding
target side {x1 x2} means to preserve the top-level
word-order in the translation, and results in two
unfinished subtrees with root labels NP[2] and VP[3],
respectively. The rule r2 finishes the translation on
the subtree of NP[2], in which the Chinese word
“��” is translated into an English string “the
Chinese side”. The rule r3 is applied to perform
translation on the subtree of VP[3], and results in an
An example tree-to-string derivation d consisting of five
translation rules is given as follows:
</bodyText>
<equation confidence="0.990193333333333">
r1: IP[1] (x1:NP[2] x2:VP[3]) → x1 x2
r2: NP[2] (NN (��)) → the Chinese side
r3: VP[3] (ADVP(AD(j%A)) VP(VV(W*) AS(f)
x1:NP[4])) → highly appreciated x1
r4: NP[4] (DP(DT(A) CLP(M(&amp;))) x1:NP[5]) → this x1
r5: NP[5] (NN(-&apos;__6iX)) → talk
</equation>
<figureCaption confidence="0.89665775">
Translation results: The Chinese side highly appreciated
this talk.
Figure 1. An example derivation performs translation
over the Chinese parse tree T.
</figureCaption>
<bodyText confidence="0.9904886">
unfinished subtree of NP[4]. Similarly, rules r4 and
r5 are sequentially used to finish the translation on
the remaining. This process is a depth-first search
over the whole source tree, and visits every node
only once.
</bodyText>
<sectionHeader confidence="0.994784" genericHeader="method">
3 Decoding Generalization
</sectionHeader>
<subsectionHeader confidence="0.99991">
3.1 Similarity-based Matching Constraints
</subsectionHeader>
<bodyText confidence="0.9997299">
In typical tree-to-string decoding, an ordered se-
quence of rules can be reassembled to form a deri-
vation d whose source side matches the given
source parse tree T. The source side of each rule in
d should match one of subtrees of T, referred to as
matching constraint. Before discussing how to ap-
ply our similarity-based matching constraints to
reconstruct new similar source parse trees for de-
coding at the decoding time, we first define the
similarity between two tree-to-string rules.
</bodyText>
<construct confidence="0.962281">
Definition 1 Given two tree-to-string rules t and u,
we say that t and u are similar such that their
source sides ts and us have the same root label and
frontier nodes, written as t ≅ u, otherwise not.
</construct>
<page confidence="0.998873">
419
</page>
<figureCaption confidence="0.999643333333333">
Figure 2: Two similar tree-to-string rules. (a) rule r3
used by the example derivation d in Figure 1, and (b) a
similar rule z3 of r3.
</figureCaption>
<bodyText confidence="0.999340333333333">
Here we use an example figure to explain our
similarity-based matching constraint scheme (simi-
larity-based scheme for short).
</bodyText>
<figureCaption confidence="0.98954625">
Figure 3: (a) a typical tree-to-string derivation d using
rule t, and (b) a new derivation d* is generated by the
similarity-based matching constraint scheme by using
rule t* instead of rule t, where t* = t.
</figureCaption>
<bodyText confidence="0.991099411764706">
Given a source-language parse tree T, in typical
tree-to-string matching constraint scheme shown in
Figure 3(a), rule t used by the derivation d should
match a substree ABC of T. In our similarity-based
scheme, the similar rule t* ( = t) is used to form a
new derivation d* that performs translation over
the same source sentence {w1 ... wj. In such a case,
this new derivation d* can yield a new similar
parse tree T* of T.
Since an incorrect source parse tree might filter
out good derivations during tree-to-string decoding,
our similarity-based scheme is much more likely to
recover the correct tree for decoding at the decod-
ing time, and does not rule out good (potentially
correct) translation choices. In our method, many
new source-language trees T* that are similar to but
different from the original source tree T can be re-
constructed at the decoding time. In theory our
similarity-based scheme can increase the search
space of the tree-to-string decoder, but we did not
change any rule extraction and pruning settings.
In practice, our similarity-based scheme can ef-
fectively keep the advantage of fast decoding for
tree-to-string translation because its implementa-
tion is very simple. Let’s revisit the example deri-
vation d in Figure 1, i.e., d=r1®r2®r3®r4®r51. In
such a case, the decoder can effectively produce a
new derivation d* by simply replacing rule r3 with
its similar rule z3 ( = r3 ) shown in Figure 2, that is,
d*=r1®r2®z3®r4®r5.
With beam search, typical tree-to-string decod-
ing with an integrated language model can run in
time2 O(ncb2) in practice (Huang 2007). For our
decoding time complexity computation, only the
parameter c value can be affected by our similar-
ity-based scheme. In other words, our similarity-
based scheme would result in a larger c value at
decoding time as many similar translation rules
might be matched at each node. In practice, there
are two feasible optimization techniques to allevi-
ate this problem. The first technique is to limit the
maximum number of similar translation rules
matched at each node. The second one is to prede-
fine a similarity threshold to filter out less similar
translation rules in advance.
In the implementation, we add a new feature
into the model: similarity-based matching counting
feature. This feature counts the number of similar
rules used to form the derivation. The weight Asim
of this feature is tuned via minimal error rate train-
ing (MERT) (Och 2003) with other feature weights.
</bodyText>
<subsectionHeader confidence="0.994969">
3.2 Pseudo-rule Generation
</subsectionHeader>
<bodyText confidence="0.999709583333333">
In the implementation of tree-to-string decoding,
the first step is to load all translation rules matched
at each node of the source tree T. It is possible that
some nonterminal nodes do not have any matched
rules when decoding some new sentences. If the
root node of the source tree has no any matched
rules, it would cause decoding failure. To tackle
this problem, motivated by “glue” rules (Chiang
2005), for some node S without any matched rules,
we introduce a special pseudo-rule which reassem-
bles all child nodes with local reordering to form
new translation rules for S to complete decoding.
</bodyText>
<footnote confidence="0.98243425">
1 The symbol®denotes the composition (leftmost substitution)
operation of two tree-to-string rules.
2 Where n is the number of words, b is the size of the beam,
and c is the number of translation rules matched at each node.
</footnote>
<page confidence="0.992858">
420
</page>
<figure confidence="0.961606">
S S(x1:A x2:B x3:C x4:D)→x1 x2 x3 x4
S(x1:A x2:B x3:C x4:D)→x2 x1 x3 x4
S(x1:A x2:B x3:C x4:D)→x1 x3 x2 x4
A B C D S(x1:A x2:B x3:C x4:D)→x1 x2 x4 x3
(a) (b)
</figure>
<figureCaption confidence="0.9978705">
Figure 4: (a) An example unseen substree, and (b) its
four pseudo-rules.
</figureCaption>
<bodyText confidence="0.967264857142857">
Figure 4 (a) depicts an example unseen substree
where no any rules is matched at its root node S.
Its simplest pseudo-rule is to simply combine a
sequence of S’s child nodes. To give the model
more options to build partial translations, we util-
ize a local reordering technique in which any two
adjacent frontier (child) nodes are reordered during
decoding. Figure 4(b) shows four pseudo-rules in
total generated from this example unseen substree.
In the implementation, we add a new feature to
the model: pseudo-rule counting feature. This fea-
ture counts the number of pseudo-rules used to
form the derivation. The weight λpseudo of this fea-
ture is tuned via MERT with other feature weights.
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.96927">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.99998512">
Our bilingual training data consists of 140K Chi-
nese-English sentence pairs in the FBIS data set.
For rule extraction, the minimal GHKM rules (Gal-
ley et al. 2004) were extracted from the bitext, and
the composed rules were generated by combining
two or three minimal GHKM rules. A 5-gram lan-
guage model was trained on the target-side of the
bilingual data and the Xinhua portion of English
Gigaword corpus. The beam size for beam search
was set to 20. The base feature set used for all sys-
tems is similar to that used in (Marcu et al. 2006),
including 14 base features in total such as 5-gram
language model, bidirectional lexical and phrase-
based translation probabilities. All features were
linearly combined and their weights are optimized
by using MERT. The development data set used
for weight training in our approaches comes from
NIST MT03 evaluation set. To speed up MERT,
sentences with more than 20 words were removed
from the development set (Dev set). The test sets
are the NIST MT04 and MT05 evaluation sets. The
translation quality was evaluated in terms of case-
insensitive NIST version BLEU metric. Statistical
significance test was conducted by using the boot-
strap re-sampling method (Koehn 2004).
</bodyText>
<sectionHeader confidence="0.841712" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<table confidence="0.999713">
DEV MT04 MT05
MT03 &lt;=20 ALL &lt;=20 ALL
Baseline 32.99 36.54 32.70 34.61 30.60
This 34.67* 36.99+ 35.03* 35.16+ 33.12*
work (+1.68) (+0.45) (+2.33) (+0.55) (+2.52)
</table>
<tableCaption confidence="0.9908715">
Table 1. BLEU4 (%) scores of various methods on Dev
set (MT03) and two test sets (MT04 and MT05). Each
small test set (&lt;=20) was built by removing the sen-
tences with more than 20 words from the full set (ALL).
+ and * indicate significantly better on performance
comparison at p &lt; .05 and p &lt; .01, respectively.
</tableCaption>
<bodyText confidence="0.99431448">
Table 1 depicts the BLEU scores of various meth-
ods on the Dev set and four test sets. Compared to
typical tree-to-string decoding (the baseline), our
method can achieve significant improvements on
all datasets. It is noteworthy that the improvement
achieved by our approach on full test sets is bigger
than that on small test sets. For example, our
method results in an improvement of 2.52 BLEU
points over the baseline on the MT05 full test set,
but only 0.55 points on the MT05 small test set. As
mentioned before, tree-to-string approaches are
more vulnerable to parsing errors. In practice, the
Berkeley parser (Petrov et al. 2006) we used yields
unsatisfactory parsing performance on some long
sentences in the full test sets. In such a case, it
would result in negative effects on the performance
of the baseline method on the full test sets. Ex-
perimental results show that our SDG approach
can effectively alleviate this problem, and signifi-
cantly improve tree-to-string translation.
Another issue we are interested in is the decod-
ing speed of our method in practice. To investigate
this issue, we evaluate the average decoding speed
of our SDG method and the baseline on the Dev set
and all test sets.
</bodyText>
<table confidence="0.9971644">
Decoding Time
(seconds per sentence)
&lt;=20 ALL
Baseline 0.43s 1.1s
This work 0.50s 1.3s
</table>
<tableCaption confidence="0.997323333333333">
Table 2. Average decoding speed of various methods on
small (&lt;=20) and full (ALL) datasets in terms of sec-
onds per sentence. The parsing time of each sentence is
not included. The decoders were implemented in C++
codes on an X86-based PC with two processors of
2.4GHZ and 4GB physical memory.
</tableCaption>
<page confidence="0.998504">
421
</page>
<bodyText confidence="0.999085151515152">
Table 2 shows that our approach only has little
impact on decoding speed in practice, compared to
the typical tree-to-string decoding (baseline). No-
tice that in these comparisons our method did not
adopt any optimization techniques mentioned in
Section 3.1, e.g., to limit the maximum number of
similar rules matched at each node. It is obviously
that the use of such an optimization technique can
effectively increase the decoding speed of our
method, but might hurt the performance in practice.
Besides, to speed up decoding long sentences, it
seems a feasible solution to first divide a long sen-
tence into multiple short sub-sentences for decod-
ing, e.g., based on comma. In other words, we can
segment a complex source-language parse tree into
multiple smaller subtrees for decoding, and com-
bine the translations of these small subtrees to form
the final translation. This practical solution can
speed up the decoding on long sentences in real-
world MT applications, but might hurt the transla-
tion performance.
For convenience, here we call the rule τ3 in Fig-
ure 2(b) similar-rules. It is worth investigating how
many similar-rules and pseudo-rules are used to
form the best derivations in our similarity-based
scheme. To do it, we count the number of similar-
rules and pseudo-rules used to form the best deri-
vations when decoding on the MT05 full set. Ex-
perimental results show that on average 13.97% of
rules used to form the best derivations are similar-
rules, and one pseudo-rule per sentence is used.
Roughly speaking, average five similar-rules per
sentence are utilized for decoding generalization.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999976029411764">
String-to-tree SMT approaches also utilize the
similarity-based matching constraint on target side
to generate target translation. This paper applies it
on source side to reconstruct new similar source
parse trees for decoding at the decoding time,
which aims to increase the tree-to-string search
space for decoding, and improve decoding gener-
alization for tree-to-string translation.
The most related work is the forest-based trans-
lation method (Mi et al. 2008; Mi and Huang 2008;
Zhang et al. 2009) in which rule extraction and
decoding are implemented over k-best parse trees
(e.g., in the form of packed forest) instead of one
best tree as translation input. Liu and Liu (2010)
proposed a joint parsing and translation model by
casting tree-based translation as parsing (Eisner
2003), in which the decoder does not respect the
source tree. These methods can increase the tree-
to-string search space. However, the decoding time
complexity of their methods is high, i.e., more than
ten or several dozen times slower than typical tree-
to-string decoding (Liu and Liu 2010).
Some previous efforts utilized the techniques of
soft syntactic constraints to increase the search
space in hierarchical phrase-based models (Marton
and Resnik 2008; Chiang et al. 2009; Huang et al.
2010), string-to-tree models (Venugopal et al.
2009) or tree-to-tree (Chiang 2010) systems. These
methods focus on softening matching constraints
on the root label of each rule regardless of its in-
ternal tree structure, and often generate many new
syntactic categories3. It makes them more difficult
to satisfy syntactic constraints for the tree-to-string
decoding.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.998929533333333">
This paper addresses the parse error issue for tree-
to-string translation, and proposes a similarity-
based decoding generation solution by reconstruct-
ing new similar source parse trees for decoding at
the decoding time. It is noteworthy that our SDG
approach is very easy to implement. In principle,
forest-based and tree sequence-based approaches
improve rule coverage by changing the rule extrac-
tion settings, and use exact tree-to-string matching
constraints for decoding. Since our SDG approach
is independent of any rule extraction and pruning
techniques, it is also applicable to forest-based ap-
proaches or other tree-based translation models,
e.g., in the case of casting tree-to-tree translation as
tree parsing (Eisner 2003).
</bodyText>
<sectionHeader confidence="0.997452" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998810444444445">
We would like to thank Feiliang Ren, Muhua Zhu
and Hao Zhang for discussions and the anonymous
reviewers for comments. This research was sup-
ported in part by the National Science Foundation
of China (60873091; 61073140), the Specialized
Research Fund for the Doctoral Program of Higher
Education (20100042110031) and the Fundamental
Research Funds for the Central Universities in
China.
</bodyText>
<footnote confidence="0.450609">
3 Latent syntactic categories were introduced in the method of
Huang et al. (2010).
</footnote>
<page confidence="0.995099">
422
</page>
<sectionHeader confidence="0.990038" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999304898550725">
Chiang David. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of
ACL2005, pp263-270
Chiang David. 2010. Learning to translate with source
and target syntax. In Proc. of ACL2010, pp1443-
1452
Chiang David, Kevin Knight and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proc. of NAACL2009, pp218-226
Eisner Jason. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. of ACL 2003,
pp205-208.
Galley Michel, Mark Hopkins, Kevin Knight and Daniel
Marcu. 2004. What&apos;s in a translation rule? In Proc. of
HLT-NAACL 2004, pp273-280.
Huang Liang. 2007. Binarization, synchronous binariza-
tion and target-side binarization. In Proc. of NAACL
Workshop on Syntax and Structure in Statistical
Translation.
Huang Liang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL 2007, pp144-151.
Huang Liang, Kevin Knight and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006, pp66-73.
Huang Zhongqiang, Martin Cmejrek and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
bution. In Proc. of EMNLP2010, pp138-147
Koehn Philipp. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pp388-395.
Liu Yang and Qun Liu. 2010. Joint parsing and transla-
tion. In Proc. of Coling2010, pp707-715
Liu Yang, Qun Liu and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine
translation. In Proc. of COLING/ACL 2006, pp609-
616.
Marcu Daniel, Wei Wang, Abdessamad Echihabi and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. of EMNLP 2006, pp44-52.
Marton Yuval and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrase-based translation.
In Proc. of ACL08, pp1003-1011
Mi Haitao and Liang Huang. 2008. Forest-based Trans-
lation Rule Extraction. In Proc. of EMNLP 2008,
pp206-214.
Mi Haitao, Liang Huang and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL2008.
Och Franz Josef. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL2003.
Petrov Slav, Leon Barrett, Roman Thibaux and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of ACL2006,
pp433-440.
Xiao Tong, Jingbo Zhu, Hao Zhang and Muhua Zhu.
2010. An empirical study of translation rule extrac-
tion with multiple parsers. In Proc. of Coling2010,
pp1345-1353
Venugopal Ashish, Andreas Zollmann, Noah A. Smith
and Stephan Vogel. 2009. Preference grammars: sof-
tening syntactic constraints to improve statistical ma-
chine translation. In Proc. of NAACL2009, pp236-
244
Zhang Hui, Min Zhang, Haizhou Li, Aiti Aw and Chew
Lim Tan. 2009. Forest-based tree sequence to string
translation model. In Proc. of ACL-IJCNLP2009,
pp172-180
</reference>
<page confidence="0.999419">
423
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.545711">
<title confidence="0.999892">Improving Decoding Generalization for Tree-to-String Translation</title>
<author confidence="0.99629">Jingbo Zhu Tong Xiao</author>
<affiliation confidence="0.959456">Natural Language Processing Laboratory Natural Language Processing Laboratory Northeastern University, Shenyang, China Northeastern University, Shenyang, China</affiliation>
<email confidence="0.626262">zhujingbo@mail.neu.edu.cnxiaotong@mail.neu.edu.cn</email>
<abstract confidence="0.998090466666667">To address the parse error issue for tree-tostring translation, this paper proposes a similarity-based decoding generation (SDG) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding. Experiments on Chinese-English translation demonstrated that our approach can achieve a significant improvement over the standard method, and has little impact on decoding speed in practice. Our approach is very easy to implement, and can be applied to other paradigms such as tree-to-tree models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chiang David</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL2005,</booktitle>
<pages>263--270</pages>
<marker>David, 2005</marker>
<rawString>Chiang David. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL2005, pp263-270</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiang David</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proc. of ACL2010,</booktitle>
<pages>1443--1452</pages>
<marker>David, 2010</marker>
<rawString>Chiang David. 2010. Learning to translate with source and target syntax. In Proc. of ACL2010, pp1443-1452</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiang David</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL2009,</booktitle>
<pages>218--226</pages>
<marker>David, Knight, Wang, 2009</marker>
<rawString>Chiang David, Kevin Knight and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proc. of NAACL2009, pp218-226</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eisner Jason</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>205--208</pages>
<marker>Jason, 2003</marker>
<rawString>Eisner Jason. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. of ACL 2003, pp205-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galley Michel</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What&apos;s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<pages>273--280</pages>
<marker>Michel, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Galley Michel, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What&apos;s in a translation rule? In Proc. of HLT-NAACL 2004, pp273-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huang Liang</author>
</authors>
<title>Binarization, synchronous binarization and target-side binarization.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<marker>Liang, 2007</marker>
<rawString>Huang Liang. 2007. Binarization, synchronous binarization and target-side binarization. In Proc. of NAACL Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huang Liang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>144--151</pages>
<marker>Liang, Chiang, 2007</marker>
<rawString>Huang Liang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. of ACL 2007, pp144-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huang Liang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA</booktitle>
<pages>66--73</pages>
<marker>Liang, Knight, Joshi, 2006</marker>
<rawString>Huang Liang, Kevin Knight and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. of AMTA 2006, pp66-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huang Zhongqiang</author>
<author>Martin Cmejrek</author>
<author>Bowen Zhou</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distribution.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP2010,</booktitle>
<pages>138--147</pages>
<marker>Zhongqiang, Cmejrek, Zhou, 2010</marker>
<rawString>Huang Zhongqiang, Martin Cmejrek and Bowen Zhou. 2010. Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distribution. In Proc. of EMNLP2010, pp138-147</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn Philipp</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>388--395</pages>
<marker>Philipp, 2004</marker>
<rawString>Koehn Philipp. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proc. of EMNLP 2004, pp388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu Yang</author>
<author>Qun Liu</author>
</authors>
<title>Joint parsing and translation.</title>
<date>2010</date>
<booktitle>In Proc. of Coling2010,</booktitle>
<pages>707--715</pages>
<marker>Yang, Liu, 2010</marker>
<rawString>Liu Yang and Qun Liu. 2010. Joint parsing and translation. In Proc. of Coling2010, pp707-715</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu Yang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<pages>609--616</pages>
<marker>Yang, Liu, Lin, 2006</marker>
<rawString>Liu Yang, Qun Liu and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proc. of COLING/ACL 2006, pp609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcu Daniel</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>44--52</pages>
<marker>Daniel, Wang, Echihabi, Knight, 2006</marker>
<rawString>Marcu Daniel, Wei Wang, Abdessamad Echihabi and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. of EMNLP 2006, pp44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marton Yuval</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrase-based translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL08,</booktitle>
<pages>1003--1011</pages>
<marker>Yuval, Resnik, 2008</marker>
<rawString>Marton Yuval and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrase-based translation. In Proc. of ACL08, pp1003-1011</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mi Haitao</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based Translation Rule Extraction.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>206--214</pages>
<marker>Haitao, Huang, 2008</marker>
<rawString>Mi Haitao and Liang Huang. 2008. Forest-based Translation Rule Extraction. In Proc. of EMNLP 2008, pp206-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mi Haitao</author>
</authors>
<title>Liang Huang and Qun Liu.</title>
<date>2008</date>
<booktitle>In Proc. of ACL2008.</booktitle>
<marker>Haitao, 2008</marker>
<rawString>Mi Haitao, Liang Huang and Qun Liu. 2008. Forestbased translation. In Proc. of ACL2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Och Franz Josef</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL2003.</booktitle>
<marker>Josef, 2003</marker>
<rawString>Och Franz Josef. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petrov Slav</author>
<author>Leon Barrett</author>
<author>Roman Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL2006,</booktitle>
<pages>433--440</pages>
<marker>Slav, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Petrov Slav, Leon Barrett, Roman Thibaux and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of ACL2006, pp433-440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Tong</author>
<author>Jingbo Zhu</author>
<author>Hao Zhang</author>
<author>Muhua Zhu</author>
</authors>
<title>An empirical study of translation rule extraction with multiple parsers.</title>
<date>2010</date>
<booktitle>In Proc. of Coling2010,</booktitle>
<pages>1345--1353</pages>
<marker>Tong, Zhu, Zhang, Zhu, 2010</marker>
<rawString>Xiao Tong, Jingbo Zhu, Hao Zhang and Muhua Zhu. 2010. An empirical study of translation rule extraction with multiple parsers. In Proc. of Coling2010, pp1345-1353</rawString>
</citation>
<citation valid="true">
<authors>
<author>Venugopal Ashish</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Preference grammars: softening syntactic constraints to improve statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL2009,</booktitle>
<pages>236--244</pages>
<marker>Ashish, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Venugopal Ashish, Andreas Zollmann, Noah A. Smith and Stephan Vogel. 2009. Preference grammars: softening syntactic constraints to improve statistical machine translation. In Proc. of NAACL2009, pp236-244</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang Hui</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Aiti Aw and Chew Lim Tan.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP2009,</booktitle>
<pages>172--180</pages>
<marker>Hui, Zhang, Li, 2009</marker>
<rawString>Zhang Hui, Min Zhang, Haizhou Li, Aiti Aw and Chew Lim Tan. 2009. Forest-based tree sequence to string translation model. In Proc. of ACL-IJCNLP2009, pp172-180</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>