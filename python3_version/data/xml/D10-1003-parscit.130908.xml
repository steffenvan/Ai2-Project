<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000153">
<title confidence="0.99429">
Utilizing Extra-sentential Context for Parsing
</title>
<author confidence="0.999258">
Jackie Chi Kit Cheung and Gerald Penn
</author>
<affiliation confidence="0.998514">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.993565">
Toronto, ON, M5S 3G4, Canada
</address>
<email confidence="0.999856">
{jcheung,gpenn}@cs.toronto.edu
</email>
<sectionHeader confidence="0.996659" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999662608695652">
Syntactic consistency is the preference to
reuse a syntactic construction shortly after its
appearance in a discourse. We present an anal-
ysis of the WSJ portion of the Penn Tree-
bank, and show that syntactic consistency is
pervasive across productions with various left-
hand side nonterminals. Then, we implement
a reranking constituent parser that makes use
of extra-sentential context in its feature set.
Using a linear-chain conditional random field,
we improve parsing accuracy over the gen-
erative baseline parser on the Penn Treebank
WSJ corpus, rivalling a similar model that
does not make use of context. We show that
the context-aware and the context-ignorant
rerankers perform well on different subsets of
the evaluation data, suggesting a combined ap-
proach would provide further improvement.
We also compare parses made by models, and
suggest that context can be useful for parsing
by capturing structural dependencies between
sentences as opposed to lexically governed de-
pendencies.
</bodyText>
<sectionHeader confidence="0.99888" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9988595">
Recent corpus linguistics work has produced ev-
idence of syntactic consistency, the preference to
reuse a syntactic construction shortly after its ap-
pearance in a discourse (Gries, 2005; Dubey et al.,
2005; Reitter, 2008). In addition, experimental stud-
ies have confirmed the existence of syntactic prim-
ing, the psycholinguistic phenomenon of syntactic
consistency1. Both types of studies, however, have
</bodyText>
<footnote confidence="0.930038">
1Whether or not corpus-based studies of consistency have
any bearing on syntactic priming as a reality in the human mind
</footnote>
<bodyText confidence="0.968260264705882">
limited the constructions that are examined to partic-
ular syntactic constructions and alternations. For in-
stance, Bock (1986) and Gries (2005) examine spe-
cific constructions such as the passive voice, dative
alternation and particle placement in phrasal verbs,
and Dubey et al. (2005) deal with the internal struc-
ture of noun phrases. In this work, we extend these
results and present an analysis of the distribution of
all syntactic productions in the Penn Treebank WSJ
corpus. We provide evidence that syntactic consis-
tency is a widespread phenomenon across produc-
tions of various types of LHS nonterminals, includ-
ing all of the commonly occurring ones.
Despite this growing evidence that the probability
of syntactic constructions is not independent of the
extra-sentential context, current high-performance
statistical parsers (e.g. (Petrov and Klein, 2007; Mc-
Closky et al., 2006; Finkel et al., 2008)) rely solely
on intra-sentential features, considering the partic-
ular grammatical constructions and lexical items
within the sentence being parsed. We address this
by implementing a reranking parser which takes ad-
vantage of features based on the context surrounding
the sentence. The reranker outperforms the genera-
tive baseline parser, and rivals a similar model that
does not make use of context. We show that the
context-aware and the context-ignorant models per-
form well on different subsets of the evaluation data,
suggesting a feature set that combines the two mod-
els would provide further improvement. Analysis of
the rerankings made provides cases where contex-
tual information has clearly improved parsing per-
is a subject of debate. See (Pickering and Branigan, 1999) and
(Gries, 2005) for opposing viewpoints.
</bodyText>
<page confidence="0.98122">
23
</page>
<note confidence="0.9210755">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 23–33,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9801205">
Figure 1: Visual representation of calculation of prior and
positive adaptation probabilities. t represents the pres-
ence of a construction in the target set. p represents the
presence of the construction in the prime set.
</figureCaption>
<figure confidence="0.437974">
less frequent Production-type deciles more frequent
</figure>
<figureCaption confidence="0.988947">
Figure 2: Production-types (singletons removed) catego-
rized into deciles by frequency and the proportion of the
production-types in that bin that is consistent to a signifi-
cant degree.
</figureCaption>
<figure confidence="0.985239666666667">
¬p p
¬t
t
prior denominator
prior numerator
f¬p,¬t
f¬p,t
pos_adapt denominator
pos_adapt numerator
fp,¬t
fp,t
Proportion of consistent production-types
</figure>
<bodyText confidence="0.9995285">
formance, indicating the potential of extra-sentential
contextual information to aid parsing, especially for
structural dependencies between sentences, such as
parallelism effects.
</bodyText>
<sectionHeader confidence="0.9398205" genericHeader="method">
2 Syntactic Consistency in the Penn
Treebank WSJ
</sectionHeader>
<bodyText confidence="0.999830147058824">
Syntactic consistency has been examined by Dubey
et al. (2005) for several English corpora, including
the WSJ, Brown, and Switchboard corpora. They
have provided evidence that syntactic consistency
exists not only within coordinate structures, but also
in a variety of other contexts, such as within sen-
tences, between sentences, within documents, and
between speaker turns in the Switchboard corpus.
However, their analysis rests on a selected number
of constructions concerning the internal structure of
noun phrases. We extend their result here to arbi-
trary syntactic productions.
There have also been studies into syntactic con-
sistency that consider all syntactic productions in
dialogue corpora (Reitter, 2008; Buch and Pietsch,
2010). These studies find an inverse correlation be-
tween the probability of the appearance of a syn-
tactic structure and the distance since its last occur-
rence, which indicates syntactic consistency. These
studies, however, do not provide consistency results
on subsets of production-types, such as by produc-
tion LHS as our study does, so the implications that
can be drawn from them for improving parsing are
less apparent.
We adopt the measure used by Dubey et al. (2005)
to quantify syntactic consistency, adaptation prob-
ability. This measure originates in work on lexical
priming (Church, 2000), and quantifies the probabil-
ity of a target word or construction w appearing in a
“primed” context. Specifically, four frequencies are
calculated, based on whether the target construction
appears in the previous context (the prime set), and
whether the construction appears after this context
(the target set):
</bodyText>
<equation confidence="0.9493445">
fp,¬t(w) = # of times w in prime set only
f¬p,t(w) = # of times w in target set only
f¬p,¬t(w) = # of times w in neither set
fp,t(w) = # of times w in both sets
</equation>
<bodyText confidence="0.957548">
We also define N to be the sum of the four fre-
</bodyText>
<page confidence="0.998408">
24
</page>
<table confidence="0.9997892">
LHS prior pos adapt ratio + &gt; prior sig. insig. + &lt; prior sig.
ADJP 0.03 0.05 1.96 26 251 0
ADVP 0.21 0.24 1.15 26 122 0
NP 0.17 0.22 1.27 281 2284 0
PP 0.56 0.58 1.04 32 125 0
PRN 0.01 0.03 4.60 12 82 0
PRT 0.06 0.08 1.40 3 3 0
QP 0.03 0.18 5.41 24 147 0
S 0.30 0.34 1.13 42 689 1
SBAR 0.15 0.20 1.31 13 68 0
SINV 0.01 0.01 1.00 3 77 0
VP 0.08 0.12 1.56 148 1459 0
WHADVP 0.04 0.08 1.84 2 8 0
WHNP 0.07 0.10 1.39 3 47 0
WHPP 0.01 0.02 2.65 1 1 0
</table>
<tableCaption confidence="0.998436">
Table 1: Weighted average by production frequency among non-singleton production-types of prior and positive adap-
</tableCaption>
<bodyText confidence="0.9904615">
tation probabilities, and the ratio between them. The columns on the right show the number of production-types
for which the positive adaptation probability is significantly greater than, not different from, or less than the prior
probability. We exclude LHSs with a weighted average prior of less than 0.005, due to the small sample size.
quencies. Then, we define the prior and the positive
adaptation probability of a construction as follows
(See also Figure 1):
</bodyText>
<equation confidence="0.999796">
prior(w) = fp,t(w) + f¬p,t(w)
N
pos adapt(w) = ( fp,t(w)
fp,t w) + fp,¬t(w)
</equation>
<bodyText confidence="0.999371395348837">
A positive adaptation probability that is greater
than the prior probability would be interpreted as
evidence for syntactic consistency for that construc-
tion. We conduct x2 tests for statistical signif-
icance testing. We analyze the Penn Treebank
WSJ corpus according this schema for all produc-
tions that occur in sections 2 to 22. These are the
standard training and development sets for train-
ing parsers. We did not analyze section 23 in or-
der not to use its characteristics in designing our
reranking parser so that we can use this section as
our evaluation test set. Our analysis focuses on the
consistency of rules between sentences, so we take
the previous sentence within the same article as the
prime set, and the current sentence as the target set
in calculating the probabilities given above. The
raw data from which we produced our analysis are
available at http://www.cs.toronto.edu/
˜jcheung/wsj_parallelism_data.txt.
We first present results for consistency in all the
production-types2, grouped by the LHS of the pro-
duction. Table 1 shows the weighted average prior
and positive adaptation probabilities for productions
by LHS, where the weighting is done by the num-
ber of occurrence of that production. Production-
types that only occur once are removed. It also
shows the number of production-types in which the
positive adaptation probability is statistically signif-
icantly greater than, not significantly different from,
and significantly lower than the prior probability.
Quite remarkably, very few production-types are
significantly less likely to reoccur compared to the
prior probability. Also note the wide variety ofLHSs
for which there is a large number of production-
types that are consistent to a significant degree.
While a large number of production-types appears
not to be significantly more likely to occur in a
primed context, this is due to the large number of
production-types which only appear a few times.
Frequently occurring production-types mostly ex-
hibit syntactic consistency.
We show this in Figure 2, in which we put
non-singleton production-types into ten bins by fre-
</bodyText>
<footnote confidence="0.98469">
2That is, all occurrences of a production with a particular
LHS and RHS.
</footnote>
<page confidence="0.996473">
25
</page>
<table confidence="0.979126833333333">
Ten most frequent production-types
production f¬p,t fp,t fp,¬t prior pos adapt ratio
PP → IN NP 5624 26224 5793 0.80 0.82 1.02
NP → NP PP 9033 12451 9388 0.54 0.57 1.05
NP → DT NN 9198 10585 9172 0.50 0.54 1.07
S → NP VP 8745 9897 9033 0.47 0.52 1.11
S → NP VP . 8576 8501 8888 0.43 0.49 1.13
S → VP 8717 7867 9042 0.42 0.47 1.11
NP → PRP 7208 5309 7285 0.32 0.42 1.33
ADVP → RB 7986 3949 7905 0.30 0.33 1.10
NP → NN 7630 3390 7568 0.28 0.31 1.11
VP → TO VP 7039 3552 7250 0.27 0.33 1.23
Ten most consistent among 10% most frequent production-types
production f¬p,t fp,t fp,¬t prior pos adapt ratio
QP → # CD CD 51 18 45 0.00 0.29 163.85
NP → JJ NNPS 52 7 53 0.00 0.12 78.25
NP → NP , ADVP 109 24 99 0.00 0.20 58.05
NP → DT JJ CD NN 63 6 67 0.00 0.08 47.14
PP → IN NP NP 83 10 87 0.00 0.10 43.86
QP → IN $ CD 51 3 49 0.00 0.06 42.28
NP → NP : NP . 237 128 216 0.01 0.37 40.34
INTJ → UH 59 4 60 0.00 0.06 39.26
ADVP → IN NP 108 11 83 0.00 0.12 38.91
NP → CD CD 133 21 128 0.00 0.14 36.21
</table>
<tableCaption confidence="0.9663445">
Table 2: Some instances of consistency effects of productions. All productions’ pos adapt probability is significantly
greater than its prior probability at p &lt; 10−6.
</tableCaption>
<bodyText confidence="0.9999695">
quency and calculated the proportion of production-
types in that bin for which the positive adaptation
probability is significantly greater than the prior. It is
clear that the most frequently occurring production-
types are also the ones most likely to exhibit evi-
dence of syntactic consistency.
Table 2 shows the breakdown of the prior and
positive adaptation calculation components for the
ten most frequent production-types and the ten most
consistent (by the ratio pos adapt / prior) produc-
tions among the top decile of production-types. Note
that all of these production-types are consistent to a
statistically significant degree. Interestingly, many
of the most consistent production-types have NP as
the LHS, but overall, productions with many differ-
ent LHS parents exhibit consistency.
</bodyText>
<sectionHeader confidence="0.986478" genericHeader="method">
3 A Context-Aware Reranker
</sectionHeader>
<bodyText confidence="0.999729928571429">
Having established evidence for widespread syntac-
tic consistency in the WSJ corpus, we now investi-
gate incorporating extra-sentential context into a sta-
tistical parser. The first decision to make is whether
to incorporate the context into a generative or a dis-
criminative parsing model.
Employing a generative model would allow us to
train the parser in one step, and one such parser
which incorporates the previous context has been
implemented by Dubey et al. (2006). They imple-
ment a PCFG, learning the production probabilities
by a variant of standard PCFG-MLE probability es-
timation that conditions on whether a rule has re-
cently occurred in the context or not:
</bodyText>
<equation confidence="0.738519">
c(LHS → RHS, Prime)
P(RHS|LHS, Prime) =
c(LHS, Prime)
LHS and RHS represent the left-hand side and
</equation>
<page confidence="0.96611">
26
</page>
<bodyText confidence="0.999906511111112">
right-hand side of a production, respectively. Prime
is a binary variable which is True if and only if
the current production has occurred in the prime set
(the previous sentence). c represents the frequency
count.
The drawback of such a system is that it doubles
the state space of the model, and hence likely in-
creases the amount of data needed to train the parser
to a comparable level of performance as a more com-
pact model, or would require elaborate smoothing.
Dubey et al. (2006) find that this system performs
worse than the baseline PCFG-MLE model, drop-
ping F1 from 73.3% to 71.6%3.
We instead opt to incorporate the extra-sentential
context into a discriminative reranking parser, which
naturally allows additional features to be incorpo-
rated into the statistical model. Many discriminative
models of constituent parsing have been proposed in
recent literature. They can be divided into two broad
categories–those that rerank the N-best outputs of a
generative parser, and those that make all parsing de-
cisions using the discriminative model. We choose
to implement an N-best reranking parser so that we
can utilize state-of-the-art generative parsers to en-
sure a good selection of candidate parses to feed
into our reranking module. Also, fully discrimina-
tive models tend to suffer from efficiency problems,
though recent models have started to overcome this
problem (Finkel et al., 2008).
Our approach is similar to N-best reranking
parsers such as Charniak and Johnson (2005)
and Collins and Koo (2005), which implement a va-
riety of features to capture within-sentence lexical
and structural dependencies. It is also similar to
work which focuses on coordinate noun phrase pars-
ing (e.g. (Hogan, 2007; K¨ubler et al., 2009)) in that
we also attempt to exploit syntactic parallelism, but
in a between-sentence setting rather than in a within-
sentence setting that only considers coordination.
As evidence of the potential of an N-best rerank-
ing approach with respect to extra-sentential con-
text, we considered the 50-best parses in the devel-
opment set produced by the generative parser, and
categorized each into one of nine bins depending
on whether this candidate parse exhibits more, less,
</bodyText>
<tableCaption confidence="0.608635666666667">
3A similar model which conditions on whether productions
have previously occurred within the same sentence, however,
improves F1 to 73.6%.
</tableCaption>
<table confidence="0.9996145">
less Overlap more
equal
worse F1 32519 7224 17280
equal F1 (81.8%) (69.3%) (75.4%)
better F1 1023 1674 540
(2.6%) (16.1%) (2.4%)
6224 1527 5106
(15.7%) (14.6%) (22.3%)
</table>
<tableCaption confidence="0.955702666666667">
Table 3: Correlation between rule overlap and F1 com-
pared to the generative baseline for the 50-best parses in
the development set.
</tableCaption>
<bodyText confidence="0.999315555555556">
or the same amount of rule overlap with the previ-
ous correct parse than the generative baseline, and
whether the candidate parse has a better, worse, or
the same F1 measure than the generative baseline
(Table 3). We find that a larger percentage of candi-
date parses which share more productions with the
previous parse are better than the generative base-
line parse than for the other categories, and this dif-
ference is statistically significant (x2 test).
</bodyText>
<subsectionHeader confidence="0.99755">
3.1 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.99997">
For our statistical reranker, we implement a linear-
chain conditional random field (CRF). CRFs are a
very flexible class of graphical models which have
been used for various sequence and relational la-
belling tasks (Lafferty et al., 2001). They have been
used for tree labelling, in XML tree labelling (Jousse
et al., 2006) and semantic role labelling tasks (Cohn
and Blunsom, 2005). They have also been used for
shallow parsing (Sha and Pereira, 2003), and full
constituent parsing (Finkel et al., 2008; Tsuruoka et
al., 2009). We exploit the flexibility of CRFs by in-
corporating features that depend on extra-sentential
context.
In a linear-chain CRF, the conditional probabil-
ity of a sequence of labels y = y{t=1...T} given a se-
quence of observed output x = x{t=1...T} and weight
vector 0 = B{k=1...x} is given as follows:
</bodyText>
<equation confidence="0.977538">
1 T
p(y|x) = Z exp(y:
t=1
y: BkA(yt−1, yt, x, t))
k
</equation>
<page confidence="0.985068">
27
</page>
<bodyText confidence="0.999982533333333">
where Z is the partition function. The feature func-
tions fk(yt−1, yt, x, t) can depend on two neighbour-
ing parses, the sentences in the sequence, and the
position of the sentence in the sequence. Since our
feature functions do not depend on the words or
the time-step within the sequence, however, we will
write fk(yt−1, yt) from now on.
We treat each document in the corpus as one CRF
sequence, and each sentence as one time-step in
the sequence. The label sequence then is the se-
quence of parses, and the outputs are the sentences
in the document. Since there is a large number of
parses possible for each sentence and correspond-
ingly many possible states for each label variable,
we restrict the possible label state-space by extract-
ing the N-best parses from a generative parser, and
rerank over the sequences of candidate parses thus
provided. We use the generative parser of Petrov
and Klein (2007), a state-splitting parser that uses an
EM algorithm to find splits in the nonterminal sym-
bols to maximize training data likelihood. We use
the 20-best parses, with an oracle F1 of 94.96% on
section 23.
To learn the weight vector, we employ a stochastic
gradient ascent method on the conditional log like-
lihood, which has been shown to perform well for
parsing tasks (Finkel et al., 2008). In standard gra-
dient ascent, the conditional log likelihood with a L2
regularization term for a Gaussian prior for a train-
ing corpus of N sequences is
</bodyText>
<equation confidence="0.968593148148148">
L(0) = XN X θkfk(y(i)
i=1 t,k t−1, y(i)
t )
X
log Z(i) −
k
And the partial derivatives with respect to the
weights are
∂L XN X fk(y(i)
= i=1 t t−1, y(i)
∂θk t )
X− θk
k σ2
fk(y, y′)P(y, y′ x(i))
−
XN
i=1
−
X
t
X
y,y′
XN
i=1
θ2
k
2σ2
</equation>
<bodyText confidence="0.999759">
The first term is the feature counts in the train-
ing data, and the second term is the feature expecta-
tions according to the current weight vector. The
third term corresponds to the penalty to non-zero
weight values imposed by regularization. The prob-
abilities in the second term can be efficiently calcu-
lated by the CRF-version of the forward-backward
algorithm.
In standard gradient ascent, we update the weight
vector after iterating through the whole training cor-
pus. Because this is computationally expensive, we
instead use stochastic gradient ascent, which ap-
proximates the true gradient by the gradient calcu-
lated from a single sample from the training corpus.
We thus do not have to sum over the training set in
the above expressions. We also employ a learning
rate multiplier on the gradient. Thus, the weight up-
date for the ith encountered training sequence during
training is
</bodyText>
<equation confidence="0.998154">
0 = 0 + αiVLstochastic(0)
τ x N
αi = η τ x N + i
</equation>
<bodyText confidence="0.999967625">
The learning rate function is modelled on the one
used by Finkel et al. (2008). It is designed such that
αi is halved after τ passes through the training set.
We train the model by iterating through the train-
ing set in a randomly permuted order, updating the
weight vector after each sequence. The parameters
η, τ, and σ are tuned to the development set. The fi-
nal settings we use are η = 0.08, τ = 5, and σ = 50.
We use sections 2–21 of the Penn Treebank WSJ for
training, 22 for development, and 23 for testing. We
conduct 20-fold cross validation to generate the N-
best parses for the training set, as is standard for N-
best rerankers.
To rerank, we do inference with the linear-chain
CRF for the most likely sequence of parses using
the Viterbi algorithm.
</bodyText>
<subsectionHeader confidence="0.998481">
3.2 Feature Functions
</subsectionHeader>
<bodyText confidence="0.9999322">
We experiment with various feature functions that
depend on the syntactic and lexical parallelism be-
tween yt−1 and yt. We use the occurrence of a rule
in yt that occurred in yt−1 as a feature. Based on the
results of the corpus analysis, the first representation
</bodyText>
<page confidence="0.994404">
28
</page>
<listItem confidence="0.984918">
(1) (S (NP (DT NN)) (VP (VBD)))
(2) (S (NP (NNS)) (VP (VBD)))
Phrasal features:
</listItem>
<table confidence="0.638194333333333">
Template: (parent, childL, childR, repeated)
(S, edge, NP, +), (S, NP, VP, +), (S, VP, edge, +), (NP, edge,
NNS, −), (NP, NNS, edge, −), (VP, edge, VBD, +), (VP, VBD,
edge, +)
Lexical features:
Template: (parent, POSL, POSR, repeated)
(S, edge, NNS, −), (S, NNS, VBD, −), (S, VBD, edge, +),
(NP, edge, NNS, −), (NP, NNS, edge, −), (VP, edge, VBD,
+), (VP, VBD, edge, +)
</table>
<figureCaption confidence="0.95922">
Figure 3: Example of features extracted from a parse se-
quence specified down to the POS level.
</figureCaption>
<table confidence="0.9991873">
Method F1 (%)
Model-averaged 90.47
Combined, jointly trained −Context 90.33
Combined, jointly trained 90.31
Model-averaged −Context 90.22
lexical −Context 90.21
lexical 90.20
phrasal 90.12
phrasal −Context 89.74
Generative 89.70
</table>
<tableCaption confidence="0.997436">
Table 4: Development set (section 22) results of various
models that we trained. Italicized are the models we use
for the test set.
</tableCaption>
<bodyText confidence="0.999966875">
we tried was to simply enumerate the (non-lexical)
productions in yt along with whether that production
is found in yt−1. However, we found that our most
successful feature function is to consider overlaps in
partial structures of productions.
Specifically, we decompose a tree into all of the
nonlexical vertically and horizontally markovized
subtrees. Each of the subtrees in yt marked by
whether that same subtree occurs in the previous
tree is a feature. The simple production represen-
tation corresponds to a vertical markovization of 1
and a horizontal markovization of infinite. We found
that a vertical markovization of 1 and a horizontal
markovization of 2 produced the best results on our
data. We will call this model the phrasal model.
This schema so far only considers local substruc-
tures of parse trees, without being informed by the
lexical information found in the leaves of the tree.
We try another schema which considers the POS tag
sequences found in each subtree. A feature then is
the node label of the root of the subtree with the POS
tag sequence it dominates, again decomposed into
sequences of length 2 by markovization. We will
call this model the lexical model.
To extract features from this sequence, we con-
sider the substructures in the second parse, and mark
whether they are found in the first parse as well. We
add edge markers to mark the beginning and end of
constituents. See Figure 3 for an example of features
extracted by the two models.
We will consider various ways of combining the
two schemata above in the next section. In addition,
we also add a feature corresponding to the scaled log
probability of a parse tree derived from the genera-
tive parsing baseline. Scaling is necessary because
of the large differences in the magnitude of the log
probability for different sentences. The scaling for-
mula that we found to work best is to scale the max-
imum log probability among the N-best candidate
parses to be 1.0 and the minimum to be 0.0.
</bodyText>
<subsectionHeader confidence="0.900069">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.9999775">
We train the two models which make use of extra-
sentential context described in the previous section,
and use the model to parse the development and
test set. We also trained a model which combines
both sets of features, but we found that we get better
performance by training the two models separately,
then averaging the models by computing the respec-
tive averages of their features’ weights. Thus, we
use the model-averaged version of the models that
consider context in the test set experiments. The
generative parser forms the first baseline method
to which we compare our results. We also train a
reranker which makes use of the same features as we
described above, but without marking whether each
substructure occurs in the previous sentence. This is
thus a reranking method which does not make use
of the previous context. Again, we tried model aver-
aging, but this produces less accurate parses on the
</bodyText>
<page confidence="0.99587">
29
</page>
<table confidence="0.9994252">
LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CB
development set – length &lt; 40 development set – all sentences
Generative 90.33 90.20 90.27 39.92 0.68 71.99 89.64 89.75 89.70 37.76 0.82 68.65
+Context 91.25 90.71 90.98 41.25 0.61 73.45 90.62 90.33 90.47 38.88 0.74 70.47
−Context 90.85 90.78 90.82 40.62 0.62 73.00 90.28 90.38 90.22 38.24 0.74 70.00
</table>
<tableCaption confidence="0.99249425">
Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative
is the generative baseline of Petrov and Klein (2007), +Context is the best performing reranking model using previous
context (model-averaged phrasal and lexical), −Context is the best performing reranking model not using
previous context (jointly trained phrasal and lexical).
</tableCaption>
<table confidence="0.99994">
LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CB
test set – length &lt; 40 test set – all sentences
Generative 90.04 89.84 89.94 38.31 0.80 68.33 89.60 89.35 89.47 36.05 0.94 65.81
+Context 90.63 90.11 90.37 39.02 0.73 69.40 90.17 89.64 89.91 36.84 0.87 67.09
−Context 90.64 90.43 90.54 38.62 0.72 69.84 90.20 89.97 90.08 36.47 0.85 67.55
</table>
<tableCaption confidence="0.999373">
Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
</tableCaption>
<bodyText confidence="0.9998145">
development set, so we use the jointly trained model
on the test set. We will refer to this model as the
context-ignorant or −Context model, as opposed to
the previous context-aware or +Context model. The
results of these experiments on the development set
are shown in Table 4.
PARSEVAL results4 on the development and test
set are presented in Tables 5 and 6. We see that
the reranked models outperform the generative base-
line model in terms of F1, and that the reranked
model that uses extra-sentential context outperforms
the version that does not use extra-sentential context
in the development set, but not in the test set. Us-
ing Bikel’s randomized parsing evaluation compara-
tor5, we find that both reranking models outperform
the baseline generative model to statistical signifi-
cance for recall and precision. The context-ignorant
reranker outperforms the context-aware reranker on
recall (p &lt; 0.01), but not on precision (p = 0.42).
However, the context-aware model has the highest
exact match scores in both the development and the
test set.
The F1 result suggests two possibilities–either the
context-aware model captures the same information
as the context-ignorant model, but less effectively, or
the two models capture different information about
</bodyText>
<footnote confidence="0.975125">
4This evaluation ignores punctuation and corresponds to the
new.prm parameter setting on evalb.
5http://www.cis.upenn.edu/˜dbikel/
software.html
</footnote>
<table confidence="0.998616333333333">
Sec. −Context better same +Context better
22 157 1357 186
23 258 1904 254
</table>
<tableCaption confidence="0.991231">
Table 7: Context-aware vs. context-ignorant reranking
results, by sentential F1.
</tableCaption>
<bodyText confidence="0.999951666666667">
the parses. Two pieces of evidence point to the
latter possibility. First, if the context-aware model
were truly inferior, then we would expect it to out-
perform the context-ignorant model on almost no
sentences. Otherwise, we would expect them to
do well on different sentences. Table 7 shows that
the context-aware model outperforms the context-
ignorant model on nearly as many trees in the test
section as the reverse. Second, if we hypotheti-
cally had an oracle that could determine whether the
context-ignorant or the context-aware model would
be more accurate on a sentence and if the two models
were complementary to each other, we would expect
to achieve a gain in F1 over the generative baseline
which is roughly the sum of the gain achieved by
each model separately. This is indeed the case, as
we are able to achieve F1s of 91.23% and 90.89%
on sections 22 and 23 respectively, roughly twice the
improvement that the individual models obtain.
To put our results in perspective, we now compare
the magnitude of the improvement in F1 our context-
</bodyText>
<page confidence="0.990474">
30
</page>
<table confidence="0.994952">
System Baseline Best Imp. (rel.)
Dubey et al. (2006) 73.3 73.6 0.3 (1.1%)
Hogan (2007) 89.4 89.6 0.2 (1.9%)
This work 89.5 89.9 0.4 (3.8%)
</table>
<tableCaption confidence="0.997696">
Table 8: A comparison of parsers specialized to exploit
</tableCaption>
<bodyText confidence="0.94072575">
intra- or extra-sentential syntactic parallelism on section
23 in terms of the generative baseline they compare them-
selves against, the best F1 their non-baseline models
achieve, and the absolute and relative improvements.
aware model achieves over the generative baseline
to that of other systems specialized to exploit intra-
or extra-sentential parallelism. We achieve a greater
improvement despite the fact that our generative
baseline provides a higher level of performance, and
is presumably thus more difficult to improve upon
(Table 8). These systems do not compare themselves
against a reranked model that does not use paral-
lelism as we do in this work.
During inference, the Viterbi algorithm recov-
ers the most probable sequence of parses, and this
means that we are relying on the generative parser to
provide the context (i.e. the previous parses) when
analyzing any given sentence. We do another type of
oracle analysis in which we provide the parser with
the correct, manually annotated parse tree of the
previous sentence when extracting features for the
current sentence during training and parsing. This
“perfect context” model achieves F1s of 90.42% and
90.00% on sections 22 and 23 respectively, which is
comparable to the best results of our reranking mod-
els. This indicates that the lack of perfect contextual
information is not a major obstacle to further im-
proving parsing performance.
</bodyText>
<subsectionHeader confidence="0.967692">
3.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999737">
We now analyze several specific cases in the devel-
opment set in which the reranker makes correct use
of contextual information. They concretely illustrate
how context can improve parsing performance, and
confirm our initial intuition that extra-sentential con-
text can be useful for parsing. The sentence in (3)
and (4) is one such case.
</bodyText>
<listItem confidence="0.701275">
(3) Generative/Context-ignorant: (S (S A BMA
</listItem>
<bodyText confidence="0.9297175">
spokesman said “runaway medical costs” have
made health insurance “a significant
challenge) ,” and (S margins also have been
pinched ...) (..))
</bodyText>
<listItem confidence="0.720998">
(4) Context-aware: (S (NP A BMA spokesman)
(VP said “runaway medical costs” have made
health insurance “a significant challenge,” and
margins also have been pinched ...) (..))
</listItem>
<bodyText confidence="0.978915411764706">
The baseline and the context-ignorant models
parse the sentence as a conjunction of two S clauses,
misanalyzing the scope of what is said by the BMA
spokesman to the first part of the conjunct. By an-
alyzing the features and feature weight values ex-
tracted from the parse sequence, we determined that
the context-aware reranker is able to correct the
analysis of the scoping due to a parallelism in the
syntactic structure. Specifically, the substructure
5 → V P. is present in both this sentence and the
previous sentence of the reranked sequence, which
also contains a reporting verb.
(5) (S (NP BMA Corp., Kansas City, Mo.,) (VP
said it’s weighing “strategic alternatives” ...
and is contacting possible buyers ...) (..))
As a second example, consider the following sen-
tence.
</bodyText>
<listItem confidence="0.979128125">
(6) Generative/Context-ignorant: To achieve
maximum liquidity and minimize price
volatility, (NP either all markets) (VP should
be open to trading or none).
(7) Context-aware: To achieve maximum liquidity
and minimize price volatility, (CC either) (S
(NP all markets) should be open to trading or
none).
</listItem>
<bodyText confidence="0.999838272727273">
The original generative and context-ignorant
parses posit that “either all markets” is a noun
phrase, which is incorrect. Syntactic parallelism cor-
rects this for two reasons. First, the reranker prefers
a determiner to start an NP in a consistent context,
as both surround sentences also contain this sub-
structure. Also, the previous sentence also contains
a conjunction CC followed by a S node under a S
node, which the reranker prefers.
While these examples show contextual features to
be useful for parsing coordinations, we also found
</bodyText>
<page confidence="0.999582">
31
</page>
<bodyText confidence="0.999936">
context-awareness to be useful for other types of
structural ambiguity such as PP attachment ambi-
guity. Notice that the method we employ to cor-
rect coordination errors is different from previous
approaches which usually rely on lexical or syntac-
tic similarity between conjuncts rather than between
sentences. Our approach can thus broaden the range
of sentences that can be usefully reranked. For ex-
ample, there is little similarity between conjuncts to
avail of in the second example (Sentences 6 and 7).
Based on these analyses, it appears that con-
text awareness provides a source of information for
parsing which is not available to context-ignorant
parsers. We should thus consider integrating both
types of features into the reranking parser to build
on the advantages of each. Specifically, within-
sentence features are most appropriate for lexi-
cal dependencies and some structural dependencies.
Extra-sentential features, on the other hand, are ap-
propriate for capturing the syntactic consistency ef-
fects as we have demonstrated in this paper.
</bodyText>
<sectionHeader confidence="0.999704" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999966941176471">
In this paper, we have examined evidence for syn-
tactic consistency between neighbouring sentences.
First, we conducted a corpus analysis of the Penn
Treebank WSJ, and shown that parallelism exists
between sentences for productions with a variety
of LHS types, generalizing previous results for
noun phrase structure. Then, we explored a novel
source of features for parsing informed by the extra-
sentential context. We improved on the parsing ac-
curacy over a generative baseline parser, and rival a
similar reranking model that does not rely on extra-
sentential context. By examining the subsets of
the evaluation data on which each model performs
best and also individual cases, we argue that con-
text allows a type of structural ambiguity resolution
not available to parsers which only rely on intra-
sentential context.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99962075">
We would like to thank the anonymous reviewers
and Timothy Fowler for their comments. This work
is supported in part by the Natural Sciences and En-
gineering Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.994321" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836078431372">
J.K. Bock. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology, 18(3):355–387.
A. Buch and C. Pietsch. 2010. Measuring syntactic
priming in dialog corpora. In Proceedings of the Con-
ference on Linguistic Evidence 2010: Empirical, The-
oretical and Computational Perspectives.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd ACL, pages 173–180. Association
for Computational Linguistics.
K.W. Church. 2000. Empirical estimates of adaptation:
the chance of two Noriegas is closer to p/2 than p2. In
Proceedings of 18th COLING, pages 180–186. Asso-
ciation for Computational Linguistics.
T. Cohn and P. Blunsom. 2005. Semantic role labelling
with tree conditional random fields. In Ninth Confer-
ence on Computational Natural Language Learning,
pages 169–172.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25–70.
A. Dubey, P. Sturt, and F. Keller. 2005. Parallelism in
coordination as an instance of syntactic priming: Evi-
dence from corpus-based modeling. In Proceedings of
HLT/EMNLP 2005, pages 827–834.
A. Dubey, F. Keller, and P. Sturt. 2006. Integrating syn-
tactic priming into an incremental probabilistic parser,
with an application to psycholinguistic modeling. In
Proceedings of the 21st COLING and the 44th ACL,
pages 417–424. Association for Computational Lin-
guistics.
J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proceedings ofACL-08: HLT, pages 959–967.
S.T. Gries. 2005. Syntactic priming: A corpus-based
approach. Journal of Psycholinguistic Research,
34(4):365–399.
D. Hogan. 2007. Coordinate noun phrase disambigua-
tion in a generative parsing model. In Proceedings of
45th ACL, volume 45, pages 680–687.
F. Jousse, R. Gilleron, I. Tellier, and M. Tommasi. 2006.
Conditional random fields for XML trees. In ECML
Workshop on Mining and Learning in Graphs.
S. K¨ubler, W. Maier, E. Hinrichs, and E. Klett. 2009.
Parsing coordinations. In Proceedings of the 12th
EACL, pages 406–414. Association for Computational
Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In International
Conference on Machine Learning, pages 282–289.
</reference>
<page confidence="0.983944">
32
</page>
<reference confidence="0.9998571">
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proceedings of
HLT-NAACL 2006.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings ofHLT-NAACL
2007, pages 404–411. Association for Computational
Linguistics.
M.J. Pickering and H.P. Branigan. 1999. Syntactic prim-
ing in language production. Trends in Cognitive Sci-
ences, 3(4):136–141.
D. Reitter. 2008. Context Effects in Language Produc-
tion: Models of Syntactic Priming in Dialogue Cor-
pora. Ph.D. thesis, University of Edinburgh.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL,
pages 213–220.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Fast full
parsing by linear-chain conditional random fields. In
Proceedings of the 12th EACL, pages 790–798. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.999381">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732977">
<title confidence="0.999975">Utilizing Extra-sentential Context for Parsing</title>
<author confidence="0.99898">Jackie Chi Kit Cheung</author>
<author confidence="0.99898">Gerald</author>
<affiliation confidence="0.9997425">Department of Computer University of</affiliation>
<address confidence="0.968897">Toronto, ON, M5S 3G4,</address>
<abstract confidence="0.989361166666667">Syntactic consistency is the preference to reuse a syntactic construction shortly after its appearance in a discourse. We present an analysis of the WSJ portion of the Penn Treebank, and show that syntactic consistency is pervasive across productions with various lefthand side nonterminals. Then, we implement a reranking constituent parser that makes use of extra-sentential context in its feature set. Using a linear-chain conditional random field, we improve parsing accuracy over the generative baseline parser on the Penn Treebank WSJ corpus, rivalling a similar model that does not make use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Bock</author>
</authors>
<title>Syntactic persistence in language production.</title>
<date>1986</date>
<journal>Cognitive Psychology,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="1863" citStr="Bock (1986)" startWordPosition="275" endWordPosition="276">duced evidence of syntactic consistency, the preference to reuse a syntactic construction shortly after its appearance in a discourse (Gries, 2005; Dubey et al., 2005; Reitter, 2008). In addition, experimental studies have confirmed the existence of syntactic priming, the psycholinguistic phenomenon of syntactic consistency1. Both types of studies, however, have 1Whether or not corpus-based studies of consistency have any bearing on syntactic priming as a reality in the human mind limited the constructions that are examined to particular syntactic constructions and alternations. For instance, Bock (1986) and Gries (2005) examine specific constructions such as the passive voice, dative alternation and particle placement in phrasal verbs, and Dubey et al. (2005) deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic const</context>
</contexts>
<marker>Bock, 1986</marker>
<rawString>J.K. Bock. 1986. Syntactic persistence in language production. Cognitive Psychology, 18(3):355–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Buch</author>
<author>C Pietsch</author>
</authors>
<title>Measuring syntactic priming in dialog corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Linguistic Evidence 2010: Empirical, Theoretical and Computational Perspectives.</booktitle>
<contexts>
<context position="5240" citStr="Buch and Pietsch, 2010" startWordPosition="772" endWordPosition="775">, and Switchboard corpora. They have provided evidence that syntactic consistency exists not only within coordinate structures, but also in a variety of other contexts, such as within sentences, between sentences, within documents, and between speaker turns in the Switchboard corpus. However, their analysis rests on a selected number of constructions concerning the internal structure of noun phrases. We extend their result here to arbitrary syntactic productions. There have also been studies into syntactic consistency that consider all syntactic productions in dialogue corpora (Reitter, 2008; Buch and Pietsch, 2010). These studies find an inverse correlation between the probability of the appearance of a syntactic structure and the distance since its last occurrence, which indicates syntactic consistency. These studies, however, do not provide consistency results on subsets of production-types, such as by production LHS as our study does, so the implications that can be drawn from them for improving parsing are less apparent. We adopt the measure used by Dubey et al. (2005) to quantify syntactic consistency, adaptation probability. This measure originates in work on lexical priming (Church, 2000), and qu</context>
</contexts>
<marker>Buch, Pietsch, 2010</marker>
<rawString>A. Buch and C. Pietsch. 2010. Measuring syntactic priming in dialog corpora. In Proceedings of the Conference on Linguistic Evidence 2010: Empirical, Theoretical and Computational Perspectives.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13869" citStr="Charniak and Johnson (2005)" startWordPosition="2273" endWordPosition="2276">iterature. They can be divided into two broad categories–those that rerank the N-best outputs of a generative parser, and those that make all parsing decisions using the discriminative model. We choose to implement an N-best reranking parser so that we can utilize state-of-the-art generative parsers to ensure a good selection of candidate parses to feed into our reranking module. Also, fully discriminative models tend to suffer from efficiency problems, though recent models have started to overcome this problem (Finkel et al., 2008). Our approach is similar to N-best reranking parsers such as Charniak and Johnson (2005) and Collins and Koo (2005), which implement a variety of features to capture within-sentence lexical and structural dependencies. It is also similar to work which focuses on coordinate noun phrase parsing (e.g. (Hogan, 2007; K¨ubler et al., 2009)) in that we also attempt to exploit syntactic parallelism, but in a between-sentence setting rather than in a withinsentence setting that only considers coordination. As evidence of the potential of an N-best reranking approach with respect to extra-sentential context, we considered the 50-best parses in the development set produced by the generative</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd ACL, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p2.</title>
<date>2000</date>
<booktitle>In Proceedings of 18th COLING,</booktitle>
<pages>180--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5832" citStr="Church, 2000" startWordPosition="868" endWordPosition="869">ch and Pietsch, 2010). These studies find an inverse correlation between the probability of the appearance of a syntactic structure and the distance since its last occurrence, which indicates syntactic consistency. These studies, however, do not provide consistency results on subsets of production-types, such as by production LHS as our study does, so the implications that can be drawn from them for improving parsing are less apparent. We adopt the measure used by Dubey et al. (2005) to quantify syntactic consistency, adaptation probability. This measure originates in work on lexical priming (Church, 2000), and quantifies the probability of a target word or construction w appearing in a “primed” context. Specifically, four frequencies are calculated, based on whether the target construction appears in the previous context (the prime set), and whether the construction appears after this context (the target set): fp,¬t(w) = # of times w in prime set only f¬p,t(w) = # of times w in target set only f¬p,¬t(w) = # of times w in neither set fp,t(w) = # of times w in both sets We also define N to be the sum of the four fre24 LHS prior pos adapt ratio + &gt; prior sig. insig. + &lt; prior sig. ADJP 0.03 0.05 </context>
</contexts>
<marker>Church, 2000</marker>
<rawString>K.W. Church. 2000. Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p2. In Proceedings of 18th COLING, pages 180–186. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>P Blunsom</author>
</authors>
<title>Semantic role labelling with tree conditional random fields.</title>
<date>2005</date>
<booktitle>In Ninth Conference on Computational Natural Language Learning,</booktitle>
<pages>169--172</pages>
<contexts>
<context position="15889" citStr="Cohn and Blunsom, 2005" startWordPosition="2600" endWordPosition="2603">ge of candidate parses which share more productions with the previous parse are better than the generative baseline parse than for the other categories, and this difference is statistically significant (x2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T} given a sequence of observed output x = x{t=1...T} and weight vector 0 = B{k=1...x} is given as follows: 1 T p(y|x) = Z exp(y: t=1 y: BkA(yt−1, yt, x, t)) k 27 where Z is the partition function. The feature functions fk(yt−1, yt, x, t) can depend on two neighbo</context>
</contexts>
<marker>Cohn, Blunsom, 2005</marker>
<rawString>T. Cohn and P. Blunsom. 2005. Semantic role labelling with tree conditional random fields. In Ninth Conference on Computational Natural Language Learning, pages 169–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="13896" citStr="Collins and Koo (2005)" startWordPosition="2278" endWordPosition="2281">nto two broad categories–those that rerank the N-best outputs of a generative parser, and those that make all parsing decisions using the discriminative model. We choose to implement an N-best reranking parser so that we can utilize state-of-the-art generative parsers to ensure a good selection of candidate parses to feed into our reranking module. Also, fully discriminative models tend to suffer from efficiency problems, though recent models have started to overcome this problem (Finkel et al., 2008). Our approach is similar to N-best reranking parsers such as Charniak and Johnson (2005) and Collins and Koo (2005), which implement a variety of features to capture within-sentence lexical and structural dependencies. It is also similar to work which focuses on coordinate noun phrase parsing (e.g. (Hogan, 2007; K¨ubler et al., 2009)) in that we also attempt to exploit syntactic parallelism, but in a between-sentence setting rather than in a withinsentence setting that only considers coordination. As evidence of the potential of an N-best reranking approach with respect to extra-sentential context, we considered the 50-best parses in the development set produced by the generative parser, and categorized ea</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>M. Collins and T. Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
<author>P Sturt</author>
<author>F Keller</author>
</authors>
<title>Parallelism in coordination as an instance of syntactic priming: Evidence from corpus-based modeling.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<pages>827--834</pages>
<contexts>
<context position="1418" citStr="Dubey et al., 2005" startWordPosition="208" endWordPosition="211"> use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies. 1 Introduction Recent corpus linguistics work has produced evidence of syntactic consistency, the preference to reuse a syntactic construction shortly after its appearance in a discourse (Gries, 2005; Dubey et al., 2005; Reitter, 2008). In addition, experimental studies have confirmed the existence of syntactic priming, the psycholinguistic phenomenon of syntactic consistency1. Both types of studies, however, have 1Whether or not corpus-based studies of consistency have any bearing on syntactic priming as a reality in the human mind limited the constructions that are examined to particular syntactic constructions and alternations. For instance, Bock (1986) and Gries (2005) examine specific constructions such as the passive voice, dative alternation and particle placement in phrasal verbs, and Dubey et al. (2</context>
<context position="4563" citStr="Dubey et al. (2005)" startWordPosition="673" endWordPosition="676">Production-types (singletons removed) categorized into deciles by frequency and the proportion of the production-types in that bin that is consistent to a significant degree. ¬p p ¬t t prior denominator prior numerator f¬p,¬t f¬p,t pos_adapt denominator pos_adapt numerator fp,¬t fp,t Proportion of consistent production-types formance, indicating the potential of extra-sentential contextual information to aid parsing, especially for structural dependencies between sentences, such as parallelism effects. 2 Syntactic Consistency in the Penn Treebank WSJ Syntactic consistency has been examined by Dubey et al. (2005) for several English corpora, including the WSJ, Brown, and Switchboard corpora. They have provided evidence that syntactic consistency exists not only within coordinate structures, but also in a variety of other contexts, such as within sentences, between sentences, within documents, and between speaker turns in the Switchboard corpus. However, their analysis rests on a selected number of constructions concerning the internal structure of noun phrases. We extend their result here to arbitrary syntactic productions. There have also been studies into syntactic consistency that consider all synt</context>
</contexts>
<marker>Dubey, Sturt, Keller, 2005</marker>
<rawString>A. Dubey, P. Sturt, and F. Keller. 2005. Parallelism in coordination as an instance of syntactic priming: Evidence from corpus-based modeling. In Proceedings of HLT/EMNLP 2005, pages 827–834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
<author>F Keller</author>
<author>P Sturt</author>
</authors>
<title>Integrating syntactic priming into an incremental probabilistic parser, with an application to psycholinguistic modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st COLING and the 44th ACL,</booktitle>
<pages>417--424</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12080" citStr="Dubey et al. (2006)" startWordPosition="1982" endWordPosition="1985">onsistent production-types have NP as the LHS, but overall, productions with many different LHS parents exhibit consistency. 3 A Context-Aware Reranker Having established evidence for widespread syntactic consistency in the WSJ corpus, we now investigate incorporating extra-sentential context into a statistical parser. The first decision to make is whether to incorporate the context into a generative or a discriminative parsing model. Employing a generative model would allow us to train the parser in one step, and one such parser which incorporates the previous context has been implemented by Dubey et al. (2006). They implement a PCFG, learning the production probabilities by a variant of standard PCFG-MLE probability estimation that conditions on whether a rule has recently occurred in the context or not: c(LHS → RHS, Prime) P(RHS|LHS, Prime) = c(LHS, Prime) LHS and RHS represent the left-hand side and 26 right-hand side of a production, respectively. Prime is a binary variable which is True if and only if the current production has occurred in the prime set (the previous sentence). c represents the frequency count. The drawback of such a system is that it doubles the state space of the model, and h</context>
<context position="27694" citStr="Dubey et al. (2006)" startWordPosition="4629" endWordPosition="4632">ther the context-ignorant or the context-aware model would be more accurate on a sentence and if the two models were complementary to each other, we would expect to achieve a gain in F1 over the generative baseline which is roughly the sum of the gain achieved by each model separately. This is indeed the case, as we are able to achieve F1s of 91.23% and 90.89% on sections 22 and 23 respectively, roughly twice the improvement that the individual models obtain. To put our results in perspective, we now compare the magnitude of the improvement in F1 our context30 System Baseline Best Imp. (rel.) Dubey et al. (2006) 73.3 73.6 0.3 (1.1%) Hogan (2007) 89.4 89.6 0.2 (1.9%) This work 89.5 89.9 0.4 (3.8%) Table 8: A comparison of parsers specialized to exploit intra- or extra-sentential syntactic parallelism on section 23 in terms of the generative baseline they compare themselves against, the best F1 their non-baseline models achieve, and the absolute and relative improvements. aware model achieves over the generative baseline to that of other systems specialized to exploit intraor extra-sentential parallelism. We achieve a greater improvement despite the fact that our generative baseline provides a higher l</context>
</contexts>
<marker>Dubey, Keller, Sturt, 2006</marker>
<rawString>A. Dubey, F. Keller, and P. Sturt. 2006. Integrating syntactic priming into an incremental probabilistic parser, with an application to psycholinguistic modeling. In Proceedings of the 21st COLING and the 44th ACL, pages 417–424. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>Proceedings ofACL-08: HLT,</booktitle>
<pages>959--967</pages>
<contexts>
<context position="2643" citStr="Finkel et al., 2008" startWordPosition="394" endWordPosition="397">deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. (Petrov and Klein, 2007; McClosky et al., 2006; Finkel et al., 2008)) rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that combines the two models would provide further imp</context>
<context position="13780" citStr="Finkel et al., 2008" startWordPosition="2259" endWordPosition="2262">. Many discriminative models of constituent parsing have been proposed in recent literature. They can be divided into two broad categories–those that rerank the N-best outputs of a generative parser, and those that make all parsing decisions using the discriminative model. We choose to implement an N-best reranking parser so that we can utilize state-of-the-art generative parsers to ensure a good selection of candidate parses to feed into our reranking module. Also, fully discriminative models tend to suffer from efficiency problems, though recent models have started to overcome this problem (Finkel et al., 2008). Our approach is similar to N-best reranking parsers such as Charniak and Johnson (2005) and Collins and Koo (2005), which implement a variety of features to capture within-sentence lexical and structural dependencies. It is also similar to work which focuses on coordinate noun phrase parsing (e.g. (Hogan, 2007; K¨ubler et al., 2009)) in that we also attempt to exploit syntactic parallelism, but in a between-sentence setting rather than in a withinsentence setting that only considers coordination. As evidence of the potential of an N-best reranking approach with respect to extra-sentential co</context>
<context position="16010" citStr="Finkel et al., 2008" startWordPosition="2620" endWordPosition="2623">n for the other categories, and this difference is statistically significant (x2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T} given a sequence of observed output x = x{t=1...T} and weight vector 0 = B{k=1...x} is given as follows: 1 T p(y|x) = Z exp(y: t=1 y: BkA(yt−1, yt, x, t)) k 27 where Z is the partition function. The feature functions fk(yt−1, yt, x, t) can depend on two neighbouring parses, the sentences in the sequence, and the position of the sentence in the sequence. Since our feature function</context>
<context position="17680" citStr="Finkel et al., 2008" startWordPosition="2913" endWordPosition="2916"> variable, we restrict the possible label state-space by extracting the N-best parses from a generative parser, and rerank over the sequences of candidate parses thus provided. We use the generative parser of Petrov and Klein (2007), a state-splitting parser that uses an EM algorithm to find splits in the nonterminal symbols to maximize training data likelihood. We use the 20-best parses, with an oracle F1 of 94.96% on section 23. To learn the weight vector, we employ a stochastic gradient ascent method on the conditional log likelihood, which has been shown to perform well for parsing tasks (Finkel et al., 2008). In standard gradient ascent, the conditional log likelihood with a L2 regularization term for a Gaussian prior for a training corpus of N sequences is L(0) = XN X θkfk(y(i) i=1 t,k t−1, y(i) t ) X log Z(i) − k And the partial derivatives with respect to the weights are ∂L XN X fk(y(i) = i=1 t t−1, y(i) ∂θk t ) X− θk k σ2 fk(y, y′)P(y, y′ x(i)) − XN i=1 − X t X y,y′ XN i=1 θ2 k 2σ2 The first term is the feature counts in the training data, and the second term is the feature expectations according to the current weight vector. The third term corresponds to the penalty to non-zero weight values</context>
<context position="19080" citStr="Finkel et al. (2008)" startWordPosition="3176" endWordPosition="3179"> update the weight vector after iterating through the whole training corpus. Because this is computationally expensive, we instead use stochastic gradient ascent, which approximates the true gradient by the gradient calculated from a single sample from the training corpus. We thus do not have to sum over the training set in the above expressions. We also employ a learning rate multiplier on the gradient. Thus, the weight update for the ith encountered training sequence during training is 0 = 0 + αiVLstochastic(0) τ x N αi = η τ x N + i The learning rate function is modelled on the one used by Finkel et al. (2008). It is designed such that αi is halved after τ passes through the training set. We train the model by iterating through the training set in a randomly permuted order, updating the weight vector after each sequence. The parameters η, τ, and σ are tuned to the development set. The final settings we use are η = 0.08, τ = 5, and σ = 50. We use sections 2–21 of the Penn Treebank WSJ for training, 22 for development, and 23 for testing. We conduct 20-fold cross validation to generate the Nbest parses for the training set, as is standard for Nbest rerankers. To rerank, we do inference with the linea</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Efficient, feature-based, conditional random field parsing. Proceedings ofACL-08: HLT, pages 959–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Gries</author>
</authors>
<title>Syntactic priming: A corpus-based approach.</title>
<date>2005</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1398" citStr="Gries, 2005" startWordPosition="206" endWordPosition="207">does not make use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies. 1 Introduction Recent corpus linguistics work has produced evidence of syntactic consistency, the preference to reuse a syntactic construction shortly after its appearance in a discourse (Gries, 2005; Dubey et al., 2005; Reitter, 2008). In addition, experimental studies have confirmed the existence of syntactic priming, the psycholinguistic phenomenon of syntactic consistency1. Both types of studies, however, have 1Whether or not corpus-based studies of consistency have any bearing on syntactic priming as a reality in the human mind limited the constructions that are examined to particular syntactic constructions and alternations. For instance, Bock (1986) and Gries (2005) examine specific constructions such as the passive voice, dative alternation and particle placement in phrasal verbs,</context>
<context position="3437" citStr="Gries, 2005" startWordPosition="518" endWordPosition="519"> reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that combines the two models would provide further improvement. Analysis of the rerankings made provides cases where contextual information has clearly improved parsing peris a subject of debate. See (Pickering and Branigan, 1999) and (Gries, 2005) for opposing viewpoints. 23 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 23–33, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Figure 1: Visual representation of calculation of prior and positive adaptation probabilities. t represents the presence of a construction in the target set. p represents the presence of the construction in the prime set. less frequent Production-type deciles more frequent Figure 2: Production-types (singletons removed) categorized into deciles by frequency and the proportio</context>
</contexts>
<marker>Gries, 2005</marker>
<rawString>S.T. Gries. 2005. Syntactic priming: A corpus-based approach. Journal of Psycholinguistic Research, 34(4):365–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hogan</author>
</authors>
<title>Coordinate noun phrase disambiguation in a generative parsing model.</title>
<date>2007</date>
<booktitle>In Proceedings of 45th ACL,</booktitle>
<volume>45</volume>
<pages>680--687</pages>
<contexts>
<context position="14093" citStr="Hogan, 2007" startWordPosition="2311" endWordPosition="2312">r so that we can utilize state-of-the-art generative parsers to ensure a good selection of candidate parses to feed into our reranking module. Also, fully discriminative models tend to suffer from efficiency problems, though recent models have started to overcome this problem (Finkel et al., 2008). Our approach is similar to N-best reranking parsers such as Charniak and Johnson (2005) and Collins and Koo (2005), which implement a variety of features to capture within-sentence lexical and structural dependencies. It is also similar to work which focuses on coordinate noun phrase parsing (e.g. (Hogan, 2007; K¨ubler et al., 2009)) in that we also attempt to exploit syntactic parallelism, but in a between-sentence setting rather than in a withinsentence setting that only considers coordination. As evidence of the potential of an N-best reranking approach with respect to extra-sentential context, we considered the 50-best parses in the development set produced by the generative parser, and categorized each into one of nine bins depending on whether this candidate parse exhibits more, less, 3A similar model which conditions on whether productions have previously occurred within the same sentence, h</context>
<context position="27728" citStr="Hogan (2007)" startWordPosition="4637" endWordPosition="4638">aware model would be more accurate on a sentence and if the two models were complementary to each other, we would expect to achieve a gain in F1 over the generative baseline which is roughly the sum of the gain achieved by each model separately. This is indeed the case, as we are able to achieve F1s of 91.23% and 90.89% on sections 22 and 23 respectively, roughly twice the improvement that the individual models obtain. To put our results in perspective, we now compare the magnitude of the improvement in F1 our context30 System Baseline Best Imp. (rel.) Dubey et al. (2006) 73.3 73.6 0.3 (1.1%) Hogan (2007) 89.4 89.6 0.2 (1.9%) This work 89.5 89.9 0.4 (3.8%) Table 8: A comparison of parsers specialized to exploit intra- or extra-sentential syntactic parallelism on section 23 in terms of the generative baseline they compare themselves against, the best F1 their non-baseline models achieve, and the absolute and relative improvements. aware model achieves over the generative baseline to that of other systems specialized to exploit intraor extra-sentential parallelism. We achieve a greater improvement despite the fact that our generative baseline provides a higher level of performance, and is presum</context>
</contexts>
<marker>Hogan, 2007</marker>
<rawString>D. Hogan. 2007. Coordinate noun phrase disambiguation in a generative parsing model. In Proceedings of 45th ACL, volume 45, pages 680–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jousse</author>
<author>R Gilleron</author>
<author>I Tellier</author>
<author>M Tommasi</author>
</authors>
<title>Conditional random fields for XML trees.</title>
<date>2006</date>
<booktitle>In ECML Workshop on Mining and Learning in Graphs.</booktitle>
<contexts>
<context position="15830" citStr="Jousse et al., 2006" startWordPosition="2591" endWordPosition="2594">ative baseline (Table 3). We find that a larger percentage of candidate parses which share more productions with the previous parse are better than the generative baseline parse than for the other categories, and this difference is statistically significant (x2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T} given a sequence of observed output x = x{t=1...T} and weight vector 0 = B{k=1...x} is given as follows: 1 T p(y|x) = Z exp(y: t=1 y: BkA(yt−1, yt, x, t)) k 27 where Z is the partition function. The fea</context>
</contexts>
<marker>Jousse, Gilleron, Tellier, Tommasi, 2006</marker>
<rawString>F. Jousse, R. Gilleron, I. Tellier, and M. Tommasi. 2006. Conditional random fields for XML trees. In ECML Workshop on Mining and Learning in Graphs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K¨ubler</author>
<author>W Maier</author>
<author>E Hinrichs</author>
<author>E Klett</author>
</authors>
<title>Parsing coordinations.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th EACL,</booktitle>
<pages>406--414</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>K¨ubler, Maier, Hinrichs, Klett, 2009</marker>
<rawString>S. K¨ubler, W. Maier, E. Hinrichs, and E. Klett. 2009. Parsing coordinations. In Proceedings of the 12th EACL, pages 406–414. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="15745" citStr="Lafferty et al., 2001" startWordPosition="2576" endWordPosition="2579"> whether the candidate parse has a better, worse, or the same F1 measure than the generative baseline (Table 3). We find that a larger percentage of candidate parses which share more productions with the previous parse are better than the generative baseline parse than for the other categories, and this difference is statistically significant (x2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T} given a sequence of observed output x = x{t=1...T} and weight vector 0 = B{k=1...x} is given as follows: 1 T p(y|x) =</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="2621" citStr="McClosky et al., 2006" startWordPosition="389" endWordPosition="393">nd Dubey et al. (2005) deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. (Petrov and Klein, 2007; McClosky et al., 2006; Finkel et al., 2008)) rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that combines the two models wou</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Effective self-training for parsing. In Proceedings of HLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings ofHLT-NAACL 2007,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2598" citStr="Petrov and Klein, 2007" startWordPosition="385" endWordPosition="388">ment in phrasal verbs, and Dubey et al. (2005) deal with the internal structure of noun phrases. In this work, we extend these results and present an analysis of the distribution of all syntactic productions in the Penn Treebank WSJ corpus. We provide evidence that syntactic consistency is a widespread phenomenon across productions of various types of LHS nonterminals, including all of the commonly occurring ones. Despite this growing evidence that the probability of syntactic constructions is not independent of the extra-sentential context, current high-performance statistical parsers (e.g. (Petrov and Klein, 2007; McClosky et al., 2006; Finkel et al., 2008)) rely solely on intra-sentential features, considering the particular grammatical constructions and lexical items within the sentence being parsed. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that comb</context>
<context position="17292" citStr="Petrov and Klein (2007)" startWordPosition="2846" endWordPosition="2849">quence, however, we will write fk(yt−1, yt) from now on. We treat each document in the corpus as one CRF sequence, and each sentence as one time-step in the sequence. The label sequence then is the sequence of parses, and the outputs are the sentences in the document. Since there is a large number of parses possible for each sentence and correspondingly many possible states for each label variable, we restrict the possible label state-space by extracting the N-best parses from a generative parser, and rerank over the sequences of candidate parses thus provided. We use the generative parser of Petrov and Klein (2007), a state-splitting parser that uses an EM algorithm to find splits in the nonterminal symbols to maximize training data likelihood. We use the 20-best parses, with an oracle F1 of 94.96% on section 23. To learn the weight vector, we employ a stochastic gradient ascent method on the conditional log likelihood, which has been shown to perform well for parsing tasks (Finkel et al., 2008). In standard gradient ascent, the conditional log likelihood with a L2 regularization term for a Gaussian prior for a training corpus of N sequences is L(0) = XN X θkfk(y(i) i=1 t,k t−1, y(i) t ) X log Z(i) − k </context>
<context position="24374" citStr="Petrov and Klein (2007)" startWordPosition="4091" endWordPosition="4094"> of the previous context. Again, we tried model averaging, but this produces less accurate parses on the 29 LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CB development set – length &lt; 40 development set – all sentences Generative 90.33 90.20 90.27 39.92 0.68 71.99 89.64 89.75 89.70 37.76 0.82 68.65 +Context 91.25 90.71 90.98 41.25 0.61 73.45 90.62 90.33 90.47 38.88 0.74 70.47 −Context 90.85 90.78 90.82 40.62 0.62 73.00 90.28 90.38 90.22 38.24 0.74 70.00 Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative is the generative baseline of Petrov and Klein (2007), +Context is the best performing reranking model using previous context (model-averaged phrasal and lexical), −Context is the best performing reranking model not using previous context (jointly trained phrasal and lexical). LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CB test set – length &lt; 40 test set – all sentences Generative 90.04 89.84 89.94 38.31 0.80 68.33 89.60 89.35 89.47 36.05 0.94 65.81 +Context 90.63 90.11 90.37 39.02 0.73 69.40 90.17 89.64 89.91 36.84 0.87 67.09 −Context 90.64 90.43 90.54 38.62 0.72 69.84 90.20 89.97 90.08 36.47 0.85 67.55 Table 6: Parsing results on the test set (se</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings ofHLT-NAACL 2007, pages 404–411. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Pickering</author>
<author>H P Branigan</author>
</authors>
<title>Syntactic priming in language production.</title>
<date>1999</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="3419" citStr="Pickering and Branigan, 1999" startWordPosition="513" endWordPosition="516">. We address this by implementing a reranking parser which takes advantage of features based on the context surrounding the sentence. The reranker outperforms the generative baseline parser, and rivals a similar model that does not make use of context. We show that the context-aware and the context-ignorant models perform well on different subsets of the evaluation data, suggesting a feature set that combines the two models would provide further improvement. Analysis of the rerankings made provides cases where contextual information has clearly improved parsing peris a subject of debate. See (Pickering and Branigan, 1999) and (Gries, 2005) for opposing viewpoints. 23 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 23–33, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Figure 1: Visual representation of calculation of prior and positive adaptation probabilities. t represents the presence of a construction in the target set. p represents the presence of the construction in the prime set. less frequent Production-type deciles more frequent Figure 2: Production-types (singletons removed) categorized into deciles by frequency</context>
</contexts>
<marker>Pickering, Branigan, 1999</marker>
<rawString>M.J. Pickering and H.P. Branigan. 1999. Syntactic priming in language production. Trends in Cognitive Sciences, 3(4):136–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Reitter</author>
</authors>
<title>Context Effects in Language Production: Models of Syntactic Priming in Dialogue Corpora.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1434" citStr="Reitter, 2008" startWordPosition="212" endWordPosition="213">show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies. 1 Introduction Recent corpus linguistics work has produced evidence of syntactic consistency, the preference to reuse a syntactic construction shortly after its appearance in a discourse (Gries, 2005; Dubey et al., 2005; Reitter, 2008). In addition, experimental studies have confirmed the existence of syntactic priming, the psycholinguistic phenomenon of syntactic consistency1. Both types of studies, however, have 1Whether or not corpus-based studies of consistency have any bearing on syntactic priming as a reality in the human mind limited the constructions that are examined to particular syntactic constructions and alternations. For instance, Bock (1986) and Gries (2005) examine specific constructions such as the passive voice, dative alternation and particle placement in phrasal verbs, and Dubey et al. (2005) deal with t</context>
<context position="5215" citStr="Reitter, 2008" startWordPosition="770" endWordPosition="771"> the WSJ, Brown, and Switchboard corpora. They have provided evidence that syntactic consistency exists not only within coordinate structures, but also in a variety of other contexts, such as within sentences, between sentences, within documents, and between speaker turns in the Switchboard corpus. However, their analysis rests on a selected number of constructions concerning the internal structure of noun phrases. We extend their result here to arbitrary syntactic productions. There have also been studies into syntactic consistency that consider all syntactic productions in dialogue corpora (Reitter, 2008; Buch and Pietsch, 2010). These studies find an inverse correlation between the probability of the appearance of a syntactic structure and the distance since its last occurrence, which indicates syntactic consistency. These studies, however, do not provide consistency results on subsets of production-types, such as by production LHS as our study does, so the implications that can be drawn from them for improving parsing are less apparent. We adopt the measure used by Dubey et al. (2005) to quantify syntactic consistency, adaptation probability. This measure originates in work on lexical primi</context>
</contexts>
<marker>Reitter, 2008</marker>
<rawString>D. Reitter. 2008. Context Effects in Language Production: Models of Syntactic Priming in Dialogue Corpora. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="15959" citStr="Sha and Pereira, 2003" startWordPosition="2612" endWordPosition="2615">arse are better than the generative baseline parse than for the other categories, and this difference is statistically significant (x2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T} given a sequence of observed output x = x{t=1...T} and weight vector 0 = B{k=1...x} is given as follows: 1 T p(y|x) = Z exp(y: t=1 y: BkA(yt−1, yt, x, t)) k 27 where Z is the partition function. The feature functions fk(yt−1, yt, x, t) can depend on two neighbouring parses, the sentences in the sequence, and the position of the s</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL, pages 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
<author>S Ananiadou</author>
</authors>
<title>Fast full parsing by linear-chain conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th EACL,</booktitle>
<pages>790--798</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16034" citStr="Tsuruoka et al., 2009" startWordPosition="2624" endWordPosition="2627">ories, and this difference is statistically significant (x2 test). 3.1 Conditional Random Fields For our statistical reranker, we implement a linearchain conditional random field (CRF). CRFs are a very flexible class of graphical models which have been used for various sequence and relational labelling tasks (Lafferty et al., 2001). They have been used for tree labelling, in XML tree labelling (Jousse et al., 2006) and semantic role labelling tasks (Cohn and Blunsom, 2005). They have also been used for shallow parsing (Sha and Pereira, 2003), and full constituent parsing (Finkel et al., 2008; Tsuruoka et al., 2009). We exploit the flexibility of CRFs by incorporating features that depend on extra-sentential context. In a linear-chain CRF, the conditional probability of a sequence of labels y = y{t=1...T} given a sequence of observed output x = x{t=1...T} and weight vector 0 = B{k=1...x} is given as follows: 1 T p(y|x) = Z exp(y: t=1 y: BkA(yt−1, yt, x, t)) k 27 where Z is the partition function. The feature functions fk(yt−1, yt, x, t) can depend on two neighbouring parses, the sentences in the sequence, and the position of the sentence in the sequence. Since our feature functions do not depend on the w</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Fast full parsing by linear-chain conditional random fields. In Proceedings of the 12th EACL, pages 790–798. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>