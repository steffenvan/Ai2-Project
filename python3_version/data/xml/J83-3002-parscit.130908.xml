<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.74533175">
Parse Fitting and Prose Fixing:
Getting a Hold on 111-formedness1
K. Jensen, G. E. Heidorn, L. A. Miller, and Y. Ravin
Computer Sciences Department
</title>
<note confidence="0.533802">
IBM Thomas J. Watson Research Center
P.O. Box 218
</note>
<author confidence="0.68179">
Yorktown Heights, New York 10598
</author>
<bodyText confidence="0.995605266666667">
Processing syntactically ill-formed language is an important mission of the EPISTLE
system. Ill-formed input is treated by this system in various ways. Misspellings are
highlighted by a standard spelling checker; syntactic errors are detected and corrections are
suggested; and stylistic infelicities are called to the user&apos;s attention.
Central to the EPISTLE processing strategy is its technique of fitted parsing. When the
rules of a conventional syntactic grammar are unable to produce a parse for an input string,
this technique can be used to produce a reasonable approximate parse that can serve as
input to the remaining stages of processing.
This paper first describes the fitting process and gives examples of ill-formed language
situations where it is called into play. We then show how a fitted parse allows EPISTLE to
carry on its text-critiquing mission where conventional grammars would fail either because
of input problems or because of limitations in the grammars themselves. Some inherent
difficulties of the fitting technique are also discussed. In addition, we explore how style
critiquing relates to the handling of ill-formed input, and how a fitted parse can be used in
style checking.
</bodyText>
<sectionHeader confidence="0.835261" genericHeader="abstract">
Introduction
</sectionHeader>
<bodyText confidence="0.999717823529412">
In its current form, the EPISTLE system addresses the
problems of grammar and style checking of texts writ-
ten in ordinary English (letters, reports, and manuals,
as opposed to novels, plays, and poems). It is this
goal that involves us so intimately with the processing
of ill-formed language. Grammar checking deals with
such errors as disagreement in number between sub-
ject and verb; style checking calls attention to such
infelicities as sentences that are too wordy or too com-
plex. A standard spelling checker is also included.
Our grammar is written in NLP (Heidorn 1972), an
augmented phrase structure language which is current-
ly implemented in LISP/370. At this time the EPISTLE
grammar uses syntactic, but not semantic, information.
Access to an on-line standard dictionary with about
130,000 entries, including part-of-speech and some
other syntactic information (such as transitivity of
</bodyText>
<footnote confidence="0.783438666666667">
1 The work described here is a continuation of work first
presented at the Conference on Applied Natural Language Process-
ing in Santa Monica, California (Jensen and Heidorn 1983).
</footnote>
<bodyText confidence="0.992372214285714">
verbs), makes the system&apos;s vocabulary essentially un-
limited. We test and improve the grammar by regular-
ly running it on a data base of 2254 sentences from
411 actual business letters. Most of these sentences
are rather complicated; the longest contains 63 words,
and the average length is 19.2 words.
Since the subset of English represented in business
documents is very large, we need a very comprehen-
sive grammar and a robust parser. We take a heuristic
approach and consider that a natural language parser
can be divided into three parts:
(a) a set of rules, called the core grammar, that pre-
cisely defines the central, agreed-upon grammat-
ical structures of a language;
</bodyText>
<listItem confidence="0.887549">
(b) peripheral procedures that handle parsing ambi-
guity: when the core grammar produces more
than one parse, these procedures decide which
of the multiple parses is to be preferred;
(c) peripheral procedures that handle parsing fail-
ure: when the core grammar cannot define an
acceptable parse, these procedures assign some
reasonable structure to the input.
</listItem>
<footnote confidence="0.410838">
Copyright 1984 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted
provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on
the first page. To copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/83/030147-14$03.00
</footnote>
<note confidence="0.66157">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 147
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
In EPISTLE,
</note>
<listItem confidence="0.871651333333333">
(a) the core grammar consists at present of a set of
about 300 syntax rules;
(b) ambiguity is resolved by using a metric that
ranks alternative parses (Heidorn 1982); and
(c) parse failure is handled by the fitting procedure
described here.
</listItem>
<bodyText confidence="0.9999611875">
In using the terms core grammar and periphery, we
are consciously echoing recent work in generative
grammar, but we are applying the terms in a somewhat
different way. Core grammar, in current linguistic
theory, suggests the notion of a set of very general
rules that define universal properties of human lan-
guage and effectively set limits on the types of gram-
mars any particular language may have; periphery
phenomena are those constructions that are peculiar to
particular languages and that require rules beyond
what the core grammar will provide (Lasnik and Freid-
in 1981). Our current work is not concerned with the
meta-rules of a Universal Grammar. But we have
found that a distinction between core and periphery is
useful even within a grammar of a particular language
— in this case, English.
</bodyText>
<subsectionHeader confidence="0.948016">
Parsing in EPISTLE
</subsectionHeader>
<bodyText confidence="0.999833760869565">
EPISTLE&apos;s parser is written in the NLP programming
language, which works with augmented phrase struc-
ture rules and with attribute-value records, which are
manipulated by the rules. When NLP is used to parse
natural language text, the records describe constitu-
ents, and the rules put these constituents together to
form ever larger constituent (or record) structures.
Records contain all the computational and linguistic
information associated with words, with larger constit-
uents, and with the parse formation. At this time our
grammar is sentence-based; we do not, for instance,
create record structures to describe paragraphs. De-
tails of the EPISTLE system and of its core grammar
may be found in Heidorn et al. (1982). An earlier
overview of the system is presented in Miller et al.
(1981).
A close examination of parse trees produced by the
core grammar will often reveal branch attachments
that are not quite right: for example, semantically
incongruous prepositional phrase attachments. In line
with our pragmatic parsing philosophy, our core gram-
mar is designed to produce unique approximate parses.
(Recall that we currently have access only to syntactic
and morphological information about constituents.) In
the cases where semantic or pragmatic information is
needed before a proper attachment can be made, rath-
er than produce a confusion of multiple parses we
force the grammar to try to assign a single parse. This
is usually done by forcing some attachments to be
made to the closest, or rightmost, available constitu-
ent. This strategy only rarely impedes the type of
grammar-checking and style-checking that we are
working on. And we feel that a single parse with a
consistent attachment scheme will yield much more
easily to later semantic processing than would a large
number of different structures.
The rules of the core grammar (CG) produce a
single approximate parse for almost 70% percent of
input text, and a small number of multiple parses for
another 16%. The CG can always be improved and
its coverage extended; work on improving the EPISTLE
CG is continual. But the coverage of a core grammar
will never reach 100%. For those strings that cannot
be fully parsed by rules of the core grammar we use a
heuristic best fit procedure that produces a reasonable
parse structure.
</bodyText>
<subsectionHeader confidence="0.991585">
The Fitting Procedure
</subsectionHeader>
<bodyText confidence="0.9999408125">
The fitting procedure begins after the CG rules have
been applied in a bottom-up, parallel fashion, but have
failed to produce an S node that covers the string. At
this point, as a by-product of bottom-up parsing, rec-
ords are available for inspection that describe the vari-
ous segments of the input string from many perspec-
tives, according to the rules that have been applied.
The term fitting has to do with selecting and fitting
these pieces of the analysis together in a reasonable
fashion.
The fitting algorithm, which is itself implemented
as a set of NLP rules, proceeds in two main stages:
first, a head constituent is chosen; next, remaining
constituents are fitted in. In our current implementa-
tion, candidates for the head are tested preferentially
as follows, from most to least desirable:
</bodyText>
<listItem confidence="0.9788644">
(a) VPs with tense and subject;
(b) VPs with tense but no subject;
(c) phrases without verbs (e.g., NPs, PPs);
(d) non-finite VPs;
(e) others.
</listItem>
<bodyText confidence="0.9997712">
If more than one candidate is found in any category,
the one preferred is the widest (covering most text).
If there is a tie for widest, the leftmost of those is
preferred. If there is a tie for leftmost, the one with
the best value for the parse metric is chosen. If there
is still a tie (a very unlikely case), an arbitrary choice
is made. (Note that we consider a VP to be any seg-
ment of text that has a verb as its head element.)
The fitting process is complete if the head constitu-
ent covers the entire input string (as would be the case
if the string contained just a noun phrase, for example,
&amp;quot;Salutations and congratulations&amp;quot;). If the head con-
stituent does not cover the entire string, remaining
constituents are added on either side, with the follow-
ing order of preference:
</bodyText>
<listItem confidence="0.992468666666667">
(a) segments other than VP;
(b) untensed VPs;
(c) tensed VPs.
</listItem>
<bodyText confidence="0.928269">
As with the choice of head, the widest candidate is
preferred at each step. The fit moves outward from
</bodyText>
<page confidence="0.892509">
148 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<figure confidence="0.9169593">
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
FITTEDI- NP NOUN*---&amp;quot; Example&amp;quot;
I---PUNC &amp;quot;:&amp;quot;
I---VP*I NPI DET
I I NOUN*---&amp;quot;percentage&amp;quot;
I I PPI PREP &amp;quot;of&amp;quot;
I I MONEY*----&amp;quot;$250.00&amp;quot;
I----VERB*---&amp;quot;is&amp;quot;
l----NP MONEY*--&amp;quot;$187.50&amp;quot;
I---PUNC----&amp;quot;.&amp;quot;
</figure>
<figureCaption confidence="0.997518">
Figure 1. An example of a fitted parse tree.
</figureCaption>
<bodyText confidence="0.99159376">
ADJ* &amp;quot;Your&amp;quot;
the head, both leftward to the beginning of the string,
and rightward to the end, until the entire input string
has been fitted into a best approximate parse tree.
The overall effect of the fitting process is to select the
largest chunk of sentence-like material within a text
string and consider it to be central, with left-over
chunks of text attached in some reasonable manner.
As a simple example, consider this text string:
&amp;quot;Example: Your percentage of $250.00 is $187.50.&amp;quot;
Because this string has a capitalized first word and a
period at its end, it is submitted to the core grammar
for consideration as a sentence. But it is not a sen-
tence, and so the CG will fail to arrive at a completed
parse. However, during processing, the CG will have
assigned many structures to its substrings. Looking
for a head constituent among these structures, the
fitting procedure will first seek VPs with tense and
subject. Several are present: &amp;quot;$250.00 is&amp;quot;,
&amp;quot;percentage of $250.00 is&amp;quot;, &amp;quot;$250.00 is $187.50&amp;quot;,
and so on. The widest and leftmost of these VP con-
stituents is the one which covers the string &amp;quot;Your per-
centage of $250.00 is $187.50&amp;quot;, so it will be chosen as
head.
The fitting process then looks for additional con-
stituents to the left, favoring ones other than VP. It
finds first the colon, and then the word &amp;quot;Example&amp;quot;.
In this string the only constituent following the head is
the final period, which is duly added. The complete
fitted parse is shown in Figure 1.
The form of parse tree used here shows the top-
down structure of the string from left to right, with the
terminal nodes being the last item on each line. At
each level of the tree (in a vertical column), the head
element of a constituent is marked with an asterisk.
The other elements above and below are pre- and
post-modifiers. The highest element of the trees
shown here is FITTED, rather than the more usual
SENT. (It is important to remember that these parse
diagrams are only shorthand representations for the
NLP record structures, which contain an abundance of
information about the string processed.)
The tree of Figure 1, which would be lost if we
restricted ourselves to the rules of the core grammar,
is now available for examination, for grammar and
style checking, and ultimately for semantic interpreta-
tion. It can take its place in the stream of continuous
text and be analyzed for what it is — a sentence frag-
ment, interpretable only by reference to other sen-
tences in context.
</bodyText>
<subsectionHeader confidence="0.994541">
Further Examples
</subsectionHeader>
<bodyText confidence="0.994738266666667">
The fitted parse approach can help to deal with many
difficult natural language problems, including frag-
ments, difficult cases of ellipsis, proliferation of rules
to handle single phenomena, phenomena for which no
rule seems adequate, and punctuation horrors. Each
of these is discussed here with examples.
Fragments. There are many of these in running
text; they are frequently NPs, as in Figure 2, and
include common greetings, farewells, and sentiments.
(N.B., most of the examples in this paper are taken
from the EPISTLE data base.)
Difficult cases of ellipsis. In the sentence of
Figure 3, what we really have semantically is a con-
junction of two propositions which, if generated di-
rectly, would read: &amp;quot;Secondly, the Annual Commission
</bodyText>
<table confidence="0.38073425">
FITTEDI---NP*I----NPI AJP ADJ*----&amp;quot;Good&amp;quot;
I---PUNc----&amp;quot;.&amp;quot; NOUN*---&amp;quot;luck&amp;quot;
AJP ADJ*----&amp;quot;good&amp;quot;
NOUN* &amp;quot;selling&amp;quot;
</table>
<figureCaption confidence="0.999201">
Figure 2. Fitted noun phrase (fragment).
</figureCaption>
<figure confidence="0.986335">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 149
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
FITTEDI---VP*1----AVPI----ADV*----&amp;quot;Secondly&amp;quot;
I----PUNC----&amp;quot;,&amp;quot;
I----NPI AJP ADJ*----&amp;quot;the&amp;quot;
NP NOUN*---&amp;quot;Annual&amp;quot;
NP NOUN*---&amp;quot;Commission&amp;quot;
1 I NP NOUN*---&amp;quot;Statement&amp;quot;
NOUN*---&amp;quot;total&amp;quot;
I----VERB &amp;quot;should&amp;quot;
NP MONEY*--&amp;quot;$14,682.61&amp;quot;
I---PUNC----&amp;quot;,&amp;quot;
I---AVP ADV*----&amp;quot;not&amp;quot;
I---NP MONEY*--&amp;quot;$14,682.67&amp;quot;
I---PUNC &amp;quot;.&amp;quot;
</figure>
<figureCaption confidence="0.983619">
Figure 3. Fitted sentence with ellipsis.
</figureCaption>
<figure confidence="0.997037727272727">
FITTEDI- NP NOUN*---&amp;quot;Bill&amp;quot;
I---PUNC----&amp;quot;,&amp;quot;
1---VP*I NP PRON*---&amp;quot;I&amp;quot;
I----VERB &amp;quot;been&amp;quot;
I----VERB* &amp;quot;asked&amp;quot;
1----INFCLI INFT0---&amp;quot;to&amp;quot;
I VERB*---&amp;quot;clarify&amp;quot;
1 NPI AJP ADJ*----&amp;quot;the&amp;quot;
AJP VERB*---&amp;quot;enclosed&amp;quot;
NOUN*---&amp;quot;letter&amp;quot;
I---PUNC----&amp;quot;.&amp;quot;
</figure>
<figureCaption confidence="0.996034">
Figure 4. Fitted sentence with initial vocative.
</figureCaption>
<figure confidence="0.989851944444444">
FITTEDI---NPI AJP ADJ*----&amp;quot;Good&amp;quot;
NOUN*---&amp;quot;luck&amp;quot;
I---PPI PREP----&amp;quot;to&amp;quot;
NP PRON*---&amp;quot;you&amp;quot;
CONJ*---&amp;quot;and&amp;quot;
1I NP PRON*---&amp;quot;yours&amp;quot;
I---CONJ----&amp;quot;,and&amp;quot;
I---VP*1- NP PRON*---&amp;quot;I&amp;quot;
I----VERB* &amp;quot;wish&amp;quot;
I----NP PRON*---&amp;quot;you&amp;quot;
I----NPI AJP ADJ*----&amp;quot;the&amp;quot;
ADV &amp;quot;VERY&amp;quot;
I ADJ*----&amp;quot;best&amp;quot;
I----PPI PREP----&amp;quot;in&amp;quot;
AJP ADJ* &amp;quot;your&amp;quot;
AJP ADJ*----&amp;quot;future&amp;quot;
NOUN*---&amp;quot;efforts&amp;quot;
I---PUNC----&amp;quot;.&amp;quot;
</figure>
<figureCaption confidence="0.999845">
Figure 5. Fitted conjunction of noun phrase with clause.
</figureCaption>
<bodyText confidence="0.833954666666667">
150 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
Statement total should be $14,682.61; the Annual
Commission Statement total should not be
$14,682.67.&amp;quot; Deletion processes operating on the
second proposition are lawful (deletion of identical
elements) but massive. It would be unwise to write a
core grammar rule that routinely allowed negativized
NPs to follow main clauses, because:
</bodyText>
<listItem confidence="0.944172285714286">
(a) the proper analysis of this sentence would be
obscured: some pieces — namely, the inferred
concepts — are missing from the second part of
the surface sentence;
(b) the linguistic generalization would be lost: any
two conjoined propositions can undergo deletion
of identical (recoverable) elements.
</listItem>
<bodyText confidence="0.979682636363636">
A fitted parse such as Figure 3 allows us to inspect the
main clause for syntactic and stylistic deviances, and
at the same time makes clear the breaking point be-
tween the two propositions and opens the door for a
later semantic processing of the elided elements.
Proliferation of rules to handle single
phenomena. There are some English constructions
that, although they have a fairly simple and unitary
form, do not hold anything like a unitary ordering
relation within clause boundaries. The vocative is one
of these:
</bodyText>
<listItem confidence="0.985293833333333">
(a) Bill, I&apos;ve been asked to clarify the enclosed
letter.
(b) I&apos;ve been asked, Bill, to clarify the enclosed
letter.
(c) I&apos;ve been asked to clarify the enclosed letter,
Bill.
</listItem>
<bodyText confidence="0.997686566037736">
In longer sentences there would be even more possible
places to insert the vocative.
Rules could be written that would explicitly allow
the placement of a proper name, surrounded by com-
mas, at different positions in the sentence — a different
rule for each position. But this solution lacks ele-
gance, makes a simple phenomenon seem complicated,
and always runs the risk of overlooking yet one more
position where some other writer might insert a voca-
tive. The parse fitting procedure provides an alterna-
tive that preserves the integrity of the main clause and
allows the vocative to be added onto the structure, as
shown, for example, in Figure 4. Other similar phe-
nomena, such as parenthetical expressions, can be
handled in this same fashion.
Phenomena for which no rule seems adequate.
The sentence &amp;quot;Good luck to you and yours, and I wish
you the very best in your future efforts.&amp;quot; is, on the
face of it, a conjunction of a noun phrase (or NP plus
PP) with a finite verb phrase. Such constructions are
not usually considered to be fully grammatical, and a
core grammar that contains a rule describing this con-
struction ought probably to be called a faulty gram-
mar. Nevertheless, ordinary English correspondence
abounds with strings of this sort, and readers have no
difficulty construing them. The fitted parse for this
sentence in Figure 5 presents the finite clause as its
head and adds the remaining constituents in a reasona-
ble fashion. From this structure later semantic proc-
essing could infer that &amp;quot;Good luck to you and yours&amp;quot;
really means &amp;quot;I express/send/wish good luck to you
and yours&amp;quot; — a special case of formalized, ritualized
ellipsis.
Punctuation horrors. In any large sample of natu-
ral language text, there will be many irregularities of
punctuation that, although perfectly understandable to
readers, can completely disable an explicit computa-
tional grammar. In business text these difficulties are
frequent. Some can be caught and corrected by punc-
tuation checkers and balancers. But others cannot,
sometimes because, for all their trickiness, they are not
really wrong. Yet few grammarians would care to
dignify, by describing it with rules of the core gram-
mar, a text string like:
&amp;quot;Options: A1-(Transmitter Clocked by Data-
set) B3-(without the 605 Recall Unit) CS-
(with ABC Ring Indicator) D8-(without Auto
Answer) E10-(Auto Ring Selective).&amp;quot;
Our parse fitting procedure handles this example by
building a string of NPs separated with punctuation
marks, as shown in Figure 6. This solution at least
enables us to get a handle on the contents of the
string.
</bodyText>
<subsectionHeader confidence="0.826767">
Benefits
</subsectionHeader>
<bodyText confidence="0.999980833333333">
There are two main benefits to be gained from using
the fitted parse approach. First, it allows for syntactic
processing — for our purposes, grammar and style
checking — to proceed in the absence of a perfect
parse. Second, it provides a promising structure to
submit to later semantic processing routines. And
parenthetically, a fitted parse diagram is a great aid to
grammar rule debugging. The place where the first
break occurs between the head constituent and its pre-
or post-modifiers usually indicates fairly precisely
where the core grammar failed.
It should be emphasized that a fitting procedure
cannot be used as a substitute for explicit rules, and
that it in no way lessens the importance of the core
grammar. There is a tight interaction between the two
components. The success of the fitted parse depends
on the accuracy and completeness of the core rules; a
fit is only as good as its grammar.
</bodyText>
<subsectionHeader confidence="0.985944">
Correcting Syntactic Errors in a Fitted Parse
</subsectionHeader>
<bodyText confidence="0.999428833333333">
Suppose the text string in Figure 1 had contained an
ungrammaticality, such as disagreement in number
between its subject and its verb. Then our troubles
would be compounded. There would be two reasons
for the CG to reject that string: (a) it is a fragment;
and (b) it contains a syntax error.
</bodyText>
<figure confidence="0.976455204545454">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 151
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
FITTEDI---NP NOUN*---&amp;quot;Options&amp;quot;
I---PUNC &amp;quot;:&amp;quot;
I---NP NOUN*---&amp;quot;Al&amp;quot;
I---PUNC &amp;quot;-&amp;quot;
1---PUNC &amp;quot;(&amp;quot;
I---NPI NP
NOUN*---&amp;quot;Clocked&amp;quot;
I---PPI PREP----&amp;quot;by&amp;quot;
NOUN*---&amp;quot;Dataset&amp;quot;
I---PUNC &amp;quot;)&amp;quot;
I---NP NOUN*---&amp;quot;B3&amp;quot;
I---PUNC &amp;quot;-&amp;quot;
1---pp*I PUNC----&amp;quot;(&amp;quot;
I PREP----&amp;quot;without&amp;quot;
I AJP ADJ*----&amp;quot;the&amp;quot;
1----QUANT NUM*----&amp;quot;605&amp;quot;
1----NP NOUN*---&amp;quot;Recall&amp;quot;
I----NOUN* &amp;quot;Unit&amp;quot;
I----PUNC &amp;quot;)&amp;quot;
I---NP NOUN*---&amp;quot;C5&amp;quot;
I---PUNC &amp;quot;-&amp;quot;
I---PPI PUNC----&amp;quot;(&amp;quot;
PREP----&amp;quot;with&amp;quot;
NP NOUN*---&amp;quot;ABC&amp;quot;
NP NOUN*---&amp;quot;Ring&amp;quot;
NOUN*---&amp;quot;Indicator&amp;quot;
PUNC----&amp;quot;)&amp;quot;
I---NP NOUN*---&amp;quot;D8&amp;quot;
I---PUNC &amp;quot;-&amp;quot;
I---PPI PUNC----&amp;quot;(&amp;quot;
I I PREP----&amp;quot;without&amp;quot;
I I NP NOUN*---&amp;quot;Auto&amp;quot;
1 I NOUN*---&amp;quot;Answer&amp;quot;
I I PUNC----&amp;quot;)&amp;quot;
I---NP NOUN*---&amp;quot;E10&amp;quot;
I---PUNC &amp;quot;-&amp;quot;
I---NPI PUNC----&amp;quot;(&amp;quot;
I I NP NOUN*---&amp;quot;Auto&amp;quot;
I I NP NOUN*---&amp;quot;Ring&amp;quot;
I I NOUN*---&amp;quot;Selective&amp;quot;
I I PUNC----&amp;quot;)&amp;quot;
I---PUNC &amp;quot;.&amp;quot;
</figure>
<figureCaption confidence="0.999513">
Figure 6. Fitted list.
</figureCaption>
<bodyText confidence="0.976086142857143">
NOUN* ---&amp;quot;Transmitter&amp;quot;
But the CG can recover from many syntax errors:
it can diagnose and correct them, producing the parse
tree that would be appropriate if the correction were
made. Figure 7 illustrates this ability. This number-
disagreement phenomenon is fairly common in current
American English. The tensed verb seems to want to
agree with its closest noun neighbor (in this sentence,
&amp;quot;forms...are&amp;quot;) rather than with its subject NP (&amp;quot;a car-
bon copy...is&amp;quot;). A prescriptive rule still insists that
subject and verb should agree in number, however,
and the EPISTLE grammar provides a correction for
such cases. Note that in the last line of Figure 7 the
word &amp;quot;are&amp;quot; has been changed to &amp;quot;is&amp;quot;. (See Heidorn
et al. (1982) for a more thorough discussion of the
error correction technique.)
And now the fitting procedure allows us to contin-
ue this work even under wildly ungrammatical condi-
tions. Figure 8 is a fitted parse for the string in Figure
1, with a number disagreement error introduced into
the fragment.
</bodyText>
<page confidence="0.547611">
152 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<figure confidence="0.9338614">
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
DECLI---NPI DET ADJ*----&amp;quot;A&amp;quot;
NP NOUN*---&amp;quot;carbon&amp;quot;
NOUN*---&amp;quot;copy&amp;quot;
PPI PREP----&amp;quot;of&amp;quot;
DET ADJ*----&amp;quot;the&amp;quot;
NPI NOUN*---&amp;quot;Workman&amp;quot;
POSS----&amp;quot;&apos;s&amp;quot;
NP NOUN*---&amp;quot;Compensation&amp;quot;
NOUN*---&amp;quot;forms&amp;quot;
I---VERB----&amp;quot;are&amp;quot;
I---VERB*---&amp;quot;enclosed&amp;quot;
I---PPI PREP----&amp;quot;for&amp;quot;
DET ADJ*----&amp;quot;your&amp;quot;
NOUN*---&amp;quot;information&amp;quot;
I---PUNC &amp;quot;.&amp;quot;
GRAMMATICAL ERROR: SUBJECT-VERB NUMBER DISAGREEMENT.
A carbon copy.. .ARE enclosed for your information.
CONSIDER:
A carbon copy.. .IS enclosed for your information.
</figure>
<figureCaption confidence="0.999468">
Figure 7. Diagnosis and correction of a syntax error (not a fitted parse).
</figureCaption>
<figure confidence="0.970663307692308">
FITTED1---NP NOUN*---&amp;quot;Example&amp;quot;
I---PUNC----&amp;quot;:&amp;quot;
I---VP*1----NPI DET ADJ*----&amp;quot;your&amp;quot;
NOUN*---&amp;quot;percentage&amp;quot;
PPI PREP----&amp;quot;of&amp;quot;
MONEY*--&amp;quot;$250.00&amp;quot;
I----VERB*---&amp;quot;are&amp;quot;
I----NP MONEY*--&amp;quot;$187.50&amp;quot;
I---PUNC----&amp;quot;.&amp;quot;
POSSIBLE GRAMMATICAL ERROR: SUBJECT-VERB NUMBER DISAGREEMENT.
Example: your percentage.. .ARE $187.50.
CONSIDER:
Example: your percentage.. .IS $187.50.
</figure>
<figureCaption confidence="0.998651">
Figure 8. Fitted parse containing clause with syntax error.
</figureCaption>
<figure confidence="0.825393777777778">
FITTED1---PP*1----PREP----&amp;quot;Between&amp;quot;
I----NP PRON*---&amp;quot;you&amp;quot;
I----CONJ* &amp;quot;and&amp;quot;
I----NP PRON*---&amp;quot;I&amp;quot;
I---PUNC----&amp;quot;.&amp;quot;
POSSIBLE GRAMMATICAL ERROR: WRONG PRONOUN IN OBJECT POSITION.
BETWEEN you and I.
CONSIDER:
BETWEEN you and ME.
</figure>
<figureCaption confidence="0.999525">
Figure 9. Case error in prepositional phrase.
</figureCaption>
<note confidence="0.4142">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 153
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
</note>
<bodyText confidence="0.999875416666667">
Thanks to the flexibility of this approach, it is pos-
sible to check grammar within the smallest imaginable
constituents (Figure 9) — and in the largest imaginable
(Figure 10).
In summary, there are many different causes for
syntactic ill-formedness in the processing of text:
misspellings, ungrammaticalities, fragments, crazy
punctuation, deficits in the processing grammar, etc.
The techniques described here give us a chance to
recover from all such cases of ill-formedness. First we
develop a core grammar, which itself is capable of
detecting spelling mistakes, and of correcting certain
</bodyText>
<figure confidence="0.908913512820513">
FITTEDI---VP*I----SUBCLI--CONJ----&amp;quot;Before&amp;quot;
I--NPI DET ADJ*----&amp;quot;an&amp;quot;
NOUN*---&amp;quot;approval&amp;quot;
I--VERB &amp;quot;can&amp;quot;
I--VERB &amp;quot;be&amp;quot;
I--VERB* &amp;quot;issued&amp;quot;
NP PRON*---&amp;quot;it&amp;quot;
I----VERB &amp;quot;will&amp;quot;
I----VERB* &amp;quot;be&amp;quot;
1----AJPI ADJ*----&amp;quot;necessary&amp;quot;
1 INFCLI--INFT0---&amp;quot;to&amp;quot;
I--VERB*---&amp;quot;submit&amp;quot;
NP NOUN* &amp;quot;blueprint&amp;quot;
NOUN* &amp;quot;drawings&amp;quot;
PREP &amp;quot;in&amp;quot;
AJP ADJ*----&amp;quot;triplicate&amp;quot;
NOUN* &amp;quot;sets&amp;quot;
PREP &amp;quot;on&amp;quot;
NOUN* &amp;quot;sheets&amp;quot;
AJPI AVP ADV* ----&amp;quot;no&amp;quot;
I----ADJ* &amp;quot;smaller&amp;quot;
I----PPI PREP----&amp;quot;than&amp;quot;
QUANT---ADJ*---&amp;quot;15&amp;quot;
NOUN*---&amp;quot;inches&amp;quot;
I---CONJ----&amp;quot;and&amp;quot;
I---PTPRTCLIVERB*---&amp;quot;drawn&amp;quot;
IPPI PREP----&amp;quot;to&amp;quot;
DET
NOUN* &amp;quot; scale&amp;quot;
AJPI AVP ADV* &amp;quot;no&amp;quot;
I----ADJ*----&amp;quot;smaller&amp;quot;
I----PPI PREP----&amp;quot;than&amp;quot;
NOUN*---&amp;quot; /8th&amp;quot;
PPI PREP----&amp;quot;of&amp;quot;
DET ADJ*----&amp;quot;an&amp;quot;
NOUN*---&amp;quot;inch&amp;quot;
I----PPI PREP &amp;quot;to&amp;quot;
DET ADJ*----&amp;quot;the&amp;quot;
NOUN* &amp;quot;foot&amp;quot;
</figure>
<bodyText confidence="0.695728166666667">
I---PUNC----&amp;quot;.&amp;quot;
POSSIBLE GRAMMATICAL ERROR: MISSING COMMA.
Before an approval can be issued it will be necessary...
CONSIDER:
Before an approval can be issued, it will be necessary...
A COMMA IS NEEDED TO DEFINE CLAUSE BOUNDARIES
</bodyText>
<figureCaption confidence="0.910112">
Figure 10. Comma error in long complex sentence.
</figureCaption>
<page confidence="0.940389">
154 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.763108">
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
</note>
<bodyText confidence="0.999950125">
syntactic mistakes when they occur in otherwise legiti-
mate sentences. To this core grammar we couple a
fitting procedure that produces a reasonable best-guess
parse for all other text strings, regardless of whether
they meet the grammar&apos;s criteria for sentencehood.
The fitted parse then allows us to check even non-
sentences for those categories of syntactic errors
that we can correct.
</bodyText>
<subsectionHeader confidence="0.992059">
Critiquing Stylistic III-Formedness in EPISTLE
</subsectionHeader>
<bodyText confidence="0.999989333333333">
The style component of EPISTLE uses the sentence
structures provided by the parser as input for stylistic
critiquing. It consists of a set of NLP rules that apply
to parse trees of sentences, identify stylistic errors and
suggest corrections. There is a fundamental difference
between style analysis and grammar analysis, however:
the grammar rule-system is based on a set of objective
syntactic criteria that determine whether the input is
well-formed or not. The style rules, by contrast, are
based on relative criteria. They place the input on a
continuum of stylistic acceptability so that what is a
stylistic error becomes a matter of degree.
</bodyText>
<subsectionHeader confidence="0.375373">
Types of stylistic ill-formedness.
</subsectionHeader>
<bodyText confidence="0.920305333333333">
(1) Punctuation. Stylistic ill-formedness is
relative since it depends on both the linguistic and the
extra-linguistic contexts. Linguistically, a combination
of grammatical factors in the sentence can make a
sentence more or less ill-formed. For example, the
need for a comma in a compound sentence increases
with the length of the conjoined clauses. Thus, in &amp;quot;a
decision was reached and the meeting ended,&amp;quot; a com-
ma before the &amp;quot;and&amp;quot; is optional; but in &amp;quot;a decision
which was moderate enough to satisfy even my objec-
tions was reached, and the meeting was finally
adjourned,&amp;quot; a comma is necessary. An even longer
sentence containing several other commas might re-
quire a semi-colon before the &amp;quot;and&amp;quot;.
To be able to detect the missing comma, the stylis-
tic rule must have access to syntactic information pro-
vided by the parser. It has to know that the sentence
is compound. Moreover, it has to know that each
clause contains its own subject (in this case, &amp;quot;a
decision&amp;quot; and &amp;quot;the meeting&amp;quot;), since a compound sen-
tence with only one subject does not take a comma.
After all the syntactic conditions are checked and met,
the rule measures the length of the clauses to deter-
mine how badly the comma is needed. The output is
shown in Figure 11.
(2) Other Types of Stylistic Ill-Formedness.
The style component of EPISTLE detects other in-
stances of missing or faulty punctuation (a comma at
the end of a subordinate clause, no colon before a
single noun-phrase, etc). It also addresses some types
of complicated grammatical constructions that may
impede the reader&apos;s comprehension, such as excessive
length, excessive noun-modification (e.g. &amp;quot;early child-
hood thought disorder misdiagnosis&amp;quot;), and excessive
negation (e.g., &amp;quot;neither the professor, nor his two as-
sistants, who have been working with him on this pro-
ject, haven&apos;t noticed the theft&amp;quot;). Some usage viola-
tions are signaled, such as &amp;quot;split infinitives&amp;quot; and the
usage of &amp;quot;most&amp;quot; instead of &amp;quot;almost&amp;quot;; and finally,
some cosmetic changes are proposed when the syntac-
tic structure is too uniform or when there is excessive
repetition.
</bodyText>
<listItem confidence="0.822629">
(3) Repetition. Repetition is another instance
of the relative nature of stylistic ill-formedness. Gen-
erally, repetition of strings is to be avoided; however,
some cases of repetition are more acceptable than
</listItem>
<bodyText confidence="0.891684666666667">
others. The degree of acceptability depends on the
syntactic function of the repeated strings. In &amp;quot;the
meeting is very very important,&amp;quot; the two instances of
&amp;quot;very&amp;quot; have the same syntactic function: they both
intensify the adjective &amp;quot;important.&amp;quot; This double repe-
tition, lexical and syntactic, is considered poor style.
The error correction for this sentence can be seen in
Figure 12. By contrast, in &amp;quot;it does not surprise me
that that institution no longer exists,&amp;quot; the two in-
stances of &amp;quot;that&amp;quot; have different syntactic roles — one
is a conjunction; the other, a determiner. This sen-
tence is stylistically more acceptable. Finally, in &amp;quot;what
he does does not concern us,&amp;quot; the two instances of
&amp;quot;does&amp;quot; belong to two different clauses. The style rules
accept lexical repetition of this kind.
</bodyText>
<subsectionHeader confidence="0.660624">
Correcting stylistic errors in a fitted parse.
</subsectionHeader>
<bodyText confidence="0.99995175862069">
Because syntactic information is always available, the
style rules can apply to fitted parses, as they do to
regular sentences. They not only signal stylistic errors
within the fitted parse but also assign different degrees
of acceptability to different types of fitted parses. As
noun phrases are the most commonly encountered type
of fragment, the rules accept noun phrases (&amp;quot;My
warmest regards to your son&amp;quot;) but mark subordinate
clauses as incomplete (&amp;quot;Because he refused to sign the
papers&amp;quot;), as shown in Figures 13 and 14.
Extra-linguistic factors. The degree of stylistic
ill-formedness of a sentence depends on extra-
linguistic factors. The use of contracted verb-forms
(e.g. &amp;quot;don&apos;t&amp;quot;, &amp;quot;I&apos;ll&amp;quot;) is quite acceptable in informal
writing; it is to be avoided, though, in formal docu-
ments. Style rules should accommodate different de-
grees of formality. They should also be sensitive to the
stylistic norms observed in different domains. In a
technical manual, for example, a uniform sentence-
pattern is preferred, as it facilitates the reader&apos;s com-
prehension; in freshman compositions, on the other
hand, a variety of sentence-patterns is more appropri-
ate as it breaks the monotony. The style component of
EPISTLE will address such extra-linguistic factors in
addition to the purely linguistic factors. In order to do
so, it will present the user with a menu of style op-
tions. The selection of the formal option will activate
the &amp;quot;no verb-contraction&amp;quot; rule; the selection of the
informal option will suppress it. Similarly, the
</bodyText>
<note confidence="0.620527">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 155
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
</note>
<figure confidence="0.975079608695652">
CMPDI- VPI NPI DET ADJ*----&amp;quot;A&amp;quot;
NOUN*---&amp;quot;decision&amp;quot;
RELCLI--NP PRON*---&amp;quot;which&amp;quot;
1--VERB* &amp;quot;was&amp;quot;
1--AJPI ADJ*----&amp;quot;moderate&amp;quot;
I ADV &amp;quot;enough&amp;quot;
I----INFCLI INFTO---&amp;quot;to&amp;quot;
I VERB*---&amp;quot;satisfy&amp;quot;
I NPI AJP ADV*----&amp;quot;even&amp;quot;
DET ADJ*----&amp;quot;my
NOUN*---&amp;quot;objections&amp;quot;
VERB----&amp;quot;was&amp;quot;
VERB*---&amp;quot;reached&amp;quot;
I---CONJ* &amp;quot;and&amp;quot;
NPI DET ADJ*----&amp;quot;the&amp;quot;
NOUN*---&amp;quot;meeting&amp;quot;
VERB----&amp;quot;was&amp;quot;
1 AVP ADV*----&amp;quot;finally&amp;quot;
VERB*---&amp;quot;adjourned&amp;quot;
I---PUNC &amp;quot;.&amp;quot;
STYLISTIC WEAKNESS: MISSING COMMA IN COMPOUND SENTENCE.
WHY NOT HAVE A COMMA BEFORE THE CONJUNCTION?
...was reached, and the meeting was finally adjourned.
</figure>
<figureCaption confidence="0.998417">
Figure 11. Diagnosis of a punctuation problem.
</figureCaption>
<table confidence="0.671536888888889">
DECLI---NPI DET ADJ*----&amp;quot;The&amp;quot;
NOUN*---&amp;quot;meeting&amp;quot;
1---AJP1 AVP ADV*----&amp;quot;very&amp;quot;
AVP ADV*----&amp;quot;very&amp;quot;
1----ADJ* &amp;quot;important&amp;quot;
I---PUNC----&amp;quot;.&amp;quot;
STYLISTIC WEAKNESS: REPETITION.
WHY NOT AVOID REPETITION?
The meeting is very important.
</table>
<figureCaption confidence="0.960958">
Figure 12. Diagnosis of a repetition problem.
</figureCaption>
<figure confidence="0.965601714285714">
FITTEDI---NP*I----DET ADJ*----&amp;quot;My&amp;quot;
1 I----AJP ADJ*----&amp;quot;warmest&amp;quot;
1 I----NOUN* &amp;quot;regards&amp;quot;
I---PPI PREP----&amp;quot;to&amp;quot;
DET ADJ*----&amp;quot;your&amp;quot;
NOUN*---&amp;quot;son&amp;quot;
I---PUNC &amp;quot;.&amp;quot;
</figure>
<figureCaption confidence="0.998514">
Figure 13. Fitted noun phrase (no style problems).
</figureCaption>
<page confidence="0.636879">
156 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<note confidence="0.493611">
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
</note>
<figure confidence="0.7427787">
FITTEDI---CONJ----&amp;quot;Because&amp;quot;
I---VP*I NP PRON*---&amp;quot;he&amp;quot;
I----VERB* &amp;quot;refused&amp;quot;
I----INFCLI INFT0---&amp;quot;to&amp;quot;
I VERB*---&amp;quot;sign&amp;quot;
I--NPI DET ADJ*----&amp;quot;the&amp;quot;
NOUN* &amp;quot;papers&amp;quot;
I---PUNC----&amp;quot;.&amp;quot;
POSSIBLE STYLISTIC WEAKNESS: INCOMPLETE SENTENCE.
WHY NOT COMPLETE THIS SENTENCE BY ADDING A MAIN CLAUSE?
</figure>
<figureCaption confidence="0.99789">
Figure 14. Fragment (subordinate clause) with diagnosis.
</figureCaption>
<bodyText confidence="0.999722666666667">
technical-writing option will diagnose excessive syntac-
tic variety, whereas the creative-writing option will
diagnose monotonous regularity.
</bodyText>
<subsectionHeader confidence="0.963896">
Potential Difficulties
</subsectionHeader>
<bodyText confidence="0.999954865671642">
Because the error-detection and fitting procedures
permit all sorts of non-sentences to survive, they will
inevitably increase the number of ambiguities that the
system produces, and will require that additional effort
be spent to restrict the number of possible parses.
This is a difficult but by no means impossible task,
since all it entails is the addition of more thorough
constraints on the core grammar.
As an example, consider the input string
&amp;quot;What exactly does that 15 months do.&amp;quot;
The intended meaning of this string could probably
be paraphrased as &amp;quot;What exactly does that 15-month
period mean?&amp;quot; But there are two problems with the
input. First and most confusingly, the phrase &amp;quot;that 15
months,&amp;quot; as given, has a plural head noun (&amp;quot;months&amp;quot;)
and a singular determiner (&amp;quot;that&amp;quot;). Since the CG
cannot understand meaning, it has no way of telling
that the given phrase might be an elided form of &amp;quot;that
15-month period.&amp;quot; It therefore detects and corrects a
syntactic error: number disagreement between premo-
difier and noun. Secondly, the input string should end
with a question mark. But in order to diagnose this
error, the grammar needs to realize that a question
was intended.
When the problem sentence was submitted to an
earlier version of the CG, three parses resulted
(Figures 15-17).
The parse in Figure 15 would be appropriate for a
sentence such as &amp;quot;Who(ever) exactly does that job
prevails.&amp;quot; However, it is thoroughly unhelpful for the
sentence at hand. It diagnoses two errors, neither of
which really exists.
Figure 16 is close to acceptable for the input string.
If the time adverbial NP (AVPNP in the parse tree)
were replaced by a subject NP, the syntactic structure
would be correct for the intended meaning. As things
stand, this parse is only 50% helpful: it correctly
diagnoses the missing question mark, but it incorrectly
insists that &amp;quot;month&amp;quot; should be singular in number.
The third parse (Figure 17) gives the desired single
error correction, but it does so on the basis of a totally
inappropriate parse. The structure in this figure would
fit a question like &amp;quot;Who exactly suffers (in order) that
many people might live?&amp;quot;
The current version of the CG blocks the three
parses in Figures 15 through 17 on a principled basis.
Figure 15 can be blocked by tightening some con-
straints on the diagnosis of subject-verb number disa-
greement in fitted parses. Figure 16 is blocked be-
cause of the presence of an adverbial NP where the
subject NP ought to be. Figure 17 is blocked by stipu-
lating that all subordinate clauses beginning with a
&amp;quot;that&amp;quot; conjunction should have modal or subjunctive
predicates.
An acceptable parse (Figure 18) is provided when
the number agreement restriction is removed from
phrases like &amp;quot;that 15 months.&amp;quot; This is accomplished
by telling the proper rule to ignore number agreement
in particular cases that involve a small subset of quan-
tified English time words. Admittedly, the fix would
be more pleasing if it were part of a larger scheme for
understanding meaning and context. But the correc-
tion moves in the right direction, and certainly does
not prevent future processing with a more intelligent
semantic component. This situation clearly illustrates
how error detection results in the addition of finer
constraints on the core grammar.
</bodyText>
<subsectionHeader confidence="0.892932">
Related Work
</subsectionHeader>
<bodyText confidence="0.99870025">
The parsing approach closest in spirit to our fitting
procedure is that described in Slocum (1983, p. 170):
the LRC Machine Translation System uses a &amp;quot;shortest
path&amp;quot; technique to construct a &amp;quot;phrasal analysis&amp;quot; of
ungrammatical input. With this analysis, phrases can
be translated separately, even in the absence of a total
sentence parse. Aside from Slocum&apos;s work, most of
the reports in this field suggest that unparsable or
</bodyText>
<table confidence="0.928634421052632">
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 157
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
DECLI---NPI PRON*---&amp;quot;What&amp;quot;
VPI AVP ADV*----&amp;quot;exactly&amp;quot;
VERB----&amp;quot;does&amp;quot;
DET ADJ*----&amp;quot;that&amp;quot;
QUANT ADJ*----&amp;quot;15&amp;quot;
NOUN* &amp;quot;months&amp;quot;
I---VERB*---&amp;quot;do&amp;quot;
GRAMMATICAL ERROR: SUBJECT-VERB NUMBER DISAGREEMENT.
WHAT exactly does that 15 months DO.
CONSIDER:
WHAT exactly does that 15 months DOES.
GRAMMATICAL ERROR: PREMODIFIER-NOUN NUMBER DISAGREEMENT.
What exactly does THAT...MONTHS do.
CONSIDER:
What exactly does THAT...MONTH do.
THE COMBINED GRAMMATICAL CORRECTIONS ARE:
What exactly does that 15 month does.
</table>
<figureCaption confidence="0.990356">
Figure 15. Two faulty error diagnoses; inappropriate parse.
</figureCaption>
<bodyText confidence="0.992976047619048">
ill-formed input should be handled by relaxation
techniques, that is, by relaxing restrictions in the gram-
mar rules in some principled way. This is undoubtedly
a useful strategy — one which EPISTLE makes use of,
in fact, in its rules for detecting grammatical errors
(Heidorn et al. 1982). However, it is questionable
whether such a strategy can ultimately succeed in the
face of the overwhelming (for all practical purposes,
infinite) variety of ill-formedness with which we are
faced when we set out to parse truly unrestricted natu-
ral language input. If all ill-formedness is rule-based
(Weischedel and Sondheimer 1981, p. 3), it can only
be by some very loose definition of the term rule, such
as that which might apply to the fitting algorithm de-
scribed here.
Thus Weischedel and Black (1980) suggest three
techniques for responding intelligently to unparsable
inputs:
(a) using presuppositions to determine user assump-
tions; this course is not available to a syntactic
grammar like EPISTLE&apos;s;
</bodyText>
<listItem confidence="0.951011">
(b) using relaxation techniques;
(c) supplying the user with information about the
</listItem>
<bodyText confidence="0.999957424242424">
point where the parse is blocked; this would
require an interactive environment, which would
not be possible for every type of natural lan-
guage processing application.
Kwasny and Sondheimer (1981) are strong propo-
nents of relaxation techniques, which they use to han-
dle both cases of clearly ungrammatical structures,
such as co-occurrence violations like subject/verb disa-
greement, and cases of perfectly acceptable but diffi-
cult constructions (ellipsis and conjunction).
Weischedel and Sondheimer (1982) describe an
improved ellipsis processor. No longer is ellipsis han-
dled with relaxation techniques, but by predicting
transformations of previous parsing paths that would
allow for the matching of fragments with plausible
contexts. This plan would be appropriate as a next
step after the fitted parse, but it does not guarantee a
parse for all elided inputs.
Hayes and Mouradian (1981) also use the relaxa-
tion method. They achieve flexibility in their parser
by relaxing consistency constraints (grammatical restric-
tions, like Kwasny and Sondheimer&apos;s co-occurrence
violations) and also by relaxing ordering constraints.
However, they are working with a restricted-domain
semantic system and their approach, as they admit,
&amp;quot;does not embody a solution for flexible parsing of
natural language in general&amp;quot; (p. 236).
The work of Wilks is heavily semantic and there-
fore quite different from EPISTLE, but his general
philosophy meshes nicely with the philosophy of the
fitted parse: &amp;quot;It is proper to prefer the normal ... but
it would be absurd ... not to accept the abnormal if it
is described&amp;quot; (Wilks 1975, p. 267).
</bodyText>
<page confidence="0.851747">
158 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
<figure confidence="0.981448">
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
DECLI---NP PRON*---&amp;quot;What&amp;quot;
I---AVP ADV*____&amp;quot; exactly&amp;quot;
I---VERB &amp;quot;does&amp;quot;
I---AVPNPI DET ADJ*----&amp;quot;that&amp;quot;
I---QUANT ADJ*----&amp;quot;15&amp;quot;
I---NOUN* &amp;quot;months&amp;quot;
I---VERB*----&amp;quot;do&amp;quot;
I---PUNC &amp;quot;..
GRAMMATICAL ERROR: MISSING QUESTION MARK.
What exactly does that 15 months do.
CONSIDER:
What exactly does that 15 months do?
GRAMMATICAL ERROR: PREMODIFIER-NOUN NUMBER DISAGREEMENT.
What exactly does THAT...MONTHS do.
CONSIDER:
What exactly does THAT...MONTH do.
THE COMBINED GRAMMATICAL CORRECTIONS ARE:
What exactly does that 15 month do?
</figure>
<figureCaption confidence="0.94583">
Figure 16. One faulty diagnosis, one correct; near-satisfactory parse.
</figureCaption>
<figure confidence="0.994728">
DECLI---NP PRON*---&amp;quot;What&amp;quot;
I---AVP ADV*----&amp;quot;exactly&amp;quot;
I---VERB* &amp;quot;does&amp;quot;
I---SUBCLI CONJ----&amp;quot;that&amp;quot;
I NPI QUANT---ADJ*----&amp;quot;15&amp;quot;
NOUN*---&amp;quot;months&amp;quot;
I--VERB* &amp;quot;do&amp;quot;
I---PUNC----&amp;quot;.&amp;quot;
GRAMMATICAL ERROR: MISSING QUESTION MARK.
What exactly does that 15 months do.
CONSIDER:
What exactly does that 15 months do?
</figure>
<figureCaption confidence="0.985021">
Figure 17. Correct error diagnosis but misleading parse.
</figureCaption>
<reference confidence="0.80557237254902">
DECLI---NP PRON*---&amp;quot;What&amp;quot;
I---AVP ADV*----&amp;quot;exactly&amp;quot;
I---VERB &amp;quot;does&amp;quot;
I---NPI DET ADJ*----&amp;quot;that&amp;quot;
QUANT---ADJ*----&amp;quot;15&amp;quot;
NOUN*---&amp;quot;months&amp;quot;
I---PUNC &amp;quot;.&amp;quot;
GRAMMATICAL ERROR: MISSING QUESTION MARK.
What exactly does that 15 months do.
CONSIDER:
What exactly does that 15 months do?
Figure 18. Correct error diagnosis; satisfactory parse.
American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 159
K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing
References
Hayes, P.J. and Mouradian, G.V. 1981 Flexible Parsing. Am. J.
Comp. Ling. 7(4): 232-242.
Heidorn, G.E. 1972 Natural Language Inputs to a Simulation
Programming System. Technical Report NPS-55HD72101A.
Monterey, California: Naval Postgraduate School.
Heidorn, G.E. 1982 Experience with an Easily Computed Metric
for Ranking Alternative Parses. Proc. 20th Annual Meeting of
the ACL. Toronto, Canada: 82-84.
Heidorn, G.E.; Jensen, K.; Miller, L.A.; Byrd, R.J.; and Chodorow,
M.S. 1982 The EPISTLE Text-Critiquing System. IBM Sys-
tems Journal 21(3): 305-326.
Jensen, K. and Heidorn, G.E. 1983 The Fitted Parse: 100%
Parsing Capability in a Syntactic Grammar of English. Proc.
Conf. on Applied Natural Language Processing. Santa Monica,
California: 93-98.
Kwasny, S.C. and Sondheimer, N.K. 1981 Relaxation Techniques
for Parsing Ill-Formed Input. Am. J. Comp. Ling. 7(2): 99-
108.
Lasnik, H. and Freidin, R. 1981 Core Grammar, Case Theory,
and Markedness. Proc. 1979 GLOW Conf. Pisa, Italy.
Miller, L.A.; Heidorn, G.E; and Jensen, K. 1981 Text-Critiquing
with the EPISTLE System: An Authors&apos;s Aid to Better Syn-
tax. AFIPS Conf. Proc., Vol. 50. Arlington, Virginia: 649-655.
Slocum, Jonathan. 1983 A Status Report on the LRC Machine
Translation System. Proc. Conf. on Applied Natural Language
Processing. Santa Monica, California: 166-173.
Weischedel, R.M. and Black, J.E. 1980 Responding Intelligently
to Unparsable Inputs. Am. J. Comp. Ling. 6(2): 97-109.
Weischedel, R.M. and Sondheimer, N.K. 1981 A Framework for
Processing Ill-Formed Input. Research Report. University of
Delaware.
Weischedel, R.M. and Sondheimer, N.K. 1982 An Improved
Heuristic for Ellipsis Processing. Proc. 20th Annual Meeting of
the ACL. Toronto, Canada: 85-88.
Wilks, Yorick 1975 An Intelligent Analyzer and Understander of
English. Comm. ACM 18(5): 264-274.
</reference>
<page confidence="0.868896">
160 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.150828">
<title confidence="0.853038">Parse Fitting and Prose a Hold on</title>
<author confidence="0.997326">K Jensen</author>
<author confidence="0.997326">G E Heidorn</author>
<author confidence="0.997326">L A Miller</author>
<author confidence="0.997326">Y Ravin</author>
<affiliation confidence="0.8983845">Computer Sciences IBM Thomas J. Watson Research</affiliation>
<address confidence="0.409181">P.O. Box Yorktown Heights, New York 10598</address>
<abstract confidence="0.9804994">Processing syntactically ill-formed language is an important mission of the EPISTLE system. Ill-formed input is treated by this system in various ways. Misspellings are highlighted by a standard spelling checker; syntactic errors are detected and corrections are suggested; and stylistic infelicities are called to the user&apos;s attention. to the EPISTLE processing strategy is its technique of When the rules of a conventional syntactic grammar are unable to produce a parse for an input string, technique can be used to produce a reasonable that can serve as input to the remaining stages of processing. This paper first describes the fitting process and gives examples of ill-formed language situations where it is called into play. We then show how a fitted parse allows EPISTLE to carry on its text-critiquing mission where conventional grammars would fail either because of input problems or because of limitations in the grammars themselves. Some inherent difficulties of the fitting technique are also discussed. In addition, we explore how style critiquing relates to the handling of ill-formed input, and how a fitted parse can be used in style checking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>DECLI---NP PRON---What</author>
</authors>
<title>I---AVP ADV*----&amp;quot;exactly&amp;quot; I---VERB &amp;quot;does&amp;quot; I---NPI DET ADJ*----&amp;quot;that&amp;quot;</title>
<marker>PRON---What, </marker>
<rawString>DECLI---NP PRON*---&amp;quot;What&amp;quot; I---AVP ADV*----&amp;quot;exactly&amp;quot; I---VERB &amp;quot;does&amp;quot; I---NPI DET ADJ*----&amp;quot;that&amp;quot;</rawString>
</citation>
<citation valid="false">
<authors>
<author>GRAMMATICAL ERROR MISSING QUESTION MARK</author>
</authors>
<title>What exactly does that 15 months do.</title>
<publisher>CONSIDER:</publisher>
<marker>MARK, </marker>
<rawString>GRAMMATICAL ERROR: MISSING QUESTION MARK. What exactly does that 15 months do. CONSIDER:</rawString>
</citation>
<citation valid="false">
<title>What exactly does that 15 months do? Figure 18. Correct error diagnosis; satisfactory parse.</title>
<marker></marker>
<rawString>What exactly does that 15 months do? Figure 18. Correct error diagnosis; satisfactory parse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
<author>G E Heidorn</author>
<author>L A Miller</author>
<author>Y Ravin</author>
</authors>
<title>Parse Fitting and Prose Fixing References</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics, Volume 9, Numbers</journal>
<pages>3--4</pages>
<marker>Jensen, Heidorn, Miller, Ravin, 1983</marker>
<rawString>American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 159 K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing References</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
<author>G V Mouradian</author>
</authors>
<title>Flexible Parsing.</title>
<date>1981</date>
<journal>Am. J. Comp. Ling.</journal>
<volume>7</volume>
<issue>4</issue>
<pages>232--242</pages>
<contexts>
<context position="39182" citStr="Hayes and Mouradian (1981)" startWordPosition="6045" endWordPosition="6048">andle both cases of clearly ungrammatical structures, such as co-occurrence violations like subject/verb disagreement, and cases of perfectly acceptable but difficult constructions (ellipsis and conjunction). Weischedel and Sondheimer (1982) describe an improved ellipsis processor. No longer is ellipsis handled with relaxation techniques, but by predicting transformations of previous parsing paths that would allow for the matching of fragments with plausible contexts. This plan would be appropriate as a next step after the fitted parse, but it does not guarantee a parse for all elided inputs. Hayes and Mouradian (1981) also use the relaxation method. They achieve flexibility in their parser by relaxing consistency constraints (grammatical restrictions, like Kwasny and Sondheimer&apos;s co-occurrence violations) and also by relaxing ordering constraints. However, they are working with a restricted-domain semantic system and their approach, as they admit, &amp;quot;does not embody a solution for flexible parsing of natural language in general&amp;quot; (p. 236). The work of Wilks is heavily semantic and therefore quite different from EPISTLE, but his general philosophy meshes nicely with the philosophy of the fitted parse: &amp;quot;It is p</context>
</contexts>
<marker>Hayes, Mouradian, 1981</marker>
<rawString>Hayes, P.J. and Mouradian, G.V. 1981 Flexible Parsing. Am. J. Comp. Ling. 7(4): 232-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Natural Language Inputs to a Simulation Programming System.</title>
<date>1972</date>
<tech>Technical Report NPS-55HD72101A.</tech>
<location>Monterey, California: Naval Postgraduate School.</location>
<contexts>
<context position="2029" citStr="Heidorn 1972" startWordPosition="319" endWordPosition="320">used in style checking. Introduction In its current form, the EPISTLE system addresses the problems of grammar and style checking of texts written in ordinary English (letters, reports, and manuals, as opposed to novels, plays, and poems). It is this goal that involves us so intimately with the processing of ill-formed language. Grammar checking deals with such errors as disagreement in number between subject and verb; style checking calls attention to such infelicities as sentences that are too wordy or too complex. A standard spelling checker is also included. Our grammar is written in NLP (Heidorn 1972), an augmented phrase structure language which is currently implemented in LISP/370. At this time the EPISTLE grammar uses syntactic, but not semantic, information. Access to an on-line standard dictionary with about 130,000 entries, including part-of-speech and some other syntactic information (such as transitivity of 1 The work described here is a continuation of work first presented at the Conference on Applied Natural Language Processing in Santa Monica, California (Jensen and Heidorn 1983). verbs), makes the system&apos;s vocabulary essentially unlimited. We test and improve the grammar by reg</context>
</contexts>
<marker>Heidorn, 1972</marker>
<rawString>Heidorn, G.E. 1972 Natural Language Inputs to a Simulation Programming System. Technical Report NPS-55HD72101A. Monterey, California: Naval Postgraduate School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Experience with an Easily Computed Metric for Ranking Alternative Parses.</title>
<date>1982</date>
<booktitle>Proc. 20th Annual Meeting of the ACL.</booktitle>
<pages>82--84</pages>
<location>Toronto, Canada:</location>
<contexts>
<context position="4311" citStr="Heidorn 1982" startWordPosition="677" endWordPosition="678">rovided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/83/030147-14$03.00 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 147 K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing In EPISTLE, (a) the core grammar consists at present of a set of about 300 syntax rules; (b) ambiguity is resolved by using a metric that ranks alternative parses (Heidorn 1982); and (c) parse failure is handled by the fitting procedure described here. In using the terms core grammar and periphery, we are consciously echoing recent work in generative grammar, but we are applying the terms in a somewhat different way. Core grammar, in current linguistic theory, suggests the notion of a set of very general rules that define universal properties of human language and effectively set limits on the types of grammars any particular language may have; periphery phenomena are those constructions that are peculiar to particular languages and that require rules beyond what the</context>
</contexts>
<marker>Heidorn, 1982</marker>
<rawString>Heidorn, G.E. 1982 Experience with an Easily Computed Metric for Ranking Alternative Parses. Proc. 20th Annual Meeting of the ACL. Toronto, Canada: 82-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
<author>K Jensen</author>
<author>L A Miller</author>
<author>R J Byrd</author>
<author>M S Chodorow</author>
</authors>
<title>The EPISTLE Text-Critiquing System.</title>
<date>1982</date>
<journal>IBM Systems Journal</journal>
<volume>21</volume>
<issue>3</issue>
<pages>305--326</pages>
<contexts>
<context position="5928" citStr="Heidorn et al. (1982)" startWordPosition="936" endWordPosition="939">cture rules and with attribute-value records, which are manipulated by the rules. When NLP is used to parse natural language text, the records describe constituents, and the rules put these constituents together to form ever larger constituent (or record) structures. Records contain all the computational and linguistic information associated with words, with larger constituents, and with the parse formation. At this time our grammar is sentence-based; we do not, for instance, create record structures to describe paragraphs. Details of the EPISTLE system and of its core grammar may be found in Heidorn et al. (1982). An earlier overview of the system is presented in Miller et al. (1981). A close examination of parse trees produced by the core grammar will often reveal branch attachments that are not quite right: for example, semantically incongruous prepositional phrase attachments. In line with our pragmatic parsing philosophy, our core grammar is designed to produce unique approximate parses. (Recall that we currently have access only to syntactic and morphological information about constituents.) In the cases where semantic or pragmatic information is needed before a proper attachment can be made, rat</context>
<context position="21434" citStr="Heidorn et al. (1982)" startWordPosition="3407" endWordPosition="3410">t them, producing the parse tree that would be appropriate if the correction were made. Figure 7 illustrates this ability. This numberdisagreement phenomenon is fairly common in current American English. The tensed verb seems to want to agree with its closest noun neighbor (in this sentence, &amp;quot;forms...are&amp;quot;) rather than with its subject NP (&amp;quot;a carbon copy...is&amp;quot;). A prescriptive rule still insists that subject and verb should agree in number, however, and the EPISTLE grammar provides a correction for such cases. Note that in the last line of Figure 7 the word &amp;quot;are&amp;quot; has been changed to &amp;quot;is&amp;quot;. (See Heidorn et al. (1982) for a more thorough discussion of the error correction technique.) And now the fitting procedure allows us to continue this work even under wildly ungrammatical conditions. Figure 8 is a fitted parse for the string in Figure 1, with a number disagreement error introduced into the fragment. 152 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing DECLI---NPI DET ADJ*----&amp;quot;A&amp;quot; NP NOUN*---&amp;quot;carbon&amp;quot; NOUN*---&amp;quot;copy&amp;quot; PPI PREP----&amp;quot;of&amp;quot; DET ADJ*----&amp;quot;the&amp;quot; NPI NOUN*---&amp;quot;Workman&amp;quot; POSS----&amp;quot;&apos;s&amp;quot; </context>
<context position="37498" citStr="Heidorn et al. 1982" startWordPosition="5787" endWordPosition="5790"> DO. CONSIDER: WHAT exactly does that 15 months DOES. GRAMMATICAL ERROR: PREMODIFIER-NOUN NUMBER DISAGREEMENT. What exactly does THAT...MONTHS do. CONSIDER: What exactly does THAT...MONTH do. THE COMBINED GRAMMATICAL CORRECTIONS ARE: What exactly does that 15 month does. Figure 15. Two faulty error diagnoses; inappropriate parse. ill-formed input should be handled by relaxation techniques, that is, by relaxing restrictions in the grammar rules in some principled way. This is undoubtedly a useful strategy — one which EPISTLE makes use of, in fact, in its rules for detecting grammatical errors (Heidorn et al. 1982). However, it is questionable whether such a strategy can ultimately succeed in the face of the overwhelming (for all practical purposes, infinite) variety of ill-formedness with which we are faced when we set out to parse truly unrestricted natural language input. If all ill-formedness is rule-based (Weischedel and Sondheimer 1981, p. 3), it can only be by some very loose definition of the term rule, such as that which might apply to the fitting algorithm described here. Thus Weischedel and Black (1980) suggest three techniques for responding intelligently to unparsable inputs: (a) using pres</context>
</contexts>
<marker>Heidorn, Jensen, Miller, Byrd, Chodorow, 1982</marker>
<rawString>Heidorn, G.E.; Jensen, K.; Miller, L.A.; Byrd, R.J.; and Chodorow, M.S. 1982 The EPISTLE Text-Critiquing System. IBM Systems Journal 21(3): 305-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
<author>G E Heidorn</author>
</authors>
<title>The Fitted Parse: 100% Parsing Capability in a Syntactic Grammar of English.</title>
<date>1983</date>
<booktitle>Proc. Conf. on Applied Natural Language Processing.</booktitle>
<pages>93--98</pages>
<location>Santa Monica, California:</location>
<contexts>
<context position="2528" citStr="Jensen and Heidorn 1983" startWordPosition="391" endWordPosition="394">that are too wordy or too complex. A standard spelling checker is also included. Our grammar is written in NLP (Heidorn 1972), an augmented phrase structure language which is currently implemented in LISP/370. At this time the EPISTLE grammar uses syntactic, but not semantic, information. Access to an on-line standard dictionary with about 130,000 entries, including part-of-speech and some other syntactic information (such as transitivity of 1 The work described here is a continuation of work first presented at the Conference on Applied Natural Language Processing in Santa Monica, California (Jensen and Heidorn 1983). verbs), makes the system&apos;s vocabulary essentially unlimited. We test and improve the grammar by regularly running it on a data base of 2254 sentences from 411 actual business letters. Most of these sentences are rather complicated; the longest contains 63 words, and the average length is 19.2 words. Since the subset of English represented in business documents is very large, we need a very comprehensive grammar and a robust parser. We take a heuristic approach and consider that a natural language parser can be divided into three parts: (a) a set of rules, called the core grammar, that precis</context>
</contexts>
<marker>Jensen, Heidorn, 1983</marker>
<rawString>Jensen, K. and Heidorn, G.E. 1983 The Fitted Parse: 100% Parsing Capability in a Syntactic Grammar of English. Proc. Conf. on Applied Natural Language Processing. Santa Monica, California: 93-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Kwasny</author>
<author>N K Sondheimer</author>
</authors>
<title>Relaxation Techniques for Parsing Ill-Formed Input.</title>
<date>1981</date>
<journal>Am. J. Comp. Ling.</journal>
<volume>7</volume>
<issue>2</issue>
<pages>99--108</pages>
<contexts>
<context position="38488" citStr="Kwasny and Sondheimer (1981)" startWordPosition="5941" endWordPosition="5944">me very loose definition of the term rule, such as that which might apply to the fitting algorithm described here. Thus Weischedel and Black (1980) suggest three techniques for responding intelligently to unparsable inputs: (a) using presuppositions to determine user assumptions; this course is not available to a syntactic grammar like EPISTLE&apos;s; (b) using relaxation techniques; (c) supplying the user with information about the point where the parse is blocked; this would require an interactive environment, which would not be possible for every type of natural language processing application. Kwasny and Sondheimer (1981) are strong proponents of relaxation techniques, which they use to handle both cases of clearly ungrammatical structures, such as co-occurrence violations like subject/verb disagreement, and cases of perfectly acceptable but difficult constructions (ellipsis and conjunction). Weischedel and Sondheimer (1982) describe an improved ellipsis processor. No longer is ellipsis handled with relaxation techniques, but by predicting transformations of previous parsing paths that would allow for the matching of fragments with plausible contexts. This plan would be appropriate as a next step after the fit</context>
</contexts>
<marker>Kwasny, Sondheimer, 1981</marker>
<rawString>Kwasny, S.C. and Sondheimer, N.K. 1981 Relaxation Techniques for Parsing Ill-Formed Input. Am. J. Comp. Ling. 7(2): 99-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lasnik</author>
<author>R Freidin</author>
</authors>
<title>Core Grammar, Case Theory, and Markedness.</title>
<date>1981</date>
<booktitle>Proc. 1979 GLOW Conf.</booktitle>
<location>Pisa, Italy.</location>
<contexts>
<context position="4963" citStr="Lasnik and Freidin 1981" startWordPosition="780" endWordPosition="784">andled by the fitting procedure described here. In using the terms core grammar and periphery, we are consciously echoing recent work in generative grammar, but we are applying the terms in a somewhat different way. Core grammar, in current linguistic theory, suggests the notion of a set of very general rules that define universal properties of human language and effectively set limits on the types of grammars any particular language may have; periphery phenomena are those constructions that are peculiar to particular languages and that require rules beyond what the core grammar will provide (Lasnik and Freidin 1981). Our current work is not concerned with the meta-rules of a Universal Grammar. But we have found that a distinction between core and periphery is useful even within a grammar of a particular language — in this case, English. Parsing in EPISTLE EPISTLE&apos;s parser is written in the NLP programming language, which works with augmented phrase structure rules and with attribute-value records, which are manipulated by the rules. When NLP is used to parse natural language text, the records describe constituents, and the rules put these constituents together to form ever larger constituent (or record) </context>
</contexts>
<marker>Lasnik, Freidin, 1981</marker>
<rawString>Lasnik, H. and Freidin, R. 1981 Core Grammar, Case Theory, and Markedness. Proc. 1979 GLOW Conf. Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Miller</author>
<author>G E Heidorn</author>
<author>K Jensen</author>
</authors>
<title>Text-Critiquing with the EPISTLE System: An Authors&apos;s Aid to Better Syntax.</title>
<date>1981</date>
<booktitle>AFIPS Conf. Proc.,</booktitle>
<volume>50</volume>
<pages>649--655</pages>
<location>Arlington, Virginia:</location>
<contexts>
<context position="6000" citStr="Miller et al. (1981)" startWordPosition="949" endWordPosition="952">e rules. When NLP is used to parse natural language text, the records describe constituents, and the rules put these constituents together to form ever larger constituent (or record) structures. Records contain all the computational and linguistic information associated with words, with larger constituents, and with the parse formation. At this time our grammar is sentence-based; we do not, for instance, create record structures to describe paragraphs. Details of the EPISTLE system and of its core grammar may be found in Heidorn et al. (1982). An earlier overview of the system is presented in Miller et al. (1981). A close examination of parse trees produced by the core grammar will often reveal branch attachments that are not quite right: for example, semantically incongruous prepositional phrase attachments. In line with our pragmatic parsing philosophy, our core grammar is designed to produce unique approximate parses. (Recall that we currently have access only to syntactic and morphological information about constituents.) In the cases where semantic or pragmatic information is needed before a proper attachment can be made, rather than produce a confusion of multiple parses we force the grammar to </context>
</contexts>
<marker>Miller, Heidorn, Jensen, 1981</marker>
<rawString>Miller, L.A.; Heidorn, G.E; and Jensen, K. 1981 Text-Critiquing with the EPISTLE System: An Authors&apos;s Aid to Better Syntax. AFIPS Conf. Proc., Vol. 50. Arlington, Virginia: 649-655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Slocum</author>
</authors>
<title>A Status Report on the LRC Machine Translation System.</title>
<date>1983</date>
<booktitle>Proc. Conf. on Applied Natural Language Processing.</booktitle>
<pages>166--173</pages>
<location>Santa Monica, California:</location>
<contexts>
<context position="36151" citStr="Slocum (1983" startWordPosition="5594" endWordPosition="5595"> proper rule to ignore number agreement in particular cases that involve a small subset of quantified English time words. Admittedly, the fix would be more pleasing if it were part of a larger scheme for understanding meaning and context. But the correction moves in the right direction, and certainly does not prevent future processing with a more intelligent semantic component. This situation clearly illustrates how error detection results in the addition of finer constraints on the core grammar. Related Work The parsing approach closest in spirit to our fitting procedure is that described in Slocum (1983, p. 170): the LRC Machine Translation System uses a &amp;quot;shortest path&amp;quot; technique to construct a &amp;quot;phrasal analysis&amp;quot; of ungrammatical input. With this analysis, phrases can be translated separately, even in the absence of a total sentence parse. Aside from Slocum&apos;s work, most of the reports in this field suggest that unparsable or American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 157 K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing DECLI---NPI PRON*---&amp;quot;What&amp;quot; VPI AVP ADV*----&amp;quot;exactly&amp;quot; VERB----&amp;quot;does&amp;quot; DET ADJ*----&amp;quot;that&amp;quot; QUANT ADJ</context>
</contexts>
<marker>Slocum, 1983</marker>
<rawString>Slocum, Jonathan. 1983 A Status Report on the LRC Machine Translation System. Proc. Conf. on Applied Natural Language Processing. Santa Monica, California: 166-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Weischedel</author>
<author>J E Black</author>
</authors>
<title>Responding Intelligently to Unparsable Inputs.</title>
<date>1980</date>
<journal>Am. J. Comp. Ling.</journal>
<volume>6</volume>
<issue>2</issue>
<pages>97--109</pages>
<contexts>
<context position="38007" citStr="Weischedel and Black (1980)" startWordPosition="5871" endWordPosition="5874">trategy — one which EPISTLE makes use of, in fact, in its rules for detecting grammatical errors (Heidorn et al. 1982). However, it is questionable whether such a strategy can ultimately succeed in the face of the overwhelming (for all practical purposes, infinite) variety of ill-formedness with which we are faced when we set out to parse truly unrestricted natural language input. If all ill-formedness is rule-based (Weischedel and Sondheimer 1981, p. 3), it can only be by some very loose definition of the term rule, such as that which might apply to the fitting algorithm described here. Thus Weischedel and Black (1980) suggest three techniques for responding intelligently to unparsable inputs: (a) using presuppositions to determine user assumptions; this course is not available to a syntactic grammar like EPISTLE&apos;s; (b) using relaxation techniques; (c) supplying the user with information about the point where the parse is blocked; this would require an interactive environment, which would not be possible for every type of natural language processing application. Kwasny and Sondheimer (1981) are strong proponents of relaxation techniques, which they use to handle both cases of clearly ungrammatical structure</context>
</contexts>
<marker>Weischedel, Black, 1980</marker>
<rawString>Weischedel, R.M. and Black, J.E. 1980 Responding Intelligently to Unparsable Inputs. Am. J. Comp. Ling. 6(2): 97-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Weischedel</author>
<author>N K Sondheimer</author>
</authors>
<title>A Framework for Processing Ill-Formed Input. Research Report.</title>
<date>1981</date>
<institution>University of Delaware.</institution>
<contexts>
<context position="37831" citStr="Weischedel and Sondheimer 1981" startWordPosition="5838" endWordPosition="5841">e parse. ill-formed input should be handled by relaxation techniques, that is, by relaxing restrictions in the grammar rules in some principled way. This is undoubtedly a useful strategy — one which EPISTLE makes use of, in fact, in its rules for detecting grammatical errors (Heidorn et al. 1982). However, it is questionable whether such a strategy can ultimately succeed in the face of the overwhelming (for all practical purposes, infinite) variety of ill-formedness with which we are faced when we set out to parse truly unrestricted natural language input. If all ill-formedness is rule-based (Weischedel and Sondheimer 1981, p. 3), it can only be by some very loose definition of the term rule, such as that which might apply to the fitting algorithm described here. Thus Weischedel and Black (1980) suggest three techniques for responding intelligently to unparsable inputs: (a) using presuppositions to determine user assumptions; this course is not available to a syntactic grammar like EPISTLE&apos;s; (b) using relaxation techniques; (c) supplying the user with information about the point where the parse is blocked; this would require an interactive environment, which would not be possible for every type of natural lang</context>
</contexts>
<marker>Weischedel, Sondheimer, 1981</marker>
<rawString>Weischedel, R.M. and Sondheimer, N.K. 1981 A Framework for Processing Ill-Formed Input. Research Report. University of Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Weischedel</author>
<author>N K Sondheimer</author>
</authors>
<title>An Improved Heuristic for Ellipsis Processing.</title>
<date>1982</date>
<booktitle>Proc. 20th Annual Meeting of the ACL.</booktitle>
<pages>85--88</pages>
<location>Toronto, Canada:</location>
<contexts>
<context position="38797" citStr="Weischedel and Sondheimer (1982)" startWordPosition="5984" endWordPosition="5987">able to a syntactic grammar like EPISTLE&apos;s; (b) using relaxation techniques; (c) supplying the user with information about the point where the parse is blocked; this would require an interactive environment, which would not be possible for every type of natural language processing application. Kwasny and Sondheimer (1981) are strong proponents of relaxation techniques, which they use to handle both cases of clearly ungrammatical structures, such as co-occurrence violations like subject/verb disagreement, and cases of perfectly acceptable but difficult constructions (ellipsis and conjunction). Weischedel and Sondheimer (1982) describe an improved ellipsis processor. No longer is ellipsis handled with relaxation techniques, but by predicting transformations of previous parsing paths that would allow for the matching of fragments with plausible contexts. This plan would be appropriate as a next step after the fitted parse, but it does not guarantee a parse for all elided inputs. Hayes and Mouradian (1981) also use the relaxation method. They achieve flexibility in their parser by relaxing consistency constraints (grammatical restrictions, like Kwasny and Sondheimer&apos;s co-occurrence violations) and also by relaxing or</context>
</contexts>
<marker>Weischedel, Sondheimer, 1982</marker>
<rawString>Weischedel, R.M. and Sondheimer, N.K. 1982 An Improved Heuristic for Ellipsis Processing. Proc. 20th Annual Meeting of the ACL. Toronto, Canada: 85-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>An Intelligent Analyzer and</title>
<date>1975</date>
<journal>Understander of English. Comm. ACM</journal>
<volume>18</volume>
<issue>5</issue>
<pages>264--274</pages>
<contexts>
<context position="39898" citStr="Wilks 1975" startWordPosition="6160" endWordPosition="6161">nts (grammatical restrictions, like Kwasny and Sondheimer&apos;s co-occurrence violations) and also by relaxing ordering constraints. However, they are working with a restricted-domain semantic system and their approach, as they admit, &amp;quot;does not embody a solution for flexible parsing of natural language in general&amp;quot; (p. 236). The work of Wilks is heavily semantic and therefore quite different from EPISTLE, but his general philosophy meshes nicely with the philosophy of the fitted parse: &amp;quot;It is proper to prefer the normal ... but it would be absurd ... not to accept the abnormal if it is described&amp;quot; (Wilks 1975, p. 267). 158 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 K. Jensen, G.E. Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixing DECLI---NP PRON*---&amp;quot;What&amp;quot; I---AVP ADV*____&amp;quot; exactly&amp;quot; I---VERB &amp;quot;does&amp;quot; I---AVPNPI DET ADJ*----&amp;quot;that&amp;quot; I---QUANT ADJ*----&amp;quot;15&amp;quot; I---NOUN* &amp;quot;months&amp;quot; I---VERB*----&amp;quot;do&amp;quot; I---PUNC &amp;quot;.. GRAMMATICAL ERROR: MISSING QUESTION MARK. What exactly does that 15 months do. CONSIDER: What exactly does that 15 months do? GRAMMATICAL ERROR: PREMODIFIER-NOUN NUMBER DISAGREEMENT. What exactly does THAT...MONTHS do. CONSIDER: What exactly </context>
</contexts>
<marker>Wilks, 1975</marker>
<rawString>Wilks, Yorick 1975 An Intelligent Analyzer and Understander of English. Comm. ACM 18(5): 264-274.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>