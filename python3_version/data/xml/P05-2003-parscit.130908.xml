<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000436">
<title confidence="0.981937">
An Extensive Empirical Study of Collocation Extraction Methods
</title>
<author confidence="0.982181">
Pavel Pecina
</author>
<affiliation confidence="0.883304">
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
</affiliation>
<email confidence="0.994666">
pecina@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.993825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994904">
This paper presents a status quo of an
ongoing research study of collocations –
an essential linguistic phenomenon hav-
ing a wide spectrum of applications in
the field of natural language processing.
The core of the work is an empirical eval-
uation of a comprehensive list of auto-
matic collocation extraction methods us-
ing precision-recall measures and a pro-
posal of a new approach integrating mul-
tiple basic methods and statistical classi-
fication. We demonstrate that combining
multiple independent techniques leads to
a significant performance improvement in
comparisonwith individualbasic methods.
</bodyText>
<sectionHeader confidence="0.981526" genericHeader="categories and subject descriptors">
1 Introduction and motivation
</sectionHeader>
<bodyText confidence="0.999959125">
Natural language cannot be simply reduced to lex-
icon and syntax. The fact that individual words
cannot be combined freely or randomly is common
for most natural languages. The ability of a word
to combine with other words can be expressed ei-
ther intensionally or extensionally. The former case
refers to valency. Instances of the latter case are
called collocations (ˇCermák and Holub, 1982). The
term collocation has several other definitions but
none of them is widely accepted. Most attempts
are based on a characteristic property of colloca-
tions: non-compositionality. Choueka (1988) de-
fines a collocational expression as “a syntactic and
semantic unit whose exact and unambiguous mean-
ing or connotation cannot be derived directly from
the meaning or connotation of its components”.
</bodyText>
<page confidence="0.985302">
13
</page>
<bodyText confidence="0.999965305555556">
The term collocation has both linguistic and lexi-
cographic character. It covers a wide range of lexical
phenomena, such as phrasal verbs, light verb com-
pounds, idioms, stock phrases, technological ex-
pressions, and proper names. Collocations are of
high importance for many applications in the field
of NLP. The most desirable ones are machine trans-
lation, word sense disambiguation, language genera-
tion, and information retrieval. The recent availabil-
ity of large amounts of textual data has attracted in-
terest in automatic collocation extraction from text.
In the last thirty years a number of different methods
employing various association measures have been
proposed. Overview of the most widely used tech-
niques is given e.g. in (Manning and Schütze, 1999)
or (Pearce, 2002). Several researches also attempted
to compare existing methods and suggested different
evaluation schemes, e.g Kita (1994) or Evert (2001).
A comprehensive study of statistical aspects of word
cooccurrences can be found in (Evert, 2004).
In this paper we present a compendium of 84
methods for automatic collocation extraction. They
came from different research areas and some of them
have not been used for this purpose yet. A brief
overview of these methods is followed by their com-
parative evaluation against manually annotated data
by the means of precision and recall measures. In
the end we propose a statistical classification method
for combining multiple methods and demonstrate a
substantial performance improvement.
In our research we focus on two-word (bigram)
collocations, mainly for the reason that experiments
with longer expressions would require processing of
much larger amounts of data and limited scalability
of some methods to high order n-grams. The exper-
iments are performed on Czech data.
</bodyText>
<note confidence="0.90695">
Proceedings of the ACL Student Research Workshop, pages 13–18,
</note>
<page confidence="0.47127">
Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics
</page>
<figure confidence="0.992739">
a=f(xy) b=f(xy) f(x∗)
�=f(.ty) d=f(xv) f(x∗)
f(∗y) f(∗v) iv
a)
b)
</figure>
<sectionHeader confidence="0.810957" genericHeader="method">
2 Collocation extraction
</sectionHeader>
<bodyText confidence="0.999878586956522">
Most methods for collocation extraction are based
on verification of typical collocation properties.
These properties are formally described by mathe-
matical formulas that determine the degree of as-
sociation between components of collocation. Such
formulas are called association measures and com-
pute an association score for each collocation candi-
date extracted from a corpus. The scores indicate a
chance of a candidate to be a collocation. They can
be used for ranking or for classification – by setting
a threshold. Finding such a threshold depends on the
intended application.
The most widely tested property of collocations is
non-compositionality: If words occur together more
often than by a chance, then this is the evidence that
they have a special function that is not simply ex-
plained as a result of their combination (Manning
and Schütze, 1999). We think of a corpus as a ran-
domly generated sequence of words that is viewed as
a sequence of word pairs. Occurrence frequencies
of these bigrams are extracted and kept in contin-
gency tables (Table 1a). Values from these tables are
used in several association measures that reflect how
much the word coocurrence is accidental. A list of
such measures is given in Table 2 and includes: es-
timation of bigram and unigram probabilities (rows
3–5), mutual information and derived measures (6–
11), statistical tests of independence (12–16), likeli-
hood measures (17–18), and various other heuristic
association measures and coefficients (19–57).
Another frequently tested property is taken di-
rectly from the definition that a collocation is a syn-
tactic and semantic unit. For each bigram occurring
in the corpus, information of its empirical context
(frequencies of open-class words occurring within
a specified context window) and left and right im-
mediate contexts (frequencies of words immediately
preceding or following the bigram) is extracted (Ta-
ble 1b). By determining the entropy of the im-
mediate contexts of a word sequence, the associa-
tion measures rank collocations according to the as-
sumption that they occur as units in a (information-
theoretically) noisy environment (Shimohata et al.,
1997) (58–62). By comparing empirical contexts of
a word sequence and its components, the associa-
tion measures rank collocations according to the as-
</bodyText>
<table confidence="0.95138825">
Cw empirical context of w
C. empirical context of xy
Csa left immediate context of xy
C,&apos;, right immediate context of xy
</table>
<tableCaption confidence="0.992011">
Table 1: a) A contingency table with observed frequencies and
marginal frequencies for a bigram xy; w� stands for any word
except w; * stands for any word; N is a total number of bi-
grams. The table cells are sometimes referred as fu. Statistical
tests of independence work with contingency tables of expected
frequencies �f(xy)=f(x∗)f(∗y)/iv. b) Different notions ofem-
pirical contexts.
</tableCaption>
<bodyText confidence="0.999526733333333">
sumption that semantically non-compositional ex-
pressions typically occur in different contexts than
their components (Zhai, 1997). Measures (63–76)
have information theory background and measures
(77–84) are adopted from the field of information
retrieval. Context association measures are mainly
used for extracting idioms.
Besides all the association measures described
above, we also take into account other recommended
measures (1–2) (Manning and Schütze, 1999) and
some basic linguistic characteristics used for filter-
ing non-collocations (85–87). This information can
be obtained automatically from morphological tag-
gers and syntactic parsers available with reasonably
high accuracy for many languages.
</bodyText>
<sectionHeader confidence="0.989615" genericHeader="method">
3 Empirical evaluation
</sectionHeader>
<bodyText confidence="0.999351619047619">
Evaluation of collocation extraction methods is a
complicated task. On one hand, different applica-
tions require different setting of association score
thresholds. On the other hand, methods give differ-
ent results within different ranges of their associa-
tion scores. We need a complex evaluation scheme
covering all demands. In such a case, Evert (2001)
and other authors suggest using precision and recall
measures on a full reference data or on n-best lists.
Data. All the presented experiments were per-
formed on morphologically and syntactically anno-
tated Czech text from the Prague Dependency Tree-
bank (PDT) (Hajiˇc et al., 2001). Dependency trees
were broken down into dependency bigrams consist-
ing of: lemmas and part-of-speech of the compo-
nents, and type of dependence between the compo-
nents.
For each bigram type we counted frequencies in
its contingency table, extracted empirical and imme-
diate contexts, and computed all the 84 association
measures from Table 2. We processed 81614 sen-
</bodyText>
<page confidence="0.997921">
14
</page>
<table confidence="0.959088375">
# Name Formula
1. Mean component offset 1
2. Variance component offset Pn
di
n i=1
1 `di− ¯d´2
Pn
n−1 i=1
</table>
<listItem confidence="0.820928727272727">
3. Joint probability P(xy)
4. Conditional probability P(y|x)
5. Reverse conditional prob. P(x|y)
?6. Pointwise mutual inform. log P (xy)
7. Mutual dependency (MD)
8. Log frequency biased MD
9. Normalized expectation
?10. Mutual expectation
11. Salience
12. Pearson’s X2 test
13. Fisher’s exact test
</listItem>
<figure confidence="0.996398615384615">
14. t test
15. z score
16. Poison significance measure
17. Log likelihood ratio
18. Squared log likelihood ratio
P (x∗)P (∗y)
log P (xy)2
P (x∗)P (∗y)
log P (xy)2 P(xy)
+log
P (x∗)P (∗y)
2f(xy)
f(x∗)+f(∗y)
2f(xy)
f(x∗)+f(∗y) ·P(xy)
P (xy)2
log logf
P (x∗)P (∗y) · (xy)
(fij −ˆfij )
PM ,
&apos; ˆfij
f(x∗)!f(¯x∗)!f(∗y)!f(∗¯y)!
N!f(xy)!f(x¯y)!f(¯xy)!f(¯x¯y)!
f(xy)− ˆf(xy)
√f(xy)(1−(f(xy)/N))
f(xy)− ˆf(xy)
√ ˆf(xy)(1−( ˆf(xy)/N))
f (xy)−f (xy) lio fNxy)+logf (xy)!
g
−2P fij
i,jfijlog ˆfij
logfij2
−2P i,j ˆfij
Association coefficients: a
19. Russel-Rao
20. Sokal-Michiner
?21. Rogers-Tanimoto
22. Hamann
23. Third Sokal-Sneath
24. Jaccard
?25. First Kulczynsky
26. Second Sokal-Sneath
27. Second Kulczynski
28. Fourth Sokal-Sneath
29. Odds ratio
30. Yulle’s w
?31. Yulle’s Q
32. Driver-Kroeber
33. Fifth Sokal-Sneath
34. Pearson
35. Baroni-Urbani
36. Braun-Blanquet
37. Simpson
38. Michael
39. Mountford
40. Fager
41. Unigram subtuples
42. Ucost
43. S cost
44. R cost
45. T combined cost
46. Phi
47. Kappa
48. J measure
a+b+c+d
a+d
a+b+c+d
a+d
a+2b+2c+d
(a+d)−(b+c)
a+b+c+d
b+c
a+d
a
a+b+c
a
b+c
a
a+2(b+c)
1 a a
2 ( a+b + a+c )
1aa d d
4 (a a+c +
+b + d+b + d+c )
ad
bc
√ad−√bc
√ad+√bc
addbc
+bc
a
√(a+b)(a+c)
ad
√(a+b)(a+c)(d+b)(d+c)
ad−bc
√(a+b) (a+c) (d+b) (d+c)
a+√ad
a+b+c+ ad
√
a
max(a+b,a+c)
a
min(a+b,a+c)
4(ad−bc)
</figure>
<table confidence="0.979294544">
(a+d)2+(b+c)2
2a
2bc+ab+ac
max(b,
a − 1
2 c)
√(a+b)(a+c)
log ad q 1
bc −3.29 a + 1b + 1 c+ 1d
log(1+ min(b,c)+a
max(b,c)+a )
min(b,c)
log(1+ a+1)−1 2
log(1+ a log(1+ a
a+b )·a+c )
V U xSxR
P (xy)−P (x∗)P (∗y)
√P (x∗)P (∗y)(1−P (x∗))(1−P (∗y))
P (xy)+P (¯x¯y)−P (x∗)P (∗y)−P (¯x∗)P (∗¯y)
1−P (x∗)P (∗y)−P (¯x∗)P (∗¯y)
P (y|x) P(¯y|x)
max[P(xy)log P (∗y) +P (x¯y)log P (∗¯y) ,
P(xy)log P (x|y) P (¯x|y)
P (x∗) +P(¯xy)log P (¯x∗) ]
# Name Formula
49. Gini index max[P(x*)(P(y|x)2+P(¯y|x)2)−P(*y)2
50. Confidence +P(¯x*)(P(y|¯x)2+P(¯y|¯x)2)−P(*¯y)2,
51. Laplace P(*y)(P(x|y)2+P(¯x|y)2)−P(x*)2
52. Conviction +P(*¯y)(P(x|¯y)2+P(¯x|¯y)2)−P(¯x*)2]
53. Piatersky-Shapiro max[P(y|x), P(x|y)]
54. Certainity factor NP (xy)+1 NP (xy)+1
55. Added value (AV)
?56. Collective strength
57. Klosgen
max[ NP (x∗)+2 � NP (∗y)+2 ]
P (x∗)P (∗y)&apos; P (¯x∗)P (∗y)]
max[ P (x¯y) P(¯xy)
P(xy)−P(x*)P(*y)
P (y|x)−P (∗y) P (x|y)−P (x∗)
max[ 1−P (∗y) � 1−P (x∗) ]
max[P(y|x)−P(*y), P(x|y)−P(x*)]
P (xy)+P (¯x¯y)
P (x∗)P (y)+P (¯x∗)P (∗y) ·
1−P (x∗)P (∗y)−P (¯x∗)P (∗y)
1−P (xy)−P (¯x¯y)
p P(xy) ·AV
Context measures: −Pw P(w|Cxy) logP(w|Cxy)
?58. Context entropy −P w P(w|Clxy) logP(w|Clxy)
59. Left context entropy −P w P(w|Crxy) logP(w|Crxy)
60. Right context entropy P(x*) logP(x*)
?61. Left context divergence −PwP(w|Clxy) logP(w|Clxy)
62. Right context divergence P(*y)logP(*y)
63. Cross entropy −PwP(w|Crxy) logP(w|Crxy)
64. Reverse cross entropy −P wP(w|Cx) log P(w|Cy)
65. Intersection measure −P wP(w|Cy) log P(w|Cx)
66. Euclidean norm 2|Cx∩Cy |
67. Cosine norm
68. L1 norm
69. Confusion probability
70. Reverse confusion prob.
?71. Jensen-Shannon diverg.
72. Cosine of pointwise MI
?73. KL divergence
?74. Reverse KL divergence
75. Skew divergence
76. Reverse skew divergence
77. Phrase word coocurrence
78. Word association
Cosine context similarity:
?79. in boolean vector space
80. in tf vector space
81. in tf·idf vector space
Dice context similarity:
?82. in boolean vector space
?83. in tf vector space
?84. in tf·idf vector space
|Cx|+|Cy |
qP w(P (w|Cx)−P (w|Cy))2
Pw P (w|Cx)P (w|Cy)
Pw P (w|Cx)2·Pw P (w|Cy)2
P w |P(w|Cx)−P(w|Cy)|
P (x|Cw)P (y|Cw)P (w)
Pw P (x∗)
P (y|Cw)P (x|Cw)P (w)
Pw P (∗y)
12[D(p(w|Cx)||12(p(w|Cx)+p(w|Cy)))
+D(p(w|Cy)||12 (p(w|Cx)+p(w|Cy)))]
Pw MI(w,x)MI(w,y)
√Pw MI(w,x)2·√Pw MI(w,y)2
P(w|Cx) log P (w|Cx)
P w P (w|Cy )
P(wCy) log P(w|Cy )
Pw |P (w|Cx)
D(p(w|Cx)||α(w|Cy)+(1−α)p(w|Cx))
D(p(w|Cy)||αp(w|Cx)+(1−α)p(w|Cy))
1f(x|Cxy) f(y|Cxy)
2( f(xy) + f(xy) )
1 f(x|Cy )−f(xy) f(y|Cx)−f(xy)
2 ( +
f(xy) f(xy) )
1
2 (cos(cx,cxy)+cos(cy,cxy))
P xiyi
cz=(zi); cos(cx,cy)= √P xi2·√P yi2
zi = δ(f(wi|Cz))
zi = f(wi|Cz)
N
df(wi)=|�x:wi�Cx}|
zi =f(wi|Cz)·
df(wi);
1
2 (dice(cx,cxy)+dice(cy,cxy))
2 P xiyi
dice(cx,cy)=
cz=(zi); P xi2+P yi2
zi = δ(f(wi|Cz))
zi = f(wi|Cz)
N
df(wi)=|�x:wi�Cx}|
zi =f(wi|Cz)·
df(wi);
Linguistic features: {Adjective:Noun, Noun:Noun, Noun:Verb, ... }
?85. Part of speech {Attribute, Object, Subject, ... }
?86. Dependency type {�, &apos;`}
87. Dependency structure
</table>
<tableCaption confidence="0.9939665">
Table 2: Association measures and linguistic features used in bigram collocation extraction methods. * denotes those selected by
the attribute selection method discussed in Section 4. References can be found at the end of the paper.
</tableCaption>
<page confidence="0.997877">
15
</page>
<bodyText confidence="0.999960722222223">
tences with 1255 590 words and obtained a total of
202171 different dependency bigrams.
Krenn (2000) argues that collocation extraction
methods should be evaluated against a reference set
of collocations manually extracted from the full can-
didate data from a corpus. However, we reduced the
full candidate data from PDT to 21597 bigram by
filtering out any bigrams which occurred 5 or less
times in the data and thus we obtained a reference
dataset which fulfills requirements of a sufficient
size and a minimal frequency of observations which
is needed for the assumption of normal distribution
required by some methods.
We manually processed the entire reference data
set and extracted bigrams that were considered to be
collocations. At this point we applied part-of-speech
filtering: First, we identified POSpatterns that never
form a collocation. Second, all dependency bigrams
having such a POS pattern were removed from the
reference data and a final reference set of 8 904 bi-
grams was created. We no longer consider bigrams
with such patterns to be collocation candidates.
This data set contained 2 649 items considered to
be collocations. The a priori probability of a bi-
gram to be a collocation was 29.75 %. A strati-
fied one-third subsample of this data was selected
as test data and used for evaluation and testing pur-
poses in this work. The rest was taken apart and used
as training data in later experiments.
Evaluation metrics. Since we manually anno-
tated the entire reference data set we could use the
suggested precision and recall measures (and their
harmonic mean F-measure). A collocation extrac-
tion method using any association measure with a
given threshold can be considered a classifier and
the measures can be computed in the following way:
</bodyText>
<equation confidence="0.80293">
Precision =
</equation>
<construct confidence="0.74493875">
# correctly classified collocations
# total predicted as collocations
Recall = # correctly classified collocations
# total collocations
</construct>
<bodyText confidence="0.999951857142857">
The higher these scores, the better the classifier is.
By changing the threshold we can tune the clas-
sifier performance and “trade” recall for precision.
Therefore, collocation extraction methods can be
thoroughly compared by comparing their precision-
-recall curves: The closer the curve to the top right
corner, the better the method is.
</bodyText>
<figure confidence="0.9466072">
100
90
80
Precision (%) 60
30
</figure>
<figureCaption confidence="0.999927">
Figure 1: Precision-recall curves for selected assoc. measures.
</figureCaption>
<bodyText confidence="0.99971075">
Results. Presenting individual results for all of
the 84 association measures is not possible in a paper
of this length. Therefore, we present precision-recall
graphs only for the best methods from each group
mentioned in Section 2; see Figure 1. The baseline
system that classifies bigrams randomly, operates
with a precision of 29.75 %. The overall best re-
sult was achieved by Pointwise mutual information:
30 % recall with 85.5 % precision (F-measure 44.4),
60 % recall with 78.4 % precision (F-measure 68.0),
and 90 % recall with 62.5 % precision (F-measure
73.8).
</bodyText>
<sectionHeader confidence="0.972058" genericHeader="method">
4 Statistical classification
</sectionHeader>
<bodyText confidence="0.99986945">
In the previous section we mentioned that collo-
cation extraction is a classification problem. Each
method classifies instances of the candidate data set
according to the values of an association score. Now
we have several association scores for each candi-
date bigram and want to combine them together to
achieve better performance. A motivating example
is depicted in Figure 3: Association scores of Point-
wise mutual information and Cosine context simi-
larity are independent enough to be linearly com-
bined to provide better results. Considering all as-
sociation measures, we deal with a problem of high-
dimensional classification into two classes.
In our case, each bigram x is described by the
attribute vector x=(x1, ... , x87) consisting of lin-
guistic features and association scores from Table 2.
Now we look for a function assigning each bigram
one class : f(x) —*{collocation, non-collocation}.
The result of this approach is similar to setting a
threshold of the association score in methods us-
</bodyText>
<figure confidence="0.953759833333333">
0 20 40 60 80 100
Pointwise mutual information
Pearson’s test
Mountford
Kappa
Left context divergence
Context intersection measure
Cosine context similarity in boolean VS
baseline = 29.75 %
Recall (%)
16
Pointwise mutual information
</figure>
<figureCaption confidence="0.998769">
Figure 2: Data visualization in two dimensions. The dashed line
denotes a linear discriminant obtained by logistic linear regres-
sion. By moving this boundary we can tune the classifier output
</figureCaption>
<bodyText confidence="0.985399923076923">
(a 5 % stratified sample of the test data is displayed).
ing one association measure, which is not very use-
full for our purpose. Some classification meth-
ods, however, output also the predicted probability
P(x is collocation) that can be considered a regular
association measure as described above. Thus, the
classification method can be also tuned by changing
a threshold of this probability and can be compared
with other methods by the same means of precision
and recall.
One of the basic classification methods that gives
a predicted probability is Logistic linear regression.
The model defines the predicted probability as:
</bodyText>
<equation confidence="0.932558">
1 + exp)30+)31x1...+)3nxn
</equation>
<bodyText confidence="0.999752222222222">
where the coefficients Oi are obtained by the iter-
atively reweighted least squares (IRLS) algorithm
which solves the weighted least squares problem
at each iteration. Categorial attributes need to be
transformed to numeric dummy variables. It is also
recommended to normalize all numeric attributes to
have zero mean and unit variance.
We employed the datamining software Weka by
Witten and Frank (2000) in our experiments. As
training data we used a two-third subsample of the
reference data described above. The test data was
the same as in the evaluation of the basic methods.
By combining all the 87 attributes, we achieved
the results displayed in Table 3 and illustrated in Fig-
ure 3. At a recall level of 90 % the relative increase
in precision was 35.2 % and at a precision level of
90 % the relative increase in recall was impressive
242.3%.
</bodyText>
<figure confidence="0.483114">
Recall (%)
</figure>
<figureCaption confidence="0.997758">
Figure 3: Precision-recall curves of two classifiers based on
</figureCaption>
<bodyText confidence="0.959425421052632">
i) logistic linear regression on the full set of 87 attributes and
ii) on the selected subset with 17 attributes. The thin unlabeled
curves refer to the methods from the 17 selected attributes
Attribute selection. In the final step of our exper-
iments, we attempted to reduce the attribute space of
our data and thus obtain an attribute subset with the
same prediction ability. We employed a greedy step-
wise search method with attribute subset evaluation
via logistic regression implemented in Weka. It per-
forms a greedy search through the space of attribute
subsets and iteratively merges subsets that give the
best results until the performance is no longer im-
proved.
We ended up with a subset consisting of the fol-
lowing 17 attributes: (6, 10, 21, 25, 31, 56, 58, 61, 71,
73, 74, 79, 82, 83, 84, 85, 86) which are also marked in
Table 2. The overview of achieved results is shown
in Table 3 and precision-recall graphs of the selected
attributes and their combinations are in Figure 3.
</bodyText>
<sectionHeader confidence="0.994604" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999779">
We implemented 84 automatic collocation extrac-
tion methods and performed series of experiments
on morphologically and syntactically annotated
data. The methods were evaluated against a refer-
ence set of collocations manually extracted from the
</bodyText>
<table confidence="0.998426333333333">
Recall Precision
30 60 90 70 80 90
P. mutual information 85.5 78.4 62.5 78.0 56.0 16.3
Logistic regression-17 92.6 89.5 84.5 96.7 86.7 55.8
Absolute improvement 7.1 11.1 22.0 17.7 30.7 39.2
Relative improvement 8.3 14.2 35.2 23.9 54.8 242.3
</table>
<tableCaption confidence="0.994561666666667">
Table 3: Precision (the 3 left columns) and recall (the 3 right
columns) scores (in %) for the best individual method and linear
combination of the 17 selected ones.
</tableCaption>
<figure confidence="0.998833">
0.7 8.8 16.9
Cosine context similarity in boolean vector space
0.9
0.5
0.1
collocations
non-collocations
linear discriminant
0 20 40 60 80 100
Precision (%)
100
90
80
60
30
Logistic regression on all attributes
Logistic regression on 17 selected attributes
baseline = 29.75 %
exp)30+)31x1...+)3nxn
P(x is collocation) _
</figure>
<page confidence="0.994779">
17
</page>
<bodyText confidence="0.999980114285714">
same source. The best method (Pointwise mutual in-
formation) achieved 68.3 % recall with 73.0 % pre-
cision (F-measure 70.6) on this data. We proposed
to combine the association scores of each candidate
bigram and employed Logistic linear regression to
find a linear combination of the association scores
of all the basic methods. Thus we constructed a col-
location extraction method which achieved 80.8 %
recall with 84.8 % precision (F-measure 82.8). Fur-
thermore, we applied an attribute selection tech-
nique in order to lower the high dimensionality of
the classification problem and reduced the number
of regressors from 87 to 17 with comparable perfor-
mance. This result can be viewed as a kind of evalu-
ation of basic collocation extraction techniques. We
can obtain the smallest subset that still gives the best
result. The other measures therefore become unin-
teresting and need not be further processed and eval-
uated.
The reseach presented in this paper is in progress.
The list of collocation extraction methods and as-
sociation measures is far from complete. Our long
term goal is to collect, implement, and evaluate all
available methods suitable for this task, and release
the toolkit for public use.
In the future, we will focus especially on im-
proving quality of the training and testing data, em-
ploying other classification and attribute-selection
techniques, and performing experiments on English
data. A necessary part of the work will be a rigorous
theoretical study of all applied methods and appro-
priateness of their usage. Finally, we will attempt to
demonstrate contribution of collocations in selected
application areas, such as machine translation or in-
formation retrieval.
</bodyText>
<sectionHeader confidence="0.998371" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99070925">
This research has been supported by the Ministry
of Education of the Czech Republic, project MSM
0021620838. I would also like to thank my advisor,
Dr. Jan Hajiˇc, for his continued support.
</bodyText>
<sectionHeader confidence="0.99909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999095982758621">
Y. Choueka. 1988. Looking for needles in a haystack or lo-
cating interesting collocational expressions in large textual
databases. In Proceedings of the RIAO, pages 43–38.
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based models
of word cooccurrence probabilities. Machine Learning, 34.
T. E. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguistics,
19(1):61–74.
S. Evert and B. Krenn. 2001. Methods for the qualitative eval-
uation of lexical association measures. In Proceedings 39th
Annual Meeting of the Association for Computational Lin-
guistics, pages 188–195.
S. Evert. 2004. The Statistics of Word Cooccurrences: Word
Pairs and Collocations. Ph.D. thesis, University of Stuttgart.
J. Hajiˇc, E. Hajiˇcová, P. Pajas, J. Panevová, P. Sgall, and
B. Vidová-Hladká. 2001. Prague dependency treebank 1.0.
Published by LDC, University of Pennsylvania.
K. Kita, Y. Kato, T. Omoto, and Y. Yano. 1994. A comparative
study of automatic extraction of collocations from corpora:
Mutual information vs. cost criteria. Journal ofNatural Lan-
guage Processing, 1(1):21–33.
B. Krenn. 2000. Collocation Mining: Exploiting Corpora for
Collocation Idenfication and Representation. In Proceedings
ofKONVENS 2000.
L. Lee. 2001. On the effectiveness of the skew divergence
for statistical language analysis. Artificial Inteligence and
Statistics, pages 65–72.
C. D. Manning and H. Schütze. 1999. Foundations of Statis-
tical Natural Language Processing. The MIT Press, Cam-
bridge, Massachusetts.
D. Pearce. 2002. A comparative evaluation of collocation ex-
traction techniques. In Third International Conference on
language Resources and Evaluation, Las Palmas, Spain.
T. Pedersen. 1996. Fishing for exactness. In Proceedings of
the South Central SAS User’s Group Conference, pages 188–
200, Austin, TX.
S. Shimohata, T. Sugio, and J. Nagata. 1997. Retrieving col-
locations by co-occurrences and word order constraints. In
Proc. of the 35th Annual Meeting of the ACL and 8th Con-
ference of the EACL, pages 476–81, Madrid. Spain.
P. Tan, V. Kumar, and J. Srivastava. 2002. Selecting the right
interestingness measure for association patterns. In Proceed-
ings of the Eight A CM SIGKDD International Conference
on Knowledge Discovery and Data Mining.
A. Thanopoulos, N. Fakotakis, and G. Kokkinakis. 2002. Com-
parative evaluation of collocation extraction metrics. In 3rd
International Conference on Language Resources and Eval-
uation, volume 2, pages 620–625, Las Palmas, Spain.
F. ˇCermák and J. Holub. 1982. Syntagmatika a paradigmatika
ˇcesk eho slova: Valence a kolokabilita. Státní pedagogické
nakladatelství, Praha.
I. H. Witten and E. Frank. 2000. Data Mining: Practical
machine learning tools with Java implementations. Morgan
Kaufmann, San Francisco.
C. Zhai. 1997. Exploiting context to identify lexical atoms
– A statistical view of linguistic context. In International
and Interdisciplinary Conference on Modelling and Using
Context (CONTEXT-97).
</reference>
<page confidence="0.999272">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.721852">
<title confidence="0.999946">An Extensive Empirical Study of Collocation Extraction Methods</title>
<author confidence="0.999835">Pavel Pecina</author>
<affiliation confidence="0.8787055">Institute of Formal and Applied Linguistics Charles University, Prague, Czech Republic</affiliation>
<email confidence="0.975962">pecina@ufal.mff.cuni.cz</email>
<abstract confidence="0.9984101875">paper presents a quo an ongoing research study of collocations – an essential linguistic phenomenon having a wide spectrum of applications in the field of natural language processing. The core of the work is an empirical evaluation of a comprehensive list of automatic collocation extraction methods using precision-recall measures and a proposal of a new approach integrating multiple basic methods and statistical classification. We demonstrate that combining multiple independent techniques leads to a significant performance improvement in comparisonwith individualbasic methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Choueka</author>
</authors>
<title>Looking for needles in a haystack or locating interesting collocational expressions in large textual databases.</title>
<date>1988</date>
<booktitle>In Proceedings of the RIAO,</booktitle>
<pages>43--38</pages>
<contexts>
<context position="1412" citStr="Choueka (1988)" startWordPosition="207" endWordPosition="208">ntroduction and motivation Natural language cannot be simply reduced to lexicon and syntax. The fact that individual words cannot be combined freely or randomly is common for most natural languages. The ability of a word to combine with other words can be expressed either intensionally or extensionally. The former case refers to valency. Instances of the latter case are called collocations (ˇCermák and Holub, 1982). The term collocation has several other definitions but none of them is widely accepted. Most attempts are based on a characteristic property of collocations: non-compositionality. Choueka (1988) defines a collocational expression as “a syntactic and semantic unit whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components”. 13 The term collocation has both linguistic and lexicographic character. It covers a wide range of lexical phenomena, such as phrasal verbs, light verb compounds, idioms, stock phrases, technological expressions, and proper names. Collocations are of high importance for many applications in the field of NLP. The most desirable ones are machine translation, word sense disambiguation, language gener</context>
</contexts>
<marker>Choueka, 1988</marker>
<rawString>Y. Choueka. 1988. Looking for needles in a haystack or locating interesting collocational expressions in large textual databases. In Proceedings of the RIAO, pages 43–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T E Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<marker>Dunning, 1993</marker>
<rawString>T. E. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Evert</author>
<author>B Krenn</author>
</authors>
<title>Methods for the qualitative evaluation of lexical association measures.</title>
<date>2001</date>
<booktitle>In Proceedings 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>188--195</pages>
<marker>Evert, Krenn, 2001</marker>
<rawString>S. Evert and B. Krenn. 2001. Methods for the qualitative evaluation of lexical association measures. In Proceedings 39th Annual Meeting of the Association for Computational Linguistics, pages 188–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences: Word Pairs and Collocations.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="2630" citStr="Evert, 2004" startWordPosition="394" endWordPosition="395"> and information retrieval. The recent availability of large amounts of textual data has attracted interest in automatic collocation extraction from text. In the last thirty years a number of different methods employing various association measures have been proposed. Overview of the most widely used techniques is given e.g. in (Manning and Schütze, 1999) or (Pearce, 2002). Several researches also attempted to compare existing methods and suggested different evaluation schemes, e.g Kita (1994) or Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in (Evert, 2004). In this paper we present a compendium of 84 methods for automatic collocation extraction. They came from different research areas and some of them have not been used for this purpose yet. A brief overview of these methods is followed by their comparative evaluation against manually annotated data by the means of precision and recall measures. In the end we propose a statistical classification method for combining multiple methods and demonstrate a substantial performance improvement. In our research we focus on two-word (bigram) collocations, mainly for the reason that experiments with longe</context>
</contexts>
<marker>Evert, 2004</marker>
<rawString>S. Evert. 2004. The Statistics of Word Cooccurrences: Word Pairs and Collocations. Ph.D. thesis, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>E Hajiˇcová</author>
<author>P Pajas</author>
<author>J Panevová</author>
<author>P Sgall</author>
<author>B Vidová-Hladká</author>
</authors>
<title>Prague dependency treebank 1.0. Published by LDC,</title>
<date>2001</date>
<institution>University of Pennsylvania.</institution>
<marker>Hajiˇc, Hajiˇcová, Pajas, Panevová, Sgall, Vidová-Hladká, 2001</marker>
<rawString>J. Hajiˇc, E. Hajiˇcová, P. Pajas, J. Panevová, P. Sgall, and B. Vidová-Hladká. 2001. Prague dependency treebank 1.0. Published by LDC, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kita</author>
<author>Y Kato</author>
<author>T Omoto</author>
<author>Y Yano</author>
</authors>
<title>A comparative study of automatic extraction of collocations from corpora: Mutual information vs. cost criteria.</title>
<date>1994</date>
<journal>Journal ofNatural Language Processing,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Kita, Kato, Omoto, Yano, 1994</marker>
<rawString>K. Kita, Y. Kato, T. Omoto, and Y. Yano. 1994. A comparative study of automatic extraction of collocations from corpora: Mutual information vs. cost criteria. Journal ofNatural Language Processing, 1(1):21–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Krenn</author>
</authors>
<title>Collocation Mining: Exploiting Corpora for Collocation Idenfication and Representation.</title>
<date>2000</date>
<booktitle>In Proceedings ofKONVENS</booktitle>
<contexts>
<context position="13232" citStr="Krenn (2000)" startWordPosition="2044" endWordPosition="2045">,cy)= cz=(zi); P xi2+P yi2 zi = δ(f(wi|Cz)) zi = f(wi|Cz) N df(wi)=|�x:wi�Cx}| zi =f(wi|Cz)· df(wi); Linguistic features: {Adjective:Noun, Noun:Noun, Noun:Verb, ... } ?85. Part of speech {Attribute, Object, Subject, ... } ?86. Dependency type {�, &apos;`} 87. Dependency structure Table 2: Association measures and linguistic features used in bigram collocation extraction methods. * denotes those selected by the attribute selection method discussed in Section 4. References can be found at the end of the paper. 15 tences with 1255 590 words and obtained a total of 202171 different dependency bigrams. Krenn (2000) argues that collocation extraction methods should be evaluated against a reference set of collocations manually extracted from the full candidate data from a corpus. However, we reduced the full candidate data from PDT to 21597 bigram by filtering out any bigrams which occurred 5 or less times in the data and thus we obtained a reference dataset which fulfills requirements of a sufficient size and a minimal frequency of observations which is needed for the assumption of normal distribution required by some methods. We manually processed the entire reference data set and extracted bigrams that</context>
</contexts>
<marker>Krenn, 2000</marker>
<rawString>B. Krenn. 2000. Collocation Mining: Exploiting Corpora for Collocation Idenfication and Representation. In Proceedings ofKONVENS 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>On the effectiveness of the skew divergence for statistical language analysis. Artificial Inteligence and Statistics,</title>
<date>2001</date>
<pages>65--72</pages>
<marker>Lee, 2001</marker>
<rawString>L. Lee. 2001. On the effectiveness of the skew divergence for statistical language analysis. Artificial Inteligence and Statistics, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="2375" citStr="Manning and Schütze, 1999" startWordPosition="355" endWordPosition="358">bs, light verb compounds, idioms, stock phrases, technological expressions, and proper names. Collocations are of high importance for many applications in the field of NLP. The most desirable ones are machine translation, word sense disambiguation, language generation, and information retrieval. The recent availability of large amounts of textual data has attracted interest in automatic collocation extraction from text. In the last thirty years a number of different methods employing various association measures have been proposed. Overview of the most widely used techniques is given e.g. in (Manning and Schütze, 1999) or (Pearce, 2002). Several researches also attempted to compare existing methods and suggested different evaluation schemes, e.g Kita (1994) or Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in (Evert, 2004). In this paper we present a compendium of 84 methods for automatic collocation extraction. They came from different research areas and some of them have not been used for this purpose yet. A brief overview of these methods is followed by their comparative evaluation against manually annotated data by the means of precision and recall measures</context>
<context position="4499" citStr="Manning and Schütze, 1999" startWordPosition="681" endWordPosition="684">ollocation. Such formulas are called association measures and compute an association score for each collocation candidate extracted from a corpus. The scores indicate a chance of a candidate to be a collocation. They can be used for ranking or for classification – by setting a threshold. Finding such a threshold depends on the intended application. The most widely tested property of collocations is non-compositionality: If words occur together more often than by a chance, then this is the evidence that they have a special function that is not simply explained as a result of their combination (Manning and Schütze, 1999). We think of a corpus as a randomly generated sequence of words that is viewed as a sequence of word pairs. Occurrence frequencies of these bigrams are extracted and kept in contingency tables (Table 1a). Values from these tables are used in several association measures that reflect how much the word coocurrence is accidental. A list of such measures is given in Table 2 and includes: estimation of bigram and unigram probabilities (rows 3–5), mutual information and derived measures (6– 11), statistical tests of independence (12–16), likelihood measures (17–18), and various other heuristic asso</context>
<context position="6914" citStr="Manning and Schütze, 1999" startWordPosition="1054" endWordPosition="1057">d as fu. Statistical tests of independence work with contingency tables of expected frequencies �f(xy)=f(x∗)f(∗y)/iv. b) Different notions ofempirical contexts. sumption that semantically non-compositional expressions typically occur in different contexts than their components (Zhai, 1997). Measures (63–76) have information theory background and measures (77–84) are adopted from the field of information retrieval. Context association measures are mainly used for extracting idioms. Besides all the association measures described above, we also take into account other recommended measures (1–2) (Manning and Schütze, 1999) and some basic linguistic characteristics used for filtering non-collocations (85–87). This information can be obtained automatically from morphological taggers and syntactic parsers available with reasonably high accuracy for many languages. 3 Empirical evaluation Evaluation of collocation extraction methods is a complicated task. On one hand, different applications require different setting of association score thresholds. On the other hand, methods give different results within different ranges of their association scores. We need a complex evaluation scheme covering all demands. In such a</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>C. D. Manning and H. Schütze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pearce</author>
</authors>
<title>A comparative evaluation of collocation extraction techniques.</title>
<date>2002</date>
<booktitle>In Third International Conference on language Resources and Evaluation,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="2393" citStr="Pearce, 2002" startWordPosition="360" endWordPosition="361">s, stock phrases, technological expressions, and proper names. Collocations are of high importance for many applications in the field of NLP. The most desirable ones are machine translation, word sense disambiguation, language generation, and information retrieval. The recent availability of large amounts of textual data has attracted interest in automatic collocation extraction from text. In the last thirty years a number of different methods employing various association measures have been proposed. Overview of the most widely used techniques is given e.g. in (Manning and Schütze, 1999) or (Pearce, 2002). Several researches also attempted to compare existing methods and suggested different evaluation schemes, e.g Kita (1994) or Evert (2001). A comprehensive study of statistical aspects of word cooccurrences can be found in (Evert, 2004). In this paper we present a compendium of 84 methods for automatic collocation extraction. They came from different research areas and some of them have not been used for this purpose yet. A brief overview of these methods is followed by their comparative evaluation against manually annotated data by the means of precision and recall measures. In the end we pr</context>
</contexts>
<marker>Pearce, 2002</marker>
<rawString>D. Pearce. 2002. A comparative evaluation of collocation extraction techniques. In Third International Conference on language Resources and Evaluation, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Fishing for exactness.</title>
<date>1996</date>
<booktitle>In Proceedings of the South Central SAS User’s Group Conference,</booktitle>
<pages>188--200</pages>
<location>Austin, TX.</location>
<marker>Pedersen, 1996</marker>
<rawString>T. Pedersen. 1996. Fishing for exactness. In Proceedings of the South Central SAS User’s Group Conference, pages 188– 200, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shimohata</author>
<author>T Sugio</author>
<author>J Nagata</author>
</authors>
<title>Retrieving collocations by co-occurrences and word order constraints.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Meeting of the ACL and 8th Conference of the EACL,</booktitle>
<pages>476--81</pages>
<location>Madrid.</location>
<contexts>
<context position="5799" citStr="Shimohata et al., 1997" startWordPosition="886" endWordPosition="889"> is taken directly from the definition that a collocation is a syntactic and semantic unit. For each bigram occurring in the corpus, information of its empirical context (frequencies of open-class words occurring within a specified context window) and left and right immediate contexts (frequencies of words immediately preceding or following the bigram) is extracted (Table 1b). By determining the entropy of the immediate contexts of a word sequence, the association measures rank collocations according to the assumption that they occur as units in a (informationtheoretically) noisy environment (Shimohata et al., 1997) (58–62). By comparing empirical contexts of a word sequence and its components, the association measures rank collocations according to the asCw empirical context of w C. empirical context of xy Csa left immediate context of xy C,&apos;, right immediate context of xy Table 1: a) A contingency table with observed frequencies and marginal frequencies for a bigram xy; w� stands for any word except w; * stands for any word; N is a total number of bigrams. The table cells are sometimes referred as fu. Statistical tests of independence work with contingency tables of expected frequencies �f(xy)=f(x∗)f(∗</context>
</contexts>
<marker>Shimohata, Sugio, Nagata, 1997</marker>
<rawString>S. Shimohata, T. Sugio, and J. Nagata. 1997. Retrieving collocations by co-occurrences and word order constraints. In Proc. of the 35th Annual Meeting of the ACL and 8th Conference of the EACL, pages 476–81, Madrid. Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tan</author>
<author>V Kumar</author>
<author>J Srivastava</author>
</authors>
<title>Selecting the right interestingness measure for association patterns.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eight A CM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<marker>Tan, Kumar, Srivastava, 2002</marker>
<rawString>P. Tan, V. Kumar, and J. Srivastava. 2002. Selecting the right interestingness measure for association patterns. In Proceedings of the Eight A CM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Thanopoulos</author>
<author>N Fakotakis</author>
<author>G Kokkinakis</author>
</authors>
<title>Comparative evaluation of collocation extraction metrics.</title>
<date>2002</date>
<booktitle>In 3rd International Conference on Language Resources and Evaluation,</booktitle>
<volume>2</volume>
<pages>620--625</pages>
<location>Las Palmas,</location>
<marker>Thanopoulos, Fakotakis, Kokkinakis, 2002</marker>
<rawString>A. Thanopoulos, N. Fakotakis, and G. Kokkinakis. 2002. Comparative evaluation of collocation extraction metrics. In 3rd International Conference on Language Resources and Evaluation, volume 2, pages 620–625, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F ˇCermák</author>
<author>J Holub</author>
</authors>
<title>Syntagmatika a paradigmatika ˇcesk eho slova: Valence a kolokabilita. Státní pedagogické nakladatelství,</title>
<date>1982</date>
<location>Praha.</location>
<marker>ˇCermák, Holub, 1982</marker>
<rawString>F. ˇCermák and J. Holub. 1982. Syntagmatika a paradigmatika ˇcesk eho slova: Valence a kolokabilita. Státní pedagogické nakladatelství, Praha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools with Java implementations.</title>
<date>2000</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="18557" citStr="Witten and Frank (2000)" startWordPosition="2890" endWordPosition="2893">the same means of precision and recall. One of the basic classification methods that gives a predicted probability is Logistic linear regression. The model defines the predicted probability as: 1 + exp)30+)31x1...+)3nxn where the coefficients Oi are obtained by the iteratively reweighted least squares (IRLS) algorithm which solves the weighted least squares problem at each iteration. Categorial attributes need to be transformed to numeric dummy variables. It is also recommended to normalize all numeric attributes to have zero mean and unit variance. We employed the datamining software Weka by Witten and Frank (2000) in our experiments. As training data we used a two-third subsample of the reference data described above. The test data was the same as in the evaluation of the basic methods. By combining all the 87 attributes, we achieved the results displayed in Table 3 and illustrated in Figure 3. At a recall level of 90 % the relative increase in precision was 35.2 % and at a precision level of 90 % the relative increase in recall was impressive 242.3%. Recall (%) Figure 3: Precision-recall curves of two classifiers based on i) logistic linear regression on the full set of 87 attributes and ii) on the se</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>I. H. Witten and E. Frank. 2000. Data Mining: Practical machine learning tools with Java implementations. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
</authors>
<title>Exploiting context to identify lexical atoms – A statistical view of linguistic context.</title>
<date>1997</date>
<booktitle>In International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97).</booktitle>
<contexts>
<context position="6578" citStr="Zhai, 1997" startWordPosition="1010" endWordPosition="1011"> C. empirical context of xy Csa left immediate context of xy C,&apos;, right immediate context of xy Table 1: a) A contingency table with observed frequencies and marginal frequencies for a bigram xy; w� stands for any word except w; * stands for any word; N is a total number of bigrams. The table cells are sometimes referred as fu. Statistical tests of independence work with contingency tables of expected frequencies �f(xy)=f(x∗)f(∗y)/iv. b) Different notions ofempirical contexts. sumption that semantically non-compositional expressions typically occur in different contexts than their components (Zhai, 1997). Measures (63–76) have information theory background and measures (77–84) are adopted from the field of information retrieval. Context association measures are mainly used for extracting idioms. Besides all the association measures described above, we also take into account other recommended measures (1–2) (Manning and Schütze, 1999) and some basic linguistic characteristics used for filtering non-collocations (85–87). This information can be obtained automatically from morphological taggers and syntactic parsers available with reasonably high accuracy for many languages. 3 Empirical evaluati</context>
</contexts>
<marker>Zhai, 1997</marker>
<rawString>C. Zhai. 1997. Exploiting context to identify lexical atoms – A statistical view of linguistic context. In International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>