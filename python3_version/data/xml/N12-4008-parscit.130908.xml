<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051278">
<title confidence="0.997590333333333">
T8: Predicting Structures in NLP:
Constrained Conditional Models and Integer
Linear Programming NLP
</title>
<author confidence="0.956411">
Dan Goldwasser, Vivek Srikumar, Dan Roth
</author>
<sectionHeader confidence="0.931639" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999847780487805">
Making decisions in natural language processing problems often involves assigning
values to sets of interdependent variables where the expressive dependency structure
can influence, or even dictate what assignments are possible. This setting includes a
broad range of structured prediction problems such as semantic role labeling, named
entity and relation recognition, co-reference resolution, dependency parsing and
semantic parsing. The setting is also appropriate for cases that may require making
global decisions that involve multiple components, possibly pre-designed or pre-
learned, as in summarization, paraphrasing, textual entailment and question answering.
In all these cases, it is natural to formulate the decision problem as a constrained
optimization problem, with an objective function that is composed of learned models,
subject to domain or problem specific constraints.
Constrained Conditional Models (CCM) formulation of NLP problems (also known as:
Integer Linear Programming for NLP) is a learning and inference framework that
augments the learning of conditional (probabilistic or discriminative) models with
declarative constraints (written, for example, using a first-order representation). The key
advantage of the CCM formulation is its support for making decisions in an expressive
output space while maintaining modularity and tractability of training and inference. In
most applications of this framework in NLP, following [Roth &amp; Yih, CoNLL&apos;04], integer
linear programming (ILP) has been used as the inference framework, although other
algorithms can be used.
This framework has attracted much attention within the NLP community over the last
few years, with multiple papers in all the recent major conferences. Formulating
structured prediction as a constrained optimization problem over the output of learned
models has several advantages. It allows the incorporation of problem specific global
constraints using a first order language â€“ thus freeing the developer from (much of
the) low level feature engineering â€“ and guarantees exact inference. Importantly, it
provides also the freedom of decoupling model generation (learning) from the
constrained inference stage, often simplifying the learning stage as well as the
engineering aspect of building an NLP system, while improving the quality of the
solutions. These advantages and the availability of off-the-shelf solvers have led to a
large variety of NLP tasks being formulated within it, including semantic role labeling,
syntactic parsing, co-reference resolution, summarization, transliteration and joint
information extraction.
The goal of this tutorial is to introduce the framework of Constrained Conditional Models
to the broader ACL community, motivate it as a generic framework for structured
inference in global NLP decision problems, present some of the key theoretical and
practical issues involved in using CCMs and survey some of the existing applications of
it as a way to promote further development of the framework and additional
applications. The tutorial will be useful for senior and junior researchers who are
interested in structured prediction and global decision problems in NLP, providing a
concise overview of recent perspectives and research results.
</bodyText>
<sectionHeader confidence="0.819433" genericHeader="keywords">
OUTLINE
</sectionHeader>
<bodyText confidence="0.9998114">
After briefly motivating and introducing the general framework, the main part of the
tutorial is a methodological presentation of some of the key computational issues
studied within CCMs that we will present by looking at case studies published in the
NLP literature. In the last part of the tutorial, we will discuss engineering issues that
arise in using CCMs.
</bodyText>
<sectionHeader confidence="0.929633" genericHeader="introduction">
1. Motivation
</sectionHeader>
<bodyText confidence="0.99981825">
We will begin by introducing structured prediction with various NLP examples.
We will motivate the framework of Constrained Conditional Models using
examples from sequential inference, sentence compression and semantic role
labeling. [30 min]
</bodyText>
<sectionHeader confidence="0.832004" genericHeader="method">
2. Examples of Existing CCM Applications
</sectionHeader>
<bodyText confidence="0.9999752">
We will present several applications that use CCMs â€“ including co-reference
resolution, sentence compression and information extraction and use these to
explain several of the key advantages the framework offers. In this context, we
will discuss several ways in which expressive constraints can be introduced into
an application. [30 min]
</bodyText>
<sectionHeader confidence="0.663549" genericHeader="method">
3. Modeling CCMs: Inference methods and Constraints
</sectionHeader>
<bodyText confidence="0.999798444444444">
We will present and discuss several possibilities for modeling inference in CCMs.
We will discuss ways to model problems as structured prediction problems and
the use of hard and soft constraints to represent prior knowledge. We will look at
ways in which Constrained Conditional Models can be used to augment
probabilistic models with declarative knowledge based constraints and how these
support expressive global decisions. We will also mention various possibilities for
performing the inference, from commercial Integer Linear Programming
packages to search techniques to Lagrangian relaxation approximation methods.
[30 min]
</bodyText>
<sectionHeader confidence="0.946665" genericHeader="method">
4. Training Paradigms
</sectionHeader>
<bodyText confidence="0.999973571428571">
The objective function used by CCMs can be decomposed and learned in several
ways, ranging from â€œstandardâ€ structured learning, i.e., a complete joint
training of the model along with the constraints, to a complete decoupling
between the learning and the inference stage. We will present the advantages
and disadvantages offered by different training paradigms and provide theoretical
and experimental understanding. In this part, we will also compare CCMs to
other approaches studied in the literature. [30 min]
</bodyText>
<sectionHeader confidence="0.943353" genericHeader="method">
5. Beyond Supervised Learning
</sectionHeader>
<bodyText confidence="0.999769333333333">
In the standard supervised setting, we require a corpus that is annotated with the
structures of interest, which is expensive and often impractical. In this part of the
tutorial, we will show how to go beyond the supervised setting using CCMs,
which provide ways to use declarative constraints to guide supervised and semi-
supervised training in the presence of partial and indirect supervision. We will
also present recent advances in training structured predictors indirectly using a
data for a companion binary task. We will review several successful applications
of these methods in diverse tasks such as information extraction and textual
entailment. [30 min]
</bodyText>
<sectionHeader confidence="0.881553" genericHeader="method">
6. Developing CCMs Applications
</sectionHeader>
<bodyText confidence="0.998966">
We will present a â€œcookbook approachâ€ for developing applications within
the CCM framework and discuss templates for possible applications. [30 min]
</bodyText>
<sectionHeader confidence="0.962134" genericHeader="method">
BIOS
</sectionHeader>
<subsectionHeader confidence="0.709533">
Dan Goldwasser
</subsectionHeader>
<bodyText confidence="0.907338555555555">
Computer Science Department, University of Illinois at Urbana-Champaign, IL, 61801
Email: goldwas1--AT--illinois.edu
Phone: +(217) 333-2584
Dan Goldwasser is a PhD candidate in the Department of Computer Science at the
University of Illinois at Urbana-Champaign. He is the recipient of the 2010 C.L. and
Jane Liu Award for research promise. He published several papers in natural language
processing, machine learning and semantic interpretation. His research work studies
the role that an external context, such as the real world or a simulated world, plays in
semantic interpretation and learning protocols.
</bodyText>
<subsectionHeader confidence="0.554201">
Vivek Srikumar
</subsectionHeader>
<bodyText confidence="0.872628375">
Computer Science Department, University of Illinois at Urbana-Champaign, IL. 61801
Email: vsrikum2--AT--illinois.edu
Phone: +(217) 333-2584
Vivek Srikumar is a Ph.D. candidate at the University of Illinois, Urbana-Champaign. He
has worked on Machine Learning in the context of Natural Language Processing and
has published papers in several conferences. His research deals with applying structure
learning and prediction formalized as Constrained Conditional Models to semantic role
labeling and its extensions.
</bodyText>
<subsectionHeader confidence="0.771383">
Dan Roth
</subsectionHeader>
<bodyText confidence="0.967001125">
Computer Science Department, University of Illinois at Urbana-Champaign, IL, 61801
Email: danr--AT--cs.uiuc.edu
Phone: +(217) 244-7068
Dan Roth is a Professor in the Department of Computer Science at the University of
Illinois at Urbana-Champaign and the Beckman Institute of Advanced Science and
Technology (UIUC) and a University of Illinois Scholar. He is a fellow of AAAI and the
ACM. Roth has published broadly in machine learning, natural language processing,
knowledge representation and reasoning and received several paper, teaching and
research awards. He has developed several machine learning based natural language
processing systems that are widely used in the computational linguistics community and
in industry and has presented invited talks and tutorials in several major conferences.
Dan Roth has written the first paper on formulating global NLP decisions as ILP
problems with his student Scott Yih, presented in CoNLL&apos;04, and since then has worked
on further developing Constrained Conditional Models, on learning and inference issues
within this framework and on applying it to several NLP problems, including Semantic
Role Labeling, Information and Relation Extraction and Transliteration.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.010899">
<title confidence="0.992583">T8: Predicting Structures in NLP: Constrained Conditional Models and Integer Programming NLP</title>
<author confidence="0.998578">Dan Goldwasser</author>
<author confidence="0.998578">Vivek Srikumar</author>
<author confidence="0.998578">Dan Roth</author>
<abstract confidence="0.998489943181818">Making decisions in natural language processing problems often involves assigning values to sets of interdependent variables where the expressive dependency structure can influence, or even dictate what assignments are possible. This setting includes a broad range of structured prediction problems such as semantic role labeling, named entity and relation recognition, co-reference resolution, dependency parsing and semantic parsing. The setting is also appropriate for cases that may require making global decisions that involve multiple components, possibly pre-designed or prelearned, as in summarization, paraphrasing, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain or problem specific constraints. Constrained Conditional Models (CCM) formulation of NLP problems (also known as: Integer Linear Programming for NLP) is a learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints (written, for example, using a first-order representation). The key advantage of the CCM formulation is its support for making decisions in an expressive output space while maintaining modularity and tractability of training and inference. In most applications of this framework in NLP, following [Roth &amp; Yih, CoNLL&apos;04], integer linear programming (ILP) has been used as the inference framework, although other algorithms can be used. This framework has attracted much attention within the NLP community over the last few years, with multiple papers in all the recent major conferences. Formulating structured prediction as a constrained optimization problem over the output of learned models has several advantages. It allows the incorporation of problem specific global constraints using a first order language â€“ thus freeing the developer from (much of the) low level feature engineering â€“ and guarantees exact inference. Importantly, it provides also the freedom of decoupling model generation (learning) from the constrained inference stage, often simplifying the learning stage as well as the engineering aspect of building an NLP system, while improving the quality of the solutions. These advantages and the availability of off-the-shelf solvers have led to a large variety of NLP tasks being formulated within it, including semantic role labeling, syntactic parsing, co-reference resolution, summarization, transliteration and joint information extraction. The goal of this tutorial is to introduce the framework of Constrained Conditional Models to the broader ACL community, motivate it as a generic framework for structured inference in global NLP decision problems, present some of the key theoretical and practical issues involved in using CCMs and survey some of the existing applications of it as a way to promote further development of the framework and additional applications. The tutorial will be useful for senior and junior researchers who are interested in structured prediction and global decision problems in NLP, providing a concise overview of recent perspectives and research results. After briefly motivating and introducing the general framework, the main part of the tutorial is a methodological presentation of some of the key computational issues studied within CCMs that we will present by looking at case studies published in the NLP literature. In the last part of the tutorial, we will discuss engineering issues that arise in using CCMs. 1. Motivation We will begin by introducing structured prediction with various NLP examples. We will motivate the framework of Constrained Conditional Models using examples from sequential inference, sentence compression and semantic role labeling. [30 min] 2. Examples of Existing CCM Applications We will present several applications that use CCMs â€“ including co-reference resolution, sentence compression and information extraction and use these to explain several of the key advantages the framework offers. In this context, we will discuss several ways in which expressive constraints can be introduced into an application. [30 min] 3. Modeling CCMs: Inference methods and Constraints We will present and discuss several possibilities for modeling inference in CCMs. We will discuss ways to model problems as structured prediction problems and the use of hard and soft constraints to represent prior knowledge. We will look at ways in which Constrained Conditional Models can be used to augment probabilistic models with declarative knowledge based constraints and how these support expressive global decisions. We will also mention various possibilities for performing the inference, from commercial Integer Linear Programming packages to search techniques to Lagrangian relaxation approximation methods. [30 min] 4. Training Paradigms The objective function used by CCMs can be decomposed and learned in several ways, ranging from â€œstandardâ€ structured learning, i.e., a complete joint training of the model along with the constraints, to a complete decoupling between the learning and the inference stage. We will present the advantages and disadvantages offered by different training paradigms and provide theoretical and experimental understanding. In this part, we will also compare CCMs to other approaches studied in the literature. [30 min] 5. Beyond Supervised Learning In the standard supervised setting, we require a corpus that is annotated with the structures of interest, which is expensive and often impractical. In this part of the tutorial, we will show how to go beyond the supervised setting using CCMs, which provide ways to use declarative constraints to guide supervised and semisupervised training in the presence of partial and indirect supervision. We will also present recent advances in training structured predictors indirectly using a data for a companion binary task. We will review several successful applications of these methods in diverse tasks such as information extraction and textual entailment. [30 min] 6. Developing CCMs Applications We will present a â€œcookbook approachâ€ for developing applications within the CCM framework and discuss templates for possible applications. [30 min]</abstract>
<author confidence="0.993985">Dan Goldwasser</author>
<address confidence="0.988747">Computer Science Department, University of Illinois at Urbana-Champaign, IL, 61801</address>
<email confidence="0.999948">goldwas1--AT--illinois.edu</email>
<phone confidence="0.998398">217) 333-2584</phone>
<author confidence="0.741163">Dan Goldwasser is a PhD candidate in the Department of Computer Science at the</author>
<affiliation confidence="0.842699">University of Illinois at Urbana-Champaign. He is the recipient of the 2010 C.L. and</affiliation>
<abstract confidence="0.8937295">Jane Liu Award for research promise. He published several papers in natural language processing, machine learning and semantic interpretation. His research work studies the role that an external context, such as the real world or a simulated world, plays in semantic interpretation and learning protocols.</abstract>
<author confidence="0.422854">Vivek Srikumar</author>
<address confidence="0.916207">Computer Science Department, University of Illinois at Urbana-Champaign, IL. 61801</address>
<email confidence="0.999921">vsrikum2--AT--illinois.edu</email>
<phone confidence="0.998284">217) 333-2584</phone>
<author confidence="0.666427">He</author>
<abstract confidence="0.9776295">has worked on Machine Learning in the context of Natural Language Processing and has published papers in several conferences. His research deals with applying structure learning and prediction formalized as Constrained Conditional Models to semantic role labeling and its extensions.</abstract>
<author confidence="0.986487">Dan Roth</author>
<address confidence="0.956134">Computer Science Department, University of Illinois at Urbana-Champaign, IL, 61801</address>
<email confidence="0.999991">danr--AT--cs.uiuc.edu</email>
<phone confidence="0.987055">217) 244-7068</phone>
<note confidence="0.650365">Dan Roth is a Professor in the Department of Computer Science at the University of Illinois at Urbana-Champaign and the Beckman Institute of Advanced Science and Technology (UIUC) and a University of Illinois Scholar. He is a fellow of AAAI and the</note>
<abstract confidence="0.971858666666667">ACM. Roth has published broadly in machine learning, natural language processing, knowledge representation and reasoning and received several paper, teaching and research awards. He has developed several machine learning based natural language processing systems that are widely used in the computational linguistics community and in industry and has presented invited talks and tutorials in several major conferences. Dan Roth has written the first paper on formulating global NLP decisions as ILP problems with his student Scott Yih, presented in CoNLL&apos;04, and since then has worked on further developing Constrained Conditional Models, on learning and inference issues within this framework and on applying it to several NLP problems, including Semantic</abstract>
<intro confidence="0.430205">Role Labeling, Information and Relation Extraction and Transliteration.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>