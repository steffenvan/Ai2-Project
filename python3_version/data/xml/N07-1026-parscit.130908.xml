<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000052">
<title confidence="0.99872">
Data-Driven Graph Construction for Semi-Supervised Graph-Based
Learning in NLP
</title>
<author confidence="0.966918">
Andrei Alexandrescu
</author>
<affiliation confidence="0.9940815">
Dept. of Computer Science and Engineering
University of Washington
</affiliation>
<address confidence="0.986457">
Seattle, WA, 98195
</address>
<email confidence="0.999428">
andrei@cs.washington.edu
</email>
<author confidence="0.934278">
Katrin Kirchhoff
</author>
<affiliation confidence="0.990854">
Dept. of Electrical Engineering
University of Washington
</affiliation>
<address confidence="0.949141">
Seattle, WA 98195
</address>
<email confidence="0.999494">
katrin@ee.washington.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999822952380952">
Graph-based semi-supervised learning has
recently emerged as a promising approach
to data-sparse learning problems in natu-
ral language processing. All graph-based
algorithms rely on a graph that jointly rep-
resents labeled and unlabeled data points.
The problem of how to best construct this
graph remains largely unsolved. In this
paper we introduce a data-driven method
that optimizes the representation of the
initial feature space for graph construc-
tion by means of a supervised classifier.
We apply this technique in the frame-
work of label propagation and evaluate
it on two different classification tasks, a
multi-class lexicon acquisition task and a
word sense disambiguation task. Signifi-
cant improvements are demonstrated over
both label propagation using conventional
graph construction and state-of-the-art su-
pervised classifiers.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999670906976745">
Natural Language Processing (NLP) applications
benefit from the availability of large amounts of an-
notated data. However, such data is often scarce,
particularly for non-mainstream languages. Semi-
supervised learning addresses this problem by com-
bining large amounts of unlabeled data with a small
set of labeled data in order to learn a classifica-
tion function. One class of semi-supervised learn-
ing algorithms that has recently attracted increased
interest is graph-based learning. Graph-based tech-
niques represent labeled and unlabeled data points
as nodes in a graph with weighted edges encoding
the similarity of pairs of samples. Various tech-
niques are then available for transferring class la-
bels from the labeled to the unlabeled data points.
These approaches have shown good performance in
cases where the data is characterized by an underly-
ing manifold structure and samples are judged to be
similar by local similarity measures. However, the
question of how to best construct the graph forming
the basis of the learning procedure is still an under-
investigated research problem. NLP learning tasks
present additional problems since they often rely on
discrete or heterogeneous feature spaces for which
standard similarity measures (such as Euclidean or
cosine distance) are suboptimal.
We propose a two-pass data-driven technique for
graph construction in the framework of label propa-
gation (Zhu, 2005). First, we use a supervised clas-
sifier trained on the labeled subset to transform the
initial feature space (consisting of e.g. lexical, con-
textual, or syntactic features) into a continuous rep-
resentation in the form of soft label predictions. This
representation is then used as a basis for measur-
ing similarity among samples that determines the
structure of the graph used for the second, semi-
supervised learning step. It is important to note that,
rather than simply cascading the supervised and the
semi-supervised learner, we optimize the combina-
tion with respect to the properties required of the
graph. We present several techniques for such op-
timization, including regularization of the first-pass
classifier, biasing by class priors, and linear combi-
</bodyText>
<page confidence="0.976239">
204
</page>
<note confidence="0.7984835">
Proceedings of NAACL HLT 2007, pages 204–211,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998163125">
nation of classifier predictions with known features.
The proposed approach is evaluated on a lexicon
learning task using the Wall Street Journal (WSJ)
corpus, and on the SENSEVAL-3 word sense dis-
ambiguation task. In both cases our technique sig-
nificantly outperforms our baseline systems (label
propagation using standard graph construction and
discriminatively trained supervised classifiers).
</bodyText>
<sectionHeader confidence="0.985255" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.998719571428571">
Several graph-based learning techniques have re-
cently been developed and applied to NLP prob-
lems: minimum cuts (Pang and Lee, 2004), random
walks (Mihalcea, 2005; Otterbacher et al., 2005),
graph matching (Haghighi et al., 2005), and label
propagation (Niu et al., 2005). Here we focus on
label propagation as a learning technique.
</bodyText>
<subsectionHeader confidence="0.994391">
2.1 Label propagation
</subsectionHeader>
<bodyText confidence="0.992093">
The basic label propagation (LP) algorithm (Zhu and
Ghahramani, 2002; Zhu, 2005) has as inputs:
</bodyText>
<listItem confidence="0.9754135">
• a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)},
where xi are samples (feature vectors) and yi E
{1, 2,... , C} are their corresponding labels;
• an unlabeled set {xn+1, ... , xN};
• a distance measure d(i, j) i, j E {1,... N} de-
fined on the feature space.
</listItem>
<bodyText confidence="0.9994185">
The goal is to infer the labels {yn+1, ... , yN} for
the unlabeled set. The algorithm represents all N
data points as vertices in an undirected graph with
weighted edges. Initially, only the known data ver-
tices are labeled. The edge linking vertices i and j
has weight:
</bodyText>
<equation confidence="0.932323">
wij = exp(_ d(i,2 )2 J (1)
α
</equation>
<bodyText confidence="0.999759333333333">
where α is a hyperparameter that needs to be empir-
ically chosen or learned separately. wij indicates the
label affinity of vertices: the larger wij is, the more
likely it is that i and j have the same label. The LP
algorithm constructs a row-normalized N x N tran-
sition probability matrix P as follows:
</bodyText>
<equation confidence="0.984332">
Pij = P(i —* j) = N wij (2)
,k=1 wik
</equation>
<bodyText confidence="0.999876428571429">
The algorithm probabilistically pushes labels from
the labeled nodes to the unlabeled nodes. To do so, it
defines the nxC hard labels matrix Y and the N xC
soft labels matrix f, whose first n rows are identical
to Y . The hard labels matrix Y is invariant through
the algorithm and is initialized with probability 1 for
the known label and 0 for all other labels:
</bodyText>
<equation confidence="0.748962">
Yic = δ(yi, C) (3)
</equation>
<bodyText confidence="0.995969">
where δ is Kronecker’s delta function. The algo-
rithm iterates as follows:
</bodyText>
<listItem confidence="0.9677178">
1. f&apos;+— P x f
2. f&apos;[rows 1 to n] �Y
3. If f&apos; f, stop
4. f — f&apos;
5. Repeat from step 1
</listItem>
<bodyText confidence="0.99816975">
In each iteration, step 2 fixes the known labels,
which might otherwise be overriden by propagated
labels. The resulting labels for each feature xi,
where i E {n + 1, ... , N}, are:
</bodyText>
<equation confidence="0.874261">
fij (4)
</equation>
<bodyText confidence="0.999798058823529">
It is important that the distance measure is locally
accurate, i.e. nodes connected by an edge with a
high weight should have the same label. The global
distance is less relevant since label information will
be propagated from labeled points through the entire
space. This is why LP works well with a local dis-
tance measure that might be unsuitable as a global
distance measure.
Applications of LP include handwriting recogni-
tion (Zhu and Ghahramani, 2002), image classifi-
cation (Balcan et al., 2005) and retrieval (Qin et
al., 2005), and protein classification (Weston et al.,
2003). In NLP, label propagation has been used for
word sense disambiguation (Niu et al., 2005), doc-
ument classification (Zhu, 2005), sentiment analy-
sis (Goldberg and Zhu, 2006), and relation extrac-
tion (Chen et al., 2006).
</bodyText>
<subsectionHeader confidence="0.999391">
2.2 Graph construction
</subsectionHeader>
<bodyText confidence="0.999959076923077">
One of the main problems in LP, as well as other
graph-based learning techniques, is how to best con-
struct the graph. Currently, graph construction “is
more of an art than science” (Zhu, 2005). Typically,
edge weights are derived from a simple Euclidean
or cosine distance measure, regardless of the nature
of the underlying features. Edges are then estab-
lished either by connecting all nodes, by applying
a single global threshold to the edge weights, or by
connecting each node to its k nearest neighbors ac-
cording to the edge weights. This procedure is often
suboptimal: Euclidean distance relies on a model of
normally distributed i.i.d. random variables; cosine
</bodyText>
<equation confidence="0.9525445">
li = arg max
j=1,...,c
</equation>
<page confidence="0.991346">
205
</page>
<bodyText confidence="0.999956">
distance likewise assumes that the different feature
vector dimensions are uncorrelated. However, many
applications, particularly in NLP, rely on feature
spaces with correlated dimensions. Moreover, fea-
tures may have different ranges and different types
(e.g. continuous, binary, multi-valued), which en-
tails the need for normalization, binning, or scaling.
Finally, common distance measures do not take ad-
vantage of domain knowledge that might be avail-
able.
Some attempts have been made at improving the
standard method of graph construction. For in-
stance, in a face identification task (Balcan et al.,
2005), domain knowledge was used to identify three
different edge sets based on time, color and face
features, associating a different hyperparameter with
each. The resulting graph was then created by super-
posing edge sets. Zhu (Zhu, 2005, Ch. 7) describes
graph construction using separate α hyperparame-
ters for each feature dimension, and presents a data-
driven way (evidence maximization) for learning the
values of the parameters.
</bodyText>
<sectionHeader confidence="0.896946" genericHeader="method">
3 Data-driven graph construction
</sectionHeader>
<bodyText confidence="0.999846833333333">
Unlike previous work, we propose to optimize the
feature representation used for graph construction
by learning it with a first-pass supervised classi-
fier. Under this approach, similarity of samples is
defined as similarity of the output values produced
by a classifier applied to the original feature repre-
sentation of the samples. This idea bears similar-
ity to classifier cascading (Alpaydin and Kaynak,
1998), where classifiers are trained around a rule-
exceptions paradigm; however, in our case, the clas-
sifiers work together, the first acting as a jointly op-
timized feature mapping function for the second.
</bodyText>
<listItem confidence="0.987819583333334">
1. Train a first-pass supervised classifier that out-
puts soft label predictions Zi for all sam-
ples i E 11.... N}, e.g. a posterior prob-
ability distribution over target labels: Zi =
(pi1ipi2i... ipiC);
2. Apply postprocessing to Zi if needed.
3. Use vectors Zi and an appropriately chosen dis-
tance measure to construct a graph for LP.
4. Perform label propagation over the constructed
graph to find the labeling of the test samples.
The advantages of this procedure are:
• Uniform range and type of features: The out-
</listItem>
<bodyText confidence="0.9438272">
put from a first-pass classifier can produce well-
defined features, e.g. posterior probability distribu-
tions. This eliminates the problem of input features
of different ranges and types (e.g. binary vs. multi-
valued, continuous vs. categorical attributes) which
are often used in combination.
• Feature postprocessing: The transformation of
features into a different space also opens up pos-
sibilities for postprocessing (e.g. probability distri-
bution warping) depending on the requirements of
the second-pass learner. In addition, different dis-
tance functions (e.g. those defined on probability
spaces) can be used, which avoids violating assump-
tions made by metrics such as Euclidean and cosine
distance.
</bodyText>
<listItem confidence="0.866157333333333">
• Optimizing class separation: The learned repre-
sentation of labeled training samples might reveal
better clusters in the data than the original represen-
</listItem>
<bodyText confidence="0.9068715625">
tation: a discriminatively-trained first pass classifier
will attempt to maximize the separation of samples
belonging to different classes. Moreover, the first-
pass classifier may learn a feature transformation
that suppresses noise in the original input space.
Difficulties with the proposed approach might arise
when the first-pass classifier yields confident but
wrong predictions, especially for outlier samples in
the original space. For this reason, the first-pass
classifier and the graph-based learner should not
simply be concatenated without modification, but
the first classifier should be optimized with respect
to the requirements of the second. In our case, the
choice of first-pass classifier and joint optimization
techniques are determined by the particular learning
task and are detailed below.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="method">
4 Tasks
</sectionHeader>
<subsectionHeader confidence="0.999569">
4.1 Lexicon acquisition task
</subsectionHeader>
<bodyText confidence="0.9999861">
Our first task is a part-of-speech (POS) lexicon ac-
quisition task, i.e. the labels to be predicted are the
sets of POS tags associated with each word in a lex-
icon. Note that this is not a tagging task: we are not
attempting to identify the correct POS of each word
in running text. Rather, for each word in the vocab-
ulary, we attempt to infer the set of possible POS
tags. Our choice of this task is motivated by our
long-term goal of applying this technique to lexicon
acquisition for resource-poor languages: POS lexi-
</bodyText>
<page confidence="0.992147">
206
</page>
<bodyText confidence="0.999780314285714">
cons are one of the most basic language resources,
which enable subsequent training of taggers, chun-
kers, etc. We assume that a small set of words can be
reliably annotated, and that POS-sets for the remain-
ing words can be inferred by semi-supervised learn-
ing. Rather than choosing a genuinely resource-poor
language for this task, we use the English Wall Street
Journal (WSJ) corpus and artificially limit the size
of the labeled set. This is because the WSJ corpus is
widely obtainable and allows easy replication of our
experiments.
We use sections 0-18 of the Wall Street Journal
corpus (N = 44, 492). Words have between 1 and
4 POS tags, with an average of 1.1 per word. The
number of POS tags is 36, and we treat every POS
combination as a unique class, resulting in C = 158
distinct labels. We use three different randomly se-
lected training sets of various sizes: 5000, 10000,
and 15000 words, representing about 11%, 22%, and
34% of the entire data set respectively; the rest of the
data was used for testing. In order to avoid experi-
mental bias, we run all experiments on five differ-
ent randomly chosen labeled subsets and report av-
erages and standard deviations. Due to the random
sampling of the data it is possible that some labels
never occur in the training set or only occur once.
We train our classifiers only on those labels that oc-
cur at least twice, which results in 60-63 classes. La-
bels not present in the training set will therefore not
be hypothesized and are guaranteed to be errors. We
delete samples with unknown labels from our unla-
beled set since their percentage is less than 0.5% on
average.
We use the following features to represent sam-
ples:
</bodyText>
<listItem confidence="0.999524454545455">
• Integer: the three-letter suffix of the word;
• Integer: The four-letter suffix of the word;
• Integer x 4: The indices of the four most fre-
quent words that immediately precede the word
in the WSJ text;
• Boolean: word contains capital letters;
• Boolean: word consists only of capital letters;
• Boolean: word contains digits;
• Boolean: word contains a hyphen;
• Boolean: word contains other special charac-
ters (e.g. “&amp;”).
</listItem>
<bodyText confidence="0.986303071428572">
We have also experimented with shorter suffixes and
with prefixes but those features tended to degrade
performance.
4.2 SENSEVAL-3 word sense disambiguation
task
The second task is word sense disambiguation using
the SENSEVAL-3 corpus (Mihalcea et al., 2004), to
enable a comparison of our method with previously
published results. The goal is to disambiguate the
different senses of each of 57 words given the sen-
tences within which they occur. There are 7860 sam-
ples for training and 3944 for testing. In line with
existing work (Lee and Ng, 2002; Niu et al., 2005),
we use the following features:
</bodyText>
<listItem confidence="0.904609615384615">
• Integer x 7: seven features consisting of the
POS of the previous three words, the POS of
the next three words, and the POS of the word
itself. We used the MXPOST tagger (Ratna-
parkhi, 1996) for POS annotation.
• Integer x(variable length): a bag of all words
in the surrounding context.
• Integer x 15: Local collocations CZE (i, j are
the bounds of the collocation window)—word
combinations from the context of the word to
disambiguate. In addition to the 11 collocations
used in similar work (Lee and Ng, 2002), we
also used C−3,1, C−3,2, C−2,3, C−1,3.
</listItem>
<bodyText confidence="0.9999305">
Note that syntactic features, which have been used in
some previous studies on this dataset (Mohammad
and Pedersen, 2004), were not included. We apply a
simple feature selection method: a feature X is se-
lected if the conditional entropy H(Y |X) is above
a fixed threshold (1 bit) in the training set, and if X
also occurs in the test set (note that no label infor-
mation from the test data is used for this purpose).
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9981414">
For both tasks we compare the performance of a su-
pervised classifier, label propagation using the stan-
dard input features and either Euclidean or cosine
distance, and LP using the output from a first-pass
supervised classifier.
</bodyText>
<subsectionHeader confidence="0.862909">
5.1 Lexicon acquisition task
5.1.1 First-pass classifier
</subsectionHeader>
<bodyText confidence="0.99993">
For this task, the first-pass classifier is a multi-
layer perceptron (MLP) with the topology shown
in Fig. 1. The input features are mapped to con-
</bodyText>
<page confidence="0.997188">
207
</page>
<figureCaption confidence="0.997339">
Figure 1: Architecture of first-pass supervised classifier (MLP)
for lexicon acquisition
</figureCaption>
<bodyText confidence="0.99466275">
.
tinuous values by a discrete-to-continuous mapping
layer M, which is itself learned during the MLP
training process. This layer connects to the hidden
layer h, which in turn is connected to the output
layer o. The entire network is trained via backprop-
agation. The training criterion maximizes the regu-
larized log-likelihood of the training data:
</bodyText>
<equation confidence="0.997106">
log P(yt|xt, θ) + R(θ) (5)
</equation>
<bodyText confidence="0.999846764705882">
The use of an additional continuous mapping layer
is similar to the use of hidden continuous word rep-
resentations in neural language modeling (Bengio et
al., 2000) and yields better results than a standard
3-layer MLP topology.
Problems caused by data scarcity arise when some
of the input features of the unlabeled words have
never been seen in the training set, resulting in un-
trained, randomly-initialized values for those fea-
ture vector components. We address this problem
by creating an approximation layer A that finds the
known input feature vector x&apos; that is most similar
to x (by measuring the cosine similarity between
the vectors). Then xk is replaced with x&apos;k, resulting
in vector x� = (x1, ... , xk−1, x&apos;k, xk+1, . . . , xf) that
has no unseen features and is closest to the original
vector.
</bodyText>
<subsubsectionHeader confidence="0.69333">
5.1.2 LP Setup
</subsubsectionHeader>
<bodyText confidence="0.998193419354839">
We use a dense graph approach. The WSJ set
has a total of 44,492 words, therefore the P ma-
trix that the algorithm requires would have 44, 492x
44, 492 &apos;= 2 x 109 elements. Due to the matrix size,
we avoid the analytical solution of the LP problem,
which requires inverting the P matrix, and choose
the iterative approach described above (Sec. 2.1) in-
stead. Convergence is stopped when the maximum
relative difference between each cell of f and the
corresponding cell of f&apos; is less than 1%.
Also for data size reasons, we apply LP in chunks.
While the training set stays in memory, the test
data is loaded in fixed-size chunks, labeled, and dis-
carded. This approach has yielded similar results
for various chunk sizes, suggesting that chunking is
a good approximation of whole-set label propaga-
tion.&apos; LP in chunks is also amenable to paralleliza-
tion: Our system labels different chunks in parallel.
We trained the α hyperparameter by three-fold
cross-validation on the training data, using a geo-
metric progression with limits 0.1 and 10 and ratio
2. We set fixed upper limits of edges between an
unlabeled node and its labeled neighbors to 15, and
between an unlabeled node and its unlabeled neigh-
bors to 5. The approach of setting different limits
among different kinds of nodes is also used in re-
lated work (Goldberg and Zhu, 2006).
For graph construction we tested: (a) the original
discrete input representation with cosine distance;
(b) the classifier output features (probability distri-
butions) with the Jeffries-Matusita distance.
</bodyText>
<subsectionHeader confidence="0.998696">
5.2 Combination optimization
</subsectionHeader>
<bodyText confidence="0.972499">
The static parameters of the MLP (learning rate, reg-
ularization rate, and number of hidden units) were
optimized for the LP step by 5-fold cross-validation
on the training data. This process is important be-
cause overspecialization is detrimental to the com-
bined system: an overspecialized first-pass classi-
fier may output very confident but wrong predic-
tions for unseen patterns, thus placing such samples
at large distances from all correctly labeled sam-
ples. A strongly regularized neural network, by con-
trast, will output smoother probability distributions
for unseen patterns. Such outputs also result in a
smoother graph, which in turn helps the LP process.
Thus, we found that a network with only 12 hidden
units and relatively high R(θ) in Eq. 5 (10% of the
weight value) performed best in combination with
LP (at an insignificant cost in accuracy when used
&apos;In fact, experiments have shown that performance tends to
degrade for larger chunk sizes, suggesting that whole-set LP
might be affected by “artifact” clusters that are not related to
the labels.
</bodyText>
<figure confidence="0.920274846153846">
M
A
x1
x2
x3
x4
Wih
h
Who
o P(y  |x)
i
L= 1 n
n t=1
</figure>
<page confidence="0.98539">
208
</page>
<bodyText confidence="0.573444">
as an isolated classifier).
</bodyText>
<sectionHeader confidence="0.521668" genericHeader="evaluation">
5.2.1 Results
</sectionHeader>
<bodyText confidence="0.999038666666667">
We first conducted an experiment to measure the
smoothness of the underlying graph, S(G), in the
two LP experiments according to the following for-
</bodyText>
<equation confidence="0.9983795">
mula: S(G) = � wij (6)
yz7�yj,(i&gt;nVj&gt;n)
</equation>
<bodyText confidence="0.999996181818182">
where yi is the label of sample i. (Lower values are
better as they reflect less affinity between nodes of
different labels.) The value of S(G) was in all cases
significantly better on graphs constructed with our
proposed technique than on graphs constructed in
the standard way (see Table 1). Table 1 also shows
the performance comparison between LP over the
discrete representation and cosine distance (“LP”),
the neural network itself (“NN”), and LP over the
continuous representation (“NN+LP”), on all dif-
ferent subsets and for different training sizes. For
scarce labeled data (5000 samples) the neural net-
work, which uses a strictly supervised training pro-
cedure, is at a clear disadvantage. However, for a
larger training set the network is able to perform
more accurately than the LP learner that uses the
discrete features directly. The third, combined tech-
nique outperforms the first two significantly.2 The
differences are more pronounced for smaller train-
ing set sizes. Interestingly, the LP is able to extract
information from largely erroneous (noisy) distribu-
tions learned by the neural network.
</bodyText>
<subsectionHeader confidence="0.999109">
5.3 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999800769230769">
We compare the performance of an SVM classifier,
an LP learner using the same input features as the
SVM, and an LP learner using the SVM outputs as
input features. To analyze the influence of train-
ing set size on accuracy, we randomly sample sub-
sets of the training data (25%, 50%, and 75%) and
use the remaining training data plus the test data
as unlabeled data, similarly to the procedure fol-
lowed in related work (Niu et al., 2005). The re-
sults are averaged over five different random sam-
plings. The samplings were chosen such that there
was at least one sample for each label in the training
set. SENSEVAL-3 sports multi-labeled samples and
</bodyText>
<footnote confidence="0.908250333333333">
2Significance was tested using a difference of proportions
significance test; the significance level is 0.01 or smaller in all
cases.
</footnote>
<bodyText confidence="0.987885333333333">
samples with the “unknown” label. We eliminate all
samples labeled as unknown and retain only the first
label for the multi-labeled instances.
</bodyText>
<subsectionHeader confidence="0.703445">
5.3.1 SVM setup
</subsectionHeader>
<bodyText confidence="0.9996883125">
The use of SVM vs. MLP in this case was justi-
fied by the very small training data set. An MLP has
many parameters and needs a considerable amount
of data for effective training, so for this task with
only on the order of 102 training samples per classi-
fier, an SVM was deemed more appropriate. We use
the SVMliyht package to build a set of binary clas-
sifiers in a one-versus-all formulation of the multi-
class classification problem. The features input to
each SVM consist of the discrete features described
above (Sec. 4.2) after feature selection. After train-
ing SVMs for each target label against the union of
all others, we evaluate the SVM approach against the
test set by using the winner-takes-all strategy: the
predicted label corresponds to the SVM that outputs
the largest value.
</bodyText>
<subsectionHeader confidence="0.851136">
5.3.2 LP setup
</subsectionHeader>
<bodyText confidence="0.999879333333333">
Again we set up two LP systems: one using the
original feature space (after feature selection, which
benefited all of the tested systems) and one using the
SVM outputs. Both use a cosine distance measure.
The α parameter (see Eq. 1) is optimized through
3-fold cross-validation on the training set.
</bodyText>
<subsectionHeader confidence="0.997492">
5.4 Combination optimization
</subsectionHeader>
<bodyText confidence="0.9999844">
Unlike MLPs, SVMs do not compute a smooth out-
put distribution but base the classification decision
on the sign of the output values. In order to smooth
output values with a view towards graph construc-
tion we applied the following techniques:
</bodyText>
<listItem confidence="0.416843">
1. Combining SVM predictions and perfect fea-
</listItem>
<bodyText confidence="0.9934191">
ture vectors: After training, the SVM actu-
ally outputs wrong label predictions for a small
number (Pt� 5%) of training samples. These out-
puts could simply be replaced with the perfect
SVM predictions (1 for the true class, -1 else-
where) since the labels are known. However,
the second-pass learner might actually bene-
fit from the information contained in the mis-
classifications. We therefore linearly combine
the SVM predictions with the “perfect” feature
</bodyText>
<page confidence="0.995342">
209
</page>
<table confidence="0.999368454545455">
Initial labels Model S(G) avg. Accuracy (%)
Set 1 Set 2 Set 3 Set 4 Set 5 Average
5000 NN − 50.70 59.22 63.77 60.09 54.58 57.67 f 4.55
LP 451.54 58.37 59.91 60.88 62.01 59.47 60.13 f 1.24
NN+LP 409.79 58.03 63.91 66.62 65.93 57.76 62.45 f 3.83
10000 NN − 65.86 60.19 67.52 65.68 65.64 64.98 f 2.49
LP 381.16 58.27 60.04 60.85 61.99 62.06 60.64 f 1.40
NN+LP 315.53 69.36 64.73 69.50 70.26 67.71 68.31 f 1.97
15000 NN − 69.85 66.42 70.88 70.71 72.18 70.01 f 1.94
LP 299.10 58.51 61.00 60.94 63.53 60.98 60.99 f 1.59
NN+LP 235.83 70.59 69.45 69.99 71.20 73.45 70.94 f 1.39
</table>
<tableCaption confidence="0.9892825">
Table 1: Accuracy results of neural classification (NN), LP with discrete features (LP), and combined (NN+LP), over 5 random
samplings of 5000, 10000, and 15000 labeled words in the WSJ lexicon acquisition task. S(G) is the smoothness of the graph
</tableCaption>
<bodyText confidence="0.972654083333333">
vectors v that contain 1 at the correct label po-
sition and -1 elsewhere:
s�i = γsi + (1 − γ)vi (7)
where si, sz are the i’th input and output feature
vectors and γ a parameter fixed at 0.5.
2. Biasing uninformative distributions: For some
training samples, although the predicted class
label was correct, the outputs of the SVM were
relatively close to one another, i.e. the decision
was borderline. We decided to bias these SVM
outputs in the right direction by using the same
formula as in equation 7.
3. Weighting by class priors: For each training
sample, a corresponding sample with the per-
fect output features was added, thus doubling
the total number of labeled nodes in the graph.
These synthesized nodes are akin to the “don-
gle” nodes (Goldberg and Zhu, 2006). The dif-
ference is that, while dongle nodes are only
linked to one node, our artificial nodes are
treated like any other node and as such can con-
nect to several other nodes. The role of the arti-
ficial nodes is to serve as authorities during the
LP process and to emphasize class priors.
</bodyText>
<sectionHeader confidence="0.822265" genericHeader="evaluation">
5.4.1 Results
</sectionHeader>
<bodyText confidence="0.999442785714285">
As before, we measured the smoothness of the
graphs in the two label propagation setups and found
that in all cases the smoothness of the graph pro-
duced with our method was better when compared
to the graphs produced using the standard approach,
as shown in Table 3, which also shows accuracy re-
sults for the SVM (“SVM” label), LP over the stan-
dard graph (“LP”), and label propagation over SVM
outputs (“SVM+LP”). The latter system consistently
performs best in all cases, although the most marked
gains occur in the upper range of labeled samples
percentage. The gain of the best data-driven LP over
the knowledge-based LP is significant in the 100%
and 75% cases.
</bodyText>
<note confidence="0.738299875">
# System Acc. (%)
1 htsa3 (Grozea, 2004) 72.9
2 IRST-kernels (Strapparava et al., 2004) 72.6
3 nusels (Lee et al., 2004) 72.4
4 SENSEVAL-3 contest baseline 55.2
5 Niu et al. (Niu et al., 2005) LP/J-S 70.3
6 Niu et al. LP/cosine 68.4
7 Niu et al. SVM 69.7
</note>
<tableCaption confidence="0.992196666666667">
Table 2: Accuracy results of other published systems on
SENSEVAL-3. 1-3 use syntactic features; 5-7 are directly com-
parably to our system.
</tableCaption>
<bodyText confidence="0.999896692307692">
For comparison purposes, Table 2 shows results
of other published systems against the SENSEVAL
corpus. The “htsa3”, “IRST-kernels”, and “nusels”
systems were the winners of the SENSEVAL-3 con-
test and used extra input features (syntactic rela-
tions). The Niu et al. work (Niu et al., 2005) is
most comparable to ours. We attribute the slightly
higher performance of our SVM due to our feature
selection process. The LP/cosine system is a system
similar to our LP system using the discrete features,
and the LP/Jensen-Shannon system is also similar
but uses a distance measure derived from Jensen-
Shannon divergence.
</bodyText>
<sectionHeader confidence="0.999619" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999145">
We have presented a data-driven graph construction
technique for label propagation that utilizes a first-
</bodyText>
<page confidence="0.9926">
210
</page>
<table confidence="0.999430142857143">
Initial labels Model S(G) avg. Accuracy (%)
Set 1 Set 2 Set 3 Set 4 Set 5 Average
25% SVM − 62.94 62.53 62.69 63.52 62.99 62.93 f 0.34
LP 44.71 63.27 61.84 63.26 62.96 63.30 62.93 f 0.56
SVM+LP 39.67 63.39 63.20 63.95 63.68 63.91 63.63 f 0.29
50% SVM − 67.90 66.75 67.57 67.44 66.79 67.29 f 0.45
LP 33.17 67.84 66.57 67.35 66.52 66.35 66.93 f 0.57
SVM+LP 24.19 67.95 67.54 67.93 68.21 68.11 67.95 f 0.23
75% SVM − 69.54 70.19 68.75 69.80 68.73 69.40 f 0.58
LP 29.93 68.87 68.65 68.58 68.42 67.19 68.34 f 0.59
SVM+LP 16.19 69.98 70.05 69.69 70.38 68.94 69.81 f 0.49
100% SVM − 70.74
LP 21.72 69.69
SVM+LP 13.17 71.72
</table>
<tableCaption confidence="0.983829666666667">
Table 3: Accuracy results of support vector machine (SVM), label propagation over discrete features (LP), and label propagation
over SVM outputs (SVM+LP), each trained with 25%, 50%, 75% (5 random samplings each), and 100% of the train set. The
improvements of SVM+LP are significant over LP in the 75% and 100% cases. S(G) is the graph smoothness
</tableCaption>
<bodyText confidence="0.999509615384615">
pass supervised classifier. The outputs from this
classifier (especially when optimized for the second-
pass learner) were shown to serve as a better repre-
sentation for graph-based semi-supervised learning.
Classification results on two learning tasks showed
significantly better performance compared to LP us-
ing standard graph construction and the supervised
classifier alone.
Acknowledgments This work was funded by
NSF under grant no. IIS-0326276. Any opinions,
findings and conclusions, or recommendations ex-
pressed herein are those of the authors and do not
necessarily reflect the views of this agency.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999866403225807">
E. Alpaydin and C. Kaynak. 1998. Cascading classifiers. Ky-
bernetika, 34:369–374.
Balcan et al. 2005. Person identification in webcam images. In
ICML Workshop on Learning with Partially Classified Train-
ing Data.
Y. Bengio, R. Ducharme, and P. Vincent. 2000. A neural prob-
abilistic language model. In NIPS.
J. Chen, D. Ji, C.L. Tan, and Z. Niu. 2006. Relation Extraction
Using Label Propagation Based Semi-supervised Learning.
In Proceedings of ACL, pages 129–136.
A. Goldberg and J. Zhu. 2006. Seeing stars when there aren’t
many stars: Graph-based semi-supervised learning for sen-
timent categorization. In HLT-NAACL Workshop on Graph-
based Algorithms for Natural Language Processing.
C. Grozea. 2004. Finding optimal parameter settings for high
performance word sense disambiguation. Proceedings of
Senseval-3 Workshop.
A. Haghighi, A. Ng, and C.D. Manning. 2005. Robust textual
inference via graph matching. Proceedings of EMNLP.
Y.K. Lee and H.T. Ng. 2002. An empirical evaluation of knowl-
edge sources and learning algorithms for word sense disam-
biguation. In Proceedings of EMNLP, pages 41–48.
Y.K. Lee, H.T. Ng, and T.K. Chia. 2004. Supervised Word
Sense Disambiguation with Support Vector Machines and
Multiple Knowledge Sources. SENSEVAL-3.
R. Mihalcea, T. Chklovski, and A. Killgariff. 2004. The
Senseval-3 English Lexical Sample Task. In Proceedings
of ACL/SIGLEX Senseval-3.
R. Mihalcea. 2005. Unsupervised large-vocabulary word sense
disambiguation with graph-based algorithms for sequence
data labeling. In Proceedings of HLT/EMNLP, pages 411–
418.
S. Mohammad and T. Pedersen. 2004. Complementarity of
Lexical and Simple Syntactic Features: The SyntaLex Ap-
proach to Senseval-3. Proceedings of the SENSEVAL-3.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005. Word
sense disambiguation using label propagation based semi-
supervised learning. In ACL ’05.
J. Otterbacher, G. Erkan, and D.R. Radev. 2005. Using Ran-
dom Walks for Question-focused Sentence Retrieval. Pro-
ceedings of HLT/EMNLP, pages 915–922.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL, pages 271–278.
T. Qin, T.-Y. Liu, X.-D. Zhang, W.-Y. Ma, and H.-J. Zhang.
2005. Subspace clustering and label propagation for active
feedback in image retrieval. In MMM, pages 172–179.
A. Ratnaparkhi. 1996. A maximum entropy model for part-of-
speech tagging. In Proceedings of EMNLP, pages 133–142.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern
abstraction and term similarity for word sense disambigua-
tion: IRST at SENSEVAL-3. Proc. of SENSEVAL-3, pages
229–234.
J. Weston, C. Leslie, D. Zhou, A. Elisseeff, and W. Noble.
2003. Semi-supervised protein classification using cluster
kernels.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical report,
CMU-CALD-02.
Xiaojin Zhu. 2005. Semi-Supervised Learning with Graphs.
Ph.D. thesis, Carnegie Mellon University. CMU-LTI-05-
192.
</reference>
<page confidence="0.998791">
211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.312159">
<title confidence="0.999266">Data-Driven Graph Construction for Semi-Supervised Learning in NLP</title>
<author confidence="0.99215">Andrei</author>
<affiliation confidence="0.999637">Dept. of Computer Science and University of</affiliation>
<address confidence="0.844252">Seattle, WA,</address>
<email confidence="0.999377">andrei@cs.washington.edu</email>
<author confidence="0.433474">Katrin</author>
<affiliation confidence="0.9994825">Dept. of Electrical University of</affiliation>
<address confidence="0.983648">Seattle, WA</address>
<email confidence="0.999889">katrin@ee.washington.edu</email>
<abstract confidence="0.994341272727273">Graph-based semi-supervised learning has recently emerged as a promising approach to data-sparse learning problems in natural language processing. All graph-based algorithms rely on a graph that jointly represents labeled and unlabeled data points. The problem of how to best construct this graph remains largely unsolved. In this paper we introduce a data-driven method that optimizes the representation of the initial feature space for graph construction by means of a supervised classifier. We apply this technique in the framework of label propagation and evaluate it on two different classification tasks, a multi-class lexicon acquisition task and a word sense disambiguation task. Significant improvements are demonstrated over both label propagation using conventional graph construction and state-of-the-art supervised classifiers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Alpaydin</author>
<author>C Kaynak</author>
</authors>
<title>Cascading classifiers.</title>
<date>1998</date>
<journal>Kybernetika,</journal>
<pages>34--369</pages>
<contexts>
<context position="8984" citStr="Alpaydin and Kaynak, 1998" startWordPosition="1432" endWordPosition="1435">ibes graph construction using separate α hyperparameters for each feature dimension, and presents a datadriven way (evidence maximization) for learning the values of the parameters. 3 Data-driven graph construction Unlike previous work, we propose to optimize the feature representation used for graph construction by learning it with a first-pass supervised classifier. Under this approach, similarity of samples is defined as similarity of the output values produced by a classifier applied to the original feature representation of the samples. This idea bears similarity to classifier cascading (Alpaydin and Kaynak, 1998), where classifiers are trained around a ruleexceptions paradigm; however, in our case, the classifiers work together, the first acting as a jointly optimized feature mapping function for the second. 1. Train a first-pass supervised classifier that outputs soft label predictions Zi for all samples i E 11.... N}, e.g. a posterior probability distribution over target labels: Zi = (pi1ipi2i... ipiC); 2. Apply postprocessing to Zi if needed. 3. Use vectors Zi and an appropriately chosen distance measure to construct a graph for LP. 4. Perform label propagation over the constructed graph to find th</context>
</contexts>
<marker>Alpaydin, Kaynak, 1998</marker>
<rawString>E. Alpaydin and C. Kaynak. 1998. Cascading classifiers. Kybernetika, 34:369–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balcan</author>
</authors>
<title>Person identification in webcam images.</title>
<date>2005</date>
<booktitle>In ICML Workshop on Learning with Partially Classified Training Data.</booktitle>
<marker>Balcan, 2005</marker>
<rawString>Balcan et al. 2005. Person identification in webcam images. In ICML Workshop on Learning with Partially Classified Training Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2000</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="16678" citStr="Bengio et al., 2000" startWordPosition="2711" endWordPosition="2714">: Architecture of first-pass supervised classifier (MLP) for lexicon acquisition . tinuous values by a discrete-to-continuous mapping layer M, which is itself learned during the MLP training process. This layer connects to the hidden layer h, which in turn is connected to the output layer o. The entire network is trained via backpropagation. The training criterion maximizes the regularized log-likelihood of the training data: log P(yt|xt, θ) + R(θ) (5) The use of an additional continuous mapping layer is similar to the use of hidden continuous word representations in neural language modeling (Bengio et al., 2000) and yields better results than a standard 3-layer MLP topology. Problems caused by data scarcity arise when some of the input features of the unlabeled words have never been seen in the training set, resulting in untrained, randomly-initialized values for those feature vector components. We address this problem by creating an approximation layer A that finds the known input feature vector x&apos; that is most similar to x (by measuring the cosine similarity between the vectors). Then xk is replaced with x&apos;k, resulting in vector x� = (x1, ... , xk−1, x&apos;k, xk+1, . . . , xf) that has no unseen featur</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2000</marker>
<rawString>Y. Bengio, R. Ducharme, and P. Vincent. 2000. A neural probabilistic language model. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>D Ji</author>
<author>C L Tan</author>
<author>Z Niu</author>
</authors>
<title>Relation Extraction Using Label Propagation Based Semi-supervised Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="6783" citStr="Chen et al., 2006" startWordPosition="1091" endWordPosition="1094">ce label information will be propagated from labeled points through the entire space. This is why LP works well with a local distance measure that might be unsuitable as a global distance measure. Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classification (Balcan et al., 2005) and retrieval (Qin et al., 2005), and protein classification (Weston et al., 2003). In NLP, label propagation has been used for word sense disambiguation (Niu et al., 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph. Currently, graph construction “is more of an art than science” (Zhu, 2005). Typically, edge weights are derived from a simple Euclidean or cosine distance measure, regardless of the nature of the underlying features. Edges are then established either by connecting all nodes, by applying a single global threshold to the edge weights, or by connecting each node to its k nearest neighbors according to the edge weights. This procedure is often suboptimal: E</context>
</contexts>
<marker>Chen, Ji, Tan, Niu, 2006</marker>
<rawString>J. Chen, D. Ji, C.L. Tan, and Z. Niu. 2006. Relation Extraction Using Label Propagation Based Semi-supervised Learning. In Proceedings of ACL, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Goldberg</author>
<author>J Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In HLT-NAACL Workshop on Graphbased Algorithms for Natural Language Processing.</booktitle>
<contexts>
<context position="6738" citStr="Goldberg and Zhu, 2006" startWordPosition="1083" endWordPosition="1086">me label. The global distance is less relevant since label information will be propagated from labeled points through the entire space. This is why LP works well with a local distance measure that might be unsuitable as a global distance measure. Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classification (Balcan et al., 2005) and retrieval (Qin et al., 2005), and protein classification (Weston et al., 2003). In NLP, label propagation has been used for word sense disambiguation (Niu et al., 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph. Currently, graph construction “is more of an art than science” (Zhu, 2005). Typically, edge weights are derived from a simple Euclidean or cosine distance measure, regardless of the nature of the underlying features. Edges are then established either by connecting all nodes, by applying a single global threshold to the edge weights, or by connecting each node to its k nearest neighbors according to the edge w</context>
<context position="18667" citStr="Goldberg and Zhu, 2006" startWordPosition="3058" endWordPosition="3061">k sizes, suggesting that chunking is a good approximation of whole-set label propagation.&apos; LP in chunks is also amenable to parallelization: Our system labels different chunks in parallel. We trained the α hyperparameter by three-fold cross-validation on the training data, using a geometric progression with limits 0.1 and 10 and ratio 2. We set fixed upper limits of edges between an unlabeled node and its labeled neighbors to 15, and between an unlabeled node and its unlabeled neighbors to 5. The approach of setting different limits among different kinds of nodes is also used in related work (Goldberg and Zhu, 2006). For graph construction we tested: (a) the original discrete input representation with cosine distance; (b) the classifier output features (probability distributions) with the Jeffries-Matusita distance. 5.2 Combination optimization The static parameters of the MLP (learning rate, regularization rate, and number of hidden units) were optimized for the LP step by 5-fold cross-validation on the training data. This process is important because overspecialization is detrimental to the combined system: an overspecialized first-pass classifier may output very confident but wrong predictions for uns</context>
<context position="25790" citStr="Goldberg and Zhu, 2006" startWordPosition="4256" endWordPosition="4259">ture vectors and γ a parameter fixed at 0.5. 2. Biasing uninformative distributions: For some training samples, although the predicted class label was correct, the outputs of the SVM were relatively close to one another, i.e. the decision was borderline. We decided to bias these SVM outputs in the right direction by using the same formula as in equation 7. 3. Weighting by class priors: For each training sample, a corresponding sample with the perfect output features was added, thus doubling the total number of labeled nodes in the graph. These synthesized nodes are akin to the “dongle” nodes (Goldberg and Zhu, 2006). The difference is that, while dongle nodes are only linked to one node, our artificial nodes are treated like any other node and as such can connect to several other nodes. The role of the artificial nodes is to serve as authorities during the LP process and to emphasize class priors. 5.4.1 Results As before, we measured the smoothness of the graphs in the two label propagation setups and found that in all cases the smoothness of the graph produced with our method was better when compared to the graphs produced using the standard approach, as shown in Table 3, which also shows accuracy resul</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>A. Goldberg and J. Zhu. 2006. Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization. In HLT-NAACL Workshop on Graphbased Algorithms for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grozea</author>
</authors>
<title>Finding optimal parameter settings for high performance word sense disambiguation.</title>
<date>2004</date>
<booktitle>Proceedings of Senseval-3 Workshop.</booktitle>
<contexts>
<context position="26798" citStr="Grozea, 2004" startWordPosition="4437" endWordPosition="4438">ound that in all cases the smoothness of the graph produced with our method was better when compared to the graphs produced using the standard approach, as shown in Table 3, which also shows accuracy results for the SVM (“SVM” label), LP over the standard graph (“LP”), and label propagation over SVM outputs (“SVM+LP”). The latter system consistently performs best in all cases, although the most marked gains occur in the upper range of labeled samples percentage. The gain of the best data-driven LP over the knowledge-based LP is significant in the 100% and 75% cases. # System Acc. (%) 1 htsa3 (Grozea, 2004) 72.9 2 IRST-kernels (Strapparava et al., 2004) 72.6 3 nusels (Lee et al., 2004) 72.4 4 SENSEVAL-3 contest baseline 55.2 5 Niu et al. (Niu et al., 2005) LP/J-S 70.3 6 Niu et al. LP/cosine 68.4 7 Niu et al. SVM 69.7 Table 2: Accuracy results of other published systems on SENSEVAL-3. 1-3 use syntactic features; 5-7 are directly comparably to our system. For comparison purposes, Table 2 shows results of other published systems against the SENSEVAL corpus. The “htsa3”, “IRST-kernels”, and “nusels” systems were the winners of the SENSEVAL-3 contest and used extra input features (syntactic relations</context>
</contexts>
<marker>Grozea, 2004</marker>
<rawString>C. Grozea. 2004. Finding optimal parameter settings for high performance word sense disambiguation. Proceedings of Senseval-3 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>A Ng</author>
<author>C D Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4126" citStr="Haghighi et al., 2005" startWordPosition="601" endWordPosition="604">ifier predictions with known features. The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi are samples (feature vectors) and yi E {1, 2,... , C} are their corresponding labels; • an unlabeled set {xn+1, ... , xN}; • a distance measure d(i, j) i, j E {1,... N} defined on the feature space. The goal is to infer the labels {yn+1, ... , yN} for the unlabeled set. The algorithm represents all N data points</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>A. Haghighi, A. Ng, and C.D. Manning. 2005. Robust textual inference via graph matching. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y K Lee</author>
<author>H T Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="14577" citStr="Lee and Ng, 2002" startWordPosition="2355" endWordPosition="2358">phen; • Boolean: word contains other special characters (e.g. “&amp;”). We have also experimented with shorter suffixes and with prefixes but those features tended to degrade performance. 4.2 SENSEVAL-3 word sense disambiguation task The second task is word sense disambiguation using the SENSEVAL-3 corpus (Mihalcea et al., 2004), to enable a comparison of our method with previously published results. The goal is to disambiguate the different senses of each of 57 words given the sentences within which they occur. There are 7860 samples for training and 3944 for testing. In line with existing work (Lee and Ng, 2002; Niu et al., 2005), we use the following features: • Integer x 7: seven features consisting of the POS of the previous three words, the POS of the next three words, and the POS of the word itself. We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation. • Integer x(variable length): a bag of all words in the surrounding context. • Integer x 15: Local collocations CZE (i, j are the bounds of the collocation window)—word combinations from the context of the word to disambiguate. In addition to the 11 collocations used in similar work (Lee and Ng, 2002), we also used C−3,1, C−3,2, C−2,3</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Y.K. Lee and H.T. Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings of EMNLP, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y K Lee</author>
<author>H T Ng</author>
<author>T K Chia</author>
</authors>
<title>Supervised Word Sense Disambiguation with Support Vector Machines and Multiple Knowledge Sources.</title>
<date>2004</date>
<contexts>
<context position="26878" citStr="Lee et al., 2004" startWordPosition="4449" endWordPosition="4452">was better when compared to the graphs produced using the standard approach, as shown in Table 3, which also shows accuracy results for the SVM (“SVM” label), LP over the standard graph (“LP”), and label propagation over SVM outputs (“SVM+LP”). The latter system consistently performs best in all cases, although the most marked gains occur in the upper range of labeled samples percentage. The gain of the best data-driven LP over the knowledge-based LP is significant in the 100% and 75% cases. # System Acc. (%) 1 htsa3 (Grozea, 2004) 72.9 2 IRST-kernels (Strapparava et al., 2004) 72.6 3 nusels (Lee et al., 2004) 72.4 4 SENSEVAL-3 contest baseline 55.2 5 Niu et al. (Niu et al., 2005) LP/J-S 70.3 6 Niu et al. LP/cosine 68.4 7 Niu et al. SVM 69.7 Table 2: Accuracy results of other published systems on SENSEVAL-3. 1-3 use syntactic features; 5-7 are directly comparably to our system. For comparison purposes, Table 2 shows results of other published systems against the SENSEVAL corpus. The “htsa3”, “IRST-kernels”, and “nusels” systems were the winners of the SENSEVAL-3 contest and used extra input features (syntactic relations). The Niu et al. work (Niu et al., 2005) is most comparable to ours. We attribu</context>
</contexts>
<marker>Lee, Ng, Chia, 2004</marker>
<rawString>Y.K. Lee, H.T. Ng, and T.K. Chia. 2004. Supervised Word Sense Disambiguation with Support Vector Machines and Multiple Knowledge Sources. SENSEVAL-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Chklovski</author>
<author>A Killgariff</author>
</authors>
<title>The Senseval-3 English Lexical Sample Task.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL/SIGLEX Senseval-3.</booktitle>
<contexts>
<context position="14287" citStr="Mihalcea et al., 2004" startWordPosition="2303" endWordPosition="2306">four-letter suffix of the word; • Integer x 4: The indices of the four most frequent words that immediately precede the word in the WSJ text; • Boolean: word contains capital letters; • Boolean: word consists only of capital letters; • Boolean: word contains digits; • Boolean: word contains a hyphen; • Boolean: word contains other special characters (e.g. “&amp;”). We have also experimented with shorter suffixes and with prefixes but those features tended to degrade performance. 4.2 SENSEVAL-3 word sense disambiguation task The second task is word sense disambiguation using the SENSEVAL-3 corpus (Mihalcea et al., 2004), to enable a comparison of our method with previously published results. The goal is to disambiguate the different senses of each of 57 words given the sentences within which they occur. There are 7860 samples for training and 3944 for testing. In line with existing work (Lee and Ng, 2002; Niu et al., 2005), we use the following features: • Integer x 7: seven features consisting of the POS of the previous three words, the POS of the next three words, and the POS of the word itself. We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation. • Integer x(variable length): a bag of all wor</context>
</contexts>
<marker>Mihalcea, Chklovski, Killgariff, 2004</marker>
<rawString>R. Mihalcea, T. Chklovski, and A. Killgariff. 2004. The Senseval-3 English Lexical Sample Task. In Proceedings of ACL/SIGLEX Senseval-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>411--418</pages>
<contexts>
<context position="4059" citStr="Mihalcea, 2005" startWordPosition="593" endWordPosition="594">7 Association for Computational Linguistics nation of classifier predictions with known features. The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi are samples (feature vectors) and yi E {1, 2,... , C} are their corresponding labels; • an unlabeled set {xn+1, ... , xN}; • a distance measure d(i, j) i, j E {1,... N} defined on the feature space. The goal is to infer the labels {yn+1, ... , yN</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>R. Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of HLT/EMNLP, pages 411– 418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
<author>T Pedersen</author>
</authors>
<title>Complementarity of Lexical and Simple Syntactic Features: The SyntaLex Approach to</title>
<date>2004</date>
<booktitle>Senseval-3. Proceedings of the SENSEVAL-3.</booktitle>
<contexts>
<context position="15307" citStr="Mohammad and Pedersen, 2004" startWordPosition="2482" endWordPosition="2485">S of the previous three words, the POS of the next three words, and the POS of the word itself. We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation. • Integer x(variable length): a bag of all words in the surrounding context. • Integer x 15: Local collocations CZE (i, j are the bounds of the collocation window)—word combinations from the context of the word to disambiguate. In addition to the 11 collocations used in similar work (Lee and Ng, 2002), we also used C−3,1, C−3,2, C−2,3, C−1,3. Note that syntactic features, which have been used in some previous studies on this dataset (Mohammad and Pedersen, 2004), were not included. We apply a simple feature selection method: a feature X is selected if the conditional entropy H(Y |X) is above a fixed threshold (1 bit) in the training set, and if X also occurs in the test set (note that no label information from the test data is used for this purpose). 5 Experiments For both tasks we compare the performance of a supervised classifier, label propagation using the standard input features and either Euclidean or cosine distance, and LP using the output from a first-pass supervised classifier. 5.1 Lexicon acquisition task 5.1.1 First-pass classifier For th</context>
</contexts>
<marker>Mohammad, Pedersen, 2004</marker>
<rawString>S. Mohammad and T. Pedersen. 2004. Complementarity of Lexical and Simple Syntactic Features: The SyntaLex Approach to Senseval-3. Proceedings of the SENSEVAL-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Dong-Hong Ji</author>
<author>Chew Lim Tan</author>
</authors>
<title>Word sense disambiguation using label propagation based semisupervised learning.</title>
<date>2005</date>
<booktitle>In ACL ’05.</booktitle>
<contexts>
<context position="4168" citStr="Niu et al., 2005" startWordPosition="608" endWordPosition="611">osed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi are samples (feature vectors) and yi E {1, 2,... , C} are their corresponding labels; • an unlabeled set {xn+1, ... , xN}; • a distance measure d(i, j) i, j E {1,... N} defined on the feature space. The goal is to infer the labels {yn+1, ... , yN} for the unlabeled set. The algorithm represents all N data points as vertices in an undirected graph with w</context>
<context position="6656" citStr="Niu et al., 2005" startWordPosition="1071" endWordPosition="1074">urate, i.e. nodes connected by an edge with a high weight should have the same label. The global distance is less relevant since label information will be propagated from labeled points through the entire space. This is why LP works well with a local distance measure that might be unsuitable as a global distance measure. Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classification (Balcan et al., 2005) and retrieval (Qin et al., 2005), and protein classification (Weston et al., 2003). In NLP, label propagation has been used for word sense disambiguation (Niu et al., 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph. Currently, graph construction “is more of an art than science” (Zhu, 2005). Typically, edge weights are derived from a simple Euclidean or cosine distance measure, regardless of the nature of the underlying features. Edges are then established either by connecting all nodes, by applying a single global threshold to the edge weig</context>
<context position="14596" citStr="Niu et al., 2005" startWordPosition="2359" endWordPosition="2362">ord contains other special characters (e.g. “&amp;”). We have also experimented with shorter suffixes and with prefixes but those features tended to degrade performance. 4.2 SENSEVAL-3 word sense disambiguation task The second task is word sense disambiguation using the SENSEVAL-3 corpus (Mihalcea et al., 2004), to enable a comparison of our method with previously published results. The goal is to disambiguate the different senses of each of 57 words given the sentences within which they occur. There are 7860 samples for training and 3944 for testing. In line with existing work (Lee and Ng, 2002; Niu et al., 2005), we use the following features: • Integer x 7: seven features consisting of the POS of the previous three words, the POS of the next three words, and the POS of the word itself. We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation. • Integer x(variable length): a bag of all words in the surrounding context. • Integer x 15: Local collocations CZE (i, j are the bounds of the collocation window)—word combinations from the context of the word to disambiguate. In addition to the 11 collocations used in similar work (Lee and Ng, 2002), we also used C−3,1, C−3,2, C−2,3, C−1,3. Note that </context>
<context position="21822" citStr="Niu et al., 2005" startWordPosition="3574" endWordPosition="3577">er training set sizes. Interestingly, the LP is able to extract information from largely erroneous (noisy) distributions learned by the neural network. 5.3 Word Sense Disambiguation We compare the performance of an SVM classifier, an LP learner using the same input features as the SVM, and an LP learner using the SVM outputs as input features. To analyze the influence of training set size on accuracy, we randomly sample subsets of the training data (25%, 50%, and 75%) and use the remaining training data plus the test data as unlabeled data, similarly to the procedure followed in related work (Niu et al., 2005). The results are averaged over five different random samplings. The samplings were chosen such that there was at least one sample for each label in the training set. SENSEVAL-3 sports multi-labeled samples and 2Significance was tested using a difference of proportions significance test; the significance level is 0.01 or smaller in all cases. samples with the “unknown” label. We eliminate all samples labeled as unknown and retain only the first label for the multi-labeled instances. 5.3.1 SVM setup The use of SVM vs. MLP in this case was justified by the very small training data set. An MLP ha</context>
<context position="26950" citStr="Niu et al., 2005" startWordPosition="4463" endWordPosition="4466">ach, as shown in Table 3, which also shows accuracy results for the SVM (“SVM” label), LP over the standard graph (“LP”), and label propagation over SVM outputs (“SVM+LP”). The latter system consistently performs best in all cases, although the most marked gains occur in the upper range of labeled samples percentage. The gain of the best data-driven LP over the knowledge-based LP is significant in the 100% and 75% cases. # System Acc. (%) 1 htsa3 (Grozea, 2004) 72.9 2 IRST-kernels (Strapparava et al., 2004) 72.6 3 nusels (Lee et al., 2004) 72.4 4 SENSEVAL-3 contest baseline 55.2 5 Niu et al. (Niu et al., 2005) LP/J-S 70.3 6 Niu et al. LP/cosine 68.4 7 Niu et al. SVM 69.7 Table 2: Accuracy results of other published systems on SENSEVAL-3. 1-3 use syntactic features; 5-7 are directly comparably to our system. For comparison purposes, Table 2 shows results of other published systems against the SENSEVAL corpus. The “htsa3”, “IRST-kernels”, and “nusels” systems were the winners of the SENSEVAL-3 contest and used extra input features (syntactic relations). The Niu et al. work (Niu et al., 2005) is most comparable to ours. We attribute the slightly higher performance of our SVM due to our feature selecti</context>
</contexts>
<marker>Niu, Ji, Tan, 2005</marker>
<rawString>Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005. Word sense disambiguation using label propagation based semisupervised learning. In ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Otterbacher</author>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>Using Random Walks for Question-focused Sentence Retrieval.</title>
<date>2005</date>
<booktitle>Proceedings of HLT/EMNLP,</booktitle>
<pages>915--922</pages>
<contexts>
<context position="4086" citStr="Otterbacher et al., 2005" startWordPosition="595" endWordPosition="598">r Computational Linguistics nation of classifier predictions with known features. The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi are samples (feature vectors) and yi E {1, 2,... , C} are their corresponding labels; • an unlabeled set {xn+1, ... , xN}; • a distance measure d(i, j) i, j E {1,... N} defined on the feature space. The goal is to infer the labels {yn+1, ... , yN} for the unlabeled set. Th</context>
</contexts>
<marker>Otterbacher, Erkan, Radev, 2005</marker>
<rawString>J. Otterbacher, G. Erkan, and D.R. Radev. 2005. Using Random Walks for Question-focused Sentence Retrieval. Proceedings of HLT/EMNLP, pages 915–922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="4029" citStr="Pang and Lee, 2004" startWordPosition="587" endWordPosition="590">1, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics nation of classifier predictions with known features. The proposed approach is evaluated on a lexicon learning task using the Wall Street Journal (WSJ) corpus, and on the SENSEVAL-3 word sense disambiguation task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi are samples (feature vectors) and yi E {1, 2,... , C} are their corresponding labels; • an unlabeled set {xn+1, ... , xN}; • a distance measure d(i, j) i, j E {1,... N} defined on the feature space. The goal is to in</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Qin</author>
<author>T-Y Liu</author>
<author>X-D Zhang</author>
<author>W-Y Ma</author>
<author>H-J Zhang</author>
</authors>
<title>Subspace clustering and label propagation for active feedback in image retrieval.</title>
<date>2005</date>
<booktitle>In MMM,</booktitle>
<pages>172--179</pages>
<contexts>
<context position="6516" citStr="Qin et al., 2005" startWordPosition="1049" endWordPosition="1052"> The resulting labels for each feature xi, where i E {n + 1, ... , N}, are: fij (4) It is important that the distance measure is locally accurate, i.e. nodes connected by an edge with a high weight should have the same label. The global distance is less relevant since label information will be propagated from labeled points through the entire space. This is why LP works well with a local distance measure that might be unsuitable as a global distance measure. Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classification (Balcan et al., 2005) and retrieval (Qin et al., 2005), and protein classification (Weston et al., 2003). In NLP, label propagation has been used for word sense disambiguation (Niu et al., 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph. Currently, graph construction “is more of an art than science” (Zhu, 2005). Typically, edge weights are derived from a simple Euclidean or cosine distance measure, regardless of the nature o</context>
</contexts>
<marker>Qin, Liu, Zhang, Ma, Zhang, 2005</marker>
<rawString>T. Qin, T.-Y. Liu, X.-D. Zhang, W.-Y. Ma, and H.-J. Zhang. 2005. Subspace clustering and label propagation for active feedback in image retrieval. In MMM, pages 172–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-ofspeech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="14820" citStr="Ratnaparkhi, 1996" startWordPosition="2402" endWordPosition="2404">task is word sense disambiguation using the SENSEVAL-3 corpus (Mihalcea et al., 2004), to enable a comparison of our method with previously published results. The goal is to disambiguate the different senses of each of 57 words given the sentences within which they occur. There are 7860 samples for training and 3944 for testing. In line with existing work (Lee and Ng, 2002; Niu et al., 2005), we use the following features: • Integer x 7: seven features consisting of the POS of the previous three words, the POS of the next three words, and the POS of the word itself. We used the MXPOST tagger (Ratnaparkhi, 1996) for POS annotation. • Integer x(variable length): a bag of all words in the surrounding context. • Integer x 15: Local collocations CZE (i, j are the bounds of the collocation window)—word combinations from the context of the word to disambiguate. In addition to the 11 collocations used in similar work (Lee and Ng, 2002), we also used C−3,1, C−3,2, C−2,3, C−1,3. Note that syntactic features, which have been used in some previous studies on this dataset (Mohammad and Pedersen, 2004), were not included. We apply a simple feature selection method: a feature X is selected if the conditional entro</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-ofspeech tagging. In Proceedings of EMNLP, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Gliozzo</author>
<author>C Giuliano</author>
</authors>
<title>Pattern abstraction and term similarity for word sense disambiguation: IRST at SENSEVAL-3.</title>
<date>2004</date>
<booktitle>Proc. of SENSEVAL-3,</booktitle>
<pages>229--234</pages>
<contexts>
<context position="26845" citStr="Strapparava et al., 2004" startWordPosition="4442" endWordPosition="4445">ss of the graph produced with our method was better when compared to the graphs produced using the standard approach, as shown in Table 3, which also shows accuracy results for the SVM (“SVM” label), LP over the standard graph (“LP”), and label propagation over SVM outputs (“SVM+LP”). The latter system consistently performs best in all cases, although the most marked gains occur in the upper range of labeled samples percentage. The gain of the best data-driven LP over the knowledge-based LP is significant in the 100% and 75% cases. # System Acc. (%) 1 htsa3 (Grozea, 2004) 72.9 2 IRST-kernels (Strapparava et al., 2004) 72.6 3 nusels (Lee et al., 2004) 72.4 4 SENSEVAL-3 contest baseline 55.2 5 Niu et al. (Niu et al., 2005) LP/J-S 70.3 6 Niu et al. LP/cosine 68.4 7 Niu et al. SVM 69.7 Table 2: Accuracy results of other published systems on SENSEVAL-3. 1-3 use syntactic features; 5-7 are directly comparably to our system. For comparison purposes, Table 2 shows results of other published systems against the SENSEVAL corpus. The “htsa3”, “IRST-kernels”, and “nusels” systems were the winners of the SENSEVAL-3 contest and used extra input features (syntactic relations). The Niu et al. work (Niu et al., 2005) is mo</context>
</contexts>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation: IRST at SENSEVAL-3. Proc. of SENSEVAL-3, pages 229–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weston</author>
<author>C Leslie</author>
<author>D Zhou</author>
<author>A Elisseeff</author>
<author>W Noble</author>
</authors>
<title>Semi-supervised protein classification using cluster kernels.</title>
<date>2003</date>
<contexts>
<context position="6566" citStr="Weston et al., 2003" startWordPosition="1056" endWordPosition="1059">e i E {n + 1, ... , N}, are: fij (4) It is important that the distance measure is locally accurate, i.e. nodes connected by an edge with a high weight should have the same label. The global distance is less relevant since label information will be propagated from labeled points through the entire space. This is why LP works well with a local distance measure that might be unsuitable as a global distance measure. Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classification (Balcan et al., 2005) and retrieval (Qin et al., 2005), and protein classification (Weston et al., 2003). In NLP, label propagation has been used for word sense disambiguation (Niu et al., 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph. Currently, graph construction “is more of an art than science” (Zhu, 2005). Typically, edge weights are derived from a simple Euclidean or cosine distance measure, regardless of the nature of the underlying features. Edges are then establis</context>
</contexts>
<marker>Weston, Leslie, Zhou, Elisseeff, Noble, 2003</marker>
<rawString>J. Weston, C. Leslie, D. Zhou, A. Elisseeff, and W. Noble. 2003. Semi-supervised protein classification using cluster kernels.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report, CMU-CALD-02.</tech>
<contexts>
<context position="4320" citStr="Zhu and Ghahramani, 2002" startWordPosition="631" endWordPosition="634"> task. In both cases our technique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi are samples (feature vectors) and yi E {1, 2,... , C} are their corresponding labels; • an unlabeled set {xn+1, ... , xN}; • a distance measure d(i, j) i, j E {1,... N} defined on the feature space. The goal is to infer the labels {yn+1, ... , yN} for the unlabeled set. The algorithm represents all N data points as vertices in an undirected graph with weighted edges. Initially, only the known data vertices are labeled. The edge linking vertices i and j has weight: wij = exp(_ d(i,2 )2 J (1) α where α i</context>
<context position="6439" citStr="Zhu and Ghahramani, 2002" startWordPosition="1036" endWordPosition="1039">ep 2 fixes the known labels, which might otherwise be overriden by propagated labels. The resulting labels for each feature xi, where i E {n + 1, ... , N}, are: fij (4) It is important that the distance measure is locally accurate, i.e. nodes connected by an edge with a high weight should have the same label. The global distance is less relevant since label information will be propagated from labeled points through the entire space. This is why LP works well with a local distance measure that might be unsuitable as a global distance measure. Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classification (Balcan et al., 2005) and retrieval (Qin et al., 2005), and protein classification (Weston et al., 2003). In NLP, label propagation has been used for word sense disambiguation (Niu et al., 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph. Currently, graph construction “is more of an art than science” (Zhu, 2005). Typically, edge weights are derived f</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>X. Zhu and Z. Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical report, CMU-CALD-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-Supervised Learning with Graphs.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<pages>05--192</pages>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="2603" citStr="Zhu, 2005" startWordPosition="375" endWordPosition="376">es where the data is characterized by an underlying manifold structure and samples are judged to be similar by local similarity measures. However, the question of how to best construct the graph forming the basis of the learning procedure is still an underinvestigated research problem. NLP learning tasks present additional problems since they often rely on discrete or heterogeneous feature spaces for which standard similarity measures (such as Euclidean or cosine distance) are suboptimal. We propose a two-pass data-driven technique for graph construction in the framework of label propagation (Zhu, 2005). First, we use a supervised classifier trained on the labeled subset to transform the initial feature space (consisting of e.g. lexical, contextual, or syntactic features) into a continuous representation in the form of soft label predictions. This representation is then used as a basis for measuring similarity among samples that determines the structure of the graph used for the second, semisupervised learning step. It is important to note that, rather than simply cascading the supervised and the semi-supervised learner, we optimize the combination with respect to the properties required of </context>
<context position="4332" citStr="Zhu, 2005" startWordPosition="635" endWordPosition="636">echnique significantly outperforms our baseline systems (label propagation using standard graph construction and discriminatively trained supervised classifiers). 2 Background Several graph-based learning techniques have recently been developed and applied to NLP problems: minimum cuts (Pang and Lee, 2004), random walks (Mihalcea, 2005; Otterbacher et al., 2005), graph matching (Haghighi et al., 2005), and label propagation (Niu et al., 2005). Here we focus on label propagation as a learning technique. 2.1 Label propagation The basic label propagation (LP) algorithm (Zhu and Ghahramani, 2002; Zhu, 2005) has as inputs: • a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi are samples (feature vectors) and yi E {1, 2,... , C} are their corresponding labels; • an unlabeled set {xn+1, ... , xN}; • a distance measure d(i, j) i, j E {1,... N} defined on the feature space. The goal is to infer the labels {yn+1, ... , yN} for the unlabeled set. The algorithm represents all N data points as vertices in an undirected graph with weighted edges. Initially, only the known data vertices are labeled. The edge linking vertices i and j has weight: wij = exp(_ d(i,2 )2 J (1) α where α is a hyperpar</context>
<context position="6693" citStr="Zhu, 2005" startWordPosition="1078" endWordPosition="1079">a high weight should have the same label. The global distance is less relevant since label information will be propagated from labeled points through the entire space. This is why LP works well with a local distance measure that might be unsuitable as a global distance measure. Applications of LP include handwriting recognition (Zhu and Ghahramani, 2002), image classification (Balcan et al., 2005) and retrieval (Qin et al., 2005), and protein classification (Weston et al., 2003). In NLP, label propagation has been used for word sense disambiguation (Niu et al., 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al., 2006). 2.2 Graph construction One of the main problems in LP, as well as other graph-based learning techniques, is how to best construct the graph. Currently, graph construction “is more of an art than science” (Zhu, 2005). Typically, edge weights are derived from a simple Euclidean or cosine distance measure, regardless of the nature of the underlying features. Edges are then established either by connecting all nodes, by applying a single global threshold to the edge weights, or by connecting each node to it</context>
<context position="8344" citStr="Zhu, 2005" startWordPosition="1338" endWordPosition="1339"> and different types (e.g. continuous, binary, multi-valued), which entails the need for normalization, binning, or scaling. Finally, common distance measures do not take advantage of domain knowledge that might be available. Some attempts have been made at improving the standard method of graph construction. For instance, in a face identification task (Balcan et al., 2005), domain knowledge was used to identify three different edge sets based on time, color and face features, associating a different hyperparameter with each. The resulting graph was then created by superposing edge sets. Zhu (Zhu, 2005, Ch. 7) describes graph construction using separate α hyperparameters for each feature dimension, and presents a datadriven way (evidence maximization) for learning the values of the parameters. 3 Data-driven graph construction Unlike previous work, we propose to optimize the feature representation used for graph construction by learning it with a first-pass supervised classifier. Under this approach, similarity of samples is defined as similarity of the output values produced by a classifier applied to the original feature representation of the samples. This idea bears similarity to classifi</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>Xiaojin Zhu. 2005. Semi-Supervised Learning with Graphs. Ph.D. thesis, Carnegie Mellon University. CMU-LTI-05-192.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>