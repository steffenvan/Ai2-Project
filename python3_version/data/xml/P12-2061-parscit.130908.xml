<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009016">
<title confidence="0.9991895">
Post-ordering by Parsing for Japanese-English Statistical
Machine Translation
</title>
<author confidence="0.976683">
Isao Goto Masao Utiyama Eiichiro Sumita
</author>
<affiliation confidence="0.874609666666667">
Multilingual Translation Laboratory, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
</affiliation>
<email confidence="0.992201">
{igoto, mutiyama, eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.998585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999806071428571">
Reordering is a difficult task in translating
between widely different languages such as
Japanese and English. We employ the post-
ordering framework proposed by (Sudoh et
al., 2011b) for Japanese to English transla-
tion and improve upon the reordering method.
The existing post-ordering method reorders
a sequence of target language words in a
source language word order via SMT, while
our method reorders the sequence by: 1) pars-
ing the sequence to obtain syntax structures
similar to a source language structure, and 2)
transferring the obtained syntax structures into
the syntax structures of the target language.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.949399777777778">
The word reordering problem is a challenging one
when translating between languages with widely
different word orders such as Japanese and En-
glish. Many reordering methods have been proposed
in statistical machine translation (SMT) research.
Those methods can be classified into the following
three types:
Type-1: Conducting the target word selection and
reordering jointly. These include phrase-based SMT
</bodyText>
<note confidence="0.887926714285714">
(Koehn et al., 2003), hierarchical phrase-based SMT
(Chiang, 2007), and syntax-based SMT (Galley et
al., 2004; Ding and Palmer, 2005; Liu et al., 2006;
Liu et al., 2009).
Type-2: Pre-ordering (Xia and McCord, 2004;
Collins et al., 2005; Tromble and Eisner, 2009; Ge,
2010; Isozaki et al., 2010b; DeNero and Uszkoreit,
</note>
<bodyText confidence="0.991684060606061">
2011; Wu et al., 2011). First, these methods re-
order the source language sentence into the target
language word order. Then, they translate the re-
ordered source word sequence using SMT methods.
Type-3: Post-ordering (Sudoh et al., 2011b; Ma-
tusov et al., 2005). First, these methods translate
the source sentence almost monotonously into a se-
quence of the target language words. Then, they
reorder the translated word sequence into the target
language word order.
This paper employs the post-ordering framework
for Japanese-English translation based on the dis-
cussions given in Section 2, and improves upon the
reordering method. Our method uses syntactic struc-
tures, which are essential for improving the target
word order in translating long sentences between
Japanese (a Subject-Object-Verb (SOV) language)
and English (an SVO language).
Before explaining our method, we explain the pre-
ordering method for English to Japanese used in the
post-ordering framework.
In English-Japanese translation, Isozaki et al.
(2010b) proposed a simple pre-ordering method that
achieved the best quality in human evaluations,
which were conducted for the NTCIR-9 patent ma-
chine translation task (Sudoh et al., 2011a; Goto et
al., 2011). The method, which is called head final-
ization, simply moves syntactic heads to the end of
corresponding syntactic constituents (e.g., phrases
and clauses). This method first changes the English
word order into a word order similar to Japanese
word order using the head finalization rule. Then,
it translates (almost monotonously) the pre-ordered
</bodyText>
<page confidence="0.988447">
311
</page>
<note confidence="0.9019555">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311–316,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999415">
Figure 1: Post-ordering framework.
</figureCaption>
<figure confidence="0.8125995">
g
p
posodeng
monotone translation
</figure>
<bodyText confidence="0.995917764705882">
English words into Japanese.
There are two key reasons why this pre-ordering
method works for estimating Japanese word order.
The first reason is that Japanese is a typical head-
final language. That is, a syntactic head word comes
after nonhead (dependent) words. Second, input En-
glish sentences are parsed by a high-quality parser,
Enju (Miyao and Tsujii, 2008), which outputs syn-
tactic heads. Consequently, the parsed English in-
put sentences can be pre-ordered into a Japanese-
like word order using the head finalization rule.
Pre-ordering using the head finalization rule nat-
urally cannot be applied to Japanese-English trans-
lation, because English is not a head-final language.
If we want to pre-order Japanese sentences into an
English-like word order, we therefore have to build
complex rules (Sudoh et al., 2011b).
</bodyText>
<sectionHeader confidence="0.98878" genericHeader="method">
2 Post-ordering for Japanese to English
</sectionHeader>
<bodyText confidence="0.954613458333334">
Sudoh et al. (2011b) proposed a post-ordering
method for Japanese-English translation. The trans-
lation flow for the post-ordering method is shown in
Figure 1, where “HFE” is an abbreviation of “Head
Final English”. An HFE sentence consists of En-
glish words in a Japanese-like structure. It can be
constructed by applying the head-finalization rule
(Isozaki et al., 2010b) to an English sentence parsed
by Enju. Therefore, if good rules are applied to this
HFE sentence, the underlying English sentence can
be recovered. This is the key observation of the post-
ordering method.
The process of post-ordering translation consists
of two steps. First, the Japanese input sentence is
translated into HFE almost monotonously. Then, the
word order of HFE is changed into an English word
order.
Training for the post-ordering method is con-
ducted by first converting the English sentences in
a Japanese-English parallel corpus into HFE sen-
tences using the head-finalization rule. Next, a
monotone phrase-based Japanese-HFE SMT model
is built using the Japanese-HFE parallel corpus
HFE: he _va0 yesterday books _va2 bouht
</bodyText>
<figureCaption confidence="0.823072">
English: he (_va0) bought books (_va2) yesterd
Figure 2: Example of post-ordering by parsing.
</figureCaption>
<bodyText confidence="0.999868666666667">
whose HFE was converted from English. Finally,
an HFE-to-English word reordering model is built
using the HFE-English parallel corpus.
</bodyText>
<sectionHeader confidence="0.994548" genericHeader="method">
3 Post-ordering Models
</sectionHeader>
<subsectionHeader confidence="0.969815">
3.1 SMT Model
</subsectionHeader>
<bodyText confidence="0.999959">
Sudoh et al. (2011b) have proposed using phrase-
based SMT for converting HFE sentences into En-
glish sentences. The advantage of their method is
that they can use off-the-shelf SMT techniques for
post-ordering.
</bodyText>
<subsectionHeader confidence="0.999073">
3.2 Parsing Model
</subsectionHeader>
<bodyText confidence="0.967125411764706">
Our proposed model is called the parsing model.
The translation process for the parsing model is
shown in Figure 2. In this method, we first parse the
HFE sentence into a binary tree. We then swap the
nodes annotated with “ SW” suffixes in this binary
tree in order to produce an English sentence.
The structures of the HFE sentences, which are
used for training our parsing model, can be obtained
from the corresponding English sentences as fol-
lows.&apos; First, each English sentence in the training
Japanese-English parallel corpus is parsed into a bi-
nary tree by applying Enju. Then, for each node in
this English binary tree, the two children of each
node are swapped if its first child is the head node
(See (Isozaki et al., 2010b) for details of the head
&apos;The explanations of pseudo-particles ( va0 and va2) and
other details of the HFE is given in Section 4.2.
</bodyText>
<figure confidence="0.999260583333333">
w w
_va0 yy _va2
Parsing
S�ST
VP�SW
VP�SW
NP�ST NP�ST
Reordering
S
VP
VP
NP NP
</figure>
<page confidence="0.993581">
312
</page>
<bodyText confidence="0.999684565217392">
final rules). At the same time, these swapped nodes
are annotated with “ SW”. When the two nodes are
not swapped, they are annotated with “ ST” (indi-
cating “Straight”). A node with only one child is
not annotated with either “ ST” or “ SW”. The re-
sult is an HFE sentence in a binary tree annotated
with “ SW” and “ ST” suffixes.
Observe that the HFE sentences can be regarded
as binary trees annotated with syntax tags aug-
mented with swap/straight suffixes. Therefore, the
structures of these binary trees can be learnable by
using an off-the-shelf grammar learning algorithm.
The learned parsing model can be regarded as an
ITG model (Wu, 1997) between the HFE and En-
glish sentences. 2
In this paper, we used the Berkeley Parser (Petrov
and Klein, 2007) for learning these structures. The
HFE sentences can be parsed by using the learned
parsing model. Then the parsed structures can be
converted into their corresponding English struc-
tures by swapping the “ SW” nodes. Note that this
parsing model jointly learns how to parse and swap
the HFE sentences.
</bodyText>
<sectionHeader confidence="0.995889" genericHeader="method">
4 Detailed Explanation of Our Method
</sectionHeader>
<bodyText confidence="0.999793">
This section explains the proposed method, which
is based on the post-ordering framework using the
parsing model.
</bodyText>
<subsectionHeader confidence="0.989757">
4.1 Translation Method
</subsectionHeader>
<bodyText confidence="0.849841470588235">
First, we produce N-best HFE sentences us-
ing Japanese-to-HFE monotone phrase-based SMT.
Next, we produce K-best parse trees for each HFE
sentence by parsing, and produce English sentences
by swapping any nodes annotated with “ SW”. Then
we score the English sentences and select the En-
glish sentence with the highest score.
For the score of an English sentence, we use
the sum of the log-linear SMT model score for
Japanese-to-HFE and the logarithm of the language
model probability of the English sentence.
2There are works using the ITG model in SMT: ITG was
used for training pre-ordering models (DeNero and Uszkoreit,
2011); hierarchical phrase-based SMT (Chiang, 2007), which is
an extension of ITG; and reordering models using ITG (Chen et
al., 2009; He et al., 2010). These methods are not post-ordering
methods.
</bodyText>
<subsectionHeader confidence="0.919497">
4.2 HFE and Articles
</subsectionHeader>
<bodyText confidence="0.999605076923077">
This section describes the details of HFE sentences.
In HFE sentences: 1) Heads are final except for
coordination. 2) Pseudo-particles are inserted after
verb arguments: va0 (subject of sentence head),
va1 (subject of verb), and va2 (object of verb).
3) Articles (a, an, the) are dropped.
In our method of HFE construction, unlike that
used by (Sudoh et al., 2011b), plural nouns are left
as-is instead of converted to the singular.
Applying our parsing model to an HFE sentence
produces an English sentence that does not have
articles, but does have pseudo-particles. We re-
moved the pseudo-particles from the reordered sen-
tences before calculating the probabilities used for
the scores of the reordered sentences. A reordered
sentence without pseudo-particles is represented by
E. A language model P(E) was trained from En-
glish sentences whose articles were dropped.
In order to output a genuine English sentence E′
from E, articles must be inserted into E. A language
model trained using genuine English sentences is
used for this purpose. We try to insert one of the
articles {a, an, the} or no article for each word in E.
Then we calculate the maximum probability word
sequence through dynamic programming for obtain-
ing E′.
</bodyText>
<sectionHeader confidence="0.999122" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.941855">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.998996866666667">
We used patent sentence data for the Japanese to
English translation subtask from the NTCIR-9 and
8 (Goto et al., 2011; Fujii et al., 2010). There
were 2,000 test sentences for NTCIR-9 and 1,251
for NTCIR-8. XML entities included in the data
were decoded to UTF-8 characters before use.
We used Enju (Miyao and Tsujii, 2008) v2.4.2 for
parsing the English side of the training data. Mecab
3 v0.98 was used for the Japanese morphological
analysis. The translation model was trained using
sentences of 64 words or less from the training cor-
pus as (Sudoh et al., 2011b). We used 5-gram lan-
guage models using SRILM (Stolcke et al., 2011).
We used the Berkeley parser (Petrov and Klein,
2007) to train the parsing model for HFE and to
</bodyText>
<footnote confidence="0.959897">
3http://mecab.sourceforge.net/
</footnote>
<page confidence="0.99937">
313
</page>
<bodyText confidence="0.999883875">
parse HFE. The parsing model was trained using 0.5
million sentences randomly selected from training
sentences of 40 words or less. We used the phrase-
based SMT system Moses (Koehn et al., 2007) to
calculate the SMT score and to produce HFE sen-
tences. The distortion limit was set to 0. We used
10-best Moses outputs and 10-best parsing results
of Berkeley parser.
</bodyText>
<subsectionHeader confidence="0.886971">
5.2 Compared Methods
</subsectionHeader>
<bodyText confidence="0.9999127">
We used the following 5 comparison methods:
Phrase-based SMT (PBMT), Hierarchical phrase-
based SMT (HPBMT), String-to-tree syntax-based
SMT (SBMT), Post-ordering based on phrase-based
SMT (PO-PBMT) (Sudoh et al., 2011b), and Post-
ordering based on hierarchical phrase-based SMT
(PO-HPBMT).
We used Moses for these 5 systems. For
PO-PBMT, a distortion limit 0 was used for the
Japanese-to-HFE translation and a distortion limit
20 was used for the HFE-to-English translation.
The PO-HPBMT method changes the post-ordering
method of PO-PBMT from a phrase-based SMT
to a hierarchical phrase-based SMT. We used a
max-chart-span 15 for the hierarchical phrase-based
SMT. We used distortion limits of 12 or 20 for
PBMT and a max-chart-span 15 for HPBMT.
The parameters for SMT were tuned by MERT
using the first half of the development data with HFE
converted from English.
</bodyText>
<subsectionHeader confidence="0.841136">
5.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.989135">
We evaluated translation quality based on the case-
insensitive automatic evaluation scores of RIBES
v1.1 (Isozaki et al., 2010a) and BLEU-4. The results
are shown in Table 1.
</bodyText>
<table confidence="0.999601888888889">
Ja-to-En NTCIR-9 NTCIR-8
RIBES BLEU RIBES BLEU
Proposed 72.57 31.75 73.48 32.80
PBMT (limit 12) 68.44 29.64 69.18 30.72
PBMT (limit 20) 68.86 30.13 69.63 31.22
HPBMT 69.92 30.15 70.18 30.94
SBMT 69.22 29.53 69.87 30.37
PO-PBMT 68.81 30.39 69.80 31.71
PO-HPBMT 70.47 27.49 71.34 28.78
</table>
<tableCaption confidence="0.999954">
Table 1: Evaluation results (case insensitive).
</tableCaption>
<bodyText confidence="0.99979448">
From the results, the proposed method achieved
the best scores for both RIBES and BLEU for
NTCIR-9 and NTCIR-8 test data. Since RIBES is
sensitive to global word order and BLEU is sensitive
to local word order, the effectiveness of the proposed
method for both global and local reordering can be
demonstrated through these comparisons.
In order to investigate the effects of our post-
ordering method in detail, we conducted an “HFE-
to-English reordering” experiment, which shows the
main contribution of our post-ordering method in
the framework of post-ordering SMT as compared
with (Sudoh et al., 2011b). In this experiment, we
changed the word order of the oracle-HFE sentences
made from reference sentences into English, this is
the same way as Table 4 in (Sudoh et al., 2011b).
The results are shown in Table 2.
This results show that our post-ordering method
is more effective than PO-PBMT and PO-HPBMT.
Since RIBES is based on the rank order correla-
tion coefficient, these results show that the proposed
method correctly recovered the word order of the
English sentences. These high scores also indicate
that the parsing results for high quality HFE are
fairly trustworthy.
</bodyText>
<table confidence="0.9992782">
oracle-HFE-to-En NTCIR-9 NTCIR-8
RIBES BLEU RIBES BLEU
Proposed 94.66 80.02 94.93 79.99
PO-PBMT 77.34 62.24 78.14 63.14
PO-HPBMT 77.99 53.62 80.85 58.34
</table>
<tableCaption confidence="0.999688">
Table 2: Evaluation resutls focusing on post-ordering.
</tableCaption>
<bodyText confidence="0.999978888888889">
In these experiments, we did not compare our
method to pre-ordering methods. However, some
groups used pre-ordering methods in the NTCIR-9
Japanese to English translation subtask. The NTT-
UT (Sudoh et al., 2011a) and NAIST (Kondo et al.,
2011) groups used pre-ordering methods, but could
not produce RIBES and BLEU scores that both were
better than those of the baseline results. In contrast,
our method was able to do so.
</bodyText>
<sectionHeader confidence="0.997202" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999895142857143">
This paper has described a new post-ordering
method. The proposed method parses sentences that
consist of target language words in a source lan-
guage word order, and does reordering by transfer-
ring the syntactic structures similar to the source lan-
guage syntactic structures into the target language
syntactic structures.
</bodyText>
<page confidence="0.998752">
314
</page>
<sectionHeader confidence="0.95723" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998958990566038">
Han-Bin Chen, Jian-Cheng Wu, and Jason S. Chang.
2009. Learning Bilingual Linguistic Reordering
Model for Statistical Machine Translation. In Pro-
ceedings of Human Language Technologies: The 2009
NAACL, pages 254–262, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201–228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd ACL, pages
531–540, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
John DeNero and Jakob Uszkoreit. 2011. Inducing Sen-
tence Structure from Parallel Corpora for Reordering.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 193–
203, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Yuan Ding and Martha Palmer. 2005. Machine Transla-
tion Using Probabilistic Synchronous Dependency In-
sertion Grammars. In Proceedings of the 43rd ACL,
pages 541–548, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Take-
hito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and
Sayori Shimohata. 2010. Overview of the Patent
Translation Task at the NTCIR-8 Workshop. In Pro-
ceedings of NTCIR-8, pages 371–376.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
273–280, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Niyu Ge. 2010. A Direct Syntax-Driven Reordering
Model for Phrase-Based Machine Translation. In Pro-
ceedings of NAACL-HLT, pages 849–857, Los Ange-
les, California, June. Association for Computational
Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent Ma-
chine Translation Task at the NTCIR-9 Workshop. In
Proceedings of NTCIR-9, pages 559–578.
Yanqing He, Yu Zhou, Chengqing Zong, and Huilin
Wang. 2010. A Novel Reordering Model Based on
Multi-layer Phrase for Statistical Machine Translation.
In Proceedings of the 23rd Coling, pages 447–455,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic Eval-
uation of Translation Quality for Distant Language
Pairs. In Proceedings of the 2010 EMNLP, pages 944–
952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 244–251, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of the 2003 HLT-NAACL, pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th ACL, pages 177–180, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Shuhei Kondo, Mamoru Komachi, Yuji Matsumoto, Kat-
suhito Sudoh, Kevin Duh, and Hajime Tsukada. 2011.
Learning of Linear Ordering Problems and its Applica-
tion to J-E Patent Translation in NTCIR-9 PatentMT.
In Proceedings of NTCIR-9, pages 641–645.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proceedings of the 21st ACL, pages
609–616, Sydney, Australia, July. Association for
Computational Linguistics.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
Tree-to-Tree Translation with Packed Forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558–566, Suntec, Singapore, August.
Association for Computational Linguistics.
E. Matusov, S. Kanthak, and Hermann Ney. 2005. On
the Integration of Speech Recognition and Statistical
Machine Translation. In Proceedings of Interspeech,
pages 3177–3180.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics, Volume 34, Number 1, pages 81–88.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In NAACL-HLT, pages
404–411, Rochester, New York, April. Association for
Computational Linguistics.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and
Outlook. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop.
</reference>
<page confidence="0.989572">
315
</page>
<reference confidence="0.999644296296297">
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki
Nagata, Xianchao Wu, Takuya Matsuzaki, and
Jun’ichi Tsujii. 2011a. NTT-UT Statistical Machine
Translation in NTCIR-9 PatentMT. In Proceedings of
NTCIR-9, pages 585–592.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011b. Post-ordering
in Statistical Machine Translation. In Proceedings of
the 13th Machine Translation Summit, pages 316–323.
Roy Tromble and Jason Eisner. 2009. Learning Linear
Ordering Problems for Better Translation. In Proceed-
ings of the 2009 EMNLP, pages 1007–1016, Singa-
pore, August. Association for Computational Linguis-
tics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting Pre-
ordering Rules from Chunk-based Dependency Trees
for Japanese-to-English Translation. In Proceedings
of the 13th Machine Translation Summit, pages 300–
307.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377–403.
Fei Xia and Michael McCord. 2004. Improving a Statis-
tical MT System with Automatically Learned Rewrite
Patterns. In Proceedings of Coling, pages 508–514,
Geneva, Switzerland, Aug 23–Aug 27. COLING.
</reference>
<page confidence="0.999277">
316
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.713602">
<title confidence="0.998533">Post-ordering by Parsing for Japanese-English Statistical Machine Translation</title>
<author confidence="0.888642">Isao Goto Masao Utiyama Eiichiro Sumita</author>
<affiliation confidence="0.859621">Multilingual Translation Laboratory, MASTAR National Institute of Information and Communications</affiliation>
<address confidence="0.954868">3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289,</address>
<email confidence="0.99059">mutiyama,</email>
<abstract confidence="0.9984956">Reordering is a difficult task in translating between widely different languages such as Japanese and English. We employ the postordering framework proposed by (Sudoh et al., 2011b) for Japanese to English translation and improve upon the reordering method. The existing post-ordering method reorders a sequence of target language words in a source language word order via SMT, while our method reorders the sequence by: 1) parsing the sequence to obtain syntax structures similar to a source language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Han-Bin Chen</author>
<author>Jian-Cheng Wu</author>
<author>Jason S Chang</author>
</authors>
<title>Learning Bilingual Linguistic Reordering Model for Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 NAACL,</booktitle>
<pages>254--262</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="8936" citStr="Chen et al., 2009" startWordPosition="1403" endWordPosition="1406">entence by parsing, and produce English sentences by swapping any nodes annotated with “ SW”. Then we score the English sentences and select the English sentence with the highest score. For the score of an English sentence, we use the sum of the log-linear SMT model score for Japanese-to-HFE and the logarithm of the language model probability of the English sentence. 2There are works using the ITG model in SMT: ITG was used for training pre-ordering models (DeNero and Uszkoreit, 2011); hierarchical phrase-based SMT (Chiang, 2007), which is an extension of ITG; and reordering models using ITG (Chen et al., 2009; He et al., 2010). These methods are not post-ordering methods. 4.2 HFE and Articles This section describes the details of HFE sentences. In HFE sentences: 1) Heads are final except for coordination. 2) Pseudo-particles are inserted after verb arguments: va0 (subject of sentence head), va1 (subject of verb), and va2 (object of verb). 3) Articles (a, an, the) are dropped. In our method of HFE construction, unlike that used by (Sudoh et al., 2011b), plural nouns are left as-is instead of converted to the singular. Applying our parsing model to an HFE sentence produces an English sentence that d</context>
</contexts>
<marker>Chen, Wu, Chang, 2009</marker>
<rawString>Han-Bin Chen, Jian-Cheng Wu, and Jason S. Chang. 2009. Learning Bilingual Linguistic Reordering Model for Statistical Machine Translation. In Proceedings of Human Language Technologies: The 2009 NAACL, pages 254–262, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrase-Based Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1452" citStr="Chiang, 2007" startWordPosition="201" endWordPosition="202">rce language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence </context>
<context position="8854" citStr="Chiang, 2007" startWordPosition="1390" endWordPosition="1391"> monotone phrase-based SMT. Next, we produce K-best parse trees for each HFE sentence by parsing, and produce English sentences by swapping any nodes annotated with “ SW”. Then we score the English sentences and select the English sentence with the highest score. For the score of an English sentence, we use the sum of the log-linear SMT model score for Japanese-to-HFE and the logarithm of the language model probability of the English sentence. 2There are works using the ITG model in SMT: ITG was used for training pre-ordering models (DeNero and Uszkoreit, 2011); hierarchical phrase-based SMT (Chiang, 2007), which is an extension of ITG; and reordering models using ITG (Chen et al., 2009; He et al., 2010). These methods are not post-ordering methods. 4.2 HFE and Articles This section describes the details of HFE sentences. In HFE sentences: 1) Heads are final except for coordination. 2) Pseudo-particles are inserted after verb arguments: va0 (subject of sentence head), va1 (subject of verb), and va2 (object of verb). 3) Articles (a, an, the) are dropped. In our method of HFE construction, unlike that used by (Sudoh et al., 2011b), plural nouns are left as-is instead of converted to the singular.</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause Restructuring for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL,</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1621" citStr="Collins et al., 2005" startWordPosition="228" endWordPosition="231">roblem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated word sequence into the target language word order. This paper employs the post-ordering framework for Jap</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings of the 43rd ACL, pages 531–540, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing Sentence Structure from Parallel Corpora for Reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>193--203</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1708" citStr="DeNero and Uszkoreit, 2011" startWordPosition="242" endWordPosition="245">ent word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated word sequence into the target language word order. This paper employs the post-ordering framework for Japanese-English translation based on the discussions given in Section 2, and improves upo</context>
<context position="8808" citStr="DeNero and Uszkoreit, 2011" startWordPosition="1383" endWordPosition="1386">First, we produce N-best HFE sentences using Japanese-to-HFE monotone phrase-based SMT. Next, we produce K-best parse trees for each HFE sentence by parsing, and produce English sentences by swapping any nodes annotated with “ SW”. Then we score the English sentences and select the English sentence with the highest score. For the score of an English sentence, we use the sum of the log-linear SMT model score for Japanese-to-HFE and the logarithm of the language model probability of the English sentence. 2There are works using the ITG model in SMT: ITG was used for training pre-ordering models (DeNero and Uszkoreit, 2011); hierarchical phrase-based SMT (Chiang, 2007), which is an extension of ITG; and reordering models using ITG (Chen et al., 2009; He et al., 2010). These methods are not post-ordering methods. 4.2 HFE and Articles This section describes the details of HFE sentences. In HFE sentences: 1) Heads are final except for coordination. 2) Pseudo-particles are inserted after verb arguments: va0 (subject of sentence head), va1 (subject of verb), and va2 (object of verb). 3) Articles (a, an, the) are dropped. In our method of HFE construction, unlike that used by (Sudoh et al., 2011b), plural nouns are le</context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing Sentence Structure from Parallel Corpora for Reordering. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 193– 203, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL,</booktitle>
<pages>541--548</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1518" citStr="Ding and Palmer, 2005" startWordPosition="210" endWordPosition="213">yntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated wo</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars. In Proceedings of the 43rd ACL, pages 541–548, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
</authors>
<title>Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and Sayori Shimohata.</title>
<date>2010</date>
<booktitle>Overview of the Patent Translation Task at the NTCIR-8 Workshop. In Proceedings of NTCIR-8,</booktitle>
<pages>371--376</pages>
<marker>Fujii, 2010</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, and Sayori Shimohata. 2010. Overview of the Patent Translation Task at the NTCIR-8 Workshop. In Proceedings of NTCIR-8, pages 371–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>273--280</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="1495" citStr="Galley et al., 2004" startWordPosition="206" endWordPosition="209">erring the obtained syntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they re</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 273–280, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
</authors>
<title>A Direct Syntax-Driven Reordering Model for Phrase-Based Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>849--857</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="1657" citStr="Ge, 2010" startWordPosition="236" endWordPosition="237">ween languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated word sequence into the target language word order. This paper employs the post-ordering framework for Japanese-English translation based on t</context>
</contexts>
<marker>Ge, 2010</marker>
<rawString>Niyu Ge. 2010. A Direct Syntax-Driven Reordering Model for Phrase-Based Machine Translation. In Proceedings of NAACL-HLT, pages 849–857, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<date>2011</date>
<booktitle>Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop. In Proceedings of NTCIR-9,</booktitle>
<pages>559--578</pages>
<contexts>
<context position="2925" citStr="Goto et al., 2011" startWordPosition="427" endWordPosition="430">on the reordering method. Our method uses syntactic structures, which are essential for improving the target word order in translating long sentences between Japanese (a Subject-Object-Verb (SOV) language) and English (an SVO language). Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g., phrases and clauses). This method first changes the English word order into a word order similar to Japanese word order using the head finalization rule. Then, it translates (almost monotonously) the pre-ordered 311 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311–316, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Post-ordering framework. g p posodeng monoto</context>
<context position="10391" citStr="Goto et al., 2011" startWordPosition="1643" endWordPosition="1646">articles is represented by E. A language model P(E) was trained from English sentences whose articles were dropped. In order to output a genuine English sentence E′ from E, articles must be inserted into E. A language model trained using genuine English sentences is used for this purpose. We try to insert one of the articles {a, an, the} or no article for each word in E. Then we calculate the maximum probability word sequence through dynamic programming for obtaining E′. 5 Experiment 5.1 Setup We used patent sentence data for the Japanese to English translation subtask from the NTCIR-9 and 8 (Goto et al., 2011; Fujii et al., 2010). There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8. XML entities included in the data were decoded to UTF-8 characters before use. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. Mecab 3 v0.98 was used for the Japanese morphological analysis. The translation model was trained using sentences of 64 words or less from the training corpus as (Sudoh et al., 2011b). We used 5-gram language models using SRILM (Stolcke et al., 2011). We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop. In Proceedings of NTCIR-9, pages 559–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanqing He</author>
<author>Yu Zhou</author>
<author>Chengqing Zong</author>
<author>Huilin Wang</author>
</authors>
<title>A Novel Reordering Model Based on Multi-layer Phrase for Statistical Machine Translation.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd Coling,</booktitle>
<pages>447--455</pages>
<location>Beijing, China,</location>
<contexts>
<context position="8954" citStr="He et al., 2010" startWordPosition="1407" endWordPosition="1410"> and produce English sentences by swapping any nodes annotated with “ SW”. Then we score the English sentences and select the English sentence with the highest score. For the score of an English sentence, we use the sum of the log-linear SMT model score for Japanese-to-HFE and the logarithm of the language model probability of the English sentence. 2There are works using the ITG model in SMT: ITG was used for training pre-ordering models (DeNero and Uszkoreit, 2011); hierarchical phrase-based SMT (Chiang, 2007), which is an extension of ITG; and reordering models using ITG (Chen et al., 2009; He et al., 2010). These methods are not post-ordering methods. 4.2 HFE and Articles This section describes the details of HFE sentences. In HFE sentences: 1) Heads are final except for coordination. 2) Pseudo-particles are inserted after verb arguments: va0 (subject of sentence head), va1 (subject of verb), and va2 (object of verb). 3) Articles (a, an, the) are dropped. In our method of HFE construction, unlike that used by (Sudoh et al., 2011b), plural nouns are left as-is instead of converted to the singular. Applying our parsing model to an HFE sentence produces an English sentence that does not have artic</context>
</contexts>
<marker>He, Zhou, Zong, Wang, 2010</marker>
<rawString>Yanqing He, Yu Zhou, Chengqing Zong, and Huilin Wang. 2010. A Novel Reordering Model Based on Multi-layer Phrase for Statistical Machine Translation. In Proceedings of the 23rd Coling, pages 447–455, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
</authors>
<title>Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic Evaluation of Translation Quality for Distant Language Pairs.</title>
<booktitle>In Proceedings of the 2010 EMNLP,</booktitle>
<pages>944--952</pages>
<marker>Isozaki, Hirao, Duh, </marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic Evaluation of Translation Quality for Distant Language Pairs. In Proceedings of the 2010 EMNLP, pages 944– 952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Kevin Duh</author>
</authors>
<title>Head Finalization: A Simple Reordering Rule for SOV Languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>244--251</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1679" citStr="Isozaki et al., 2010" startWordPosition="238" endWordPosition="241">ages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated word sequence into the target language word order. This paper employs the post-ordering framework for Japanese-English translation based on the discussions given i</context>
<context position="4769" citStr="Isozaki et al., 2010" startWordPosition="708" endWordPosition="711">ese-English translation, because English is not a head-final language. If we want to pre-order Japanese sentences into an English-like word order, we therefore have to build complex rules (Sudoh et al., 2011b). 2 Post-ordering for Japanese to English Sudoh et al. (2011b) proposed a post-ordering method for Japanese-English translation. The translation flow for the post-ordering method is shown in Figure 1, where “HFE” is an abbreviation of “Head Final English”. An HFE sentence consists of English words in a Japanese-like structure. It can be constructed by applying the head-finalization rule (Isozaki et al., 2010b) to an English sentence parsed by Enju. Therefore, if good rules are applied to this HFE sentence, the underlying English sentence can be recovered. This is the key observation of the postordering method. The process of post-ordering translation consists of two steps. First, the Japanese input sentence is translated into HFE almost monotonously. Then, the word order of HFE is changed into an English word order. Training for the post-ordering method is conducted by first converting the English sentences in a Japanese-English parallel corpus into HFE sentences using the head-finalization rule.</context>
<context position="6734" citStr="Isozaki et al., 2010" startWordPosition="1027" endWordPosition="1030">re 2. In this method, we first parse the HFE sentence into a binary tree. We then swap the nodes annotated with “ SW” suffixes in this binary tree in order to produce an English sentence. The structures of the HFE sentences, which are used for training our parsing model, can be obtained from the corresponding English sentences as follows.&apos; First, each English sentence in the training Japanese-English parallel corpus is parsed into a binary tree by applying Enju. Then, for each node in this English binary tree, the two children of each node are swapped if its first child is the head node (See (Isozaki et al., 2010b) for details of the head &apos;The explanations of pseudo-particles ( va0 and va2) and other details of the HFE is given in Section 4.2. w w _va0 yy _va2 Parsing S�ST VP�SW VP�SW NP�ST NP�ST Reordering S VP VP NP NP 312 final rules). At the same time, these swapped nodes are annotated with “ SW”. When the two nodes are not swapped, they are annotated with “ ST” (indicating “Straight”). A node with only one child is not annotated with either “ ST” or “ SW”. The result is an HFE sentence in a binary tree annotated with “ SW” and “ ST” suffixes. Observe that the HFE sentences can be regarded as bina</context>
<context position="12441" citStr="Isozaki et al., 2010" startWordPosition="1974" endWordPosition="1977">ion and a distortion limit 20 was used for the HFE-to-English translation. The PO-HPBMT method changes the post-ordering method of PO-PBMT from a phrase-based SMT to a hierarchical phrase-based SMT. We used a max-chart-span 15 for the hierarchical phrase-based SMT. We used distortion limits of 12 or 20 for PBMT and a max-chart-span 15 for HPBMT. The parameters for SMT were tuned by MERT using the first half of the development data with HFE converted from English. 5.3 Results and Discussion We evaluated translation quality based on the caseinsensitive automatic evaluation scores of RIBES v1.1 (Isozaki et al., 2010a) and BLEU-4. The results are shown in Table 1. Ja-to-En NTCIR-9 NTCIR-8 RIBES BLEU RIBES BLEU Proposed 72.57 31.75 73.48 32.80 PBMT (limit 12) 68.44 29.64 69.18 30.72 PBMT (limit 20) 68.86 30.13 69.63 31.22 HPBMT 69.92 30.15 70.18 30.94 SBMT 69.22 29.53 69.87 30.37 PO-PBMT 68.81 30.39 69.80 31.71 PO-HPBMT 70.47 27.49 71.34 28.78 Table 1: Evaluation results (case insensitive). From the results, the proposed method achieved the best scores for both RIBES and BLEU for NTCIR-9 and NTCIR-8 test data. Since RIBES is sensitive to global word order and BLEU is sensitive to local word order, the effe</context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2010</marker>
<rawString>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010b. Head Finalization: A Simple Reordering Rule for SOV Languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 244–251, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 HLT-NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1406" citStr="Koehn et al., 2003" startWordPosition="194" endWordPosition="197">equence to obtain syntax structures similar to a source language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 HLT-NAACL, pages 48–54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Alexandra</location>
<contexts>
<context position="11231" citStr="Koehn et al., 2007" startWordPosition="1785" endWordPosition="1788">sing the English side of the training data. Mecab 3 v0.98 was used for the Japanese morphological analysis. The translation model was trained using sentences of 64 words or less from the training corpus as (Sudoh et al., 2011b). We used 5-gram language models using SRILM (Stolcke et al., 2011). We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for HFE and to 3http://mecab.sourceforge.net/ 313 parse HFE. The parsing model was trained using 0.5 million sentences randomly selected from training sentences of 40 words or less. We used the phrasebased SMT system Moses (Koehn et al., 2007) to calculate the SMT score and to produce HFE sentences. The distortion limit was set to 0. We used 10-best Moses outputs and 10-best parsing results of Berkeley parser. 5.2 Compared Methods We used the following 5 comparison methods: Phrase-based SMT (PBMT), Hierarchical phrasebased SMT (HPBMT), String-to-tree syntax-based SMT (SBMT), Post-ordering based on phrase-based SMT (PO-PBMT) (Sudoh et al., 2011b), and Postordering based on hierarchical phrase-based SMT (PO-HPBMT). We used Moses for these 5 systems. For PO-PBMT, a distortion limit 0 was used for the Japanese-to-HFE translation and a </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th ACL, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuhei Kondo</author>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Learning of Linear Ordering Problems and its Application to J-E Patent Translation in NTCIR-9 PatentMT.</title>
<date>2011</date>
<booktitle>In Proceedings of NTCIR-9,</booktitle>
<pages>641--645</pages>
<marker>Kondo, Komachi, Matsumoto, Sudoh, Duh, Tsukada, 2011</marker>
<rawString>Shuhei Kondo, Mamoru Komachi, Yuji Matsumoto, Katsuhito Sudoh, Kevin Duh, and Hajime Tsukada. 2011. Learning of Linear Ordering Problems and its Application to J-E Patent Translation in NTCIR-9 PatentMT. In Proceedings of NTCIR-9, pages 641–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-String Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st ACL,</booktitle>
<pages>609--616</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1536" citStr="Liu et al., 2006" startWordPosition="214" endWordPosition="217">he syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated word sequence into t</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-String Alignment Template for Statistical Machine Translation. In Proceedings of the 21st ACL, pages 609–616, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving Tree-to-Tree Translation with Packed Forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>558--566</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving Tree-to-Tree Translation with Packed Forests. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 558–566, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>S Kanthak</author>
<author>Hermann Ney</author>
</authors>
<title>On the Integration of Speech Recognition and Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<pages>3177--3180</pages>
<contexts>
<context position="1963" citStr="Matusov et al., 2005" startWordPosition="283" endWordPosition="287">ing jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated word sequence into the target language word order. This paper employs the post-ordering framework for Japanese-English translation based on the discussions given in Section 2, and improves upon the reordering method. Our method uses syntactic structures, which are essential for improving the target word order in translating long sentences between Japanese (a Subject-Object-Verb (SOV) language) and English (an SVO language). Before explaining o</context>
</contexts>
<marker>Matusov, Kanthak, Ney, 2005</marker>
<rawString>E. Matusov, S. Kanthak, and Hermann Ney. 2005. On the Integration of Speech Recognition and Statistical Machine Translation. In Proceedings of Interspeech, pages 3177–3180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature Forest Models for Probabilistic HPSG Parsing.</title>
<date>2008</date>
<booktitle>In Computational Linguistics, Volume</booktitle>
<volume>34</volume>
<pages>81--88</pages>
<contexts>
<context position="3901" citStr="Miyao and Tsujii, 2008" startWordPosition="573" endWordPosition="576">roceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 311–316, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Post-ordering framework. g p posodeng monotone translation English words into Japanese. There are two key reasons why this pre-ordering method works for estimating Japanese word order. The first reason is that Japanese is a typical headfinal language. That is, a syntactic head word comes after nonhead (dependent) words. Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. Consequently, the parsed English input sentences can be pre-ordered into a Japaneselike word order using the head finalization rule. Pre-ordering using the head finalization rule naturally cannot be applied to Japanese-English translation, because English is not a head-final language. If we want to pre-order Japanese sentences into an English-like word order, we therefore have to build complex rules (Sudoh et al., 2011b). 2 Post-ordering for Japanese to English Sudoh et al. (2011b) proposed a post-ordering method for Japanese-English translation. The translatio</context>
<context position="10597" citStr="Miyao and Tsujii, 2008" startWordPosition="1678" endWordPosition="1681">nto E. A language model trained using genuine English sentences is used for this purpose. We try to insert one of the articles {a, an, the} or no article for each word in E. Then we calculate the maximum probability word sequence through dynamic programming for obtaining E′. 5 Experiment 5.1 Setup We used patent sentence data for the Japanese to English translation subtask from the NTCIR-9 and 8 (Goto et al., 2011; Fujii et al., 2010). There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8. XML entities included in the data were decoded to UTF-8 characters before use. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. Mecab 3 v0.98 was used for the Japanese morphological analysis. The translation model was trained using sentences of 64 words or less from the training corpus as (Sudoh et al., 2011b). We used 5-gram language models using SRILM (Stolcke et al., 2011). We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for HFE and to 3http://mecab.sourceforge.net/ 313 parse HFE. The parsing model was trained using 0.5 million sentences randomly selected from training sentences of 40 words or less. We used the phrasebased SMT</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. In Computational Linguistics, Volume 34, Number 1, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing. In</title>
<date>2007</date>
<booktitle>NAACL-HLT,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="7705" citStr="Petrov and Klein, 2007" startWordPosition="1204" endWordPosition="1207">d with “ ST” (indicating “Straight”). A node with only one child is not annotated with either “ ST” or “ SW”. The result is an HFE sentence in a binary tree annotated with “ SW” and “ ST” suffixes. Observe that the HFE sentences can be regarded as binary trees annotated with syntax tags augmented with swap/straight suffixes. Therefore, the structures of these binary trees can be learnable by using an off-the-shelf grammar learning algorithm. The learned parsing model can be regarded as an ITG model (Wu, 1997) between the HFE and English sentences. 2 In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures. The HFE sentences can be parsed by using the learned parsing model. Then the parsed structures can be converted into their corresponding English structures by swapping the “ SW” nodes. Note that this parsing model jointly learns how to parse and swap the HFE sentences. 4 Detailed Explanation of Our Method This section explains the proposed method, which is based on the post-ordering framework using the parsing model. 4.1 Translation Method First, we produce N-best HFE sentences using Japanese-to-HFE monotone phrase-based SMT. Next, we produce K-best parse trees </context>
<context position="10960" citStr="Petrov and Klein, 2007" startWordPosition="1741" endWordPosition="1744">tion subtask from the NTCIR-9 and 8 (Goto et al., 2011; Fujii et al., 2010). There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8. XML entities included in the data were decoded to UTF-8 characters before use. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. Mecab 3 v0.98 was used for the Japanese morphological analysis. The translation model was trained using sentences of 64 words or less from the training corpus as (Sudoh et al., 2011b). We used 5-gram language models using SRILM (Stolcke et al., 2011). We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for HFE and to 3http://mecab.sourceforge.net/ 313 parse HFE. The parsing model was trained using 0.5 million sentences randomly selected from training sentences of 40 words or less. We used the phrasebased SMT system Moses (Koehn et al., 2007) to calculate the SMT score and to produce HFE sentences. The distortion limit was set to 0. We used 10-best Moses outputs and 10-best parsing results of Berkeley parser. 5.2 Compared Methods We used the following 5 comparison methods: Phrase-based SMT (PBMT), Hierarchical phrasebased SMT (HPBMT), String-to-tree syntax-based SM</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In NAACL-HLT, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>SRILM at Sixteen: Update and Outlook.</title>
<date>2011</date>
<booktitle>In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop.</booktitle>
<contexts>
<context position="10906" citStr="Stolcke et al., 2011" startWordPosition="1732" endWordPosition="1735">nt sentence data for the Japanese to English translation subtask from the NTCIR-9 and 8 (Goto et al., 2011; Fujii et al., 2010). There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8. XML entities included in the data were decoded to UTF-8 characters before use. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. Mecab 3 v0.98 was used for the Japanese morphological analysis. The translation model was trained using sentences of 64 words or less from the training corpus as (Sudoh et al., 2011b). We used 5-gram language models using SRILM (Stolcke et al., 2011). We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for HFE and to 3http://mecab.sourceforge.net/ 313 parse HFE. The parsing model was trained using 0.5 million sentences randomly selected from training sentences of 40 words or less. We used the phrasebased SMT system Moses (Koehn et al., 2007) to calculate the SMT score and to produce HFE sentences. The distortion limit was set to 0. We used 10-best Moses outputs and 10-best parsing results of Berkeley parser. 5.2 Compared Methods We used the following 5 comparison methods: Phrase-based SMT (PBMT), Hierarchical p</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. SRILM at Sixteen: Update and Outlook. In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
</authors>
<title>Hajime Tsukada, Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii.</title>
<booktitle>2011a. NTT-UT Statistical Machine Translation in NTCIR-9 PatentMT. In Proceedings of NTCIR-9,</booktitle>
<pages>585--592</pages>
<marker>Sudoh, Duh, </marker>
<rawString>Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii. 2011a. NTT-UT Statistical Machine Translation in NTCIR-9 PatentMT. In Proceedings of NTCIR-9, pages 585–592.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Katsuhito Sudoh</author>
<author>Xianchao Wu</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<booktitle>2011b. Post-ordering in Statistical Machine Translation. In Proceedings of the 13th Machine Translation Summit,</booktitle>
<pages>316--323</pages>
<marker>Sudoh, Wu, Duh, Tsukada, Nagata, </marker>
<rawString>Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011b. Post-ordering in Statistical Machine Translation. In Proceedings of the 13th Machine Translation Summit, pages 316–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning Linear Ordering Problems for Better Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 EMNLP,</booktitle>
<pages>1007--1016</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="1647" citStr="Tromble and Eisner, 2009" startWordPosition="232" endWordPosition="235">g one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated word sequence into the target language word order. This paper employs the post-ordering framework for Japanese-English translation </context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning Linear Ordering Problems for Better Translation. In Proceedings of the 2009 EMNLP, pages 1007–1016, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Extracting Preordering Rules from Chunk-based Dependency Trees for Japanese-to-English Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th Machine Translation Summit,</booktitle>
<pages>300--307</pages>
<contexts>
<context position="1726" citStr="Wu et al., 2011" startWordPosition="246" endWordPosition="249">nese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated word sequence into the target language word order. This paper employs the post-ordering framework for Japanese-English translation based on the discussions given in Section 2, and improves upon the reordering m</context>
</contexts>
<marker>Wu, Sudoh, Duh, Tsukada, Nagata, 2011</marker>
<rawString>Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011. Extracting Preordering Rules from Chunk-based Dependency Trees for Japanese-to-English Translation. In Proceedings of the 13th Machine Translation Summit, pages 300– 307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="7596" citStr="Wu, 1997" startWordPosition="1186" endWordPosition="1187">e swapped nodes are annotated with “ SW”. When the two nodes are not swapped, they are annotated with “ ST” (indicating “Straight”). A node with only one child is not annotated with either “ ST” or “ SW”. The result is an HFE sentence in a binary tree annotated with “ SW” and “ ST” suffixes. Observe that the HFE sentences can be regarded as binary trees annotated with syntax tags augmented with swap/straight suffixes. Therefore, the structures of these binary trees can be learnable by using an off-the-shelf grammar learning algorithm. The learned parsing model can be regarded as an ITG model (Wu, 1997) between the HFE and English sentences. 2 In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures. The HFE sentences can be parsed by using the learned parsing model. Then the parsed structures can be converted into their corresponding English structures by swapping the “ SW” nodes. Note that this parsing model jointly learns how to parse and swap the HFE sentences. 4 Detailed Explanation of Our Method This section explains the proposed method, which is based on the post-ordering framework using the parsing model. 4.1 Translation Method First, we produ</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a Statistical MT System with Automatically Learned Rewrite Patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>508--514</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1599" citStr="Xia and McCord, 2004" startWordPosition="224" endWordPosition="227"> The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011). First, these methods reorder the source language sentence into the target language word order. Then, they translate the reordered source word sequence using SMT methods. Type-3: Post-ordering (Sudoh et al., 2011b; Matusov et al., 2005). First, these methods translate the source sentence almost monotonously into a sequence of the target language words. Then, they reorder the translated word sequence into the target language word order. This paper employs the post-orde</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite Patterns. In Proceedings of Coling, pages 508–514, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>