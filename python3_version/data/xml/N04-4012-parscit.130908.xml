<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004578">
<title confidence="0.889027">
UI on the Fly: Generating a Multimodal User Interface
</title>
<author confidence="0.960054">
David Reitter Erin Marie Panttaja Fred Cummins
</author>
<affiliation confidence="0.71925">
Media Lab Europe Media Lab Europe University College Dublin
Dublin, Ireland Dublin, Ireland Dublin, Ireland
</affiliation>
<email confidence="0.99906">
{reitter,erin}@mle .media.mit.edu fred.cummins@ucd.ie
</email>
<sectionHeader confidence="0.995634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998959">
UI on the Fly is a system that dynami-
cally presents coordinated multimodal content
through natural language and a small-screen
graphical user interface. It adapts to the
user’s preferences and situation. Multimodal
Functional Unification Grammar (MUG) is a
unification-based formalism that uses rules to
generate content that is coordinated across sev-
eral communication modes. Faithful variants
are scored with a heuristic function.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99898025">
Multimodal user interfaces are everywhere. The use of
a keyboard and mouse on a desktop PC is ubiquitous,
if not natural. However, the click-then-type paradigm
of common interfaces misses the cross-modal synchro-
nization of timing and meaning that is evident in human-
human communication. With coordinated output, novice
users could get explanations (redundant content) and ex-
perienced users could receive additional (complemen-
tary) information, increasing the bandwidth of the inter-
face. Coordinated input (“put that there!”) speeds up in-
put and relieves speech recognition of notoriously hard-
to-recognize referring expressions such as names. If a
user interface is generated on the fly, it can adapt to the
situation and special needs of the user as well as to the
device.
While users are not necessarily prone to make multi-
modal inputs (Oviatt, 1999), they can still integrate com-
plementary output or use redundant output in noisy sit-
uations. Consequently, this paper deals with generating
output. We propose a grammar formalism that general-
izes decisions about how to deliver content in an adapt-
able multimodal user interface. We demonstrate it in the
context of a user interface for a mobile personal informa-
tion manager.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999914">
Since Bolt’s (1980) Put-That-There system introduced
cross-modal coordination in multimodal user input, vari-
ous projects have investigated multimodal input and out-
put methods. Users display a preference for the touch-
screen in map-based positioning acts and object selection
(Oviatt et al., 1997). WIP (Andr´e et al., 1993) and other
systems (Feiner and McKeown, 1990; Roth and Hefley,
1993) generate static multimodal documents. In an in-
teractive user interface, however, layout should remain
consistent (Woods and Roth, 1988, perceived stability).
SmartKom (Wahlster, 2002) is a recent effort that pro-
duces a multimodal user interface, using XML/XSLT
techniques to render the output. These are determinis-
tic, which makes soft constraints such as usability hard to
implement. SUPPLE (Gajos and Weld, 2004) overcomes
this problem in its model of the user and the expected
workload for various interfaces, generating a unimodal
(graphical) user interface without natural language gener-
ation elements. On the integration side, Johnston (1998)
presents a unification-based grammar that recasts multi-
modal signal fusion as a parsing problem.
Our approach employs a non-deterministic grammar to
derive variants which are evaluated with a comparatively
simple user and situation model according to their utility
(information conveyed) and the projected cognitive load
imposed on the user. It also removes the requirement in-
herent in Johnston’s system of explicitly defining rules to
integrate multimodal information.
In the following, we discuss the grammar formalism
used to create output, as well as consistency and adapta-
tion considerations.
</bodyText>
<sectionHeader confidence="0.999227" genericHeader="method">
3 Formalism
</sectionHeader>
<bodyText confidence="0.9998625">
In this section, we will explain how the Multimodal Func-
tional Unification Grammar (MUG) allows us to generate
content. Our formalism and the associated evaluation al-
gorithm work closely with a dialogue manager. As in-
put, they receive an unambiguous, language- and mode-
independent representation of the next dialogue turn.
</bodyText>
<figure confidence="0.996729971153846">
3
7 7 7 7 7 7
3 7 7 7
7 7
7 7
3 7 7
7 7 7
3 7
7 7 7
7 7 7
7
7 7 7
7
7 7 7
7 7
7 7
7 7
7 7
7 7
7 7
7 7
7 7
7
7 7 7
7 7
7 7
7 7
7 7
7
7 7 7
7
7 7 7
7 7
7 7
7
7 7 7
7 7
7 7
7 7
7 7
7 7
7 7
7
7 7 7
7
7 7 7
7 7
7 7
7 7
7 7
7 7
7 7
7
7 7 7
7 7
7 7
7 7
7 7
7
7 7 7
7
7 7 7
7 7
7 7
7
5 7 7
5 5 5 7
type askconfirmation
initiative implicit
experience novice
error none
2 type task
6 6contexttype email
6 2
6type send-email
6 6 6 2
6 6type email
6 6 2 3
6
6 6
6 6 6type contact
6
6 6to67
6
66firstname Fred 4 5
6 6 6lastname Cummins
6 6 6 6 2 3
6 6
6 6 6type contact
action6 6
6 6cc6firstname Erin
task 6 66 lastname Panttaja
6 6 email 6
from
&amp;quot;type emailaddress#
adr reitter@mle.ie
6 6 6type text
subject&amp;quot; 6 6 6content Aussie weather
6 6 6
6 6 6 &amp;quot;
6 6 6
4 4 4type text
body
content G’day mates....-Dave
</figure>
<figureCaption confidence="0.9907765">
Figure 1: Input representation: confirmation of sending
of an email
</figureCaption>
<subsectionHeader confidence="0.987057">
3.1 Dialogue acts as input
</subsectionHeader>
<bodyText confidence="0.9999860625">
Although the semantic input is independent of mode
(screen, voice) and language (Portuguese), the input se-
mantics are domain-specific. The representation uses the
following types of dialogue acts at the top level: ask for
missing information, ask for a confirmation of an action
or data, inform the user about the state of objects, or give
context-dependent help.
An example is shown in Figure 1. The input-FD spec-
ifies type of act in progress (askconfirmation), and the
details of the interaction type. It then specifies the details
of the current action, in this case, the email that the user
is sending.
Furthermore, the dialogue manager may indicate the
need to realize a certain portion of an utterance with an
attribute realize. The input format integrates with princi-
pled, object-oriented dialogue managers.
</bodyText>
<subsectionHeader confidence="0.997641">
3.2 The domain: a personal assistant.
</subsectionHeader>
<bodyText confidence="0.998949714285714">
In this example, we have constructed a personal assistant
to be used in the domain of sending email messages.
We implemented a MUG for a PDA-size handheld de-
vice with a color touch-screen (see Figure 2a). The initial
steps to adapt it to a mobile phone (Figure 2b) involved
creating a device profile that uses no GUI widgets and
associates a higher cost (see Section 5) with the screen
</bodyText>
<figure confidence="0.994674">
(a) (b)
</figure>
<figureCaption confidence="0.997488666666667">
Figure 2: a) Voice: “Do you want to send the email? Yes
or No?”. b) Voice: “Send the email regarding Aussie
weather to Fred Cummins now?”
</figureCaption>
<bodyText confidence="0.92465">
output, as the screen is smaller. All devices used have
server-driven TTS output capabilities.
</bodyText>
<subsectionHeader confidence="0.998434">
3.3 The grammar
</subsectionHeader>
<bodyText confidence="0.992199433962264">
MUG is a collection of components. Each of them spec-
ifies a realization variant for a given partial semantic or
syntactic representation. This representation may be spe-
cific to a mode or general. We call these components
functional descriptions (FDs) in the tradition of the Func-
tional Unification Grammar (Kay, 1979), from which
MUG is derived.
For each output, the MUG identifies an utterance plan,
consisting of separate constituents in the output. For
example, when we ask for missing information (“Who
would you like to send the e-mail to?”), the utterance con-
sists of an instruction and an interaction section. Such a
plan is defined in a component, as is each more specific
generation level down to the choice of GUI widgets or
lexicon entries.
MUG is based on the unification of such attribute-value
structures. Unification can be seen as a process that aug-
ments an FD with additional information. FDs are re-
cursive: a value can be atomic or a nested FD. Values in
an FD can be bound to the values in a substructure FD
(structure sharing).
To realize a semantic representation R, we unify a suit-
able grammar component FD with each m-constituent
substructure F in R, until all substructures have been ex-
panded. An m-constituent is an FD that has an attribute
path m1cat, that is, which has been designated as a con-
stituent for mode m. Note that zero or one grammar com-
ponents for a given mode can be unified with F.
Components from the grammar invoke each other by
instantiating the cat attribute in the mode-specific part of
a substructure. Figure 3 shows a component that applies
to all modes.
There may be several competing components in the
grammar. This creates the ambiguity needed to gener-
ate a variety of outputs from the same input. Each out-
put will be faithful to the original input. However, only
one variant will be optimally adapted to the given situa-
tion, user, and device (see Section 5). Our final markup
is text for the text to speech system as well as HTML to
be displayed in a browser, similar to the MATCH system
(Johnston et al., 2002).
The nested attribute-value structures and unification
are powerful principles that allow us to cover a broad
range of planning tasks, including syntactic and lexical
choices. The declarative nature of the grammar allows us
to easily add new ways to express a given semantic en-
tity. The information that each component has access to
is explicitly encapsulated by an FD.
A grammar workbench allows us to debug the genera-
tion grammar. We could improve the debugging process
with a type-hierarchy, which defines allowed attributes
for each type.
2 2 h i 3
</bodyText>
<figure confidence="0.989653">
Mode cat 1
6 6 action 3 45
6 6 type 1
6 2 6 6action 3
6 6
6 6 instruction 6&amp;quot;cat confirm-mod #
4
6Mode
6 6text 4
6 2
6&amp;quot;cat yesnolist #3
6 6 4Mode
6 user-input 5
6 text 5
6&amp;quot;cat6 6 #
askconfirmation
4 Mode
text concat([ 4 , 5 ])
</figure>
<figureCaption confidence="0.753029333333333">
Figure 3: A MUG component that handles the confirma-
tion of tasks or user input. The mode in variable Mode
may be voice or screen.
</figureCaption>
<sectionHeader confidence="0.995335" genericHeader="method">
4 Planning for Coherence
</sectionHeader>
<bodyText confidence="0.999398096774194">
Coherence is a key element in designing a multimodal
user interface, where the potential for confusion is in-
creased. Our user interface attempts to be both consis-
tent and coherent. For example, lexical choice does not
vary: it is either ‘mobile phone’ or ‘cell phone,’ but it
is the same whether it is in text or voice. This is in line
with priming effects, which are known to occur in human-
human dialogue.
Like humans (McNeill, 1992; Oviatt et al., 1997),
our system aims to be coherent and consistent across all
modes. We present redundant content, for example, by
choosing the same lexical realizations (never mix cell
phone and mobile phone). We present complementary
input in linked components. If, for example, a deictic
expression such as these two e-mails (by voice) requires
the e-mails to be put in focus on the screen, it will set a
feature accordingly in the complementary mode.
This is possible because of a very simple principle en-
coded in the generation algorithm: all components real-
izing one semantic entity must unify. Components may
still specify mode-specific information. This is done in
a feature named after the mode, so it will not interfere
with the realization instructions of a component that real-
izes the same semantic entity in another mode. The FDs
allow us to distinguish information a) that needs to be
shared across all output modes, b) that is specific to a par-
ticular output mode, or c) that requires collaboration be-
tween two modes, such as deictic pronouns. The unifica-
tion principle replaces explicit integration rules for each
coordination scheme, such as the ones used by Johnston
(1998), which accounts for the integration of user input.
</bodyText>
<sectionHeader confidence="0.703613" genericHeader="method">
5 Adaptively Choosing the Best Variant
</sectionHeader>
<bodyText confidence="0.999782038461538">
The application of the MUG generates several output
variants. They may include or exclude pieces of infor-
mation, which may be of more or less utility to the user.
(When information is being confirmed, it should be fully
described, but in later interactions, the email could be re-
ferred to as ‘it.’)
For example, several components applied to the sub-
FD for task in Figure 1 may depend more on the screen
(Figure 2a) or be redundant in screen and voice output
(Figure 2b). This allows the system to reflect a low ben-
efit for output on the screen if the user is driving a car
or to increase the cost of voice output if the user is in a
meeting, or reflect the fact that one doesn’t hear the voice
output on a mobile phone while reading the screen.
The system adapts to the user’s abilities, her prefer-
ences, and the situation she is in by choosing an appro-
priate variant. These properties are scalar, and the result-
ing constraints are to be weighted against each other in
our objective function. Each piece of output is scored ac-
cording to a simple trade-off: a) realize content where re-
quested, b) maximize utility to the user, and c) minimize
cognitive load in perceiving and analyzing the output.
These constraints are formalized in a score that is as-
signed to each variant ω, given a set of available Modes
M, a situation model &lt; α, β &gt;, a device model φ and a
utility/time trade-off coefficient λ:
</bodyText>
<figure confidence="0.574435166666667">
3
5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7
�s(ω) = λ u(e, d) + maxmEM(βmtm(ω))
&lt;e,d&gt;EE(w)
u(e, d) = P(d, � (φmαmem|realized), erealize)
mEM
</figure>
<bodyText confidence="0.9999414375">
The first part of the sum in s describes the utility ben-
efit. The function E returns a set of semantic entities
in a (substructures) and their embedding depths in d.
The function P penalizes the non-realization of requested
(attribute realize) semantic entities, while rewarding the
(possibly redundant) realization of an entity. The reward
decreases with the embedding depth d of the semantic en-
tity. (Deeper entities give less relevant details by default.)
The cognitive load (second part of the sum) is repre-
sented by a prediction of the time t,..(w) it would take to
interpret the output. This is the utterance output time for
text spoken by the text-to-speech system, or an estimated
reading time for text on the screen.
Further work will allow us to cover the range of
novice to experienced users by relying on natural lan-
guage phrases versus graphical user interface widgets.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999990178571428">
We have demonstrated a formalism that generates coher-
ent multimodal user interfaces, as well its application in
a small-screen email client. As the generation algorithm
makes use of both hard constraints and scalar scores, it
caters for adaptability. We have proven its functionality
and efficiency in a series of examples in the context of a
dialogue system, where content is generated in real-time
for various usage situations and different devices.
Further evaluation will show whether the fitness func-
tion can accurately mirror user satisfaction with a given
output variant and whether our form of adaptivity is ac-
tually an advantage to users on the go. Without a gold
standard for a generation system for dynamic multimodal
user interfaces to qualitatively compare against, con-
trolled user trials will allow us to evaluate the usability
of the interfaces we have created. Task completion times,
user frustration levels, and user satisfaction can then be
used to evaluate the success of this model of multimodal
interactions.
The underlying formalism is intended to be used in cre-
ating, using the MUG Workbench, any multimodal sys-
tem that can be constructed compositionally, using natu-
ral language and other auditory and visual components.
As possible examples for future applications, we see a
multimodal interface that allows mobile users or users
with sensory impairments to traverse information-rich so-
cial networks, and a kiosk for multimodal, multilingual
access to public transportation options.
</bodyText>
<sectionHeader confidence="0.998757" genericHeader="conclusions">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.491454">
The authors would like to thank Stefan Agamanolis,
Robert Dale, John Kelleher, Kerry Robinson, and the
anonymous reviewers. This research was partially funded
by the European Commission under the FASiL project,
contract number: IST-2001-38685.
</bodyText>
<sectionHeader confidence="0.884842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999845914893617">
E. Andr´e, W. Finkler, W. Graf, T. Rist, A. Schauder, and
W. Wahlster. 1993. Wip: The automatic synthesis of
multimodal presentations. In M. T. Maybury, editor,
Intelligent Multimedia Interfaces. AAAI Press, Menlo
Park, CA.
Richard A. Bolt. 1980. Put-that-there:voice and gesture
at the graphics interface. In Proceedings of the 7th an-
nual conference on Computer graphics and interactive
techniques, pages 262 – 270, Seattle.
Steven Feiner and Kathleen McKeown. 1990. Coordi-
nating text and graphics in explanation generation. In
Proc. ofAAAI-90, pages 442–449, Boston, MA.
Krzysztof Gajos and Daniel S. Weld. 2004. Supple: Au-
tomatically generating user interfaces. In Proceedings
ofIUI-2004, Funchal, Portugal.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Maloor.
2002. Match: An architecture for multimodal dialogue
systems. In Proceedings ofACL-2002.
Michael Johnston. 1998. Unification-based multimodal
parsing. In Proceedings of COLING-ACL 1998, pages
624–630.
Martin Kay. 1979. Functional grammar. In Proceedings
of the Fifth Meeting of the Berkeley Linguistics Society,
pages 142–158, Berkeley, CA.
David McNeill. 1992. Hand and mind: What gestures
reveal about thought. University of Chicago Press.
Sharon Oviatt, Antonella DeAngeli, and Karen Kuhn.
1997. Integration and synchronization of input modes
during multimodal human-computer interaction. In
Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 415–422. ACM
Press.
Sharon Oviatt. 1999. Ten myths of multimodal interac-
tion. Communications of the ACM, 42(11):74–81.
Steven F. Roth and William E. Hefley. 1993. Intelligent
multimedia presentation systems: Research and princi-
ples. In M. T. Maybury, editor, Intelligent Multimedia
Interfaces. AAAI Press, Menlo Park, CA.
Wolfgang Wahlster. 2002. Smartkom: Fusion and fission
of speech, gestures, and facial expressions. In Pro-
ceedings of the 1st International Workshop on Man-
Machine Symbiotic Systems, Kyoto, Japan.
David Woods and Emilie Roth. 1988. Cognitive sys-
tems engineering. In M. Helander, editor, Handbook
ofHuman-Computer Interaction, pages 1–43. Elsevier,
North Holland.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.830159">
<title confidence="0.999805">UI on the Fly: Generating a Multimodal User Interface</title>
<author confidence="0.999343">David Reitter Erin Marie Panttaja Fred Cummins</author>
<affiliation confidence="0.993354">Media Lab Europe Media Lab Europe University College Dublin</affiliation>
<address confidence="0.865667">Dublin, Ireland Dublin, Ireland Dublin, Ireland</address>
<email confidence="0.975422">.media.mit.edufred.cummins@ucd.ie</email>
<abstract confidence="0.998699454545455">UI on the Fly is a system that dynamically presents coordinated multimodal content through natural language and a small-screen graphical user interface. It adapts to the user’s preferences and situation. Multimodal Functional Unification Grammar (MUG) is a unification-based formalism that uses rules to generate content that is coordinated across several communication modes. Faithful variants are scored with a heuristic function.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Andr´e</author>
<author>W Finkler</author>
<author>W Graf</author>
<author>T Rist</author>
<author>A Schauder</author>
<author>W Wahlster</author>
</authors>
<title>Wip: The automatic synthesis of multimodal presentations.</title>
<date>1993</date>
<booktitle>Intelligent Multimedia Interfaces.</booktitle>
<editor>In M. T. Maybury, editor,</editor>
<publisher>AAAI Press,</publisher>
<location>Menlo Park, CA.</location>
<marker>Andr´e, Finkler, Graf, Rist, Schauder, Wahlster, 1993</marker>
<rawString>E. Andr´e, W. Finkler, W. Graf, T. Rist, A. Schauder, and W. Wahlster. 1993. Wip: The automatic synthesis of multimodal presentations. In M. T. Maybury, editor, Intelligent Multimedia Interfaces. AAAI Press, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Bolt</author>
</authors>
<title>Put-that-there:voice and gesture at the graphics interface.</title>
<date>1980</date>
<booktitle>In Proceedings of the 7th annual conference on Computer graphics and interactive techniques,</booktitle>
<pages>262--270</pages>
<location>Seattle.</location>
<marker>Bolt, 1980</marker>
<rawString>Richard A. Bolt. 1980. Put-that-there:voice and gesture at the graphics interface. In Proceedings of the 7th annual conference on Computer graphics and interactive techniques, pages 262 – 270, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Feiner</author>
<author>Kathleen McKeown</author>
</authors>
<title>Coordinating text and graphics in explanation generation.</title>
<date>1990</date>
<booktitle>In Proc. ofAAAI-90,</booktitle>
<pages>442--449</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2326" citStr="Feiner and McKeown, 1990" startWordPosition="345" endWordPosition="348">nerating output. We propose a grammar formalism that generalizes decisions about how to deliver content in an adaptable multimodal user interface. We demonstrate it in the context of a user interface for a mobile personal information manager. 2 Related Work Since Bolt’s (1980) Put-That-There system introduced cross-modal coordination in multimodal user input, various projects have investigated multimodal input and output methods. Users display a preference for the touchscreen in map-based positioning acts and object selection (Oviatt et al., 1997). WIP (Andr´e et al., 1993) and other systems (Feiner and McKeown, 1990; Roth and Hefley, 1993) generate static multimodal documents. In an interactive user interface, however, layout should remain consistent (Woods and Roth, 1988, perceived stability). SmartKom (Wahlster, 2002) is a recent effort that produces a multimodal user interface, using XML/XSLT techniques to render the output. These are deterministic, which makes soft constraints such as usability hard to implement. SUPPLE (Gajos and Weld, 2004) overcomes this problem in its model of the user and the expected workload for various interfaces, generating a unimodal (graphical) user interface without natur</context>
</contexts>
<marker>Feiner, McKeown, 1990</marker>
<rawString>Steven Feiner and Kathleen McKeown. 1990. Coordinating text and graphics in explanation generation. In Proc. ofAAAI-90, pages 442–449, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krzysztof Gajos</author>
<author>Daniel S Weld</author>
</authors>
<title>Supple: Automatically generating user interfaces.</title>
<date>2004</date>
<booktitle>In Proceedings ofIUI-2004,</booktitle>
<location>Funchal, Portugal.</location>
<contexts>
<context position="2765" citStr="Gajos and Weld, 2004" startWordPosition="410" endWordPosition="413">splay a preference for the touchscreen in map-based positioning acts and object selection (Oviatt et al., 1997). WIP (Andr´e et al., 1993) and other systems (Feiner and McKeown, 1990; Roth and Hefley, 1993) generate static multimodal documents. In an interactive user interface, however, layout should remain consistent (Woods and Roth, 1988, perceived stability). SmartKom (Wahlster, 2002) is a recent effort that produces a multimodal user interface, using XML/XSLT techniques to render the output. These are deterministic, which makes soft constraints such as usability hard to implement. SUPPLE (Gajos and Weld, 2004) overcomes this problem in its model of the user and the expected workload for various interfaces, generating a unimodal (graphical) user interface without natural language generation elements. On the integration side, Johnston (1998) presents a unification-based grammar that recasts multimodal signal fusion as a parsing problem. Our approach employs a non-deterministic grammar to derive variants which are evaluated with a comparatively simple user and situation model according to their utility (information conveyed) and the projected cognitive load imposed on the user. It also removes the req</context>
</contexts>
<marker>Gajos, Weld, 2004</marker>
<rawString>Krzysztof Gajos and Daniel S. Weld. 2004. Supple: Automatically generating user interfaces. In Proceedings ofIUI-2004, Funchal, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>G Vasireddy</author>
<author>A Stent</author>
<author>P Ehlen</author>
<author>M Walker</author>
<author>S Whittaker</author>
<author>P Maloor</author>
</authors>
<title>Match: An architecture for multimodal dialogue systems.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL-2002.</booktitle>
<contexts>
<context position="8380" citStr="Johnston et al., 2002" startWordPosition="1461" endWordPosition="1464">e grammar invoke each other by instantiating the cat attribute in the mode-specific part of a substructure. Figure 3 shows a component that applies to all modes. There may be several competing components in the grammar. This creates the ambiguity needed to generate a variety of outputs from the same input. Each output will be faithful to the original input. However, only one variant will be optimally adapted to the given situation, user, and device (see Section 5). Our final markup is text for the text to speech system as well as HTML to be displayed in a browser, similar to the MATCH system (Johnston et al., 2002). The nested attribute-value structures and unification are powerful principles that allow us to cover a broad range of planning tasks, including syntactic and lexical choices. The declarative nature of the grammar allows us to easily add new ways to express a given semantic entity. The information that each component has access to is explicitly encapsulated by an FD. A grammar workbench allows us to debug the generation grammar. We could improve the debugging process with a type-hierarchy, which defines allowed attributes for each type. 2 2 h i 3 Mode cat 1 6 6 action 3 45 6 6 type 1 6 2 6 6a</context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen, M. Walker, S. Whittaker, and P. Maloor. 2002. Match: An architecture for multimodal dialogue systems. In Proceedings ofACL-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
</authors>
<title>Unification-based multimodal parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>624--630</pages>
<contexts>
<context position="2999" citStr="Johnston (1998)" startWordPosition="446" endWordPosition="447">nts. In an interactive user interface, however, layout should remain consistent (Woods and Roth, 1988, perceived stability). SmartKom (Wahlster, 2002) is a recent effort that produces a multimodal user interface, using XML/XSLT techniques to render the output. These are deterministic, which makes soft constraints such as usability hard to implement. SUPPLE (Gajos and Weld, 2004) overcomes this problem in its model of the user and the expected workload for various interfaces, generating a unimodal (graphical) user interface without natural language generation elements. On the integration side, Johnston (1998) presents a unification-based grammar that recasts multimodal signal fusion as a parsing problem. Our approach employs a non-deterministic grammar to derive variants which are evaluated with a comparatively simple user and situation model according to their utility (information conveyed) and the projected cognitive load imposed on the user. It also removes the requirement inherent in Johnston’s system of explicitly defining rules to integrate multimodal information. In the following, we discuss the grammar formalism used to create output, as well as consistency and adaptation considerations. 3</context>
<context position="10942" citStr="Johnston (1998)" startWordPosition="1913" endWordPosition="1914">antic entity must unify. Components may still specify mode-specific information. This is done in a feature named after the mode, so it will not interfere with the realization instructions of a component that realizes the same semantic entity in another mode. The FDs allow us to distinguish information a) that needs to be shared across all output modes, b) that is specific to a particular output mode, or c) that requires collaboration between two modes, such as deictic pronouns. The unification principle replaces explicit integration rules for each coordination scheme, such as the ones used by Johnston (1998), which accounts for the integration of user input. 5 Adaptively Choosing the Best Variant The application of the MUG generates several output variants. They may include or exclude pieces of information, which may be of more or less utility to the user. (When information is being confirmed, it should be fully described, but in later interactions, the email could be referred to as ‘it.’) For example, several components applied to the subFD for task in Figure 1 may depend more on the screen (Figure 2a) or be redundant in screen and voice output (Figure 2b). This allows the system to reflect a lo</context>
</contexts>
<marker>Johnston, 1998</marker>
<rawString>Michael Johnston. 1998. Unification-based multimodal parsing. In Proceedings of COLING-ACL 1998, pages 624–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Functional grammar.</title>
<date>1979</date>
<booktitle>In Proceedings of the Fifth Meeting of the Berkeley Linguistics Society,</booktitle>
<pages>142--158</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="6634" citStr="Kay, 1979" startWordPosition="1153" endWordPosition="1154">ost (see Section 5) with the screen (a) (b) Figure 2: a) Voice: “Do you want to send the email? Yes or No?”. b) Voice: “Send the email regarding Aussie weather to Fred Cummins now?” output, as the screen is smaller. All devices used have server-driven TTS output capabilities. 3.3 The grammar MUG is a collection of components. Each of them specifies a realization variant for a given partial semantic or syntactic representation. This representation may be specific to a mode or general. We call these components functional descriptions (FDs) in the tradition of the Functional Unification Grammar (Kay, 1979), from which MUG is derived. For each output, the MUG identifies an utterance plan, consisting of separate constituents in the output. For example, when we ask for missing information (“Who would you like to send the e-mail to?”), the utterance consists of an instruction and an interaction section. Such a plan is defined in a component, as is each more specific generation level down to the choice of GUI widgets or lexicon entries. MUG is based on the unification of such attribute-value structures. Unification can be seen as a process that augments an FD with additional information. FDs are rec</context>
</contexts>
<marker>Kay, 1979</marker>
<rawString>Martin Kay. 1979. Functional grammar. In Proceedings of the Fifth Meeting of the Berkeley Linguistics Society, pages 142–158, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McNeill</author>
</authors>
<title>Hand and mind: What gestures reveal about thought.</title>
<date>1992</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="9748" citStr="McNeill, 1992" startWordPosition="1716" endWordPosition="1717">e text concat([ 4 , 5 ]) Figure 3: A MUG component that handles the confirmation of tasks or user input. The mode in variable Mode may be voice or screen. 4 Planning for Coherence Coherence is a key element in designing a multimodal user interface, where the potential for confusion is increased. Our user interface attempts to be both consistent and coherent. For example, lexical choice does not vary: it is either ‘mobile phone’ or ‘cell phone,’ but it is the same whether it is in text or voice. This is in line with priming effects, which are known to occur in humanhuman dialogue. Like humans (McNeill, 1992; Oviatt et al., 1997), our system aims to be coherent and consistent across all modes. We present redundant content, for example, by choosing the same lexical realizations (never mix cell phone and mobile phone). We present complementary input in linked components. If, for example, a deictic expression such as these two e-mails (by voice) requires the e-mails to be put in focus on the screen, it will set a feature accordingly in the complementary mode. This is possible because of a very simple principle encoded in the generation algorithm: all components realizing one semantic entity must uni</context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>David McNeill. 1992. Hand and mind: What gestures reveal about thought. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Oviatt</author>
<author>Antonella DeAngeli</author>
<author>Karen Kuhn</author>
</authors>
<title>Integration and synchronization of input modes during multimodal human-computer interaction.</title>
<date>1997</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>415--422</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="2255" citStr="Oviatt et al., 1997" startWordPosition="333" endWordPosition="336"> output in noisy situations. Consequently, this paper deals with generating output. We propose a grammar formalism that generalizes decisions about how to deliver content in an adaptable multimodal user interface. We demonstrate it in the context of a user interface for a mobile personal information manager. 2 Related Work Since Bolt’s (1980) Put-That-There system introduced cross-modal coordination in multimodal user input, various projects have investigated multimodal input and output methods. Users display a preference for the touchscreen in map-based positioning acts and object selection (Oviatt et al., 1997). WIP (Andr´e et al., 1993) and other systems (Feiner and McKeown, 1990; Roth and Hefley, 1993) generate static multimodal documents. In an interactive user interface, however, layout should remain consistent (Woods and Roth, 1988, perceived stability). SmartKom (Wahlster, 2002) is a recent effort that produces a multimodal user interface, using XML/XSLT techniques to render the output. These are deterministic, which makes soft constraints such as usability hard to implement. SUPPLE (Gajos and Weld, 2004) overcomes this problem in its model of the user and the expected workload for various int</context>
<context position="9770" citStr="Oviatt et al., 1997" startWordPosition="1718" endWordPosition="1721"> 4 , 5 ]) Figure 3: A MUG component that handles the confirmation of tasks or user input. The mode in variable Mode may be voice or screen. 4 Planning for Coherence Coherence is a key element in designing a multimodal user interface, where the potential for confusion is increased. Our user interface attempts to be both consistent and coherent. For example, lexical choice does not vary: it is either ‘mobile phone’ or ‘cell phone,’ but it is the same whether it is in text or voice. This is in line with priming effects, which are known to occur in humanhuman dialogue. Like humans (McNeill, 1992; Oviatt et al., 1997), our system aims to be coherent and consistent across all modes. We present redundant content, for example, by choosing the same lexical realizations (never mix cell phone and mobile phone). We present complementary input in linked components. If, for example, a deictic expression such as these two e-mails (by voice) requires the e-mails to be put in focus on the screen, it will set a feature accordingly in the complementary mode. This is possible because of a very simple principle encoded in the generation algorithm: all components realizing one semantic entity must unify. Components may sti</context>
</contexts>
<marker>Oviatt, DeAngeli, Kuhn, 1997</marker>
<rawString>Sharon Oviatt, Antonella DeAngeli, and Karen Kuhn. 1997. Integration and synchronization of input modes during multimodal human-computer interaction. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 415–422. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Oviatt</author>
</authors>
<title>Ten myths of multimodal interaction.</title>
<date>1999</date>
<journal>Communications of the ACM,</journal>
<volume>42</volume>
<issue>11</issue>
<contexts>
<context position="1571" citStr="Oviatt, 1999" startWordPosition="230" endWordPosition="231">ing that is evident in humanhuman communication. With coordinated output, novice users could get explanations (redundant content) and experienced users could receive additional (complementary) information, increasing the bandwidth of the interface. Coordinated input (“put that there!”) speeds up input and relieves speech recognition of notoriously hardto-recognize referring expressions such as names. If a user interface is generated on the fly, it can adapt to the situation and special needs of the user as well as to the device. While users are not necessarily prone to make multimodal inputs (Oviatt, 1999), they can still integrate complementary output or use redundant output in noisy situations. Consequently, this paper deals with generating output. We propose a grammar formalism that generalizes decisions about how to deliver content in an adaptable multimodal user interface. We demonstrate it in the context of a user interface for a mobile personal information manager. 2 Related Work Since Bolt’s (1980) Put-That-There system introduced cross-modal coordination in multimodal user input, various projects have investigated multimodal input and output methods. Users display a preference for the </context>
</contexts>
<marker>Oviatt, 1999</marker>
<rawString>Sharon Oviatt. 1999. Ten myths of multimodal interaction. Communications of the ACM, 42(11):74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven F Roth</author>
<author>William E Hefley</author>
</authors>
<title>Intelligent multimedia presentation systems: Research and principles.</title>
<date>1993</date>
<booktitle>Intelligent Multimedia Interfaces.</booktitle>
<editor>In M. T. Maybury, editor,</editor>
<publisher>AAAI Press,</publisher>
<location>Menlo Park, CA.</location>
<contexts>
<context position="2350" citStr="Roth and Hefley, 1993" startWordPosition="349" endWordPosition="352">e a grammar formalism that generalizes decisions about how to deliver content in an adaptable multimodal user interface. We demonstrate it in the context of a user interface for a mobile personal information manager. 2 Related Work Since Bolt’s (1980) Put-That-There system introduced cross-modal coordination in multimodal user input, various projects have investigated multimodal input and output methods. Users display a preference for the touchscreen in map-based positioning acts and object selection (Oviatt et al., 1997). WIP (Andr´e et al., 1993) and other systems (Feiner and McKeown, 1990; Roth and Hefley, 1993) generate static multimodal documents. In an interactive user interface, however, layout should remain consistent (Woods and Roth, 1988, perceived stability). SmartKom (Wahlster, 2002) is a recent effort that produces a multimodal user interface, using XML/XSLT techniques to render the output. These are deterministic, which makes soft constraints such as usability hard to implement. SUPPLE (Gajos and Weld, 2004) overcomes this problem in its model of the user and the expected workload for various interfaces, generating a unimodal (graphical) user interface without natural language generation e</context>
</contexts>
<marker>Roth, Hefley, 1993</marker>
<rawString>Steven F. Roth and William E. Hefley. 1993. Intelligent multimedia presentation systems: Research and principles. In M. T. Maybury, editor, Intelligent Multimedia Interfaces. AAAI Press, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>Smartkom: Fusion and fission of speech, gestures, and facial expressions.</title>
<date>2002</date>
<booktitle>In Proceedings of the 1st International Workshop on ManMachine Symbiotic Systems,</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="2534" citStr="Wahlster, 2002" startWordPosition="375" endWordPosition="376">onal information manager. 2 Related Work Since Bolt’s (1980) Put-That-There system introduced cross-modal coordination in multimodal user input, various projects have investigated multimodal input and output methods. Users display a preference for the touchscreen in map-based positioning acts and object selection (Oviatt et al., 1997). WIP (Andr´e et al., 1993) and other systems (Feiner and McKeown, 1990; Roth and Hefley, 1993) generate static multimodal documents. In an interactive user interface, however, layout should remain consistent (Woods and Roth, 1988, perceived stability). SmartKom (Wahlster, 2002) is a recent effort that produces a multimodal user interface, using XML/XSLT techniques to render the output. These are deterministic, which makes soft constraints such as usability hard to implement. SUPPLE (Gajos and Weld, 2004) overcomes this problem in its model of the user and the expected workload for various interfaces, generating a unimodal (graphical) user interface without natural language generation elements. On the integration side, Johnston (1998) presents a unification-based grammar that recasts multimodal signal fusion as a parsing problem. Our approach employs a non-determinis</context>
</contexts>
<marker>Wahlster, 2002</marker>
<rawString>Wolfgang Wahlster. 2002. Smartkom: Fusion and fission of speech, gestures, and facial expressions. In Proceedings of the 1st International Workshop on ManMachine Symbiotic Systems, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Woods</author>
<author>Emilie Roth</author>
</authors>
<title>Cognitive systems engineering.</title>
<date>1988</date>
<booktitle>Handbook ofHuman-Computer Interaction,</booktitle>
<pages>1--43</pages>
<editor>In M. Helander, editor,</editor>
<publisher>Elsevier, North Holland.</publisher>
<contexts>
<context position="2485" citStr="Woods and Roth, 1988" startWordPosition="368" endWordPosition="371">t in the context of a user interface for a mobile personal information manager. 2 Related Work Since Bolt’s (1980) Put-That-There system introduced cross-modal coordination in multimodal user input, various projects have investigated multimodal input and output methods. Users display a preference for the touchscreen in map-based positioning acts and object selection (Oviatt et al., 1997). WIP (Andr´e et al., 1993) and other systems (Feiner and McKeown, 1990; Roth and Hefley, 1993) generate static multimodal documents. In an interactive user interface, however, layout should remain consistent (Woods and Roth, 1988, perceived stability). SmartKom (Wahlster, 2002) is a recent effort that produces a multimodal user interface, using XML/XSLT techniques to render the output. These are deterministic, which makes soft constraints such as usability hard to implement. SUPPLE (Gajos and Weld, 2004) overcomes this problem in its model of the user and the expected workload for various interfaces, generating a unimodal (graphical) user interface without natural language generation elements. On the integration side, Johnston (1998) presents a unification-based grammar that recasts multimodal signal fusion as a parsi</context>
</contexts>
<marker>Woods, Roth, 1988</marker>
<rawString>David Woods and Emilie Roth. 1988. Cognitive systems engineering. In M. Helander, editor, Handbook ofHuman-Computer Interaction, pages 1–43. Elsevier, North Holland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>