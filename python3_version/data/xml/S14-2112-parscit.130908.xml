<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.088757">
<title confidence="0.997821">
TeamZ: Measuring Semantic Textual Similarity for Spanish Using an
Overlap-Based Approach
</title>
<author confidence="0.648239">
Anubhav Gupta
</author>
<affiliation confidence="0.3074795">
UFR SLHS
Universit´e de Franche-Comt´e
</affiliation>
<email confidence="0.991347">
anubhav.gupta@edu.univ-fcomte.fr
</email>
<sectionHeader confidence="0.993702" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796571428571">
This paper presents an overlap-based ap-
proach using bag of words and the Spanish
WordNet to solve the STS-Spanish sub-
task (STS-Es) of SemEval-2014 Task 10.
Since bag of words is the most commonly
used method to ascertain similarity, the
performance is modest.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998943666666667">
The objective of STS-Es is to score a pair of sen-
tences in Spanish on the scale of 0 (the two sen-
tences are on different topics) to 4 (the two sen-
tences are completely equivalent, as they mean the
same thing) (Agirre et al., 2014). The textual sim-
ilarity finds its utility in various NLP applications
such as information retrieval, text categorisation,
word sense disambiguation, text summarisation,
topic detection, etc. (Besanc¸on et al., 1999; Mi-
halcea et al., 2006; Islam and Inkpen, 2008).
The method presented in this paper calculates
the similarity based on the number of words that
are common in two given sentences. This ap-
proach, being simplistic, suffers from various
drawbacks. Firstly, the semantically similar sen-
tences need not have many words in common (Li
et al., 2006). Secondly, even if the sentences have
many words in common, the context in which they
are used can be different (Sahami and Heilman,
2006). For example, based on the bag of words ap-
proach, the sentences in Table 1 would be scored
the same:
However, only sentences [2] and [3] mean the
same.
Despite the flaws, this approach was used be-
cause of the Basic Principle of Compositional-
ity (Zimmermann, 2011), which states that the
</bodyText>
<footnote confidence="0.98981425">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<table confidence="0.98939775">
No. Spanish English
1 ´El es listo. He is clever.
2 ´El est´a listo. He is ready.
3 ´El est´a preparado. He is prepared.
</table>
<tableCaption confidence="0.999878">
Table 1: Examples.
</tableCaption>
<bodyText confidence="0.999695625">
meaning of a complex expression depends upon
the meaning of its components and the man-
ner in which they are composed. Furthermore,
mainly nouns were considered in the bag of words
because Spanish is an exocentric language, and
nouns contain more specific, concrete semantic
information than verbs (Michael Herslund, 2010;
Michael Herslund, 2012).
</bodyText>
<sectionHeader confidence="0.98941" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.998338055555556">
The training dataset provided for the task con-
sisted of 65 pairs of sentences along with their cor-
responding similarity scores. There were two test
sets: one consisted of 480 sentence pairs from a
news corpus, and the other had 324 sentence pairs
taken from Wikipedia.
The approach consisted of learning the scoring
with the help of linear regression. Two runs were
submitted as solutions. The first run used three-
feature vectors, whereas the second one used four-
feature vectors. The features are the Jaccard in-
dices for the lemmas, noun lemmas, synsets, and
noun subjects in each sentence pair. For both runs,
the sentence pairs were parsed using the TreeTag-
ger (Schmid, 1994). The TreeTagger was used be-
cause it provides the part-of-speech tag and lemma
for each word of a sentence.
Run 1 used these features:
</bodyText>
<listItem confidence="0.8634358">
• The fraction of lemmas that were common
between the two sentences. In other words,
the number of unique lemmas common be-
tween the sentences divided by the total num-
ber of unique lemmas of the two sentences.
</listItem>
<page confidence="0.965886">
633
</page>
<note confidence="0.871764">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 633–635,
Dublin, Ireland, August 23-24, 2014.
</note>
<listItem confidence="0.964895333333333">
• The fraction of noun lemmas common be-
tween the two sentences.
• The fraction of synsets common between the
</listItem>
<bodyText confidence="0.985812375">
two sentences. For each noun, its correspond-
ing synset1 was extracted from the Span-
ish WordNet (spaWN) of the Multilingual
Central Repository2 (MCR 3.0) (Gonzalez-
Agirre et al., 2012).
Run 2 employed one more feature in addition
to the aforementioned, which was the fraction of
synsets of noun subjects that were common for
each sentence pair. The subject nouns were ex-
tracted from the sentences after parsing them with
the MaltParser (Nivre et al., 2007). Since the Tree-
Tagger PoS tagset3 differed from the EAGLES
(Expert Advisory Group on Language Engineer-
ing Standards) tagset4 required by the MaltParser,
rules were written to best translate the TreeTag-
ger tags into EAGLES tags. However, one-to-
one mapping was not possible: EAGLES tags are
seven characters long and encode number and gen-
der, whereas TreeTagger tags do not. For example,
using the EAGLES tagset, the masculine singular
common noun ´arbol ‘tree’ is tagged as NCMS000,
whereas the feminine singular common noun hoja
‘leaf’ is tagged as NCFS000; TreeTagger, on the
other hand, tags both as NC.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="background">
3 Results and Conclusions
</sectionHeader>
<bodyText confidence="0.92604375">
Table 2 presents the performance, measured us-
ing the Pearson correlation, of the approach. Run
1 achieved a weighted correlation of 0.66723 and
ranked 15th among 22 submissions to the task.
</bodyText>
<table confidence="0.9998865">
Dataset Run 1 Run 2
Training 0.83693 0.83773
Wikipedia (Test) 0.61020 0.60425
News (Test) 0.71654 0.70974
</table>
<tableCaption confidence="0.999632">
Table 2: Performance of the Approach.
</tableCaption>
<bodyText confidence="0.99972675">
Given that the approach relied mostly on bag
of words, a modest performance was expected.
The performance was also affected by the fact
that the spaWN did not have synsets for most of
</bodyText>
<footnote confidence="0.999024">
1stored as synset offset in wei spa-30 variant.tsv
2The resource can be obtained from
http://grial.uab.es/descarregues.php
3http://www.cis.uni-muenchen.de/⇠schmid/tools/
TreeTagger/data/spanish-tagset.txt
4http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.html
</footnote>
<bodyText confidence="0.999740375">
the words. Finally, converting TreeTagger tags to
those required by the MaltParser instead of using
a parser which annotates with EAGLES tags may
also have contributed to the relatively low Run 2
score. However, the confidence intervals of the
two runs obtained after bootstrapping overlapped.
Thus, the difference between the two runs for both
the datasets is not statistically significant.
</bodyText>
<sectionHeader confidence="0.969633" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9864435">
I would like to thank Vlad Niculae, `Angels Catena
and Calvin Cheng for their inputs and feedback.
</bodyText>
<sectionHeader confidence="0.994776" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999540225000001">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. “SemEval-2014 Task 10: Multilingual
Semantic Textual Similarity.” In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014). Dublin, Ireland.
Romaric Besanc¸on, Martin Rajman, and Jean-C´edric
Chappelier. 1999. Textual Similarities Based on a
Distributional Approach. In Proceedings of the 10th
International Workshop on Database &amp; Expert Sys-
tems Applications. 180–184. DEXA ‘99. Washing-
ton, DC, USA: IEEE Computer Society.
Aitor Gonzalez-Agirre, Egoitz Laparra, and German
Rigau. 2012. Multilingual Central Repository ver-
sion 3.0: upgrading a very large lexical knowledge
base. In Proceedings of the Sixth International
Global WordNet Conference (GWC ‘12).
Aminul Islam and Diana Inkpen. 2008. Semantic Text
Similarity Using Corpus-Based Word Similarity and
String Similarity. ACM Transactions on Knowledge
Discovery from Data 2 (2): 1–25.
Michael Herslund. 2010. Predicati e sostantivi comp-
lessi. In Language, Cognition and Identity, eds. Irn
Korzen and Emanuela Cresti. 1–9. Strumenti per La
Didattica E La Ricerca. Firenze University Press.
Michael Herslund. 2012. Structures lexicales et ty-
pologie. In S´emantique et lexicologie des langues
d’Europe, eds. Louis Begioni and Christine Brac-
quenier. 35–52. Rivages Linguistiques. Presses Uni-
versitaires de Rennes.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. 2006. Corpus-Based and Knowledge-Based
Measures of Text Semantic Similarity. In Proceed-
ings of the 21st National Conference on Artifi-
cial Intelligence. 775–80. AAAI‘06. Boston, Mas-
sachusetts: AAAI Press.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2006. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
</reference>
<page confidence="0.98601">
634
</page>
<reference confidence="0.998812181818182">
IEEE Transactions on Knowledge and Data Engi-
neering, 18 (8): 1138–50.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨uls¸en Eryiˇgit, Sandra K¨ubler, Svetovlas
Marinov, and Erwin Marsi. 2007. MaltParser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering, 13
(2): 95–135.
Mehran Sahami and Timothy D. Heilman. 2006. A
Web-Based Kernel Function for Measuring the Sim-
ilarity of Short Text Snippets. In Proceedings of the
15th International Conference on World Wide Web,
377–86. WWW ’06. New York, NY, USA: ACM.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. Proceedings of Inter-
national Conference on New Methods in Language
Processing. Manchester, UK.
Thomas Ede Zimmermann. 2011. Model-theoretic
semantics. Semantics. An International Handbook
of Natural Language Meaning. edited by Claudia
Maienborn, Klaus von Heusinger, and Paul Portner.
Vol. 1. Berlin, Boston: De Gruyter Mouton.
</reference>
<page confidence="0.998787">
635
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.578466">
<title confidence="0.9930315">TeamZ: Measuring Semantic Textual Similarity for Spanish Using Overlap-Based Approach</title>
<author confidence="0.71293">Anubhav</author>
<affiliation confidence="0.8322485">UFR Universit´e de</affiliation>
<email confidence="0.811916">anubhav.gupta@edu.univ-fcomte.fr</email>
<abstract confidence="0.99797675">This paper presents an overlap-based approach using bag of words and the Spanish WordNet to solve the STS-Spanish subtask (STS-Es) of SemEval-2014 Task 10. Since bag of words is the most commonly used method to ascertain similarity, the performance is modest.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 10: Multilingual Semantic Textual Similarity.” In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="689" citStr="Agirre et al., 2014" startWordPosition="107" endWordPosition="110">rlap-Based Approach Anubhav Gupta UFR SLHS Universit´e de Franche-Comt´e anubhav.gupta@edu.univ-fcomte.fr Abstract This paper presents an overlap-based approach using bag of words and the Spanish WordNet to solve the STS-Spanish subtask (STS-Es) of SemEval-2014 Task 10. Since bag of words is the most commonly used method to ascertain similarity, the performance is modest. 1 Introduction The objective of STS-Es is to score a pair of sentences in Spanish on the scale of 0 (the two sentences are on different topics) to 4 (the two sentences are completely equivalent, as they mean the same thing) (Agirre et al., 2014). The textual similarity finds its utility in various NLP applications such as information retrieval, text categorisation, word sense disambiguation, text summarisation, topic detection, etc. (Besanc¸on et al., 1999; Mihalcea et al., 2006; Islam and Inkpen, 2008). The method presented in this paper calculates the similarity based on the number of words that are common in two given sentences. This approach, being simplistic, suffers from various drawbacks. Firstly, the semantically similar sentences need not have many words in common (Li et al., 2006). Secondly, even if the sentences have many </context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. “SemEval-2014 Task 10: Multilingual Semantic Textual Similarity.” In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014). Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Romaric Besanc¸on</author>
<author>Martin Rajman</author>
<author>Jean-C´edric Chappelier</author>
</authors>
<title>Textual Similarities Based on a Distributional Approach.</title>
<date>1999</date>
<booktitle>In Proceedings of the 10th International Workshop on Database &amp; Expert Systems Applications. 180–184. DEXA ‘99.</booktitle>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA:</location>
<marker>Besanc¸on, Rajman, Chappelier, 1999</marker>
<rawString>Romaric Besanc¸on, Martin Rajman, and Jean-C´edric Chappelier. 1999. Textual Similarities Based on a Distributional Approach. In Proceedings of the 10th International Workshop on Database &amp; Expert Systems Applications. 180–184. DEXA ‘99. Washington, DC, USA: IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitor Gonzalez-Agirre</author>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Multilingual Central Repository version 3.0: upgrading a very large lexical knowledge base.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Global WordNet Conference (GWC ‘12).</booktitle>
<marker>Gonzalez-Agirre, Laparra, Rigau, 2012</marker>
<rawString>Aitor Gonzalez-Agirre, Egoitz Laparra, and German Rigau. 2012. Multilingual Central Repository version 3.0: upgrading a very large lexical knowledge base. In Proceedings of the Sixth International Global WordNet Conference (GWC ‘12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data</journal>
<volume>2</volume>
<issue>2</issue>
<pages>1--25</pages>
<contexts>
<context position="952" citStr="Islam and Inkpen, 2008" startWordPosition="145" endWordPosition="148">4 Task 10. Since bag of words is the most commonly used method to ascertain similarity, the performance is modest. 1 Introduction The objective of STS-Es is to score a pair of sentences in Spanish on the scale of 0 (the two sentences are on different topics) to 4 (the two sentences are completely equivalent, as they mean the same thing) (Agirre et al., 2014). The textual similarity finds its utility in various NLP applications such as information retrieval, text categorisation, word sense disambiguation, text summarisation, topic detection, etc. (Besanc¸on et al., 1999; Mihalcea et al., 2006; Islam and Inkpen, 2008). The method presented in this paper calculates the similarity based on the number of words that are common in two given sentences. This approach, being simplistic, suffers from various drawbacks. Firstly, the semantically similar sentences need not have many words in common (Li et al., 2006). Secondly, even if the sentences have many words in common, the context in which they are used can be different (Sahami and Heilman, 2006). For example, based on the bag of words approach, the sentences in Table 1 would be scored the same: However, only sentences [2] and [3] mean the same. Despite the fla</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity. ACM Transactions on Knowledge Discovery from Data 2 (2): 1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Herslund</author>
</authors>
<title>Predicati e sostantivi complessi.</title>
<date>2010</date>
<booktitle>In Language, Cognition and Identity, eds. Irn Korzen and Emanuela Cresti. 1–9. Strumenti per La Didattica E La Ricerca.</booktitle>
<publisher>Firenze University Press.</publisher>
<contexts>
<context position="2344" citStr="Herslund, 2010" startWordPosition="374" endWordPosition="375"> 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ No. Spanish English 1 ´El es listo. He is clever. 2 ´El est´a listo. He is ready. 3 ´El est´a preparado. He is prepared. Table 1: Examples. meaning of a complex expression depends upon the meaning of its components and the manner in which they are composed. Furthermore, mainly nouns were considered in the bag of words because Spanish is an exocentric language, and nouns contain more specific, concrete semantic information than verbs (Michael Herslund, 2010; Michael Herslund, 2012). 2 Methodology The training dataset provided for the task consisted of 65 pairs of sentences along with their corresponding similarity scores. There were two test sets: one consisted of 480 sentence pairs from a news corpus, and the other had 324 sentence pairs taken from Wikipedia. The approach consisted of learning the scoring with the help of linear regression. Two runs were submitted as solutions. The first run used threefeature vectors, whereas the second one used fourfeature vectors. The features are the Jaccard indices for the lemmas, noun lemmas, synsets, and </context>
</contexts>
<marker>Herslund, 2010</marker>
<rawString>Michael Herslund. 2010. Predicati e sostantivi complessi. In Language, Cognition and Identity, eds. Irn Korzen and Emanuela Cresti. 1–9. Strumenti per La Didattica E La Ricerca. Firenze University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Herslund</author>
</authors>
<title>Structures lexicales et typologie.</title>
<date>2012</date>
<booktitle>In S´emantique et lexicologie des langues d’Europe, eds. Louis Begioni and Christine Bracquenier. 35–52. Rivages Linguistiques. Presses Universitaires de Rennes.</booktitle>
<contexts>
<context position="2369" citStr="Herslund, 2012" startWordPosition="377" endWordPosition="378">ce. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ No. Spanish English 1 ´El es listo. He is clever. 2 ´El est´a listo. He is ready. 3 ´El est´a preparado. He is prepared. Table 1: Examples. meaning of a complex expression depends upon the meaning of its components and the manner in which they are composed. Furthermore, mainly nouns were considered in the bag of words because Spanish is an exocentric language, and nouns contain more specific, concrete semantic information than verbs (Michael Herslund, 2010; Michael Herslund, 2012). 2 Methodology The training dataset provided for the task consisted of 65 pairs of sentences along with their corresponding similarity scores. There were two test sets: one consisted of 480 sentence pairs from a news corpus, and the other had 324 sentence pairs taken from Wikipedia. The approach consisted of learning the scoring with the help of linear regression. Two runs were submitted as solutions. The first run used threefeature vectors, whereas the second one used fourfeature vectors. The features are the Jaccard indices for the lemmas, noun lemmas, synsets, and noun subjects in each sen</context>
</contexts>
<marker>Herslund, 2012</marker>
<rawString>Michael Herslund. 2012. Structures lexicales et typologie. In S´emantique et lexicologie des langues d’Europe, eds. Louis Begioni and Christine Bracquenier. 35–52. Rivages Linguistiques. Presses Universitaires de Rennes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-Based and Knowledge-Based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence. 775–80. AAAI‘06.</booktitle>
<publisher>AAAI Press.</publisher>
<location>Boston, Massachusetts:</location>
<contexts>
<context position="927" citStr="Mihalcea et al., 2006" startWordPosition="140" endWordPosition="144">(STS-Es) of SemEval-2014 Task 10. Since bag of words is the most commonly used method to ascertain similarity, the performance is modest. 1 Introduction The objective of STS-Es is to score a pair of sentences in Spanish on the scale of 0 (the two sentences are on different topics) to 4 (the two sentences are completely equivalent, as they mean the same thing) (Agirre et al., 2014). The textual similarity finds its utility in various NLP applications such as information retrieval, text categorisation, word sense disambiguation, text summarisation, topic detection, etc. (Besanc¸on et al., 1999; Mihalcea et al., 2006; Islam and Inkpen, 2008). The method presented in this paper calculates the similarity based on the number of words that are common in two given sentences. This approach, being simplistic, suffers from various drawbacks. Firstly, the semantically similar sentences need not have many words in common (Li et al., 2006). Secondly, even if the sentences have many words in common, the context in which they are used can be different (Sahami and Heilman, 2006). For example, based on the bag of words approach, the sentences in Table 1 would be scored the same: However, only sentences [2] and [3] mean </context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-Based and Knowledge-Based Measures of Text Semantic Similarity. In Proceedings of the 21st National Conference on Artificial Intelligence. 775–80. AAAI‘06. Boston, Massachusetts: AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence Similarity Based on Semantic Nets and Corpus Statistics.</title>
<date>2006</date>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bandar, James D. O’Shea, and Keeley Crockett. 2006. Sentence Similarity Based on Semantic Nets and Corpus Statistics.</rawString>
</citation>
<citation valid="false">
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<issue>8</issue>
<pages>1138--50</pages>
<marker></marker>
<rawString>IEEE Transactions on Knowledge and Data Engineering, 18 (8): 1138–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨uls¸en Eryiˇgit, Sandra K¨ubler, Svetovlas Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>95--135</pages>
<contexts>
<context position="4110" citStr="Nivre et al., 2007" startWordPosition="667" endWordPosition="670">4), pages 633–635, Dublin, Ireland, August 23-24, 2014. • The fraction of noun lemmas common between the two sentences. • The fraction of synsets common between the two sentences. For each noun, its corresponding synset1 was extracted from the Spanish WordNet (spaWN) of the Multilingual Central Repository2 (MCR 3.0) (GonzalezAgirre et al., 2012). Run 2 employed one more feature in addition to the aforementioned, which was the fraction of synsets of noun subjects that were common for each sentence pair. The subject nouns were extracted from the sentences after parsing them with the MaltParser (Nivre et al., 2007). Since the TreeTagger PoS tagset3 differed from the EAGLES (Expert Advisory Group on Language Engineering Standards) tagset4 required by the MaltParser, rules were written to best translate the TreeTagger tags into EAGLES tags. However, one-toone mapping was not possible: EAGLES tags are seven characters long and encode number and gender, whereas TreeTagger tags do not. For example, using the EAGLES tagset, the masculine singular common noun ´arbol ‘tree’ is tagged as NCMS000, whereas the feminine singular common noun hoja ‘leaf’ is tagged as NCFS000; TreeTagger, on the other hand, tags both </context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨uls¸en Eryiˇgit, Sandra K¨ubler, Svetovlas Marinov, and Erwin Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13 (2): 95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A Web-Based Kernel Function for Measuring the Similarity of Short Text Snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th International Conference on World Wide Web, 377–86. WWW ’06.</booktitle>
<publisher>ACM.</publisher>
<location>New York, NY, USA:</location>
<contexts>
<context position="1384" citStr="Sahami and Heilman, 2006" startWordPosition="217" endWordPosition="220"> such as information retrieval, text categorisation, word sense disambiguation, text summarisation, topic detection, etc. (Besanc¸on et al., 1999; Mihalcea et al., 2006; Islam and Inkpen, 2008). The method presented in this paper calculates the similarity based on the number of words that are common in two given sentences. This approach, being simplistic, suffers from various drawbacks. Firstly, the semantically similar sentences need not have many words in common (Li et al., 2006). Secondly, even if the sentences have many words in common, the context in which they are used can be different (Sahami and Heilman, 2006). For example, based on the bag of words approach, the sentences in Table 1 would be scored the same: However, only sentences [2] and [3] mean the same. Despite the flaws, this approach was used because of the Basic Principle of Compositionality (Zimmermann, 2011), which states that the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ No. Spanish English 1 ´El es listo. He is clever. 2 ´El est´a listo. He is ready. 3 ´El est´a prepar</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D. Heilman. 2006. A Web-Based Kernel Function for Measuring the Similarity of Short Text Snippets. In Proceedings of the 15th International Conference on World Wide Web, 377–86. WWW ’06. New York, NY, USA: ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>Proceedings of International Conference on New Methods in Language Processing.</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="3062" citStr="Schmid, 1994" startWordPosition="493" endWordPosition="494"> of sentences along with their corresponding similarity scores. There were two test sets: one consisted of 480 sentence pairs from a news corpus, and the other had 324 sentence pairs taken from Wikipedia. The approach consisted of learning the scoring with the help of linear regression. Two runs were submitted as solutions. The first run used threefeature vectors, whereas the second one used fourfeature vectors. The features are the Jaccard indices for the lemmas, noun lemmas, synsets, and noun subjects in each sentence pair. For both runs, the sentence pairs were parsed using the TreeTagger (Schmid, 1994). The TreeTagger was used because it provides the part-of-speech tag and lemma for each word of a sentence. Run 1 used these features: • The fraction of lemmas that were common between the two sentences. In other words, the number of unique lemmas common between the sentences divided by the total number of unique lemmas of the two sentences. 633 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 633–635, Dublin, Ireland, August 23-24, 2014. • The fraction of noun lemmas common between the two sentences. • The fraction of synsets common between the two se</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings of International Conference on New Methods in Language Processing. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Ede Zimmermann</author>
</authors>
<title>Model-theoretic semantics. Semantics. An International Handbook of Natural Language Meaning. edited by Claudia Maienborn,</title>
<date>2011</date>
<volume>1</volume>
<institution>De Gruyter Mouton.</institution>
<location>Klaus</location>
<contexts>
<context position="1648" citStr="Zimmermann, 2011" startWordPosition="266" endWordPosition="267">ber of words that are common in two given sentences. This approach, being simplistic, suffers from various drawbacks. Firstly, the semantically similar sentences need not have many words in common (Li et al., 2006). Secondly, even if the sentences have many words in common, the context in which they are used can be different (Sahami and Heilman, 2006). For example, based on the bag of words approach, the sentences in Table 1 would be scored the same: However, only sentences [2] and [3] mean the same. Despite the flaws, this approach was used because of the Basic Principle of Compositionality (Zimmermann, 2011), which states that the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ No. Spanish English 1 ´El es listo. He is clever. 2 ´El est´a listo. He is ready. 3 ´El est´a preparado. He is prepared. Table 1: Examples. meaning of a complex expression depends upon the meaning of its components and the manner in which they are composed. Furthermore, mainly nouns were considered in the bag of words because Spanish is an exocentric language, a</context>
</contexts>
<marker>Zimmermann, 2011</marker>
<rawString>Thomas Ede Zimmermann. 2011. Model-theoretic semantics. Semantics. An International Handbook of Natural Language Meaning. edited by Claudia Maienborn, Klaus von Heusinger, and Paul Portner. Vol. 1. Berlin, Boston: De Gruyter Mouton.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>