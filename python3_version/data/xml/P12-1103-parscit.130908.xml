<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.989634">
Improve SMT Quality with Automatically Extracted Paraphrase Rules
</title>
<author confidence="0.999784">
Wei He1, Hua Wu2, Haifeng Wang2, Ting Liu1*
</author>
<affiliation confidence="0.9896265">
1Research Center for Social Computing and Information
Retrieval, Harbin Institute of Technology
</affiliation>
<email confidence="0.760952666666667">
{whe,tliu}@ir.hit.edu.cn
2Baidu
{wu_hua,wanghaifeng}@baidu.com
</email>
<sectionHeader confidence="0.995589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999995666666667">
We propose a novel approach to improve
SMT via paraphrase rules which are
automatically extracted from the bilingual
training data. Without using extra
paraphrase resources, we acquire the rules
by comparing the source side of the parallel
corpus with the target-to-source
translations of the target side. Besides the
word and phrase paraphrases, the acquired
paraphrase rules mainly cover the
structured paraphrases on the sentence
level. These rules are employed to enrich
the SMT inputs for translation quality
improvement. The experimental results
show that our proposed approach achieves
significant improvements of 1.6~3.6 points
of BLEU in the oral domain and 0.5~1
points in the news domain.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894909090909">
The translation quality of the SMT system is
highly related to the coverage of translation models.
However, no matter how much data is used for
training, it is still impossible to completely cover
the unlimited input sentences. This problem is
more serious for online SMT systems in real-world
applications. Naturally, a solution to the coverage
problem is to bridge the gaps between the input
sentences and the translation models, either from
the input side, which targets on rewriting the input
sentences to the MT-favored expressions, or from
</bodyText>
<note confidence="0.6870095">
This work was done when the first author was visiting Baidu.
*Correspondence author: tliu@ir.hit.edu.cn
</note>
<bodyText confidence="0.999329457142857">
the side of translation models, which tries to enrich
the translation models to cover more expressions.
In recent years, paraphrasing has been proven
useful for improving SMT quality. The proposed
methods can be classified into two categories
according to the paraphrase targets: (1) enrich
translation models to cover more bilingual
expressions; (2) paraphrase the input sentences to
reduce OOVs or generate multiple inputs. In the
first category, He et al. (2011), Bond et al. (2008)
and Nakov (2008) enriched the SMT models via
paraphrasing the training corpora. Kuhn et al.
(2010) and Max (2010) used paraphrases to
smooth translation models. For the second
category, previous studies mainly focus on finding
translations for unknown terms using phrasal
paraphrases. Callison-Burch et al. (2006) and
Marton et al. (2009) paraphrase unknown terms in
the input sentences using phrasal paraphrases
extracted from bilingual and monolingual corpora.
Mirkin et al. (2009) rewrite OOVs with
entailments and paraphrases acquired from
WordNet. Onishi et al. (2010) and Du et al. (2010)
use phrasal paraphrases to build a word lattice to
get multiple input candidates. In the above
methods, only word or phrasal paraphrases are
used for input sentence rewriting. No structured
paraphrases on the sentence level have been
investigated. However, the information in the
sentence level is very important for disambiguation.
For example, we can only substitute play with
drama in a context related to stage or theatre.
Phrasal paraphrase substitutions can hardly solve
such kind of problems.
In this paper, we propose a method that rewrites
</bodyText>
<page confidence="0.9912">
979
</page>
<note confidence="0.987112">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 979–987,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.987581">
Initial Parallel Text
1st Round Translation
2nd Round Translation
Source Language Target Language
S0
S1
T0
T1
T2
Forward-
Translation
Back-
Translation
</figure>
<figureCaption confidence="0.999989">
Figure 1: Procedure of Forward-Translation and Back-Translation.
</figureCaption>
<bodyText confidence="0.997851233333333">
the input sentences of the SMT system using
automatically extracted paraphrase rules which can
capture structures on sentence level in addition to
paraphrases on the word or phrase level. Without
extra paraphrase resources, a novel approach is
proposed to acquire paraphrase rules from the
bilingual training corpus based on the results of
Forward-Translation and Back-Translation. The
rules target on rewriting the input sentences to an
MT-favored expression to ensure a better
translation. The paraphrase rules cover all kinds of
paraphrases on the word, phrase and sentence
levels, enabling structure reordering, word or
phrase insertion, deletion and substitution. The
experimental results show that our proposed
approach achieves significant improvements of
1.6~3.6 points of BLEU in the oral domain and
0.5~1 points in the news domain.
The remainder of the paper is organized as
follows: Section 2 makes a comparison between
the Forward-Translation and Back-Translation.
Section 3 introduces our methods that extract
paraphrase rules from the bilingual corpus of SMT.
Section 4 describes the strategies for constructing
word lattice with paraphrase rules. The
experimental results and some discussions are
presented in Section 5 and Section 6. Section 7
compares our work to the previous researches.
Finally, Section 8 concludes the paper and suggests
directions for future work.
</bodyText>
<sectionHeader confidence="0.885533" genericHeader="introduction">
2 Forward-Translation vs. Back-
Translation
</sectionHeader>
<bodyText confidence="0.6953365">
The Back-Translation method is mainly used for
automatic MT evaluation (Rapp 2009). This
approach is very helpful when no target language
reference is available. The only requirement is that
the MT system needs to be bidirectional. The
procedure includes translating a text into certain
foreign language with the MT system (Forward-
Translation), and translating it back into the
original language with the same system (Back-
Translation). Finally the translation quality of
Back-Translation is evaluated by using the original
source texts as references.
</bodyText>
<figureCaption confidence="0.6870284">
Sun et al. (2010) reported an interesting
phenomenon: given a bilingual text, the Back-
Translation results of the target sentences is better
than the Forward-Translation results of the source
sentences. Clearly, let (50, T0) be the initial pair of
bilingual text. A source-to-target translation system
5Y5_5T and a target-to-source translation system
5Y5_T5 are trained using the bilingual corpus.
FT(-) is a Forward-Translation function, and
BT(-) is a function of Back-Translation which can
</figureCaption>
<bodyText confidence="0.850438125">
be deduced with two rounds of translations:
BT(s) = SYS_TS(SYS_ST(S)). In the first round
of translation, 50 and T0 are fed into 5Y5_5T and
5Y5_T5, and we get T1 and 51 as translation results.
In the second round, we translate 51 back into the
target side with 5Y5_5T, and get the translation T2.
The procedure is illustrated in Figure 1, which can
also formally be described as:
</bodyText>
<equation confidence="0.700009">
1. T1 = FT(50) = 5Y5_5T(50).
</equation>
<bodyText confidence="0.8027925">
2. T2 = BT(T0), which can be decomposed into
two steps: 51 = 5Y5_T5(T0), T2 = 5Y5_5T(51).
Using T0 as reference, an interesting result is
reported in Sun et al. (2010) that T2 achieves a
higher score than T1 in automatic MT evaluation.
This outcome is important because T2 is translated
</bodyText>
<page confidence="0.995036">
980
</page>
<table confidence="0.999500833333333">
No. LHS RHS
1 乘坐/ride X1 公共汽车/bus 乘坐/ride X1 巴士/bus
2 在/at X1 处/location 向左拐/turn left 向左拐/turn left 在/at X1 处/location
3 把/NULL X1 给/give 我/me 给/give 我/me X1
4 从/from X1 到/to X2 要/need 多长/how long 要/need 花/spend 多长/how long 时间/time
时间/time 从/from X1 到/to X2
</table>
<tableCaption confidence="0.999917">
Table 1: Examples of Chinese Paraphrase rules, together with English translations for every word
</tableCaption>
<bodyText confidence="0.99975425">
from a machine-generated text S1, but T1 is
translated from a human-write text S0. Why the
machine-generated text results in a better
translation than the human-write text? Two
possible reasons may explain this phenomenon: (1)
in the first round of translation T0 4 S1, some
target word orders are reserved due to the
reordering failure, and these reserved orders lead to
a better result in the second round of translation; (2)
the text generated by an MT system is more likely
to be matched by the reversed but homologous MT
system.
Note that all the texts of S0, S1, S2, T0 and T1 are
sentence aligned because the initial parallel corpus
(S0, T0) is aligned in the sentence level. The aligned
sentence pairs in (S0, S1) can be considered as
paraphrases. Since S1 has some MT-favored
structures which may result in a better translation,
an intuitive idea is whether we can learn these
structures by comparing S1 with S0. This is the
main assumption of this paper. Taking (S0, S1) as
paraphrase resource, we propose a method that
automatically extracts paraphrase rules to capture
the MT-favored structures.
</bodyText>
<sectionHeader confidence="0.742932" genericHeader="method">
3 Extraction of Paraphrase Rules
</sectionHeader>
<subsectionHeader confidence="0.999884">
3.1 Definition of Paraphrase Rules
</subsectionHeader>
<bodyText confidence="0.999879">
We define a paraphrase rule as follows:
</bodyText>
<listItem confidence="0.997864857142857">
1. A paraphrase rule consists of two parts, left-
hand-side (LHS) and right-hand-side (RHS).
Both of LHS and RHS consist of non-
terminals (slot) and terminals (words).
2. LHS must start/end with a terminal.
3. There must be at least one terminal between
two non-terminals in LHS.
</listItem>
<bodyText confidence="0.961562">
A paraphrase rule in the format of:
LHS 4 RHS
which means the words matched by LHS can be
paraphrased to RHS. Taking Chinese as a case
study, some examples of paraphrase rules are
shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.999959">
3.2 Selecting Paraphrase Sentence Pairs
</subsectionHeader>
<bodyText confidence="0.999931823529412">
Following the methods in Section 2, the initial
bilingual corpus is (S0, T0). We train a source-to-
target PBMT system (SYS_ST) and a target-to-
source PBMT system (SYS_TS) on the parallel
corpus. Then a Forward-Translation is performed
on S0 using SYS_ST, and a Back-Translation is
performed on T0 using SYS_TS and SYS_ST. As
mentioned above, the detailed procedure is: T1 =
SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1).
Finally we compute BLEU (Papineni et al. 2002)
score for every sentence in T2 and T1, using the
corresponding sentence in T0 as reference. If the
sentence in T2 has a higher BLEU score than the
aligned sentence in T1, the corresponding sentences
in S0 and S1 are selected as candidate paraphrase
sentence pairs, which are used in the following
steps of paraphrase extractions.
</bodyText>
<subsectionHeader confidence="0.99848">
3.3 Word Alignments Filtering
</subsectionHeader>
<bodyText confidence="0.973570588235294">
We can construct word alignment between S0 and
S1 through T0. On the initial corpus of (S0, T0), we
conduct word alignment with Giza++ (Och and
Ney, 2000) in both directions and then apply the
grow-diag-final heuristic (Koehn et al., 2005) for
symmetrization. Because S1 is generated by
feeding T0 into the PBMT system SYS_TS, the
word alignment between T0 and S1 can be acquired
from the verbose information of the decoder.
The word alignments of S0 and S1 contain noises
which are produced by either wrong alignment of
GIZA++ or translation errors of SYS_TS. To ensure
the alignment quality, we use some heuristics to
filter the alignment between S0 and S1:
1. If two identical words are aligned in S0 and
S1, then remove all the other links to the two
words.
</bodyText>
<page confidence="0.979187">
981
</page>
<figure confidence="0.956635384615385">
I to that N/A blue handbag have interest
我 对 那 只 蓝色 手提包 有 兴趣 。
我 很 感 兴趣 那 个 蓝色 手提包 。
0 1 2 3
welcome ride No.10 bus
Rule
LHS:乘坐/ride X1 公共汽车/bus
RHS:乘坐/ride X1 巴士/bus
very feel interest that N/A blue handbag
I to that N/A blue handbag have interest
我 对 那 只 蓝色 手提包 有 兴趣
我 很 感 兴趣 那 个 蓝色 手提包
I very feel interest that N/A blue handbag
</figure>
<figureCaption confidence="0.637024428571429">
Figure 2: Example for Word Alignment
Filtration
2. Stop words (including some function words
and punctuations) can only be aligned to
either stop words or null.
Figure 2 illustrates an example of using the
heuristics to filter alignment.
</figureCaption>
<subsectionHeader confidence="0.997974">
3.4 Extracting Paraphrase Rules
</subsectionHeader>
<bodyText confidence="0.995598625">
From the word-aligned sentence pairs, we then
extract a set of rules that are consistent with the
word alignments. We use the rule extracting
methods of Chiang (2005). Take the sentence pair
in Figure 2 as an example, two initial phrase pairs
感 兴趣 那 个 蓝色 手提包” are identified, and
PP1 is contained by PP2, then we could form the
rule:
</bodyText>
<equation confidence="0.970918">
对 X1 有 兴趣  很 感 兴趣 X1
</equation>
<bodyText confidence="0.731398">
to have interest very feel interest
</bodyText>
<sectionHeader confidence="0.936239" genericHeader="method">
4 Paraphrasing the Input Sentences
</sectionHeader>
<bodyText confidence="0.9999844">
The extracted paraphrase rules aim to rewrite the
input sentences to an MT-favored form which may
lead to a better translation. However, it is risky to
directly replace the input sentence with a
paraphrased sentence, since the errors in automatic
paraphrase substitution may jeopardize the
translation result seriously. To avoid such damage,
for a given input sentence, we first transform all
paraphrase rules that match the input sentences to
phrasal paraphrases, and then build a word lattice
</bodyText>
<equation confidence="0.820066666666667">
欢迎 乘坐 [10 路] 公共汽车
乘坐 [10 路] 巴士
ride No.10 bus
</equation>
<figureCaption confidence="0.99779">
Figure 3: Example for Applying Paraphrase Rules
</figureCaption>
<bodyText confidence="0.999859583333333">
for SMT decoder using the phrasal paraphrases. In
this case, the decoder can search for the best result
among all the possible paths.
The input sentences are first segmented into sub-
sentences by punctuations. Then for each sub-
sentence, the matched paraphrase rules are ranked
according to: (1) the number of matched words; (2)
the frequency of the paraphrase rule in the training
data. Actually, the ranking strategy tends to select
paraphrase rules that have more matched words
(therefore less ambiguity) and higher frequency
(therefore more reliable).
</bodyText>
<subsectionHeader confidence="0.998139">
4.1 Applying Paraphrase Rules
</subsectionHeader>
<bodyText confidence="0.997093227272727">
Given an input sentence S and a paraphrase rule R
&lt;RLHS, RRHS&gt;, if S matches RLHS, then the matched
part can be replaced by RRHS. An example for
applying the paraphrase rules is illustrated in
Figure 3.
From Figure 3, we can see that the words of
position 1~3 are replaced to “乘坐 10 路 巴士”.
Actually, only the words at position 3 and 4 are
paraphrased to the word “巴士”, other words are
left unchanged. Therefore, we can use a triple,
&lt;MIN_RP_TEXT, COVER_START, COVER_LEN&gt;
(&lt;巴 士 , 3, 1&gt; in this example) to denote the
paraphrase rule, which means the minimal text to
replace is “巴士”, and the paraphrasing starts at
position 3 and covers 1 words.
In this manner, all the paraphrase rules matched
for a certain sentence can be converted to the
format of &lt;MIN_RP_TEXT, COVER_START,
COVER_LEN&gt;, which can also be considered as
phrasal paraphrases. Then the methods of building
phrasal paraphrases into word lattice for SMT
inputs can be used in our approaches.
</bodyText>
<page confidence="0.985791">
982
</page>
<figureCaption confidence="0.968211">
Figure 4: An example of lattice-based
paraphrases for an input sentence.
</figureCaption>
<subsectionHeader confidence="0.997942">
4.2 Construction of Paraphrase Lattice
</subsectionHeader>
<bodyText confidence="0.994982647058824">
Given an input sentence, all the matched
paraphrase rules are converted to phrasal
paraphrases first. Then we build the phrasal
paraphrases into word lattices using the methods
proposed by Du et al. (2010). The construction
process takes advantage of the correspondence
between detected phrasal paraphrases and positions
of the original words in the input sentence, and
then creates extra edges in the lattices to allow the
decoder to consider paths involving the paraphrase
words. An example is illustrated in Figure 4: given
a sequence of words {w1,...,wN} as the input, two
phrases α ={α1,...αp} and β = {β1,..., βq} are
detected as paraphrases for P1 = {wx,..., wy} (1 ≤ x
≤ y ≤ N) and P2 = {wm,...,wj (1 ≤ m ≤ n ≤ N)
respectively. The following steps are taken to
transform them into word lattices:
</bodyText>
<listItem confidence="0.987594818181818">
1. Transform the original source sentence into
word lattice. N + 1 nodes (θk, 0 ≤ k ≤ N) are
created, and N edges labeled with wi (1 ≤ i ≤
N) are generated to connect them
sequentially.
2. Generate extra nodes and edges for each of
the paraphrases. Taking α as an example,
firstly, p – 1 nodes are created, and then p
edges labeled with αj (1 ≤ j ≤ p) are
generated to connect node θx-1, p-1 nodes
and θy-1.
</listItem>
<bodyText confidence="0.7389365">
Via step 2, word lattices are generated by adding
new nodes and edges coming from paraphrases.
</bodyText>
<subsectionHeader confidence="0.989733">
4.3 Weight Estimation
</subsectionHeader>
<bodyText confidence="0.999901428571428">
The weights of new edges in the lattices are
estimated by an empirical method base on ranking
positions. Following Du et al. (2010), supposing
that E = {e1,...,ek} are a set of new edges
constructed from k paraphrase rules, which are
sorted in a descending order. Then the weight for
an edge ei is calculated as:
</bodyText>
<equation confidence="0.9855355">
1
w(ei) = k + i (1 &lt; i &lt; k)
</equation>
<bodyText confidence="0.997934">
where k is a predefined tradeoff parameter between
decoding speed and the number of potential
paraphrases being considered.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.988943">
5.1 Experimental Data
</subsectionHeader>
<bodyText confidence="0.6779658">
In our experiments, we used Moses (Koehn et al.,
2007) as the baseline system which can support
lattice decoding. The alignment was obtained using
GIZA++ (Och and Ney, 2003) and then we
symmetrized the word alignment using the grow-
diag-final heuristic. Parameters were tuned using
Minimum Error Rate Training (Och, 2003). To
comprehensively evaluate the proposed methods in
different domains, two groups of experiments were
carried out, namely, the oral group (Goral) and the
news group (Gnews). The experiments were
conducted in both Chinese-English and English-
Chinese directions for the oral group, and Chinese-
English direction for the news group. The English
sentences were all tokenized and lowercased, and
the Chinese sentences were segmented into words
by Language Technology Platform (LTP)1. We
used SRILM2 for the training of language models
(5-gram in all the experiments). The metrics for
automatic evaluation were BLEU 3 and TER 4
(Snover et al., 2005).
The detailed statistics of the training data in Goral
are showed in Table 2. For the bilingual corpus, we
used the BTEC and PIVOT data of IWSLT 2008,
HIT corpus 5 and other Chinese LDC (CLDC)
</bodyText>
<footnote confidence="0.996815625">
1 http://ir.hit.edu.cn/ltp/
2 http://www.speech.sri.com/projects/srilm/
3 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl
4 http://www.umiacs.umd.edu/~snover/terp/
5 The HIT corpus contains the CLDC Olympic corpus (2004-
863-008) and the other HIT corpora available at
http://mitlab.hit.edu.cn/index.php/resources/29-the-
resource/111-share-bilingual-corpus.html.
</footnote>
<page confidence="0.995466">
983
</page>
<table confidence="0.996826333333333">
Corpus #Sen. pairs #Ch. words #En words
BETC 19,972 174k 190k
PIVOT 20,000 162k 196k
HIT 80,868 788k 850k
CLDC 190,447 1,167k 1,898k
Tanaka 149,207 - 1,375k
</table>
<tableCaption confidence="0.97392">
Table 2: Statistics of training data in Goral
</tableCaption>
<table confidence="0.999363571428571">
Corpus #Sen. #Ref.
develop CSTAR03 test set 506 16
IWSLT06 dev set 489 7
test IWSLT04 test set 500 16
IWSLT05 test set 506 16
IWSLT06 test set 500 7
IWSLT07 test set 489 6
</table>
<tableCaption confidence="0.994938">
Table 3: Statistics of test/develop sets in Goral
</tableCaption>
<table confidence="0.9995435">
Corpus #Sen. #Ref.
develop NIST 2002 878 10
NIST 2005 1,082 4
test NIST 2004 1,788 5
NIST 2006 1,664 4
NIST 2008 1,357 4
</table>
<tableCaption confidence="0.99965">
Table 4: Statistics of test/develop sets in Gnews
</tableCaption>
<bodyText confidence="0.998015264705882">
corpora, including the Chinese-English Sentence
Aligned Bilingual Corpus (CLDC-LAC-2003-004)
and the Chinese-English Parallel Corpora (CLDC-
LAC-2003-006). We trained a Chinese language
model for the E-C translation on the Chinese part
of the bi-text. For the English language model of
C-E translation, an extra corpus named Tanaka was
used besides the English part of the bilingual
corpora. For testing and developing, we used six
Chinese-English development corpora of IWSLT
2008. The statistics are shown in Table 3.
In detail, we chose CSTAR03-test and
IWSLT06-dev as the development set; and used
IWSLT04-test, IWSLT05-test, IWSLT06-dev and
IWSLT07-test for testing. For English-Chinese
evaluation, we used IWSLT English-Chinese MT
evaluation 2005 as the test set. Due to the lacking
of development set, we did not tune parameters on
English-Chinese side, instead, we just used the
default parameters of Moses.
In the experiments of the news group, we used
the Sinorama and FBIS corpora (LDC2005T10 and
LDC2003E14) for bilingual corpus. After
tokenization and filtering, this bilingual corpus
contained 319,694 sentence pairs (7.9M tokens on
Chinese side and 9.2M tokens on English side).
We trained a 5-gram language model on the
English side of the bi-text. The system was tested
using the Chinese-English MT evaluation sets of
NIST 2004, NIST 2006 and NIST 2008. For
development, we used the Chinese-English MT
evaluation sets of NIST 2002 and NIST 2005.
Table 4 shows the statistics of test/development
sets used in the news group.
</bodyText>
<subsectionHeader confidence="0.844777">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9791584">
We extract both Chinese and English rules in Goral,
and Chinese paraphrase rules in Gnews by
comparing the results of Forward-Translation and
Back-Translation as described in Section 3. During
the extraction, some heuristics are used to ensure
the quality of paraphrase rules. Take the extraction
of Chinese paraphrase rules in Goral as a case study.
Suppose (C0, E0) are the initial bilingual corpus of
Goral. A Chinese-English and an English-Chinese
MT system are trained on (C0, E0). We perform
</bodyText>
<equation confidence="0.563626">
EtoC CtoE
Back-Translation on E0 (Eo ) C1 ) Ez), and
CtoE
</equation>
<bodyText confidence="0.989364538461538">
Forward-Translation on C0 (Co ���� E1). Suppose
e1a and e2a are two aligned sentences in E1 and E2,
c0a and c1a are the corresponding sentences in C0
and C1. (c0a, c1a) are selected for the extraction of
paraphrase rules if two conditions are satisfied: (1)
BLEU(e2a) – BLEU(e1a) &gt; θ,, and (2) BLEU(e2a) &gt;
θ2, where BLEU(-) is a function for computing
BLEU score; θ, and θ2 are thresholds for balancing
the rules number and the quality of paraphrase
rules. In our experiment, θ, and θ2 are empirically
set to 0.1 and 0.3.
As a result, we extract 912,625 Chinese and
1,116,375 English paraphrase rules for Goral, and
for Gnews the number of Chinese paraphrase rules is
2,877,960. Then we use the extracted paraphrase
rules to improve SMT by building word lattices for
the input sentences.
The Chinese-English experimental results of
Goral and Gnews are shown in Table 5 and Table 6,
respectively. It can be seen that our method
outperforms the baselines in both oral and news
domains. Our system gains significant
improvements of 1.6~3.6 points of BLEU in the
oral domain, and 0.5~1 points of BLEU in the
news domain. Figure 5 shows the effect of
considered paraphrases (k) in the step of building
</bodyText>
<page confidence="0.994744">
984
</page>
<table confidence="0.995869">
Model BLEU TER
iwslt 04 iwslt 05 iwslt 06 iwslt 07 iwslt 04 iwslt 05 iwslt 06 iwslt 07
baseline 0.5353 0.5887 0.2765 0.3977 0.3279 0.2874 0.5559 0.4390
para. improved 0.5712 0.6107 0.2924 0.4193 0.3055 0.2722 0.5374 0.4217
</table>
<tableCaption confidence="0.972188">
Table 5: Experimental results of Goral in Chinese-English direction
</tableCaption>
<table confidence="0.99928025">
Model BLEU TER
nist 04 nist 06 nist 08 nist 04 nist 06 nist 08
baseline 0.2795 0.2389 0.1933 0.6554 0.6515 0.6652
para. improved 0.2891 0.2485 0.1978 0.6451 0.6407 0.6582
</table>
<tableCaption confidence="0.954407">
Table 6: Experimental results of Gnews in Chinese-English direction
</tableCaption>
<table confidence="0.99928925">
model IWSLT 2005
BLEU TER
baseline 0.4644 0.4164
para. improved 0.4853 0.3883
</table>
<tableCaption confidence="0.981201">
Table 7: Experimental results of Goral in
English-Chinese direction
</tableCaption>
<table confidence="0.9951825">
trans. improve comparable worsen total
para.
correct 36 20 4 60
incorrect 1 9 14 24
</table>
<tableCaption confidence="0.9782925">
Table 8: Human analysis of the paraphrasing
results in IWSLT 2007 CE translation
</tableCaption>
<bodyText confidence="0.993759">
word lattices. The result of English-Chinese
experiments in Goral is shown in Table 7.
</bodyText>
<sectionHeader confidence="0.998788" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9989195">
We make a detailed analysis on the Chinese-
English translation results that are affected by our
paraphrase rules. The aim is to investigate what
kinds of paraphrases have been captured in the
rules. Firstly the input path is recovered from the
translation results according to the tracing
information of the decoder, and therefore we can
examine which path is selected by the SMT
decoder from the paraphrase lattice. A human
annotator is asked to judge whether the recovered
paraphrase sentence keeps the same meaning as the
original input. Then the annotator compares the
baseline translation with the translations proposed
by our approach. The analysis is carried out on the
IWSLT 2007 Chinese-English test set, 84 out of
489 input sentences have been affected by
paraphrases, and the statistic of human evaluation
is shown in Table 8.
It can be seen in Table 8 that the paraphrases
achieve a relatively high accuracy, 60 (71.4%)
</bodyText>
<figure confidence="0.974784">
0 10 20 30 40
Considered paraprhases (k)
</figure>
<figureCaption confidence="0.9816395">
Figure 5: Effect of considered paraphrases (k)
on BLEU score
</figureCaption>
<bodyText confidence="0.998686111111111">
paraphrased sentences retain the same meaning,
and the other 24 (28.6%) are incorrect. Among the
60 correct paraphrases, 36 sentences finally result
in an improved translation. We further analyze
these paraphrases and the translation results to
investigate what kinds of transformation finally
lead to the translation quality improvement. The
paraphrase variations can be classified into four
categories:
</bodyText>
<listItem confidence="0.996150818181818">
(1) Reordering: The original source sentences
are reordered to be similar to the order of
the target language.
(2) Word substitution: A phrase with multi-
word translations is replaced by a phrase
with a single-word translation.
(3) Recovering omitted words: Ellipsis occurs
frequently in spoken language. Recovering
the omitted words often leads to a better
translation.
(4) Removing redundant words: Mostly,
</listItem>
<bodyText confidence="0.53797925">
translating redundant words may confuse
the SMT system and would be unnecessary.
Removing redundant words can mitigate
this problem.
</bodyText>
<figure confidence="0.90934925">
45.4
BLEU score (%)
45.2
45.0
44.8
44.6
44.4
44.2
</figure>
<page confidence="0.992158">
985
</page>
<table confidence="0.999837">
Cate. Num Original Sentence/Translation Paraphrased Sentence/Translation
11 香烟/cigarette 可以/can 免税/duty-free 带 多少/how much 香烟/cigarettes 可以/can 免税
/take 多少/how much 支/N/A ? /duty-free 带/take 支/N/A ?
what a cigarette can i take duty-free ? how many cigarettes can i take duty-free one ?
18 你/you 有/have 多久/how long 的/N/A 你/you 有/have 多少/how much 教学/teaching
教学/teaching 经验/experience ? 经验/experience ?
you have how long teaching experience ? how much teaching experience you have ?
10 需要/need 押金/deposit 吗/N/A ? 你/you 需要/need 押金/deposit 吗/N/A ?
you need a deposit ? do you need a deposit ?
4 戒指/ring 掉/fall 进/into 洗脸池/washbasin 戒指/ring 掉/fall 进/into 洗脸池/washbasin 了
里/in 了/N/A 。 /N/A 。
ring off into the washbasin is in . ring off into the washbasin .
</table>
<tableCaption confidence="0.99599">
Table 9: Examples for classification of paraphrase rules
</tableCaption>
<bodyText confidence="0.999717809523809">
Four examples for category (1), (2), (3) and (4)
are shown in Table 9, respectively. The numbers in
the second column indicates the number of the
sentences affected by the rules, among the 36
sentences with improved paraphrasing and
translation. A sentence can be classified into
multiple categories. Except category (2), the other
three categories cannot be detected by the previous
approaches, which verify our statement that our
rules can capture structured paraphrases on the
sentence level in addition to the paraphrases on the
word or phrase level.
Not all the paraphrased results are correct.
Sometimes an ill paraphrased sentence can produce
better translations. Take the first line of Table 9 as
an example, the paraphrased sentence “多少/How
many 香烟/cigarettes 可以/can 免税/duty-free 带
/take 支/NULL” is not a fluent Chinese sentence,
however, the rearranged word order is closer to
English, which finally results in a much better
translation.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999948483870968">
Previous studies on improving SMT using
paraphrase rules focus on hand-crafted rules.
Nakov (2008) employs six rules for paraphrasing
the training corpus. Bond et al. (2008) use
grammars to paraphrase the source side of training
data, covering aspects like word order and minor
lexical variations (tenses etc.) but not content
words. The paraphrases are added to the source
side of the corpus and the corresponding target
sentences are duplicated.
A disadvantage for hand-crafted paraphrase
rules is that it is language dependent. In contrast,
our method that automatically extracted paraphrase
rules from bilingual corpus is flexible and suitable
for any language pairs.
Our work is similar to Sun et al. (2010). Both
tried to capture the MT-favored structures from
bilingual corpus. However, a clear difference is
that Sun et al. (2010) captures the structures
implicitly by training an MT system on (S0, S1) and
“translates” the SMT input to an MT-favored
expression. Actually, the rewriting process is
considered as a black box in Sun et al. (2010). In
this paper, the MT-favored expressions are
captured explicitly by automatically extracted
paraphrase rules. The advantages of the paraphrase
rules are: (1) Our method can explicitly capture the
structure information in the sentence level,
enabling global reordering, which is impossible in
Sun et al. (2010). (2) For each rule, we can control
its quality automatically or manually.
</bodyText>
<sectionHeader confidence="0.997373" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999924411764706">
In this paper, we propose a novel method for
extracting paraphrase rules by comparing the
source side of bilingual corpus to the target-to-
source translation of the target side. The acquired
paraphrase rules are employed to enrich the SMT
inputs, which target on rewriting the input
sentences to an MT-favored form. The paraphrase
rules cover all kinds of paraphrases on the word,
phrase and sentence levels, enabling structure
reordering, word or phrase insertion, deletion and
substitution. Experimental results show that the
paraphrase rules can improve SMT quality in both
the oral and news domains. The manual
investigation on oral translation results indicate
that the paraphrase rules capture four kinds of MT-
favored transformation to ensure translation quality
improvement.
</bodyText>
<page confidence="0.998">
986
</page>
<sectionHeader confidence="0.989337" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.998544">
This work was supported by National Natural
Science Foundation of China (NSFC) (61073126,
61133012), 863 High Technology Program
(2011AA01A207).
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948535714286">
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of the IWSLT, pages 150–157.
Chris Callison-Burch, Philipp Koehn, and Miles
Osborne. 2006. Improved Statistical Machine
Translation Using Paraphrases. In Proceedings of
NAACL, pages 17-24.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263–270.
Jinhua Du, Jie Jiang, Andy Way. 2010. Facilitating
Translation Using Source Language Paraphrase
Lattices. In Proceedings of EMNLP, pages 420-429.
Wei He, Shiqi Zhao, Haifeng Wang and Ting Liu. 2011.
Enriching SMT Training Data via Paraphrasing. In
Proceedings of IJCNLP, pages 803-810.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proceedings of HLT/NAACL, pages 48–54
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388-395.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL Demo and Poster Sessions,
pages 177–180.
Roland Kuhn, Boxing Chen, George Foster and Evan
Stratford. 2010. Phrase Clustering for Smoothing TM
Probabilities-or, How to Extract Paraphrases from
Phrase Tables. In Proceedings of COLING, pages
608–616.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved Statistical Machine Translation
Using Monolingually-Dervied Paraphrases. In
Proceedings of EMNLP, pages 381-390.
Aurélien Max. 2010. Example-Based Paraphrasing for
Improved Phrase-Based Statistical Machine
TranslationIn Proceedings of EMNLP, pages 656-
666.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido
Dagan, Marc Dymetman, Idan Szpektor. 2009.
Source-Language Entailment Modeling for
Translation Unknown Terms. In Proceedings of ACL,
pages 791-799.
Preslav Nakov. 2008. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In
Proceedings of ECAI, pages 338-342.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440-447.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160-167.
Takashi Onishi, Masao Utiyama, Eiichiro Sumita. 2010.
Paraphrase Lattice for Statistical Machine
Translation. In Proceedings of ACL, pages 1-5.
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings of
ACL, pages 311-318.
Reinhard Rapp. 2009. The Back-translation Score:
Automatic MT Evaluation at the Sentence Level
without Reference Translations. In Proceedings of
ACL-IJCNLP, pages 133-136.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
John Makhoul, Linnea Micciulla, and Ralph
Weischedel. 2005. A study of translation error rate
with targeted human annotation. Technical Report
LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-
58, University of Maryland, July, 2005.
Yanli Sun, Sharon O’Brien, Minako O’Hagan and Fred
Hollowood. 2010. A Novel Statistical Pre-Processing
Model for Rule-Based Machine Translation System.
In Proceedings of EAMT, 8pp.
</reference>
<page confidence="0.997542">
987
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.525142">
<title confidence="0.999817">Improve SMT Quality with Automatically Extracted Paraphrase Rules</title>
<author confidence="0.988213">Hua Haifeng Ting</author>
<affiliation confidence="0.994898">Center for Social Computing and</affiliation>
<address confidence="0.571367">Retrieval, Harbin Institute of</address>
<email confidence="0.997913">wu_hua@baidu.com</email>
<email confidence="0.997913">wanghaifeng@baidu.com</email>
<abstract confidence="0.995795315789474">We propose a novel approach to improve SMT via paraphrase rules which are automatically extracted from the bilingual training data. Without using extra paraphrase resources, we acquire the rules by comparing the source side of the parallel corpus with the target-to-source translations of the target side. Besides the word and phrase paraphrases, the acquired paraphrase rules mainly cover the structured paraphrases on the sentence level. These rules are employed to enrich the SMT inputs for translation quality improvement. The experimental results show that our proposed approach achieves significant improvements of 1.6~3.6 points of BLEU in the oral domain and 0.5~1 points in the news domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<date>2011</date>
<contexts>
<context position="2108" citStr="(2011)" startWordPosition="310" endWordPosition="310"> the MT-favored expressions, or from This work was done when the first author was visiting Baidu. *Correspondence author: tliu@ir.hit.edu.cn the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and D</context>
</contexts>
<marker>2011</marker>
<rawString>(2011AA01A207).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Eric Nichols</author>
<author>Darren Scott Appling</author>
<author>Michael Paul</author>
</authors>
<title>Improving Statistical Machine Translation by Paraphrasing the Training Data.</title>
<date>2008</date>
<booktitle>In Proceedings of the IWSLT,</booktitle>
<pages>150--157</pages>
<contexts>
<context position="2128" citStr="Bond et al. (2008)" startWordPosition="311" endWordPosition="314">favored expressions, or from This work was done when the first author was visiting Baidu. *Correspondence author: tliu@ir.hit.edu.cn the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use </context>
<context position="25988" citStr="Bond et al. (2008)" startWordPosition="4200" endWordPosition="4203">phrases on the word or phrase level. Not all the paraphrased results are correct. Sometimes an ill paraphrased sentence can produce better translations. Take the first line of Table 9 as an example, the paraphrased sentence “多少/How many 香烟/cigarettes 可以/can 免税/duty-free 带 /take 支/NULL” is not a fluent Chinese sentence, however, the rearranged word order is closer to English, which finally results in a much better translation. 7 Related Work Previous studies on improving SMT using paraphrase rules focus on hand-crafted rules. Nakov (2008) employs six rules for paraphrasing the training corpus. Bond et al. (2008) use grammars to paraphrase the source side of training data, covering aspects like word order and minor lexical variations (tenses etc.) but not content words. The paraphrases are added to the source side of the corpus and the corresponding target sentences are duplicated. A disadvantage for hand-crafted paraphrase rules is that it is language dependent. In contrast, our method that automatically extracted paraphrase rules from bilingual corpus is flexible and suitable for any language pairs. Our work is similar to Sun et al. (2010). Both tried to capture the MT-favored structures from biling</context>
</contexts>
<marker>Bond, Nichols, Appling, Paul, 2008</marker>
<rawString>Francis Bond, Eric Nichols, Darren Scott Appling, and Michael Paul. 2008. Improving Statistical Machine Translation by Paraphrasing the Training Data. In Proceedings of the IWSLT, pages 150–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved Statistical Machine Translation Using Paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="2442" citStr="Callison-Burch et al. (2006)" startWordPosition="357" endWordPosition="360">SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for d</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved Statistical Machine Translation Using Paraphrases. In Proceedings of NAACL, pages 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="11301" citStr="Chiang (2005)" startWordPosition="1808" endWordPosition="1809">e X1 巴士/bus very feel interest that N/A blue handbag I to that N/A blue handbag have interest 我 对 那 只 蓝色 手提包 有 兴趣 我 很 感 兴趣 那 个 蓝色 手提包 I very feel interest that N/A blue handbag Figure 2: Example for Word Alignment Filtration 2. Stop words (including some function words and punctuations) can only be aligned to either stop words or null. Figure 2 illustrates an example of using the heuristics to filter alignment. 3.4 Extracting Paraphrase Rules From the word-aligned sentence pairs, we then extract a set of rules that are consistent with the word alignments. We use the rule extracting methods of Chiang (2005). Take the sentence pair in Figure 2 as an example, two initial phrase pairs 感 兴趣 那 个 蓝色 手提包” are identified, and PP1 is contained by PP2, then we could form the rule: 对 X1 有 兴趣  很 感 兴趣 X1 to have interest very feel interest 4 Paraphrasing the Input Sentences The extracted paraphrase rules aim to rewrite the input sentences to an MT-favored form which may lead to a better translation. However, it is risky to directly replace the input sentence with a paraphrased sentence, since the errors in automatic paraphrase substitution may jeopardize the translation result seriously. To avoid such damag</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinhua Du</author>
<author>Jie Jiang</author>
<author>Andy Way</author>
</authors>
<title>Facilitating Translation Using Source Language Paraphrase Lattices.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>420--429</pages>
<contexts>
<context position="2723" citStr="Du et al. (2010)" startWordPosition="400" endWordPosition="403">), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For example, we can only substitute play with drama in a context related to stage or theatre. Phrasal paraphrase substitutions can hardly solve such kind of problems. In this paper, we propose a method that rewrites 979 Proceedings of the 50th Annual Meeting of the </context>
<context position="14011" citStr="Du et al. (2010)" startWordPosition="2264" endWordPosition="2267">all the paraphrase rules matched for a certain sentence can be converted to the format of &lt;MIN_RP_TEXT, COVER_START, COVER_LEN&gt;, which can also be considered as phrasal paraphrases. Then the methods of building phrasal paraphrases into word lattice for SMT inputs can be used in our approaches. 982 Figure 4: An example of lattice-based paraphrases for an input sentence. 4.2 Construction of Paraphrase Lattice Given an input sentence, all the matched paraphrase rules are converted to phrasal paraphrases first. Then we build the phrasal paraphrases into word lattices using the methods proposed by Du et al. (2010). The construction process takes advantage of the correspondence between detected phrasal paraphrases and positions of the original words in the input sentence, and then creates extra edges in the lattices to allow the decoder to consider paths involving the paraphrase words. An example is illustrated in Figure 4: given a sequence of words {w1,...,wN} as the input, two phrases α ={α1,...αp} and β = {β1,..., βq} are detected as paraphrases for P1 = {wx,..., wy} (1 ≤ x ≤ y ≤ N) and P2 = {wm,...,wj (1 ≤ m ≤ n ≤ N) respectively. The following steps are taken to transform them into word lattices: 1</context>
<context position="15266" citStr="Du et al. (2010)" startWordPosition="2494" endWordPosition="2497">e into word lattice. N + 1 nodes (θk, 0 ≤ k ≤ N) are created, and N edges labeled with wi (1 ≤ i ≤ N) are generated to connect them sequentially. 2. Generate extra nodes and edges for each of the paraphrases. Taking α as an example, firstly, p – 1 nodes are created, and then p edges labeled with αj (1 ≤ j ≤ p) are generated to connect node θx-1, p-1 nodes and θy-1. Via step 2, word lattices are generated by adding new nodes and edges coming from paraphrases. 4.3 Weight Estimation The weights of new edges in the lattices are estimated by an empirical method base on ranking positions. Following Du et al. (2010), supposing that E = {e1,...,ek} are a set of new edges constructed from k paraphrase rules, which are sorted in a descending order. Then the weight for an edge ei is calculated as: 1 w(ei) = k + i (1 &lt; i &lt; k) where k is a predefined tradeoff parameter between decoding speed and the number of potential paraphrases being considered. 5 Experiments 5.1 Experimental Data In our experiments, we used Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. The alignment was obtained using GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the gro</context>
</contexts>
<marker>Du, Jiang, Way, 2010</marker>
<rawString>Jinhua Du, Jie Jiang, Andy Way. 2010. Facilitating Translation Using Source Language Paraphrase Lattices. In Proceedings of EMNLP, pages 420-429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei He</author>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Enriching SMT Training Data via Paraphrasing. In</title>
<date>2011</date>
<booktitle>Proceedings of IJCNLP,</booktitle>
<pages>803--810</pages>
<contexts>
<context position="2108" citStr="He et al. (2011)" startWordPosition="307" endWordPosition="310">ntences to the MT-favored expressions, or from This work was done when the first author was visiting Baidu. *Correspondence author: tliu@ir.hit.edu.cn the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and D</context>
</contexts>
<marker>He, Zhao, Wang, Liu, 2011</marker>
<rawString>Wei He, Shiqi Zhao, Haifeng Wang and Ting Liu. 2011. Enriching SMT Training Data via Paraphrasing. In Proceedings of IJCNLP, pages 803-810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>48--54</pages>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of HLT/NAACL, pages 48–54</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>388--395</pages>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>Edinburgh System Description for the</booktitle>
<contexts>
<context position="10006" citStr="Koehn et al., 2005" startWordPosition="1563" endWordPosition="1566">pineni et al. 2002) score for every sentence in T2 and T1, using the corresponding sentence in T0 as reference. If the sentence in T2 has a higher BLEU score than the aligned sentence in T1, the corresponding sentences in S0 and S1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions. 3.3 Word Alignments Filtering We can construct word alignment between S0 and S1 through T0. On the initial corpus of (S0, T0), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005) for symmetrization. Because S1 is generated by feeding T0 into the PBMT system SYS_TS, the word alignment between T0 and S1 can be acquired from the verbose information of the decoder. The word alignments of S0 and S1 contain noises which are produced by either wrong alignment of GIZA++ or translation errors of SYS_TS. To ensure the alignment quality, we use some heuristics to filter the alignment between S0 and S1: 1. If two identical words are aligned in S0 and S1, then remove all the other links to the two words. 981 I to that N/A blue handbag have interest 我 对 那 只 蓝色 手提包 有 兴趣 。 我 很 感 兴趣 那</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="15690" citStr="Koehn et al., 2007" startWordPosition="2571" endWordPosition="2574"> nodes and edges coming from paraphrases. 4.3 Weight Estimation The weights of new edges in the lattices are estimated by an empirical method base on ranking positions. Following Du et al. (2010), supposing that E = {e1,...,ek} are a set of new edges constructed from k paraphrase rules, which are sorted in a descending order. Then the weight for an edge ei is calculated as: 1 w(ei) = k + i (1 &lt; i &lt; k) where k is a predefined tradeoff parameter between decoding speed and the number of potential paraphrases being considered. 5 Experiments 5.1 Experimental Data In our experiments, we used Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. The alignment was obtained using GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the growdiag-final heuristic. Parameters were tuned using Minimum Error Rate Training (Och, 2003). To comprehensively evaluate the proposed methods in different domains, two groups of experiments were carried out, namely, the oral group (Goral) and the news group (Gnews). The experiments were conducted in both Chinese-English and EnglishChinese directions for the oral group, and ChineseEnglish direction for the news group. The </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Boxing Chen</author>
<author>George Foster</author>
<author>Evan Stratford</author>
</authors>
<title>Phrase Clustering for Smoothing TM Probabilities-or, How to Extract Paraphrases from Phrase Tables.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>608--616</pages>
<contexts>
<context position="2227" citStr="Kuhn et al. (2010)" startWordPosition="327" endWordPosition="330">ndence author: tliu@ir.hit.edu.cn the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods,</context>
</contexts>
<marker>Kuhn, Chen, Foster, Stratford, 2010</marker>
<rawString>Roland Kuhn, Boxing Chen, George Foster and Evan Stratford. 2010. Phrase Clustering for Smoothing TM Probabilities-or, How to Extract Paraphrases from Phrase Tables. In Proceedings of COLING, pages 608–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Chris Callison-Burch</author>
<author>Philip Resnik</author>
</authors>
<title>Improved Statistical Machine Translation Using Monolingually-Dervied Paraphrases.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>381--390</pages>
<contexts>
<context position="2467" citStr="Marton et al. (2009)" startWordPosition="362" endWordPosition="365"> can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For exampl</context>
</contexts>
<marker>Marton, Callison-Burch, Resnik, 2009</marker>
<rawString>Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Improved Statistical Machine Translation Using Monolingually-Dervied Paraphrases. In Proceedings of EMNLP, pages 381-390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurélien Max</author>
</authors>
<title>Example-Based Paraphrasing for Improved Phrase-Based Statistical Machine TranslationIn</title>
<date>2010</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<pages>656--666</pages>
<contexts>
<context position="2242" citStr="Max (2010)" startWordPosition="332" endWordPosition="333">hit.edu.cn the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or p</context>
</contexts>
<marker>Max, 2010</marker>
<rawString>Aurélien Max. 2010. Example-Based Paraphrasing for Improved Phrase-Based Statistical Machine TranslationIn Proceedings of EMNLP, pages 656-666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
</authors>
<title>Ido Dagan, Marc Dymetman, Idan Szpektor.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>791--799</pages>
<contexts>
<context position="2612" citStr="Mirkin et al. (2009)" startWordPosition="382" endWordPosition="385">) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For example, we can only substitute play with drama in a context related to stage or theatre. Phrasal paraphrase substitutions can hardly solve such kind o</context>
</contexts>
<marker>Mirkin, Specia, Cancedda, 2009</marker>
<rawString>Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Dagan, Marc Dymetman, Idan Szpektor. 2009. Source-Language Entailment Modeling for Translation Unknown Terms. In Proceedings of ACL, pages 791-799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Improved Statistical Machine Translation Using Monolingual Paraphrases.</title>
<date>2008</date>
<booktitle>In Proceedings of ECAI,</booktitle>
<pages>338--342</pages>
<contexts>
<context position="2145" citStr="Nakov (2008)" startWordPosition="316" endWordPosition="317"> from This work was done when the first author was visiting Baidu. *Correspondence author: tliu@ir.hit.edu.cn the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphras</context>
<context position="25913" citStr="Nakov (2008)" startWordPosition="4190" endWordPosition="4191"> structured paraphrases on the sentence level in addition to the paraphrases on the word or phrase level. Not all the paraphrased results are correct. Sometimes an ill paraphrased sentence can produce better translations. Take the first line of Table 9 as an example, the paraphrased sentence “多少/How many 香烟/cigarettes 可以/can 免税/duty-free 带 /take 支/NULL” is not a fluent Chinese sentence, however, the rearranged word order is closer to English, which finally results in a much better translation. 7 Related Work Previous studies on improving SMT using paraphrase rules focus on hand-crafted rules. Nakov (2008) employs six rules for paraphrasing the training corpus. Bond et al. (2008) use grammars to paraphrase the source side of training data, covering aspects like word order and minor lexical variations (tenses etc.) but not content words. The paraphrases are added to the source side of the corpus and the corresponding target sentences are duplicated. A disadvantage for hand-crafted paraphrase rules is that it is language dependent. In contrast, our method that automatically extracted paraphrase rules from bilingual corpus is flexible and suitable for any language pairs. Our work is similar to Sun</context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008. Improved Statistical Machine Translation Using Monolingual Paraphrases. In Proceedings of ECAI, pages 338-342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="9921" citStr="Och and Ney, 2000" startWordPosition="1550" endWordPosition="1553">e is: T1 = SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). Finally we compute BLEU (Papineni et al. 2002) score for every sentence in T2 and T1, using the corresponding sentence in T0 as reference. If the sentence in T2 has a higher BLEU score than the aligned sentence in T1, the corresponding sentences in S0 and S1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions. 3.3 Word Alignments Filtering We can construct word alignment between S0 and S1 through T0. On the initial corpus of (S0, T0), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005) for symmetrization. Because S1 is generated by feeding T0 into the PBMT system SYS_TS, the word alignment between T0 and S1 can be acquired from the verbose information of the decoder. The word alignments of S0 and S1 contain noises which are produced by either wrong alignment of GIZA++ or translation errors of SYS_TS. To ensure the alignment quality, we use some heuristics to filter the alignment between S0 and S1: 1. If two identical words are aligned in S0 and S1, then remove all the other links to the two</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL, pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="15956" citStr="Och, 2003" startWordPosition="2614" endWordPosition="2615">hrase rules, which are sorted in a descending order. Then the weight for an edge ei is calculated as: 1 w(ei) = k + i (1 &lt; i &lt; k) where k is a predefined tradeoff parameter between decoding speed and the number of potential paraphrases being considered. 5 Experiments 5.1 Experimental Data In our experiments, we used Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. The alignment was obtained using GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the growdiag-final heuristic. Parameters were tuned using Minimum Error Rate Training (Och, 2003). To comprehensively evaluate the proposed methods in different domains, two groups of experiments were carried out, namely, the oral group (Goral) and the news group (Gnews). The experiments were conducted in both Chinese-English and EnglishChinese directions for the oral group, and ChineseEnglish direction for the news group. The English sentences were all tokenized and lowercased, and the Chinese sentences were segmented into words by Language Technology Platform (LTP)1. We used SRILM2 for the training of language models (5-gram in all the experiments). The metrics for automatic evaluation </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Onishi</author>
</authors>
<title>Masao Utiyama, Eiichiro Sumita.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1--5</pages>
<marker>Onishi, 2010</marker>
<rawString>Takashi Onishi, Masao Utiyama, Eiichiro Sumita. 2010. Paraphrase Lattice for Statistical Machine Translation. In Proceedings of ACL, pages 1-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="9406" citStr="Papineni et al. 2002" startWordPosition="1462" endWordPosition="1465">S can be paraphrased to RHS. Taking Chinese as a case study, some examples of paraphrase rules are shown in Table 1. 3.2 Selecting Paraphrase Sentence Pairs Following the methods in Section 2, the initial bilingual corpus is (S0, T0). We train a source-totarget PBMT system (SYS_ST) and a target-tosource PBMT system (SYS_TS) on the parallel corpus. Then a Forward-Translation is performed on S0 using SYS_ST, and a Back-Translation is performed on T0 using SYS_TS and SYS_ST. As mentioned above, the detailed procedure is: T1 = SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). Finally we compute BLEU (Papineni et al. 2002) score for every sentence in T2 and T1, using the corresponding sentence in T0 as reference. If the sentence in T2 has a higher BLEU score than the aligned sentence in T1, the corresponding sentences in S0 and S1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions. 3.3 Word Alignments Filtering We can construct word alignment between S0 and S1 through T0. On the initial corpus of (S0, T0), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005)</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>The Back-translation Score: Automatic MT Evaluation at the Sentence Level without Reference Translations.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>133--136</pages>
<contexts>
<context position="5194" citStr="Rapp 2009" startWordPosition="766" endWordPosition="767">n 2 makes a comparison between the Forward-Translation and Back-Translation. Section 3 introduces our methods that extract paraphrase rules from the bilingual corpus of SMT. Section 4 describes the strategies for constructing word lattice with paraphrase rules. The experimental results and some discussions are presented in Section 5 and Section 6. Section 7 compares our work to the previous researches. Finally, Section 8 concludes the paper and suggests directions for future work. 2 Forward-Translation vs. BackTranslation The Back-Translation method is mainly used for automatic MT evaluation (Rapp 2009). This approach is very helpful when no target language reference is available. The only requirement is that the MT system needs to be bidirectional. The procedure includes translating a text into certain foreign language with the MT system (ForwardTranslation), and translating it back into the original language with the same system (BackTranslation). Finally the translation quality of Back-Translation is evaluated by using the original source texts as references. Sun et al. (2010) reported an interesting phenomenon: given a bilingual text, the BackTranslation results of the target sentences i</context>
</contexts>
<marker>Rapp, 2009</marker>
<rawString>Reinhard Rapp. 2009. The Back-translation Score: Automatic MT Evaluation at the Sentence Level without Reference Translations. In Proceedings of ACL-IJCNLP, pages 133-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Linnea Micciulla</author>
<author>Ralph Weischedel</author>
</authors>
<title>A study of translation error rate with targeted human annotation.</title>
<date>2005</date>
<tech>Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58,</tech>
<institution>University of Maryland,</institution>
<contexts>
<context position="16599" citStr="Snover et al., 2005" startWordPosition="2711" endWordPosition="2714">aluate the proposed methods in different domains, two groups of experiments were carried out, namely, the oral group (Goral) and the news group (Gnews). The experiments were conducted in both Chinese-English and EnglishChinese directions for the oral group, and ChineseEnglish direction for the news group. The English sentences were all tokenized and lowercased, and the Chinese sentences were segmented into words by Language Technology Platform (LTP)1. We used SRILM2 for the training of language models (5-gram in all the experiments). The metrics for automatic evaluation were BLEU 3 and TER 4 (Snover et al., 2005). The detailed statistics of the training data in Goral are showed in Table 2. For the bilingual corpus, we used the BTEC and PIVOT data of IWSLT 2008, HIT corpus 5 and other Chinese LDC (CLDC) 1 http://ir.hit.edu.cn/ltp/ 2 http://www.speech.sri.com/projects/srilm/ 3 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl 4 http://www.umiacs.umd.edu/~snover/terp/ 5 The HIT corpus contains the CLDC Olympic corpus (2004- 863-008) and the other HIT corpora available at http://mitlab.hit.edu.cn/index.php/resources/29-theresource/111-share-bilingual-corpus.html. 983 Corpus #Sen. pairs #Ch. words #En</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Makhoul, Micciulla, Weischedel, 2005</marker>
<rawString>Matthew Snover, Bonnie J. Dorr, Richard Schwartz, John Makhoul, Linnea Micciulla, and Ralph Weischedel. 2005. A study of translation error rate with targeted human annotation. Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58, University of Maryland, July, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanli Sun</author>
<author>Sharon O’Brien</author>
<author>Minako O’Hagan</author>
<author>Fred Hollowood</author>
</authors>
<date>2010</date>
<journal>A Novel Statistical Pre-Processing</journal>
<marker>Sun, O’Brien, O’Hagan, Hollowood, 2010</marker>
<rawString>Yanli Sun, Sharon O’Brien, Minako O’Hagan and Fred Hollowood. 2010. A Novel Statistical Pre-Processing</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>