<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990435">
Co-Feedback Ranking for Query-Focused Summarization
</title>
<author confidence="0.910961">
Furu Wei1,2,3 Wenjie Li1 and Yanxiang He2
</author>
<affiliation confidence="0.493435">
1 The Hong Kong Polytechnic University, Hong Kong 2 Wuhan University, China
</affiliation>
<email confidence="0.947069">
{csfwei,cswjli}@comp.polyu.edu.hk {frwei,yxhe}@whu.edu.cn
</email>
<sectionHeader confidence="0.950876" genericHeader="abstract">
3 IBM China Research Laboratory, Beijing, China
Abstract
</sectionHeader>
<bodyText confidence="0.9965021875">
In this paper, we propose a novel ranking
framework – Co-Feedback Ranking (Co-
FRank), which allows two base rankers to
supervise each other during the ranking
process by providing their own ranking results
as feedback to the other parties so as to boost
the ranking performance. The mutual ranking
refinement process continues until the two
base rankers cannot learn from each other any
more. The overall performance is improved by
the enhancement of the base rankers through
the mutual learning mechanism. We apply this
framework to the sentence ranking problem in
query-focused summarization and evaluate its
effectiveness on the DUC 2005 data set. The
results are promising.
</bodyText>
<sectionHeader confidence="0.989254" genericHeader="introduction">
1 Introduction and Background
</sectionHeader>
<bodyText confidence="0.998529742857143">
Sentence ranking is the issue of most concern in
extractive summarization. Feature-based
approaches rank the sentences based on the
features elaborately designed to characterize the
different aspects of the sentences. They have
been extensively investigated in the past due to
their easy implementation and the ability to
achieve promising results. The use of feature-
based ranking has led to many successful (e.g.
top five) systems in DUC 2005-2007 query-
focused summarization (Over et al., 2007). A
variety of statistical and linguistic features, such
as term distribution, sentence length, sentence
position, and named entity, etc., can be found in
literature. Among them, query relevance,
centroid (Radev et al., 2004) and signature term
(Lin and Hovy, 2000) are most remarkable.
There are two alternative approaches to
integrate the features. One is to combine features
into a unified representation first, and then use it
to rank the sentences. The other is to utilize rank
fusion or rank aggregation techniques to combine
the ranking results (orders, ranks or scores)
produced by the multiple ranking functions into a
unified rank. The most popular implementation
of the latter approaches is to linearly combine the
features to obtain an overall score which is then
used as the ranking criterion. The weights of the
features are either experimentally tuned or
automatically derived by applying learning-based
mechanisms. However, both of the above-
mentioned “combine-then-rank” and “rank-then-
combine” approaches have a common drawback.
They do not make full use of the information
provided by the different ranking functions and
neglect the interaction among them before
combination. We believe that each individual
ranking function (we call it base ranker) is able
to provide valuable information to the other base
rankers such that they learn from each other by
means of mutual ranking refinement, which in
turn results in overall improvement in ranking.
To the best of our knowledge, this is a research
area that has not been well addressed in the past.
The inspiration for the work presented in this
paper comes from the idea of Co-Training (Blum
and Mitchell, 1998), which is a very successful
paradigm in the semi-supervised learning
framework for classification. In essence, co-
training employs two weak classifiers that help
augment each other to boost the performance of
the learning algorithms. Two classifiers mutually
cooperate with each other by providing their own
labeling results to enrich the training data for the
other parties during the supervised learning
process. Analogously, in the context of ranking,
although each base ranker cannot decide the
overall ranking well on itself, its ranking results
indeed reflect its opinion towards the ranking
from its point of view. The two base rankers can
then share their own opinions by providing the
ranking results to each other as feedback. For
each ranker, the feedback from the other ranker
contains additional information to guide the
refinement of its ranking results if the feedback
is defined and used appropriately. This process
continues until the two base rankers can not learn
from each other any more. We call this ranking
paradigm Co-Feedback Ranking (Co-FRank).
The way how to use the feedback information
</bodyText>
<page confidence="0.969187">
117
</page>
<note confidence="0.6681085">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 117–120,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999539714285714">
varies depending on the nature of a ranking task.
In this paper, we particularly consider the task of
query-focused summarization. We design a new
sentence ranking algorithm which allows a
query-dependent ranker and a query-independent
ranker mutually learn from each other under the
Co-FRank framework.
</bodyText>
<sectionHeader confidence="0.7611935" genericHeader="method">
2 Co-Feedback Ranking for Query-
Focused Summarization
</sectionHeader>
<subsectionHeader confidence="0.7423675">
2.1 Co-Feedback Ranking Framework
Given a set of objects O, one can define two base
</subsectionHeader>
<bodyText confidence="0.951993789473684">
ranker f1 and f2: f o f o o O
1   , 2   ,   . The
ranking results produced by f1 and f2 individually
are by no means perfect but the two rankers can
provide relatively reasonable ranking
information to supervise each other so as to
jointly improve themselves. One way to do Co-
Feedback ranking is to take the most confident
ranking results (e.g. highly ranked instances
based on orders, ranks or scores) from one base
ranker as feedback to update the other’s ranking
results, and vice versa. This process continues
iteratively until the termination condition is
reached, as depicted in Procedure 1. While the
standard Co-Training algorithm requires two
sufficient and redundant views, we suggest f1 and
f2 be two independent rankers which emphasize
two different aspects of the objects in O.
Procedure 1. Co-FRank(f1, f2, O)
</bodyText>
<listItem confidence="0.9940695">
1: Rank O with f1 and obtain the ranking results r1;
2: Rank O with f2 and obtain the ranking results r2;
3: Repeat
4: Select the top N ranked objects 1 from r1 as
</listItem>
<bodyText confidence="0.8024272">
feedback to supervise f2, and re-rank O using f2
and 1; Update r2;
5: Select the top N ranked objects 2 from r2 as
feedback to supervise f1, and re-rank O using f1
and 2 ; Update r1;
</bodyText>
<sectionHeader confidence="0.753753" genericHeader="method">
5: Until I(O).
</sectionHeader>
<bodyText confidence="0.978004375">
The termination condition I(O) can be defined
according to different application scenarios. For
example, I(O) may require the top K ranked
objects in r1 and r2 to be identical if one is
particularly interested in the top ranked objects.
It is also very likely that r1 and r2 do not change
any more after several iterations (or the top K
objects do not change). In this case, the two base
rankers can not learn from each other any more,
and the Co-Feedback ranking process should
terminate either. The final ranking results can be
easily determined by combining the two base
rankers without any parameter, because they
have already learnt from each other and can be
equally treated.
2.2 Query-Focused Summarization based
on Co-FRank
The task of query-focused summarization is to
produce a short summary (250 words in length)
for a set of related documents D with respect to
the query q that reflects a user’s information
need. We follow the traditional extractive
summarization framework in this study, where
the two critical processes involved are sentence
ranking and sentence selection, yet we focus
more on the sentence ranking algorithm based on
Co-FRank. As for sentence selection, we
incrementally add into the summary the highest
ranked sentence if it doesn’t significantly repeat1
the information already included in the summary
until the word limitation is reached.
In the context of query-focused summarization,
two kinds of features, i.e. query-dependent and
query-independent features are necessary and
they are supposed to complement each other. We
then use these two kinds of features to develop
the two base rankers. The query-dependent
feature (i.e. the relevance of the sentence s to the
query q) is defined as the cosine similarity
between s and q.
</bodyText>
<equation confidence="0.432873333333333">
f rel  s q   s q  s q
 ,  cos ,   /
1
</equation>
<bodyText confidence="0.9681755">
The words in s and q vectors are weighted by
tf*isf. Meanwhile, the query-independent feature
(i.e. the sentence significance based on word
centroid) is defined as
</bodyText>
<equation confidence="0.4903165">
f2 cs  w  s cw/ s
  (2)
</equation>
<bodyText confidence="0.9486455">
where c(w) is the centroid weight of the word w
in s and     D
</bodyText>
<figure confidence="0.872274090909091">
c w     . D
NS is the total
s D tf w s isf N
w S
number of the sentences in D, s
tf is the
w
frequency of w in s, and  w
D
isfw  log N sf is the
S
</figure>
<bodyText confidence="0.99564625">
inverse sentence frequency (ISF) of w, where sfw
is the sentence frequency of w in D. The sentence
ranking algorithm based on Co-FRank is detailed
in the following Algorithm 1.
</bodyText>
<construct confidence="0.445526">
Algorithm 1. Co-FRank(f1, f2, D, q)
</construct>
<listItem confidence="0.791823166666667">
1: Extract sentences S={s1, ... sm} from D;
2: Rank S with f1 and obtain the ranking results r1;
3: Rank S with f2 and obtain the ranking results r2;
4: Normalize r1, r1 si  r1 si  minr1  maxr1 minr1 ;
5: Normalize r2, r2sir2siminr2maxr2minr2;
6: Repeat
</listItem>
<tableCaption confidence="0.29813">
1 A sentence is discarded if the cosine similarity of it to any
sentence already selected into the summary is greater than
0.9.
</tableCaption>
<figure confidence="0.958498">
(1)
s  q
118
7: Select the top N ranked sentences at round n n

1
r2si    f2 si  1  2 si  (3)
8: Select the top N ranked sentences at round n n
2
from r2 as feedback for f1, and re-rank S using f1
and n
2
n  min
s  sims  n
i, /
k   1 1
   ,  

1 i 2 1
 max  1   1 
  min 
k 1
9: Until the top K sentences in
an
r 1  s i     f 1  s i    1      1  s i  (4)
r1
d r2 are the same,
both r1 and r2 do not change any more, or
maximum iteration round is achieved.
10: Calculate the final ranking results,
rsi  r1 si  r2 si  2 . (5)
</figure>
<bodyText confidence="0.9916516">
The update strategies used in Algorithm 1, as
formulated in Formulas (3) and (4), are designed
based on the intuition that the new ranking of the
sentence s from one base ranker (say f1) consists
of two parts. The first part is the initial ranking
produced by f1. The second part is the similarity
between s and the top N feedback provided by
the other ranker (say f2), and vice versa. The top
K ranked sentences by f2 are supposed to be
highly supported by f2. As a result, a sentence
that is similar to those top ranked sentences
should deserve a high rank as well.  n 
sim s i 2
,
captures the effect of such feedback at round n
and the definition of it may vary with regard to
the application background. For example, it can
be defined as the maximum, the minimum or the
average similarity value between si and a set of
feedback sentences in 2. Through this mutual
interaction, the two base rankers supervise each
other and are expected as a whole to produce
more reliable ranking results.
We assume each base ranker is most confident
with its first ranked sentence and set N to 1.
</bodyText>
<subsectionHeader confidence="0.260136">
Accordingly,  n 
</subsectionHeader>
<bodyText confidence="0.853538181818182">
sim si 2
, is defined as the similarity
between si and the one sentence in n
 .  is a
2
balance factor which can be viewed as the
proportion of the dependence of the new ranking
results on its initial ranking results. K is set to 10
as 10 sentences are basically sufficient for the
summarization task we work on. We carry out at
most 5 iterations in the current implementation.
</bodyText>
<sectionHeader confidence="0.977758" genericHeader="evaluation">
3 Experimental Study
</sectionHeader>
<bodyText confidence="0.998819434782609">
We take the DUC 2005 data set as the evaluation
corpus in this preliminary study. ROUGE (Lin
and Hovy, 2003), which has been officially
adopted in the DUC for years is used as the
evaluation criterion. For the purpose of
comparison, we implement the following two
basic ranking functions and the linear
combination of them for reference, i.e. the query
relevance based ranker (denoted by QRR, same
as f1) and the word centroid based ranker
(denoted by WCR, same as f2), and the linear
combined ranker, LCR=  QRR+(1-  )WCR,
where  is a combination parameter. QRR and
WCR are normalized by x min max min ,
where x, max and min denote the original ranking
score, the maximum ranking score and minimum
ranking score produced by a ranker, respectively.
Table 1 shows the results of the average recall
scores of ROUGE-1, ROUGE-2 and ROUGE-
SU4 along with their 95% confidence intervals
included within square brackets. Among them,
ROUGE-2 is the primary DUC evaluation
criterion.
</bodyText>
<table confidence="0.976217869565217">
[0.3540, 0.3654] [0.0630,
[0.1196,
0.3504 0.0644 0.1171
WCR
[0.3436, 0.3565] [0.0614, 0.0675] [0.1138, 0.1202]
0.3513 0.0645 0.1177
[0.3449,
QRR 0.3597 0.0664 0.1229
0.0697]
0.1261]
LCR*
0.3572] [0.0613, 0.0676] [0.1145, 0.1209]
Co- 0.3769 0.0762 0.1317
FRank+ [0.3712, 0.3829] [0.0724, 0.0799] [0.1282, 0.1351]
0.3753 0.0757 0.1302
LCR**
[0.3692, 0.38131 [0.0719, 0.07961 [0.1265, 0.1340]
Co- 0.3783 0.0775 0.1323
FRank++ [0.3719, 0.3852] [0.0733, 0.0810] [0.1293, 0.1360]
* The worst results produced by LCR when  = 0.1
+ The worst results produced by Co-FRank when  = 0.6
** The best results produced by LCR when  = 0.4
++ The best results produced by Co-FRank when  = 0.8
</table>
<tableCaption confidence="0.6394695">
Table 1 Compare different ranking strategies
ROUGE-1 ROUGE-2 ROUGE-SU4
</tableCaption>
<bodyText confidence="0.999974705882353">
Note that the improvement of LCR over QRR
and WCR is rather significant if the combination
parameter  is selected appropriately. Besides,
Co-FRank is always superior to LCR regardless
of the best or the worst ouput, and the
improvement is visible. The reason is that both
QRR and WCR are enhanced step by step in Co-
FRank, which in turn results in the increased
overall performance. The trend of the
improvement has been clearly observed in the
experiments. This observation validates our
motivation and the rationality of the algorithm
proposed in this paper and motivates our further
investigation on this topic.
We continue to examine the parameter settings
in LCR and Co-FRank. Table 2 shows the results
of LCR when the value of  changes from 0.1 to
</bodyText>
<equation confidence="0.8174375">
,
n  min  
 2
s sim  s  n
i, /
k 2
   ,
 
2 i 1 2
 max 2  2 
  min 
k 1
</equation>
<bodyText confidence="0.601311">
from r1 as feedback for f2, and re-rank S using f2
and n
</bodyText>
<figure confidence="0.944764333333333">

1
;
</figure>
<page confidence="0.99411">
119
</page>
<bodyText confidence="0.998378727272727">
1.0, and Table 3 shows the results of Co-FRank
with  ranging from 0.5 to 0.9. Notice that  is
not a combination parameter. We believe that a
base ranker should have at least half belief in its
initial ranking results and thus the value of the 
should be greater than 0.5. We find that LCR
heavily depends on  . LCR produces relatively
good and stable results with  varying from 0.4
to 0.6. However, the ROUGE scores drop
apparently when  heading towards its two end
values, i.e. 0.1 and 1.0.
</bodyText>
<table confidence="0.989094473684211">
 ROUGE-1 ROUGE-2 ROUGE-SU4
0.1 0.3513 0.0645 0.1177
[0.3449, 0.3572] [0.0613, 0.0676] [0.1145, 0.1209]
0.2 0.3623 0.0699 0.1235
[0.3559, 0.3685] [0.0662, 0.0736] [0.1197, 0.1271]
0.3 0.3721 0.0741 0.1281
[0.3660, 0.3778] [0.0706, 0.0778] [0.1246, 0.1318]
0.4 0.3753 0.0757 0.1302
[0.3692, 0.3813] [0.0719, 0.0796] [0.1265, 0.1340]
0.5 0.3756 0.0755 0.1307
[0.3698, 0.3814] [0.0717, 0.0793] [0.1272, 0.1342]
0.6 0.3770 0.0754 0.1323
[0.3710, 0.3826] [0.0716, 0.0791] [0.1286, 0.1357]
0.7 0.3698 0.0718 0.1284
[0.3636, 0.3759] [0.0680, 0.0756] [0.1246, 0.1318]
0.8 0.3672 0.0706 0.1271
[0.3613, 0.3730] [0.0669, 0.0743] [0.1234, 0.1305]
0.9 0.3651 0.0689 0.1258
[0.3591, 0.3708] [0.0652, 0.0726] [0.1220, 0.1293]
</table>
<tableCaption confidence="0.998132">
Table 2 LCR with different  values
</tableCaption>
<bodyText confidence="0.9990072">
As shown in Table 3, the Co-FRank can
always produce stable and promising results
regardless of the change of  . More important,
even the worst result produced by Co-FRank still
outperforms the best result produced by LCR.
</bodyText>
<table confidence="0.948100363636364">
 ROUGE-1 ROUGE-2 ROUGE-SU4
0.5 0.3750 0.0766 0.1308
[0.3687, 0.3810] [0.0727, 0.0804] [0.1270, 0.1344]
0.6 0.3769 0.0762 0.1317
[0.3712, 0.3829] [0.0724, 0.0799] [0.1282, 0.1351]
0.7 0.3775 0.0763 0.1319
[0.3713, 0.3835] [0.0724, 0.0801] [0.1282, 0.1354]
0.8 0.3783 0.0775 0.1323
[0.3719, 0.3852] [0.0733, 0.0810] [0.1293, 0.1360]
0.9 0.3779 0.0765 0.1319
[0.3722, 0.3835] [0.0728, 0.0803] [0.1285, 0.1354
</table>
<tableCaption confidence="0.999211">
Table 3 Co-FRank with different  values
</tableCaption>
<bodyText confidence="0.998424142857143">
We then compare our results to the DUC
participating systems. We present the following
representative ROUGE results of (1) the top
three DUC participating systems according to
ROUGE-2 scores (S15, S17 and S10); and (2)
the NIST baseline which simply selects the first
sentences from the documents.
</bodyText>
<table confidence="0.896719">
ROUGE-1 ROUGE-2 ROUGE-SU4
Co-FRank 0.3783 0.0775 0.1323
S15 - 0.0725 0.1316
S17 - 0.0717 0.1297
S10 - 0.0698 0.1253
Baseline 0.0403 0.0872
</table>
<tableCaption confidence="0.993257">
Table 4 Compare with DUC participating systems
</tableCaption>
<bodyText confidence="0.999948">
It is clearly shown in Table 4 that Co-FRank
can produce a very competitive result, which
significantly outperforms the NIST baseline and
meanwhile it is superior to the best participating
system in the DUC 2005.
</bodyText>
<sectionHeader confidence="0.997073" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999858928571429">
In this paper, we propose a novel ranking
framework, namely Co-Feedback Ranking (Co-
FRank), and examine its effectiveness in query-
focused summarization. There is still a lot of
work to be done on this topic. Although we show
the promising achievements of Co-Frank from
the perspective of experimental studies, we
expect a more theoretical analysis on Co-FRank.
Meanwhile, we would like to investigate more
appropriate techniques to use feedback, and we
are interested in applying Co-FRank to the other
applications, such as opinion summarization
where the integration of opinion-biased and
document-biased ranking is necessary.
</bodyText>
<sectionHeader confidence="0.997761" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99943575">
The work described in this paper was supported
by the Hong Kong Polytechnic University
internal the grants (G-YG80 and G-YH53) and
the China NSF grant (60703008).
</bodyText>
<sectionHeader confidence="0.99922" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9956724">
Avrim Blum and Tom Mitchell. 1998. Combining
Labeled and Unlabeled Data with Co-Training. In
Proceedings of the Eleventh Annual Conference on
Computational Learning Theory, pp92-100.
Chin-Yew Lin and Eduard Hovy. 2000. The
Automated Acquisition of Topic Signature for Text
Summarization. In Proceedings of COLING,
pp495-501.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL, pp71-78.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys,
and Daniel Tam. 2004. Centroid-based
Summarization of Multiple Documents.
Information Processing and Management, 40:919-
938.
Paul Over, Hoa Dang and Donna Harman. 2007. DUC
in Context. Information Processing and
Management, 43(6):1506-1520.
</reference>
<page confidence="0.996258">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.133139">
<title confidence="0.996572">Co-Feedback Ranking for Query-Focused Summarization Wenjie and Yanxiang</title>
<author confidence="0.898435">Hong Kong Polytechnic University</author>
<author confidence="0.898435">Hong Kong University</author>
<author confidence="0.898435">China</author>
<email confidence="0.473545">csfwei@whu.edu.cn</email>
<email confidence="0.473545">cswjli}@comp.polyu.edu.hk{frwei@whu.edu.cn</email>
<email confidence="0.473545">yxhe@whu.edu.cn</email>
<affiliation confidence="0.343265">China Research Laboratory, Beijing, China</affiliation>
<abstract confidence="0.999763647058824">In this paper, we propose a novel ranking framework – Co-Feedback Ranking (Co- FRank), which allows two base rankers to supervise each other during the ranking process by providing their own ranking results as feedback to the other parties so as to boost the ranking performance. The mutual ranking refinement process continues until the two base rankers cannot learn from each other any more. The overall performance is improved by the enhancement of the base rankers through the mutual learning mechanism. We apply this framework to the sentence ranking problem in query-focused summarization and evaluate its effectiveness on the DUC 2005 data set. The results are promising.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Eleventh Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="3166" citStr="Blum and Mitchell, 1998" startWordPosition="481" endWordPosition="484">t make full use of the information provided by the different ranking functions and neglect the interaction among them before combination. We believe that each individual ranking function (we call it base ranker) is able to provide valuable information to the other base rankers such that they learn from each other by means of mutual ranking refinement, which in turn results in overall improvement in ranking. To the best of our knowledge, this is a research area that has not been well addressed in the past. The inspiration for the work presented in this paper comes from the idea of Co-Training (Blum and Mitchell, 1998), which is a very successful paradigm in the semi-supervised learning framework for classification. In essence, cotraining employs two weak classifiers that help augment each other to boost the performance of the learning algorithms. Two classifiers mutually cooperate with each other by providing their own labeling results to enrich the training data for the other parties during the supervised learning process. Analogously, in the context of ranking, although each base ranker cannot decide the overall ranking well on itself, its ranking results indeed reflect its opinion towards the ranking fr</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pp92-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The Automated Acquisition of Topic Signature for Text Summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="1752" citStr="Lin and Hovy, 2000" startWordPosition="256" endWordPosition="259">elaborately designed to characterize the different aspects of the sentences. They have been extensively investigated in the past due to their easy implementation and the ability to achieve promising results. The use of featurebased ranking has led to many successful (e.g. top five) systems in DUC 2005-2007 queryfocused summarization (Over et al., 2007). A variety of statistical and linguistic features, such as term distribution, sentence length, sentence position, and named entity, etc., can be found in literature. Among them, query relevance, centroid (Radev et al., 2004) and signature term (Lin and Hovy, 2000) are most remarkable. There are two alternative approaches to integrate the features. One is to combine features into a unified representation first, and then use it to rank the sentences. The other is to utilize rank fusion or rank aggregation techniques to combine the ranking results (orders, ranks or scores) produced by the multiple ranking functions into a unified rank. The most popular implementation of the latter approaches is to linearly combine the features to obtain an overall score which is then used as the ranking criterion. The weights of the features are either experimentally tune</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The Automated Acquisition of Topic Signature for Text Summarization. In Proceedings of COLING, pp495-501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram Cooccurrence Statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="11127" citStr="Lin and Hovy, 2003" startWordPosition="1959" endWordPosition="1962"> assume each base ranker is most confident with its first ranked sentence and set N to 1. Accordingly,  n  sim si 2 , is defined as the similarity between si and the one sentence in n  .  is a 2 balance factor which can be viewed as the proportion of the dependence of the new ranking results on its initial ranking results. K is set to 10 as 10 sentences are basically sufficient for the summarization task we work on. We carry out at most 5 iterations in the current implementation. 3 Experimental Study We take the DUC 2005 data set as the evaluation corpus in this preliminary study. ROUGE (Lin and Hovy, 2003), which has been officially adopted in the DUC for years is used as the evaluation criterion. For the purpose of comparison, we implement the following two basic ranking functions and the linear combination of them for reference, i.e. the query relevance based ranker (denoted by QRR, same as f1) and the word centroid based ranker (denoted by WCR, same as f2), and the linear combined ranker, LCR=  QRR+(1-  )WCR, where  is a combination parameter. QRR and WCR are normalized by x min max min , where x, max and min denote the original ranking score, the maximum ranking score and minimum r</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic Evaluation of Summaries Using N-gram Cooccurrence Statistics. In Proceedings of HLTNAACL, pp71-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Stys</author>
<author>Daniel Tam</author>
</authors>
<date>2004</date>
<booktitle>Centroid-based Summarization of Multiple Documents. Information Processing and Management,</booktitle>
<pages>40--919</pages>
<contexts>
<context position="1712" citStr="Radev et al., 2004" startWordPosition="249" endWordPosition="252">ank the sentences based on the features elaborately designed to characterize the different aspects of the sentences. They have been extensively investigated in the past due to their easy implementation and the ability to achieve promising results. The use of featurebased ranking has led to many successful (e.g. top five) systems in DUC 2005-2007 queryfocused summarization (Over et al., 2007). A variety of statistical and linguistic features, such as term distribution, sentence length, sentence position, and named entity, etc., can be found in literature. Among them, query relevance, centroid (Radev et al., 2004) and signature term (Lin and Hovy, 2000) are most remarkable. There are two alternative approaches to integrate the features. One is to combine features into a unified representation first, and then use it to rank the sentences. The other is to utilize rank fusion or rank aggregation techniques to combine the ranking results (orders, ranks or scores) produced by the multiple ranking functions into a unified rank. The most popular implementation of the latter approaches is to linearly combine the features to obtain an overall score which is then used as the ranking criterion. The weights of the</context>
</contexts>
<marker>Radev, Jing, Stys, Tam, 2004</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, and Daniel Tam. 2004. Centroid-based Summarization of Multiple Documents. Information Processing and Management, 40:919-938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
<author>Hoa Dang</author>
<author>Donna Harman</author>
</authors>
<date>2007</date>
<booktitle>DUC in Context. Information Processing and Management,</booktitle>
<pages>43--6</pages>
<contexts>
<context position="1487" citStr="Over et al., 2007" startWordPosition="216" endWordPosition="219">rization and evaluate its effectiveness on the DUC 2005 data set. The results are promising. 1 Introduction and Background Sentence ranking is the issue of most concern in extractive summarization. Feature-based approaches rank the sentences based on the features elaborately designed to characterize the different aspects of the sentences. They have been extensively investigated in the past due to their easy implementation and the ability to achieve promising results. The use of featurebased ranking has led to many successful (e.g. top five) systems in DUC 2005-2007 queryfocused summarization (Over et al., 2007). A variety of statistical and linguistic features, such as term distribution, sentence length, sentence position, and named entity, etc., can be found in literature. Among them, query relevance, centroid (Radev et al., 2004) and signature term (Lin and Hovy, 2000) are most remarkable. There are two alternative approaches to integrate the features. One is to combine features into a unified representation first, and then use it to rank the sentences. The other is to utilize rank fusion or rank aggregation techniques to combine the ranking results (orders, ranks or scores) produced by the multip</context>
</contexts>
<marker>Over, Dang, Harman, 2007</marker>
<rawString>Paul Over, Hoa Dang and Donna Harman. 2007. DUC in Context. Information Processing and Management, 43(6):1506-1520.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>