<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.631703">
A Syntax-Free Approach to Japanese Sentence Compression
</title>
<note confidence="0.854431333333333">
Tsutomu HIRAO, Jun SUZUKI and Hideki ISOZAKI
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan
</note>
<email confidence="0.949629">
{hirao,jun,isozaki}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.993956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940636363636">
Conventional sentence compression meth-
ods employ a syntactic parser to compress
a sentence without changing its mean-
ing. However, the reference compres-
sions made by humans do not always re-
tain the syntactic structures of the original
sentences. Moreover, for the goal of on-
demand sentence compression, the time
spent in the parsing stage is not negligi-
ble. As an alternative to syntactic pars-
ing, we propose a novel term weighting
technique based on the positional infor-
mation within the original sentence and
a novel language model that combines
statistics from the original sentence and a
general corpus. Experiments that involve
both human subjective evaluations and au-
tomatic evaluations show that our method
outperforms Hori’s method, a state-of-the-
art conventional technique. Because our
method does not use a syntactic parser, it
is 4.3 times faster than Hori’s method.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999552070175439">
In order to compress a sentence while retaining
its original meaning, the subject-predicate rela-
tionship of the original sentence should be pre-
served after compression. In accordance with this
idea, conventional sentence compression methods
employ syntactic parsers. English sentences are
usually analyzed by a full parser to make parse
trees, and the trees are then trimmed (Knight and
Marcu, 2002; Turner and Charniak, 2005; Unno
et al., 2006). For Japanese, dependency trees are
trimmed instead of full parse trees (Takeuchi and
Matsumoto, 2001; Oguro et al., 2002; Nomoto,
2008)1 This parsing approach is reasonable be-
cause the compressed output is grammatical if the
1Hereafter, we refer these compression processes as “tree
trimming.”
input is grammatical, but it offers only moderate
compression rates.
An alternative to the tree trimming approach
is the sequence-oriented approach (McDonald,
2006; Nomoto, 2007; Clarke and Lapata, 2006;
Hori and Furui, 2003). It treats a sentence as a se-
quence of words and structural information, such
as a syntactic or dependency tree, is encoded in
the sequence as features. Their methods have the
potential to drop arbitrary words from the original
sentence without considering the boundary deter-
mined by the tree structures. However, they still
rely on syntactic information derived from fully
parsed syntactic or dependency trees.
We found that humans usually ignored the syn-
tactic structures when compressing sentences. For
example, in many cases, they compressed the sen-
tence by dropping intermediate nodes of the syn-
tactic tree derived from the source sentence. We
believe that making compression strongly depen-
dent on syntax is not appropriate for reproducing
reference compressions. Moreover, on-demand
sentence compression is made problematic by the
time spent in the parsing stage.
This paper proposes a syntax-free sequence-
oriented sentence compression method. To main-
tain the subject-predicate relationship in the com-
pressed sentence and retain fluency without us-
ing syntactic parsers, we propose two novel fea-
tures: intra-sentence positional term weighting
(IPTW) and the patched language model (PLM).
IPTW is defined by the term’s positional informa-
tion in the original sentence. PLM is a form of
summarization-oriented fluency statistics derived
from the original sentence and the general lan-
guage model. The weight parameters for these
features are optimized within the Minimum Clas-
sification Error (MCE) (Juang and Katagiri, 1992)
learning framework.
Experiments that utilize both human subjective
and automatic evaluations show that our method is
</bodyText>
<page confidence="0.979682">
826
</page>
<note confidence="0.9996695">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.9637615">
Figure 1: An example of the dependency relation between an original sentence and its compressed
variant.
</figureCaption>
<figure confidence="0.922603515151515">
Chunk7
Compressed Sentence
Source Sentence
Chunk4
Chunk1
Chunk 2 Chunk 3
Chunk1
推定 し た
suitei shi ta
Chunk 2 Chunk 3
配点 について 福武 が
haiten nitsuite fukutake ga
配点 について 福武 が
haiten nitsuite fukutake ga
枝問 部分 の
Compression
センタ試験枝問 の
edamon bubun no
center shiken
edamon no
Chunk7 = a part of Chunk6 + parts of Chunk4
センタ試験 枝問 の
center shiken
compound noun
Chunk5
公表 し て い な޿
kouhyou shi te i nai
edamon no
Chunk6
センタ試験 で
center shiken de
推定 し た
suitei shi ta
</figure>
<bodyText confidence="0.978654333333333">
superior to conventional sequence-oriented meth-
ods that employ syntactic parsers while being
about 4.3 times faster.
</bodyText>
<subsectionHeader confidence="0.734397">
2 Analysis of reference compressions
</subsectionHeader>
<bodyText confidence="0.985347690909091">
Syntactic information does not always yield im-
proved compression performance because humans
usually ignore the syntactic structures when they
compress sentences. Figure 1 shows an exam-
ple. English translation of the source sentence is
“Fukutake Publishing Co., Ltd. presumed prefer-
ential treatment with regard to its assessed scores
for a part of the questions for a series of Center
Examinations.” and its compression is “Fukutake
presumed preferential scores for questions for a
series of Center Examinations.”
In the figure, each box indicates a syntactic
chunk, bunsetsu. The solid arrows indicate de-
pendency relations between words2. We observe
that the dependency relations are changed by com-
pression; humans create compound nouns using
the components derived from different portions of
the original sentence without regard to syntactic
constraints. ‘Chunk 7’ in the compressed sen-
tence was constructed by dropping both content
and functional words and joining other content
words contained in ‘Chunk 4’ and ‘Chunk 6’ of
2Generally, a dependency relation is defined between bun-
setsu. Therefore, in order to identify word dependencies, we
followed Kudo’s rule (Kudo and Matsumoto, 2004)
the original sentence. ‘Chunk 5’ is dropped com-
pletely. This compression cannot be achieved by
tree trimming.
According to an investigation in our corpus of
manually compressed Japanese sentences, which
we used in the experimental evaluation, 98.7% of
them contain at least one segment that does not
retain the original tree structure. Human usually
compress sentences by dropping the intermediate
nodes in the dependency tree. However, the re-
sulting compressions retain both adequacy and flu-
ency. This statistic supports the view that sentence
compression that strongly depends on syntax is
not useful in reproducing reference compressions.
We need a sentence compression method that can
drop intermediate nodes in the syntactic tree ag-
gressively beyond the tree-scoped boundary.
In addition, sentence compression methods that
strongly depend on syntactic parsers have two
problems: ‘parse error’ and ‘decoding speed.’
44% of sentences output by a state-of-the-art
Japanese dependency parser contain at least one
error (Kudo and Matsumoto, 2005). Even more, it
is well known that if we parse a sentence whose
source is different from the training data of the
parser, the performance could be much worse.
This critically degrades the overall performance
of sentence compression. Moreover, summariza-
tion systems often have to process megabytes of
documents. Parsers are still slow and users of on-
</bodyText>
<page confidence="0.984335">
827
</page>
<bodyText confidence="0.997196">
demand summarization systems are not prepared
to wait for parsing to finish.
</bodyText>
<sectionHeader confidence="0.810248" genericHeader="method">
3 A Syntax Free Sequence-oriented
Sentence Compression Method
</sectionHeader>
<bodyText confidence="0.9999028">
As an alternative to syntactic parsing, we pro-
pose two novel features, intra-sentence positional
term weighting (IPTW) and the patched language
model (PLM) for our syntax-free sentence com-
pressor.
</bodyText>
<subsectionHeader confidence="0.9819795">
3.1 Sentence Compression as a
Combinatorial Optimization Problem
</subsectionHeader>
<bodyText confidence="0.963125818181818">
Suppose that a compression system reads sen-
tence x= x1 , x2, ... , xj, ... , xN, where xj
is the j-th word in the input sentence. The
system then outputs the compressed sentence y
=y1, y2, ... , yi, ..., yM, where yi is the i-
th word in the output sentence. Here, yi ∈
{x1, ... , xN}. We assume y0=x0=&lt;s&gt; (BOS)
and yM+1=xN+1=&lt;/s&gt; (EOS). We define func-
tion I(·), which maps word yi to the index of
the word in the original sentence. For example,
if source sentence is x = x1, x2, ... , x5 and its
</bodyText>
<equation confidence="0.6891845">
compressed variant is y = x1, x3, x4, I(y1) = 1,
I(y2) = 3, I(y3) = 4.
</equation>
<bodyText confidence="0.999683">
We define a significance score f(x, y, A) for
compressed sentence y based on Hori’s method
(Hori and Furui, 2003). A = {λg, λh} is a pa-
rameter vector.
</bodyText>
<equation confidence="0.99555125">
M+1
f(x, y; A) = {g(x, I(yi); λg) +
i=1
h(x, I(yi), I(yi−1); λh)} (1)
</equation>
<bodyText confidence="0.95153275">
The first term of equation (1) (g(·)) is the impor-
tance of each word in the output sentence, and the
second term (h(·)) is the the linguistic likelihood
between adjacent words in the output sentence.
The best subsequence ˆy= argmax f(x, y; A) is
y
identified by dynamic programming (DP) (Hori
and Furui, 2003).
</bodyText>
<subsectionHeader confidence="0.980126">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999951">
We use IPTW to define the significance score
g(x, I(yi); λg). Moreover, we use PLM to define
the linguistic likelihood h(x, I(yi+1), I(yi); λh).
</bodyText>
<subsectionHeader confidence="0.502842">
3.2.1 Intra-sentence Positional Term
Weighting (IPTW)
</subsectionHeader>
<bodyText confidence="0.998114">
IDF is a global term weighting scheme in that it
measures the significance score of a word in a
text corpus, which could be extremely large. By
contrast, this paper proposes another type of term
weighting; it measures the positional significance
score of a word within its sentence. Here, we as-
sume the following hypothesis:
• The “significance” of a word depends on its
position within its sentence.
In Japanese, the main subject of a sentence
usually appears at the beginning of the sentence
(BOS) and the main verb phrase almost always
appears at the end of the sentence (EOS). These
words or phrases are usually more important than
the other words in the sentence. In order to
add this knowledge to the scoring function, term
weight is modeled by the following Gaussian mix-
ture.
</bodyText>
<equation confidence="0.99689">
N(psn(x, I(yi)); λg) =
� 1 exp−2 1 (psn(x, I(yi)) − μ1 1 +
J
√2πσ1
2 σ1
2 1 exp
1 (psn(x, I(y
))
2
2
�m
−
i
− μ
√
πσ
2σ2
λg
{μk,σk, mk}k=1,2. psn(x, I(yi))
</equation>
<bodyText confidence="0.679448">
returns the relative position of yi in the original
sentence x which is defined as follows:
Here,
=
</bodyText>
<equation confidence="0.978569">
start(x, I(yi))
psn(x, I(yi)) = (3)
length(x)
</equation>
<bodyText confidence="0.985868222222222">
‘length(x)’ denotes the number of characters in
the source sentence and ‘start(x,I(yi))’ denotes
the accumulated run of characters from BOS to
(x,I(yi)). In equation (2), μk,σk indicates the
mean and the standard deviation for the normal
distribution, respectively. mk is a mixture param-
eter.
We use the distribution (2) in defining
g(x, I(yi); λg) as follows:
</bodyText>
<equation confidence="0.878451375">
g(x,I(yi);λg) =
2)2) (2)
{ IDF(x, I(yi)) × N(psn(x, I(yi); λg)
if pos(x,I(yi)) =noun, verb, adjective(4)
Constant × N(psn(x, I(yi); λg)
otherwise
828
m1
</equation>
<bodyText confidence="0.97969">
Here, pos(x, I(yi)) denotes the part-of-speech tag
for yi. Ag is optimized by using the MCE learning
framework.
</bodyText>
<subsectionHeader confidence="0.730416">
3.2.2 Patched Language Model
</subsectionHeader>
<bodyText confidence="0.999864764705882">
Many studies on sentence compression employ the
n-gram language model to evaluate the linguistic
likelihood of a compressed sentence. However,
this model is usually computed by using a huge
volume of text data that contains both short and
long sentences. N-gram distribution of short sen-
tences may different from that of long sentences.
Therefore, the n-gram probability sometimes dis-
agrees with our intuition in terms of sentence com-
pression. Moreover, we cannot obtain a huge
corpus consisting solely of compressed sentences.
Even if we collect headlines as a kind of com-
pressed sentence from newspaper articles, corpus
size is still too small. Therefore, we propose
the following novel linguistic likelihood based on
statistics derived from the original sentences and a
huge corpus:
</bodyText>
<equation confidence="0.9636545">
PLM(x, I(yj), I(yj−1)) =
1 if I(yj) = I(yj−1) + 1
APLM Bigram(x, I(yj), I(yj−1)) (5)
otherwise
</equation>
<bodyText confidence="0.999875571428571">
PLM stands for Patched Language Model.
Here, 0 &lt; APLM &lt; 1, Bigram(·) indicates word
bigram probability. The first line of equation (5)
agrees with Jing’s observation on sentence align-
ment tasks (Jing and McKeown, 1999); that is,
most (or almost all) bigrams in a compressed sen-
tence appear in the original sentence as they are.
</bodyText>
<subsectionHeader confidence="0.502366">
3.2.3 POS bigram
</subsectionHeader>
<bodyText confidence="0.9910795">
Since POS bigrams are useful for rejecting un-
grammatical sentences, we adopt them as follows:
</bodyText>
<equation confidence="0.998528">
Ppos(x,I(yi+1)|I(yi)) =
P(pos(x, I(yi+1))|pos(x,I(yi))). (6)
</equation>
<bodyText confidence="0.9964875">
Finally, the linguistic likelihood between adja-
cent words within y is defined as follows:
</bodyText>
<equation confidence="0.928756333333333">
h(x,I(yi+1),I(yi);Ah) =
PLM(x,I(yi+1),I(yi)) +
A(pos(x,I(y,+,))|pos(x,I(y,)))Ppos(x, I(yi+1)|I(yi))
</equation>
<subsectionHeader confidence="0.990878">
3.3 Parameter Optimization
</subsectionHeader>
<bodyText confidence="0.999991117647059">
We can regard sentence compression as a two class
problem: we give a word in the original sentence
class label +1 (the word is used in the compressed
output) or −1 (the word is not used). In order to
consider the interdependence of words, we employ
the Minimum Classification Error (MCE) learning
framework (Juang and Katagiri, 1992), which was
proposed for learning the goodness of a sequence.
xt denotes the t-th original sentence in the training
data set T. yt denotes the reference compression
that is made by humans and yt is a compressed
sentence output by a system.
When using the MCE framework, the misclas-
sification measure is defined as the difference be-
tween the score of the reference sentence and that
of the best non-reference output and we optimize
the parameters by minimizing the measure.
</bodyText>
<equation confidence="0.9961172">
|T|
d(y, x; A) = { L f(xt,yt;A)
t=1
− max f(xt, yt; A)} (7)
147�y∗�
</equation>
<bodyText confidence="0.99997275">
It is impossible to minimize equation (7) because
we cannot derive the gradient of the function.
Therefore, we employ the following sigmoid func-
tion to smooth this measure.
</bodyText>
<equation confidence="0.987831214285714">
L(d(x, y; A)) =
L
t=1
|T|
1 + exp(−c x d(xt, yt; A)) (8)
1
Here, c is a constant parameter. To minimize equa-
tion (8), we use the following equation.
VL= aL �ad �
ad aA1, aA2, ... =
ad 0 (9)
Here, ∂L∂d is given by:
1 + exp (−c x d) (1 1 + exp (−c x d) )
(10)
</equation>
<bodyText confidence="0.954793666666667">
Finally, the parameters are optimized by using
the iterative form. For example, Aw is optimized
as follows:
</bodyText>
<equation confidence="0.986402125">
aL
Aw(new) = Aw(old) − � (11)
aAw(old)
⎧
⎨
⎩
aL
ad =
</equation>
<page confidence="0.992975">
829
</page>
<bodyText confidence="0.9993002">
Our parameter optimization procedure can be
replaced by another one such as MIRA (McDon-
ald et al., 2005) or CRFs (Lafferty et al., 2001).
The reason why we employed MCE is that it is
very easy to implement.
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999772">
4.1 Corpus and Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999991878787879">
We randomly selected 1,000 lead sentences (a lead
sentence is the first sentence of an article exclud-
ing the headline.) whose length (number of words)
was greater than 30 words from the Mainichi
Newspaper from 1994 to 2002. There were five
different ideal compressions (reference compres-
sions produced by human) for each sentence; all
had a 0.6 compression rate. The average length of
the input sentences was about 42 words and that of
the reference compressions was about 24 words.
For MCE learning, we selected the reference
compression that maximize the BLEU score (Pap-
ineni et al., 2002) (= argmaxr∈RBLEU(r, R\r))
from the set of reference compressions and used it
as correct data for training. Note that r is a ref-
erence compression and R is the set of reference
compressions.
We employed both automatic evaluation and hu-
man subjective evaluation. For automatic evalua-
tion, we employed BLEU (Papineni et al., 2002)
by following (Unno et al., 2006). We utilized 5-
fold cross validation, i.e., we broke the whole data
set into five blocks and used four of them for train-
ing and the remainder for testing and repeated the
evaluation on the test data five times changing the
test block each time.
We also employed human subjective evaluation,
i.e., we presented the compressed sentences to six
human subjects and asked them to evaluate the
sentence for fluency and importance on a scale 1
(worst) to 5 (best). For each source sentence, the
order in which the compressed sentences were pre-
sented was random.
</bodyText>
<subsectionHeader confidence="0.773664">
4.2 Comparison of Sentence Compression
Methods
</subsectionHeader>
<bodyText confidence="0.916955142857143">
In order to investigate the effectiveness of the pro-
posed features, we compared our method against
Hori’s model (Hori and Furui, 2003), which is
a state-of-the-art Japanese sentence compressor
based on the sequence-oriented approach.
Table 1 shows the feature set used in our exper-
iment. Note that ‘Hori−’ indicates the earlier ver-
</bodyText>
<tableCaption confidence="0.999493">
Table 1: Configuration setup
</tableCaption>
<table confidence="0.999822444444444">
Label g() h()
Proposed IPTW PLM + POS
w/o PLM IPTW Bigram+POS
w/o IPTW IDF PLM+POS
Hori− IDF Trigram
Proposed+Dep IPTW PLM + POS +Dep
w/o PLM+Dep IPTW Bigram+POS+Dep
w/o IPTW+Dep IDF PLM+POS+Dep
Hori IDF Trigram+Dep
</table>
<tableCaption confidence="0.996438">
Table 2: Results: automatic evaluation
</tableCaption>
<equation confidence="0.6814202">
Label BLEU
Proposed
w/o PLM
w/o IPTW
Hori−
</equation>
<bodyText confidence="0.9537181875">
Proposed+Dep
w/o PLM+Dep
w/o IPTW+Dep
Hori
sion of Hori’s method which does not require the
dependency parser. For example, label ‘w/o IPTW
+ Dep’ employs IDF term weighting as function
g(·) and word bigram, part-of-speech bigram and
dependency probability between words as func-
tion h(·) in equation (1).
To obtain the word dependency probability, we
use Kudo’s relative-CaboCha (Kudo and Mat-
sumoto, 2005). We developed the n-gram lan-
guage model from a 9 year set of Mainichi News-
paper articles. We optimized the parameters by
using the MCE learning framework.
</bodyText>
<sectionHeader confidence="0.999729" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.999115">
5.1 Results: automatic evaluation
</subsectionHeader>
<bodyText confidence="0.999953181818182">
Table 2 shows the evaluation results yielded by
BLUE at the compression rate of 0.60.
Without introducing dependency probability,
both IPTW and PLM worked well. Our method
achieved the highest BLEU score. Compared to
‘Proposed’, ‘w/o IPTW’ offers significantly worse
performance. The results support the view that our
hypothesis, namely that the significance score of
a word depends on its position within a sentence,
is effective for sentence compression. Figure 2
shows an example of Gaussian mixture with pre-
</bodyText>
<figure confidence="0.9767806">
.679
.617
.635
.493
.632
.669
.656
.600
830
0.2
0.15
0.1
0.05
0
&lt;S&gt;
</figure>
<figureCaption confidence="0.9620395">
Figure 2: An example of Gaussian mixture with
predicted parameters
</figureCaption>
<bodyText confidence="0.9988155625">
dicted parameters. From the figure, we can see
that the positional weights for words have peaks
at BOS and EOS. This is because, in many cases,
the subject appears at the beginning of Japanese
sentences and the predicate at the end.
Replacing PLM with the bigram language
model (w/o PLM) degrades the performance sig-
nificantly. This result shows that the n-gram lan-
guage model is improper for sentence compres-
sion because the n-gram probability is computed
by using a corpus that includes both short and long
sentences. Most bigrams in a compressed sentence
followed those in the source sentence.
The dependency probability is very helpful pro-
vided either IPTW or PLM is employed. For ex-
ample, ‘w/o PLM + Dep’ achieved the second
highest BLEU score. The difference of the score
between ‘Proposed’ and ‘w/o PLM + Dep’ is only
0.01 but there were significant differences as de-
termined by Wilcoxon signed rank test. Compared
to ‘Hori−’, ‘Hori’ achieved a significantly higher
BLEU score.
The introduction of both IPTW and PLM makes
the use of dependency probability unnecessary. In
fact, the score of ‘Proposed + Dep’ is not good.
We believe that this is due to overfitting. PLM
is similar to dependency probability in that both
features emphasize word pairs that occurred as
bigrams in the source sentence. Therefore, by
introducing dependency probability, the informa-
tion within the feature vector is not increased even
though the number of features is increased.
</bodyText>
<tableCaption confidence="0.999428">
Table 3: Results: human subjective evaluations
</tableCaption>
<table confidence="0.997991666666667">
Label Fluency Importance
Proposed 4.05 (±0.846) 3.33 (±0.854)
w/o PLM + Dep 3.91 (±0.759) 3.24 (±0.753)
Hori− 3.09 (±0.899) 2.34 (±0.696)
Hori 3.28 (±0.924) 2.64 (±0.819)
Human 4.86 (±0.268) 4.66 (±0.317)
</table>
<subsectionHeader confidence="0.933671">
5.2 Results: human subjective evaluation
</subsectionHeader>
<bodyText confidence="0.995711586206897">
We used human subjective evaluations to compare
our method to human compression, ‘w/o PLM +
Dep’ which achieved the second highest perfor-
mance in the automatic evaluation, ‘Hori−’ and
‘Hori’. We randomly selected 100 sentences from
the test corpus and evaluated their compressed
variants in terms of ‘fluency’ and ‘importance.’
Table 3 shows the results, mean score of all
judgements as well as the standard deviation.
The results indicate that human compression
achieved the best score in both fluency and impor-
tance. Human compression significantly outper-
formed other compression methods. This results
supports the idea that humans can easily compress
sentences with the compression rate of 0.6. Of
the automatic methods, our method achieved the
best score in both fluency and importance while
‘Hori−’ was the worst performer. Our method sig-
nificantly outperformed both ‘Hori’ and ‘Hori−’
on both metrics. Moreover, our method outper-
formed ‘w/o PLM + Dep’ again. However, the
differences in the scores are not significant. We
believe that this is due to a lack of data. If we use
more data for the significant test, significant dif-
ferences will be found. Although our method does
not employ any explicit syntactic information, its
fluency and importance are extremely good. This
confirms the effectiveness of the new features of
IPTW and PLM.
</bodyText>
<subsectionHeader confidence="0.998881">
5.3 Comparison of decoding speed
</subsectionHeader>
<bodyText confidence="0.987275833333333">
We compare the decoding speed of our method
against that of Hori’s method.
We measured the decoding time for all 1,000
test sentences on a standard Linux Box (CPU:
Intel© CoreTm 2 Extreme QX9650 (3.00GHz),
Memory: 8G Bytes). The results were as follows:
</bodyText>
<figure confidence="0.575574428571429">
Proposed: 22.14 seconds
(45.2 sentences / sec),
0 N/4 N/2 3N/4 N
x1s x2, ,xjl lxN &lt;/S&gt;
831
Hori: 95.34 seconds
(10.5 sentences / sec).
</figure>
<bodyText confidence="0.99941025">
Our method was about 4.3 times faster than
Hori’s method due to the latter’s use of depen-
dency parser. This speed advantage is significant
when on-demand sentence compression is needed.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="evaluation">
6 Related work
</sectionHeader>
<bodyText confidence="0.999980690476191">
Conventional sentence compression methods em-
ploy the tree trimming approach to compress a
sentence without changing its meaning. For in-
stance, most English sentence compression meth-
ods make full parse trees and trim them by ap-
plying the generative model (Knight and Marcu,
2002; Turner and Charniak, 2005), discrimina-
tive model (Knight and Marcu, 2002; Unno et
al., 2006). For Japanese sentences, instead of us-
ing full parse trees, existing sentence compression
methods trim dependency trees by the discrim-
inative model (Takeuchi and Matsumoto, 2001;
Nomoto, 2008) through the use of simple lin-
ear combined features (Oguro et al., 2002). The
tree trimming approach guarantees that the com-
pressed sentence is grammatical if the source sen-
tence does not trigger parsing error. However, as
we mentioned in Section 2, the tree trimming ap-
proach is not suitable for Japanese sentence com-
pression because in many cases it cannot repro-
duce human-produced compressions.
As an alternative to these tree trimming
approaches, sequence-oriented approaches have
been proposed (McDonald, 2006; Nomoto, 2007;
Hori and Furui, 2003; Clarke and Lapata, 2006).
Nomoto (2007) and McDonald (2006) employed
the random field based approach. Hori et al.
(2003) and Clarke et al. (2006) employed the lin-
ear model with simple combined features. They
simply regard a sentence as a word sequence and
structural information, such as full parse tree or
dependency trees, are encoded in the sequence as
features. The advantage of these methods over the
tree trimming approach is that they have the poten-
tial to drop arbitrary words from the original sen-
tence without the need to consider the boundaries
determined by the tree structures. This approach is
more suitable for Japanese compression than tree
trimming. However, they still rely on syntactic
information derived from full parsed trees or de-
pendency trees. Moreover, their use of syntactic
parsers seriously degrades the decoding speed.
</bodyText>
<sectionHeader confidence="0.998261" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.985023">
We proposed a syntax free sequence-oriented
Japanese sentence compression method with two
novel features: IPTW and PLM. Our method
needs only a POS tagger. It is significantly supe-
rior to the methods that employ syntactic parsers.
An experiment on a Japanese news corpus re-
vealed the effectiveness of the new features. Al-
though the proposed method does not employ any
explicit syntactic information, it outperformed,
with statistical significance, Hori’s method a state-
of-the-art Japanese sentence compression method
based on the sequence-oriented approach.
The contributions of this paper are as follows:
• We revealed that in compressing Japanese
sentences, humans usually ignore syntactic
structures; they drop intermediate nodes of
the dependency tree and drop words within
bunsetsu,
• As an alternative to the syntactic parser, we
proposed two novel features, Intra-sentence
positional term weighting (IPTW) and the
Patched language model (PLM), and showed
their effectiveness by conducting automatic
and human evaluations,
• We showed that our method is about 4.3 times
faster than Hori’s method which employs a
dependency parser.
</bodyText>
<sectionHeader confidence="0.998855" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9992929375">
J. Clarke and M. Lapata. 2006. Models for sentence
compression: A comparison across domains, train-
ing requirements and evaluation measures. In Proc.
of the 21st COLING and 44th ACL, pages 377–384.
C. Hori and S. Furui. 2003. A new approach to auto-
matic speech summarization. IEEE trans. on Multi-
media, 5(3):368–378.
H. Jing and K. McKeown. 1999. The Decomposition
of Human-Written Summary Sentences. In Proc. of
the 22nd SIGIR, pages 129–136.
B. H. Juang and S. Katagiri. 1992. Discriminative
Learning for Minimum Error Classification. IEEE
Trans. on Signal Processing, 40(12):3043–3053.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction. Artificial Intelligence,
139(1):91–107.
</reference>
<page confidence="0.976079">
832
</page>
<reference confidence="0.997807395348837">
T. Kudo and Y. Matsumoto. 2004. A Boosting Algo-
rithm for Classification of Semi-Structured Text. In
Proc. of the EMNLP, pages 301–308.
T. Kudo and Y. Matsumoto. 2005. Japanese De-
pendency Parsing Using Relative Preference of De-
pendency (in japanese). IPSJ Journal, 46(4):1082–
1092.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
the 18th ICML, pages 282–289.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large Margrin Training of Dependency Parser.
In Proc. of the 43rd ACL, pages 91–98.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proc. of
the 11th EACL, pages 297–304.
T. Nomoto. 2007. Discriminative sentence compres-
sion with conditional random fields. Information
Processing and Management, 43(6):1571–1587.
T. Nomoto. 2008. A generic sentence trimmer with
crfs. In Proc. of the ACL-08: HLT, pages 299–307.
R. Oguro, H. Sekiya, Y. Morooka, K. Takagi, and
K. Ozeki. 2002. Evaluation of a japanese sentence
compression method based on phrase significance
and inter-phrase dependency. In Proc. of the TSD
2002, pages 27–32.
K. Papineni, S. Roukos, T. Ward, and W-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistic (ACL),
pages 311–318.
K. Takeuchi and Y. Matsumoto. 2001. Acquisition
of sentence reduction rules for improving quality of
text summaries. In Proc. of the 6th NLPRS, pages
447–452.
J. Turner and E. Charniak. 2005. Supervised and un-
supervised learning for sentence compression. In
Proc. of the 43rd ACL, pages 290–297.
Y. Unno, T. Ninomiya, Y. Miyao, and J. Tsujii. 2006.
Trimming cfg parse trees for sentence compression
using machine learning approach. In Proc. of the
21st COLING and 44th ACL, pages 850–857.
</reference>
<page confidence="0.999169">
833
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.645852">
<title confidence="0.993564">A Syntax-Free Approach to Japanese Sentence Compression</title>
<author confidence="0.81305">Jun SUZUKI ISOZAKI HIRAO</author>
<affiliation confidence="0.938164">NTT Communication Science Laboratories, NTT Corp.</affiliation>
<address confidence="0.885681">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan</address>
<abstract confidence="0.99408852173913">Conventional sentence compression methods employ a syntactic parser to compress a sentence without changing its meaning. However, the reference compressions made by humans do not always retain the syntactic structures of the original sentences. Moreover, for the goal of ondemand sentence compression, the time spent in the parsing stage is not negligible. As an alternative to syntactic parsing, we propose a novel term weighting technique based on the positional information within the original sentence and a novel language model that combines statistics from the original sentence and a general corpus. Experiments that involve both human subjective evaluations and automatic evaluations show that our method outperforms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Models for sentence compression: A comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st COLING and 44th ACL,</booktitle>
<pages>377--384</pages>
<contexts>
<context position="2095" citStr="Clarke and Lapata, 2006" startWordPosition="306" endWordPosition="309">er to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequence of words and structural information, such as a syntactic or dependency tree, is encoded in the sequence as features. Their methods have the potential to drop arbitrary words from the original sentence without considering the boundary determined by the tree structures. However, they still rely on syntactic information derived from fully parsed syntactic or dependency trees. We found that humans usually ignored the syntactic structures when compressing sentences. For example, in many cases, they compressed the sentence by dropping inter</context>
<context position="22357" citStr="Clarke and Lapata, 2006" startWordPosition="3626" endWordPosition="3629">del (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determined by the tree structures. This approach is more suitable fo</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proc. of the 21st COLING and 44th ACL, pages 377–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hori</author>
<author>S Furui</author>
</authors>
<title>A new approach to automatic speech summarization.</title>
<date>2003</date>
<journal>IEEE trans. on Multimedia,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="2118" citStr="Hori and Furui, 2003" startWordPosition="310" endWordPosition="313">nd the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequence of words and structural information, such as a syntactic or dependency tree, is encoded in the sequence as features. Their methods have the potential to drop arbitrary words from the original sentence without considering the boundary determined by the tree structures. However, they still rely on syntactic information derived from fully parsed syntactic or dependency trees. We found that humans usually ignored the syntactic structures when compressing sentences. For example, in many cases, they compressed the sentence by dropping intermediate nodes of the sy</context>
<context position="8306" citStr="Hori and Furui, 2003" startWordPosition="1303" endWordPosition="1306">2, ... , xj, ... , xN, where xj is the j-th word in the input sentence. The system then outputs the compressed sentence y =y1, y2, ... , yi, ..., yM, where yi is the ith word in the output sentence. Here, yi ∈ {x1, ... , xN}. We assume y0=x0=&lt;s&gt; (BOS) and yM+1=xN+1=&lt;/s&gt; (EOS). We define function I(·), which maps word yi to the index of the word in the original sentence. For example, if source sentence is x = x1, x2, ... , x5 and its compressed variant is y = x1, x3, x4, I(y1) = 1, I(y2) = 3, I(y3) = 4. We define a significance score f(x, y, A) for compressed sentence y based on Hori’s method (Hori and Furui, 2003). A = {λg, λh} is a parameter vector. M+1 f(x, y; A) = {g(x, I(yi); λg) + i=1 h(x, I(yi), I(yi−1); λh)} (1) The first term of equation (1) (g(·)) is the importance of each word in the output sentence, and the second term (h(·)) is the the linguistic likelihood between adjacent words in the output sentence. The best subsequence ˆy= argmax f(x, y; A) is y identified by dynamic programming (DP) (Hori and Furui, 2003). 3.2 Features We use IPTW to define the significance score g(x, I(yi); λg). Moreover, we use PLM to define the linguistic likelihood h(x, I(yi+1), I(yi); λh). 3.2.1 Intra-sentence Po</context>
<context position="15703" citStr="Hori and Furui, 2003" startWordPosition="2561" endWordPosition="2564">training and the remainder for testing and repeated the evaluation on the test data five times changing the test block each time. We also employed human subjective evaluation, i.e., we presented the compressed sentences to six human subjects and asked them to evaluate the sentence for fluency and importance on a scale 1 (worst) to 5 (best). For each source sentence, the order in which the compressed sentences were presented was random. 4.2 Comparison of Sentence Compression Methods In order to investigate the effectiveness of the proposed features, we compared our method against Hori’s model (Hori and Furui, 2003), which is a state-of-the-art Japanese sentence compressor based on the sequence-oriented approach. Table 1 shows the feature set used in our experiment. Note that ‘Hori−’ indicates the earlier verTable 1: Configuration setup Label g() h() Proposed IPTW PLM + POS w/o PLM IPTW Bigram+POS w/o IPTW IDF PLM+POS Hori− IDF Trigram Proposed+Dep IPTW PLM + POS +Dep w/o PLM+Dep IPTW Bigram+POS+Dep w/o IPTW+Dep IDF PLM+POS+Dep Hori IDF Trigram+Dep Table 2: Results: automatic evaluation Label BLEU Proposed w/o PLM w/o IPTW Hori− Proposed+Dep w/o PLM+Dep w/o IPTW+Dep Hori sion of Hori’s method which does </context>
<context position="22331" citStr="Hori and Furui, 2003" startWordPosition="3622" endWordPosition="3625"> the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determined by the tree structures. This ap</context>
</contexts>
<marker>Hori, Furui, 2003</marker>
<rawString>C. Hori and S. Furui. 2003. A new approach to automatic speech summarization. IEEE trans. on Multimedia, 5(3):368–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>K McKeown</author>
</authors>
<title>The Decomposition of Human-Written Summary Sentences.</title>
<date>1999</date>
<booktitle>In Proc. of the 22nd SIGIR,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="11776" citStr="Jing and McKeown, 1999" startWordPosition="1890" endWordPosition="1893">e corpus consisting solely of compressed sentences. Even if we collect headlines as a kind of compressed sentence from newspaper articles, corpus size is still too small. Therefore, we propose the following novel linguistic likelihood based on statistics derived from the original sentences and a huge corpus: PLM(x, I(yj), I(yj−1)) = 1 if I(yj) = I(yj−1) + 1 APLM Bigram(x, I(yj), I(yj−1)) (5) otherwise PLM stands for Patched Language Model. Here, 0 &lt; APLM &lt; 1, Bigram(·) indicates word bigram probability. The first line of equation (5) agrees with Jing’s observation on sentence alignment tasks (Jing and McKeown, 1999); that is, most (or almost all) bigrams in a compressed sentence appear in the original sentence as they are. 3.2.3 POS bigram Since POS bigrams are useful for rejecting ungrammatical sentences, we adopt them as follows: Ppos(x,I(yi+1)|I(yi)) = P(pos(x, I(yi+1))|pos(x,I(yi))). (6) Finally, the linguistic likelihood between adjacent words within y is defined as follows: h(x,I(yi+1),I(yi);Ah) = PLM(x,I(yi+1),I(yi)) + A(pos(x,I(y,+,))|pos(x,I(y,)))Ppos(x, I(yi+1)|I(yi)) 3.3 Parameter Optimization We can regard sentence compression as a two class problem: we give a word in the original sentence cl</context>
</contexts>
<marker>Jing, McKeown, 1999</marker>
<rawString>H. Jing and K. McKeown. 1999. The Decomposition of Human-Written Summary Sentences. In Proc. of the 22nd SIGIR, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Juang</author>
<author>S Katagiri</author>
</authors>
<title>Discriminative Learning for Minimum Error Classification.</title>
<date>1992</date>
<journal>IEEE Trans. on Signal Processing,</journal>
<volume>40</volume>
<issue>12</issue>
<contexts>
<context position="3644" citStr="Juang and Katagiri, 1992" startWordPosition="541" endWordPosition="544">ree sequenceoriented sentence compression method. To maintain the subject-predicate relationship in the compressed sentence and retain fluency without using syntactic parsers, we propose two novel features: intra-sentence positional term weighting (IPTW) and the patched language model (PLM). IPTW is defined by the term’s positional information in the original sentence. PLM is a form of summarization-oriented fluency statistics derived from the original sentence and the general language model. The weight parameters for these features are optimized within the Minimum Classification Error (MCE) (Juang and Katagiri, 1992) learning framework. Experiments that utilize both human subjective and automatic evaluations show that our method is 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP Figure 1: An example of the dependency relation between an original sentence and its compressed variant. Chunk7 Compressed Sentence Source Sentence Chunk4 Chunk1 Chunk 2 Chunk 3 Chunk1 推定 し た suitei shi ta Chunk 2 Chunk 3 配点 について 福武 が haiten nitsuite fukutake ga 配点 について 福武 が haiten nitsuite fukutake ga 枝問 部分 の Compression</context>
<context position="12608" citStr="Juang and Katagiri, 1992" startWordPosition="2013" endWordPosition="2016">m as follows: Ppos(x,I(yi+1)|I(yi)) = P(pos(x, I(yi+1))|pos(x,I(yi))). (6) Finally, the linguistic likelihood between adjacent words within y is defined as follows: h(x,I(yi+1),I(yi);Ah) = PLM(x,I(yi+1),I(yi)) + A(pos(x,I(y,+,))|pos(x,I(y,)))Ppos(x, I(yi+1)|I(yi)) 3.3 Parameter Optimization We can regard sentence compression as a two class problem: we give a word in the original sentence class label +1 (the word is used in the compressed output) or −1 (the word is not used). In order to consider the interdependence of words, we employ the Minimum Classification Error (MCE) learning framework (Juang and Katagiri, 1992), which was proposed for learning the goodness of a sequence. xt denotes the t-th original sentence in the training data set T. yt denotes the reference compression that is made by humans and yt is a compressed sentence output by a system. When using the MCE framework, the misclassification measure is defined as the difference between the score of the reference sentence and that of the best non-reference output and we optimize the parameters by minimizing the measure. |T| d(y, x; A) = { L f(xt,yt;A) t=1 − max f(xt, yt; A)} (7) 147�y∗� It is impossible to minimize equation (7) because we cannot</context>
</contexts>
<marker>Juang, Katagiri, 1992</marker>
<rawString>B. H. Juang and S. Katagiri. 1992. Discriminative Learning for Minimum Error Classification. IEEE Trans. on Signal Processing, 40(12):3043–3053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Summarization beyond sentence extraction.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="1550" citStr="Knight and Marcu, 2002" startWordPosition="225" endWordPosition="228">d automatic evaluations show that our method outperforms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequ</context>
<context position="21494" citStr="Knight and Marcu, 2002" startWordPosition="3493" endWordPosition="3496">follows: Proposed: 22.14 seconds (45.2 sentences / sec), 0 N/4 N/2 3N/4 N x1s x2, ,xjl lxN &lt;/S&gt; 831 Hori: 95.34 seconds (10.5 sentences / sec). Our method was about 4.3 times faster than Hori’s method due to the latter’s use of dependency parser. This speed advantage is significant when on-demand sentence compression is needed. 6 Related work Conventional sentence compression methods employ the tree trimming approach to compress a sentence without changing its meaning. For instance, most English sentence compression methods make full parse trees and trim them by applying the generative model (Knight and Marcu, 2002; Turner and Charniak, 2005), discriminative model (Knight and Marcu, 2002; Unno et al., 2006). For Japanese sentences, instead of using full parse trees, existing sentence compression methods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentenc</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction. Artificial Intelligence, 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>A Boosting Algorithm for Classification of Semi-Structured Text.</title>
<date>2004</date>
<booktitle>In Proc. of the EMNLP,</booktitle>
<pages>301--308</pages>
<contexts>
<context position="5826" citStr="Kudo and Matsumoto, 2004" startWordPosition="894" endWordPosition="897">bunsetsu. The solid arrows indicate dependency relations between words2. We observe that the dependency relations are changed by compression; humans create compound nouns using the components derived from different portions of the original sentence without regard to syntactic constraints. ‘Chunk 7’ in the compressed sentence was constructed by dropping both content and functional words and joining other content words contained in ‘Chunk 4’ and ‘Chunk 6’ of 2Generally, a dependency relation is defined between bunsetsu. Therefore, in order to identify word dependencies, we followed Kudo’s rule (Kudo and Matsumoto, 2004) the original sentence. ‘Chunk 5’ is dropped completely. This compression cannot be achieved by tree trimming. According to an investigation in our corpus of manually compressed Japanese sentences, which we used in the experimental evaluation, 98.7% of them contain at least one segment that does not retain the original tree structure. Human usually compress sentences by dropping the intermediate nodes in the dependency tree. However, the resulting compressions retain both adequacy and fluency. This statistic supports the view that sentence compression that strongly depends on syntax is not use</context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>T. Kudo and Y. Matsumoto. 2004. A Boosting Algorithm for Classification of Semi-Structured Text. In Proc. of the EMNLP, pages 301–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese Dependency Parsing Using Relative Preference of Dependency (in japanese).</title>
<date>2005</date>
<journal>IPSJ Journal,</journal>
<volume>46</volume>
<issue>4</issue>
<pages>1092</pages>
<contexts>
<context position="6873" citStr="Kudo and Matsumoto, 2005" startWordPosition="1049" endWordPosition="1052"> tree. However, the resulting compressions retain both adequacy and fluency. This statistic supports the view that sentence compression that strongly depends on syntax is not useful in reproducing reference compressions. We need a sentence compression method that can drop intermediate nodes in the syntactic tree aggressively beyond the tree-scoped boundary. In addition, sentence compression methods that strongly depend on syntactic parsers have two problems: ‘parse error’ and ‘decoding speed.’ 44% of sentences output by a state-of-the-art Japanese dependency parser contain at least one error (Kudo and Matsumoto, 2005). Even more, it is well known that if we parse a sentence whose source is different from the training data of the parser, the performance could be much worse. This critically degrades the overall performance of sentence compression. Moreover, summarization systems often have to process megabytes of documents. Parsers are still slow and users of on827 demand summarization systems are not prepared to wait for parsing to finish. 3 A Syntax Free Sequence-oriented Sentence Compression Method As an alternative to syntactic parsing, we propose two novel features, intra-sentence positional term weight</context>
<context position="16632" citStr="Kudo and Matsumoto, 2005" startWordPosition="2706" endWordPosition="2710">+POS Hori− IDF Trigram Proposed+Dep IPTW PLM + POS +Dep w/o PLM+Dep IPTW Bigram+POS+Dep w/o IPTW+Dep IDF PLM+POS+Dep Hori IDF Trigram+Dep Table 2: Results: automatic evaluation Label BLEU Proposed w/o PLM w/o IPTW Hori− Proposed+Dep w/o PLM+Dep w/o IPTW+Dep Hori sion of Hori’s method which does not require the dependency parser. For example, label ‘w/o IPTW + Dep’ employs IDF term weighting as function g(·) and word bigram, part-of-speech bigram and dependency probability between words as function h(·) in equation (1). To obtain the word dependency probability, we use Kudo’s relative-CaboCha (Kudo and Matsumoto, 2005). We developed the n-gram language model from a 9 year set of Mainichi Newspaper articles. We optimized the parameters by using the MCE learning framework. 5 Results and Discussion 5.1 Results: automatic evaluation Table 2 shows the evaluation results yielded by BLUE at the compression rate of 0.60. Without introducing dependency probability, both IPTW and PLM worked well. Our method achieved the highest BLEU score. Compared to ‘Proposed’, ‘w/o IPTW’ offers significantly worse performance. The results support the view that our hypothesis, namely that the significance score of a word depends on</context>
</contexts>
<marker>Kudo, Matsumoto, 2005</marker>
<rawString>T. Kudo and Y. Matsumoto. 2005. Japanese Dependency Parsing Using Relative Preference of Dependency (in japanese). IPSJ Journal, 46(4):1082– 1092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proc. of the 18th ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="13881" citStr="Lafferty et al., 2001" startWordPosition="2257" endWordPosition="2260"> employ the following sigmoid function to smooth this measure. L(d(x, y; A)) = L t=1 |T| 1 + exp(−c x d(xt, yt; A)) (8) 1 Here, c is a constant parameter. To minimize equation (8), we use the following equation. VL= aL �ad � ad aA1, aA2, ... = ad 0 (9) Here, ∂L∂d is given by: 1 + exp (−c x d) (1 1 + exp (−c x d) ) (10) Finally, the parameters are optimized by using the iterative form. For example, Aw is optimized as follows: aL Aw(new) = Aw(old) − � (11) aAw(old) ⎧ ⎨ ⎩ aL ad = 829 Our parameter optimization procedure can be replaced by another one such as MIRA (McDonald et al., 2005) or CRFs (Lafferty et al., 2001). The reason why we employed MCE is that it is very easy to implement. 4 Experimental Evaluation 4.1 Corpus and Evaluation Measures We randomly selected 1,000 lead sentences (a lead sentence is the first sentence of an article excluding the headline.) whose length (number of words) was greater than 30 words from the Mainichi Newspaper from 1994 to 2002. There were five different ideal compressions (reference compressions produced by human) for each sentence; all had a 0.6 compression rate. The average length of the input sentences was about 42 words and that of the reference compressions was a</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc. of the 18th ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online Large Margrin Training of Dependency Parser.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="13849" citStr="McDonald et al., 2005" startWordPosition="2250" endWordPosition="2254">t of the function. Therefore, we employ the following sigmoid function to smooth this measure. L(d(x, y; A)) = L t=1 |T| 1 + exp(−c x d(xt, yt; A)) (8) 1 Here, c is a constant parameter. To minimize equation (8), we use the following equation. VL= aL �ad � ad aA1, aA2, ... = ad 0 (9) Here, ∂L∂d is given by: 1 + exp (−c x d) (1 1 + exp (−c x d) ) (10) Finally, the parameters are optimized by using the iterative form. For example, Aw is optimized as follows: aL Aw(new) = Aw(old) − � (11) aAw(old) ⎧ ⎨ ⎩ aL ad = 829 Our parameter optimization procedure can be replaced by another one such as MIRA (McDonald et al., 2005) or CRFs (Lafferty et al., 2001). The reason why we employed MCE is that it is very easy to implement. 4 Experimental Evaluation 4.1 Corpus and Evaluation Measures We randomly selected 1,000 lead sentences (a lead sentence is the first sentence of an article excluding the headline.) whose length (number of words) was greater than 30 words from the Mainichi Newspaper from 1994 to 2002. There were five different ideal compressions (reference compressions produced by human) for each sentence; all had a 0.6 compression rate. The average length of the input sentences was about 42 words and that of </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online Large Margrin Training of Dependency Parser. In Proc. of the 43rd ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proc. of the 11th EACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="2056" citStr="McDonald, 2006" startWordPosition="302" endWordPosition="303">sually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequence of words and structural information, such as a syntactic or dependency tree, is encoded in the sequence as features. Their methods have the potential to drop arbitrary words from the original sentence without considering the boundary determined by the tree structures. However, they still rely on syntactic information derived from fully parsed syntactic or dependency trees. We found that humans usually ignored the syntactic structures when compressing sentences. For example, in many cases, they co</context>
<context position="22295" citStr="McDonald, 2006" startWordPosition="3618" endWordPosition="3619">thods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determ</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proc. of the 11th EACL, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
</authors>
<title>Discriminative sentence compression with conditional random fields.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--6</pages>
<contexts>
<context position="2070" citStr="Nomoto, 2007" startWordPosition="304" endWordPosition="305">by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequence of words and structural information, such as a syntactic or dependency tree, is encoded in the sequence as features. Their methods have the potential to drop arbitrary words from the original sentence without considering the boundary determined by the tree structures. However, they still rely on syntactic information derived from fully parsed syntactic or dependency trees. We found that humans usually ignored the syntactic structures when compressing sentences. For example, in many cases, they compressed the s</context>
<context position="22309" citStr="Nomoto, 2007" startWordPosition="3620" endWordPosition="3621">dency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determined by the tr</context>
</contexts>
<marker>Nomoto, 2007</marker>
<rawString>T. Nomoto. 2007. Discriminative sentence compression with conditional random fields. Information Processing and Management, 43(6):1571–1587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
</authors>
<title>A generic sentence trimmer with crfs.</title>
<date>2008</date>
<booktitle>In Proc. of the ACL-08: HLT,</booktitle>
<pages>299--307</pages>
<contexts>
<context position="1734" citStr="Nomoto, 2008" startWordPosition="256" endWordPosition="257"> Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequence of words and structural information, such as a syntactic or dependency tree, is encoded in the sequence as features. Their methods have the potential to drop arbitrary words from </context>
<context position="21781" citStr="Nomoto, 2008" startWordPosition="3538" endWordPosition="3539">d sentence compression is needed. 6 Related work Conventional sentence compression methods employ the tree trimming approach to compress a sentence without changing its meaning. For instance, most English sentence compression methods make full parse trees and trim them by applying the generative model (Knight and Marcu, 2002; Turner and Charniak, 2005), discriminative model (Knight and Marcu, 2002; Unno et al., 2006). For Japanese sentences, instead of using full parse trees, existing sentence compression methods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDo</context>
</contexts>
<marker>Nomoto, 2008</marker>
<rawString>T. Nomoto. 2008. A generic sentence trimmer with crfs. In Proc. of the ACL-08: HLT, pages 299–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Oguro</author>
<author>H Sekiya</author>
<author>Y Morooka</author>
<author>K Takagi</author>
<author>K Ozeki</author>
</authors>
<title>Evaluation of a japanese sentence compression method based on phrase significance and inter-phrase dependency.</title>
<date>2002</date>
<booktitle>In Proc. of the TSD</booktitle>
<pages>27--32</pages>
<contexts>
<context position="1719" citStr="Oguro et al., 2002" startWordPosition="252" endWordPosition="255">.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequence of words and structural information, such as a syntactic or dependency tree, is encoded in the sequence as features. Their methods have the potential to drop arbitr</context>
<context position="21853" citStr="Oguro et al., 2002" startWordPosition="3549" endWordPosition="3552">ence compression methods employ the tree trimming approach to compress a sentence without changing its meaning. For instance, most English sentence compression methods make full parse trees and trim them by applying the generative model (Knight and Marcu, 2002; Turner and Charniak, 2005), discriminative model (Knight and Marcu, 2002; Unno et al., 2006). For Japanese sentences, instead of using full parse trees, existing sentence compression methods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003)</context>
</contexts>
<marker>Oguro, Sekiya, Morooka, Takagi, Ozeki, 2002</marker>
<rawString>R. Oguro, H. Sekiya, Y. Morooka, K. Takagi, and K. Ozeki. 2002. Evaluation of a japanese sentence compression method based on phrase significance and inter-phrase dependency. In Proc. of the TSD 2002, pages 27–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistic (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="14604" citStr="Papineni et al., 2002" startWordPosition="2376" endWordPosition="2380">Corpus and Evaluation Measures We randomly selected 1,000 lead sentences (a lead sentence is the first sentence of an article excluding the headline.) whose length (number of words) was greater than 30 words from the Mainichi Newspaper from 1994 to 2002. There were five different ideal compressions (reference compressions produced by human) for each sentence; all had a 0.6 compression rate. The average length of the input sentences was about 42 words and that of the reference compressions was about 24 words. For MCE learning, we selected the reference compression that maximize the BLEU score (Papineni et al., 2002) (= argmaxr∈RBLEU(r, R\r)) from the set of reference compressions and used it as correct data for training. Note that r is a reference compression and R is the set of reference compressions. We employed both automatic evaluation and human subjective evaluation. For automatic evaluation, we employed BLEU (Papineni et al., 2002) by following (Unno et al., 2006). We utilized 5- fold cross validation, i.e., we broke the whole data set into five blocks and used four of them for training and the remainder for testing and repeated the evaluation on the test data five times changing the test block eac</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W-J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistic (ACL), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Takeuchi</author>
<author>Y Matsumoto</author>
</authors>
<title>Acquisition of sentence reduction rules for improving quality of text summaries.</title>
<date>2001</date>
<booktitle>In Proc. of the 6th NLPRS,</booktitle>
<pages>447--452</pages>
<contexts>
<context position="1699" citStr="Takeuchi and Matsumoto, 2001" startWordPosition="248" endWordPosition="251">se a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequence of words and structural information, such as a syntactic or dependency tree, is encoded in the sequence as features. Their methods have the pote</context>
<context position="21766" citStr="Takeuchi and Matsumoto, 2001" startWordPosition="3534" endWordPosition="3537">e is significant when on-demand sentence compression is needed. 6 Related work Conventional sentence compression methods employ the tree trimming approach to compress a sentence without changing its meaning. For instance, most English sentence compression methods make full parse trees and trim them by applying the generative model (Knight and Marcu, 2002; Turner and Charniak, 2005), discriminative model (Knight and Marcu, 2002; Unno et al., 2006). For Japanese sentences, instead of using full parse trees, existing sentence compression methods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto </context>
</contexts>
<marker>Takeuchi, Matsumoto, 2001</marker>
<rawString>K. Takeuchi and Y. Matsumoto. 2001. Acquisition of sentence reduction rules for improving quality of text summaries. In Proc. of the 6th NLPRS, pages 447–452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turner</author>
<author>E Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd ACL,</booktitle>
<pages>290--297</pages>
<contexts>
<context position="1577" citStr="Turner and Charniak, 2005" startWordPosition="229" endWordPosition="232">show that our method outperforms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequence of words and structura</context>
<context position="21522" citStr="Turner and Charniak, 2005" startWordPosition="3497" endWordPosition="3500"> seconds (45.2 sentences / sec), 0 N/4 N/2 3N/4 N x1s x2, ,xjl lxN &lt;/S&gt; 831 Hori: 95.34 seconds (10.5 sentences / sec). Our method was about 4.3 times faster than Hori’s method due to the latter’s use of dependency parser. This speed advantage is significant when on-demand sentence compression is needed. 6 Related work Conventional sentence compression methods employ the tree trimming approach to compress a sentence without changing its meaning. For instance, most English sentence compression methods make full parse trees and trim them by applying the generative model (Knight and Marcu, 2002; Turner and Charniak, 2005), discriminative model (Knight and Marcu, 2002; Unno et al., 2006). For Japanese sentences, instead of using full parse trees, existing sentence compression methods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in man</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proc. of the 43rd ACL, pages 290–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Unno</author>
<author>T Ninomiya</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Trimming cfg parse trees for sentence compression using machine learning approach.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st COLING and 44th ACL,</booktitle>
<pages>850--857</pages>
<contexts>
<context position="1597" citStr="Unno et al., 2006" startWordPosition="233" endWordPosition="236">forms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1Hereafter, we refer these compression processes as “tree trimming.” input is grammatical, but it offers only moderate compression rates. An alternative to the tree trimming approach is the sequence-oriented approach (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2006; Hori and Furui, 2003). It treats a sentence as a sequence of words and structural information, such </context>
<context position="14965" citStr="Unno et al., 2006" startWordPosition="2437" endWordPosition="2440"> had a 0.6 compression rate. The average length of the input sentences was about 42 words and that of the reference compressions was about 24 words. For MCE learning, we selected the reference compression that maximize the BLEU score (Papineni et al., 2002) (= argmaxr∈RBLEU(r, R\r)) from the set of reference compressions and used it as correct data for training. Note that r is a reference compression and R is the set of reference compressions. We employed both automatic evaluation and human subjective evaluation. For automatic evaluation, we employed BLEU (Papineni et al., 2002) by following (Unno et al., 2006). We utilized 5- fold cross validation, i.e., we broke the whole data set into five blocks and used four of them for training and the remainder for testing and repeated the evaluation on the test data five times changing the test block each time. We also employed human subjective evaluation, i.e., we presented the compressed sentences to six human subjects and asked them to evaluate the sentence for fluency and importance on a scale 1 (worst) to 5 (best). For each source sentence, the order in which the compressed sentences were presented was random. 4.2 Comparison of Sentence Compression Meth</context>
<context position="21588" citStr="Unno et al., 2006" startWordPosition="3508" endWordPosition="3511">1 Hori: 95.34 seconds (10.5 sentences / sec). Our method was about 4.3 times faster than Hori’s method due to the latter’s use of dependency parser. This speed advantage is significant when on-demand sentence compression is needed. 6 Related work Conventional sentence compression methods employ the tree trimming approach to compress a sentence without changing its meaning. For instance, most English sentence compression methods make full parse trees and trim them by applying the generative model (Knight and Marcu, 2002; Turner and Charniak, 2005), discriminative model (Knight and Marcu, 2002; Unno et al., 2006). For Japanese sentences, instead of using full parse trees, existing sentence compression methods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alt</context>
</contexts>
<marker>Unno, Ninomiya, Miyao, Tsujii, 2006</marker>
<rawString>Y. Unno, T. Ninomiya, Y. Miyao, and J. Tsujii. 2006. Trimming cfg parse trees for sentence compression using machine learning approach. In Proc. of the 21st COLING and 44th ACL, pages 850–857.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>