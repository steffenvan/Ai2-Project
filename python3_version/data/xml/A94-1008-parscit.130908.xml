<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.973744">
Tagging accurately — Don&apos;t guess if you know
</title>
<author confidence="0.975804">
Pasi Tapanainen
</author>
<affiliation confidence="0.969779">
Rank Xerox Research Centre
Grenoble Laboratory
</affiliation>
<address confidence="0.914527">
6, chemin de Maupertuis
38240 Meylan, France
</address>
<email confidence="0.672137">
Pasi.Tapanainenaxerox.fr
</email>
<author confidence="0.865843">
Atro Voutilainen
</author>
<affiliation confidence="0.8905985">
Research Unit for Computational Linguistics
University of Helsinki
P.O. Box 4
00014 University of Helsinki, Finland
</affiliation>
<email confidence="0.514894">
Atro.VoutilainenOhelsinki.fi
</email>
<sectionHeader confidence="0.991704" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999654153846154">
We discuss combining knowledge-based (or
rule-based) and statistical part-of-speech
taggers. We use two mature taggers,
ENGCG and Xerox Tagger, to indepen-
dently tag the same text and combine the
results to produce a fully disambiguated
text. In a 27000 word test sample taken
from a previously unseen corpus we achieve
98.5 % accuracy. This paper presents the
data in detail. We describe the problems
we encountered in the course of combining
the two taggers and discuss the problem of
evaluating taggers.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999268428571428">
This paper combines knowledge-based and statisti-
cal methods for part-of-speech disambiguation, tak-
ing advantage of the best features of both ap-
proaches. The resulting output is fully and accu-
rately disambiguated.
We demonstrate a system that accurately resolves
most part-of-speech ambiguities by means of syntac-
tic rules and employs a stochastic tagger to elimi-
nate the remaining ambiguity. The overall results
are clearly superior to the reported results for state-
of-the-art stochastic systems.
The input to our part-of-speech disambiguator
consists of lexically analysed sentences. Many words
have more than one analysis. The task of the dis-
ambiguator is to select the contextually appropriate
alternative by discarding the improper ones.
Some of the inappropriate alternatives can be dis-
carded reliably by linguistic rules. For example, we
can safely exclude a finite-verb reading if the previ-
ous word is an unambiguous determiner. The appli-
cation of such rules does not always result in a fully
disambiguated output (e.g. adjective—noun ambigui-
ties may be left pending) but the amount of ambigu-
ity is reduced with next to no errors. Using a large
collection of linguistic rules, a lot of ambiguity can
be resolved, though some cases remain unresolved.
The rule system may also exploit the fact that cer-
tain linguistically possible configurations have such
a low frequency in certain types of text that they
can be ignored. A rule that assumes that a prepo-
sition is followed by a noun phrase may be a useful
heuristic rule in a practical system, considering that
dangling prepositions occur relatively infrequently.
Such heuristic rules can be applied to resolve some of
the ambiguities that survive the more reliable gram-
mar rules.
A stochastic disambiguator selects the most likely
tag for a word by consulting the neighbouring tags
or words, typically in a two or three word window.
Because of the limited size of the window, the choices
made by a stochastic disambiguator are often quite
naive from the linguistic point of view. For instance,
the correct resolution of a preposition vs. subordi-
nating conjunction ambiguity in a small window is
often impossible because both morphological cate-
gories can have identical local contexts (for instance,
both can be followed by a noun phrase). Some of the
errors made by a stochastic system can be avoided
in a knowledge-based system because the rules can
refer to words and tags in the scope of the entire
sentence.
We use both types of disambiguators. The
knowledge-based disambiguator does not resolve all
ambiguities but the choices it makes are nearly al-
ways correct. The statistical disambiguator resolves
all ambiguities but its decisions are not very reliable.
We combine these two disambiguators; here this
means that the text is analysed with both systems.
Whenever there is a conflict between the systems,
we trust the analysis proposed by the knowledge-
based system. Whenever the knowledge-based sys-
tem leaves an ambiguity unresolved, we select that
alternative which is closest to the selection made by
the statistical system.
The two systems we use are ENGCG (Karlsson
et al., 1994) and the Xerox Tagger (Cutting et al.,
1992). We discuss problems caused by the fact that
these taggers use different tag sets, and present the
results obtained by applying the combined taggers
to a previously unseen sample of text.
</bodyText>
<page confidence="0.999476">
47
</page>
<sectionHeader confidence="0.874703" genericHeader="method">
2 The taggers in outline
</sectionHeader>
<subsectionHeader confidence="0.98885">
2.1 English Constraint Grammar Parser
</subsectionHeader>
<bodyText confidence="0.999984981818182">
The English Constraint Grammar Parser, ENGCG
(Voutilainen ei al., 1992; Karlsson et al., 1994), is
based on Constraint Grammar, a parsing framework
proposed by Fred Karlsson (1990). It was developed
1989-1993 at the Research Unit for Computational
Linguistics, University of Helsinki, by Atro Voutilai-
nen, Juha Heikkila and Arto Anttila; later on, Ti-
mo Jirvinen has extended the syntactic description,
and Pasi Tapanainen has made a new fast imple-
mentation of the CG parsing program. ENGCG is
primarily designed for the analysis of standard writ-
ten English of the British and American varieties.
In the development and testing of the system, over
100 million words of running text have been used.
The ENGTWOL lexicon is based on the two-level
model (Koskenniemi, 1983). The lexicon contains
over 80,000 lexical entries, each of which represents
all inflected and central derived forms of the lexemes.
The lexicon also employs a collection of tags for part
of speech, inflection, derivation and even syntactic
category (e.g. verb classification).
Usually less than 5 % of all word-form tokens in
running text are not recognised by the morphological
analyser. Therefore the system employs a rule-based
heuristic module that provides all unknown words
with one or more readings. About 99.5 % of words
not recognised by the ENGTWOL analyser itself get
a correct analysis from the heuristic module. The
module contains a list of prefixes and suffixes, and
possible analyses for matching words. For instance,
words beginning with un... and ending in ...al are
marked as adjectives.
The grammar for morphological disambiguation
(Voutilainen, 1994) is based on 23 linguistic gen-
eralisations about the form and function of essen-
tially syntactic constructions, e.g. the form of the
noun phrase, prepositional phrase, and finite verb
chain. These generalisations are expressed as 1,100
highly reliable &apos;grammar-based&apos; and some 200 less
reliable add-on &apos;heuristic&apos; constraints, usually in a
partial and negative fashion. Using the 1,100 best
constraints results in a somewhat ambiguous out-
put. Usually there are about 1.04-1.07 morpholog-
ical analyses per word. Usually at least 997 words
out of every thousand retain the contextually appro-
priate morphological reading, i.e. the recall usually
is at least 99.7 %. If the heuristic constraints are
also used, the ambiguity rate falls to 1.02-1.04 read-
ings per word, with an overall recall of about 99.5 %.
This accuracy compares very favourably with results
reported in (de Marcken, 1990; Weischedel et al.,
1993; Kempe, 1994) — for instance, to reach the recall
of 99.3 %, the system by (Weischedel ei al., 1993)
has to leave as many as three readings per word in
its output.
</bodyText>
<subsectionHeader confidence="0.999515">
2.2 Xerox Tagger
</subsectionHeader>
<bodyText confidence="0.999799944444445">
The Xerox Tagger&apos;, XT, (Cutting et al., 1992) is
a statistical tagger made by Doug Cutting, Julian
Kupiec, Jan Pedersen and Penelope Sibun in Xerox
PARC. It was trained on the untagged Brown Cor-
pus (Francis and Kueera, 1982).
The lexicon is a word-list of 50,000 words with al-
ternative tags. Unknown words are analysed accord-
ing to their suffixes. The lexicon and suffix tables are
implemented as tries. For instance, for the word live
there are the following alternative analyses: JJ (ad-
jective) and VB (uninflected verb). Unknown words
not recognised by suffix tables get all tags from a
specific set (called open-class).
The tagger itself is based on the Hidden Markov
Model (Baum, 1972) and word equivalence classes
(Kupiec, 1989). Although the tagger is trained with
the untagged Brown corpus, there are several ways
to &apos;force&apos; it to learn.
</bodyText>
<listItem confidence="0.9952465">
• The symbol biases represent a kind of lexical
probabilities for given word equivalence classes.
• The transition biases can be used for saying that
it is likely or unlikely that a tag is followed by
some specific tag. The biases serve as default
values for the Hidden Markov Model before the
training.
• Some rare readings may be removed from the
lexicon to prevent the tagger from selecting
them.
• There are some training parameters, like the
number of iterations (how many times the same
block of text is used in training) and the size of
the block of the text used for training.
• The choice of the training corpus affects the re-
sult.
</listItem>
<bodyText confidence="0.99978825">
The tagger is reported (Cutting et al., 1992) to
have a better than 96 % accuracy in the analysis of
parts of the Brown Corpus. The accuracy is similar
to other probabilistic taggers.
</bodyText>
<sectionHeader confidence="0.6863205" genericHeader="method">
3 Grammatical representations of
the taggers
</sectionHeader>
<bodyText confidence="0.99990125">
A major difference between a knowledge-based and
a probabilistic tagger is that the knowledge-based
tagger needs as much information as possible while
the probabilistic tagger requires some compact set
of tags that does not make too many distinctions
between similar words. The difference can be seen
by comparing the Brown Corpus tag set (used by
XT) with the ENGCG tag set.
The ENGTWOL morphological analyser employs
139 tags. Each word usually receives several tags
(see Figure 1). There are also &apos;auxiliary&apos; tags for
derivational and syntactic information that do not
</bodyText>
<footnote confidence="0.655871">
1We use version 1.
</footnote>
<page confidence="0.996341">
48
</page>
<table confidence="0.999287894736842">
Brown Two most probable
tag ENGCG tags (%)
CS CS (70 %)
PREP (28 %)
DT DET DEM SG (48 %)
PRON DEM SG (27 %)
DTI DET SG/PL (68 %)
PRON SG/PL (28 %)
IN PREP (99 %)
ADV (0.5 %)
JJ A ABS (93 %)
N NOM SG (3 %)
NN N NOM SG (88 %)
N NOM SG/PL (7 %)
NP N NOM SG (80 %)
N NOM PL (7 %)
VB V INF (84 %)
V PRES -SG3 VFIN (12 %)
* NEG-PART (100 %)
</table>
<figureCaption confidence="0.9808045">
Figure 2: Some mappings from the Brown Corpus
to the ENGCG tagset.
</figureCaption>
<listItem confidence="0.8348932">
• noun—adjective homographs when the core
meanings of the adjective and noun readings are
similar,
• ambiguities due to proper nouns, common
nouns and abbreviations.
</listItem>
<table confidence="0.998999153846154">
ENGCG XT
has V PRES SG3 VFIN hvz
have V PRES -SG3 VFIN hv
V INF
V IMP VFIN
V SUBJUNCTIVE VFIN
was V PAST SG1,3 VFIN bedz
do V PRES -SG3 VFIN do
V INF
V IMP VFIN
V SUBJUNCTIVE VFIN
done PCP2 vbn
cook V PRES -SG3 VFIN vb
V INF nn
V IMP VFIN
V SUBJUNCTIVE VFIN
N NOM SG
cool V PRES -SG3 VFIN vb
V INF ii
V IMP VFIN nn
V SUBJUNCTIVE VFIN rb
A ABS
cooled PCP2 vbn
V PAST VFIN vbd
cooling PCP 1 vbg
nn
</table>
<figureCaption confidence="0.997042">
Figure 1: Some morphological ambiguities for verbs.
</figureCaption>
<bodyText confidence="0.999939666666667">
increase morphological ambiguity but serve as addi-
tional information for rules. If these auxiliary tags
are ignored, the morphological analyser produces
about 180 different tag combinations.
The XT lexicon contains 94 tags for words; 15 of
them are assigned unambiguously to only one word.
There are 32 verb tags: 8 tags for have, 13 for be, 6
for do and 5 tags for other verbs. ENGCG does not
make a distinction in the tagset between words have,
be, do and the other verbs. To see the difference with
ENGCG, see Figure 1.
The ENGCG description differs from the Brown
Corpus tag set in the following respects. ENGCG is
more distinctive in that a part of speech distinction
is spelled out (see Figure 2) in the description of
</bodyText>
<listItem confidence="0.9679851">
• determiner—pronoun homographs,
• preposition—conjunction homographs,
• determiner—adverb—pronoun homographs, and
• uninflected verb forms (see Figure 1), which are
represented as ambiguous due to the subjunc-
tive, imperative, infinitive and present tense
readings.
On the other hand, ENGCG does not spell out part-
of-speech ambiguity in the description of
• -ing and nonfinite -ed forms,
</listItem>
<sectionHeader confidence="0.952561" genericHeader="method">
4 Combining the taggers
</sectionHeader>
<bodyText confidence="0.9999272">
In our approach we apply ENGCG and XT indepen-
dently. Combining the taggers means aligning the
outputs of the taggers and transforming the result
of one tagger to that of the other.
Aligning the output is straightforward: we only
need to match the word forms in the output of the
taggers. Some minor problems occur when tokeni-
sation is done differently. For instance, XT handles
words like aren&apos;t as a single token, when ENGCG
divides it to two tokens, are and not. Also ENGCG
recognises some multiple word phrases like in spite
of as one token, while XT handles it as three tokens.
We do not need to map both Brown tags to
ENGCG and vice versa. It is enough to transform
ENGCG tags to Brown tags and select the tag that
XT has produced, or transform the tag of XT into
ENGCG tags. We do the latter because the ENGCG
tags contain more information. This is likely to be
desirable in the design of potential applications.
There are a couple of problems in mapping:
</bodyText>
<listItem confidence="0.999937333333333">
• Difference in distinctiveness. Sometimes ENG-
TWOL makes a distinction not made by the
Brown tagset; sometimes the Brown tagset
makes a distinction not made by ENGTWOL
(see Figure 2).
• Sometimes tags are used in a different way. A
</listItem>
<page confidence="0.99767">
49
</page>
<bodyText confidence="0.999958315789474">
case in point is the word as. In a sample of 76
instances of as from the tagged Brown corpus,
73 are analysed as CS; two as QL and one as
IN, while in the ENGCG description the same
instances of as were analysed 15 times as CS,
four times as ADV, and 57 times as PREP.
In ENGCG, the tag CS represents subordinat-
ing conjunctions. In the following sentences
the correct analysis for word as in ENGCG is
PREP, not CS, which the Brown corpus sug-
gests.
The city purchasing department, the
jury said, is lacking in experienced
clerical personnel as(CS) a result of
city personnel policies. — The pe-
tition listed the mayor&apos;s occupation
as(CS) attorney and his age as(CS) 71.
It listed his wife&apos;s age as(CS) 74 and
place of birth as(CS) Opelika, Ala.
The sentences are the three first sentences
where word as appears in Brown corpus. In the
Brown Corpus as appears over 7000 times and it
is the fourteenth most common word. Because
XT is trained according to the Brown Corpus,
this is likely to cause problems.
XT is applied independently to the text, and the
tagger&apos;s prediction is consulted in the analysis of
those words where ENGCG is unable to make a
unique prediction. The system selects the ENGCG
morphological reading that most closely corresponds
to the tag proposed by XT.
The mapping scheme is the following. For each
Brown Corpus tag, there is a decision list for possible
ENGCG tags, the most probable one first. We have
computed the decision list from the part of Brown
Corpus that is also manually tagged according to the
ENGCG grammatical representation. The mapping
can be used in two different ways.
</bodyText>
<listItem confidence="0.994301857142857">
• Careful mode: An ambiguous reading in the
output of ENGCG may be removed only when
it is not in the decision list. In practise this
leaves quite much ambiguity.
• Unambiguous mode: Select the reading in the
output of ENGCG that comes first in the deci-
sion list&apos;.
</listItem>
<sectionHeader confidence="0.946017" genericHeader="evaluation">
5 Performance test
</sectionHeader>
<subsectionHeader confidence="0.997676">
5.1 Test data
</subsectionHeader>
<bodyText confidence="0.982046777777778">
The system was tested against 26,711 words of
newspaper text from The Wall Street Journal, The
Economist and Today, all taken from the 200-million
word Bank of English corpus by the COBUILD team
at the University of Birmingham, England (see also
(Jarvinen, 1994)). None of these texts have been
&apos;In some cases a word may still remain ambiguous.
used in the development of the system or the de-
scription, i.e. no training effects are to be expected.
</bodyText>
<subsectionHeader confidence="0.997058">
5.2 Creation of benchmark corpus
</subsectionHeader>
<bodyText confidence="0.999982272727273">
Before the test, a benchmark version of the test
corpus was created. The texts were first analysed
using the preprocessor, the morphological analyser,
and the module for morphological heuristics. This
ambiguous data was then manually disambiguated
by judges, each having a thorough understanding of
the ENGCG grammatical representation. The cor-
pus was independently disambiguated by two judges.
In the instructions to the experts, special empha-
sis was given to the quality of the work (there was
no time pressure). The two disambiguated versions
of the corpus were compared using the Unix sdiff
program. At this stage, slightly above 99 % of all
analyses agreed. The differences were jointly exam-
ined by the judges to see whether they were caused
by inattention or by a genuine difference of opinion
that could not be resolved by consulting the docu-
mentation that outlines the principles adopted for
this grammatical representation (for the most part
documented in (Karlsson ei al., 1994)). It turned
out that almost all of these differences were due to
inattention. Only in the analysis of a few words it
was agreed that a multiple choice was appropriate
because of different meaning-level interpretations of
the utterance (these were actually headings where
some of the grammatical information was omitted).
Overall, these results agree with our previous expe-
riences (Karlsson et al., 1994): if the analysis is done
by experts in the adopted grammatical representa-
tion, with emphasis on the quality of the work, a
consensus of virtually 100 % is possible, at least at
the level of morphological analysis (for a less opti-
mistic view, see (Church, 1992)).
</bodyText>
<subsectionHeader confidence="0.999874">
5.3 Morphological analysis
</subsectionHeader>
<bodyText confidence="0.999847333333333">
The preprocessed text was submitted to the ENG-
TWOL morphological analyser, which assigns to
25,831 words of the total 26,711 (96.7 %) at least
one morphological analysis. The remaining 880
word-form tokens were analysed with the rule-based
heuristic module. After the combined effect of these
modules, there were 47,269 morphological analyses,
i.e. 1.77 morphological analyses for each word on
an average. At this stage, 23 words missed a con-
textually appropriate analysis, i.e. the error rate of
the system after morphological analysis was about
0.1 %.
</bodyText>
<subsectionHeader confidence="0.959644">
5.4 Morphological disambiguation
</subsectionHeader>
<bodyText confidence="0.9997568">
The morphologically analysed text was submitted
to five disambiguators (see Figure 3). The first one,
D1, is the grammar-based ENGCG disambiguator.
In the next step (D2) we have used also heuristic
ENGCG constraints. The probabilistic information
</bodyText>
<page confidence="0.99046">
50
</page>
<bodyText confidence="0.986308936170213">
is used in D3, where the ambiguities of D2 are re-
solved by XT. We also tested the usefulness of the
heuristic component of ENGCG by omitting it in
D4. The last test, D5, is XT alone, i.e. only proba-
bilistic techniques are used here for resolving ENG-
TWOL ambiguities.
The ENGCG disambiguator performed somewhat
less well than usually. With heuristic constraints,
the error rate was as high as 0.63 %, with 1.04 mor-
phological readings per word on an average. How-
ever, most (57 %) of the total errors were made af-
ter ENGCG analysis (i.e. in the analysis of no more
than 3.6 % of all words). In a way, this is not very
surprising because ENGCG is supposed to tackle all
the &apos;easy&apos; cases and leave the structurally hardest
cases pending. But it is quite revealing that as much
as three fourths of the probabilistic tagger&apos;s errors
occur in the analysis of the structurally &apos;easy&apos; cases;
obviously, many of the probabilistic system&apos;s deci-
sions are structurally somewhat naive. Overall, the
hybrid (D3,8) reached an accuracy of about 98.5 % —
significantly better than the 95-97 % accuracy which
state-of-the-art probabilistic taggers reach alone.
The hybrid D30 is like hybrid D3,8, but we have
used careful mapping. There some problematic
ambiguity (see Figure 2) is left pending. For in-
stance, ambiguities between preposition and infini-
tive marker (word to), or between subordinator and
preposition (word as), are resolved as far as ENGCG
disambiguates them, the prediction of XT is not con-
sulted. Also, when XT proposes tags like JJ (adjec-
tive), AP (post-determiner) or VB (verb base-form)
very little further disambiguation is done. This hy-
brid does not contain any mapping errors, and on
the other hand, not all the XT errors either.
The test without the heuristic component of
ENGCG (D4) suggests that ambiguity should be re-
solved as far as possible with rules. An open ques-
tion is, how far we can go using only linguistic infor-
mation (e.g. by writing more heuristic constraints to
be applied after the more reliable ones, in this way
avoiding many linguistically naive errors).
The last test gives further evidence for the use-
fulness of a carefully designed linguistic rule compo-
nent. Without such a rule component, the decrease
in accuracy is quite dramatic although a part of the
errors come from the mapping between tag sets3.
</bodyText>
<sectionHeader confidence="0.996438" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.940384666666667">
In this paper we have demonstrated how knowledge-
based and statistical techniques can be combined to
improve the accuracy of a part of speech tagger. Our
system reaches a better than 98 % accuracy using a
relatively fine-grained grammatical representation.
Some concluding remarks are in order.
</bodyText>
<footnote confidence="0.738646666666667">
3Even without the mapping errors, the reported 4 %
error rate of XT is considerably higher than that of our
hybrid.
</footnote>
<listItem confidence="0.95305425">
• Using linguistic information before a statistical
module provides a better result than using a
statistical module alone.
• ENGCG leaves some &apos;hard&apos; ambiguities unre-
</listItem>
<bodyText confidence="0.6208358">
solved (about 3-7 % of all words). This amount
is characteristic of the ENGCG rule-formalism,
tagset and disambiguation grammar. It does
not necessarily hold for other knowledge-based
systems.
• Only about 20-25 % of errors made by the sta-
tistical component occur in the analysis of these
&apos;hard&apos; ambiguities. That means, 75-80 % of the
errors made by the statistical tagger were re-
solved correctly using linguistic rules.
</bodyText>
<listItem confidence="0.5573868">
• Certain kinds of ambiguity left pending by
ENGCG, e.g. CS vs. PREP, are resolved rather
unreliably by XT.
• The overall result is better than other state-of-
the-art part-of-speech disambiguators. In our
</listItem>
<bodyText confidence="0.815997461538461">
27000 word test sample from previously unseen
corpus, 98.5 % of words received a correct anal-
ysis. In other words, the error rate is reduced
at least by half.
Although the result is better than provided by
any other tagger that produces fully disambiguated
output, we believe that the result could still be im-
proved. Some possibilities:
• We could use partly disambiguated text
(e.g. the output of parsers D1, D2 or D3a)
and disambiguate the result using a knowledge-
based syntactic parser (see experiments in (Vou-
tilainen and Tapanainen, 1993)).
</bodyText>
<listItem confidence="0.904159615384615">
• We could leave the text partly disambiguated,
and use a syntactic parser that uses both lin-
guistic knowledge and corpus-based heuristics
(see (Tapanainen and Jarvinen, 1994)).
• Some ambiguities are very difficult to resolve in
a small window that statistical taggers currently
use (e.g. CS vs. PREP ambiguity when a noun
phrase follows). A better way to resolve them
would probably be to write (heuristic) rules.
• We could train the statistical tagger on the out-
put of a knowledge-based tagger. That is prob-
lematic because generally statistical methods
seem to require some compact set of tags, while
</listItem>
<bodyText confidence="0.886338833333333">
a knowledge-based system needs more informa-
tive tags. The tag set of a knowledge-based sys-
tem should be reduced down to some subset.
That might prevent some mapping errors but
there is no guarantee that the statistical tagger
would work any better.
</bodyText>
<listItem confidence="0.5939688">
• We could try the components in a different or-
der: using statistics before heuristical knowl-
edge etc. However, currently the heuristic com-
ponent makes less errors than the statistical
tagger.
</listItem>
<page confidence="0.992716">
51
</page>
<table confidence="0.9956415">
Amb words Readings Readings / word Errors Error rate (%)
DO (Morphological analysis) 37.6 % 47269 1.77 23 0.09 %
D1 (DO + ENGCG) 6.4 % 28815 1.08 94 0.35 %
D2 (D1 + ENGCG heuristics) 3.6 % 27681 1.04 169 0.63 %
D3a (D2 + XT + C-mapping) 2.2 % 27358 1.02 220 0.82 %
D30 (D2 + XT + mapping) 0.0 % 26744 1.00 391 1.46 %
D4 (D1 + XT + mapping) 0.0 % 26794 1.00 597 2.24 %
D5 (DO + XT + mapping) 0.7 % 26977 1.01 1703 6.38 %
</table>
<figureCaption confidence="0.997151">
Figure 3: Performance of the taggers on a 26,711-word corpus.
</figureCaption>
<bodyText confidence="0.992810117647059">
• We could use a better statistical tagger. But
the accuracy of XT is almost the same as the
accuracy of any other statistical tagger. What
is more, the accuracy of the purely statistical
taggers has not been greatly increased since the
first of its kind, CLAWS1, (Marshall, 1983) was
published over ten years ago.
We believe that the best way to boost the accu-
racy of a tagger is to employ even more linguistic
knowledge. The knowledge should, in addition, con-
tain more syntactic information so that we could re-
fer to real (syntactic) objects of the language, not
just a sequence of words or parts of speech. Statisti-
cal information should be used only when one does
not know how to resolve the remaing ambiguity, and
there is a definite need to get fully unambiguous out-
put.
</bodyText>
<sectionHeader confidence="0.999066" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999894333333333">
We would like to thank Timo Jarvinen, Lauri Kart-
tunen, Jussi Piitulainen and anonymous referees for
useful comments on earlier versions of this paper.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.978950523809524">
L. E. Baum. 1972. An inequality and associated
maximization technique in statistical estimation
for probabilistic functions of a Markov process.
Inequaties, 3:1-8, 1972.
Kenneth W. Church. 1992. Current Practice in Part
of Speech Tagging and Suggestions for the Fu-
ture. In Simmons (ed.), Sbornik praci: In Honor
of Henry Kueera. Michigan Slavic Studies.
Doug Cutting, Julian Kupiec, Jan Pedersen and
Penelope Sibun. 1992. A Practical Part-of-Speech
Tagger. In Proceedings of ANLP-92.
W. N. Francis and F. Kueera. 1982. Frequency Anal-
ysis of English Usage. Houghton Mifflin.
Timo Jarvinen. 1994. Annotating 200 million
words: the Bank of English project. In proceed-
ings of COLING-94, Vol. 1, 565-568. Kyoto.
Fred Karlsson. 1990. Constraint Grammar as a
Framework for Parsing Running Text. In Proceed-
ings of COLING-90. Helsinki. Vol. 3, 168-173.
Fred Karlsson, Atro Voutilainen, Juha Heikkila and
Arto Anttila (eds.). 1994. Constraint Grammar:
a Language-Independent System for Parsing Un-
restricted Text. Berlin: Mouton de Gruyter.
Andre Kempe. 1994. A Probabilistic Tagger and
an Analysis of Tagging Errors. Research Report,
Institut fiir Maschinelle Sprachverarbeitung, Uni-
versitat Stuttgart.
Kimmo Koskenniemi. 1983. Two-level Morphology.
A General Computational Model for Word-form
Production and Generation. Publication No. 11,
Department of General Linguistics, University of
Helsinki.
Julian M. Kupiec. 1989. Probabilistic models of
short and long distance word dependencies in run-
ning text. In Proceedings of the 1989 DARPA
Speech and Natural Language Workshop pp. 290-
295. Philadelphia. Morgan Kaufman.
Carl de Marcken. 1990. Parsing the LOB Corpus.
In Proceedings of the 28th Annual Meeting of the
ACL. 243-251.
Ian Marshall. 1983. Choice of grammatical word-
class without global syntactic analysis: tagging
words in the LOB Corpus. Computers in the Hu-
manities 17. 139-150.
Pasi Tapanainen and Timo Jarvinen. 1994. Syn-
tactic analysis of natural language using linguistic
rules and corpus-based patterns. In proceedings of
COLING-94, Vol. 1, 629-634. Kyoto.
Atro Voutilainen. 1994. Morphological disambigua-
tion. In Karlsson et al..
Atro Voutilainen, Juha Heikkila and Arto Anttila.
1992. Constraint Grammar of English. A Per-
formance-Oriented Introduction. Publication No.
21, Department of General Linguistics, University
of Helsinki.
Atro Voutilainen and Pasi Tapanainen. 1993. Am-
biguity resolution in a reductionistic parser. Pro-
ceedings of EACL&apos;93. Utrecht, Holland. 394-403.
Ralph Weischedel, Marie Meteer, Richard Schwartz,
Lance Ramshaw and Jeff Palmuzzi. 1993. Cop-
ing with ambiguity and unknown words through
probabilistic models. Computational Linguistics,
Vol. 19, Number 2.
</reference>
<page confidence="0.998851">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.383260">
<title confidence="0.998994">Tagging accurately — Don&apos;t guess if you know</title>
<author confidence="0.976351">Pasi Tapanainen</author>
<affiliation confidence="0.9324585">Rank Xerox Research Centre Grenoble Laboratory</affiliation>
<address confidence="0.8810225">6, chemin de Maupertuis 38240 Meylan, France</address>
<email confidence="0.735396">Pasi.Tapanainenaxerox.fr</email>
<author confidence="0.990677">Atro Voutilainen</author>
<affiliation confidence="0.992912">Research Unit for Computational Linguistics University of Helsinki</affiliation>
<address confidence="0.992488">P.O. Box 4 00014 University of Helsinki, Finland</address>
<email confidence="0.827291">Atro.VoutilainenOhelsinki.fi</email>
<abstract confidence="0.999522714285714">We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers. We use two mature taggers, ENGCG and Xerox Tagger, to independently tag the same text and combine the results to produce a fully disambiguated text. In a 27000 word test sample taken from a previously unseen corpus we achieve 98.5 % accuracy. This paper presents the data in detail. We describe the problems we encountered in the course of combining the two taggers and discuss the problem of evaluating taggers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process.</title>
<date>1972</date>
<journal>Inequaties,</journal>
<pages>3--1</pages>
<contexts>
<context position="7736" citStr="Baum, 1972" startWordPosition="1228" endWordPosition="1229">oug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. It was trained on the untagged Brown Corpus (Francis and Kueera, 1982). The lexicon is a word-list of 50,000 words with alternative tags. Unknown words are analysed according to their suffixes. The lexicon and suffix tables are implemented as tries. For instance, for the word live there are the following alternative analyses: JJ (adjective) and VB (uninflected verb). Unknown words not recognised by suffix tables get all tags from a specific set (called open-class). The tagger itself is based on the Hidden Markov Model (Baum, 1972) and word equivalence classes (Kupiec, 1989). Although the tagger is trained with the untagged Brown corpus, there are several ways to &apos;force&apos; it to learn. • The symbol biases represent a kind of lexical probabilities for given word equivalence classes. • The transition biases can be used for saying that it is likely or unlikely that a tag is followed by some specific tag. The biases serve as default values for the Hidden Markov Model before the training. • Some rare readings may be removed from the lexicon to prevent the tagger from selecting them. • There are some training parameters, like t</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>L. E. Baum. 1972. An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process. Inequaties, 3:1-8, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Current Practice in Part of Speech Tagging and Suggestions for the Future.</title>
<date>1992</date>
<booktitle>Sbornik praci: In Honor of Henry Kueera. Michigan Slavic Studies.</booktitle>
<editor>In Simmons (ed.),</editor>
<contexts>
<context position="16748" citStr="Church, 1992" startWordPosition="2820" endWordPosition="2821">s were due to inattention. Only in the analysis of a few words it was agreed that a multiple choice was appropriate because of different meaning-level interpretations of the utterance (these were actually headings where some of the grammatical information was omitted). Overall, these results agree with our previous experiences (Karlsson et al., 1994): if the analysis is done by experts in the adopted grammatical representation, with emphasis on the quality of the work, a consensus of virtually 100 % is possible, at least at the level of morphological analysis (for a less optimistic view, see (Church, 1992)). 5.3 Morphological analysis The preprocessed text was submitted to the ENGTWOL morphological analyser, which assigns to 25,831 words of the total 26,711 (96.7 %) at least one morphological analysis. The remaining 880 word-form tokens were analysed with the rule-based heuristic module. After the combined effect of these modules, there were 47,269 morphological analyses, i.e. 1.77 morphological analyses for each word on an average. At this stage, 23 words missed a contextually appropriate analysis, i.e. the error rate of the system after morphological analysis was about 0.1 %. 5.4 Morphologica</context>
</contexts>
<marker>Church, 1992</marker>
<rawString>Kenneth W. Church. 1992. Current Practice in Part of Speech Tagging and Suggestions for the Future. In Simmons (ed.), Sbornik praci: In Honor of Henry Kueera. Michigan Slavic Studies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A Practical Part-of-Speech Tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of ANLP-92.</booktitle>
<contexts>
<context position="4061" citStr="Cutting et al., 1992" startWordPosition="635" endWordPosition="638">es but the choices it makes are nearly always correct. The statistical disambiguator resolves all ambiguities but its decisions are not very reliable. We combine these two disambiguators; here this means that the text is analysed with both systems. Whenever there is a conflict between the systems, we trust the analysis proposed by the knowledgebased system. Whenever the knowledge-based system leaves an ambiguity unresolved, we select that alternative which is closest to the selection made by the statistical system. The two systems we use are ENGCG (Karlsson et al., 1994) and the Xerox Tagger (Cutting et al., 1992). We discuss problems caused by the fact that these taggers use different tag sets, and present the results obtained by applying the combined taggers to a previously unseen sample of text. 47 2 The taggers in outline 2.1 English Constraint Grammar Parser The English Constraint Grammar Parser, ENGCG (Voutilainen ei al., 1992; Karlsson et al., 1994), is based on Constraint Grammar, a parsing framework proposed by Fred Karlsson (1990). It was developed 1989-1993 at the Research Unit for Computational Linguistics, University of Helsinki, by Atro Voutilainen, Juha Heikkila and Arto Anttila; later o</context>
<context position="7091" citStr="Cutting et al., 1992" startWordPosition="1118" endWordPosition="1121">ly at least 997 words out of every thousand retain the contextually appropriate morphological reading, i.e. the recall usually is at least 99.7 %. If the heuristic constraints are also used, the ambiguity rate falls to 1.02-1.04 readings per word, with an overall recall of about 99.5 %. This accuracy compares very favourably with results reported in (de Marcken, 1990; Weischedel et al., 1993; Kempe, 1994) — for instance, to reach the recall of 99.3 %, the system by (Weischedel ei al., 1993) has to leave as many as three readings per word in its output. 2.2 Xerox Tagger The Xerox Tagger&apos;, XT, (Cutting et al., 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. It was trained on the untagged Brown Corpus (Francis and Kueera, 1982). The lexicon is a word-list of 50,000 words with alternative tags. Unknown words are analysed according to their suffixes. The lexicon and suffix tables are implemented as tries. For instance, for the word live there are the following alternative analyses: JJ (adjective) and VB (uninflected verb). Unknown words not recognised by suffix tables get all tags from a specific set (called open-class). The tagger itself is </context>
<context position="8578" citStr="Cutting et al., 1992" startWordPosition="1375" endWordPosition="1378"> given word equivalence classes. • The transition biases can be used for saying that it is likely or unlikely that a tag is followed by some specific tag. The biases serve as default values for the Hidden Markov Model before the training. • Some rare readings may be removed from the lexicon to prevent the tagger from selecting them. • There are some training parameters, like the number of iterations (how many times the same block of text is used in training) and the size of the block of the text used for training. • The choice of the training corpus affects the result. The tagger is reported (Cutting et al., 1992) to have a better than 96 % accuracy in the analysis of parts of the Brown Corpus. The accuracy is similar to other probabilistic taggers. 3 Grammatical representations of the taggers A major difference between a knowledge-based and a probabilistic tagger is that the knowledge-based tagger needs as much information as possible while the probabilistic tagger requires some compact set of tags that does not make too many distinctions between similar words. The difference can be seen by comparing the Brown Corpus tag set (used by XT) with the ENGCG tag set. The ENGTWOL morphological analyser emplo</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun. 1992. A Practical Part-of-Speech Tagger. In Proceedings of ANLP-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>F Kueera</author>
</authors>
<title>Frequency Analysis of English Usage.</title>
<date>1982</date>
<publisher>Houghton Mifflin.</publisher>
<contexts>
<context position="7270" citStr="Francis and Kueera, 1982" startWordPosition="1149" endWordPosition="1152"> are also used, the ambiguity rate falls to 1.02-1.04 readings per word, with an overall recall of about 99.5 %. This accuracy compares very favourably with results reported in (de Marcken, 1990; Weischedel et al., 1993; Kempe, 1994) — for instance, to reach the recall of 99.3 %, the system by (Weischedel ei al., 1993) has to leave as many as three readings per word in its output. 2.2 Xerox Tagger The Xerox Tagger&apos;, XT, (Cutting et al., 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. It was trained on the untagged Brown Corpus (Francis and Kueera, 1982). The lexicon is a word-list of 50,000 words with alternative tags. Unknown words are analysed according to their suffixes. The lexicon and suffix tables are implemented as tries. For instance, for the word live there are the following alternative analyses: JJ (adjective) and VB (uninflected verb). Unknown words not recognised by suffix tables get all tags from a specific set (called open-class). The tagger itself is based on the Hidden Markov Model (Baum, 1972) and word equivalence classes (Kupiec, 1989). Although the tagger is trained with the untagged Brown corpus, there are several ways to</context>
</contexts>
<marker>Francis, Kueera, 1982</marker>
<rawString>W. N. Francis and F. Kueera. 1982. Frequency Analysis of English Usage. Houghton Mifflin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Jarvinen</author>
</authors>
<title>Annotating 200 million words: the Bank of English project.</title>
<date>1994</date>
<booktitle>In proceedings of COLING-94,</booktitle>
<volume>1</volume>
<pages>565--568</pages>
<publisher>Kyoto.</publisher>
<contexts>
<context position="14876" citStr="Jarvinen, 1994" startWordPosition="2515" endWordPosition="2516">tation. The mapping can be used in two different ways. • Careful mode: An ambiguous reading in the output of ENGCG may be removed only when it is not in the decision list. In practise this leaves quite much ambiguity. • Unambiguous mode: Select the reading in the output of ENGCG that comes first in the decision list&apos;. 5 Performance test 5.1 Test data The system was tested against 26,711 words of newspaper text from The Wall Street Journal, The Economist and Today, all taken from the 200-million word Bank of English corpus by the COBUILD team at the University of Birmingham, England (see also (Jarvinen, 1994)). None of these texts have been &apos;In some cases a word may still remain ambiguous. used in the development of the system or the description, i.e. no training effects are to be expected. 5.2 Creation of benchmark corpus Before the test, a benchmark version of the test corpus was created. The texts were first analysed using the preprocessor, the morphological analyser, and the module for morphological heuristics. This ambiguous data was then manually disambiguated by judges, each having a thorough understanding of the ENGCG grammatical representation. The corpus was independently disambiguated b</context>
<context position="21882" citStr="Jarvinen, 1994" startWordPosition="3659" endWordPosition="3660">r words, the error rate is reduced at least by half. Although the result is better than provided by any other tagger that produces fully disambiguated output, we believe that the result could still be improved. Some possibilities: • We could use partly disambiguated text (e.g. the output of parsers D1, D2 or D3a) and disambiguate the result using a knowledgebased syntactic parser (see experiments in (Voutilainen and Tapanainen, 1993)). • We could leave the text partly disambiguated, and use a syntactic parser that uses both linguistic knowledge and corpus-based heuristics (see (Tapanainen and Jarvinen, 1994)). • Some ambiguities are very difficult to resolve in a small window that statistical taggers currently use (e.g. CS vs. PREP ambiguity when a noun phrase follows). A better way to resolve them would probably be to write (heuristic) rules. • We could train the statistical tagger on the output of a knowledge-based tagger. That is problematic because generally statistical methods seem to require some compact set of tags, while a knowledge-based system needs more informative tags. The tag set of a knowledge-based system should be reduced down to some subset. That might prevent some mapping error</context>
</contexts>
<marker>Jarvinen, 1994</marker>
<rawString>Timo Jarvinen. 1994. Annotating 200 million words: the Bank of English project. In proceedings of COLING-94, Vol. 1, 565-568. Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Constraint Grammar as a Framework for Parsing Running Text.</title>
<date>1990</date>
<booktitle>In Proceedings of COLING-90. Helsinki.</booktitle>
<volume>3</volume>
<pages>168--173</pages>
<contexts>
<context position="4496" citStr="Karlsson (1990)" startWordPosition="706" endWordPosition="707">t alternative which is closest to the selection made by the statistical system. The two systems we use are ENGCG (Karlsson et al., 1994) and the Xerox Tagger (Cutting et al., 1992). We discuss problems caused by the fact that these taggers use different tag sets, and present the results obtained by applying the combined taggers to a previously unseen sample of text. 47 2 The taggers in outline 2.1 English Constraint Grammar Parser The English Constraint Grammar Parser, ENGCG (Voutilainen ei al., 1992; Karlsson et al., 1994), is based on Constraint Grammar, a parsing framework proposed by Fred Karlsson (1990). It was developed 1989-1993 at the Research Unit for Computational Linguistics, University of Helsinki, by Atro Voutilainen, Juha Heikkila and Arto Anttila; later on, Timo Jirvinen has extended the syntactic description, and Pasi Tapanainen has made a new fast implementation of the CG parsing program. ENGCG is primarily designed for the analysis of standard written English of the British and American varieties. In the development and testing of the system, over 100 million words of running text have been used. The ENGTWOL lexicon is based on the two-level model (Koskenniemi, 1983). The lexico</context>
</contexts>
<marker>Karlsson, 1990</marker>
<rawString>Fred Karlsson. 1990. Constraint Grammar as a Framework for Parsing Running Text. In Proceedings of COLING-90. Helsinki. Vol. 3, 168-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Atro Voutilainen, Juha Heikkila and Arto Anttila (eds.).</title>
<date>1994</date>
<location>Berlin: Mouton</location>
<note>de Gruyter.</note>
<marker>Karlsson, 1994</marker>
<rawString>Fred Karlsson, Atro Voutilainen, Juha Heikkila and Arto Anttila (eds.). 1994. Constraint Grammar: a Language-Independent System for Parsing Unrestricted Text. Berlin: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Kempe</author>
</authors>
<title>A Probabilistic Tagger and an Analysis of Tagging Errors. Research Report, Institut fiir Maschinelle Sprachverarbeitung,</title>
<date>1994</date>
<location>Universitat Stuttgart.</location>
<contexts>
<context position="6878" citStr="Kempe, 1994" startWordPosition="1079" endWordPosition="1080">c&apos; constraints, usually in a partial and negative fashion. Using the 1,100 best constraints results in a somewhat ambiguous output. Usually there are about 1.04-1.07 morphological analyses per word. Usually at least 997 words out of every thousand retain the contextually appropriate morphological reading, i.e. the recall usually is at least 99.7 %. If the heuristic constraints are also used, the ambiguity rate falls to 1.02-1.04 readings per word, with an overall recall of about 99.5 %. This accuracy compares very favourably with results reported in (de Marcken, 1990; Weischedel et al., 1993; Kempe, 1994) — for instance, to reach the recall of 99.3 %, the system by (Weischedel ei al., 1993) has to leave as many as three readings per word in its output. 2.2 Xerox Tagger The Xerox Tagger&apos;, XT, (Cutting et al., 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. It was trained on the untagged Brown Corpus (Francis and Kueera, 1982). The lexicon is a word-list of 50,000 words with alternative tags. Unknown words are analysed according to their suffixes. The lexicon and suffix tables are implemented as tries. For instance, for the word l</context>
</contexts>
<marker>Kempe, 1994</marker>
<rawString>Andre Kempe. 1994. A Probabilistic Tagger and an Analysis of Tagging Errors. Research Report, Institut fiir Maschinelle Sprachverarbeitung, Universitat Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level Morphology. A General Computational Model for Word-form Production and Generation.</title>
<date>1983</date>
<tech>Publication No. 11,</tech>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<contexts>
<context position="5084" citStr="Koskenniemi, 1983" startWordPosition="801" endWordPosition="802">oposed by Fred Karlsson (1990). It was developed 1989-1993 at the Research Unit for Computational Linguistics, University of Helsinki, by Atro Voutilainen, Juha Heikkila and Arto Anttila; later on, Timo Jirvinen has extended the syntactic description, and Pasi Tapanainen has made a new fast implementation of the CG parsing program. ENGCG is primarily designed for the analysis of standard written English of the British and American varieties. In the development and testing of the system, over 100 million words of running text have been used. The ENGTWOL lexicon is based on the two-level model (Koskenniemi, 1983). The lexicon contains over 80,000 lexical entries, each of which represents all inflected and central derived forms of the lexemes. The lexicon also employs a collection of tags for part of speech, inflection, derivation and even syntactic category (e.g. verb classification). Usually less than 5 % of all word-form tokens in running text are not recognised by the morphological analyser. Therefore the system employs a rule-based heuristic module that provides all unknown words with one or more readings. About 99.5 % of words not recognised by the ENGTWOL analyser itself get a correct analysis f</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-level Morphology. A General Computational Model for Word-form Production and Generation. Publication No. 11, Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian M Kupiec</author>
</authors>
<title>Probabilistic models of short and long distance word dependencies in running text.</title>
<date>1989</date>
<booktitle>In Proceedings of the 1989 DARPA Speech and Natural Language Workshop</booktitle>
<pages>290--295</pages>
<publisher>Morgan Kaufman.</publisher>
<location>Philadelphia.</location>
<contexts>
<context position="7780" citStr="Kupiec, 1989" startWordPosition="1234" endWordPosition="1235">nd Penelope Sibun in Xerox PARC. It was trained on the untagged Brown Corpus (Francis and Kueera, 1982). The lexicon is a word-list of 50,000 words with alternative tags. Unknown words are analysed according to their suffixes. The lexicon and suffix tables are implemented as tries. For instance, for the word live there are the following alternative analyses: JJ (adjective) and VB (uninflected verb). Unknown words not recognised by suffix tables get all tags from a specific set (called open-class). The tagger itself is based on the Hidden Markov Model (Baum, 1972) and word equivalence classes (Kupiec, 1989). Although the tagger is trained with the untagged Brown corpus, there are several ways to &apos;force&apos; it to learn. • The symbol biases represent a kind of lexical probabilities for given word equivalence classes. • The transition biases can be used for saying that it is likely or unlikely that a tag is followed by some specific tag. The biases serve as default values for the Hidden Markov Model before the training. • Some rare readings may be removed from the lexicon to prevent the tagger from selecting them. • There are some training parameters, like the number of iterations (how many times the </context>
</contexts>
<marker>Kupiec, 1989</marker>
<rawString>Julian M. Kupiec. 1989. Probabilistic models of short and long distance word dependencies in running text. In Proceedings of the 1989 DARPA Speech and Natural Language Workshop pp. 290-295. Philadelphia. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>Parsing the LOB Corpus.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the ACL.</booktitle>
<pages>243--251</pages>
<marker>de Marcken, 1990</marker>
<rawString>Carl de Marcken. 1990. Parsing the LOB Corpus. In Proceedings of the 28th Annual Meeting of the ACL. 243-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Marshall</author>
</authors>
<title>Choice of grammatical wordclass without global syntactic analysis: tagging words in</title>
<date>1983</date>
<journal>the LOB Corpus. Computers in the Humanities</journal>
<volume>17</volume>
<pages>139--150</pages>
<contexts>
<context position="23519" citStr="Marshall, 1983" startWordPosition="3959" endWordPosition="3960">4 0.35 % D2 (D1 + ENGCG heuristics) 3.6 % 27681 1.04 169 0.63 % D3a (D2 + XT + C-mapping) 2.2 % 27358 1.02 220 0.82 % D30 (D2 + XT + mapping) 0.0 % 26744 1.00 391 1.46 % D4 (D1 + XT + mapping) 0.0 % 26794 1.00 597 2.24 % D5 (DO + XT + mapping) 0.7 % 26977 1.01 1703 6.38 % Figure 3: Performance of the taggers on a 26,711-word corpus. • We could use a better statistical tagger. But the accuracy of XT is almost the same as the accuracy of any other statistical tagger. What is more, the accuracy of the purely statistical taggers has not been greatly increased since the first of its kind, CLAWS1, (Marshall, 1983) was published over ten years ago. We believe that the best way to boost the accuracy of a tagger is to employ even more linguistic knowledge. The knowledge should, in addition, contain more syntactic information so that we could refer to real (syntactic) objects of the language, not just a sequence of words or parts of speech. Statistical information should be used only when one does not know how to resolve the remaing ambiguity, and there is a definite need to get fully unambiguous output. 7 Acknowledgements We would like to thank Timo Jarvinen, Lauri Karttunen, Jussi Piitulainen and anonymo</context>
</contexts>
<marker>Marshall, 1983</marker>
<rawString>Ian Marshall. 1983. Choice of grammatical wordclass without global syntactic analysis: tagging words in the LOB Corpus. Computers in the Humanities 17. 139-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo Jarvinen</author>
</authors>
<title>Syntactic analysis of natural language using linguistic rules and corpus-based patterns.</title>
<date>1994</date>
<booktitle>In proceedings of COLING-94,</booktitle>
<volume>1</volume>
<pages>629--634</pages>
<publisher>Kyoto.</publisher>
<contexts>
<context position="21882" citStr="Tapanainen and Jarvinen, 1994" startWordPosition="3657" endWordPosition="3660">alysis. In other words, the error rate is reduced at least by half. Although the result is better than provided by any other tagger that produces fully disambiguated output, we believe that the result could still be improved. Some possibilities: • We could use partly disambiguated text (e.g. the output of parsers D1, D2 or D3a) and disambiguate the result using a knowledgebased syntactic parser (see experiments in (Voutilainen and Tapanainen, 1993)). • We could leave the text partly disambiguated, and use a syntactic parser that uses both linguistic knowledge and corpus-based heuristics (see (Tapanainen and Jarvinen, 1994)). • Some ambiguities are very difficult to resolve in a small window that statistical taggers currently use (e.g. CS vs. PREP ambiguity when a noun phrase follows). A better way to resolve them would probably be to write (heuristic) rules. • We could train the statistical tagger on the output of a knowledge-based tagger. That is problematic because generally statistical methods seem to require some compact set of tags, while a knowledge-based system needs more informative tags. The tag set of a knowledge-based system should be reduced down to some subset. That might prevent some mapping error</context>
</contexts>
<marker>Tapanainen, Jarvinen, 1994</marker>
<rawString>Pasi Tapanainen and Timo Jarvinen. 1994. Syntactic analysis of natural language using linguistic rules and corpus-based patterns. In proceedings of COLING-94, Vol. 1, 629-634. Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>Morphological disambiguation.</title>
<date>1994</date>
<note>In Karlsson et al..</note>
<contexts>
<context position="5956" citStr="Voutilainen, 1994" startWordPosition="935" endWordPosition="936">. verb classification). Usually less than 5 % of all word-form tokens in running text are not recognised by the morphological analyser. Therefore the system employs a rule-based heuristic module that provides all unknown words with one or more readings. About 99.5 % of words not recognised by the ENGTWOL analyser itself get a correct analysis from the heuristic module. The module contains a list of prefixes and suffixes, and possible analyses for matching words. For instance, words beginning with un... and ending in ...al are marked as adjectives. The grammar for morphological disambiguation (Voutilainen, 1994) is based on 23 linguistic generalisations about the form and function of essentially syntactic constructions, e.g. the form of the noun phrase, prepositional phrase, and finite verb chain. These generalisations are expressed as 1,100 highly reliable &apos;grammar-based&apos; and some 200 less reliable add-on &apos;heuristic&apos; constraints, usually in a partial and negative fashion. Using the 1,100 best constraints results in a somewhat ambiguous output. Usually there are about 1.04-1.07 morphological analyses per word. Usually at least 997 words out of every thousand retain the contextually appropriate morpho</context>
</contexts>
<marker>Voutilainen, 1994</marker>
<rawString>Atro Voutilainen. 1994. Morphological disambiguation. In Karlsson et al..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
<author>Juha Heikkila</author>
<author>Arto Anttila</author>
</authors>
<title>Constraint Grammar of English. A Performance-Oriented Introduction.</title>
<date>1992</date>
<journal>Publication</journal>
<volume>21</volume>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<marker>Voutilainen, Heikkila, Anttila, 1992</marker>
<rawString>Atro Voutilainen, Juha Heikkila and Arto Anttila. 1992. Constraint Grammar of English. A Performance-Oriented Introduction. Publication No. 21, Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
<author>Pasi Tapanainen</author>
</authors>
<title>Ambiguity resolution in a reductionistic parser.</title>
<date>1993</date>
<booktitle>Proceedings of EACL&apos;93.</booktitle>
<pages>394--403</pages>
<location>Utrecht, Holland.</location>
<contexts>
<context position="21704" citStr="Voutilainen and Tapanainen, 1993" startWordPosition="3629" endWordPosition="3633">overall result is better than other state-ofthe-art part-of-speech disambiguators. In our 27000 word test sample from previously unseen corpus, 98.5 % of words received a correct analysis. In other words, the error rate is reduced at least by half. Although the result is better than provided by any other tagger that produces fully disambiguated output, we believe that the result could still be improved. Some possibilities: • We could use partly disambiguated text (e.g. the output of parsers D1, D2 or D3a) and disambiguate the result using a knowledgebased syntactic parser (see experiments in (Voutilainen and Tapanainen, 1993)). • We could leave the text partly disambiguated, and use a syntactic parser that uses both linguistic knowledge and corpus-based heuristics (see (Tapanainen and Jarvinen, 1994)). • Some ambiguities are very difficult to resolve in a small window that statistical taggers currently use (e.g. CS vs. PREP ambiguity when a noun phrase follows). A better way to resolve them would probably be to write (heuristic) rules. • We could train the statistical tagger on the output of a knowledge-based tagger. That is problematic because generally statistical methods seem to require some compact set of tags</context>
</contexts>
<marker>Voutilainen, Tapanainen, 1993</marker>
<rawString>Atro Voutilainen and Pasi Tapanainen. 1993. Ambiguity resolution in a reductionistic parser. Proceedings of EACL&apos;93. Utrecht, Holland. 394-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Marie Meteer</author>
<author>Richard Schwartz</author>
<author>Lance Ramshaw</author>
<author>Jeff Palmuzzi</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="6864" citStr="Weischedel et al., 1993" startWordPosition="1075" endWordPosition="1078">reliable add-on &apos;heuristic&apos; constraints, usually in a partial and negative fashion. Using the 1,100 best constraints results in a somewhat ambiguous output. Usually there are about 1.04-1.07 morphological analyses per word. Usually at least 997 words out of every thousand retain the contextually appropriate morphological reading, i.e. the recall usually is at least 99.7 %. If the heuristic constraints are also used, the ambiguity rate falls to 1.02-1.04 readings per word, with an overall recall of about 99.5 %. This accuracy compares very favourably with results reported in (de Marcken, 1990; Weischedel et al., 1993; Kempe, 1994) — for instance, to reach the recall of 99.3 %, the system by (Weischedel ei al., 1993) has to leave as many as three readings per word in its output. 2.2 Xerox Tagger The Xerox Tagger&apos;, XT, (Cutting et al., 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. It was trained on the untagged Brown Corpus (Francis and Kueera, 1982). The lexicon is a word-list of 50,000 words with alternative tags. Unknown words are analysed according to their suffixes. The lexicon and suffix tables are implemented as tries. For instance, </context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmuzzi, 1993</marker>
<rawString>Ralph Weischedel, Marie Meteer, Richard Schwartz, Lance Ramshaw and Jeff Palmuzzi. 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, Vol. 19, Number 2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>