<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.997799">
A Systematic Bayesian Treatment of the IBM Alignment Models
</title>
<author confidence="0.997233">
Yarin Gal
</author>
<affiliation confidence="0.9945055">
Department of Engineering
University of Cambridge
</affiliation>
<address confidence="0.981346">
Cambridge, CB2 1PZ, United Kingdom
</address>
<email confidence="0.99785">
yg279@cam.ac.uk
</email>
<author confidence="0.991883">
Phil Blunsom
</author>
<affiliation confidence="0.997392">
Department of Computer Science
University of Oxford
</affiliation>
<address confidence="0.985462">
Oxford, OX1 3QD, United Kingdom
</address>
<email confidence="0.998426">
Phil.Blunsom@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.995513" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99767216">
The dominant yet ageing IBM and HMM
word alignment models underpin most
popular Statistical Machine Translation
implementations in use today. Though
beset by the limitations of implausible
independence assumptions, intractable
optimisation problems, and an excess of
tunable parameters, these models provide
a scalable and reliable starting point for
inducing translation systems. In this paper we
build upon this venerable base by recasting
these models in the non-parametric Bayesian
framework. By replacing the categorical
distributions at their core with hierarchical
Pitman-Yor processes, and through the use
of collapsed Gibbs sampling, we provide a
more flexible formulation and sidestep the
original heuristic optimisation techniques.
The resulting models are highly extendible,
naturally permitting the introduction of
phrasal dependencies. We present extensive
experimental results showing improvements
in both AER and BLEU when benchmarked
against Giza++, including significant
improvements over IBM model 4.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959243902439">
The IBM and HMM word alignment models (Brown
et al., 1993; Vogel et al., 1996) have underpinned the
majority of statistical machine translation systems
for almost twenty years. The key attraction of these
models is their principled probabilistic formulation,
and the existence of (mostly) tractable algorithms
for their training.
The dominant Giza++ implementation of the
IBM models (Och and Ney, 2003) employs a
variety of exact and approximate EM algorithms
to optimise categorical alignment distributions.
While effective, this parametric approach results in
a significant number of parameters to be tuned and
intractable summations over the space of alignments
for models 3 and 4. Giza++ hides the hyper-
parameters with defaults and approximates the
intractable expectations using restricted alignment
neighbourhoods. However this approach was shown
to often return alignments with probabilities well
below the true maxima (Ravi and Knight, 2010).
To overcome perceived limitations with the word
based and non-syntactic nature of the IBM models
many alternative approaches to word alignment have
been proposed (e.g. (DeNero et al., 2008; Cohn and
Blunsom, 2009; Levenberg et al., 2012)). While
interesting results have been reported, these alterna-
tives have failed to dislodge the IBM approach.
In this paper we proposed to retain the original
generative stories of the IBM models, while
replacing the inflexible categorical distributions
with hierarchical Pitman-Yor (PY) processes – a
mathematical tool which has been successfully
applied to a range of language tasks (Teh, 2006b;
Goldwater et al., 2006; Blunsom and Cohn,
2011). In the context of language modelling, the
hierarchical PY process was shown to roughly
correspond to interpolated Kneser-Ney (Kneser and
Ney, 1995; Teh, 2006a). The key contribution of
the hierarchical PY formulation is that it provides
a principle probabilistic framework that easily
extends to latent variable models, such as those used
</bodyText>
<page confidence="0.97885">
969
</page>
<note confidence="0.4712725">
Proceedings of NAACL-HLT 2013, pages 969–977,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.993987367816092">
for alignment, for which a Kneser-Ney formulation
is unclear. While Bayesian priors have previously
been applied to IBM model 1 (Riley and Gildea,
2012), in this work we go considerably further by
implementing non-parametric priors for the full
Giza++ training pipeline.
Inference for the proposed models and their
hyper-parameters is done with Gibbs sampling.
This eliminates the intractable summations
over alignments and the need for tuning hyper-
parameters. Further, we exploit the highly
extendible nature of the hierarchical PY process to
implement improvements to the original models
such as the introduction of phrasal dependencies.
We present extensive experimental results
showing improvements in both BLEU scores and
AER when compared to Giza++. The demonstrated
improvements over IBM model 4 suggest that the
heuristics used in the implementation of the EM
algorithm for this model were suboptimal.
We begin with a formal presentation of the hier-
archical PY process used in our Bayesian approach
to replace the original categorical distributions. Sec-
tion 3 introduces our Bayesian formulation of the
word alignment models, while its inference scheme
is presented in the following section. Finally, the
experimental results evaluating our models against
the originals are given in section 5, demonstrating
the superiority of the non-parametric approach.
2 The hierarchical PY process
Before giving the formal definition for the hierar-
chical Pitman-Yor (PY) process, we first give some
intuition into how this distribution works and why
it is commonly used to model problems in natural
language.
The hierarchical PY process is an atomic distri-
bution that can share its atoms between different
levels in a hierarchy. When used for language mod-
elling it captures the probability of observing a word
after any given sequence of n words. It does so by
interpolating the observed frequency of the whole
sequence followed by the word of interest, with the
observed frequency of a shorter sequence followed
by the word of interest. This interpolation is done in
such a way that tokens in a more specific distribution
are interpolated with types in a less specific one.
If there is sufficient evidence for the whole word
sequence, i.e. it is not sparse in the corpus, higher
weight will be given to the frequency of the word
of interest after the more specific sequence than
the shorter one. If the sequence was not observed
frequently and there is not enough information to
model whether the word of interest follows after it
frequently or not, the process will back-off to the
shorter sequence and assign higher weight to its fre-
quency instead. This is done in a recursive fashion,
decreasing the sequence length by one every time
until the probability is interpolated with the uniform
distribution, much like interpolated Kneser-Ney, the
state of the art for language modelling.
Unlike Kneser-Ney, the hierarchical PY approach
naturally extends to model complicated conditional
distributions involving latent variables. Moreover,
almost all instances of priors with categorical distri-
butions can be replaced by the PY process, where in
its most basic representation (with no conditional) it
provides a flexible model of power law frequencies.
The PY process generalises a number of simpler
distributions. The Dirichlet distribution is a distri-
bution over discrete probability mass functions of a
certain given length which is often used to model
prior beliefs on parameter sparsity in machine learn-
ing problems. The Dirichlet process generalises the
Dirichlet distribution to a distribution over infinite
sequences of non-negative reals that sum to one and
is often used for nonparametric Bayesian inference.
The PY process is used in the context of natural lan-
guage processing as it further generalises the Dirich-
let process by adding an additional degree of free-
dom that enables it to produce power-law discrete
probability mass functions that resemble those seen
experimentally in corpora (Goldwater et al., 2006).
Formally, draws from the PY process
G1 .. PY (d, 0, G0) with a discount parameter
0 &lt; d &lt; 1, a strength parameter 0 &gt; —d, and a base
distribution G0, are constructed using a Chinese
restaurant process analogy as follows:
</bodyText>
<equation confidence="0.9966885">
mk — d 0 + dK G0
0 + n sy&apos;` + 0 + n
</equation>
<bodyText confidence="0.997913">
Where mk denotes the number of Xis (customers)
assigned to yk (a table) and K is the total number of
yks drawn from G0.
</bodyText>
<equation confidence="0.996139666666667">
K
Xn+1JX1, ..., Xn ..
k=1
</equation>
<page confidence="0.951781">
970
</page>
<bodyText confidence="0.999271444444445">
Hierarchical PY processes (Teh, 2006b), PY
processes where the base distribution is itself a
PY process, were developed as an extension which
is often used in the context of natural language
processing due to their relationship to back-off
smoothing. Denoting a context of atoms u as
(wi−l, ..., wi−1), the hierarchical PY process is
defined using the above definition of the PY process
by:
</bodyText>
<equation confidence="0.9749916">
wi —Gu
Gu —PY (d|u|, θ|u|, Gπ(u))
...
G(wi_1) —PY (d1, θ1, G∅)
G∅ —PY (d0, θ0, G0)
</equation>
<bodyText confidence="0.999829333333333">
where π(u) = (wi−l+1, ..., wi−1) is the suffix of u,
|u |denotes the length of context u, and G0 is a base
distribution.
</bodyText>
<sectionHeader confidence="0.966262" genericHeader="method">
3 A Bayesian approach to word alignment
</sectionHeader>
<bodyText confidence="0.999971928571429">
In this work we replace the categorical distributions
at the heart of statistical alignment models with PY
processes. We start by describing the revised models
for IBM model 1 and the HMM alignment model,
before continuing to the more advanced IBM mod-
els 3 and 4. Throughout this section, we assume
that the base distributions in our models (denoted
G0, H0, etc.) are uniform over all atoms, and omit
the strength and concentration parameters of the PY
process for clarity. We use subscripts to denote
the hierarchy, and lower-case superscripts to denote
a fixed condition (for example, Gm0 is the (uni-
form) base distribution that is determined uniquely
for each possible foreign sentence length m).
</bodyText>
<subsectionHeader confidence="0.999922">
3.1 Model 1 and the HMM alignment model
</subsectionHeader>
<bodyText confidence="0.999980846153846">
The most basic word alignment model, IBM model
1, can be described using the following generative
process (Brown et al., 1993): Given an English sen-
tence E = e1,..., el, first choose a length m for
the foreign sentence F. Next, choose a vector of
random word positions from the English sentence
A = a1,..., am to be the alignment, and then for
each foreign word fi choose a translation from the
English word eai aligned to it by A. The existence
of a NULL word at the beginning of the English sen-
tence is assumed, a word to which spurious words in
the foreign sentence can align. From this generative
process the following probability model is derived:
</bodyText>
<equation confidence="0.856438">
P(F, A|E) = p(m|l) x �m p(ai)p(fi|eai)
i=1
Where p(ai) = 1
</equation>
<bodyText confidence="0.984132166666667">
l+1 is uniform over all alignments
and p(fi|eai) — Categorical.
In our approach we model these distributions
using hierarchical PY processes instead of the
categorical distributions. Thus we place the
following assumptions on IBM model 1:
</bodyText>
<equation confidence="0.999826">
ai|m — Gm0
fi|eai — Heai
Heai — PY (H∅)
H∅ — PY (H0)
</equation>
<bodyText confidence="0.999977380952381">
In this probability modelling we assume that the
alignment positions are determined using the uni-
form distribution, and that word translations are gen-
erated depending on the source word – the probabil-
ity of translating to a specific foreign word depends
on the observed frequency of pairs of the foreign
word and the given source word. We back-off to
the frequencies of the foreign words when the source
word wasn’t observed frequently.
The HMM alignment model uses the Hidden
Markov Model to find word alignments. It treats the
translations of the words of the English sentence as
observables and the alignment positions as the latent
variables to be discovered. Its generative process
can be described in an abstract way as follows: we
determine the length of the foreign sentence and
then iterate over the words of the source sentence
emitting translations for each word to fill-in the
words in the foreign sentence from left to right.
The following probability model is derived for the
HMM alignment model (Vogel et al., 1996):
</bodyText>
<equation confidence="0.999125">
P(F,A|E) =
p(m|l) x �m p(ai|ai−1, m) x p(fi|eai)
i=1
</equation>
<bodyText confidence="0.981489333333333">
For the HMM alignment model we replace
the categorical translation distribution p(fi|eai)
with the same one we used for model 1, and
</bodyText>
<page confidence="0.991673">
971
</page>
<bodyText confidence="0.996186">
replace the categorical distribution for the transition
p(ai|ai−1, m) with a hierarchical PY process with
a longer sequence of alignment positions in the
conditional:
</bodyText>
<equation confidence="0.999152">
ai|ai−1, m ∼ Gmai−1
Gmai−1 ∼ PY (Gm0 )
Gm0 ∼ PY (Gm 0 )
</equation>
<bodyText confidence="0.99995875">
We use a unique distribution for each foreign sen-
tence length, and condition the position on the pre-
vious alignment position, backing-off to the HMM’s
stationary distribution over alignment positions.
</bodyText>
<subsectionHeader confidence="0.999988">
3.2 Models 3 and 4
</subsectionHeader>
<bodyText confidence="0.999974264705883">
IBM models 3 and 4 introduce the concept of a
word’s fertility, the number of foreign words that are
generated from a specific English word. These mod-
els can be described using the following generative
process. Given an English sentence E, first deter-
mine the length of the foreign sentence: for each
word in the English sentence ei choose a fertility,
denoted φi. Every time a word is generated, an addi-
tional spurious word from the NULL word in the
English sentence can be generated with a fixed prob-
ability. After the foreign sentence length is deter-
mined translate each English word into its foreign
equivalent (including the NULL word) in the same
way as for model 1. Finally, non-spurious words
are rearranged into the final word positions and the
spurious words inserted into the empty positions. In
model 3 this is done with a zero order HMM, and in
model 4 with two first order HMMs. One controls
the distortion of the head of each English word (the
first foreign word generated from it) relative to the
centre (denoted here (D) of the set of foreign words
generated from the previous English word, and the
other controls the distortion within the set itself by
conditioning the word position on the previous word
position.
For the probability model, we follow the notation
of Och and Ney (2003) and define the alignment as
a function from the source sentence positions i to
Bi ⊂ {1, ..., m} where the Bi’s form a partition of
the set {1, ..., m}. The fertility of the English word
i is φi = |Bi|, and we use Bi,k to refer to the kth
element of Bi in ascending order.
Using the above notation, the following probabil-
ity model is derived (Och and Ney, 2003):
</bodyText>
<equation confidence="0.957410666666667">
P(F, A|E) =p(B0|B1, ..., Bl) × ril p(Bi|Bi−1, ei)
i=1
l
× ri ri p(fj|ei)
i=0 jEBi
For model 3 the dependence on previous
alignment sets is ignored and the probability
p(Bi|Bi−1, ei) is modelled as
p(Bi|Bi−1, ei) = p(φi|ei)φi! ri p(j|i, m),
jEBi
whereas in model 4 it is modelled using two HMMs:
p(Bi|Bi−1, ei) =p(φi|ei) × p=1(Bi,1 − (D(Bi−1)|·)
φi
× ri p&gt;1(Bi,k − Bi,k−1|·)
k=2
</equation>
<bodyText confidence="0.9697205">
For both these models the spurious word genera-
tion is controlled by a binomial distribution:
</bodyText>
<equation confidence="0.988814444444445">
p(B0|B1, ..., Bl) = Cm φ0φ0l (1 − p0)m−2φ0p101
φ0!
p0
p1.
p(φi|ei)
p(fj|ei)
(f1, ..., fφi)|ei ∼ Hei
∼
(HF T
</equation>
<bodyText confidence="0.833772444444444">
ei )
for some parameters
and
Replacing the categorical priors with hierarchical
PY process ones, we set the translation and fertility
probabilities
HjEBi
using a com-
mon prior that generates translation sequences:
</bodyText>
<equation confidence="0.987190333333333">
Hei
PY
H ei T((f1,..., f φ i)) = H Fei ( φi)ri HT (fj−1,ei)(fj)
j
HT (fj−1,ei) ∼ P Y (HT ei)
HF0 ∼ PY (HF0 )
</equation>
<bodyText confidence="0.982019125">
)
Here we used superscripts for the indexing of words
which do not have to occur sequentially in the sen-
tence. We generate sequences instead of individ-
ual words and fertilities, and fall-back onto these
only in sparse cases. For example, when aligning
HT0∼PY(HT0
sh sentence “I don’t speak French” to its
</bodyText>
<figure confidence="0.717243">
∼ PY (HT0 )
the Engli
∼ PY (HF0 )
F
Hei
T
Hei
</figure>
<page confidence="0.987111">
972
</page>
<bodyText confidence="0.999636285714286">
French translation “Je ne parle pas franc¸ais”, the
word “not” will generate the phrase (“ne”, “pas”),
which will later on be distorted into its place around
the verb.
The distortion probability for model 3, p(j|i, m),
is modelled simply as depending on the position of
the source word i and its class:
</bodyText>
<equation confidence="0.99978725">
j|(C(ei), i), m — Gm(C(ei),i)
Gm(C(ei),i) —PY (Gmi )
Gmi — PY (Gm∅ )
Gm∅ — PY (Gm0 )
</equation>
<bodyText confidence="0.999977833333333">
where we back-off to the source word position and
then to the frequencies of the alignment positions.
As opposed to this simple mechanism, in the dis-
tortion probability for IBM model 4 there exist two
distinct probability distributions. The first probabil-
ity distribution p=1 controls the head distortion:
</bodyText>
<equation confidence="0.9846428">
Bi,1 — 0(Bi−1)  |(C(ei), C(fBi,1)), m
— Gm(C(ei),C(fBi,1))
Gm(C(ei),C(fBi,1)) — PY (GmC(fBi,1))
GmC(fBi,1) — PY (Gm∅ )
Gm ∅ — PY (Gm0 )
</equation>
<bodyText confidence="0.999980285714286">
In this probability modelling we model the jump
size itself, as depending on the word class for the
source word and the word class for the proposed
foreign word, backing-off to the proposed foreign
word class and then to the relative jump frequencies.
The second probability distribution p&gt;1 controls
the distortion within the set of words:
</bodyText>
<equation confidence="0.98410875">
m
Bi,j — Bi,j−1|C(fBi j), m — HC(fBi,j )
HmC(fBi,j) — PY (Hm∅ )
Hm∅ — PY (Hm0 )
</equation>
<bodyText confidence="0.999800833333333">
Here we again model the jump size as depending
on the word class for the proposed foreign word,
backing-off to the relative jump frequencies.
Lastly, we add to this probability model a treat-
ment for fertility and translation of NULL words.
The fertility generation follows the idea of the orig-
inal model, where the number of spurious words is
determined by a binomial distribution created from
a set of Bernoulli experiments, each one performed
after the translation of a non-spurious word. We use
an indicator function I to signal whether a spuri-
ous word was generated after a non-spurious word
</bodyText>
<equation confidence="0.919388">
(I = 1) or not (I = 0).
I = 0,1|l — HNF
l
HNF
l — PY (HNF
∅ )
HNF
∅ — PY (HNF
0 )
Then, the translation of spurious words is done in a
straightforward manner:
HNT
fi ∅
HNT
∅ — PY (HNT
0 )
</equation>
<sectionHeader confidence="0.999369" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.999959833333333">
The Gibbs sampling inference scheme together with
the Chinese Restaurant Franchise process (Teh and
Jordan, 2009) are used to induce alignments for a
parallel corpus. A set of restaurants S is constructed
and initialised either randomly or through a pipeline
of alignment results from simpler models, and then
at each iteration each alignment position is removed
from the restaurants and re-sampled, conditioned on
the rest of the alignment positions.
Denoting e, f, a the sets of all source sen-
tences, their translations, and their corresponding
alignments in our corpus, and denoting E, F, A a
specific source sentence, its translation, and their
corresponding alignment, where ei is the i’th word
of the source sentence and fj, aj are the j’th word
in the foreign sentence and its alignment into the
source sentence, we sample a new value for aj using
the univariate conditional distribution:
</bodyText>
<equation confidence="0.8817475">
P(aj = i|E, F, A−j, e−E, f−F, a−A, S−aj)
a P(F, (A−j, aj = i)|E, e−E, f−F, a−A, S−aj)
</equation>
<bodyText confidence="0.99994925">
Where a minus sign in the subscript denotes the
structure without the mentioned element, and S−aj
denotes the configuration of the restaurants after
removing the alignment aj.
This univariate conditional distribution is propor-
tional to the probability assigned by the different
models to an alignment sequence, where the restau-
rants replace the counts of the alignment positions
</bodyText>
<page confidence="0.995878">
973
</page>
<figure confidence="0.997398394736842">
29.5
29.0
28.5
BLEU
28.0
27.5
27.0
26.5
26.0
14.75
14.50
BLEU
14.25
14.00
13.75
13.50
40
39
38
37
AER
36
35
34
33
32
English -&gt; Chinese Pipeline
1 1&gt;H 1&gt;H&gt;3 1&gt;H&gt;3&gt;4
AER Pipeline
Giza++
PY-IBM
1 1&gt;H 1&gt;H&gt;3 1&gt;H&gt;3&gt;4
Chinese -&gt; English Pipeline
PY-IBM
Giza++
1 1&gt;H 1&gt;H&gt;3 1&gt;H&gt;3&gt;4
PY-IBM
Giza++
</figure>
<figureCaption confidence="0.997054333333333">
Figure 1: BLEU scores of pipelined
Giza++ and pipelined PY-IBM trans-
lating from Chinese into English
Figure 2: BLEU scores of pipelined
Giza++ and pipelined PY-IBM trans-
lating from English into Chinese
Figure 3: AER of pipelined Giza++
and pipelined PY-IBM aligning
Chinese and English
</figureCaption>
<bodyText confidence="0.999931090909091">
in the prior. Maximum marginal decoding (Johnson
and Goldwater, 2009) can then be used to get the
MAP estimate of the probability distributions over
the alignment positions for each sentence from the
samples extracted from the Gibbs sampler.
In addition to sampling the alignments, we
also place a uniform Beta prior on the discount
parameters and a vague Gamma prior on the
strength parameters, and sample them using slice
sampling (Neal, 2003). The end result is that the
alignment models have no free parameters to tune.
</bodyText>
<sectionHeader confidence="0.99682" genericHeader="evaluation">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.999903555555556">
In order to assess our PY process alignment
models (referred to as PY-IBM henceforth) several
experiments were carried out to benchmark them
against the original models (as implemented in
Giza++). We evaluated the BLEU scores (Papineni
et al., 2002) of translations from Chinese into
English and from English into Chinese, as well
as the alignment error rates (AER) of the induced
symmetrised alignments compared to a human
aligned dataset. Moses (Koehn et al., 2007) was
used for the training of the SMT system and
the symmetrisation (using the grow-diag-final
procedure), with MERT (Och, 2003) used for tuning
of the weights, and SRILM (Stolcke, 2002) to build
the language model (5-grams based). The corpus
used for training and evaluation was the Chinese
FBIS corpus. MT02 was used for tuning, and MT03
was used for evaluation. In each case we used
one reference sentence in Chinese and 4 reference
sentences in English.
Most translation systems rely on the Giza++ pack-
age in which the implementation of the original
models is done by combining them in a pipeline.
Model 1 and the HMM alignment model are run
sequentially each for 5 iterations; then models 3 and
4 are run sequentially for 3 iterations each. This
follows the observation of Och and Ney (2003) that
bootstrapping from previous results assists the fer-
tility algorithms find the best alignment neighbour-
hood in order to estimate the expectations.
We assessed the proposed models against the
original models in a pipeline experiment where
both systems were trained on a corpus starting
at model 1, and used the results of the previous
run to initialise the next one – noting the BLEU
scores and AER at each step. The Gibbs samplers
for the pipelined PY-IBM models were run for 50
iterations for each model and started accumulating
samples after a burn-in period of 10 iterations,
each experiment was repeated three times and
the results averaged. As can be seen in figures
1 to 3, the pipelined PY-IBM models achieved
higher BLEU scores across all steps, with the
highest improvement of 1.6 percentage points in the
pipelined HMM alignment models when translating
</bodyText>
<page confidence="0.993414">
974
</page>
<figure confidence="0.99747252631579">
14.0
13.5
13.0
BLEU
12.5
12.0
11.5
11.0
10.5
65
60
55
AER
50
45
40
35
English -&gt; Chinese
HMM Model Model 4
</figure>
<figureCaption confidence="0.976593">
Figure 5: BLEU scores of Giza++’s
and PY-IBM’s HMM model and
model 4 translating from English into
</figureCaption>
<figure confidence="0.947085166666667">
Chinese
AER
HMM Model Model 4
Giza++
Giza++ 10 iter.
PY-IBM
</figure>
<figureCaption confidence="0.977035333333333">
Figure 6: AER of Giza++’s and PY-
IBM’s HMM model and model 4
aligning Chinese and English
</figureCaption>
<figure confidence="0.996921466666667">
PY-IBM
Giza++ 10 iter.
Giza++
BLEU
28
26
24
20
27
25
23
22
21
HMM Model Model 4
Chinese -&gt; English
</figure>
<figureCaption confidence="0.980108333333333">
Figure 4: BLEU scores of Giza++’s
and PY-IBM’s HMM model and
model 4 translating from Chinese into
</figureCaption>
<figure confidence="0.91524925">
English
PY-IBM
Giza++ 10 iter.
Giza++
</figure>
<figureCaption confidence="0.9888155">
Figure 7: Alignment disagreement of the Chinese to
English pipelined PY-IBM models for the 3 repetitions
</figureCaption>
<bodyText confidence="0.999916777777778">
from Chinese into English, and an improvement
of 1.2 percentage points in the overall results after
finishing the pipeline.
We also saw an improvement in AER for all
models, where the pipelined PY-IBM model 4
achieved an error rate of 32.9, as opposed to the
result obtained by the Giza++ pipelined model 4
of 34.4. We note an interesting observation that
both Giza++ and PY-IBM model 3 underperformed
compared to the previously run HMM alignment
model, as seen in the English to Chinese pipeline
results and the AER pipeline results.
The alignment disagreement (the number of
changed alignment positions between subsequent
iterations) of the Chinese to English pipelined
PY-IBM models (1 to 4) can be seen in fig. 7. This
graph shows that each model in the pipeline reaches
an alignment disagreement equilibrium after about
20 iterations, and that earlier models have greater
initial deviation from their equilibrium than later
models – which have an overall lower disagreement.
In order to assess the dependence of the fertil-
ity based models on the initialisation step another
set of experiments was carried out. The models
were trained with a randomly initialised set of align-
ments and assessed after a set number of iterations
for the Giza++ models (5 and 10 for the Giza++
HMM alignment model, and 3 and 10 for the Giza++
IBM model 4), or after 100 iterations with a burn-
in period of 10 iterations for the PY-IBM ones (we
report the average of three runs for both models).
The results, reported in figures 4 to 6, show again
that the PY-IBM model outperformed the Giza++
implementations, and to a large extent in the case
of IBM model 4. This provides further evidence
that the supposition underlying the neighbourhood
</bodyText>
<figure confidence="0.997372666666667">
1.41e6Alignment Disagreement pipeline zh-&gt;en
1.2
1.0
0.8
0.6
0.4
0.20 20 40 60 80 100 120 140 160
iteration (after burn-in)
# of alignment position disagreements
</figure>
<page confidence="0.843188">
975
</page>
<figureCaption confidence="0.995594">
Figure 8: Alignment disagreement of the Chinese to
English PY-IBM model 4 for the 3 repetitions
</figureCaption>
<bodyText confidence="0.999901041666667">
approximation for training models 3 and 4 – that
there exists a small set of alignments on which most
of the probability mass concentrates – is poor. An
interesting observation to note is that the BLEU
score of the non-pipelined PY-IBM model 4 is the
same as the PY-IBM HMM model translating in
both directions, as opposed to an improvement in
the pipelined case. This suggests that the sampler
might not have fully converged after 100 iterations
for model 4 (the number of alignment disagreements
for this experiment can be seen in figure 8). Further
confirmation for this comes from the higher standard
deviation of 0.54 observed for the PY-IBM model 4,
as opposed to a standard deviation for the PY-IBM
HMM model of 0.21 (which is still more significant
than that of the pipelined PY-IBM model 4, whose
standard deviation was 0.13).
Both the PY-IBM and the Giza++ trained mod-
els run in a linear time in the number of sentences,
where due to the nature of MCMC sampling tech-
niques, more iterations are required for its conver-
gence. In our experiments, the running time of
the unoptimised Gibbs sampler was 50 times slower
than the optimised EM.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999982978723404">
The models described in this paper allow one to
use non-parametric approaches to flexibly model
word alignment distributions, overcoming a number
of limitations of the EM algorithm for the fertility
based alignment models. The models achieved a
significant improvement in BLEU scores and AER
on the tested corpus, and are easy to extend without
the need for additional modelling tools.
The alignment models proposed mostly follow the
original generative stories while introducing addi-
tional phrasal conditioning into models 3 and 4.
However there are many other areas in which we
could make use of hierarchical tools to introduce
new dependencies easily without running into spar-
sity problems.
One example is the extension of the transition
history used in the HMM alignment model: IBM
model 1 uses a uniform distribution over transitions,
model 2 conditions on relative sentence positions,
and the HMM model uses a first order dependency.
One extension would be to use longer histories of n
previous positions, handling sparsity with back-off.
Previously proposed approaches to extend the
HMM alignment model include Och and Ney
(2003)’s use of word classes and smoothing, and
the combination of part-of-speech information of
the words surrounding the source word (Brunning
et al., 2009). Using our hierarchical model one
could easily introduce such dependencies on the
context words of the word to be translated and their
part-of-speech information. This could assist in
both translation and reordering disambiguation, and
would incorporate back-off by using smaller and
smaller contexts when such information is sparse.
Further improvements to models 3 and 4 could
be carried out by introducing longer dependencies
in the fertility and distortion distributions. Instead
of conditioning on the previous word, one could
use further information such as PoS tags, previously
translated words, or previous fertilities. Additional
research would involve the use of more effective
variational inference algorithms for hierarchical PY
process based models.
The PY-IBM models described in this paper were
implemented within the Giza++ code base, and
are available as an open source package for further
development and research.1
</bodyText>
<sectionHeader confidence="0.98556" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.3948885">
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
</bodyText>
<footnote confidence="0.808201">
1Available at github.com/yaringal/Giza-sharp
</footnote>
<figure confidence="0.98022375">
# of alignment position disagreements
4.5
4.0
5.01e5Alignment Disagreement Model 4 zh-&gt;en
3.5
3.0
2.50 10 20 30 40 50 60 70 80 90
iteration (after burn-in)
</figure>
<page confidence="0.987115">
976
</page>
<reference confidence="0.999872913461539">
speech induction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 865–874,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Jamie Brunning, Adri`a de Gispert, and William Byrne.
2009. Context-dependent alignment models for statis-
tical machine translation. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, NAACL ’09, pages 110–
118.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
352–361, Singapore, August. Association for Compu-
tational Linguistics.
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 314–323, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459–466.
MIT Press, Cambridge, MA.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ’09, pages 317–325.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. Acous-
tics, Speech, and Signal Processing, IEEE Interna-
tional Conference on, 1:181–184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
of the 45th Annual Meeting of the ACL (ACL-2007),
Prague.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 223–232, Jeju Island, Korea,
July. Association for Computational Linguistics.
Radford Neal. 2003. Slice sampling. Annals of Statis-
tics, 31:705–767.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–52.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160–
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the ACL and 3rd Annual Meeting of
the NAACL (ACL-2002), pages 311–318, Philadelphia,
Pennsylvania.
Sujith Ravi and Kevin Knight. 2010. Does Giza++ make
search errors? Computational Linguistics, 36(3):295–
302, September.
Darcey Riley and Daniel Gildea. 2012. Improving the
ibm alignment models using variational bayes. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Short Papers - Volume
2, ACL ’12, pages 306–310.
A. Stolcke. 2002. SRILM – an extensible language mod-
eling toolkit. In Proc. of the International Conference
on Spoken Language Processing.
Y. W. Teh and M. I. Jordan, 2009. Hierarchical Bayesian
Nonparametric Models with Applications. Cambridge
University Press.
Yee Whye Teh. 2006a. A Bayesian interpretation of
interpolated Kneser-Ney. Technical report, National
University of Singapore School of Computing.
Yee Whye Teh. 2006b. A hierarchical bayesian language
model based on pitman-yor processes. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, ACL-44, pages
985–992, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Stephen Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. of the 16th International Conference
on Computational Linguistics (COLING ’96), pages
836–841, Copenhagen, Denmark, August.
</reference>
<page confidence="0.997902">
977
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.343137">
<title confidence="0.999612">A Systematic Bayesian Treatment of the IBM Alignment Models</title>
<author confidence="0.967392">Yarin</author>
<affiliation confidence="0.9992575">Department of University of</affiliation>
<address confidence="0.994379">Cambridge, CB2 1PZ, United</address>
<email confidence="0.981916">yg279@cam.ac.uk</email>
<author confidence="0.825293">Phil</author>
<affiliation confidence="0.9992415">Department of Computer University of</affiliation>
<address confidence="0.947563">Oxford, OX1 3QD, United</address>
<email confidence="0.914399">Phil.Blunsom@cs.ox.ac.uk</email>
<abstract confidence="0.999912894736842">The dominant yet ageing IBM and HMM word alignment models underpin most popular Statistical Machine Translation implementations in use today. beset by the limitations of implausible independence assumptions, intractable optimisation problems, and an excess of tunable parameters, these models provide a scalable and reliable starting point for inducing translation systems. In this paper we build upon this venerable base by recasting these models in the non-parametric Bayesian framework. By replacing the categorical distributions at their core with hierarchical Pitman-Yor processes, and through the use of collapsed Gibbs sampling, we provide a more flexible formulation and sidestep the original heuristic optimisation techniques.</abstract>
<intro confidence="0.493278">The resulting models are highly extendible,</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>speech induction.</title>
<date></date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>865--874</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker></marker>
<rawString>speech induction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 865–874, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1394" citStr="Brown et al., 1993" startWordPosition="186" endWordPosition="189">ic Bayesian framework. By replacing the categorical distributions at their core with hierarchical Pitman-Yor processes, and through the use of collapsed Gibbs sampling, we provide a more flexible formulation and sidestep the original heuristic optimisation techniques. The resulting models are highly extendible, naturally permitting the introduction of phrasal dependencies. We present extensive experimental results showing improvements in both AER and BLEU when benchmarked against Giza++, including significant improvements over IBM model 4. 1 Introduction The IBM and HMM word alignment models (Brown et al., 1993; Vogel et al., 1996) have underpinned the majority of statistical machine translation systems for almost twenty years. The key attraction of these models is their principled probabilistic formulation, and the existence of (mostly) tractable algorithms for their training. The dominant Giza++ implementation of the IBM models (Och and Ney, 2003) employs a variety of exact and approximate EM algorithms to optimise categorical alignment distributions. While effective, this parametric approach results in a significant number of parameters to be tuned and intractable summations over the space of ali</context>
<context position="9374" citStr="Brown et al., 1993" startWordPosition="1459" endWordPosition="1462"> models 3 and 4. Throughout this section, we assume that the base distributions in our models (denoted G0, H0, etc.) are uniform over all atoms, and omit the strength and concentration parameters of the PY process for clarity. We use subscripts to denote the hierarchy, and lower-case superscripts to denote a fixed condition (for example, Gm0 is the (uniform) base distribution that is determined uniquely for each possible foreign sentence length m). 3.1 Model 1 and the HMM alignment model The most basic word alignment model, IBM model 1, can be described using the following generative process (Brown et al., 1993): Given an English sentence E = e1,..., el, first choose a length m for the foreign sentence F. Next, choose a vector of random word positions from the English sentence A = a1,..., am to be the alignment, and then for each foreign word fi choose a translation from the English word eai aligned to it by A. The existence of a NULL word at the beginning of the English sentence is assumed, a word to which spurious words in the foreign sentence can align. From this generative process the following probability model is derived: P(F, A|E) = p(m|l) x �m p(ai)p(fi|eai) i=1 Where p(ai) = 1 l+1 is uniform</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Brunning</author>
<author>Adri`a de Gispert</author>
<author>William Byrne</author>
</authors>
<title>Context-dependent alignment models for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>110--118</pages>
<marker>Brunning, de Gispert, Byrne, 2009</marker>
<rawString>Jamie Brunning, Adri`a de Gispert, and William Byrne. 2009. Context-dependent alignment models for statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 110– 118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>352--361</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2500" citStr="Cohn and Blunsom, 2009" startWordPosition="347" endWordPosition="350"> approach results in a significant number of parameters to be tuned and intractable summations over the space of alignments for models 3 and 4. Giza++ hides the hyperparameters with defaults and approximates the intractable expectations using restricted alignment neighbourhoods. However this approach was shown to often return alignments with probabilities well below the true maxima (Ravi and Knight, 2010). To overcome perceived limitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. (DeNero et al., 2008; Cohn and Blunsom, 2009; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a range of language tasks (Teh, 2006b; Goldwater et al., 2006; Blunsom and Cohn, 2011). In the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney (Kn</context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352–361, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>314--323</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314–323, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators. In</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>459--466</pages>
<editor>Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2945" citStr="Goldwater et al., 2006" startWordPosition="414" endWordPosition="417">ith the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. (DeNero et al., 2008; Cohn and Blunsom, 2009; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a range of language tasks (Teh, 2006b; Goldwater et al., 2006; Blunsom and Cohn, 2011). In the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney (Kneser and Ney, 1995; Teh, 2006a). The key contribution of the hierarchical PY formulation is that it provides a principle probabilistic framework that easily extends to latent variable models, such as those used 969 Proceedings of NAACL-HLT 2013, pages 969–977, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics for alignment, for which a Kneser-Ney formulation is unclear. While Bayesian priors have previously b</context>
<context position="7464" citStr="Goldwater et al., 2006" startWordPosition="1114" endWordPosition="1117"> a certain given length which is often used to model prior beliefs on parameter sparsity in machine learning problems. The Dirichlet process generalises the Dirichlet distribution to a distribution over infinite sequences of non-negative reals that sum to one and is often used for nonparametric Bayesian inference. The PY process is used in the context of natural language processing as it further generalises the Dirichlet process by adding an additional degree of freedom that enables it to produce power-law discrete probability mass functions that resemble those seen experimentally in corpora (Goldwater et al., 2006). Formally, draws from the PY process G1 .. PY (d, 0, G0) with a discount parameter 0 &lt; d &lt; 1, a strength parameter 0 &gt; —d, and a base distribution G0, are constructed using a Chinese restaurant process analogy as follows: mk — d 0 + dK G0 0 + n sy&apos;` + 0 + n Where mk denotes the number of Xis (customers) assigned to yk (a table) and K is the total number of yks drawn from G0. K Xn+1JX1, ..., Xn .. k=1 970 Hierarchical PY processes (Teh, 2006b), PY processes where the base distribution is itself a PY process, were developed as an extension which is often used in the context of natural language </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 459–466. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>317--325</pages>
<contexts>
<context position="18894" citStr="Johnson and Goldwater, 2009" startWordPosition="3126" endWordPosition="3129">BLEU 28.0 27.5 27.0 26.5 26.0 14.75 14.50 BLEU 14.25 14.00 13.75 13.50 40 39 38 37 AER 36 35 34 33 32 English -&gt; Chinese Pipeline 1 1&gt;H 1&gt;H&gt;3 1&gt;H&gt;3&gt;4 AER Pipeline Giza++ PY-IBM 1 1&gt;H 1&gt;H&gt;3 1&gt;H&gt;3&gt;4 Chinese -&gt; English Pipeline PY-IBM Giza++ 1 1&gt;H 1&gt;H&gt;3 1&gt;H&gt;3&gt;4 PY-IBM Giza++ Figure 1: BLEU scores of pipelined Giza++ and pipelined PY-IBM translating from Chinese into English Figure 2: BLEU scores of pipelined Giza++ and pipelined PY-IBM translating from English into Chinese Figure 3: AER of pipelined Giza++ and pipelined PY-IBM aligning Chinese and English in the prior. Maximum marginal decoding (Johnson and Goldwater, 2009) can then be used to get the MAP estimate of the probability distributions over the alignment positions for each sentence from the samples extracted from the Gibbs sampler. In addition to sampling the alignments, we also place a uniform Beta prior on the discount parameters and a vague Gamma prior on the strength parameters, and sample them using slice sampling (Neal, 2003). The end result is that the alignment models have no free parameters to tune. 5 Experimental results In order to assess our PY process alignment models (referred to as PY-IBM henceforth) several experiments were carried out</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 317–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>Acoustics, Speech, and Signal Processing, IEEE International Conference on,</booktitle>
<pages>1--181</pages>
<contexts>
<context position="3118" citStr="Kneser and Ney, 1995" startWordPosition="440" endWordPosition="443">09; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a range of language tasks (Teh, 2006b; Goldwater et al., 2006; Blunsom and Cohn, 2011). In the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney (Kneser and Ney, 1995; Teh, 2006a). The key contribution of the hierarchical PY formulation is that it provides a principle probabilistic framework that easily extends to latent variable models, such as those used 969 Proceedings of NAACL-HLT 2013, pages 969–977, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics for alignment, for which a Kneser-Ney formulation is unclear. While Bayesian priors have previously been applied to IBM model 1 (Riley and Gildea, 2012), in this work we go considerably further by implementing non-parametric priors for the full Giza++ training pipeline. Inf</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. Acoustics, Speech, and Signal Processing, IEEE International Conference on, 1:181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the ACL (ACL-2007),</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="19839" citStr="Koehn et al., 2007" startWordPosition="3279" endWordPosition="3282"> and sample them using slice sampling (Neal, 2003). The end result is that the alignment models have no free parameters to tune. 5 Experimental results In order to assess our PY process alignment models (referred to as PY-IBM henceforth) several experiments were carried out to benchmark them against the original models (as implemented in Giza++). We evaluated the BLEU scores (Papineni et al., 2002) of translations from Chinese into English and from English into Chinese, as well as the alignment error rates (AER) of the induced symmetrised alignments compared to a human aligned dataset. Moses (Koehn et al., 2007) was used for the training of the SMT system and the symmetrisation (using the grow-diag-final procedure), with MERT (Och, 2003) used for tuning of the weights, and SRILM (Stolcke, 2002) to build the language model (5-grams based). The corpus used for training and evaluation was the Chinese FBIS corpus. MT02 was used for tuning, and MT03 was used for evaluation. In each case we used one reference sentence in Chinese and 4 reference sentences in English. Most translation systems rely on the Giza++ package in which the implementation of the original models is done by combining them in a pipeline</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th Annual Meeting of the ACL (ACL-2007), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Dyer</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model for learning SCFGs with discontiguous rules.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>223--232</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2525" citStr="Levenberg et al., 2012" startWordPosition="351" endWordPosition="354">ignificant number of parameters to be tuned and intractable summations over the space of alignments for models 3 and 4. Giza++ hides the hyperparameters with defaults and approximates the intractable expectations using restricted alignment neighbourhoods. However this approach was shown to often return alignments with probabilities well below the true maxima (Ravi and Knight, 2010). To overcome perceived limitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. (DeNero et al., 2008; Cohn and Blunsom, 2009; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a range of language tasks (Teh, 2006b; Goldwater et al., 2006; Blunsom and Cohn, 2011). In the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney (Kneser and Ney, 1995; Teh, </context>
</contexts>
<marker>Levenberg, Dyer, Blunsom, 2012</marker>
<rawString>Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A Bayesian model for learning SCFGs with discontiguous rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 223–232, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="19270" citStr="Neal, 2003" startWordPosition="3190" endWordPosition="3191">es of pipelined Giza++ and pipelined PY-IBM translating from English into Chinese Figure 3: AER of pipelined Giza++ and pipelined PY-IBM aligning Chinese and English in the prior. Maximum marginal decoding (Johnson and Goldwater, 2009) can then be used to get the MAP estimate of the probability distributions over the alignment positions for each sentence from the samples extracted from the Gibbs sampler. In addition to sampling the alignments, we also place a uniform Beta prior on the discount parameters and a vague Gamma prior on the strength parameters, and sample them using slice sampling (Neal, 2003). The end result is that the alignment models have no free parameters to tune. 5 Experimental results In order to assess our PY process alignment models (referred to as PY-IBM henceforth) several experiments were carried out to benchmark them against the original models (as implemented in Giza++). We evaluated the BLEU scores (Papineni et al., 2002) of translations from Chinese into English and from English into Chinese, as well as the alignment error rates (AER) of the induced symmetrised alignments compared to a human aligned dataset. Moses (Koehn et al., 2007) was used for the training of t</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford Neal. 2003. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1739" citStr="Och and Ney, 2003" startWordPosition="236" endWordPosition="239">ction of phrasal dependencies. We present extensive experimental results showing improvements in both AER and BLEU when benchmarked against Giza++, including significant improvements over IBM model 4. 1 Introduction The IBM and HMM word alignment models (Brown et al., 1993; Vogel et al., 1996) have underpinned the majority of statistical machine translation systems for almost twenty years. The key attraction of these models is their principled probabilistic formulation, and the existence of (mostly) tractable algorithms for their training. The dominant Giza++ implementation of the IBM models (Och and Ney, 2003) employs a variety of exact and approximate EM algorithms to optimise categorical alignment distributions. While effective, this parametric approach results in a significant number of parameters to be tuned and intractable summations over the space of alignments for models 3 and 4. Giza++ hides the hyperparameters with defaults and approximates the intractable expectations using restricted alignment neighbourhoods. However this approach was shown to often return alignments with probabilities well below the true maxima (Ravi and Knight, 2010). To overcome perceived limitations with the word bas</context>
<context position="13221" citStr="Och and Ney (2003)" startWordPosition="2123" endWordPosition="2126">spurious words are rearranged into the final word positions and the spurious words inserted into the empty positions. In model 3 this is done with a zero order HMM, and in model 4 with two first order HMMs. One controls the distortion of the head of each English word (the first foreign word generated from it) relative to the centre (denoted here (D) of the set of foreign words generated from the previous English word, and the other controls the distortion within the set itself by conditioning the word position on the previous word position. For the probability model, we follow the notation of Och and Ney (2003) and define the alignment as a function from the source sentence positions i to Bi ⊂ {1, ..., m} where the Bi’s form a partition of the set {1, ..., m}. The fertility of the English word i is φi = |Bi|, and we use Bi,k to refer to the kth element of Bi in ascending order. Using the above notation, the following probability model is derived (Och and Ney, 2003): P(F, A|E) =p(B0|B1, ..., Bl) × ril p(Bi|Bi−1, ei) i=1 l × ri ri p(fj|ei) i=0 jEBi For model 3 the dependence on previous alignment sets is ignored and the probability p(Bi|Bi−1, ei) is modelled as p(Bi|Bi−1, ei) = p(φi|ei)φi! ri p(j|i, m</context>
<context position="20635" citStr="Och and Ney (2003)" startWordPosition="3415" endWordPosition="3418">e, 2002) to build the language model (5-grams based). The corpus used for training and evaluation was the Chinese FBIS corpus. MT02 was used for tuning, and MT03 was used for evaluation. In each case we used one reference sentence in Chinese and 4 reference sentences in English. Most translation systems rely on the Giza++ package in which the implementation of the original models is done by combining them in a pipeline. Model 1 and the HMM alignment model are run sequentially each for 5 iterations; then models 3 and 4 are run sequentially for 3 iterations each. This follows the observation of Och and Ney (2003) that bootstrapping from previous results assists the fertility algorithms find the best alignment neighbourhood in order to estimate the expectations. We assessed the proposed models against the original models in a pipeline experiment where both systems were trained on a corpus starting at model 1, and used the results of the previous run to initialise the next one – noting the BLEU scores and AER at each step. The Gibbs samplers for the pipelined PY-IBM models were run for 50 iterations for each model and started accumulating samples after a burn-in period of 10 iterations, each experiment </context>
<context position="26485" citStr="Och and Ney (2003)" startWordPosition="4400" endWordPosition="4403">odels 3 and 4. However there are many other areas in which we could make use of hierarchical tools to introduce new dependencies easily without running into sparsity problems. One example is the extension of the transition history used in the HMM alignment model: IBM model 1 uses a uniform distribution over transitions, model 2 conditions on relative sentence positions, and the HMM model uses a first order dependency. One extension would be to use longer histories of n previous positions, handling sparsity with back-off. Previously proposed approaches to extend the HMM alignment model include Och and Ney (2003)’s use of word classes and smoothing, and the combination of part-of-speech information of the words surrounding the source word (Brunning et al., 2009). Using our hierarchical model one could easily introduce such dependencies on the context words of the word to be translated and their part-of-speech information. This could assist in both translation and reordering disambiguation, and would incorporate back-off by using smaller and smaller contexts when such information is sparse. Further improvements to models 3 and 4 could be carried out by introducing longer dependencies in the fertility a</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the ACL (ACL-2003),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="19967" citStr="Och, 2003" startWordPosition="3301" endWordPosition="3302">mental results In order to assess our PY process alignment models (referred to as PY-IBM henceforth) several experiments were carried out to benchmark them against the original models (as implemented in Giza++). We evaluated the BLEU scores (Papineni et al., 2002) of translations from Chinese into English and from English into Chinese, as well as the alignment error rates (AER) of the induced symmetrised alignments compared to a human aligned dataset. Moses (Koehn et al., 2007) was used for the training of the SMT system and the symmetrisation (using the grow-diag-final procedure), with MERT (Och, 2003) used for tuning of the weights, and SRILM (Stolcke, 2002) to build the language model (5-grams based). The corpus used for training and evaluation was the Chinese FBIS corpus. MT02 was used for tuning, and MT03 was used for evaluation. In each case we used one reference sentence in Chinese and 4 reference sentences in English. Most translation systems rely on the Giza++ package in which the implementation of the original models is done by combining them in a pipeline. Model 1 and the HMM alignment model are run sequentially each for 5 iterations; then models 3 and 4 are run sequentially for 3</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the ACL (ACL-2003), pages 160– 167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the ACL and 3rd Annual Meeting of the NAACL (ACL-2002),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="19621" citStr="Papineni et al., 2002" startWordPosition="3244" endWordPosition="3247">or each sentence from the samples extracted from the Gibbs sampler. In addition to sampling the alignments, we also place a uniform Beta prior on the discount parameters and a vague Gamma prior on the strength parameters, and sample them using slice sampling (Neal, 2003). The end result is that the alignment models have no free parameters to tune. 5 Experimental results In order to assess our PY process alignment models (referred to as PY-IBM henceforth) several experiments were carried out to benchmark them against the original models (as implemented in Giza++). We evaluated the BLEU scores (Papineni et al., 2002) of translations from Chinese into English and from English into Chinese, as well as the alignment error rates (AER) of the induced symmetrised alignments compared to a human aligned dataset. Moses (Koehn et al., 2007) was used for the training of the SMT system and the symmetrisation (using the grow-diag-final procedure), with MERT (Och, 2003) used for tuning of the weights, and SRILM (Stolcke, 2002) to build the language model (5-grams based). The corpus used for training and evaluation was the Chinese FBIS corpus. MT02 was used for tuning, and MT03 was used for evaluation. In each case we u</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting of the ACL and 3rd Annual Meeting of the NAACL (ACL-2002), pages 311–318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Does Giza++ make search errors?</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<pages>302</pages>
<contexts>
<context position="2286" citStr="Ravi and Knight, 2010" startWordPosition="314" endWordPosition="317">g. The dominant Giza++ implementation of the IBM models (Och and Ney, 2003) employs a variety of exact and approximate EM algorithms to optimise categorical alignment distributions. While effective, this parametric approach results in a significant number of parameters to be tuned and intractable summations over the space of alignments for models 3 and 4. Giza++ hides the hyperparameters with defaults and approximates the intractable expectations using restricted alignment neighbourhoods. However this approach was shown to often return alignments with probabilities well below the true maxima (Ravi and Knight, 2010). To overcome perceived limitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. (DeNero et al., 2008; Cohn and Blunsom, 2009; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a </context>
</contexts>
<marker>Ravi, Knight, 2010</marker>
<rawString>Sujith Ravi and Kevin Knight. 2010. Does Giza++ make search errors? Computational Linguistics, 36(3):295– 302, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darcey Riley</author>
<author>Daniel Gildea</author>
</authors>
<title>Improving the ibm alignment models using variational bayes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>306--310</pages>
<contexts>
<context position="3596" citStr="Riley and Gildea, 2012" startWordPosition="510" endWordPosition="513">n the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney (Kneser and Ney, 1995; Teh, 2006a). The key contribution of the hierarchical PY formulation is that it provides a principle probabilistic framework that easily extends to latent variable models, such as those used 969 Proceedings of NAACL-HLT 2013, pages 969–977, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics for alignment, for which a Kneser-Ney formulation is unclear. While Bayesian priors have previously been applied to IBM model 1 (Riley and Gildea, 2012), in this work we go considerably further by implementing non-parametric priors for the full Giza++ training pipeline. Inference for the proposed models and their hyper-parameters is done with Gibbs sampling. This eliminates the intractable summations over alignments and the need for tuning hyperparameters. Further, we exploit the highly extendible nature of the hierarchical PY process to implement improvements to the original models such as the introduction of phrasal dependencies. We present extensive experimental results showing improvements in both BLEU scores and AER when compared to Giza</context>
</contexts>
<marker>Riley, Gildea, 2012</marker>
<rawString>Darcey Riley and Daniel Gildea. 2012. Improving the ibm alignment models using variational bayes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, pages 306–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="20025" citStr="Stolcke, 2002" startWordPosition="3311" endWordPosition="3312">ment models (referred to as PY-IBM henceforth) several experiments were carried out to benchmark them against the original models (as implemented in Giza++). We evaluated the BLEU scores (Papineni et al., 2002) of translations from Chinese into English and from English into Chinese, as well as the alignment error rates (AER) of the induced symmetrised alignments compared to a human aligned dataset. Moses (Koehn et al., 2007) was used for the training of the SMT system and the symmetrisation (using the grow-diag-final procedure), with MERT (Och, 2003) used for tuning of the weights, and SRILM (Stolcke, 2002) to build the language model (5-grams based). The corpus used for training and evaluation was the Chinese FBIS corpus. MT02 was used for tuning, and MT03 was used for evaluation. In each case we used one reference sentence in Chinese and 4 reference sentences in English. Most translation systems rely on the Giza++ package in which the implementation of the original models is done by combining them in a pipeline. Model 1 and the HMM alignment model are run sequentially each for 5 iterations; then models 3 and 4 are run sequentially for 3 iterations each. This follows the observation of Och and </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
</authors>
<title>Hierarchical Bayesian Nonparametric Models with Applications.</title>
<date>2009</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="16999" citStr="Teh and Jordan, 2009" startWordPosition="2812" endWordPosition="2815"> original model, where the number of spurious words is determined by a binomial distribution created from a set of Bernoulli experiments, each one performed after the translation of a non-spurious word. We use an indicator function I to signal whether a spurious word was generated after a non-spurious word (I = 1) or not (I = 0). I = 0,1|l — HNF l HNF l — PY (HNF ∅ ) HNF ∅ — PY (HNF 0 ) Then, the translation of spurious words is done in a straightforward manner: HNT fi ∅ HNT ∅ — PY (HNT 0 ) 4 Inference The Gibbs sampling inference scheme together with the Chinese Restaurant Franchise process (Teh and Jordan, 2009) are used to induce alignments for a parallel corpus. A set of restaurants S is constructed and initialised either randomly or through a pipeline of alignment results from simpler models, and then at each iteration each alignment position is removed from the restaurants and re-sampled, conditioned on the rest of the alignment positions. Denoting e, f, a the sets of all source sentences, their translations, and their corresponding alignments in our corpus, and denoting E, F, A a specific source sentence, its translation, and their corresponding alignment, where ei is the i’th word of the source</context>
</contexts>
<marker>Teh, Jordan, 2009</marker>
<rawString>Y. W. Teh and M. I. Jordan, 2009. Hierarchical Bayesian Nonparametric Models with Applications. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A Bayesian interpretation of interpolated Kneser-Ney.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>National University of Singapore School of Computing.</institution>
<contexts>
<context position="2920" citStr="Teh, 2006" startWordPosition="412" endWordPosition="413">imitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. (DeNero et al., 2008; Cohn and Blunsom, 2009; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a range of language tasks (Teh, 2006b; Goldwater et al., 2006; Blunsom and Cohn, 2011). In the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney (Kneser and Ney, 1995; Teh, 2006a). The key contribution of the hierarchical PY formulation is that it provides a principle probabilistic framework that easily extends to latent variable models, such as those used 969 Proceedings of NAACL-HLT 2013, pages 969–977, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics for alignment, for which a Kneser-Ney formulation is unclear. While Bayesian</context>
<context position="7909" citStr="Teh, 2006" startWordPosition="1209" endWordPosition="1210">gree of freedom that enables it to produce power-law discrete probability mass functions that resemble those seen experimentally in corpora (Goldwater et al., 2006). Formally, draws from the PY process G1 .. PY (d, 0, G0) with a discount parameter 0 &lt; d &lt; 1, a strength parameter 0 &gt; —d, and a base distribution G0, are constructed using a Chinese restaurant process analogy as follows: mk — d 0 + dK G0 0 + n sy&apos;` + 0 + n Where mk denotes the number of Xis (customers) assigned to yk (a table) and K is the total number of yks drawn from G0. K Xn+1JX1, ..., Xn .. k=1 970 Hierarchical PY processes (Teh, 2006b), PY processes where the base distribution is itself a PY process, were developed as an extension which is often used in the context of natural language processing due to their relationship to back-off smoothing. Denoting a context of atoms u as (wi−l, ..., wi−1), the hierarchical PY process is defined using the above definition of the PY process by: wi —Gu Gu —PY (d|u|, θ|u|, Gπ(u)) ... G(wi_1) —PY (d1, θ1, G∅) G∅ —PY (d0, θ0, G0) where π(u) = (wi−l+1, ..., wi−1) is the suffix of u, |u |denotes the length of context u, and G0 is a base distribution. 3 A Bayesian approach to word alignment I</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006a. A Bayesian interpretation of interpolated Kneser-Ney. Technical report, National University of Singapore School of Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical bayesian language model based on pitman-yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2920" citStr="Teh, 2006" startWordPosition="412" endWordPosition="413">imitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. (DeNero et al., 2008; Cohn and Blunsom, 2009; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a range of language tasks (Teh, 2006b; Goldwater et al., 2006; Blunsom and Cohn, 2011). In the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney (Kneser and Ney, 1995; Teh, 2006a). The key contribution of the hierarchical PY formulation is that it provides a principle probabilistic framework that easily extends to latent variable models, such as those used 969 Proceedings of NAACL-HLT 2013, pages 969–977, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics for alignment, for which a Kneser-Ney formulation is unclear. While Bayesian</context>
<context position="7909" citStr="Teh, 2006" startWordPosition="1209" endWordPosition="1210">gree of freedom that enables it to produce power-law discrete probability mass functions that resemble those seen experimentally in corpora (Goldwater et al., 2006). Formally, draws from the PY process G1 .. PY (d, 0, G0) with a discount parameter 0 &lt; d &lt; 1, a strength parameter 0 &gt; —d, and a base distribution G0, are constructed using a Chinese restaurant process analogy as follows: mk — d 0 + dK G0 0 + n sy&apos;` + 0 + n Where mk denotes the number of Xis (customers) assigned to yk (a table) and K is the total number of yks drawn from G0. K Xn+1JX1, ..., Xn .. k=1 970 Hierarchical PY processes (Teh, 2006b), PY processes where the base distribution is itself a PY process, were developed as an extension which is often used in the context of natural language processing due to their relationship to back-off smoothing. Denoting a context of atoms u as (wi−l, ..., wi−1), the hierarchical PY process is defined using the above definition of the PY process by: wi —Gu Gu —PY (d|u|, θ|u|, Gπ(u)) ... G(wi_1) —PY (d1, θ1, G∅) G∅ —PY (d0, θ0, G0) where π(u) = (wi−l+1, ..., wi−1) is the suffix of u, |u |denotes the length of context u, and G0 is a base distribution. 3 A Bayesian approach to word alignment I</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006b. A hierarchical bayesian language model based on pitman-yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 985–992, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th International Conference on Computational Linguistics (COLING ’96),</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="1415" citStr="Vogel et al., 1996" startWordPosition="190" endWordPosition="193">k. By replacing the categorical distributions at their core with hierarchical Pitman-Yor processes, and through the use of collapsed Gibbs sampling, we provide a more flexible formulation and sidestep the original heuristic optimisation techniques. The resulting models are highly extendible, naturally permitting the introduction of phrasal dependencies. We present extensive experimental results showing improvements in both AER and BLEU when benchmarked against Giza++, including significant improvements over IBM model 4. 1 Introduction The IBM and HMM word alignment models (Brown et al., 1993; Vogel et al., 1996) have underpinned the majority of statistical machine translation systems for almost twenty years. The key attraction of these models is their principled probabilistic formulation, and the existence of (mostly) tractable algorithms for their training. The dominant Giza++ implementation of the IBM models (Och and Ney, 2003) employs a variety of exact and approximate EM algorithms to optimise categorical alignment distributions. While effective, this parametric approach results in a significant number of parameters to be tuned and intractable summations over the space of alignments for models 3 </context>
<context position="11282" citStr="Vogel et al., 1996" startWordPosition="1789" endWordPosition="1792">sn’t observed frequently. The HMM alignment model uses the Hidden Markov Model to find word alignments. It treats the translations of the words of the English sentence as observables and the alignment positions as the latent variables to be discovered. Its generative process can be described in an abstract way as follows: we determine the length of the foreign sentence and then iterate over the words of the source sentence emitting translations for each word to fill-in the words in the foreign sentence from left to right. The following probability model is derived for the HMM alignment model (Vogel et al., 1996): P(F,A|E) = p(m|l) x �m p(ai|ai−1, m) x p(fi|eai) i=1 For the HMM alignment model we replace the categorical translation distribution p(fi|eai) with the same one we used for model 1, and 971 replace the categorical distribution for the transition p(ai|ai−1, m) with a hierarchical PY process with a longer sequence of alignment positions in the conditional: ai|ai−1, m ∼ Gmai−1 Gmai−1 ∼ PY (Gm0 ) Gm0 ∼ PY (Gm 0 ) We use a unique distribution for each foreign sentence length, and condition the position on the previous alignment position, backing-off to the HMM’s stationary distribution over align</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephen Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proc. of the 16th International Conference on Computational Linguistics (COLING ’96), pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>