<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.993402">
Unsupervised Consonant-Vowel Prediction over Hundreds of Languages
</title>
<author confidence="0.996769">
Young-Bum Kim and Benjamin Snyder
</author>
<affiliation confidence="0.998014">
University of Wisconsin-Madison
</affiliation>
<email confidence="0.993975">
{ybkim,bsnyder}@cs.wisc.edu
</email>
<sectionHeader confidence="0.997344" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992245">
In this paper, we present a solution to one
aspect of the decipherment task: the pre-
diction of consonants and vowels for an
unknown language and alphabet. Adopt-
ing a classical Bayesian perspective, we
performs posterior inference over hun-
dreds of languages, leveraging knowledge
of known languages and alphabets to un-
cover general linguistic patterns of typo-
logically coherent language clusters. We
achieve average accuracy in the unsuper-
vised consonant/vowel prediction task of
99% across 503 languages. We further
show that our methodology can be used
to predict more fine-grained phonetic dis-
tinctions. On a three-way classification
task between vowels, nasals, and non-
nasal consonants, our model yields unsu-
pervised accuracy of 89% across the same
set of languages.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999386448979592">
Over the past centuries, dozens of lost languages
have been deciphered through the painstaking
work of scholars, often after decades of slow
progress and dead ends. However, several impor-
tant writing systems and languages remain unde-
ciphered to this day.
In this paper, we present a successful solution
to one aspect of the decipherment puzzle: auto-
matically identifying basic phonetic properties of
letters in an unknown alphabetic writing system.
Our key idea is to use knowledge of the phonetic
regularities encoded in known language vocabu-
laries to automatically build a universal probabilis-
tic model to successfully decode new languages.
Our approach adopts a classical Bayesian per-
spective. We assume that each language has
an unobserved set of parameters explaining its
observed vocabulary. We further assume that
each language-specific set of parameters was itself
drawn from an unobserved common prior, shared
across a cluster of typologically related languages.
In turn, each cluster derives its parameters from
a universal prior common to all language groups.
This approach allows us to mix together data from
languages with various levels of observations and
perform joint posterior inference over unobserved
variables of interest.
At the bottom layer (see Figure 1), our
model assumes a language-specific data generat-
ing HMM over words in the language vocabulary.
Each word is modeled as an emitted sequence of
characters, depending on a corresponding Markov
sequence of phonetic tags. Since individual letters
are highly constrained in their range of phonetic
values, we make the assumption of one-tag-per-
observation-type (e.g. a single letter is constrained
to be always a consonant or always a vowel across
all words in a language).
Going one layer up, we posit that the language-
specific HMM parameters are themselves drawn
from informative, non-symmetric distributions
representing a typologically coherent language
grouping. By applying the model to a mix of lan-
guages with observed and unobserved phonetic se-
quences, the cluster-level distributions can be in-
ferred and help guide prediction for unknown lan-
guages and alphabets.
We apply this approach to two small decipher-
ment tasks:
</bodyText>
<listItem confidence="0.968040333333333">
1. predicting whether individual characters in
an unknown alphabet and language represent
vowels or consonants, and
2. predicting whether individual characters in
an unknown alphabet and language represent
vowels, nasals, or non-nasal consonants.
</listItem>
<bodyText confidence="0.592742">
For both tasks, our approach yields considerable
</bodyText>
<page confidence="0.944781">
1527
</page>
<note confidence="0.9137795">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1527–1536,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999985214285714">
success. We experiment with a data set consist-
ing of vocabularies of 503 languages from around
the world, written in a mix of Latin, Cyrillic, and
Greek alphabets. In turn for each language, we
consider it and its alphabet “unobserved” — we
hide the graphic and phonetic properties of the
symbols — while treating the vocabularies of the
remaining languages as fully observed with pho-
netic tags on each of the letters.
On average, over these 503 leave-one-language-
out scenarios, our model predicts consonant/vowel
distinctions with 99% accuracy. In the more chal-
lenging task of vowel/nasal/non-nasal prediction,
our model achieves average accuracy over 89%.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999690375">
The most direct precedent to the present work is
a section in Knight et al. (2006) on universal pho-
netic decipherment. They build a trigram HMM
with three hidden states, corresponding to conso-
nants, vowels, and spaces. As in our model, indi-
vidual characters are treated as the observed emis-
sions of the hidden states. In contrast to the present
work, they allow letters to be emitted by multiple
states.
Their experiments show that the HMM trained
with EM successfully clusters Spanish letters into
consonants and vowels. They further design a
more sophisticated finite-state model, based on
linguistic universals regarding syllable structure
and sonority. Experiments with the second model
indicate that it can distinguish sonorous conso-
nants (such as n, m, l, r) from non-sonorous con-
sonants in Spanish. An advantage of the linguis-
tically structured model is that its predictions do
not require an additional mapping step from unin-
terpreted hidden states to linguistic categories, as
they do with the HMM.
Our model and experiments can be viewed as
complementary to the work of Knight et al., while
also extending it to hundreds of languages. We
use the simple HMM with EM as our baseline. In
lieu of a linguistically designed model structure,
we choose an empirical approach, allowing poste-
rior inference over hundreds of known languages
to guide the model’s decisions for the unknown
script and language.
In this sense, our model bears some similarity
to the decipherment model of Snyder et al. (2010),
which used knowledge of a related language (He-
brew) in an elaborate Bayesian framework to de-
cipher the ancient language of Ugaritic. While the
aim of the present work is more modest (discover-
ing very basic phonetic properties of letters) it is
also more widely applicable, as we don’t required
detailed analysis of a known related language.
Other recent work has employed a simi-
lar perspective for tying learning across lan-
guages. Naseem et al. (2009) use a non-parametric
Bayesian model over parallel text to jointly learn
part-of-speech taggers across 8 languages, while
Cohen and Smith (2009) develop a shared logis-
tic normal prior to couple multilingual learning
even in the absence of parallel text. In simi-
lar veins, Berg-Kirkpatrick and Klein (2010) de-
velop hierarchically tied grammar priors over lan-
guages within the same family, and Bouchard-
Côté et al. (2013) develop a probabilistic model of
sound change using data from 637 Austronesian
languages.
In our own previous work, we have developed
the idea that supervised knowledge of some num-
ber of languages can help guide the unsupervised
induction of linguistic structure, even in the ab-
sence of parallel text (Kim et al., 2011; Kim and
Snyder, 2012)1. In the latter work we also tack-
led the problem of unsupervised phonemic predic-
tion for unknown languages by using textual reg-
ularities of known languages. However, we as-
sumed that the target language was written in a
known (Latin) alphabet, greatly reducing the dif-
ficulty of the prediction task. In our present case,
we assume no knowledge of any relationship be-
tween the writing system of the target language
and known languages, other than that they are all
alphabetic in nature.
Finally, we note some similarities of our model
to some ideas proposed in other contexts. We
make the assumption that each observation type
(letter) occurs with only one hidden state (con-
sonant or vowel). Similar constraints have been
developed for part-of-speech tagging (Lee et al.,
2010; Christodoulopoulos et al., 2011), and the
power of type-based sampling has been demon-
strated, even in the absence of explicit model con-
straints (Liang et al., 2010).
</bodyText>
<sectionHeader confidence="0.995245" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.9604105">
Our generative Bayesian model over the ob-
served vocabularies of hundreds of languages is
</bodyText>
<footnote confidence="0.9971345">
1We note that similar ideas were simultaneously proposed
by other researchers (Cohen et al., 2011).
</footnote>
<page confidence="0.994639">
1528
</page>
<figureCaption confidence="0.979183">
Figure 1: Graphical representation of our model.
</figureCaption>
<bodyText confidence="0.703036333333333">
We have K language clusters, L languages, and V
words in each language.
presented in Figure 1 and Algorithms 1, 2, and
3. We present a running commentary on the gen-
erative process from the bottom up, starting with
Algorithm 3.
</bodyText>
<subsectionHeader confidence="0.999938">
3.1 Data Generation
</subsectionHeader>
<bodyText confidence="0.999845285714286">
At the data generation stage (Algorithm 3), our
model resembles an HMM. At each time step i, a
tag tz is selected according to a language-specific
transition distribution 0, indexed by the previous
tag tz_1. We note that in practice, we implemented
a trigram version of the mode1,2 but we present the
bigram version here for notational clarity. We as-
sume that our tagset includes phonetic categories
of interest (such as consonant, vowel, nasal, etc)
as well as a special tag to denote the boundaries
between words.
An observation index j E 1 ... Nie,t, is then
drawn from the language-specific emission distri-
bution 0, indexed by the current tag tz. Ne,t, de-
notes the number of observation types associated
with tag tz in language t. Finally, we assume the
existence of a deterministic function orth which
maps each tag&apos;s observation indices to unique or-
thographic character symbols. This ensures that
each observed character type corresponds to an
observation index in exactly one tag category.
</bodyText>
<subsectionHeader confidence="0.999981">
3.2 Language Generation
</subsectionHeader>
<bodyText confidence="0.9997125">
At the next stage up (Algorithm 2), we consider
the generation of all language-specific parameters.
This process begins by selecting a language clus-
ter assignment .z.e uniformly. The language cluster
</bodyText>
<footnote confidence="0.9202745">
2where the transition distribution is indexed by the previ-
ous two tags
</footnote>
<equation confidence="0.3233115">
Algorithm 1 Cluster Generation
for each cluster k E 1...K do
for each tag t E 1...T do
II emission Dirichlet parameter
i3k,t Unif [0, 500]
II type-count Poisson parameter
Ak,t Gamma(gi, 02)
II transition Dirichlet parameters
for each tag t&apos; do
ak,t,t1 Unif [0, 500]
Algorithm 2 Language Generation
for each language t do
II draw cluster assignment
cluster .z.e Unif [1...K]
for each tag t do
II generate tag type-count
Poisson(Azf,t)
II generate emission multinomial
0e,t,1 • • Of,t,Ne,t SymmDir(,t)
II generate transition multinomial
0 f,t,1 • 0 f,t,T Dir(azf,t,i ...
Algorithm 3 Data Generation
for each language t do
for each position i do
II transition to new tag token
tzltz-1 Cat(ef,t,_1,1 ••• Of,t,_1,T)
II emit observation index token
— Cat(kt,,i .• •
II transcribe index token as character
wi orth(t, j, ti)
</equation>
<listItem confidence="0.854971">
provides priors over the HMM parameters. These
priors include:
1. Poisson distributions over the number of ob-
servation types Ne,t associated with tag t,
2. Dirichlet priors over transition distributions
0, and
3. Dirichlet priors over emission distributions 0.
</listItem>
<page confidence="0.987652">
1529
</page>
<bodyText confidence="0.999713923076923">
For example, the cluster Poisson parameter over
vowel observation types might be A = 9 (indi-
cating 9 vowel letters on average for the cluster),
while the parameter over consonant observation
types might be A = 20 (indicating 20 consonant
letters on average). These priors will be distinct
for each language cluster and serve to characterize
its general linguistic and typological properties.
We pause at this point to review the Dirich-
let distribution in more detail. A k−dimensional
Dirichlet with parameters α1 ...αk defines a distri-
bution over the k − 1 simplex with the following
density:
</bodyText>
<equation confidence="0.9909135">
�f(B1 ... Bk|α1 ... αk) ∝
i
</equation>
<bodyText confidence="0.978181">
where αi &gt; 0, Bi &gt; 0, and Ei Bi = 1. The Dirich-
let serves as the conjugate prior for the Multino-
mial, meaning that the posterior B1...Bk|X1...Xn is
again distributed as a Dirichlet (with updated pa-
rameters). It is instructive to reparameterize the
Dirichlet with k + 1 parameters:
</bodyText>
<equation confidence="0.9902585">
�f(B1 ... Bk|α0, α′1 ... α′k) ∝ Bi α0α′i−1
i
</equation>
<bodyText confidence="0.999758785714286">
where α0 = Ei αi, and α′i = αi/α0. In this
parameterization, we have ]E[Bi] = α′i. In other
words, the parameters α′i give the mean of the dis-
tribution, and α0 gives the precision of the dis-
tribution. For large α0 ≫ k, the distribution is
highly peaked around the mean (conversely, when
α0 ≪ k, the mean lies in a valley).
Thus, the Dirichlet parameters of a language
cluster characterize both the average HMMs of in-
dividual languages within the cluster, as well as
how much we expect the HMMs to vary from
the mean. In the case of emission distribu-
tions, we assume symmetric Dirichlet priors — i.e.
one-parameter Dirichlets with densities given by
</bodyText>
<equation confidence="0.794919">
f(B1...Bk|0) ∝ H B(β−1)
</equation>
<bodyText confidence="0.982726545454545">
i . This assumption is nec-
essary, as we have no way to identify characters
across languages in the decipherment scenario,
and even the number of consonants and vowels
(and thus multinomial/Dirichlet dimensions) can
vary across the languages of a cluster. Thus, the
mean of these Dirichlets will always be a uniform
emission distribution. The single Dirichlet emis-
sion parameter per cluster will specify whether
this mean is on a peak (large 0) or in a valley
(small 0). In other words, it will control the ex-
pected sparsity of the resulting per-language emis-
sion multinomials.
In contrast, the transition Dirichlet parameters
may be asymmetric, and thus very specific and
informative. For example, one cluster may have
the property that CCC consonant clusters are ex-
ceedingly rare across all its languages. This prop-
erty would be expressed by a very small mean
α′CCC ≪ 1 but large precision α0. Later we shall
see examples of learned transition Dirichlet pa-
rameters.
</bodyText>
<subsectionHeader confidence="0.999654">
3.3 Cluster Generation
</subsectionHeader>
<bodyText confidence="0.999769714285714">
The generation of the cluster parameters (Algo-
rithm 1) defines the highest layer of priors for our
model. As Dirichlets lack a standard conjugate
prior, we simply use uniform priors over the in-
terval [0, 500]. For the cluster Poisson parameters,
we use conjugate Gamma distributions with vague
priors.3
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.99974665">
In this section we detail the inference proce-
dure we followed to make predictions under our
model. We run the procedure over data from
503 languages, assuming that all languages but
one have observed character and tag sequences:
w1, w2, ... , t1, t2, ... Since each character type w
is assumed to have a single tag category, this is
equivalent to observing the character token se-
quence along with a character-type-to-tag map-
ping tw. For the target language, we observe only
character token sequence w1, w2, .. .
We assume fixed and known parameter val-
ues only at the cluster generation level. Unob-
served variables include (i) the cluster parameters
α, 0, A, (ii) the cluster assignments z, (iii) the per-
language HMM parameters B, φ for all languages,
and (iv) for the target language, the tag tokens
t1, t2, ... — or equivalently the character-type-to-
tag mappings tw — along with the observation
type-counts Nt.
</bodyText>
<subsectionHeader confidence="0.997538">
4.1 Monte Carlo Approximation
</subsectionHeader>
<bodyText confidence="0.999979333333333">
Our goal in inference is to predict the most likely
tag tw,ℓ for each character type w in our target lan-
guage ℓ according to the posterior:
</bodyText>
<equation confidence="0.764633333333333">
f (tw,ℓ  |w, t−ℓ)
ˆ= f (tℓ, z, α, 0  |w, t−ℓ) d Θ (1)
3(1,19) for consonants, (1,10) for vowels, (0.2, 15) for
nasals, and (1,16) for non-nasal consonants.
Bαi−1
i
</equation>
<page confidence="0.83201">
1530
</page>
<bodyText confidence="0.998099111111111">
where O = (t−w,ℓ, z, α, β), w are the observed
character sequences for all languages, t−ℓ are the
character-to-tag mappings for the observed lan-
guages, z are the language-to-cluster assignments,
and α and β are all the cluster-level transition and
emission Dirichlet parameters.
Sampling values (tℓ, z, α, β)Nn=1 from the inte-
grand in Equation 1 allows us to perform the stan-
dard Monte Carlo approximation:
</bodyText>
<equation confidence="0.9915875">
f (tw,ℓ = t  |w, t−ℓ)
N
, N−1 ff (tw,ℓ = t in sample n) (2)
n=1
</equation>
<bodyText confidence="0.99998725">
To maximize the Monte Carlo posterior, we sim-
ply take the most commonly sampled tag value
for character type w in language ℓ. Note that
we leave out the language-level HMM parame-
ters (θ, φ) as well as the cluster-level Poisson pa-
rameters λ from Equation 1 (and thus our sample
space), as we can analytically integrate them out
in our sampling equations.
</bodyText>
<subsectionHeader confidence="0.99537">
4.2 Gibbs Sampling
</subsectionHeader>
<bodyText confidence="0.999935416666667">
To sample values (tℓ, z, α, β) from their poste-
rior (the integrand of Equation 1), we use Gibbs
sampling, a Monte Carlo technique that constructs
a Markov chain over a high-dimensional sample
space by iteratively sampling each variable condi-
tioned on the currently drawn sample values for
the others, starting from a random initialization.
The Markov chain converges to an equilibrium
distribution which is in fact the desired joint den-
sity (Geman and Geman, 1984). We now sketch
the sampling equations for each of our sampled
variables.
</bodyText>
<subsectionHeader confidence="0.848538">
Sampling tw,ℓ
</subsectionHeader>
<bodyText confidence="0.9992025">
To sample the tag assignment to character w in
language ℓ, we need to compute:
</bodyText>
<equation confidence="0.8819365">
f (tw,ℓ  |w, t−w,ℓ, t−ℓ, z, α, β) (3)
a f (wℓ, tℓ, Nℓ  |αk, βk, Nk−ℓ) (4)
</equation>
<bodyText confidence="0.998285315789474">
where Nℓ are the types-per-tag counts implied by
the mapping tℓ, k is the current cluster assignment
for the target language (zℓ = k), αk and βk are the
cluster parameters, and Nk−ℓ are the types-per-tag
counts for all languages currently assigned to the
cluster, other than language ℓ.
Applying the chain rule along with our model’s
conditional independence structure, we can further
re-write Equation 4 as a product of three terms:
f(Nℓ|Nk−ℓ)
f(t1,t2,... |αk)
f(w1, w2, ... |Nℓ, t1, t2, ... ,βk)
The first term is the posterior predictive distribu-
tion for the Poisson-Gamma compound distribu-
tion and is easy to derive. The second term is the
tag transition predictive distribution given Dirich-
let hyperparameters, yielding a familiar Polya urn
scheme form. Removing terms that don’t depend
on the tag assignment tℓ,w gives us:
</bodyText>
<equation confidence="0.99953775">
H(αk,t,t′ + n(t, t′))[n′(t,t′)]
t,t′
H (Et′ αk,t,t′ + n(t))
t
</equation>
<bodyText confidence="0.990041090909091">
where n(t) and n(t, t′) are, respectively, unigram
and bigram tag counts excluding those containing
character w. Conversely, n′(t) and n′(t, t′) are,
respectively, unigram and bigram tag counts only
including those containing character w. The no-
tation a[n] denotes the ascending factorial: a(a +
1) · · · (a+n−1). Finally, we tackle the third term,
Equation 7, corresponding to the predictive dis-
tribution of emission observations given Dirichlet
hyperparameters. Again, removing constant terms
gives us:
</bodyText>
<equation confidence="0.822091">
β[n(w)]
k,t
Ht′ Nℓ,t′β[n(t′)]
k,t′
</equation>
<bodyText confidence="0.999667666666667">
where n(w) is the unigram count of character w,
and n(t′) is the unigram count of tag t, over all
characters tokens (including w).
</bodyText>
<subsectionHeader confidence="0.821276">
Sampling αk,t,t′
</subsectionHeader>
<bodyText confidence="0.996571">
To sample the Dirichlet hyperparameter for cluster
k and transition t -+ t′, we need to compute:
</bodyText>
<equation confidence="0.621191666666667">
f(αk,t,t′|t, z)
a f(t, z|αz,t,t′)
= f(tk|αz,t,t′)
</equation>
<bodyText confidence="0.999797428571429">
where tk are the tag sequences for all languages
currently assigned to cluster k. This term is a pre-
dictive distribution of the multinomial-Dirichlet
compound when the observations are grouped into
multiple multinomials all with the same prior.
Rather than inefficiently computing a product of
Polya urn schemes (with many repeated ascending
</bodyText>
<equation confidence="0.433008">
[n′(t)]
</equation>
<page confidence="0.940136">
1531
</page>
<bodyText confidence="0.988835">
factorials with the same base), we group common
terms together and calculate:
</bodyText>
<equation confidence="0.999141">
11j=1(αk,t,t′ + k)n(j,k,t,t′)
11j=1(Et′′ αk,t,t′′ + k)n(j,k,t)
</equation>
<bodyText confidence="0.9999627">
where n(j, k, t) and n(j, k, t, t′) are the numbers
of languages currently assigned to cluster k which
have more than j occurrences of unigram (t) and
bigram (t, t′), respectively.
This gives us an efficient way to compute un-
normalized posterior densities for α. However, we
need to sample from these distributions, not just
compute them. To do so, we turn to slice sam-
pling (Neal, 2003), a simple yet effective auxiliary
variable scheme for sampling values from unnor-
malized but otherwise computable densities.
The key idea is to supplement the variable
x, distributed according to unnormalized density
˜p(x), with a second variable u with joint density
defined as p(x, u) ∝ ff(u &lt; ˜p(x)). It is easy
to see that ˜p(x) ∝ ´ p(x, u)du. We then itera-
tively sample u|x and x|u, both of which are dis-
tributed uniformly across appropriately bounded
intervals. Our implementation follows the pseudo-
code given in Mackay (2003).
</bodyText>
<subsectionHeader confidence="0.534168">
Sampling Ok,t
</subsectionHeader>
<bodyText confidence="0.929850740740741">
To sample the Dirichlet hyperparameter for cluster
k and tag t we need to compute:
f(Ok,t|t, w, z, N)
∝ f /(w |t, z, Ok,t, N)
∝ f (wk |tk, Ok,t, Nk)
where, as before, tk are the tag sequences for
languages assigned to cluster k, Nk are the tag
observation type-counts for languages assigned
to the cluster, and likewise wk are the char-
acter sequences of all languages in the cluster.
Again, we have the predictive distribution of
the multinomial-Dirichlet compound with multi-
ple grouped observations. We can apply the same
trick as above to group terms in the ascending fac-
torials for efficient computation. As before, we
use slice sampling for obtaining samples.
Sampling zℓ
Finally, we consider sampling the cluster assign-
ment zℓ for each language ℓ. We calculate:
f(zℓ = k|w, t, N, z−ℓ, α, O)
∝ f(wℓ, tℓ, Nℓ|αk, Ok, Nk−ℓ)
= f(Nℓ|Nk−ℓ)f(tℓ|αk)f(wℓ|tℓ, Nℓ, Ok)
The three terms correspond to (1) a standard pre-
dictive distributions for the Poisson-gamma com-
pound and (2) the standard predictive distribu-
tions for the transition and emission multinomial-
Dirichlet compounds.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999949066666667">
To test our model, we apply it to a corpus of 503
languages for two decipherment tasks. In both
cases, we will assume no knowledge of our tar-
get language or its writing system, other than that
it is alphabetic in nature. At the same time, we
will assume basic phonetic knowledge of the writ-
ing systems of the other 502 languages. For our
first task, we will predict whether each character
type is a consonant or a vowel. In the second task,
we further subdivide the consonants into two ma-
jor categories: the nasal consonants, and the non-
nasal consonants. Nasal consonants are known to
be perceptually very salient and are unique in be-
ing high frequency consonants in all known lan-
guages.
</bodyText>
<subsectionHeader confidence="0.960405">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.99989036">
Our data is drawn from online electronic transla-
tions of the Bible (http://www.bible.is,
http://www.crosswire.org/index.
jsp, and http://www.biblegateway.
com). We have identified translations covering
503 distinct languages employing alphabetic
writing systems. Most of these languages (476)
use variants of the Latin alphabet, a few (26)
use Cyrillic, and one uses the Greek alphabet.
As Table 1 indicates, the languages cover a very
diverse set of families and geographic regions,
with Niger-Congo languages being the largest
represented family.4 Of these languages, 30 are
either language isolates, or sole members of their
language family in our data set.
For our experiments, we extracted unique word
types occurring at least 5 times from the down-
loaded Bible texts. We manually identified vowel,
nasal, and non-nasal character types. Since the let-
ter “y” can frequently represent both a consonant
and vowel, we exclude it from our evaluation. On
average, the resulting vocabularies contain 2,388
unique words, with 19 consonant characters, two
2 nasal characters, and 9 vowels. We include the
data as part of the paper.
</bodyText>
<footnote confidence="0.999095">
4In fact, the Niger-Congo grouping is often considered
the largest language family in the world in terms of distinct
member languages.
</footnote>
<page confidence="0.949712">
1532
</page>
<table confidence="0.980638846153846">
Non-Latin Isolates All Model Cons vs Vowel C vs V vs N
EM 93.37 74.59
SYMM 95.99 80.72
MERGE 97.14 86.13
CLUST 98.85 89.37
EM 94.50 74.53
SYMM 96.18 78.13
MERGE 97.66 86.47
CLUST 98.55 89.07
EM 92.93 78.26
SYMM 95.90 79.04
MERGE 96.06 83.78
CLUST 97.03 85.79
</table>
<tableCaption confidence="0.977364">
Table 2: Average accuracy for EM baseline and
</tableCaption>
<bodyText confidence="0.9800152">
model variants across 503 languages. First panel:
results on all languages. Second panel: results for
30 isolate and singleton languages. Third panel:
results for 27 non-Latin alphabet languages (Cyril-
lic and Greek). Standard Deviations across lan-
guages are about 2%.
our Gibbs sampling inference method for the type-
based HMM, even in the absence of multilingual
priors.
We next consider a variant of our model,
MERGE, that assumes that all languages reside in
a single cluster. This allows knowledge from the
other languages to affect our tag posteriors in a
generic, language-neutral way.
Finally, we consider the full version of our
model, CLUST, with 20 language clusters. By al-
lowing for the division of languages into smaller
groupings, we hope to learn more specific param-
eters tailored for typologically coherent clusters of
languages.
</bodyText>
<table confidence="0.99971025">
Language Family #lang
Niger-Congo 114
Austronesian 67
Oto-Manguean 41
Indo-European 39
Mayan 34
Quechuan 17
Afro-Asiatic 17
Uto-Aztecan 16
Altaic 16
Trans-New Guinea 15
Nilo-Saharan 14
Sino-Tibetan 13
Tucanoan 9
Creole 8
Chibchan 6
Maipurean 5
Tupian 5
Nakh-Daghestanian 4
Uralic 4
Cariban 4
Totonacan 4
Mixe-Zoque 3
Jivaroan 3
Choco 3
Guajiboan 2
Huavean 2
Austro-Asiatic 2
Witotoan 2
Jean 2
Paezan 2
Other 30
</table>
<tableCaption confidence="0.946297">
Table 1: Language families in our data set. The
Other category includes 9 language isolates and
21 language family singletons.
</tableCaption>
<subsectionHeader confidence="0.986094">
5.2 Baselines and Model Variants
</subsectionHeader>
<sectionHeader confidence="0.998817" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.99998116">
As our baseline, we consider the trigram HMM
model of Knight et al. (2006), trained with EM. In
all experiments, we run 10 random restarts of EM,
and pick the prediction with highest likelihood.
We map the induced tags to the gold-standard tag
categories (1-1 mapping) in the way that maxi-
mizes accuracy.
We then consider three variants of our model.
The simplest version, SYMM, disregards all in-
formation from other languages, using simple
symmetric hyperparameters on the transition and
emission Dirichlet priors (all hyperparameters set
to 1). This allows us to assess the performance of
The results of our experiments are shown in Ta-
ble 2. In all cases, we report token-level accuracy
(i.e. frequent characters count more than infre-
quent characters), and results are macro-averaged
over the 503 languages. Variance across languages
is quite low: the standard deviations are about 2
percentage points.
For the consonant vs. vowel prediction task,
all tested models perform well. Our baseline, the
EM-based HMM, achieves 93.4% accuracy. Sim-
ply using our Gibbs sampler with symmetric priors
boosts the performance up to 96%. Performance
</bodyText>
<page confidence="0.929253">
1533
</page>
<table confidence="0.805457">
VOWEL VOWEL VOWEL VOWEL
457 1 OCI4&apos; O? &apos;IP c,
context = START context = START, CONS context = START, VOWEL context = VOWEL, CONS
</table>
<figureCaption confidence="0.987549">
Figure 2: Inferred transition Dirichlet distributions for trigram MERGE model. Heat plots indicate Dirich-
let densities over the 2-simplex.
</figureCaption>
<figure confidence="0.998806">
V V
0 . a
. • .
• 0
</figure>
<figureCaption confidence="0.9975495">
Figure 3: Confusion matrix for CLUST (left) and
EM (right). Rows show true values, columns show
predicted values. Size of blobs are proportional to
counts.
</figureCaption>
<bodyText confidence="0.95488128">
increases again when we condition on other lan-
guages (MERGE), and we observe nearly 99% ac-
curacy when allowing languages to cluster.
In the three-way nasal vs. non-nasal consonant
vs. vowel prediction task, EM does not fare partic-
ularly well, only achieving 75% accuracy. As be-
fore, we see increasing performance gains for our
model variants, culminating in almost 90% accu-
racy when the language clustering is used. The
relatively weak performance of EM in this case
should not be surprising: there is no a priori rea-
son to expect any particular three-way classifica-
tion to be the most salient clustering of letters
from the perspective of EM. In contrast, our em-
pirical multilingual approach allows the language-
specific tag predictions to be guided by whatever
values are set for the other, observed, languages.
We note that although a post-hoc mapping from
inferred tags to true tags is necessary for both EM
and SYMM, this is not the case for the final two
variants of our model. Both MERGE and CLUST
break symmetries over tags by way of the asym-
metric posterior over transition Dirichlet param-
eters. Thus the reported accuracies are obtained
without the need for any additional tag mappings.
</bodyText>
<figureCaption confidence="0.902654545454545">
Figure 2 further breaks down results for lan-
guages without any other related language in our
collection. These include 9 language isolates and
21 singleton languages acting as sole representa-
tives of their families In addition, we show results
for the 27 languages which employ non-Latin al-
phabets (26 Cyrillic and one Greek). Both of these
scenarios are likely to occur in cases of lost lan-
guage decipherment. We see similar results and
trends, with somewhat lower performance in both
cases.
</figureCaption>
<sectionHeader confidence="0.975948" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.979654678571429">
To further compare our model to the EM base-
line, we show confusion matrices for the three-
way classification task in Figure 3. We can im-
mediately see that EM had considerable difficulty
making nasal predictions. Most true nasals (third
row) are assigned to the regular consonant cate-
gory, and apparently EM mostly used the addi-
tional tag as a way to further subcategorize vowels.
In contrast, our model does fairly well with nasals:
most actual nasals are assigned to the nasal cate-
gory (third row), while the plurality of nasal pre-
dictions are indeed true nasals (third column).
Next we examine the transition Dirichlet hy-
perparameters learned by our model. For the
MERGE model, we infer a posterior over param-
eters shared by all 503 languages in our data set.
Figure 2 shows MAP estimates of four of the
Dirichlets governing transition probabilities from
various contexts. As we can see, the learned hy-
perparameters yield highly asymmetric priors over
transition distributions. Most languages like to
start words with consonants, and after an initial
consonant or vowel prefer to switch to the opposite
category. In contrast, after a vowel-consonant se-
quence, languages can vary significantly in terms
of the category favored next.
Figure 4 shows MAP transition Dirichlet hy-
perparameters of the CLUST model, when trained
</bodyText>
<figure confidence="0.99723925">
V
• • •
. 41) •
• • •
</figure>
<page confidence="0.716532">
1534
</page>
<figureCaption confidence="0.999253666666667">
Figure 4: Inferred Dirichlet transition hyperparameters for bigram CLUST on three-way classification
task with four latent clusters. Row gives starting state, column gives target state. Size of red blobs are
proportional to magnitude of corresponding hyperparameters.
</figureCaption>
<table confidence="0.999835142857143">
Language Family Portion #langs Ent.
0.38 26 2.26
Indo-European 0.24 41 3.19
0.21 38 3.77
Quechuan 0.89 18 0.61
Mayan 0.64 33 1.70
Oto-Manguean 0.55 31 1.99
Maipurean 0.25 8 2.75
Tucanoan 0.2 45 3.98
Uto-Aztecan 0.4 25 2.85
Altaic 0.44 27 2.76
1 2 0.00
0.78 23 1.26
0.74 27 1.05
Niger-Congo 0.68 22 1.22
0.67 33 1.62
0.5 18 2.21
0.24 25 3.27
0.91 22 0.53
Austronesian 0.71 21 1.51
0.24 17 3.06
</table>
<tableCaption confidence="0.987403">
Table 3: Plurality language families across 20
</tableCaption>
<bodyText confidence="0.991058083333333">
clusters. The columns indicate portion of lan-
guages in the plurality family, number of lan-
guages, and entropy over families.
with a bigram HMM with four language clus-
ters. Examining just the first row, we see that
the languages are partially grouped by their pref-
erence for the initial tag of words. All clus-
ters favor languages which prefer initial conso-
nants, though this preference is most weakly ex-
pressed in cluster 3. In contrast, both clusters
2 and 4 have very dominant tendencies towards
consonant-initial languages, but differ in the rel-
ative weight given to languages preferring either
vowels or nasals initially.
Finally, we examine the relationship between
the induced clusters and language families in Ta-
ble 3, for the trigram consonant vs. vowel CLUST
model with 20 clusters. We see that for about
half the clusters, there is a majority language fam-
ily, most often Niger-Congo. We also observe
distinctive clusters devoted to Austronesian and
Quechuan languages. The largest two clusters are
rather indistinct, without any single language fam-
ily achieving more than 24% of the total.
</bodyText>
<sectionHeader confidence="0.999166" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999969590909091">
In this paper, we presented a successful solution
to one aspect of the decipherment task: the predic-
tion of consonants and vowels for an unknown lan-
guage and alphabet. Adopting a classical Bayesian
perspective, we develop a model that performs
posterior inference over hundreds of languages,
leveraging knowledge of known languages to un-
cover general linguistic patterns of typologically
coherent language clusters. Using this model, we
automatically distinguish between consonant and
vowel characters with nearly 99% accuracy across
503 languages. We further experimented on a
three-way classification task involving nasal char-
acters, achieving nearly 90% accuracy.
Future work will take us in several new direc-
tions: first, we would like to move beyond the as-
sumption of an alphabetic writing system so that
we can apply our method to undeciphered syllabic
scripts such as Linear A. We would also like to
extend our methods to achieve finer-grained reso-
lution of phonetic properties beyond nasals, con-
sonants, and vowels.
</bodyText>
<sectionHeader confidence="0.998669" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.97990325">
The authors thank the reviewers and acknowledge support by
the NSF (grant IIS-1116676) and a research gift from Google.
Any opinions, findings, or conclusions are those of the au-
thors, and do not necessarily reflect the views of the NSF.
</bodyText>
<page confidence="0.987936">
1535
</page>
<sectionHeader confidence="0.998329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935224137931">
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic
grammar induction. In Proceedings of the ACL, pages
1288–1297. Association for Computational Linguistics.
Alexandre Bouchard-Côté, David Hall, Thomas L Griffiths,
and Dan Klein. 2013. Automated reconstruction of
ancient languages using probabilistic models of sound
change. Proceedings of the National Academy of Sci-
ences, 110(11):4224–4229.
Christos Christodoulopoulos, Sharon Goldwater, and Mark
Steedman. 2011. A Bayesian mixture model for part-
of-speech induction using multiple features. In Proceed-
ings of EMNLP, pages 638–647. Association for Compu-
tational Linguistics.
Shay B Cohen and Noah A Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In Proceedings of NAACL, pages 74–
82. Association for Computational Linguistics.
Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011. Un-
supervised structure prediction with non-parallel multilin-
gual guidance. In Proceedings of EMNLP, pages 50–61.
Association for Computational Linguistics.
Stuart Geman and Donald Geman. 1984. Stochastic relax-
ation, Gibbs distributions, and the Bayesian restoration of
images. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, (6):721–741.
Young-Bum Kim and Benjamin Snyder. 2012. Univer-
sal grapheme-to-phoneme prediction over latin alphabets.
In Proceedings of EMNLP, pages 332–343, Jeju Island,
South Korea, July. Association for Computational Lin-
guistics.
Young-Bum Kim, João V Graça, and Benjamin Snyder.
2011. Universal morphological analysis using structured
nearest neighbor prediction. In Proceedings of EMNLP,
pages 322–332. Association for Computational Linguis-
tics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada.
2006. Unsupervised analysis for decipherment problems.
In Proceedings of COLING/ACL, pages 499–506. Associ-
ation for Computational Linguistics.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010.
Simple type-level unsupervised POS tagging. In Proceed-
ings of EMNLP, pages 853–861. Association for Compu-
tational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2010. Type-
based MCMC. In Proceedings of NAACL, pages 573–581.
Association for Computational Linguistics.
David JC MacKay. 2003. Information Theory, Inference and
Learning Algorithms. Cambridge University Press.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech tag-
ging: Two unsupervised approaches. Journal of Artificial
Intelligence Research, 36(1):341–385.
Radford M Neal. 2003. Slice sampling. Annals of statistics,
31:705–741.
Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010.
A statistical model for lost language decipherment. In
Proceedings of the ACL, pages 1048–1057. Association
for Computational Linguistics.
</reference>
<page confidence="0.992522">
1536
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983369">
<title confidence="0.997241">Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</title>
<author confidence="0.992384">Young-Bum Kim</author>
<author confidence="0.992384">Benjamin</author>
<affiliation confidence="0.999267">University of</affiliation>
<email confidence="0.998615">ybkim@cs.wisc.edu</email>
<email confidence="0.998615">bsnyder@cs.wisc.edu</email>
<abstract confidence="0.999602523809524">In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters. We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic distinctions. On a three-way classification task between vowels, nasals, and nonnasal consonants, our model yields unsupervised accuracy of 89% across the same set of languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Phylogenetic grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>1288--1297</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6584" citStr="Berg-Kirkpatrick and Klein (2010)" startWordPosition="1013" endWordPosition="1016">Ugaritic. While the aim of the present work is more modest (discovering very basic phonetic properties of letters) it is also more widely applicable, as we don’t required detailed analysis of a known related language. Other recent work has employed a similar perspective for tying learning across languages. Naseem et al. (2009) use a non-parametric Bayesian model over parallel text to jointly learn part-of-speech taggers across 8 languages, while Cohen and Smith (2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and BouchardCôté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages. In our own previous work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012)1. In the latter work we also tackled the problem of unsupervised phonemic prediction for unknown languages by using textual regularities of know</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylogenetic grammar induction. In Proceedings of the ACL, pages 1288–1297. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Côté</author>
<author>David Hall</author>
<author>Thomas L Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>Automated reconstruction of ancient languages using probabilistic models of sound change.</title>
<date>2013</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>110</volume>
<issue>11</issue>
<marker>Bouchard-Côté, Hall, Griffiths, Klein, 2013</marker>
<rawString>Alexandre Bouchard-Côté, David Hall, Thomas L Griffiths, and Dan Klein. 2013. Automated reconstruction of ancient languages using probabilistic models of sound change. Proceedings of the National Academy of Sciences, 110(11):4224–4229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>A Bayesian mixture model for partof-speech induction using multiple features.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>638--647</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7848" citStr="Christodoulopoulos et al., 2011" startWordPosition="1221" endWordPosition="1224"> that the target language was written in a known (Latin) alphabet, greatly reducing the difficulty of the prediction task. In our present case, we assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts. We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel). Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010). 3 Model Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1We note that similar ideas were simultaneously proposed by other researchers (Cohen et al., 2011). 1528 Figure 1: Graphical representation of our model. We have K language clusters, L languages, and V words in each language. presented in Figure 1 and Algorithms 1, 2, and 3. We present a running commentary on the generative process from the bottom up, starting with A</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2011</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2011. A Bayesian mixture model for partof-speech induction using multiple features. In Proceedings of EMNLP, pages 638–647. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>74--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6423" citStr="Cohen and Smith (2009)" startWordPosition="987" endWordPosition="990">l of Snyder et al. (2010), which used knowledge of a related language (Hebrew) in an elaborate Bayesian framework to decipher the ancient language of Ugaritic. While the aim of the present work is more modest (discovering very basic phonetic properties of letters) it is also more widely applicable, as we don’t required detailed analysis of a known related language. Other recent work has employed a similar perspective for tying learning across languages. Naseem et al. (2009) use a non-parametric Bayesian model over parallel text to jointly learn part-of-speech taggers across 8 languages, while Cohen and Smith (2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and BouchardCôté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages. In our own previous work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim a</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B Cohen and Noah A Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proceedings of NAACL, pages 74– 82. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Unsupervised structure prediction with non-parallel multilingual guidance.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>50--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8177" citStr="Cohen et al., 2011" startWordPosition="1275" endWordPosition="1278">s of our model to some ideas proposed in other contexts. We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel). Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010). 3 Model Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1We note that similar ideas were simultaneously proposed by other researchers (Cohen et al., 2011). 1528 Figure 1: Graphical representation of our model. We have K language clusters, L languages, and V words in each language. presented in Figure 1 and Algorithms 1, 2, and 3. We present a running commentary on the generative process from the bottom up, starting with Algorithm 3. 3.1 Data Generation At the data generation stage (Algorithm 3), our model resembles an HMM. At each time step i, a tag tz is selected according to a language-specific transition distribution 0, indexed by the previous tag tz_1. We note that in practice, we implemented a trigram version of the mode1,2 but we present </context>
</contexts>
<marker>Cohen, Das, Smith, 2011</marker>
<rawString>Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011. Unsupervised structure prediction with non-parallel multilingual guidance. In Proceedings of EMNLP, pages 50–61. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>Pattern Analysis and Machine Intelligence, IEEE Transactions on,</journal>
<pages>6--721</pages>
<contexts>
<context position="16336" citStr="Geman and Geman, 1984" startWordPosition="2661" endWordPosition="2664">vel Poisson parameters λ from Equation 1 (and thus our sample space), as we can analytically integrate them out in our sampling equations. 4.2 Gibbs Sampling To sample values (tℓ, z, α, β) from their posterior (the integrand of Equation 1), we use Gibbs sampling, a Monte Carlo technique that constructs a Markov chain over a high-dimensional sample space by iteratively sampling each variable conditioned on the currently drawn sample values for the others, starting from a random initialization. The Markov chain converges to an equilibrium distribution which is in fact the desired joint density (Geman and Geman, 1984). We now sketch the sampling equations for each of our sampled variables. Sampling tw,ℓ To sample the tag assignment to character w in language ℓ, we need to compute: f (tw,ℓ |w, t−w,ℓ, t−ℓ, z, α, β) (3) a f (wℓ, tℓ, Nℓ |αk, βk, Nk−ℓ) (4) where Nℓ are the types-per-tag counts implied by the mapping tℓ, k is the current cluster assignment for the target language (zℓ = k), αk and βk are the cluster parameters, and Nk−ℓ are the types-per-tag counts for all languages currently assigned to the cluster, other than language ℓ. Applying the chain rule along with our model’s conditional independence st</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (6):721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Benjamin Snyder</author>
</authors>
<title>Universal grapheme-to-phoneme prediction over latin alphabets.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>332--343</pages>
<institution>Jeju Island, South Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="7039" citStr="Kim and Snyder, 2012" startWordPosition="1089" endWordPosition="1092">2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and BouchardCôté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages. In our own previous work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012)1. In the latter work we also tackled the problem of unsupervised phonemic prediction for unknown languages by using textual regularities of known languages. However, we assumed that the target language was written in a known (Latin) alphabet, greatly reducing the difficulty of the prediction task. In our present case, we assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts. We make the assumption t</context>
</contexts>
<marker>Kim, Snyder, 2012</marker>
<rawString>Young-Bum Kim and Benjamin Snyder. 2012. Universal grapheme-to-phoneme prediction over latin alphabets. In Proceedings of EMNLP, pages 332–343, Jeju Island, South Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>João V Graça</author>
<author>Benjamin Snyder</author>
</authors>
<title>Universal morphological analysis using structured nearest neighbor prediction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>322--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7016" citStr="Kim et al., 2011" startWordPosition="1085" endWordPosition="1088"> Cohen and Smith (2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and BouchardCôté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages. In our own previous work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012)1. In the latter work we also tackled the problem of unsupervised phonemic prediction for unknown languages by using textual regularities of known languages. However, we assumed that the target language was written in a known (Latin) alphabet, greatly reducing the difficulty of the prediction task. In our present case, we assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts. W</context>
</contexts>
<marker>Kim, Graça, Snyder, 2011</marker>
<rawString>Young-Bum Kim, João V Graça, and Benjamin Snyder. 2011. Universal morphological analysis using structured nearest neighbor prediction. In Proceedings of EMNLP, pages 322–332. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Anish Nair</author>
<author>Nishit Rathod</author>
<author>Kenji Yamada</author>
</authors>
<title>Unsupervised analysis for decipherment problems.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>499--506</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4403" citStr="Knight et al. (2006)" startWordPosition="660" endWordPosition="663">rillic, and Greek alphabets. In turn for each language, we consider it and its alphabet “unobserved” — we hide the graphic and phonetic properties of the symbols — while treating the vocabularies of the remaining languages as fully observed with phonetic tags on each of the letters. On average, over these 503 leave-one-languageout scenarios, our model predicts consonant/vowel distinctions with 99% accuracy. In the more challenging task of vowel/nasal/non-nasal prediction, our model achieves average accuracy over 89%. 2 Related Work The most direct precedent to the present work is a section in Knight et al. (2006) on universal phonetic decipherment. They build a trigram HMM with three hidden states, corresponding to consonants, vowels, and spaces. As in our model, individual characters are treated as the observed emissions of the hidden states. In contrast to the present work, they allow letters to be emitted by multiple states. Their experiments show that the HMM trained with EM successfully clusters Spanish letters into consonants and vowels. They further design a more sophisticated finite-state model, based on linguistic universals regarding syllable structure and sonority. Experiments with the seco</context>
<context position="24587" citStr="Knight et al. (2006)" startWordPosition="4016" endWordPosition="4019">14 Austronesian 67 Oto-Manguean 41 Indo-European 39 Mayan 34 Quechuan 17 Afro-Asiatic 17 Uto-Aztecan 16 Altaic 16 Trans-New Guinea 15 Nilo-Saharan 14 Sino-Tibetan 13 Tucanoan 9 Creole 8 Chibchan 6 Maipurean 5 Tupian 5 Nakh-Daghestanian 4 Uralic 4 Cariban 4 Totonacan 4 Mixe-Zoque 3 Jivaroan 3 Choco 3 Guajiboan 2 Huavean 2 Austro-Asiatic 2 Witotoan 2 Jean 2 Paezan 2 Other 30 Table 1: Language families in our data set. The Other category includes 9 language isolates and 21 language family singletons. 5.2 Baselines and Model Variants 6 Results As our baseline, we consider the trigram HMM model of Knight et al. (2006), trained with EM. In all experiments, we run 10 random restarts of EM, and pick the prediction with highest likelihood. We map the induced tags to the gold-standard tag categories (1-1 mapping) in the way that maximizes accuracy. We then consider three variants of our model. The simplest version, SYMM, disregards all information from other languages, using simple symmetric hyperparameters on the transition and emission Dirichlet priors (all hyperparameters set to 1). This allows us to assess the performance of The results of our experiments are shown in Table 2. In all cases, we report token-</context>
</contexts>
<marker>Knight, Nair, Rathod, Yamada, 2006</marker>
<rawString>Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. 2006. Unsupervised analysis for decipherment problems. In Proceedings of COLING/ACL, pages 499–506. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Simple type-level unsupervised POS tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>853--861</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7814" citStr="Lee et al., 2010" startWordPosition="1217" endWordPosition="1220">owever, we assumed that the target language was written in a known (Latin) alphabet, greatly reducing the difficulty of the prediction task. In our present case, we assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts. We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel). Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010). 3 Model Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1We note that similar ideas were simultaneously proposed by other researchers (Cohen et al., 2011). 1528 Figure 1: Graphical representation of our model. We have K language clusters, L languages, and V words in each language. presented in Figure 1 and Algorithms 1, 2, and 3. We present a running commentary on the generative process f</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2010</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-level unsupervised POS tagging. In Proceedings of EMNLP, pages 853–861. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Typebased MCMC.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>573--581</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7980" citStr="Liang et al., 2010" startWordPosition="1244" endWordPosition="1247">e assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts. We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel). Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010). 3 Model Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1We note that similar ideas were simultaneously proposed by other researchers (Cohen et al., 2011). 1528 Figure 1: Graphical representation of our model. We have K language clusters, L languages, and V words in each language. presented in Figure 1 and Algorithms 1, 2, and 3. We present a running commentary on the generative process from the bottom up, starting with Algorithm 3. 3.1 Data Generation At the data generation stage (Algorithm 3), our model resembles an HMM. At each time step i, a tag t</context>
</contexts>
<marker>Liang, Jordan, Klein, 2010</marker>
<rawString>Percy Liang, Michael I Jordan, and Dan Klein. 2010. Typebased MCMC. In Proceedings of NAACL, pages 573–581. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David JC MacKay</author>
</authors>
<title>Information Theory, Inference and Learning Algorithms.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<marker>MacKay, 2003</marker>
<rawString>David JC MacKay. 2003. Information Theory, Inference and Learning Algorithms. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Benjamin Snyder</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Multilingual part-of-speech tagging: Two unsupervised approaches.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="6279" citStr="Naseem et al. (2009)" startWordPosition="966" endWordPosition="969">es to guide the model’s decisions for the unknown script and language. In this sense, our model bears some similarity to the decipherment model of Snyder et al. (2010), which used knowledge of a related language (Hebrew) in an elaborate Bayesian framework to decipher the ancient language of Ugaritic. While the aim of the present work is more modest (discovering very basic phonetic properties of letters) it is also more widely applicable, as we don’t required detailed analysis of a known related language. Other recent work has employed a similar perspective for tying learning across languages. Naseem et al. (2009) use a non-parametric Bayesian model over parallel text to jointly learn part-of-speech taggers across 8 languages, while Cohen and Smith (2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and BouchardCôté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages. In our own previous work, we have developed the idea that supervised knowledge of some numb</context>
</contexts>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and Regina Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised approaches. Journal of Artificial Intelligence Research, 36(1):341–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling. Annals of statistics,</title>
<date>2003</date>
<pages>31--705</pages>
<contexts>
<context position="19184" citStr="Neal, 2003" startWordPosition="3130" endWordPosition="3131">omputing a product of Polya urn schemes (with many repeated ascending [n′(t)] 1531 factorials with the same base), we group common terms together and calculate: 11j=1(αk,t,t′ + k)n(j,k,t,t′) 11j=1(Et′′ αk,t,t′′ + k)n(j,k,t) where n(j, k, t) and n(j, k, t, t′) are the numbers of languages currently assigned to cluster k which have more than j occurrences of unigram (t) and bigram (t, t′), respectively. This gives us an efficient way to compute unnormalized posterior densities for α. However, we need to sample from these distributions, not just compute them. To do so, we turn to slice sampling (Neal, 2003), a simple yet effective auxiliary variable scheme for sampling values from unnormalized but otherwise computable densities. The key idea is to supplement the variable x, distributed according to unnormalized density ˜p(x), with a second variable u with joint density defined as p(x, u) ∝ ff(u &lt; ˜p(x)). It is easy to see that ˜p(x) ∝ ´ p(x, u)du. We then iteratively sample u|x and x|u, both of which are distributed uniformly across appropriately bounded intervals. Our implementation follows the pseudocode given in Mackay (2003). Sampling Ok,t To sample the Dirichlet hyperparameter for cluster k</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M Neal. 2003. Slice sampling. Annals of statistics, 31:705–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
<author>Kevin Knight</author>
</authors>
<title>A statistical model for lost language decipherment.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>1048--1057</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5826" citStr="Snyder et al. (2010)" startWordPosition="890" endWordPosition="893">not require an additional mapping step from uninterpreted hidden states to linguistic categories, as they do with the HMM. Our model and experiments can be viewed as complementary to the work of Knight et al., while also extending it to hundreds of languages. We use the simple HMM with EM as our baseline. In lieu of a linguistically designed model structure, we choose an empirical approach, allowing posterior inference over hundreds of known languages to guide the model’s decisions for the unknown script and language. In this sense, our model bears some similarity to the decipherment model of Snyder et al. (2010), which used knowledge of a related language (Hebrew) in an elaborate Bayesian framework to decipher the ancient language of Ugaritic. While the aim of the present work is more modest (discovering very basic phonetic properties of letters) it is also more widely applicable, as we don’t required detailed analysis of a known related language. Other recent work has employed a similar perspective for tying learning across languages. Naseem et al. (2009) use a non-parametric Bayesian model over parallel text to jointly learn part-of-speech taggers across 8 languages, while Cohen and Smith (2009) de</context>
</contexts>
<marker>Snyder, Barzilay, Knight, 2010</marker>
<rawString>Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A statistical model for lost language decipherment. In Proceedings of the ACL, pages 1048–1057. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>