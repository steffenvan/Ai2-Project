<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.976279">
Fast Semantic Extraction Using a Novel Neural Network Architecture
</title>
<author confidence="0.943752">
Ronan Collobert
</author>
<affiliation confidence="0.886195">
NEC Laboratories America, Inc.
</affiliation>
<address confidence="0.7997275">
4Independence Way
Suite 200, Princeton, NJ 08540
</address>
<email confidence="0.80717">
collober@nec-labs.com
</email>
<author confidence="0.879144">
Jason Weston
</author>
<affiliation confidence="0.830602">
NEC Laboratories America, Inc.
</affiliation>
<address confidence="0.7982605">
4Independence Way
Suite 200, Princeton, NJ 08540
</address>
<email confidence="0.901333">
jasonw@nec-labs.com
</email>
<sectionHeader confidence="0.997858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999732">
We describe a novel neural network archi-
tecture for the problem of semantic role la-
beling. Many current solutions are compli-
cated, consist of several stages and hand-
built features, and are too slow to be applied
as part of real applications that require such
semantic labels, partly because of their use
of a syntactic parser (Pradhan et al., 2004;
Gildea and Jurafsky, 2002). Our method in-
stead learns a direct mapping from source
sentence to semantic tags for a given pred-
icate without the aid of a parser or a chun-
ker. Our resulting system obtains accuracies
comparable to the current state-of-the-art at
a fraction of the computational cost.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990675">
Semantic understanding plays an important role in
many end-user applications involving text: for infor-
mation extraction, web-crawling systems, question
and answer based systems, as well as machine trans-
lation, summarization and search. Such applications
typically have to be computationally cheap to deal
with an enormous quantity of data, e.g. web-based
systems process large numbers of documents, whilst
interactive human-machine applications require al-
most instant response. Another issue is the cost of
producing labeled training data required for statisti-
cal models, which is exacerbated when those models
also depend on syntactic features which must them-
selves be learnt.
To achieve the goal of semantic understanding,
the current consensus is to divide and conquer the
</bodyText>
<page confidence="0.493807">
560
</page>
<figure confidence="0.815811">
[The company]ARG0 [bought]REL [sugar]ARG1 [on the world
market]ARGM-LOC [to meet export commitments]ARGM-PNC
</figure>
<figureCaption confidence="0.995391">
Figure 1: Example of Semantic Role Labeling from
</figureCaption>
<bodyText confidence="0.94861908">
the PropBank dataset (Palmer et al., 2005). ARG0
is typically an actor, REL an action, ARG1 an ob-
ject, and ARGM describe various modifiers such as
location (LOC) and purpose (PNC).
problem. Researchers tackle several layers of pro-
cessing tasks ranging from the syntactic, such as
part-of-speech labeling and parsing, to the semantic:
word-sense disambiguation, semantic role-labeling,
named entity extraction, co-reference resolution and
entailment. None of these tasks are end goals in
themselves but can be seen as layers of feature ex-
traction that can help in a language-based end ap-
plication, such as the ones described above. Un-
fortunately, the state-of-the-art solutions of many of
these tasks are simply too slow to be used in the ap-
plications previously described. For example, state-
of-the-art syntactic parsers theoretically have cubic
complexity in the sentence length (Younger, 1967)1
and several semantic extraction algorithms use the
parse tree as an initial feature.
In this work, we describe a novel type of neural
network architecture that could help to solve some
of these issues. We focus our experimental study on
the semantic role labeling problem (Palmer et al.,
2005): being able to give a semantic role to a syn-
</bodyText>
<footnote confidence="0.98278425">
1Even though some parsers effectively exhibit linear be-
havior in sentence length (Ratnaparkhi, 1997), fast statistical
parsers such as (Henderson, 2004) still take around 1.5 seconds
for sentences of length 35 in tests that we made.
</footnote>
<note confidence="0.944516">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560–567,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999342557522124">
tactic constituent of a sentence, i.e. annotating the annotated parses of sentences. The current version
predicate argument structure in text (see for exam- of the dataset gives semantic tags for the same sen-
ple Figure 1). Because of its nature, role labeling tences as in the Penn Treebank (Marcus et al., 1993),
seems to require the syntactic analysis of a sentence which are excerpts from the Wall Street Journal. The
before attributing semantic labels. Using this intu- central idea is that each verb in a sentence is la-
ition, state-of-the-art systems first build a parse tree, beled with its propositional arguments, where the
and syntactic constituents are then labeled by feed- abstract numbered arguments are intended to fill typ-
ing hand-built features extracted from the parse tree ical roles. For example, ARG0 is typically the actor,
to a machine learning system, e.g. the ASSERT sys- and ARG1 is typically the thing acted upon. The
tem (Pradhan et al., 2004). This is rather slow, tak- precise usage of the numbering system is labeled for
ing a few seconds per sentence at test time, partly each particular verb as so-called frames. Addition-
because of the parse tree component, and partly be- ally, semantic roles can also be labeled with one of
cause of the use of Support Vector Machines (Boser 13 ARGM adjunct labels, such as ARGM-LOC or
et al., 1992), which have linear complexity in test- ARGM-TMP for additional locational or temporal
ing time with respect to the number of training ex- information relative to some verb.
amples. This makes it hard to apply this method to Shallow semantic parsing has immediate applica-
interesting end user applications. tions in tasks such as meta-data extraction (e.g. from
Here, we propose a radically different approach web documents) and question and answer based sys-
that avoids the more complex task of building a full tems (e.g. call center systems), amongst others.
parse tree. From a machine learning point of view, a 3 Previous Work
human does not need to be taught about parse trees Several authors have already attempted to build ma-
to talk. It is possible, however, that our brains may chine learning approaches for the semantic role-
implicitly learn features highly correlated with those labeling problem. In (Gildea and Jurafsky, 2002)
extracted from a parse tree. We propose to develop the authors presented a statistical approach to learn-
an architecture that implements this kind of implicit ing (for FrameNet), with some success. They pro-
learning, rather than using explicitly engineered fea- posed to take advantage of the syntactic tree struc-
tures. In practice, our system also provides semantic ture that can be predicted by a parser, such as Char-
tags at a fraction of the computational cost of other niak’s parser (Charniak, 2000). Their aim is, given
methods, taking on average 0.02 seconds to label a a node in the parse tree, to assign a semantic role
sentence from the Penn Treebank, with almost no label to the words that are the children of that node.
loss in accuracy. They extract several key types of features from the
The rest of the article is as follows. First, we de- parse tree to be used in a statistical model for pre-
scribe the problem of shallow semantic parsing in diction. These same features also proved crucial to
more detail, as well as existing solutions to this prob- subsequent approaches, e.g. (Pradhan et al., 2004).
lem. We then detail our algorithmic approach – the These features include:
neural network architecture we employ – followed • The parts of speech and syntactic labels of
by experiments that evaluate our method. Finally, words and nodes in the tree.
we conclude with a summary and discussion of fu- • The node’s position (left or right) in relation to
ture work. the verb.
2 Shallow Semantic Parsing • The syntactic path to the verb in the parse tree.
FrameNet (Baker et al., 1998) and the Proposition • Whether a node in the parse tree is part of a
Bank (Palmer et al., 2005), or PropBank for short, noun or verb phrase (by looking at the parent
are the two main systems currently developed for nodes of that node).
semantic role-labeling annotation. We focus here
on PropBank. PropBank encodes role labels by se-
mantically tagging the syntactic structures of hand
561
• The voice of the sentence: active or passive parse tree information from the semantic role label-
(part of the PropBank gold annotation); ing system, in fact the shared task of CONLL 2004
as well as several other features (predicate, head was devoted to this goal, but the results were not
word, verb sub-categorization, ... ). completely satisfactory. Previously, in (Gildea and
The authors of (Pradhan et al., 2004) used a Palmer, 2001), the authors tried to show that the
similar structure, but added more features, notably parse tree is necessary for good generalization by
head word part-of-speech, the predicted named en- showing that segments derived from a shallow syn-
tity class of the argument, word sense disambigua- tactic parser or chunker do not perform as well for
tion of the verb and verb clustering, and others (they this goal. A further analysis of using chunkers, with
add 25 variants of 12 new feature types overall.) improved results was also given in (Punyakanok et
Their system also uses a parser, as before, and then a al., 2005), but still concluded the full parse tree is
polynomial Support Vector Machine (SVM) (Boser most useful.
et al., 1992) is used in two further stages: to clas- 4 Neural Network Architecture
sify each node in the tree as being a semantic ar- Ideally, we want an end-to-end fast learning system
gument or not for a given verb; and then to clas- to output semantic roles for syntactic constituents
sify each semantic argument into one of the classes without using a time consuming parse tree.
(ARG1, ARG2, etc.). The first SVM solves a two- Also, as explained before, we are interesting in
class problem, the second solves a multi-class prob- exploring whether machine learning approaches can
lem using a one-vs-the-rest approach. The final sys- learn structure implicitly. Hence, even if there is a
tem, called ASSERT, gives state-of-the-art perfor- deep relationship between syntax and semantics, we
mance and is also freely available at: http:// prefer to avoid hand-engineered features that exploit
oak.colorado.edu/assert/. We compare this, and see if we can develop a model that can learn
to this system in our experimental results in Sec- these features instead. We are thus not interested
tion 5. Several other competing methods exist, e.g. in chunker-based techniques, even though they are
the ones that participated in the CONLL 2004 and faster than parser-based techniques.
2005 challenges (http://www.lsi.upc.edu/ We propose here a neural network based architec-
˜srlconll/st05/st05.html). In this paper ture which achieves these two goals.
we focus on a comparison with ASSERT because 4.1 Basic Architecture
software to re-run it is available online. This also The type of neural network that we employ is a Multi
gives us a timing result for comparison purposes. Layer Perceptron (MLP). MLPs have been used for
The three-step procedure used in ASSERT (calcu- many years in the machine learning field and slowly
lating a parse tree and then applying SVMs twice) abandoned for several reasons: partly because of
leads to good classification performance, but has the difficulty of solving the non-convex optimization
several drawbacks. First in speed: predicting a problems associated with learning (LeCun et al.,
parse tree is extremely demanding in computing re- 1998), and partly because of the difficulty of their
sources. Secondly, choosing the features necessary theoretical analysis compared to alternative convex
for SVM classification requires extensive research. approaches.
Finally, the SVM classification algorithm used in ex- An MLP works by successively projecting the
isting approaches is rather slow: SVM training is at data to be classified into different spaces. These
least quadratic in time with respect to the number projections are done in what is called hidden lay-
of training examples. The number of support vec- ers. Given an input vector z, a hidden layer applies
tors involved in the SVM decision function also in- a linear transformation (matrix M) followed by a
creases linearly with the number of training exam- squashing function h:
ples. This makes SVMs slow on large-scale prob- z H Mz H h(Mz) . (1)
lems, both during training and testing phases.
To alleviate the burden of parse tree computation,
several attempts have been made to remove the full
562
A typical squashing function is the hyperbolic tan-
gent h(·) = tanh(·). The last layer (the output
layer) linearly separates the classes. The composi-
tion of the projections in the hidden layers could be
viewed as the work done by the kernel in SVMs.
However there is a very important difference: the
kernel in SVM is fixed and arbitrarily chosen, while
the hidden layers in an MLP are trained and adapted
to the classification task. This allows us to create
much more flexible classification architectures.
Our method for semantic role labeling classifies
each word of a sentence separately. We do not use
any semantic constituent information: if the model
is powerful enough, words in the same semantic
constituent should have the same class label. This
means we also do not separate the problem into
an identification and classification phase, but rather
solve in a single step.
</bodyText>
<subsubsectionHeader confidence="0.650235">
4.1.1 Notation
</subsubsectionHeader>
<bodyText confidence="0.99674">
We represent words as indices. We consider a fi-
nite dictionary of words D C N. Let us represent a
sentence of nw words to be analyzed as a function
s(·). The ith word in the sentence is given by the
index s(i):
</bodyText>
<equation confidence="0.831881">
1 &lt; i &lt; nw s(i)ED.
</equation>
<bodyText confidence="0.9998598">
We are interested in predicting the semantic role la-
bel of the word at position posw, given a verb at po-
sition posv (1 &lt; posw, posv &lt; nw). A mathemati-
cal description of our network architecture schemat-
ically shown in Figure 2 follows.
</bodyText>
<subsubsectionHeader confidence="0.505832">
4.1.2 Transforming words into feature vectors
</subsubsectionHeader>
<bodyText confidence="0.999908071428572">
Our first concern in semantic role labeling is that
we have to deal with words, and that a simple in-
dex i E D does not carry any information specific
to a word: for each word we need a set of features
relevant for the task. As described earlier, previous
methods construct a parse tree, and then compute
hand-built features which are then fed to a classi-
fication algorithm. In order to bypass the use of a
parse tree, we convert each word i E D into a par-
ticular vector wi E Rd which is going to be learnt
for the task we are interested in. This approach has
already been used with great success in the domain
of language models (Bengio and Ducharme, 2001;
Schwenk and Gauvain, 2002).
</bodyText>
<figure confidence="0.889577333333333">
Tanh Squashing Layer
Classical Linear Layer
Softmax Squashing Layer
• ••• • • •
ARG1 ARG2 ARGM ARGM
LOC TMP
</figure>
<figureCaption confidence="0.991048">
Figure 2: MLP architecture for shallow semantic
</figureCaption>
<bodyText confidence="0.908735">
parsing. The input sequence is at the top. The out-
put class probabilities for the word of interest (“cat”)
given the verb of interest (“sat”) are given at the bot-
tom.
The first layer of our MLP is thus a lookup table
which replaces the word indices into a concatenation
of vectors:
</bodyText>
<equation confidence="0.990375">
{s(1), ... , s(nw)}
H (ws(1) ... ws(n&amp;quot;)) E Rn&amp;quot; d . (2)
</equation>
<bodyText confidence="0.99992275">
The weights {wi  |i E D} for this layer are consid-
ered during the backpropagation phase of the MLP,
and thus adapted automatically for the task we are
interested in.
</bodyText>
<subsectionHeader confidence="0.726336">
4.1.3 Integrating the verb position
</subsectionHeader>
<bodyText confidence="0.9930685">
Feeding word vectors alone to a linear classifica-
tion layer as in (Bengio and Ducharme, 2001) leads
</bodyText>
<figure confidence="0.985732789473684">
ws( 1 ) ws(2)
...
ws(6)
Input Sentence
the cat on the mat
sat
s(1) s(2) ... s(6)
Lookup Table d
C(position w.r.t. cat, position w.r.t. sat)
Classical Linear Layer
C1 C2 ... C6
ws(6)
d
Linear Layer with sentence−adapted columns
Ci
ws( 1)
ws(2)
d
nhu
</figure>
<page confidence="0.995061">
563
</page>
<bodyText confidence="0.999727714285714">
to very poor accuracy because the semantic classifi-
cation of a given word also depends on the verb in
question. We need to provide the MLP with infor-
mation about the verb position within the sentence.
For that purpose we use a kind of linear layer which
is adapted to the sentence considered. It takes the
form:
</bodyText>
<equation confidence="0.726748333333333">
�
�
(ws(1) ... ws(nw)) &apos; M �
</equation>
<bodyText confidence="0.99631925">
where M E Rnhuxnwd, and nhu is the number of
hidden units. The specific nature of this layer is
that the matrix M has a special block-column form
which depends on the sentence:
</bodyText>
<equation confidence="0.999356">
M = (C1 |... |Cnw) ,
</equation>
<bodyText confidence="0.99996125">
where each column Ci E Rnhuxd depends on the
position of the ith word in s(·), with respect to the
position posw of the word of interest, and with re-
spect to the position posv of the verb of interest:
</bodyText>
<equation confidence="0.90485">
Ci = C(i − posw, i − posv) ,
</equation>
<bodyText confidence="0.999822052631579">
where C(·, ·) is a function to be chosen.
In our experiments C(·, ·) was a linear layer with
discretized inputs (i − posw, i − posv) which were
transformed into two binary vectors of size wsz,
where a bit is set to 1 if it corresponds to the po-
sition to encode, and 0 otherwise. These two binary
vectors are then concatenated and fed to the linear
layer. We chose the “window size” wsz = 11. If
a position lies outside the window, then we still set
the leftmost or rightmost bit to 1. The parameters in-
volved in this function are also considered during the
backpropagation. With such an architecture we al-
low our MLP to automatically adapt the importance
of a word in the sentence given its distance to the
word we want to classify, and to the verb we are in-
terested in.
This idea is the major novelty in this work, and is
crucial for the success of the entire architecture, as
we will see in the experiments.
</bodyText>
<subsectionHeader confidence="0.729528">
4.1.4 Learning class probabilities
</subsectionHeader>
<bodyText confidence="0.957625666666667">
The last layer in our MLP is a classical linear
layer as described in (1), with a softmax squashing
function (Bridle, 1990). Considering (1) and given
</bodyText>
<equation confidence="0.6579255">
z� = Mz, we have
hi(�z) = exp zi
</equation>
<bodyText confidence="0.99988575">
This allows us to interpret outputs as probabilities
for each semantic role label. The training of the
whole system is achieved using a normal stochastic
gradient descent.
</bodyText>
<subsectionHeader confidence="0.993921">
4.2 Word representation
</subsectionHeader>
<bodyText confidence="0.999965933333333">
As we have seen, in our model we are learning one
d dimensional vector to represent each word. If the
dataset were large enough, this would be an elegant
solution. In practice many words occur infrequently
within PropBank, so (independent of the size of d)
we can still only learn a very poor representation for
words that only appear a few times. Hence, to con-
trol the capacity of our model we take the original
word and replace it with its part-of-speech if it is
a verb, noun, adjective, adverb or number as deter-
mined by a part-of-speech classifier, and keep the
words for all other parts of speech. This classifier is
itself a neural network. This way we keep linking
words which are important for this task. We do not
do this replacement for the predicate itself.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="introduction">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999951625">
We used Sections 02-21 of the PropBank dataset
version 1 for training and validation and Section
23 for testing as standard in all our experiments.
We first describe the part-of-speech tagger we em-
ploy, and then describe our semantic role labeling
experiments. Software for our method, SENNA (Se-
mantic Extraction using a Neural Network Archi-
tecture), more details on its implementation, an on-
line applet and test set predictions of our system
in comparison to ASSERT can be found at http:
//ml.nec-labs.com/software/senna.
Part-Of-Speech Tagger The part-of-speech clas-
sifier we employ is a neural network architecture of
the same type as in Section 4, where the function
Ci = C(i − posw) depends now only on the word
position, and not on a verb. More precisely:
</bodyText>
<equation confidence="0.45969">
Ci r 0 if 2|i − posw |&gt; wsz − 1
= Sl
Wi−posw otherwise,
</equation>
<figure confidence="0.987313714285714">
wT ) ,
s(1)
...
T
ws(nw)
E.
j exp zj
</figure>
<page confidence="0.996555">
564
</page>
<bodyText confidence="0.999416421052632">
where Wk E Rlhuxd and wsz is a window size.
We chose wsz = 5 in our experiments. The
d-dimensional vectors learnt take into account the
capitalization of a word, and the prefix and suf-
fix calculated using Porter-Stemmer. See http:
//ml.nec-labs.com/software/senna for
more details. We trained on the training set of Prop-
Bank supplemented with the Brown corpus, result-
ing in a test accuracy on the test set of PropBank of
96.85% which compares to 96.66% using the Brill
tagger (Brill, 1992).
Semantic Role Labeling In our experiments we
considered a 23-class problem of NULL (no la-
bel), the core arguments ARG0-5, REL, ARGA, and
ARGM- along with the 13 secondary modifier labels
such as ARGM-LOC and ARGM-TMP. We simpli-
fied R-ARGn and C-ARGn to be written as ARGn,
and post-processed ASSERT to do this as well.
We compared our system to the freely available
ASSERT system (Pradhan et al., 2004). Both sys-
tems are fed only the input sentence during testing,
with traces removed, so they cannot make use of
many PropBank features such as frameset identiti-
fier, person, tense, aspect, voice, and form of the
verb. As our algorithm outputs a semantic tag for
each word of a sentence, we directly compare this
per-word accuracy with ASSERT. Because ASSERT
uses a parser, and because PropBank was built by la-
beling the nodes of a hand-annotated parse tree, per-
node accuracy is usually reported in papers such as
(Pradhan et al., 2004). Unfortunately our approach
is based on a completely different premise: we tag
words, not syntactic constituents coming from the
parser. We discuss this further in Section 5.2.
The per-word accuracy comparison results can be
seen in Table 5. Before labeling the semantic roles
of each predicate, one must first identify the pred-
icates themselves. If a predicate is not identified,
NULL tags are assigned to each word for that pred-
icate. The first line of results in the table takes into
account this identification process. For the neural
network, we used our part-of-speech tagger to per-
form this as a verb-detection task.
We noticed ASSERT failed to identify relatively
many predicates. In particular, it seems predicates
such as “is” are sometimes labeled as AUX by
the part-of-speech tagger, and subsequently ignored.
We informed the authors of this, but we did not re-
ceive a response. To deal with this, we considered
the additional accuracy (second row in the table)
measured over only those sentences where the pred-
icate was identified by ASSERT.
Timing results The per-sentence compute time is
also given in Table 5, averaged over all sentences in
the test set. Our method is around 250 times faster
than ASSERT. It is not really feasible to run AS-
SERT for most applications.
</bodyText>
<table confidence="0.999258571428571">
Measurement NN ASSERT
Per-word accuracy 83.64% 83.46%
(all verbs)
Per-word accuracy 84.09% 86.06%
(ASSERT verbs)
Per-sentence 0.02 secs 5.08 secs
compute time (secs)
</table>
<tableCaption confidence="0.999895">
Table 1: Experimental comparison with ASSERT
</tableCaption>
<subsectionHeader confidence="0.999031">
5.1 Analysis of our MLP
</subsectionHeader>
<bodyText confidence="0.99939175">
While we gave an intuitive justification of the archi-
tecture choices of our model in Section 4, we now
give a systematic empirical study of those choices.
First of all, providing the position of the word and
the predicate in function Q·, ·) is essential: the best
model we obtained with a window around the word
only gave 51.3%, assuming correct identification of
all predicates. Our best model achieves 83.95% in
this setting.
If we do not cluster the words according to their
part-of-speech, we also lose some performance, ob-
taining 78.6% at best. On the other hand, clustering
all words (such as CC, DT, IN part-of-speech tags)
also gives weaker results (81.1% accuracy at best).
We believe that including all words would give very
good performance if the dataset was large enough,
but training only on PropBank leads to overfitting,
many words being infrequent. Clustering is a way
to fight against overfitting, by grouping infrequent
words: for example, words with the label NNP, JJ,
RB (which we cluster) appear on average 23, 22 and
72 times respectively in the training set, while CC,
DT, IN (which we do not cluster) appear 2420, 5659
and 1712 times respectively.
</bodyText>
<page confidence="0.996043">
565
</page>
<bodyText confidence="0.999487888888889">
Even though some verbs are infrequent, one can-
not cluster all verbs into a single group, as each verb
dictates the types of semantic roles in the sentence,
depending on its frame. Clustering all words into
their part-of-speech, including the predicate, gives
a poor 73.8% compared with 81.1%, where every-
thing is clustered apart from the predicate.
Figure 3 gives some anecdotal examples of test set
predictions of our final model compared to ASSERT.
</bodyText>
<subsectionHeader confidence="0.99934">
5.2 Argument Classification Accuracy
</subsectionHeader>
<bodyText confidence="0.999986666666667">
So far we have not used the same accuracy measures
as in previous work (Gildea and Jurafsky, 2002;
Pradhan et al., 2004). Currently our architecture is
designed to label on a per-word basis, while existing
systems perform a segmentation process, and then
label segments. While we do not optimize our model
for the same criteria, it is still possible to measure the
accuracy using the same metrics. We measured the
argument classification accuracy of our network, as-
suming the correct segmentation is given to our sys-
tem, as in (Pradhan et al., 2004), by post-processing
our per-word tags to form a majority vote over each
segment. This gives 83.18% accuracy for our net-
work when we suppose the predicate must also be
identified, and 80.53% for the ASSERT software.
Measuring only on predicates identified by ASSERT
we instead obtain 84.32% accuracy for our network,
and 87.02% for ASSERT.
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="acknowledgments">
6 Discussion
</sectionHeader>
<bodyText confidence="0.874951656716418">
We have introduced a neural network architecture
that can provide computationally efficient semantic
role tagging. It is also a general architecture that
could be applied to other problems as well. Because
our network currently outputs labels on a per-word
basis it is difficult to assess existing accuracy mea-
sures. However, it should be possible to combine
our approach with a shallow parser to enhance per-
formance, and make comparisons more direct.
We consider this work as a starting point for dif-
ferent research directions, including the following
areas:
• Incorporating hand-built features Currently,
the only prior knowledge our system encodes
comes from part-of-speech tags, in stark con-
trast to other methods. Of course, performance
TRUTH: He camped out at a high-tech nerve center
on the floor of [the Big Board, where]ARGM-LOC [he]ARG0
[could]ARGM-MOD [watch]REL [updates on prices and pend-
ing stock orders]ARG1.
ASSERT (68.7%): He camped out at a high-tech nerve
center on the floor of the Big Board, [ where]ARGM-LOC
[he]ARG0 [could]ARGM-MOD [watch]REL [updates]ARG1 on
prices and pending stock orders.
NN (100%): He camped out at a high-tech nerve center
on the floor of [the Big Board, where]ARGM-LOC [he]ARG0
[could]ARGM-MOD [watch]REL [updates on prices and pend-
ing stock orders]ARG1.
TRUTH: [United Auto Workers Local 1069, which]ARG0
[represents]REL [3,000 workers at Boeing’s helicopter unit
in Delaware County, Pa.]ARG1 , said it agreed to extend its
contract on a day-by-day basis, with a 10-day notification
to cancel, while it continues bargaining.
ASSERT (100%): [United Auto Workers Local 1069,
which]ARG0 [represents]REL [3,000 workers at Boeing’s
helicopter unit in Delaware County, Pa.]ARG1 , said it agreed
to extend its contract on a day-by-day basis, with a 10-day
notification to cancel, while it continues bargaining.
NN (89.1%): [United Auto Workers Local 1069,
which]ARG0 [represents]REL [3,000 workers at Boeing’s
helicopter unit]ARG1 [ in Delaware County]ARGM-LOC , Pa. ,
said it agreed to extend its contract on a day-by-day basis,
with a 10-day notification to cancel, while it continues
bargaining.
Figure 3: Two examples from the PropBank test set,
showing Neural Net and ASSERT and gold standard
labelings, with per-word accuracy in brackets. Note
that even though our labeling does not match the
hand-annotated one in the second sentence it still
seems to make some sense as “in Delaware County”
is labeled as a location modifier. The complete set
of predictions on the test set can be found at http:
//ml.nec-labs.com/software/senna.
would improve with more hand-built features.
For example, simply adding whether each word
is part of a noun or verb phrase using the hand-
annotated parse tree (the so-called “GOV” fea-
ture from (Gildea and Jurafsky, 2002)) im-
proves the performance of our system from
83.95% to 85.8%. One must trade the gener-
ality of the model with its specificity, and also
take into account how long the features take to
compute.
• Incorporating segment information Our system
has no prior knowledge about segmentation in
text. This could be encoded in many ways:
most obviously by using a chunker, but also by
</bodyText>
<page confidence="0.993962">
566
</page>
<bodyText confidence="0.97817048">
designing a different network architecture, e.g.
by encoding contiguity constraints. To show
the latter is useful, using hand-annotated seg-
ments to force contiguity by majority vote leads
to an improvement from 83.95% to 85.6%.
• Incorporating known invariances via virtual
training data. In image recognition problems
it is common to create artificial training data by
taking into account invariances in the images,
e.g. via rotation and scale. Such data improves
generalization substantially. It may be possible
to achieve similar results for text, by “warp-
ing” training data to create new sentences, or
by constructing sentences from scratch using a
hand-built grammar.
• Unlabeled data. Our representation of words
is as d dimensional vectors. We could try to
improve this representation by learning a lan-
guage model from unlabeled data (Bengio and
Ducharme, 2001). As many words in Prop-
Bank only appear a few times, the representa-
tion might improve, even though the learning is
unsupervised. This may also make the system
generalize better to types of data other than the
Wall Street Journal.
</bodyText>
<listItem confidence="0.50879775">
• Transductive Inference. Finally, one can also
use unlabeled data as part of the supervised
training process, which is called transduction
or semi-supervised learning.
</listItem>
<bodyText confidence="0.9999392">
In particular, we find the possibility of using un-
labeled data, invariances and the use of transduc-
tion exciting. These possibilities naturally fit into
our framework, whereas scalability issues will limit
their application in competing methods.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999638983050848">
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The
Berkeley FrameNet project. Proceedings of COLING-
ACL, 98.
Y. Bengio and R. Ducharme. 2001. A neural probabilis-
tic language model. In Advances in Neural Informa-
tion Processing Systems, NIPS 13.
B.E. Boser, I.M. Guyon, and V.N. Vapnik. 1992. A train-
ing algorithm for optimal margin classifiers. Proceed-
ings of the fifth annual workshop on Computational
learning theory, pages 144–152.
J.S. Bridle. 1990. Probabilistic interpretation of feed-
forward classification network outputs, with relation-
ships to statistical pattern recognition. In F. Fogelman
Souli´e and J. H´erault, editors, Neurocomputing: Al-
gorithms, Architectures and Applications, pages 227–
236. NATO ASI Series.
E. Brill. 1992. A simple rule-based part of speech tag-
ger. Proceedings of the Third Conference on Applied
Natural Language Processing, pages 152–155.
E. Charniak. 2000. A maximum-entropy-inspired parser.
Proceedings of the first conference on North American
chapter of the Association for Computational Linguis-
tics, pages 132–139.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245–288.
D. Gildea and M. Palmer. 2001. The necessity of pars-
ing for predicate argument recognition. Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 239–246.
J. Henderson. 2004. Discriminative training of a neural
network statistical parser. In Proceedings of the 42nd
Meeting ofAssociation for Computational Linguistics.
Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. 1998.
Efficient backprop. In G.B. Orr and K.-R. M¨uller, ed-
itors, Neural Networks: Tricks of the Trade, pages 9–
50. Springer.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the penn treebank. Computational Linguistics,
19(2):313–330.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Comput. Linguist., 31(1):71–106.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Ju-
rafsky. 2004. Shallow semantic parsing using support
vector machines. Proceedings of HLT/NAACL-2004.
V. Punyakanok, D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role labeling.
Proceedings of IJCAI’05, pages 1117–1123.
A. Ratnaparkhi. 1997. A linear observed time statistical
parser based on maximum entropy models. Proceed-
ings of EMNLP.
H. Schwenk and J.L. Gauvain. 2002. Connection-
ist language modeling for large vocabulary continu-
ousspeech recognition. Proceedings of ICASSP’02.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10.
</reference>
<page confidence="0.997334">
567
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929715">
<title confidence="0.999891">Fast Semantic Extraction Using a Novel Neural Network Architecture</title>
<author confidence="0.996501">Ronan Collobert</author>
<affiliation confidence="0.999919">NEC Laboratories America, Inc.</affiliation>
<address confidence="0.995388">4Independence Way Suite 200, Princeton, NJ 08540</address>
<email confidence="0.999555">collober@nec-labs.com</email>
<author confidence="0.999886">Jason Weston</author>
<affiliation confidence="0.999783">NEC Laboratories America, Inc.</affiliation>
<address confidence="0.99592">4Independence Way Suite 200, Princeton, NJ 08540</address>
<email confidence="0.999801">jasonw@nec-labs.com</email>
<abstract confidence="0.9967888125">We describe a novel neural network archifor the problem of role la- Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky, 2002). Our method instead learns a direct mapping from source sentence to semantic tags for a given predwithout the aid of a parser chunker. Our resulting system obtains accuracies comparable to the current state-of-the-art at a fraction of the computational cost.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>Proceedings of COLINGACL,</booktitle>
<pages>98</pages>
<contexts>
<context position="7455" citStr="Baker et al., 1998" startWordPosition="1191" endWordPosition="1194">atures also proved crucial to more detail, as well as existing solutions to this prob- subsequent approaches, e.g. (Pradhan et al., 2004). lem. We then detail our algorithmic approach – the These features include: neural network architecture we employ – followed • The parts of speech and syntactic labels of by experiments that evaluate our method. Finally, words and nodes in the tree. we conclude with a summary and discussion of fu- • The node’s position (left or right) in relation to ture work. the verb. 2 Shallow Semantic Parsing • The syntactic path to the verb in the parse tree. FrameNet (Baker et al., 1998) and the Proposition • Whether a node in the parse tree is part of a Bank (Palmer et al., 2005), or PropBank for short, noun or verb phrase (by looking at the parent are the two main systems currently developed for nodes of that node). semantic role-labeling annotation. We focus here on PropBank. PropBank encodes role labels by semantically tagging the syntactic structures of hand 561 • The voice of the sentence: active or passive parse tree information from the semantic role label(part of the PropBank gold annotation); ing system, in fact the shared task of CONLL 2004 as well as several other</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998. The Berkeley FrameNet project. Proceedings of COLINGACL, 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems, NIPS 13.</booktitle>
<contexts>
<context position="14247" citStr="Bengio and Ducharme, 2001" startWordPosition="2328" endWordPosition="2331">role labeling is that we have to deal with words, and that a simple index i E D does not carry any information specific to a word: for each word we need a set of features relevant for the task. As described earlier, previous methods construct a parse tree, and then compute hand-built features which are then fed to a classification algorithm. In order to bypass the use of a parse tree, we convert each word i E D into a particular vector wi E Rd which is going to be learnt for the task we are interested in. This approach has already been used with great success in the domain of language models (Bengio and Ducharme, 2001; Schwenk and Gauvain, 2002). Tanh Squashing Layer Classical Linear Layer Softmax Squashing Layer • ••• • • • ARG1 ARG2 ARGM ARGM LOC TMP Figure 2: MLP architecture for shallow semantic parsing. The input sequence is at the top. The output class probabilities for the word of interest (“cat”) given the verb of interest (“sat”) are given at the bottom. The first layer of our MLP is thus a lookup table which replaces the word indices into a concatenation of vectors: {s(1), ... , s(nw)} H (ws(1) ... ws(n&amp;quot;)) E Rn&amp;quot; d . (2) The weights {wi |i E D} for this layer are considered during the backpropagat</context>
<context position="28620" citStr="Bengio and Ducharme, 2001" startWordPosition="4760" endWordPosition="4763">g known invariances via virtual training data. In image recognition problems it is common to create artificial training data by taking into account invariances in the images, e.g. via rotation and scale. Such data improves generalization substantially. It may be possible to achieve similar results for text, by “warping” training data to create new sentences, or by constructing sentences from scratch using a hand-built grammar. • Unlabeled data. Our representation of words is as d dimensional vectors. We could try to improve this representation by learning a language model from unlabeled data (Bengio and Ducharme, 2001). As many words in PropBank only appear a few times, the representation might improve, even though the learning is unsupervised. This may also make the system generalize better to types of data other than the Wall Street Journal. • Transductive Inference. Finally, one can also use unlabeled data as part of the supervised training process, which is called transduction or semi-supervised learning. In particular, we find the possibility of using unlabeled data, invariances and the use of transduction exciting. These possibilities naturally fit into our framework, whereas scalability issues will l</context>
</contexts>
<marker>Bengio, Ducharme, 2001</marker>
<rawString>Y. Bengio and R. Ducharme. 2001. A neural probabilistic language model. In Advances in Neural Information Processing Systems, NIPS 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B E Boser</author>
<author>I M Guyon</author>
<author>V N Vapnik</author>
</authors>
<title>A training algorithm for optimal margin classifiers.</title>
<date>1992</date>
<booktitle>Proceedings of the fifth annual workshop on Computational learning theory,</booktitle>
<pages>144--152</pages>
<marker>Boser, Guyon, Vapnik, 1992</marker>
<rawString>B.E. Boser, I.M. Guyon, and V.N. Vapnik. 1992. A training algorithm for optimal margin classifiers. Proceedings of the fifth annual workshop on Computational learning theory, pages 144–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Bridle</author>
</authors>
<title>Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In</title>
<date>1990</date>
<booktitle>Neurocomputing: Algorithms, Architectures and Applications,</booktitle>
<pages>227--236</pages>
<editor>F. Fogelman Souli´e and J. H´erault, editors,</editor>
<publisher>NATO ASI Series.</publisher>
<contexts>
<context position="17161" citStr="Bridle, 1990" startWordPosition="2878" endWordPosition="2879">ftmost or rightmost bit to 1. The parameters involved in this function are also considered during the backpropagation. With such an architecture we allow our MLP to automatically adapt the importance of a word in the sentence given its distance to the word we want to classify, and to the verb we are interested in. This idea is the major novelty in this work, and is crucial for the success of the entire architecture, as we will see in the experiments. 4.1.4 Learning class probabilities The last layer in our MLP is a classical linear layer as described in (1), with a softmax squashing function (Bridle, 1990). Considering (1) and given z� = Mz, we have hi(�z) = exp zi This allows us to interpret outputs as probabilities for each semantic role label. The training of the whole system is achieved using a normal stochastic gradient descent. 4.2 Word representation As we have seen, in our model we are learning one d dimensional vector to represent each word. If the dataset were large enough, this would be an elegant solution. In practice many words occur infrequently within PropBank, so (independent of the size of d) we can still only learn a very poor representation for words that only appear a few ti</context>
</contexts>
<marker>Bridle, 1990</marker>
<rawString>J.S. Bridle. 1990. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In F. Fogelman Souli´e and J. H´erault, editors, Neurocomputing: Algorithms, Architectures and Applications, pages 227– 236. NATO ASI Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<contexts>
<context position="19550" citStr="Brill, 1992" startWordPosition="3296" endWordPosition="3297">n a verb. More precisely: Ci r 0 if 2|i − posw |&gt; wsz − 1 = Sl Wi−posw otherwise, wT ) , s(1) ... T ws(nw) E. j exp zj 564 where Wk E Rlhuxd and wsz is a window size. We chose wsz = 5 in our experiments. The d-dimensional vectors learnt take into account the capitalization of a word, and the prefix and suffix calculated using Porter-Stemmer. See http: //ml.nec-labs.com/software/senna for more details. We trained on the training set of PropBank supplemented with the Brown corpus, resulting in a test accuracy on the test set of PropBank of 96.85% which compares to 96.66% using the Brill tagger (Brill, 1992). Semantic Role Labeling In our experiments we considered a 23-class problem of NULL (no label), the core arguments ARG0-5, REL, ARGA, and ARGM- along with the 13 secondary modifier labels such as ARGM-LOC and ARGM-TMP. We simplified R-ARGn and C-ARGn to be written as ARGn, and post-processed ASSERT to do this as well. We compared our system to the freely available ASSERT system (Pradhan et al., 2004). Both systems are fed only the input sentence during testing, with traces removed, so they cannot make use of many PropBank features such as frameset identitifier, person, tense, aspect, voice, a</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992. A simple rule-based part of speech tagger. Proceedings of the Third Conference on Applied Natural Language Processing, pages 152–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>Proceedings of the first conference on North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="6361" citStr="Charniak, 2000" startWordPosition="999" endWordPosition="1000">antic roleimplicitly learn features highly correlated with those labeling problem. In (Gildea and Jurafsky, 2002) extracted from a parse tree. We propose to develop the authors presented a statistical approach to learnan architecture that implements this kind of implicit ing (for FrameNet), with some success. They prolearning, rather than using explicitly engineered fea- posed to take advantage of the syntactic tree structures. In practice, our system also provides semantic ture that can be predicted by a parser, such as Chartags at a fraction of the computational cost of other niak’s parser (Charniak, 2000). Their aim is, given methods, taking on average 0.02 seconds to label a a node in the parse tree, to assign a semantic role sentence from the Penn Treebank, with almost no label to the words that are the children of that node. loss in accuracy. They extract several key types of features from the The rest of the article is as follows. First, we de- parse tree to be used in a statistical model for prescribe the problem of shallow semantic parsing in diction. These same features also proved crucial to more detail, as well as existing solutions to this prob- subsequent approaches, e.g. (Pradhan e</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="682" citStr="Gildea and Jurafsky, 2002" startWordPosition="98" endWordPosition="101">chitecture Ronan Collobert NEC Laboratories America, Inc. 4Independence Way Suite 200, Princeton, NJ 08540 collober@nec-labs.com Jason Weston NEC Laboratories America, Inc. 4Independence Way Suite 200, Princeton, NJ 08540 jasonw@nec-labs.com Abstract We describe a novel neural network architecture for the problem of semantic role labeling. Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky, 2002). Our method instead learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chunker. Our resulting system obtains accuracies comparable to the current state-of-the-art at a fraction of the computational cost. 1 Introduction Semantic understanding plays an important role in many end-user applications involving text: for information extraction, web-crawling systems, question and answer based systems, as well as machine translation, summarization and search. Such applications typically have to be computationally cheap to deal with an e</context>
<context position="5859" citStr="Gildea and Jurafsky, 2002" startWordPosition="915" endWordPosition="918">ions in tasks such as meta-data extraction (e.g. from Here, we propose a radically different approach web documents) and question and answer based systhat avoids the more complex task of building a full tems (e.g. call center systems), amongst others. parse tree. From a machine learning point of view, a 3 Previous Work human does not need to be taught about parse trees Several authors have already attempted to build mato talk. It is possible, however, that our brains may chine learning approaches for the semantic roleimplicitly learn features highly correlated with those labeling problem. In (Gildea and Jurafsky, 2002) extracted from a parse tree. We propose to develop the authors presented a statistical approach to learnan architecture that implements this kind of implicit ing (for FrameNet), with some success. They prolearning, rather than using explicitly engineered fea- posed to take advantage of the syntactic tree structures. In practice, our system also provides semantic ture that can be predicted by a parser, such as Chartags at a fraction of the computational cost of other niak’s parser (Charniak, 2000). Their aim is, given methods, taking on average 0.02 seconds to label a a node in the parse tree,</context>
<context position="23770" citStr="Gildea and Jurafsky, 2002" startWordPosition="3997" endWordPosition="4000">, 5659 and 1712 times respectively. 565 Even though some verbs are infrequent, one cannot cluster all verbs into a single group, as each verb dictates the types of semantic roles in the sentence, depending on its frame. Clustering all words into their part-of-speech, including the predicate, gives a poor 73.8% compared with 81.1%, where everything is clustered apart from the predicate. Figure 3 gives some anecdotal examples of test set predictions of our final model compared to ASSERT. 5.2 Argument Classification Accuracy So far we have not used the same accuracy measures as in previous work (Gildea and Jurafsky, 2002; Pradhan et al., 2004). Currently our architecture is designed to label on a per-word basis, while existing systems perform a segmentation process, and then label segments. While we do not optimize our model for the same criteria, it is still possible to measure the accuracy using the same metrics. We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al., 2004), by post-processing our per-word tags to form a majority vote over each segment. This gives 83.18% accuracy for our network when we suppose the pre</context>
<context position="27373" citStr="Gildea and Jurafsky, 2002" startWordPosition="4561" endWordPosition="4564">PropBank test set, showing Neural Net and ASSERT and gold standard labelings, with per-word accuracy in brackets. Note that even though our labeling does not match the hand-annotated one in the second sentence it still seems to make some sense as “in Delaware County” is labeled as a location modifier. The complete set of predictions on the test set can be found at http: //ml.nec-labs.com/software/senna. would improve with more hand-built features. For example, simply adding whether each word is part of a noun or verb phrase using the handannotated parse tree (the so-called “GOV” feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%. One must trade the generality of the model with its specificity, and also take into account how long the features take to compute. • Incorporating segment information Our system has no prior knowledge about segmentation in text. This could be encoded in many ways: most obviously by using a chunker, but also by 566 designing a different network architecture, e.g. by encoding contiguity constraints. To show the latter is useful, using hand-annotated segments to force contiguity by majority vote leads to an improvement from 83.95% to </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>M Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2001</date>
<booktitle>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>239--246</pages>
<marker>Gildea, Palmer, 2001</marker>
<rawString>D. Gildea and M. Palmer. 2001. The necessity of parsing for predicate argument recognition. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 239–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting ofAssociation for Computational Linguistics.</booktitle>
<contexts>
<context position="3292" citStr="Henderson, 2004" startWordPosition="499" endWordPosition="500">d. For example, stateof-the-art syntactic parsers theoretically have cubic complexity in the sentence length (Younger, 1967)1 and several semantic extraction algorithms use the parse tree as an initial feature. In this work, we describe a novel type of neural network architecture that could help to solve some of these issues. We focus our experimental study on the semantic role labeling problem (Palmer et al., 2005): being able to give a semantic role to a syn1Even though some parsers effectively exhibit linear behavior in sentence length (Ratnaparkhi, 1997), fast statistical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560–567, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics tactic constituent of a sentence, i.e. annotating the annotated parses of sentences. The current version predicate argument structure in text (see for exam- of the dataset gives semantic tags for the same senple Figure 1). Because of its nature, role labeling tences as in the Penn Treebank (Marcus et al., 1993), seems to require t</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>J. Henderson. 2004. Discriminative training of a neural network statistical parser. In Proceedings of the 42nd Meeting ofAssociation for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y LeCun</author>
<author>L Bottou</author>
<author>G B Orr</author>
<author>K-R M¨uller</author>
</authors>
<title>Efficient backprop.</title>
<date>1998</date>
<booktitle>Neural Networks: Tricks of the Trade,</booktitle>
<pages>9--50</pages>
<editor>In G.B. Orr and K.-R. M¨uller, editors,</editor>
<publisher>Springer.</publisher>
<marker>LeCun, Bottou, Orr, M¨uller, 1998</marker>
<rawString>Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. 1998. Efficient backprop. In G.B. Orr and K.-R. M¨uller, editors, Neural Networks: Tricks of the Trade, pages 9– 50. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3872" citStr="Marcus et al., 1993" startWordPosition="588" endWordPosition="591">stical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560–567, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics tactic constituent of a sentence, i.e. annotating the annotated parses of sentences. The current version predicate argument structure in text (see for exam- of the dataset gives semantic tags for the same senple Figure 1). Because of its nature, role labeling tences as in the Penn Treebank (Marcus et al., 1993), seems to require the syntactic analysis of a sentence which are excerpts from the Wall Street Journal. The before attributing semantic labels. Using this intu- central idea is that each verb in a sentence is laition, state-of-the-art systems first build a parse tree, beled with its propositional arguments, where the and syntactic constituents are then labeled by feed- abstract numbered arguments are intended to fill typing hand-built features extracted from the parse tree ical roles. For example, ARG0 is typically the actor, to a machine learning system, e.g. the ASSERT sys- and ARG1 is typi</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: the penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1948" citStr="Palmer et al., 2005" startWordPosition="287" endWordPosition="290">ms process large numbers of documents, whilst interactive human-machine applications require almost instant response. Another issue is the cost of producing labeled training data required for statistical models, which is exacerbated when those models also depend on syntactic features which must themselves be learnt. To achieve the goal of semantic understanding, the current consensus is to divide and conquer the 560 [The company]ARG0 [bought]REL [sugar]ARG1 [on the world market]ARGM-LOC [to meet export commitments]ARGM-PNC Figure 1: Example of Semantic Role Labeling from the PropBank dataset (Palmer et al., 2005). ARG0 is typically an actor, REL an action, ARG1 an object, and ARGM describe various modifiers such as location (LOC) and purpose (PNC). problem. Researchers tackle several layers of processing tasks ranging from the syntactic, such as part-of-speech labeling and parsing, to the semantic: word-sense disambiguation, semantic role-labeling, named entity extraction, co-reference resolution and entailment. None of these tasks are end goals in themselves but can be seen as layers of feature extraction that can help in a language-based end application, such as the ones described above. Unfortunate</context>
<context position="7550" citStr="Palmer et al., 2005" startWordPosition="1211" endWordPosition="1214">ent approaches, e.g. (Pradhan et al., 2004). lem. We then detail our algorithmic approach – the These features include: neural network architecture we employ – followed • The parts of speech and syntactic labels of by experiments that evaluate our method. Finally, words and nodes in the tree. we conclude with a summary and discussion of fu- • The node’s position (left or right) in relation to ture work. the verb. 2 Shallow Semantic Parsing • The syntactic path to the verb in the parse tree. FrameNet (Baker et al., 1998) and the Proposition • Whether a node in the parse tree is part of a Bank (Palmer et al., 2005), or PropBank for short, noun or verb phrase (by looking at the parent are the two main systems currently developed for nodes of that node). semantic role-labeling annotation. We focus here on PropBank. PropBank encodes role labels by semantically tagging the syntactic structures of hand 561 • The voice of the sentence: active or passive parse tree information from the semantic role label(part of the PropBank gold annotation); ing system, in fact the shared task of CONLL 2004 as well as several other features (predicate, head was devoted to this goal, but the results were not word, verb sub-ca</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Comput. Linguist., 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>K Hacioglu</author>
<author>J Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>Proceedings of HLT/NAACL-2004.</booktitle>
<contexts>
<context position="654" citStr="Pradhan et al., 2004" startWordPosition="94" endWordPosition="97">ovel Neural Network Architecture Ronan Collobert NEC Laboratories America, Inc. 4Independence Way Suite 200, Princeton, NJ 08540 collober@nec-labs.com Jason Weston NEC Laboratories America, Inc. 4Independence Way Suite 200, Princeton, NJ 08540 jasonw@nec-labs.com Abstract We describe a novel neural network architecture for the problem of semantic role labeling. Many current solutions are complicated, consist of several stages and handbuilt features, and are too slow to be applied as part of real applications that require such semantic labels, partly because of their use of a syntactic parser (Pradhan et al., 2004; Gildea and Jurafsky, 2002). Our method instead learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chunker. Our resulting system obtains accuracies comparable to the current state-of-the-art at a fraction of the computational cost. 1 Introduction Semantic understanding plays an important role in many end-user applications involving text: for information extraction, web-crawling systems, question and answer based systems, as well as machine translation, summarization and search. Such applications typically have to be computation</context>
<context position="4530" citStr="Pradhan et al., 2004" startWordPosition="695" endWordPosition="698">ysis of a sentence which are excerpts from the Wall Street Journal. The before attributing semantic labels. Using this intu- central idea is that each verb in a sentence is laition, state-of-the-art systems first build a parse tree, beled with its propositional arguments, where the and syntactic constituents are then labeled by feed- abstract numbered arguments are intended to fill typing hand-built features extracted from the parse tree ical roles. For example, ARG0 is typically the actor, to a machine learning system, e.g. the ASSERT sys- and ARG1 is typically the thing acted upon. The tem (Pradhan et al., 2004). This is rather slow, tak- precise usage of the numbering system is labeled for ing a few seconds per sentence at test time, partly each particular verb as so-called frames. Additionbecause of the parse tree component, and partly be- ally, semantic roles can also be labeled with one of cause of the use of Support Vector Machines (Boser 13 ARGM adjunct labels, such as ARGM-LOC or et al., 1992), which have linear complexity in test- ARGM-TMP for additional locational or temporal ing time with respect to the number of training ex- information relative to some verb. amples. This makes it hard to </context>
<context position="6973" citStr="Pradhan et al., 2004" startWordPosition="1107" endWordPosition="1110">ak, 2000). Their aim is, given methods, taking on average 0.02 seconds to label a a node in the parse tree, to assign a semantic role sentence from the Penn Treebank, with almost no label to the words that are the children of that node. loss in accuracy. They extract several key types of features from the The rest of the article is as follows. First, we de- parse tree to be used in a statistical model for prescribe the problem of shallow semantic parsing in diction. These same features also proved crucial to more detail, as well as existing solutions to this prob- subsequent approaches, e.g. (Pradhan et al., 2004). lem. We then detail our algorithmic approach – the These features include: neural network architecture we employ – followed • The parts of speech and syntactic labels of by experiments that evaluate our method. Finally, words and nodes in the tree. we conclude with a summary and discussion of fu- • The node’s position (left or right) in relation to ture work. the verb. 2 Shallow Semantic Parsing • The syntactic path to the verb in the parse tree. FrameNet (Baker et al., 1998) and the Proposition • Whether a node in the parse tree is part of a Bank (Palmer et al., 2005), or PropBank for short</context>
<context position="8260" citStr="Pradhan et al., 2004" startWordPosition="1327" endWordPosition="1330">ystems currently developed for nodes of that node). semantic role-labeling annotation. We focus here on PropBank. PropBank encodes role labels by semantically tagging the syntactic structures of hand 561 • The voice of the sentence: active or passive parse tree information from the semantic role label(part of the PropBank gold annotation); ing system, in fact the shared task of CONLL 2004 as well as several other features (predicate, head was devoted to this goal, but the results were not word, verb sub-categorization, ... ). completely satisfactory. Previously, in (Gildea and The authors of (Pradhan et al., 2004) used a Palmer, 2001), the authors tried to show that the similar structure, but added more features, notably parse tree is necessary for good generalization by head word part-of-speech, the predicted named en- showing that segments derived from a shallow syntity class of the argument, word sense disambigua- tactic parser or chunker do not perform as well for tion of the verb and verb clustering, and others (they this goal. A further analysis of using chunkers, with add 25 variants of 12 new feature types overall.) improved results was also given in (Punyakanok et Their system also uses a pars</context>
<context position="19954" citStr="Pradhan et al., 2004" startWordPosition="3363" endWordPosition="3366"> details. We trained on the training set of PropBank supplemented with the Brown corpus, resulting in a test accuracy on the test set of PropBank of 96.85% which compares to 96.66% using the Brill tagger (Brill, 1992). Semantic Role Labeling In our experiments we considered a 23-class problem of NULL (no label), the core arguments ARG0-5, REL, ARGA, and ARGM- along with the 13 secondary modifier labels such as ARGM-LOC and ARGM-TMP. We simplified R-ARGn and C-ARGn to be written as ARGn, and post-processed ASSERT to do this as well. We compared our system to the freely available ASSERT system (Pradhan et al., 2004). Both systems are fed only the input sentence during testing, with traces removed, so they cannot make use of many PropBank features such as frameset identitifier, person, tense, aspect, voice, and form of the verb. As our algorithm outputs a semantic tag for each word of a sentence, we directly compare this per-word accuracy with ASSERT. Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, pernode accuracy is usually reported in papers such as (Pradhan et al., 2004). Unfortunately our approach is based on a completely different pr</context>
<context position="23793" citStr="Pradhan et al., 2004" startWordPosition="4001" endWordPosition="4004">ctively. 565 Even though some verbs are infrequent, one cannot cluster all verbs into a single group, as each verb dictates the types of semantic roles in the sentence, depending on its frame. Clustering all words into their part-of-speech, including the predicate, gives a poor 73.8% compared with 81.1%, where everything is clustered apart from the predicate. Figure 3 gives some anecdotal examples of test set predictions of our final model compared to ASSERT. 5.2 Argument Classification Accuracy So far we have not used the same accuracy measures as in previous work (Gildea and Jurafsky, 2002; Pradhan et al., 2004). Currently our architecture is designed to label on a per-word basis, while existing systems perform a segmentation process, and then label segments. While we do not optimize our model for the same criteria, it is still possible to measure the accuracy using the same metrics. We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al., 2004), by post-processing our per-word tags to form a majority vote over each segment. This gives 83.18% accuracy for our network when we suppose the predicate must also be ide</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky. 2004. Shallow semantic parsing using support vector machines. Proceedings of HLT/NAACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The necessity of syntactic parsing for semantic role labeling.</title>
<date>2005</date>
<booktitle>Proceedings of IJCAI’05,</booktitle>
<pages>1117--1123</pages>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity of syntactic parsing for semantic role labeling. Proceedings of IJCAI’05, pages 1117–1123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3240" citStr="Ratnaparkhi, 1997" startWordPosition="492" endWordPosition="493">low to be used in the applications previously described. For example, stateof-the-art syntactic parsers theoretically have cubic complexity in the sentence length (Younger, 1967)1 and several semantic extraction algorithms use the parse tree as an initial feature. In this work, we describe a novel type of neural network architecture that could help to solve some of these issues. We focus our experimental study on the semantic role labeling problem (Palmer et al., 2005): being able to give a semantic role to a syn1Even though some parsers effectively exhibit linear behavior in sentence length (Ratnaparkhi, 1997), fast statistical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560–567, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics tactic constituent of a sentence, i.e. annotating the annotated parses of sentences. The current version predicate argument structure in text (see for exam- of the dataset gives semantic tags for the same senple Figure 1). Because of its nature, role labeling tences as in the Pen</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>A. Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>J L Gauvain</author>
</authors>
<title>Connectionist language modeling for large vocabulary continuousspeech recognition.</title>
<date>2002</date>
<booktitle>Proceedings of ICASSP’02.</booktitle>
<contexts>
<context position="14275" citStr="Schwenk and Gauvain, 2002" startWordPosition="2332" endWordPosition="2335">ve to deal with words, and that a simple index i E D does not carry any information specific to a word: for each word we need a set of features relevant for the task. As described earlier, previous methods construct a parse tree, and then compute hand-built features which are then fed to a classification algorithm. In order to bypass the use of a parse tree, we convert each word i E D into a particular vector wi E Rd which is going to be learnt for the task we are interested in. This approach has already been used with great success in the domain of language models (Bengio and Ducharme, 2001; Schwenk and Gauvain, 2002). Tanh Squashing Layer Classical Linear Layer Softmax Squashing Layer • ••• • • • ARG1 ARG2 ARGM ARGM LOC TMP Figure 2: MLP architecture for shallow semantic parsing. The input sequence is at the top. The output class probabilities for the word of interest (“cat”) given the verb of interest (“sat”) are given at the bottom. The first layer of our MLP is thus a lookup table which replaces the word indices into a concatenation of vectors: {s(1), ... , s(nw)} H (ws(1) ... ws(n&amp;quot;)) E Rn&amp;quot; d . (2) The weights {wi |i E D} for this layer are considered during the backpropagation phase of the MLP, and th</context>
</contexts>
<marker>Schwenk, Gauvain, 2002</marker>
<rawString>H. Schwenk and J.L. Gauvain. 2002. Connectionist language modeling for large vocabulary continuousspeech recognition. Proceedings of ICASSP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<contexts>
<context position="2800" citStr="Younger, 1967" startWordPosition="419" endWordPosition="420">part-of-speech labeling and parsing, to the semantic: word-sense disambiguation, semantic role-labeling, named entity extraction, co-reference resolution and entailment. None of these tasks are end goals in themselves but can be seen as layers of feature extraction that can help in a language-based end application, such as the ones described above. Unfortunately, the state-of-the-art solutions of many of these tasks are simply too slow to be used in the applications previously described. For example, stateof-the-art syntactic parsers theoretically have cubic complexity in the sentence length (Younger, 1967)1 and several semantic extraction algorithms use the parse tree as an initial feature. In this work, we describe a novel type of neural network architecture that could help to solve some of these issues. We focus our experimental study on the semantic role labeling problem (Palmer et al., 2005): being able to give a semantic role to a syn1Even though some parsers effectively exhibit linear behavior in sentence length (Ratnaparkhi, 1997), fast statistical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made. Proceedings of the 45th Ann</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D. H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>