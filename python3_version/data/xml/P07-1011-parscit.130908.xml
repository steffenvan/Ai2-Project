<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004372">
<title confidence="0.9944885">
Detecting Erroneous Sentences using Automatically Mined Sequential
Patterns
</title>
<author confidence="0.998254">
Guihua Sun * Xiaohua Liu Gao Cong Ming Zhou
</author>
<affiliation confidence="0.998066">
Chongqing University Microsoft Research Asia
</affiliation>
<email confidence="0.865141">
sunguihua5018@163.com {xiaoliu, gaocong, mingzhou}@microsoft.com
</email>
<author confidence="0.998664">
Zhongyang Xiong John Lee † Chin-Yew Lin
</author>
<affiliation confidence="0.997288">
Chongqing University MIT Microsoft Research Asia
</affiliation>
<email confidence="0.993768">
zyxiong@cqu.edu.cn jsylee@mit.edu cyl@microsoft.com
</email>
<sectionHeader confidence="0.995581" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992083333333">
This paper studies the problem of identify-
ing erroneous/correct sentences. The prob-
lem has important applications, e.g., pro-
viding feedback for writers of English as
a Second Language, controlling the quality
of parallel bilingual sentences mined from
the Web, and evaluating machine translation
results. In this paper, we propose a new
approach to detecting erroneous sentences
by integrating pattern discovery with super-
vised learning models. Experimental results
show that our techniques are promising.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.977457647058824">
Detecting erroneous/correct sentences has the fol-
lowing applications. First, it can provide feedback
for writers of English as a Second Language (ESL)
as to whether a sentence contains errors. Second, it
can be applied to control the quality of parallel bilin-
gual sentences mined from the Web, which are criti-
cal sources for a wide range of applications, such as
statistical machine translation (Brown et al., 1993)
and cross-lingual information retrieval (Nie et al.,
1999). Third, it can be used to evaluate machine
translation results. As demonstrated in (Corston-
Oliver et al., 2001; Gamon et al., 2005), the better
human reference translations can be distinguished
from machine translations by a classification model,
the worse the machine translation system is.
�Work done while the author was a visiting student at MSRA
†Work done while the author was a visiting student at MSRA
</bodyText>
<page confidence="0.981927">
81
</page>
<bodyText confidence="0.998424151515151">
The previous work on identifying erroneous sen-
tences mainly aims to find errors from the writing of
ESL learners. The common mistakes (Yukio et al.,
2001; Gui and Yang, 2003) made by ESL learners
include spelling, lexical collocation, sentence struc-
ture, tense, agreement, verb formation, wrong Part-
Of-Speech (POS), article usage, etc. The previous
work focuses on grammar errors, including tense,
agreement, verb formation, article usage, etc. How-
ever, little work has been done to detect sentence
structure and lexical collocation errors.
Some methods of detecting erroneous sentences
are based on manual rules. These methods (Hei-
dorn, 2000; Michaud et al., 2000; Bender et al.,
2004) have been shown to be effective in detect-
ing certain kinds of grammatical errors in the writ-
ing of English learners. However, it could be ex-
pensive to write rules manually. Linguistic experts
are needed to write rules of high quality; Also, it
is difficult to produce and maintain a large num-
ber of non-conflicting rules to cover a wide range of
grammatical errors. Moreover, ESL writers of differ-
ent first-language backgrounds and skill levels may
make different errors, and thus different sets of rules
may be required. Worse still, it is hard to write rules
for some grammatical errors, for example, detecting
errors concerning the articles and singular plural us-
age (Nagata et al., 2006).
Instead of asking experts to write hand-crafted
rules, statistical approaches (Chodorow and Lea-
cock, 2000; Izumi et al., 2003; Brockett et al., 2006;
Nagata et al., 2006) build statistical models to iden-
tify sentences containing errors. However, existing
</bodyText>
<note confidence="0.9032685">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999671210526316">
statistical approaches focus on some pre-defined er-
rors and the reported results are not attractive. More-
over, these approaches, e.g., (Izumi et al., 2003;
Brockett et al., 2006) usually need errors to be spec-
ified and tagged in the training sentences, which re-
quires expert help to be recruited and is time con-
suming and labor intensive.
Considering the limitations of the previous work,
in this paper we propose a novel approach that is
based on pattern discovery and supervised learn-
ing to successfully identify erroneous/correct sen-
tences. The basic idea of our approach is to build
a machine learning model to automatically classify
each sentence into one of the two classes, “erro-
neous” and “correct.” To build the learning model,
we automatically extract labeled sequential patterns
(LSPs) from both erroneous sentences and correct
sentences, and use them as input features for classi-
fication models. Our main contributions are:
</bodyText>
<listItem confidence="0.87539996">
• We mine labeled sequential patterns(LSPs)
from the preprocessed training data to build
leaning models. Note that LSPs are also very
different from N-gram language models that
only consider continuous sequences.
• We also enrich the LSP features with other auto-
matically computed linguistic features, includ-
ing lexical collocation, language model, syn-
tactic score, and function word density. In con-
trast with previous work focusing on (a spe-
cific type of) grammatical errors, our model can
handle a wide range of errors, including gram-
mar, sentence structure, and lexical choice.
• We empirically evaluate our methods on two
datasets consisting of sentences written by
Japanese and Chinese, respectively. Experi-
mental results show that labeled sequential pat-
terns are highly useful for the classification
results, and greatly outperform other features.
Our method outperforms Microsoft Word03
and ALEK (Chodorow and Leacock, 2000)
from Educational Testing Service (ETS) in
some cases. We also apply our learning model
to machine translation (MT) data as a comple-
mentary measure to evaluate MT results.
</listItem>
<bodyText confidence="0.9974772">
The rest of this paper is organized as follows.
The next section discusses related work. Section 3
presents the proposed technique. We evaluate our
proposed technique in Section 4. Section 5 con-
cludes this paper and discusses future work.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999603767441861">
Research on detecting erroneous sentences can be
classified into two categories. The first category
makes use of hand-crafted rules, e.g., template
rules (Heidorn, 2000) and mal-rules in context-free
grammars (Michaud et al., 2000; Bender et al.,
2004). As discussed in Section 1, manual rule based
methods have some shortcomings.
The second category uses statistical techniques
to detect erroneous sentences. An unsupervised
method (Chodorow and Leacock, 2000) is em-
ployed to detect grammatical errors by inferring
negative evidence from TOEFL administrated by
ETS. The method (Izumi et al., 2003) aims to de-
tect omission-type and replacement-type errors and
transformation-based leaning is employed in (Shi
and Zhou, 2005) to learn rules to detect errors for
speech recognition outputs. They also require spec-
ifying error tags that can tell the specific errors
and their corrections in the training corpus. The
phrasal Statistical Machine Translation (SMT) tech-
nique is employed to identify and correct writing er-
rors (Brockett et al., 2006). This method must col-
lect a large number of parallel corpora (pairs of er-
roneous sentences and their corrections) and perfor-
mance depends on SMT techniques that are not yet
mature. The work in (Nagata et al., 2006) focuses
on a type of error, namely mass vs. count nouns.
In contrast to existing statistical methods, our tech-
nique needs neither errors tagged nor parallel cor-
pora, and is not limited to a specific type of gram-
matical error.
There are also studies on automatic essay scoring
at document-level. For example, E-rater (Burstein
et al., 1998), developed by the ETS, and Intelligent
Essay Assessor (Foltz et al., 1999). The evaluation
criteria for documents are different from those for
sentences. A document is evaluated mainly by its or-
ganization, topic, diversity of vocabulary, and gram-
mar while a sentence is done by grammar, sentence
structure, and lexical choice.
Another related work is Machine Translation (MT)
evaluation. Classification models are employed
in (Corston-Oliver et al., 2001; Gamon et al., 2005)
</bodyText>
<page confidence="0.980212">
82
</page>
<bodyText confidence="0.999225212765957">
to evaluate the well-formedness of machine transla- models. We illustrate the challenge with an exam-
tion outputs. The writers of ESL and MT normally ple. Consider an erroneous sentence, “If Maggie will
make different mistakes: in general, ESL writers can go to supermarket, she will buy a bag for you.” It is
write overall grammatically correct sentences with difficult for previous methods using statistical tech-
some local mistakes while MT outputs normally pro- niques to capture such an error. For example, N-
duce locally well-formed phrases with overall gram- gram language model is considered to be effective
matically wrong sentences. Hence, the manual fea- in writing evaluation (Burstein et al., 1998; Corston-
tures designed for MT evaluation are not applicable Oliver et al., 2001). However, it becomes very ex-
to detect erroneous sentences from ESL learners. pensive if N &gt; 3 and N-grams only consider contin-
LSPs differ from the traditional sequential pat- uous sequence of words, which is unable to detect
terns, e.g., (Agrawal and Srikant, 1995; Pei et al., the above error “if...will...will”.
2001) in that LSPs are attached with class labels and We propose labeled sequential patterns to effec-
we prefer those with discriminating ability to build tively characterize the features of correct and er-
classification model. In our other work (Sun et al., roneous sentences (Section 3.2), and design some
2007), labeled sequential patterns, together with la- complementary features ( Section 3.3).
beled tree patterns, are used to build pattern-based 3.2 Mining Labeled Sequential Patterns ( LSP )
classifier to detect erroneous sentences. The clas- Labeled Sequential Patterns (LSP). A labeled se-
sification method in (Sun et al., 2007) is different quential pattern, p, is in the form of LHS —* c, where
from those used in this paper. Moreover, instead of LHS is a sequence and c is a class label. Let I be a
labeled sequential patterns, in (Sun et al., 2007) the set of items and L be a set of class labels. Let D be a
most significant k labeled sequential patterns with sequence database in which each tuple is composed
constraints for each training sentence are mined to of a list of items in I and a class label in L. We say
build classifiers. Another related work is (Jindal and that a sequence s1 =&lt; a1,..., am &gt; is contained in
Liu, 2006), where sequential patterns with labels are a sequence s2 =&lt; b1, ..., b,,, &gt; if there exist integers
used to identify comparative sentences. i1, ...imsuch that 1 &lt; i1 &lt; i2 &lt; ... &lt; im &lt; n and
3 Proposed Technique aj = bi, for all j E 1, ..., m. Similarly, we say that
This section first gives our problem statement and a LSP p1 is contained by p2 if the sequence p1.LHS
then presents our proposed technique to build learn- is contained by p2.LHS and p1.c = p2.c. Note that
ing models. it is not required that s1 appears continuously in s2.
3.1 Problem Statement We will further refine the definition of “contain” by
In this paper we study the problem of identifying imposing some constraints (to be explained soon).
erroneous/correct sentences. A set of training data A LSP p is attached with two measures, support and
containing correct and erroneous sentences is given. confidence. The support of p, denoted by sup(p),
Unlike some previous work, our technique requires is the percentage of tuples in database D that con-
neither that the erroneous sentences are tagged with tain the LSP p. The probability of the LSP p being
detailed errors, nor that the training data consist of true is referred to as “the confidence of p ”, denoted
parallel pairs of sentences (an error sentence and its by conf(p), and is computed as sup(p)The
correction). The erroneous sentence contains a wide sup(p.LHS).
range of errors on grammar, sentence structure, and support is to measure the generality of the pattern p
lexical choice. We do not consider spelling errors in and minimum confidence is a statement of predictive
this paper. ability of p.
We address the problem by building classifica- Example 1: Consider a sequence database contain-
tion models. The main challenge is to automatically ing three tuples t1 = (&lt; a, d, e, f &gt;, E), t2 = (&lt;
extract representative features for both correct and a, f, e, f &gt;, E) and t3 = (&lt; d, a, f &gt;, C). One
erroneous sentences to build effective classification example LSP p1 = &lt; a, e, f &gt;—* E, which is con-
83 tained in tuples t1 and t2. Its support is 66.7% and
its confidence is 100%. As another example, LSP p2
</bodyText>
<equation confidence="0.458279333333333">
= &lt; a, f &gt;—* E with support 66.7% and confidence
66.7%. p1 is a better indication of class E than p2.
1-1
</equation>
<bodyText confidence="0.978893317073171">
Generating Sequence Database. We generate the
database by applying Part-Of-Speech (POS) tagger
to tag each training sentence while keeping func-
tion words1 and time words2. After the process-
ing, each sentence together with its label becomes
a database tuple. The function words and POS tags
play important roles in both grammars and sentence
structures. In addition, the time words are key
clues in detecting errors of tense usage. The com-
bination of them allows us to capture representative
features for correct/erroneous sentences by mining
LSPs. Some example LSPs include “&lt;a, NNS&gt; —*
Error”(singular determiner preceding plural noun),
and “&lt;yesterday, is&gt; —* Error”. Note that the con-
fidences of these LSPs are not necessary 100%.
First, we use MXPOST-Maximum Entropy Part of
Speech Tagger Toolkit3 for POS tags. The MXPOST
tagger can provide fine-grained tag information. For
example, noun can be tagged with “NN”(singular
noun) and “NNS”(plural noun); verb can be tagged
with “VB”, ”VBG”, ”VBN”, ”VBP”, ”VBD” and
”VBZ”. Second, the function words and time words
that we use form a key word list. If a word in a
training sentence is not contained in the key word
list, then the word will be replaced by its POS. The
processed sentence consists of POS and the words of
key word list. For example, after the processing, the
sentence “In the past, John was kind to his sister” is
converted into “In the past, NNP was JJ to his NN”,
where the words “in”, “the”, “was”, “to” and “his”
are function words, the word “past” is time word,
and “NNP”, “JJ”, and “NN” are POS tags.
Mining LSPs. The length of the discovered LSPs
is flexible and they can be composed of contiguous
or distant words/tags. Existing frequent sequential
pattern mining algorithms (e.g. (Pei et al., 2001))
use minimum support threshold to mine frequent se-
quential patterns whose support is larger than the
threshold. These algorithms are not sufficient for our
problem of mining LSPs. In order to ensure that all
our discovered LSPs are discriminating and are capa-
</bodyText>
<footnote confidence="0.999861">
1http://www.marlodge.supanet.com/museum/funcword.html
2http://www.wjh.harvard.edu/%7Einquirer/Time%40.html
3http://www.cogsci.ed.ac.uk/—jamesc/taggers/MXPOST.html
</footnote>
<bodyText confidence="0.99972471875">
ble of predicting correct or erroneous sentences, we
impose another constraint minimum confidence. Re-
call that the higher the confidence of a pattern is, the
better it can distinguish between correct sentences
and erroneous sentences. In our experiments, we
empirically set minimum support at 0.1% and mini-
mum confidence at 75%.
Mining LSPs is nontrivial since its search space
is exponential, althought there have been a host of
algorithms for mining frequent sequential patterns.
We adapt the frequent sequence mining algorithm
in (Pei et al., 2001) for mining LSPs with constraints.
Converting LSPs to Features. Each discovered LSP
forms a binary feature as the input for classification
model. If a sentence includes a LSP, the correspond-
ing feature is set at 1.
The LSPs can characterize the correct/erroneous
sentence structure and grammar. We give some ex-
amples of the discovered LSPs. (1) LSPs for erro-
neous sentences. For example, “&lt;this, NNS&gt;”(e.g.
contained in “this books is stolen.”), “&lt;past,
is&gt;”(e.g. contained in “in the past, John is kind to
his sister.”), “&lt;one, of, NN&gt;”(e.g. contained in “it is
one of important working language”, “&lt;although,
but&gt;”(e.g. contained in “although he likes it, but
he can’t buy it.”), and “&lt;only, if, I, am&gt;”(e.g. con-
tained in “only if my teacher has given permission,
I am allowed to enter this room”). (2) LSPs for cor-
rect sentences. For instance, “&lt;would, VB&gt;”(e.g.
contained in “he would buy it.”), and “&lt;VBD,
yeserday&gt;”(e.g. contained in “I bought this book
yesterday.”).
</bodyText>
<subsectionHeader confidence="0.99839">
3.3 Other Linguistic Features
</subsectionHeader>
<bodyText confidence="0.957948454545455">
We use some linguistic features that can be com-
puted automatically as complementary features.
Lexical Collocation (LC) Lexical collocation er-
ror (Yukio et al., 2001; Gui and Yang, 2003) is com-
mon in the writing of ESL learners, such as “strong
tea” but not “powerful tea.” Our LSP features can-
not capture all LCs since we replace some words
with POS tags in mining LSPs. We collect five types
of collocations: verb-object, adjective-noun, verb-
adverb, subject-verb, and preposition-object from a
general English corpus4. Correct LCs are collected
</bodyText>
<footnote confidence="0.9962675">
4The general English corpus consists of about 4.4 million
native sentences.
</footnote>
<page confidence="0.998383">
84
</page>
<bodyText confidence="0.999904666666667">
by extracting collocations of high frequency from
the general English corpus. Erroneous LC candi-
dates are generated by replacing the word in correct
collocations with its confusion words, obtained from
WordNet, including synonyms and words with sim-
ilar spelling or pronunciation. Experts are consulted
to see if a candidate is a true erroneous collocation.
We compute three statistical features for each sen-
tence below. (1) The first feature is computed by
</bodyText>
<equation confidence="0.9781415">
�m
i=1
</equation>
<bodyText confidence="0.999561411764706">
the number of collocations in each sentence, and
probability p(coi) of each CL coi is calculated us-
ing the method (L¨u and Zhou, 2004). (2) The sec-
ond feature is computed by the ratio of the number
of unknown collocations (neither correct LCs nor er-
roneous LCs) to the number of collocations in each
sentence. (3) The last feature is computed by the ra-
tio of the number of erroneous LCs to the number of
collocations in each sentence.
Perplexity from Language Model (PLM) Perplex-
ity measures are extracted from a trigram language
model trained on a general English corpus using
the SRILM-SRI Language Modeling Toolkit (Stolcke,
2002). We calculate two values for each sentence:
lexicalized trigram perplexity and part of speech
(POS) trigram perplexity. The erroneous sentences
would have higher perplexity.
Syntactic Score (SC) Some erroneous sentences of-
ten contain words and concepts that are locally cor-
rect but cannot form coherent sentences (Liu and
Gildea, 2005). To measure the coherence of sen-
tences, we use a statistical parser Toolkit (Collins,
1997) to assign each sentence a parser’s score that
is the related log probability of parsing. We assume
that erroneous sentences with undesirable sentence
structures are more likely to receive lower scores.
Function Word Density (FWD) We consider the
density of function words (Corston-Oliver et al.,
2001), i.e. the ratio of function words to content
words. This is inspired by the work (Corston-Oliver
et al., 2001) showing that function word density can
be effective in distinguishing between human refer-
ences and machine outputs. In this paper, we calcu-
late the densities of seven kinds of function words 5
</bodyText>
<footnote confidence="0.9177125">
5including determiners/yantifiers, all pronouns, different
pronoun types: Wh, 1st, 2&amp;quot; , and 3rd person pronouns, prepo-
</footnote>
<table confidence="0.9209116">
Dataset Type Source Number
JC (+) the Japan Times newspaper 16,857
and Model English Essay
(-) HEL (Hiroshima English 17,301
Learners’ Corpus) and JLE
(Japanese Learners of En-
glish Corpus)
CC (+) the 21st Century newspaper 3,200
(-) CLEC (Chinese Learner Er- 3,199
ror Corpus)
</table>
<tableCaption confidence="0.834102">
Table 1: Corpora ((+): correct; (-): erroneous)
respectively as 7 features.
</tableCaption>
<sectionHeader confidence="0.996055" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999327">
We evaluated the performance of our techniques
with support vector machine (SVM) and Naive
Bayesian (NB) classification models. We also com-
pared the effectiveness of various features. In ad-
dition, we compared our technique with two other
methods of checking errors, Microsoft Word03 and
ALEK method (Chodorow and Leacock, 2000). Fi-
nally, we also applied our technique to evaluate the
Machine Translation outputs.
</bodyText>
<subsectionHeader confidence="0.974512">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999915785714286">
Classification Models. We used two classification
models, SVM6 and NB classification model.
Data. We collected two datasets from different do-
mains, Japanese Corpus (JC) and Chinese Corpus
(CC). Table 1 gives the details of our corpora. In
the learner’s corpora, all of the sentences are erro-
neous. Note that our data does not consist of parallel
pairs of sentences (one error sentence and its correc-
tion). The erroneous sentences includes grammar,
sentence structure and lexical choice errors, but not
spelling errors.
For each sentence, we generated five kinds of fea-
tures as presented in Section 3. For a non-binary
feature X, its value x is normalized by z-score,
</bodyText>
<equation confidence="0.8506945">
norm(x) = x−mean(X) where mean(x) is the em-
.\/var(X )
</equation>
<bodyText confidence="0.9997602">
pirical mean of X and var(X) is the variance of X.
Thus each sentence is represented by a vector.
Metrics We calculated the precision, recall,
and F-score for correct and erroneous sentences,
respectively, and also report the overall accuracy.
</bodyText>
<footnote confidence="0.81537">
sitions and adverbs, auxiliary verbs, and conjunctions.
6http://svmlight.joachims.org/
</footnote>
<bodyText confidence="0.591991">
p(coi)/n, where m is the number of CLs, n is
</bodyText>
<page confidence="0.997097">
85
</page>
<bodyText confidence="0.9970755">
All the experimental results are obtained thorough
10-fold cross-validation.
</bodyText>
<subsectionHeader confidence="0.977663">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.997888975">
The Effectiveness of Various Features. The exper-
iment is to evaluate the contribution of each feature
to the classification. The results of SVM are given in
Table 2. We can see that the performance of labeled
sequential patterns (LSP) feature consistently out-
performs those of all the other individual features. It
also performs better even if we use all the other fea-
tures together. This is because other features only
provide some relatively abstract and simple linguis-
tic information, whereas the discovered LSPs char-
acterize significant linguistic features as discussed
before. We also found that the results of NB are a
little worse than those of SVM. However, all the fea-
tures perform consistently on the two classification
models and we can observe the same trend. Due to
space limitation, we do not give results of NB.
In addition, the discovered LSPs themselves are
intuitive and meaningful since they are intuitive fea-
tures that can distinguish correct sentences from er-
roneous sentences. We discovered 6309 LSPs in
JC data and 3742 LSPs in CC data. Some exam-
ple LSPs discovered from erroneous sentences are
&lt;a, NNS&gt; (support:0.39%, confidence:85.71%),
&lt;to, VBD&gt; (support:0.11%, confidence:84.21%),
and &lt;the, more, the, JJ&gt; (support:0.19%, confi-
dence:0.93%) 7; Similarly, we also give some exam-
ple LSPs mined from correct sentences: &lt;NN, VBZ&gt;
(support:2.29%, confidence:75.23%), and &lt;have,
VBN, since&gt; (support:0.11%, confidence:85.71%)
8. However, other features are abstract and it is hard
to derive some intuitive knowledge from the opaque
statistical values of these features.
As shown in Table 2, our technique achieves
the highest accuracy, e.g. 81.75% on the Japanese
dataset, when we use all the features. However, we
also notice that the improvement is not very signif-
icant compared with using LSP feature individually
(e.g. 79.63% on the Japanese dataset). The similar
results are observed when we combined the features
PLM, SC, FWD, and LC. This could be explained
</bodyText>
<footnote confidence="0.952155">
7a + plural noun; to + past tense format; the more + the +
base form of adjective
8singular or mass noun + the 3rd person singular present
format; have + past participle format + since
</footnote>
<bodyText confidence="0.997932212765958">
by two reasons: (1) A sentence may contain sev-
eral kinds of errors. A sentence detected to be er-
roneous by one feature may also be detected by an-
other feature; and (2) Various features give conflict-
ing results. The two aspects suggest the directions
of our future efforts to improve the performance of
our models.
Comparing with Other Methods. It is difficult
to find benchmark methods to compare with our
technique because, as discussed in Section 2, exist-
ing methods often require error tagged corpora or
parallel corpora, or focus on a specific type of er-
rors. In this paper, we compare our technique with
the grammar checker of Microsoft Word03 and the
ALEK (Chodorow and Leacock, 2000) method used
by ETS. ALEK is used to detect inappropriate usage
of specific vocabulary words. Note that we do not
consider spelling errors. Due to space limitation, we
only report the precision, recall, F-score
for erroneous sentences, and the overall accuracy.
As can be seen from Table 3, our method out-
performs the other two methods in terms of over-
all accuracy, F-score, and recall, while the three
methods achieve comparable precision. We realize
that the grammar checker of Word is a general tool
and the performance of ALEK (Chodorow and Lea-
cock, 2000) can be improved if larger training data is
used. We found that Word and ALEK usually cannot
find sentence structure and lexical collocation errors,
e.g., “The more you listen to English, the easy it be-
comes.” contains the discovered LSP &lt;the, more, the,
JJ&gt; —* Error.
Cross-domain Results. To study the performance
of our method on cross-domain data from writers
of the same first-language background, we collected
two datasets from Japanese writers, one is composed
of 694 parallel sentences (+:347, -:347), and the
other 1,671 non-parallel sentences (+:795, -:876).
The two datasets are used as test data while we use
JC dataset for training. Note that the test sentences
come from different domains from the JC data. The
results are given in the first two rows of Table 4. This
experiment shows that our leaning model trained for
one domain can be effectively applied to indepen-
dent data in the other domains from the writes of the
same first-language background, no matter whether
the test data is parallel or not. We also noticed that
</bodyText>
<page confidence="0.992815">
86
</page>
<table confidence="0.999852333333333">
Dataset Feature A (-)F (-)R (-)P (+)F (+)R (+)P
JC LSP 79.63 80.65 85.56 76.29 78.49 73.79 83.85
LC 69.55 71.72 77.87 66.47 67.02 61.36 73.82
PLM 61.60 55.46 50.81 64.91 62 70.28 58.43
SC 53.66 57.29 68.40 56.12 34.18 39.04 32.22
FWD 68.01 72.82 86.37 62.95 61.14 49.94 78.82
LC + PLM + SC + FWD 71.64 73.52 79.38 68.46 69.48 64.03 75.94
LSP + LC + PLM + SC + FWD 81.75 81.60 81.46 81.74 81.90 82.04 81.76
CC LSP 78.19 76.40 70.64 83.20 79.71 85.72 74.50
LC 63.82 62.36 60.12 64.77 65.17 67.49 63.01
PLM 55.46 64.41 80.72 53.61 40.41 30.22 61.30
SC 50.52 62.58 87.31 50.64 13.75 14.33 13.22
FWD 61.36 60.80 60.70 60.90 61.90 61.99 61.80
LC + PLM + SC + FWD 67.69 67.62 67.51 67.77 67.74 67.87 67.64
LSP + LC + PLM + SC + FWD 79.81 78.33 72.76 84.84 81.10 86.92 76.02
</table>
<tableCaption confidence="0.9552715">
Table 2: The Experimental Results (A: overall accuracy; (-): erroneous sentences; (+): correct sentences; F:
F-score; R: recall; P: precision)
</tableCaption>
<table confidence="0.999918285714286">
Dataset Model A (-)F (-)R (-)P
JC Ours 81.39 81.25 81.24 81.28
Word 58.87 33.67 21.03 84.73
ALEK 54.69 20.33 11.67 78.95
CC Ours 79.14 77.81 73.17 83.09
Word 58.47 32.02 19.81 84.22
ALEK 55.21 22.83 13.42 76.36
</table>
<tableCaption confidence="0.999529">
Table 3: The Comparison Results
</tableCaption>
<bodyText confidence="0.99227268">
LSPs play dominating role in achieving the results.
Due to space limitation, no details are reported.
To further see the performance of our method
on data written by writers with different first-
language backgrounds, we conducted two experi-
ments. (1) We merge the JC dataset and CC dataset.
The 10-fold cross-validation results on the merged
dataset are given in the third row of Table 4. The
results demonstrate that our models work well when
the training data and test data contain sentences from
different first-language backgrounds. (2) We use the
JC dataset (resp. CC dataset) for training while the
CC dataset (resp. JC dataset) is used as test data. As
shown in the fourth (resp. fifth) row of Table 4, the
results are worse than their corresponding results of
Word given in Table 3. The reason is that the mis-
takes made by Japanese and Chinese are different,
thus the learning model trained on one data does not
work well on the other data. Note that our method is
not designed to work in this scenario.
Application to Machine Translation Evaluation.
Our learning models could be used to evaluate the
MT results as an complementary measure. This is
based on the assumption that if the MT results can
be accurately distinguished from human references
</bodyText>
<table confidence="0.990102333333333">
Dataset A (-)F (-)R (-)P
JC(Train)+nonparallel(Test) 72.49 68.55 57.51 84.84
JC(Train)+parallel(Test) 71.33 69.53 65.42 74.18
JC + CC 79.98 79.72 79.24 80.23
JC(Train)+ CC(Test) 55.62 41.71 31.32 62.40
CC(Train)+ JC(Test) 57.57 23.64 16.94 39.11
</table>
<tableCaption confidence="0.999219">
Table 4: The Cross-domain Results of our Method
</tableCaption>
<bodyText confidence="0.999963352941176">
by our technique, the MT results are not natural and
may contain errors as well.
The experiment was conducted using 10-fold
cross validation on two LDC data, low-ranked and
high-ranked data9. The results using SVM as classi-
fication model are given in Table 5. As expected, the
classification accuracy on low-ranked data is higher
than that on high-ranked data since low-ranked MT
results are more different from human references
than high-ranked MT results. We also found that
LSPs are the most effective features. In addition, our
discovered LSPs could indicate the common errors
made by the MT systems and provide some sugges-
tions for improving machine translation results.
As a summary, the mined LSPs are indeed effec-
tive for the classification models and our proposed
technique is effective.
</bodyText>
<sectionHeader confidence="0.998743" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998588">
This paper proposed a new approach to identifying
erroneous/correct sentences. Empirical evaluating
using diverse data demonstrated the effectiveness of
</bodyText>
<footnote confidence="0.9781215">
9One LDC data contains 14,604 low ranked (score 1-3) ma-
chine translations and the corresponding human references; the
other LDC data contains 808 high ranked (score 3-5) machine
translations and the corresponding human references
</footnote>
<page confidence="0.993713">
87
</page>
<table confidence="0.9997854">
Data Feature A (-)F (-)R (-)P (+)F (+)R (+)P
Low-ranked data (1-3 score) LSP 84.20 83.95 82.19 85.82 84.44 86.25 82.73
LSP+LC+PLM+SC+FWD 86.60 86.84 88.96 84.83 86.35 84.27 88.56
High-ranked data (3-5 score) LSP 71.74 73.01 79.56 67.59 70.23 64.47 77.40
LSP+LC+PLM+SC+FWD 72.87 73.68 68.95 69.20 71.92 67.22 77.60
</table>
<tableCaption confidence="0.999671">
Table 5: The Results on Machine Translation Data
</tableCaption>
<bodyText confidence="0.999868230769231">
our techniques. Moreover, we proposed to mine
LSPs as the input of classification models from a set
of data containing correct and erroneous sentences.
The LSPs were shown to be much more effective than
the other linguistic features although the other fea-
tures were also beneficial.
We will investigate the following problems in the
future: (1) to make use of the discovered LSPs to pro-
vide detailed feedback for ESL learners, e.g. the er-
rors in a sentence and suggested corrections; (2) to
integrate the features effectively to achieve better re-
sults; (3) to further investigate the application of our
techniques for MT evaluation.
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999563">
Rakesh Agrawal and Ramakrishnan Srikant. 1995. Mining se-
quential patterns. In ICDE.
Emily M. Bender, Dan Flickinger, Stephan Oepen, Annemarie
Walsh, and Timothy Baldwin. 2004. Arboretum: Using a
precision grammar for grammmar checking in call. In Proc.
InSTIL/ICALL Symposium on Computer Assisted Learning.
Chris Brockett, William Dolan, and Michael Gamon. 2006.
Correcting esl errors using phrasal smt techniques. In ACL.
Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263–311.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Martin
Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998.
Automated scoring using a hybrid feature identification tech-
nique. In Proc. ACL.
Martin Chodorow and Claudia Leacock. 2000. An unsuper-
vised method for detecting grammatical errors. In NAACL.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. ACL.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proc. ACL.
P.W. Foltz, D. Laham, and T.K. Landauer. 1999. Automated
essay scoring: Application to educational technology. In Ed-
Media ’99.
Michael Gamon, Anthony Aue, and Martine Smets. 2005.
Sentence-level mt evaluation without reference translations:
Beyond language modeling. In Proc. EAMT.
Shicun Gui and Huizhong Yang. 2003. Zhongguo Xuexizhe
Yingyu Yuliaohu. (Chinese Learner English Corpus). Shang-
hai: Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).
George E. Heidorn. 2000. Intelligent Writing Assistance.
Handbook of Natural Language Processing. Robert Dale,
Hermann Moisi and Harold Somers (ed.). Marcel Dekker.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Sup-
nithi, and Hitoshi Isahara. 2003. Automatic error detection
in the japanese learners’ english spoken data. In Proc. ACL.
Nitin Jindal and Bing Liu. 2006. Identifying comparative sen-
tences in text documents. In SIGIR.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In Proc. ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization.
Yajuan L¨u and Ming Zhou. 2004. Collocation translation ac-
quisition using monolingual corpora. In Proc. ACL.
Lisa N. Michaud, Kathleen F. McCoy, and Christopher A. Pen-
nington. 2000. An intelligent tutoring system for deaf learn-
ers of written english. In Proc. 4th International ACM Con-
ference on Assistive Technologies.
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and Naoki Isu.
2006. A feedback-augmented method for detecting errors in
the writing of learners of english. In Proc. ACL.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Du-
rand. 1999. Cross-language information retrieval based on
parallel texts and automatic mining of parallel texts from the
web. In SIGIR, pages 74–81.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, and Helen Pinto.
2001. Prefixspan: Mining sequential patterns efficiently by
prefix-projected pattern growth. In Proc. ICDE.
Yongmei Shi and Lina Zhou. 2005. Error detection using lin-
guistic features. In HLT/EMNLP.
Andreas Stolcke. 2002. Srilm-an extensible language modeling
toolkit. In Proc. ICSLP.
Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and Ming
Zhou. 2007. Mining sequential patterns and tree patterns to
detect erroneous sentences. In AAAI.
Tono Yukio, T. Kaneko, H. Isahara, T. Saiga, and E. Izumi.
2001. The standard speaking test corpus: A 1 million-word
spoken corpus of japanese learners of english and its impli-
cations for l2 lexicography. In ASIALEX: Asian Bilingualism
and the Dictionary.
</reference>
<page confidence="0.999389">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928116">
<title confidence="0.998241">Detecting Erroneous Sentences using Automatically Mined Sequential Patterns</title>
<author confidence="0.999273">Sun Liu Gao Cong Ming Zhou</author>
<affiliation confidence="0.998914">Chongqing University Microsoft Research Asia</affiliation>
<email confidence="0.962826">gaocong,</email>
<author confidence="0.979704">Xiong John Lee Lin</author>
<affiliation confidence="0.991391">Chongqing University MIT Microsoft Research Asia</affiliation>
<email confidence="0.99787">zyxiong@cqu.edu.cnjsylee@mit.educyl@microsoft.com</email>
<abstract confidence="0.999790923076923">This paper studies the problem of identifying erroneous/correct sentences. The problem has important applications, e.g., providing feedback for writers of English as a Second Language, controlling the quality of parallel bilingual sentences mined from the Web, and evaluating machine translation results. In this paper, we propose a new approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models. Experimental results show that our techniques are promising.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Ramakrishnan Srikant</author>
</authors>
<title>Mining sequential patterns.</title>
<date>1995</date>
<booktitle>In ICDE.</booktitle>
<contexts>
<context position="9033" citStr="Agrawal and Srikant, 1995" startWordPosition="1392" endWordPosition="1395">akes while MT outputs normally pro- niques to capture such an error. For example, Nduce locally well-formed phrases with overall gram- gram language model is considered to be effective matically wrong sentences. Hence, the manual fea- in writing evaluation (Burstein et al., 1998; Corstontures designed for MT evaluation are not applicable Oliver et al., 2001). However, it becomes very exto detect erroneous sentences from ESL learners. pensive if N &gt; 3 and N-grams only consider continLSPs differ from the traditional sequential pat- uous sequence of words, which is unable to detect terns, e.g., (Agrawal and Srikant, 1995; Pei et al., the above error “if...will...will”. 2001) in that LSPs are attached with class labels and We propose labeled sequential patterns to effecwe prefer those with discriminating ability to build tively characterize the features of correct and erclassification model. In our other work (Sun et al., roneous sentences (Section 3.2), and design some 2007), labeled sequential patterns, together with la- complementary features ( Section 3.3). beled tree patterns, are used to build pattern-based 3.2 Mining Labeled Sequential Patterns ( LSP ) classifier to detect erroneous sentences. The clas-</context>
</contexts>
<marker>Agrawal, Srikant, 1995</marker>
<rawString>Rakesh Agrawal and Ramakrishnan Srikant. 1995. Mining sequential patterns. In ICDE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
<author>Annemarie Walsh</author>
<author>Timothy Baldwin</author>
</authors>
<title>Arboretum: Using a precision grammar for grammmar checking in call.</title>
<date>2004</date>
<booktitle>In Proc. InSTIL/ICALL Symposium on Computer Assisted Learning.</booktitle>
<contexts>
<context position="2475" citStr="Bender et al., 2004" startWordPosition="362" endWordPosition="365">o find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), article usage, etc. The previous work focuses on grammar errors, including tense, agreement, verb formation, article usage, etc. However, little work has been done to detect sentence structure and lexical collocation errors. Some methods of detecting erroneous sentences are based on manual rules. These methods (Heidorn, 2000; Michaud et al., 2000; Bender et al., 2004) have been shown to be effective in detecting certain kinds of grammatical errors in the writing of English learners. However, it could be expensive to write rules manually. Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for examp</context>
<context position="6154" citStr="Bender et al., 2004" startWordPosition="936" endWordPosition="939">also apply our learning model to machine translation (MT) data as a complementary measure to evaluate MT results. The rest of this paper is organized as follows. The next section discusses related work. Section 3 presents the proposed technique. We evaluate our proposed technique in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work Research on detecting erroneous sentences can be classified into two categories. The first category makes use of hand-crafted rules, e.g., template rules (Heidorn, 2000) and mal-rules in context-free grammars (Michaud et al., 2000; Bender et al., 2004). As discussed in Section 1, manual rule based methods have some shortcomings. The second category uses statistical techniques to detect erroneous sentences. An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. The method (Izumi et al., 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. They also require specifying error tags that can tell the speci</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Walsh, Baldwin, 2004</marker>
<rawString>Emily M. Bender, Dan Flickinger, Stephan Oepen, Annemarie Walsh, and Timothy Baldwin. 2004. Arboretum: Using a precision grammar for grammmar checking in call. In Proc. InSTIL/ICALL Symposium on Computer Assisted Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William Dolan</author>
<author>Michael Gamon</author>
</authors>
<title>Correcting esl errors using phrasal smt techniques.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3317" citStr="Brockett et al., 2006" startWordPosition="501" endWordPosition="504"> high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003; Brockett et al., 2006) usually need errors to be specified and tagged in the training sentences, which requires expert help to be recruited and is time </context>
<context position="6945" citStr="Brockett et al., 2006" startWordPosition="1056" endWordPosition="1059">ised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. The method (Izumi et al., 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. They also require specifying error tags that can tell the specific errors and their corrections in the training corpus. The phrasal Statistical Machine Translation (SMT) technique is employed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contrast to existing statistical methods, our technique needs neither errors tagged nor parallel corpora, and is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>Chris Brockett, William Dolan, and Michael Gamon. 2006. Correcting esl errors using phrasal smt techniques. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter E Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1316" citStr="Brown et al., 1993" startWordPosition="181" endWordPosition="184">r, we propose a new approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models. Experimental results show that our techniques are promising. 1 Introduction Detecting erroneous/correct sentences has the following applications. First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors. Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation (Brown et al., 1993) and cross-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. �Work done while the author was a visiting student at MSRA †Work done while the author was a visiting student at MSRA 81 The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mi</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Karen Kukich</author>
<author>Susanne Wolff</author>
<author>Chi Lu</author>
<author>Martin Chodorow</author>
<author>Lisa Braden-Harder</author>
<author>Mary Dee Harris</author>
</authors>
<title>Automated scoring using a hybrid feature identification technique.</title>
<date>1998</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="7500" citStr="Burstein et al., 1998" startWordPosition="1151" endWordPosition="1154">oyed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contrast to existing statistical methods, our technique needs neither errors tagged nor parallel corpora, and is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) 82 to evaluate the well-formedness of machine transla- models. We illustrate the challenge with an examtion outputs. The wr</context>
</contexts>
<marker>Burstein, Kukich, Wolff, Lu, Chodorow, Braden-Harder, Harris, 1998</marker>
<rawString>Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Martin Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998. Automated scoring using a hybrid feature identification technique. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>An unsupervised method for detecting grammatical errors.</title>
<date>2000</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3274" citStr="Chodorow and Leacock, 2000" startWordPosition="492" endWordPosition="496"> Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003; Brockett et al., 2006) usually need errors to be specified and tagged in the training sentences, which requir</context>
<context position="5476" citStr="Chodorow and Leacock, 2000" startWordPosition="829" endWordPosition="832">cluding lexical collocation, language model, syntactic score, and function word density. In contrast with previous work focusing on (a specific type of) grammatical errors, our model can handle a wide range of errors, including grammar, sentence structure, and lexical choice. • We empirically evaluate our methods on two datasets consisting of sentences written by Japanese and Chinese, respectively. Experimental results show that labeled sequential patterns are highly useful for the classification results, and greatly outperform other features. Our method outperforms Microsoft Word03 and ALEK (Chodorow and Leacock, 2000) from Educational Testing Service (ETS) in some cases. We also apply our learning model to machine translation (MT) data as a complementary measure to evaluate MT results. The rest of this paper is organized as follows. The next section discusses related work. Section 3 presents the proposed technique. We evaluate our proposed technique in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work Research on detecting erroneous sentences can be classified into two categories. The first category makes use of hand-crafted rules, e.g., template rules (Heidorn, 2000) and </context>
<context position="19906" citStr="Chodorow and Leacock, 2000" startWordPosition="3179" endWordPosition="3182">glish Essay (-) HEL (Hiroshima English 17,301 Learners’ Corpus) and JLE (Japanese Learners of English Corpus) CC (+) the 21st Century newspaper 3,200 (-) CLEC (Chinese Learner Er- 3,199 ror Corpus) Table 1: Corpora ((+): correct; (-): erroneous) respectively as 7 features. 4 Experimental Evaluation We evaluated the performance of our techniques with support vector machine (SVM) and Naive Bayesian (NB) classification models. We also compared the effectiveness of various features. In addition, we compared our technique with two other methods of checking errors, Microsoft Word03 and ALEK method (Chodorow and Leacock, 2000). Finally, we also applied our technique to evaluate the Machine Translation outputs. 4.1 Experimental Setup Classification Models. We used two classification models, SVM6 and NB classification model. Data. We collected two datasets from different domains, Japanese Corpus (JC) and Chinese Corpus (CC). Table 1 gives the details of our corpora. In the learner’s corpora, all of the sentences are erroneous. Note that our data does not consist of parallel pairs of sentences (one error sentence and its correction). The erroneous sentences includes grammar, sentence structure and lexical choice error</context>
<context position="24078" citStr="Chodorow and Leacock, 2000" startWordPosition="3850" endWordPosition="3853">ors. A sentence detected to be erroneous by one feature may also be detected by another feature; and (2) Various features give conflicting results. The two aspects suggest the directions of our future efforts to improve the performance of our models. Comparing with Other Methods. It is difficult to find benchmark methods to compare with our technique because, as discussed in Section 2, existing methods often require error tagged corpora or parallel corpora, or focus on a specific type of errors. In this paper, we compare our technique with the grammar checker of Microsoft Word03 and the ALEK (Chodorow and Leacock, 2000) method used by ETS. ALEK is used to detect inappropriate usage of specific vocabulary words. Note that we do not consider spelling errors. Due to space limitation, we only report the precision, recall, F-score for erroneous sentences, and the overall accuracy. As can be seen from Table 3, our method outperforms the other two methods in terms of overall accuracy, F-score, and recall, while the three methods achieve comparable precision. We realize that the grammar checker of Word is a general tool and the performance of ALEK (Chodorow and Leacock, 2000) can be improved if larger training data </context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>Martin Chodorow and Claudia Leacock. 2000. An unsupervised method for detecting grammatical errors. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="18475" citStr="Collins, 1997" startWordPosition="2959" endWordPosition="2960">rom Language Model (PLM) Perplexity measures are extracted from a trigram language model trained on a general English corpus using the SRILM-SRI Language Modeling Toolkit (Stolcke, 2002). We calculate two values for each sentence: lexicalized trigram perplexity and part of speech (POS) trigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more likely to receive lower scores. Function Word Density (FWD) We consider the density of function words (Corston-Oliver et al., 2001), i.e. the ratio of function words to content words. This is inspired by the work (Corston-Oliver et al., 2001) showing that function word density can be effective in distinguishing between human references and machine outputs. In this paper, we calculate the densities of seven kinds of function w</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
<author>Chris Brockett</author>
</authors>
<title>A machine learning approach to the automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="7955" citStr="Corston-Oliver et al., 2001" startWordPosition="1219" endWordPosition="1222">nd is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) 82 to evaluate the well-formedness of machine transla- models. We illustrate the challenge with an examtion outputs. The writers of ESL and MT normally ple. Consider an erroneous sentence, “If Maggie will make different mistakes: in general, ESL writers can go to supermarket, she will buy a bag for you.” It is write overall grammatically correct sentences with difficult for previous methods using statistical techsome local mistakes while MT outputs normally pro- niques to capture such an error. For example, Nduce locally well-formed phrases with overall gram- gram languag</context>
<context position="18777" citStr="Corston-Oliver et al., 2001" startWordPosition="3003" endWordPosition="3006">rigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more likely to receive lower scores. Function Word Density (FWD) We consider the density of function words (Corston-Oliver et al., 2001), i.e. the ratio of function words to content words. This is inspired by the work (Corston-Oliver et al., 2001) showing that function word density can be effective in distinguishing between human references and machine outputs. In this paper, we calculate the densities of seven kinds of function words 5 5including determiners/yantifiers, all pronouns, different pronoun types: Wh, 1st, 2&amp;quot; , and 3rd person pronouns, prepoDataset Type Source Number JC (+) the Japan Times newspaper 16,857 and Model English Essay (-) HEL (Hiroshima English 17,301 Learners’ Corpus) and JLE (Japanese Learners of Engl</context>
</contexts>
<marker>Corston-Oliver, Gamon, Brockett, 2001</marker>
<rawString>Simon Corston-Oliver, Michael Gamon, and Chris Brockett. 2001. A machine learning approach to the automatic evaluation of machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
<author>D Laham</author>
<author>T K Landauer</author>
</authors>
<title>Automated essay scoring: Application to educational technology.</title>
<date>1999</date>
<booktitle>In EdMedia ’99.</booktitle>
<contexts>
<context position="7575" citStr="Foltz et al., 1999" startWordPosition="1163" endWordPosition="1166">od must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contrast to existing statistical methods, our technique needs neither errors tagged nor parallel corpora, and is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) 82 to evaluate the well-formedness of machine transla- models. We illustrate the challenge with an examtion outputs. The writers of ESL and MT normally ple. Consider an erroneous sentence, “If Maggi</context>
</contexts>
<marker>Foltz, Laham, Landauer, 1999</marker>
<rawString>P.W. Foltz, D. Laham, and T.K. Landauer. 1999. Automated essay scoring: Application to educational technology. In EdMedia ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Martine Smets</author>
</authors>
<title>Sentence-level mt evaluation without reference translations: Beyond language modeling.</title>
<date>2005</date>
<booktitle>In Proc. EAMT.</booktitle>
<contexts>
<context position="1507" citStr="Gamon et al., 2005" startWordPosition="211" endWordPosition="214">Introduction Detecting erroneous/correct sentences has the following applications. First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors. Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation (Brown et al., 1993) and cross-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. �Work done while the author was a visiting student at MSRA †Work done while the author was a visiting student at MSRA 81 The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), ar</context>
<context position="7976" citStr="Gamon et al., 2005" startWordPosition="1223" endWordPosition="1226">ic type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a sentence is done by grammar, sentence structure, and lexical choice. Another related work is Machine Translation (MT) evaluation. Classification models are employed in (Corston-Oliver et al., 2001; Gamon et al., 2005) 82 to evaluate the well-formedness of machine transla- models. We illustrate the challenge with an examtion outputs. The writers of ESL and MT normally ple. Consider an erroneous sentence, “If Maggie will make different mistakes: in general, ESL writers can go to supermarket, she will buy a bag for you.” It is write overall grammatically correct sentences with difficult for previous methods using statistical techsome local mistakes while MT outputs normally pro- niques to capture such an error. For example, Nduce locally well-formed phrases with overall gram- gram language model is considered</context>
</contexts>
<marker>Gamon, Aue, Smets, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level mt evaluation without reference translations: Beyond language modeling. In Proc. EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shicun Gui</author>
<author>Huizhong Yang</author>
</authors>
<title>Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai: Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</title>
<date>2003</date>
<contexts>
<context position="1963" citStr="Gui and Yang, 2003" startWordPosition="286" endWordPosition="289">on retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. �Work done while the author was a visiting student at MSRA †Work done while the author was a visiting student at MSRA 81 The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), article usage, etc. The previous work focuses on grammar errors, including tense, agreement, verb formation, article usage, etc. However, little work has been done to detect sentence structure and lexical collocation errors. Some methods of detecting erroneous sentences are based on manual rules. These methods (Heidorn, 2000; Michaud et al., 2000; Bender et al., 2004) have been shown to be effective in detecting certain kinds of grammatical errors in the</context>
<context position="16510" citStr="Gui and Yang, 2003" startWordPosition="2641" endWordPosition="2644">important working language”, “&lt;although, but&gt;”(e.g. contained in “although he likes it, but he can’t buy it.”), and “&lt;only, if, I, am&gt;”(e.g. contained in “only if my teacher has given permission, I am allowed to enter this room”). (2) LSPs for correct sentences. For instance, “&lt;would, VB&gt;”(e.g. contained in “he would buy it.”), and “&lt;VBD, yeserday&gt;”(e.g. contained in “I bought this book yesterday.”). 3.3 Other Linguistic Features We use some linguistic features that can be computed automatically as complementary features. Lexical Collocation (LC) Lexical collocation error (Yukio et al., 2001; Gui and Yang, 2003) is common in the writing of ESL learners, such as “strong tea” but not “powerful tea.” Our LSP features cannot capture all LCs since we replace some words with POS tags in mining LSPs. We collect five types of collocations: verb-object, adjective-noun, verbadverb, subject-verb, and preposition-object from a general English corpus4. Correct LCs are collected 4The general English corpus consists of about 4.4 million native sentences. 84 by extracting collocations of high frequency from the general English corpus. Erroneous LC candidates are generated by replacing the word in correct collocation</context>
</contexts>
<marker>Gui, Yang, 2003</marker>
<rawString>Shicun Gui and Huizhong Yang. 2003. Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai: Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Heidorn</author>
</authors>
<title>Intelligent Writing Assistance. Handbook of Natural Language Processing.</title>
<date>2000</date>
<editor>Robert Dale, Hermann Moisi and Harold Somers (ed.).</editor>
<publisher>Marcel Dekker.</publisher>
<contexts>
<context position="2431" citStr="Heidorn, 2000" startWordPosition="355" endWordPosition="357">ing erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), article usage, etc. The previous work focuses on grammar errors, including tense, agreement, verb formation, article usage, etc. However, little work has been done to detect sentence structure and lexical collocation errors. Some methods of detecting erroneous sentences are based on manual rules. These methods (Heidorn, 2000; Michaud et al., 2000; Bender et al., 2004) have been shown to be effective in detecting certain kinds of grammatical errors in the writing of English learners. However, it could be expensive to write rules manually. Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write </context>
<context position="6071" citStr="Heidorn, 2000" startWordPosition="925" endWordPosition="926"> and Leacock, 2000) from Educational Testing Service (ETS) in some cases. We also apply our learning model to machine translation (MT) data as a complementary measure to evaluate MT results. The rest of this paper is organized as follows. The next section discusses related work. Section 3 presents the proposed technique. We evaluate our proposed technique in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work Research on detecting erroneous sentences can be classified into two categories. The first category makes use of hand-crafted rules, e.g., template rules (Heidorn, 2000) and mal-rules in context-free grammars (Michaud et al., 2000; Bender et al., 2004). As discussed in Section 1, manual rule based methods have some shortcomings. The second category uses statistical techniques to detect erroneous sentences. An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. The method (Izumi et al., 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech r</context>
</contexts>
<marker>Heidorn, 2000</marker>
<rawString>George E. Heidorn. 2000. Intelligent Writing Assistance. Handbook of Natural Language Processing. Robert Dale, Hermann Moisi and Harold Somers (ed.). Marcel Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emi Izumi</author>
</authors>
<title>Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi, and Hitoshi Isahara.</title>
<date>2003</date>
<booktitle>Proc. ACL.</booktitle>
<marker>Izumi, 2003</marker>
<rawString>Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi, and Hitoshi Isahara. 2003. Automatic error detection in the japanese learners’ english spoken data. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Identifying comparative sentences in text documents.</title>
<date>2006</date>
<booktitle>In SIGIR.</booktitle>
<marker>Jindal, Liu, 2006</marker>
<rawString>Nitin Jindal and Bing Liu. 2006. Identifying comparative sentences in text documents. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="18383" citStr="Liu and Gildea, 2005" startWordPosition="2942" endWordPosition="2945">e ratio of the number of erroneous LCs to the number of collocations in each sentence. Perplexity from Language Model (PLM) Perplexity measures are extracted from a trigram language model trained on a general English corpus using the SRILM-SRI Language Modeling Toolkit (Stolcke, 2002). We calculate two values for each sentence: lexicalized trigram perplexity and part of speech (POS) trigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more likely to receive lower scores. Function Word Density (FWD) We consider the density of function words (Corston-Oliver et al., 2001), i.e. the ratio of function words to content words. This is inspired by the work (Corston-Oliver et al., 2001) showing that function word density can be effective in distinguishing between human references</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan L¨u</author>
<author>Ming Zhou</author>
</authors>
<title>Collocation translation acquisition using monolingual corpora.</title>
<date>2004</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>L¨u, Zhou, 2004</marker>
<rawString>Yajuan L¨u and Ming Zhou. 2004. Collocation translation acquisition using monolingual corpora. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa N Michaud</author>
<author>Kathleen F McCoy</author>
<author>Christopher A Pennington</author>
</authors>
<title>An intelligent tutoring system for deaf learners of written english.</title>
<date>2000</date>
<booktitle>In Proc. 4th International ACM Conference on Assistive Technologies.</booktitle>
<contexts>
<context position="2453" citStr="Michaud et al., 2000" startWordPosition="358" endWordPosition="361">entences mainly aims to find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), article usage, etc. The previous work focuses on grammar errors, including tense, agreement, verb formation, article usage, etc. However, little work has been done to detect sentence structure and lexical collocation errors. Some methods of detecting erroneous sentences are based on manual rules. These methods (Heidorn, 2000; Michaud et al., 2000; Bender et al., 2004) have been shown to be effective in detecting certain kinds of grammatical errors in the writing of English learners. However, it could be expensive to write rules manually. Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammat</context>
<context position="6132" citStr="Michaud et al., 2000" startWordPosition="932" endWordPosition="935">TS) in some cases. We also apply our learning model to machine translation (MT) data as a complementary measure to evaluate MT results. The rest of this paper is organized as follows. The next section discusses related work. Section 3 presents the proposed technique. We evaluate our proposed technique in Section 4. Section 5 concludes this paper and discusses future work. 2 Related Work Research on detecting erroneous sentences can be classified into two categories. The first category makes use of hand-crafted rules, e.g., template rules (Heidorn, 2000) and mal-rules in context-free grammars (Michaud et al., 2000; Bender et al., 2004). As discussed in Section 1, manual rule based methods have some shortcomings. The second category uses statistical techniques to detect erroneous sentences. An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. The method (Izumi et al., 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. They also require specifying error tags t</context>
</contexts>
<marker>Michaud, McCoy, Pennington, 2000</marker>
<rawString>Lisa N. Michaud, Kathleen F. McCoy, and Christopher A. Pennington. 2000. An intelligent tutoring system for deaf learners of written english. In Proc. 4th International ACM Conference on Assistive Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Nagata</author>
<author>Atsuo Kawai</author>
<author>Koichiro Morihiro</author>
<author>Naoki Isu</author>
</authors>
<title>A feedback-augmented method for detecting errors in the writing of learners of english.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="3167" citStr="Nagata et al., 2006" startWordPosition="478" endWordPosition="481">al errors in the writing of English learners. However, it could be expensive to write rules manually. Linguistic experts are needed to write rules of high quality; Also, it is difficult to produce and maintain a large number of non-conflicting rules to cover a wide range of grammatical errors. Moreover, ESL writers of different first-language backgrounds and skill levels may make different errors, and thus different sets of rules may be required. Worse still, it is hard to write rules for some grammatical errors, for example, detecting errors concerning the articles and singular plural usage (Nagata et al., 2006). Instead of asking experts to write hand-crafted rules, statistical approaches (Chodorow and Leacock, 2000; Izumi et al., 2003; Brockett et al., 2006; Nagata et al., 2006) build statistical models to identify sentences containing errors. However, existing Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics statistical approaches focus on some pre-defined errors and the reported results are not attractive. Moreover, these approaches, e.g., (Izumi et al., 2003; Br</context>
<context position="7160" citStr="Nagata et al., 2006" startWordPosition="1094" endWordPosition="1097">ement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. They also require specifying error tags that can tell the specific errors and their corrections in the training corpus. The phrasal Statistical Machine Translation (SMT) technique is employed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contrast to existing statistical methods, our technique needs neither errors tagged nor parallel corpora, and is not limited to a specific type of grammatical error. There are also studies on automatic essay scoring at document-level. For example, E-rater (Burstein et al., 1998), developed by the ETS, and Intelligent Essay Assessor (Foltz et al., 1999). The evaluation criteria for documents are different from those for sentences. A document is evaluated mainly by its organization, topic, diversity of vocabulary, and grammar while a s</context>
</contexts>
<marker>Nagata, Kawai, Morihiro, Isu, 2006</marker>
<rawString>Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and Naoki Isu. 2006. A feedback-augmented method for detecting errors in the writing of learners of english. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Yun Nie</author>
<author>Michel Simard</author>
<author>Pierre Isabelle</author>
<author>Richard Durand</author>
</authors>
<title>Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web. In</title>
<date>1999</date>
<booktitle>SIGIR,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="1375" citStr="Nie et al., 1999" startWordPosition="189" endWordPosition="192"> by integrating pattern discovery with supervised learning models. Experimental results show that our techniques are promising. 1 Introduction Detecting erroneous/correct sentences has the following applications. First, it can provide feedback for writers of English as a Second Language (ESL) as to whether a sentence contains errors. Second, it can be applied to control the quality of parallel bilingual sentences mined from the Web, which are critical sources for a wide range of applications, such as statistical machine translation (Brown et al., 1993) and cross-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. �Work done while the author was a visiting student at MSRA †Work done while the author was a visiting student at MSRA 81 The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL</context>
</contexts>
<marker>Nie, Simard, Isabelle, Durand, 1999</marker>
<rawString>Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web. In SIGIR, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Pei</author>
<author>Jiawei Han</author>
<author>Behzad Mortazavi-Asl</author>
<author>Helen Pinto</author>
</authors>
<title>Prefixspan: Mining sequential patterns efficiently by prefix-projected pattern growth.</title>
<date>2001</date>
<booktitle>In Proc. ICDE.</booktitle>
<contexts>
<context position="14347" citStr="Pei et al., 2001" startWordPosition="2320" endWordPosition="2323"> key word list, then the word will be replaced by its POS. The processed sentence consists of POS and the words of key word list. For example, after the processing, the sentence “In the past, John was kind to his sister” is converted into “In the past, NNP was JJ to his NN”, where the words “in”, “the”, “was”, “to” and “his” are function words, the word “past” is time word, and “NNP”, “JJ”, and “NN” are POS tags. Mining LSPs. The length of the discovered LSPs is flexible and they can be composed of contiguous or distant words/tags. Existing frequent sequential pattern mining algorithms (e.g. (Pei et al., 2001)) use minimum support threshold to mine frequent sequential patterns whose support is larger than the threshold. These algorithms are not sufficient for our problem of mining LSPs. In order to ensure that all our discovered LSPs are discriminating and are capa1http://www.marlodge.supanet.com/museum/funcword.html 2http://www.wjh.harvard.edu/%7Einquirer/Time%40.html 3http://www.cogsci.ed.ac.uk/—jamesc/taggers/MXPOST.html ble of predicting correct or erroneous sentences, we impose another constraint minimum confidence. Recall that the higher the confidence of a pattern is, the better it can disti</context>
</contexts>
<marker>Pei, Han, Mortazavi-Asl, Pinto, 2001</marker>
<rawString>Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, and Helen Pinto. 2001. Prefixspan: Mining sequential patterns efficiently by prefix-projected pattern growth. In Proc. ICDE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongmei Shi</author>
<author>Lina Zhou</author>
</authors>
<title>Error detection using linguistic features.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="6626" citStr="Shi and Zhou, 2005" startWordPosition="1005" endWordPosition="1008"> use of hand-crafted rules, e.g., template rules (Heidorn, 2000) and mal-rules in context-free grammars (Michaud et al., 2000; Bender et al., 2004). As discussed in Section 1, manual rule based methods have some shortcomings. The second category uses statistical techniques to detect erroneous sentences. An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. The method (Izumi et al., 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. They also require specifying error tags that can tell the specific errors and their corrections in the training corpus. The phrasal Statistical Machine Translation (SMT) technique is employed to identify and correct writing errors (Brockett et al., 2006). This method must collect a large number of parallel corpora (pairs of erroneous sentences and their corrections) and performance depends on SMT techniques that are not yet mature. The work in (Nagata et al., 2006) focuses on a type of error, namely mass vs. count nouns. In contr</context>
</contexts>
<marker>Shi, Zhou, 2005</marker>
<rawString>Yongmei Shi and Lina Zhou. 2005. Error detection using linguistic features. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="18047" citStr="Stolcke, 2002" startWordPosition="2894" endWordPosition="2895">cations in each sentence, and probability p(coi) of each CL coi is calculated using the method (L¨u and Zhou, 2004). (2) The second feature is computed by the ratio of the number of unknown collocations (neither correct LCs nor erroneous LCs) to the number of collocations in each sentence. (3) The last feature is computed by the ratio of the number of erroneous LCs to the number of collocations in each sentence. Perplexity from Language Model (PLM) Perplexity measures are extracted from a trigram language model trained on a general English corpus using the SRILM-SRI Language Modeling Toolkit (Stolcke, 2002). We calculate two values for each sentence: lexicalized trigram perplexity and part of speech (POS) trigram perplexity. The erroneous sentences would have higher perplexity. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but cannot form coherent sentences (Liu and Gildea, 2005). To measure the coherence of sentences, we use a statistical parser Toolkit (Collins, 1997) to assign each sentence a parser’s score that is the related log probability of parsing. We assume that erroneous sentences with undesirable sentence structures are more l</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihua Sun</author>
<author>Gao Cong</author>
<author>Xiaohua Liu</author>
<author>Chin-Yew Lin</author>
<author>Ming Zhou</author>
</authors>
<title>Mining sequential patterns and tree patterns to detect erroneous sentences.</title>
<date>2007</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="9720" citStr="Sun et al., 2007" startWordPosition="1497" endWordPosition="1500">re attached with class labels and We propose labeled sequential patterns to effecwe prefer those with discriminating ability to build tively characterize the features of correct and erclassification model. In our other work (Sun et al., roneous sentences (Section 3.2), and design some 2007), labeled sequential patterns, together with la- complementary features ( Section 3.3). beled tree patterns, are used to build pattern-based 3.2 Mining Labeled Sequential Patterns ( LSP ) classifier to detect erroneous sentences. The clas- Labeled Sequential Patterns (LSP). A labeled sesification method in (Sun et al., 2007) is different quential pattern, p, is in the form of LHS —* c, where from those used in this paper. Moreover, instead of LHS is a sequence and c is a class label. Let I be a labeled sequential patterns, in (Sun et al., 2007) the set of items and L be a set of class labels. Let D be a most significant k labeled sequential patterns with sequence database in which each tuple is composed constraints for each training sentence are mined to of a list of items in I and a class label in L. We say build classifiers. Another related work is (Jindal and that a sequence s1 =&lt; a1,..., am &gt; is contained in </context>
</contexts>
<marker>Sun, Cong, Liu, Lin, Zhou, 2007</marker>
<rawString>Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and Ming Zhou. 2007. Mining sequential patterns and tree patterns to detect erroneous sentences. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tono Yukio</author>
<author>T Kaneko</author>
<author>H Isahara</author>
<author>T Saiga</author>
<author>E Izumi</author>
</authors>
<title>The standard speaking test corpus: A 1 million-word spoken corpus of japanese learners of english and its implications for l2 lexicography.</title>
<date>2001</date>
<booktitle>In ASIALEX: Asian Bilingualism and the Dictionary.</booktitle>
<contexts>
<context position="1942" citStr="Yukio et al., 2001" startWordPosition="282" endWordPosition="285">ss-lingual information retrieval (Nie et al., 1999). Third, it can be used to evaluate machine translation results. As demonstrated in (CorstonOliver et al., 2001; Gamon et al., 2005), the better human reference translations can be distinguished from machine translations by a classification model, the worse the machine translation system is. �Work done while the author was a visiting student at MSRA †Work done while the author was a visiting student at MSRA 81 The previous work on identifying erroneous sentences mainly aims to find errors from the writing of ESL learners. The common mistakes (Yukio et al., 2001; Gui and Yang, 2003) made by ESL learners include spelling, lexical collocation, sentence structure, tense, agreement, verb formation, wrong PartOf-Speech (POS), article usage, etc. The previous work focuses on grammar errors, including tense, agreement, verb formation, article usage, etc. However, little work has been done to detect sentence structure and lexical collocation errors. Some methods of detecting erroneous sentences are based on manual rules. These methods (Heidorn, 2000; Michaud et al., 2000; Bender et al., 2004) have been shown to be effective in detecting certain kinds of gram</context>
<context position="16489" citStr="Yukio et al., 2001" startWordPosition="2637" endWordPosition="2640">ed in “it is one of important working language”, “&lt;although, but&gt;”(e.g. contained in “although he likes it, but he can’t buy it.”), and “&lt;only, if, I, am&gt;”(e.g. contained in “only if my teacher has given permission, I am allowed to enter this room”). (2) LSPs for correct sentences. For instance, “&lt;would, VB&gt;”(e.g. contained in “he would buy it.”), and “&lt;VBD, yeserday&gt;”(e.g. contained in “I bought this book yesterday.”). 3.3 Other Linguistic Features We use some linguistic features that can be computed automatically as complementary features. Lexical Collocation (LC) Lexical collocation error (Yukio et al., 2001; Gui and Yang, 2003) is common in the writing of ESL learners, such as “strong tea” but not “powerful tea.” Our LSP features cannot capture all LCs since we replace some words with POS tags in mining LSPs. We collect five types of collocations: verb-object, adjective-noun, verbadverb, subject-verb, and preposition-object from a general English corpus4. Correct LCs are collected 4The general English corpus consists of about 4.4 million native sentences. 84 by extracting collocations of high frequency from the general English corpus. Erroneous LC candidates are generated by replacing the word i</context>
</contexts>
<marker>Yukio, Kaneko, Isahara, Saiga, Izumi, 2001</marker>
<rawString>Tono Yukio, T. Kaneko, H. Isahara, T. Saiga, and E. Izumi. 2001. The standard speaking test corpus: A 1 million-word spoken corpus of japanese learners of english and its implications for l2 lexicography. In ASIALEX: Asian Bilingualism and the Dictionary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>