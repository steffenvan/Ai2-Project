<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.429856">
<title confidence="0.975821">
Towards Reliability-Aware Entity Analytics and
Integration for Noisy Text at Scale
</title>
<author confidence="0.947994">
Sameep Mehta, L. Venkata Subramaniam
</author>
<affiliation confidence="0.64106">
IBM Research India
</affiliation>
<email confidence="0.98986">
sameepmehta,lvsubram@in.ibm.com
</email>
<sectionHeader confidence="0.999411" genericHeader="abstract">
1 Outline
</sectionHeader>
<bodyText confidence="0.99995364">
Due to easy to use apps (Facebook, Twitter, etc.), higher Internet connectivity and
always on facility allowed by smart phones, the key characteristics of raw data are
changing. This new data can be characterized by 4V’s - Volume, Velocity, Vari-
ety and Veracity. For example during a Football match, some people will Tweet
about goals, penalties, etc., while others may write longer blogs and further there
will be match reports filed in trusted online news media after the match. Although
the sources may be varied, the data describes and provides multiple evidences for
the same event. Such multiple evidences should be used to strengthen the belief
in the underlying physical event as the individual data points may have inherent
uncertainty. The uncertainty can arise from inconsistent, incomplete and ambigu-
ous reports. The uncertainty is also because the trust levels of the different sources
vary and affect the overall reliability. We will summarize various efforts to perform
reliability aware entity integration.
The other problem in text analysis in such setting is posed by presence of noise
in the text. Since the text is produced in several informal settings such as email,
blogs, tweet, SMS, chat and is inherently noisy and has several veracity issues.
For example, missing punctuation and the use of non-standard words can often
hinder standard natural language processing techniques such as part-of-speech tag-
ging and parsing. Further downstream applications such as entity extraction, entity
resolution and entity completion have to explicitly handle noise in order to return
useful results. Often, depending on the application, noise can be modeled and it
may be possible to develop specific strategies to immunize the system from the
effects of noise and improve performance. Also the aspect of reliability is key as a
lot of this data is ambiguous, incomplete, conflicting, untrustworthy and deceptive.
The key goals of this tutorial are:
</bodyText>
<page confidence="0.973737">
7
</page>
<bodyText confidence="0.854108">
Tutorials, NAACL-HLT 2013, pages 7–9,
Atlanta, Georgia, June 9 2013. c�2013 Association for Computational Linguistics
</bodyText>
<listItem confidence="0.9990124">
1. Draw the attention of researchers towards methods for doing entity analytics
and integration on data with 4V characteristics.
2. Differentiate between noise and uncertainty in such data.
3. Provide an in-depth discussion on handling noise in NLP based methods.
4. Finally, handling uncertainty through information fusion and integration.
</listItem>
<bodyText confidence="0.9985075">
This tutorial builds on two earlier tutorials: NAACL 2010 tutorial on Noisy
Text and COMAD 2012 tutorial on Reliability Aware Data Fusion. In parallel
the authors are also hosting a workshop on related topic ”Reliability Aware Data
Fusion” at SIAM Data Mining Conference, 2013.
</bodyText>
<sectionHeader confidence="0.99982" genericHeader="keywords">
2 Outline
</sectionHeader>
<subsectionHeader confidence="0.987692">
2.1 Data with 4V characteristics
</subsectionHeader>
<listItem confidence="0.9993115">
• Define Volume, Velocity, Variety and Veracity and metrics to quantify them
• Information extraction on data with 4V characteristics
</listItem>
<subsectionHeader confidence="0.988691">
2.2 Key technical challenges posed by the 4V dimensions and linguis-
</subsectionHeader>
<bodyText confidence="0.967278">
tics techniques to address them
</bodyText>
<listItem confidence="0.999965833333333">
• Analyzing streaming text
• Large scale distributed algorithms for NLP
• Integrating structured and unstructured data
• Noisy text analytics
• Reliability
• Use case: Generating single view of entity from social data
</listItem>
<subsectionHeader confidence="0.988743">
2.3 Computing Reliability and Trust
</subsectionHeader>
<listItem confidence="0.9979432">
• Computing source reliability
• Identifying Trust Worthy Messages
• Data fusion to improve reliability: Probabilistic data fusion, information
measures, evidential reasoning
• Use case: Event detection using social data, news and online sources
</listItem>
<page confidence="0.998074">
8
</page>
<sectionHeader confidence="0.979033" genericHeader="introduction">
3 Speaker Bios
</sectionHeader>
<bodyText confidence="0.998497166666667">
Sameep Mehta1 is researcher in Information Management Group at IBM Research
India. He received his M.S. and Ph.D. from The Ohio State University, USA in
2006. He also holds an Adjunct Faculty position at the International Institute of
Information Technology, New Delhi. Sameep regularly advises MS and PhD stu-
dents at University of Delhi and IIT Delhi. He regularly delivers Tutorials at CO-
MAD (2009, 2010 and 2011). His current research interests include Data Mining,
Business Analytics, Service Science, Text Mining, and Workforce Optimization.
L. Venkata Subramaniam2 manages the information management analytics
and solutions group at IBM Research India. He received his PhD from IIT Delhi
in 1999. His research focuses on unstructured information management, statistical
natural language processing, noisy text analytics, text and data mining, information
theory, speech and image processing. He often teaches and guides student thesis at
IIT Delhi on these topics. His tutorial titled Noisy Text Analytics was the second
largest at NAACL-HLT 2010. He co founded the AND (Analytics for Noisy Un-
structured Text Data) workshop series and also co-chaired the first four workshops,
2007-2010. He was guest co-editor of two special issues on Noisy Text Analytics
in the International Journal of Document Analysis and Recognition in 2007 and
2009.
</bodyText>
<footnote confidence="0.9998215">
1http://in.linkedin.com/in/sameepmehta
2https://sites.google.com/site/lvs004/
</footnote>
<page confidence="0.991707">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.020310">
<title confidence="0.9967775">Towards Reliability-Aware Entity Analytics Integration for Noisy Text at Scale</title>
<author confidence="0.999077">Sameep Mehta</author>
<author confidence="0.999077">L Venkata</author>
<affiliation confidence="0.999776">IBM Research India</affiliation>
<email confidence="0.967384">sameepmehta,lvsubram@in.ibm.com</email>
<abstract confidence="0.97633428">1 Outline Due to easy to use apps (Facebook, Twitter, etc.), higher Internet connectivity and always on facility allowed by smart phones, the key characteristics of raw data are changing. This new data can be characterized by 4V’s - Volume, Velocity, Variety and Veracity. For example during a Football match, some people will Tweet about goals, penalties, etc., while others may write longer blogs and further there will be match reports filed in trusted online news media after the match. Although the sources may be varied, the data describes and provides multiple evidences for the same event. Such multiple evidences should be used to strengthen the belief in the underlying physical event as the individual data points may have inherent uncertainty. The uncertainty can arise from inconsistent, incomplete and ambiguous reports. The uncertainty is also because the trust levels of the different sources vary and affect the overall reliability. We will summarize various efforts to perform reliability aware entity integration. The other problem in text analysis in such setting is posed by presence of noise in the text. Since the text is produced in several informal settings such as email, blogs, tweet, SMS, chat and is inherently noisy and has several veracity issues. For example, missing punctuation and the use of non-standard words can often hinder standard natural language processing techniques such as part-of-speech tagging and parsing. Further downstream applications such as entity extraction, entity resolution and entity completion have to explicitly handle noise in order to return useful results. Often, depending on the application, noise can be modeled and it may be possible to develop specific strategies to immunize the system from the effects of noise and improve performance. Also the aspect of reliability is key as a lot of this data is ambiguous, incomplete, conflicting, untrustworthy and deceptive.</abstract>
<note confidence="0.703615">The key goals of this tutorial are: 7 NAACL-HLT pages 7–9, Georgia, June 9 2013. Association for Computational Linguistics</note>
<abstract confidence="0.9665102">1. Draw the attention of researchers towards methods for doing entity analytics and integration on data with 4V characteristics. 2. Differentiate between noise and uncertainty in such data. 3. Provide an in-depth discussion on handling noise in NLP based methods. 4. Finally, handling uncertainty through information fusion and integration.</abstract>
<note confidence="0.871523">This tutorial builds on two earlier tutorials: NAACL 2010 tutorial on Noisy Text and COMAD 2012 tutorial on Reliability Aware Data Fusion. In parallel the authors are also hosting a workshop on related topic ”Reliability Aware Data Fusion” at SIAM Data Mining Conference, 2013. 2 Outline</note>
<abstract confidence="0.935812756756757">2.1 Data with 4V characteristics • Define Volume, Velocity, Variety and Veracity and metrics to quantify them • Information extraction on data with 4V characteristics Key technical challenges posed by the 4V dimensions and linguistics techniques to address them • Analyzing streaming text • Large scale distributed algorithms for NLP • Integrating structured and unstructured data • Noisy text analytics • Reliability • Use case: Generating single view of entity from social data 2.3 Computing Reliability and Trust • Computing source reliability • Identifying Trust Worthy Messages • Data fusion to improve reliability: Probabilistic data fusion, information measures, evidential reasoning • Use case: Event detection using social data, news and online sources 8 3 Speaker Bios is researcher in Information Management Group at IBM Research India. He received his M.S. and Ph.D. from The Ohio State University, USA in 2006. He also holds an Adjunct Faculty position at the International Institute of Information Technology, New Delhi. Sameep regularly advises MS and PhD students at University of Delhi and IIT Delhi. He regularly delivers Tutorials at CO- MAD (2009, 2010 and 2011). His current research interests include Data Mining, Business Analytics, Service Science, Text Mining, and Workforce Optimization. Venkata manages the information management analytics and solutions group at IBM Research India. He received his PhD from IIT Delhi in 1999. His research focuses on unstructured information management, statistical natural language processing, noisy text analytics, text and data mining, information theory, speech and image processing. He often teaches and guides student thesis at IIT Delhi on these topics. His tutorial titled Noisy Text Analytics was the second largest at NAACL-HLT 2010. He co founded the AND (Analytics for Noisy Unstructured Text Data) workshop series and also co-chaired the first four workshops, 2007-2010. He was guest co-editor of two special issues on Noisy Text Analytics in the International Journal of Document Analysis and Recognition in 2007 and 2009.</abstract>
<intro confidence="0.911821">9</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>