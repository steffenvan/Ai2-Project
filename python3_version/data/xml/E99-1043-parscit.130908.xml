<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.245720">
<note confidence="0.723474">
Proceedings of EACL &apos;99
</note>
<title confidence="0.967214">
The GENIA project: corpus-based knowledge acquisition and
information extraction from genome research papers
</title>
<author confidence="0.984124666666667">
Nigel Collier, Hyun Seok Park, Norihiro Ogata
YuIca Tateishi, Chikashi Nobata, Tomoko Ohta
Tateshi Sekimizu, Hisao Imai, Katsutoshi Ibushi, Jun-ichi Tsujii
</author>
<affiliation confidence="0.952789">
{nigel,hsp20,ogata,yucca,nova,okap,sekimizu,hisao,k-ibushi,tsujii}eis.s.u-tokyo.ac.jp
Department of Information Science, Graduate School of Science
University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113, Japan
</affiliation>
<sectionHeader confidence="0.989345" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999263125">
We present an outline of the genome in-
formation acquisition (GENIA) project
for automatically extracting biochemical
information from journal papers and ab-
stracts. GENIA will be available over
the Internet and is designed to aid in
information extraction, retrieval and vi-
sualisation and to help reduce informa-
tion overload on researchers. The vast
repository of papers available online in
databases such as MEDLINE is a natu-
ral environment in which to develop lan-
guage engineering methods and tools and
is an opportunity to show how language
engineering can play a key role on the
Internet.
</bodyText>
<sectionHeader confidence="0.998543" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998932421052632">
In the context of the global research effort to map
the human genome, the Genome Informatics Ex-
traction project, GENIA (GENIA, 1999), aims to
support such research by automatically extract-
ing information from biochemical papers and their
abstracts such as those available from MEDLINE
(MEDLINE, 1999) written by domain specialists.
The vast repository of research papers which are
the results of genome research are a natural envi-
ronment in which to develop language engineering
tools and methods.
This project aims to help reduce the problems
caused by information overload on the researchers
who want to access the information held inside
collections such as MEDLINE. The key elements
of the project are centered around the tasks of
information extraction and retrieval. These are
outlined below and then the interface which inte-
grates them is described.
</bodyText>
<subsectionHeader confidence="0.981387">
1.1 Terminology identification and
classification
</subsectionHeader>
<bodyText confidence="0.999904725">
Through discussions with domain experts, we
have identified several classes of useful entities
such as the names of proteins and genes. The re-
liable identification and acquisition of such class
members is one of our key goals so that terminol-
ogy databases can be automatically extended. We
should not however underestimate the difficulty of
this task as the naming conventions in this field
are very loose.
In our initial experiments we used the EN-
GCG shallow parser (Voutilainen, 1996) to iden-
tify noun phrases and classify them as proteins
(Sekimizu et al., 1998) according to their cooc-
currence with a set of verbs. Due to the difficul-
ties caused by inconsistent naming of terms, we
have decided to use multiple sources of evidence
for classifying terminology.
Currently we have extended our approach and
are exploring two models for named entity recog-
nition. The first is based on a statistical model
of word clustering (Baker and McCallum, 1998)
which is trained on pre-classified word lists from
Swissprot and other databases. We supplemented
this with short word lists to identify the class from
a term&apos;s final noun if it existed in a head final po-
sition. In our first experiments on a judgement
set of 80 expert tagged MEDLINE abstracts the
model yielded F-scores for pre-identified phrases
as follows: 69.35 for 1372 source entities, 53.00 for
3280 proteins, 66.67 for 56 RNA and 45.20 for 566
DNA. We expect this to improve with the addi-
tion of better training word lists. The second ap-
proach is based on decision trees (Quinlan, 1993),
supplemented with word lists for classes derived
from Swissprot and other databases. In these tests
the phrases for terms were not pre-identified. The
model was trained on a corpus of 60 expert tagged
MEDLINE abstracts and tested on a corpus of 20
articles yielding F-scores of: 55.38 for 356 source,
66.58 for 808 protein entities. The number of RNA
</bodyText>
<page confidence="0.985462">
271
</page>
<bodyText confidence="0.9694663">
Proceedings of EACL &apos;99
and DNA entities was too small to train with.
As part of the overall project we are creating
an expert-tagged corpus of MEDLINE abstracts
and full papers for training and testing our tools.
The markup scheme for this corpus is being de-
veloped in cooperation with groups of biologists
and is based on a conceptual domain model imple-
mented in SGML. The corpus itself will be cross-
validated with an independent group of biologists.
</bodyText>
<subsectionHeader confidence="0.952879">
1.2 Information extraction
</subsectionHeader>
<bodyText confidence="0.995795888888889">
We are using information extraction methods to
automatically extract named entity properties,
events and other domain-specific concepts from
MEDLINE abstracts and full texts. One part of
this work is the construction and maintenance of
an ontology for the domain which is executed by
a system which we are now developing called On-
tology Extraction-Mainten ace System (OEMS).
OEMS extracts three types of information about
the domain-ontology, (Ogata, 1997), called typ-
ing information, from the abstracts: taxonomy (a
subtype structure), mereology (a part-whole struc-
ture), synonymy (an identity structure). Eventu-
ally we hope to be able to identify and extract do-
main specific facts such as protein-protein binding
information from full texts and to aid biochemists
in the formation of cell signalling diagrams which
are necessary for their work.
</bodyText>
<subsectionHeader confidence="0.995095">
1.3 Thesaurus building
</subsectionHeader>
<bodyText confidence="0.999989833333333">
A further goal of our work is to construct a the-
saurus automatically from MEDLINE abstracts
and domain dictionaries consisting of medical do-
main terms for the purpose of query expansion in
information retrieval of databases such as MED-
LINE, e.g. see Ping and Croft, 1994). We
are currently working with the Med test set (30
queries and 1033 documents) on SMART (e.g. see
(Salton, 1989),(Buckley et al., 1993)). Eventually
we plan on building a specialised thesaurus for the
genome domain but this currently depends on the
creation of a suitable test set.
</bodyText>
<subsectionHeader confidence="0.979927">
1.4 Interface
</subsectionHeader>
<bodyText confidence="0.9999678">
A key aspect of this project is providing easy inter-
action between domain experts and the informa-
tion extraction programs. Our interface provides
a link to the information extraction programs as
well as clickable links to aid in querying for related
information from publically available databases on
the WWW within a single environment. For ex-
ample, a user can highlight proteins in the texts
using the named entity extraction program and
then search for the molecule structure diagram.
</bodyText>
<sectionHeader confidence="0.949985" genericHeader="conclusions">
2 Conclusion
</sectionHeader>
<bodyText confidence="0.9999204">
This paper has provided a synopsis of the GENIA
project. The project will run for a further two
years and aims to provide an online demonstration
of how language engineering can be useful in the
genome domain.
</bodyText>
<sectionHeader confidence="0.998461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.976867380952381">
L.D. Baker and A.K. McCallum. 1998. Distribu-
tional clustering of words for text classification.
In Proceedings of the 21st Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, Melbourne,
Australia.
C. Buckley, J. Allan, and G. Salton. 1993.
Automatic routing and ad-hoc retrieval using
SMART: TREC-2. In D. K. Harman, editor,
The second Text REtrieval Conference (TREC-
2), pages 45-55. NIST.
GENIA. 1999. Information on the GENIA
project can be found at:. http://www.is.s.u-
tokyo.ac.jp/
Y. Jing and W. Croft. 1994. An association the-
saurus for information retrieval. In Proceedings
of RIA0&apos;94, pages 146-160.
MEDLINE. 1999. The PubMed
database can be found at:.
http://www.ncbi.nlm.nih.gov/PubMed/.
Norihiro Ogata. 1997. Dynamic constructive
thesaurus. In Language Study and Thesaurus:
Proceedings of the National Language Research
Institute Fifth International Symposium: Ses-
sion I, pages 182-189. The National Language
Research Institute, Tokyo.
J.R. Quinlan. 1993. c4.5 Programs for Machine
Learning. Morgan Kaufmann Publishers, Inc.,
San Mateo, California.
G. Salton. 1989. Automatic Text Processing - The
Transformation, Analysis, and Retrieval of In-
formation by Computer. Addison-Wesley Pub-
lishing Company, Inc., Reading, Massachusetts.
T. Sekimizu, H. Park, and J. Tsujii. 1998. Iden-
tifying the interaction between genes and gene
products based on frequently seen verbs in med-
line abstracts. In Genome Informatics. Unvier-
sal Academy Press, Inc.
A. Voutilainen. 1996. Designing a (finite-state)
parsing grammar. In E. Roche and Y. Sch-
abes, editors, Finite-State Language Processing.
A Bradford Book, The MIT Press.
</reference>
<page confidence="0.997296">
272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.278051">
<note confidence="0.819339">Proceedings of EACL &apos;99</note>
<title confidence="0.950818">The GENIA project: corpus-based knowledge acquisition and information extraction from genome research papers</title>
<author confidence="0.885223">Nigel Collier</author>
<author confidence="0.885223">Hyun Seok Park</author>
<author confidence="0.885223">Norihiro Ogata YuIca Tateishi</author>
<author confidence="0.885223">Chikashi Nobata</author>
<author confidence="0.885223">Tomoko Ohta Tateshi Sekimizu</author>
<author confidence="0.885223">Hisao Imai</author>
<author confidence="0.885223">Katsutoshi Ibushi</author>
<author confidence="0.885223">Jun-ichi Tsujii</author>
<email confidence="0.891804">nigeleis.s.u-tokyo.ac.jp</email>
<email confidence="0.891804">hsp20eis.s.u-tokyo.ac.jp</email>
<email confidence="0.891804">ogataeis.s.u-tokyo.ac.jp</email>
<email confidence="0.891804">yuccaeis.s.u-tokyo.ac.jp</email>
<email confidence="0.891804">novaeis.s.u-tokyo.ac.jp</email>
<email confidence="0.891804">okapeis.s.u-tokyo.ac.jp</email>
<email confidence="0.891804">sekimizueis.s.u-tokyo.ac.jp</email>
<email confidence="0.891804">hisaoeis.s.u-tokyo.ac.jp</email>
<email confidence="0.891804">k-ibushieis.s.u-tokyo.ac.jp</email>
<email confidence="0.891804">tsujiieis.s.u-tokyo.ac.jp</email>
<affiliation confidence="0.879138">Department of Information Science, Graduate School of Science University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113, Japan</affiliation>
<abstract confidence="0.984025411764706">We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts. GENIA will be available over the Internet and is designed to aid in information extraction, retrieval and visualisation and to help reduce information overload on researchers. The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L D Baker</author>
<author>A K McCallum</author>
</authors>
<title>Distributional clustering of words for text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<location>Melbourne, Australia.</location>
<contexts>
<context position="2972" citStr="Baker and McCallum, 1998" startWordPosition="446" endWordPosition="449">e difficulty of this task as the naming conventions in this field are very loose. In our initial experiments we used the ENGCG shallow parser (Voutilainen, 1996) to identify noun phrases and classify them as proteins (Sekimizu et al., 1998) according to their cooccurrence with a set of verbs. Due to the difficulties caused by inconsistent naming of terms, we have decided to use multiple sources of evidence for classifying terminology. Currently we have extended our approach and are exploring two models for named entity recognition. The first is based on a statistical model of word clustering (Baker and McCallum, 1998) which is trained on pre-classified word lists from Swissprot and other databases. We supplemented this with short word lists to identify the class from a term&apos;s final noun if it existed in a head final position. In our first experiments on a judgement set of 80 expert tagged MEDLINE abstracts the model yielded F-scores for pre-identified phrases as follows: 69.35 for 1372 source entities, 53.00 for 3280 proteins, 66.67 for 56 RNA and 45.20 for 566 DNA. We expect this to improve with the addition of better training word lists. The second approach is based on decision trees (Quinlan, 1993), sup</context>
</contexts>
<marker>Baker, McCallum, 1998</marker>
<rawString>L.D. Baker and A.K. McCallum. 1998. Distributional clustering of words for text classification. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
<author>J Allan</author>
<author>G Salton</author>
</authors>
<title>Automatic routing and ad-hoc retrieval using SMART: TREC-2. In</title>
<date>1993</date>
<booktitle>The second Text REtrieval Conference (TREC2),</booktitle>
<pages>45--55</pages>
<editor>D. K. Harman, editor,</editor>
<publisher>NIST.</publisher>
<contexts>
<context position="5682" citStr="Buckley et al., 1993" startWordPosition="890" endWordPosition="893">act domain specific facts such as protein-protein binding information from full texts and to aid biochemists in the formation of cell signalling diagrams which are necessary for their work. 1.3 Thesaurus building A further goal of our work is to construct a thesaurus automatically from MEDLINE abstracts and domain dictionaries consisting of medical domain terms for the purpose of query expansion in information retrieval of databases such as MEDLINE, e.g. see Ping and Croft, 1994). We are currently working with the Med test set (30 queries and 1033 documents) on SMART (e.g. see (Salton, 1989),(Buckley et al., 1993)). Eventually we plan on building a specialised thesaurus for the genome domain but this currently depends on the creation of a suitable test set. 1.4 Interface A key aspect of this project is providing easy interaction between domain experts and the information extraction programs. Our interface provides a link to the information extraction programs as well as clickable links to aid in querying for related information from publically available databases on the WWW within a single environment. For example, a user can highlight proteins in the texts using the named entity extraction program and</context>
</contexts>
<marker>Buckley, Allan, Salton, 1993</marker>
<rawString>C. Buckley, J. Allan, and G. Salton. 1993. Automatic routing and ad-hoc retrieval using SMART: TREC-2. In D. K. Harman, editor, The second Text REtrieval Conference (TREC2), pages 45-55. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GENIA</author>
</authors>
<title>Information on the GENIA project can be found at:. http://www.is.s.utokyo.ac.jp/</title>
<date>1999</date>
<contexts>
<context position="1247" citStr="GENIA, 1999" startWordPosition="174" endWordPosition="175">n from journal papers and abstracts. GENIA will be available over the Internet and is designed to aid in information extraction, retrieval and visualisation and to help reduce information overload on researchers. The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet. 1 Introduction In the context of the global research effort to map the human genome, the Genome Informatics Extraction project, GENIA (GENIA, 1999), aims to support such research by automatically extracting information from biochemical papers and their abstracts such as those available from MEDLINE (MEDLINE, 1999) written by domain specialists. The vast repository of research papers which are the results of genome research are a natural environment in which to develop language engineering tools and methods. This project aims to help reduce the problems caused by information overload on the researchers who want to access the information held inside collections such as MEDLINE. The key elements of the project are centered around the tasks </context>
</contexts>
<marker>GENIA, 1999</marker>
<rawString>GENIA. 1999. Information on the GENIA project can be found at:. http://www.is.s.utokyo.ac.jp/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Jing</author>
<author>W Croft</author>
</authors>
<title>An association thesaurus for information retrieval.</title>
<date>1994</date>
<booktitle>In Proceedings of RIA0&apos;94,</booktitle>
<pages>146--160</pages>
<marker>Jing, Croft, 1994</marker>
<rawString>Y. Jing and W. Croft. 1994. An association thesaurus for information retrieval. In Proceedings of RIA0&apos;94, pages 146-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MEDLINE</author>
</authors>
<title>The PubMed database can be found at:.</title>
<date>1999</date>
<note>http://www.ncbi.nlm.nih.gov/PubMed/.</note>
<contexts>
<context position="1415" citStr="MEDLINE, 1999" startWordPosition="198" endWordPosition="199">lp reduce information overload on researchers. The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet. 1 Introduction In the context of the global research effort to map the human genome, the Genome Informatics Extraction project, GENIA (GENIA, 1999), aims to support such research by automatically extracting information from biochemical papers and their abstracts such as those available from MEDLINE (MEDLINE, 1999) written by domain specialists. The vast repository of research papers which are the results of genome research are a natural environment in which to develop language engineering tools and methods. This project aims to help reduce the problems caused by information overload on the researchers who want to access the information held inside collections such as MEDLINE. The key elements of the project are centered around the tasks of information extraction and retrieval. These are outlined below and then the interface which integrates them is described. 1.1 Terminology identification and classifi</context>
</contexts>
<marker>MEDLINE, 1999</marker>
<rawString>MEDLINE. 1999. The PubMed database can be found at:. http://www.ncbi.nlm.nih.gov/PubMed/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norihiro Ogata</author>
</authors>
<title>Dynamic constructive thesaurus. In Language Study and Thesaurus:</title>
<date>1997</date>
<booktitle>Proceedings of the National Language Research Institute Fifth International Symposium: Session I,</booktitle>
<pages>182--189</pages>
<institution>The National Language Research Institute,</institution>
<location>Tokyo.</location>
<contexts>
<context position="4860" citStr="Ogata, 1997" startWordPosition="760" endWordPosition="761">a conceptual domain model implemented in SGML. The corpus itself will be crossvalidated with an independent group of biologists. 1.2 Information extraction We are using information extraction methods to automatically extract named entity properties, events and other domain-specific concepts from MEDLINE abstracts and full texts. One part of this work is the construction and maintenance of an ontology for the domain which is executed by a system which we are now developing called Ontology Extraction-Mainten ace System (OEMS). OEMS extracts three types of information about the domain-ontology, (Ogata, 1997), called typing information, from the abstracts: taxonomy (a subtype structure), mereology (a part-whole structure), synonymy (an identity structure). Eventually we hope to be able to identify and extract domain specific facts such as protein-protein binding information from full texts and to aid biochemists in the formation of cell signalling diagrams which are necessary for their work. 1.3 Thesaurus building A further goal of our work is to construct a thesaurus automatically from MEDLINE abstracts and domain dictionaries consisting of medical domain terms for the purpose of query expansion </context>
</contexts>
<marker>Ogata, 1997</marker>
<rawString>Norihiro Ogata. 1997. Dynamic constructive thesaurus. In Language Study and Thesaurus: Proceedings of the National Language Research Institute Fifth International Symposium: Session I, pages 182-189. The National Language Research Institute, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<date>1993</date>
<booktitle>c4.5 Programs for Machine Learning.</booktitle>
<publisher>Morgan Kaufmann Publishers, Inc.,</publisher>
<location>San Mateo, California.</location>
<contexts>
<context position="3567" citStr="Quinlan, 1993" startWordPosition="551" endWordPosition="552">and McCallum, 1998) which is trained on pre-classified word lists from Swissprot and other databases. We supplemented this with short word lists to identify the class from a term&apos;s final noun if it existed in a head final position. In our first experiments on a judgement set of 80 expert tagged MEDLINE abstracts the model yielded F-scores for pre-identified phrases as follows: 69.35 for 1372 source entities, 53.00 for 3280 proteins, 66.67 for 56 RNA and 45.20 for 566 DNA. We expect this to improve with the addition of better training word lists. The second approach is based on decision trees (Quinlan, 1993), supplemented with word lists for classes derived from Swissprot and other databases. In these tests the phrases for terms were not pre-identified. The model was trained on a corpus of 60 expert tagged MEDLINE abstracts and tested on a corpus of 20 articles yielding F-scores of: 55.38 for 356 source, 66.58 for 808 protein entities. The number of RNA 271 Proceedings of EACL &apos;99 and DNA entities was too small to train with. As part of the overall project we are creating an expert-tagged corpus of MEDLINE abstracts and full papers for training and testing our tools. The markup scheme for this co</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J.R. Quinlan. 1993. c4.5 Programs for Machine Learning. Morgan Kaufmann Publishers, Inc., San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing - The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley Publishing Company, Inc.,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="5659" citStr="Salton, 1989" startWordPosition="889" endWordPosition="890">entify and extract domain specific facts such as protein-protein binding information from full texts and to aid biochemists in the formation of cell signalling diagrams which are necessary for their work. 1.3 Thesaurus building A further goal of our work is to construct a thesaurus automatically from MEDLINE abstracts and domain dictionaries consisting of medical domain terms for the purpose of query expansion in information retrieval of databases such as MEDLINE, e.g. see Ping and Croft, 1994). We are currently working with the Med test set (30 queries and 1033 documents) on SMART (e.g. see (Salton, 1989),(Buckley et al., 1993)). Eventually we plan on building a specialised thesaurus for the genome domain but this currently depends on the creation of a suitable test set. 1.4 Interface A key aspect of this project is providing easy interaction between domain experts and the information extraction programs. Our interface provides a link to the information extraction programs as well as clickable links to aid in querying for related information from publically available databases on the WWW within a single environment. For example, a user can highlight proteins in the texts using the named entity</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>G. Salton. 1989. Automatic Text Processing - The Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley Publishing Company, Inc., Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sekimizu</author>
<author>H Park</author>
<author>J Tsujii</author>
</authors>
<title>Identifying the interaction between genes and gene products based on frequently seen verbs in medline abstracts.</title>
<date>1998</date>
<booktitle>In Genome Informatics. Unviersal</booktitle>
<publisher>Academy Press, Inc.</publisher>
<contexts>
<context position="2587" citStr="Sekimizu et al., 1998" startWordPosition="382" endWordPosition="385">ribed. 1.1 Terminology identification and classification Through discussions with domain experts, we have identified several classes of useful entities such as the names of proteins and genes. The reliable identification and acquisition of such class members is one of our key goals so that terminology databases can be automatically extended. We should not however underestimate the difficulty of this task as the naming conventions in this field are very loose. In our initial experiments we used the ENGCG shallow parser (Voutilainen, 1996) to identify noun phrases and classify them as proteins (Sekimizu et al., 1998) according to their cooccurrence with a set of verbs. Due to the difficulties caused by inconsistent naming of terms, we have decided to use multiple sources of evidence for classifying terminology. Currently we have extended our approach and are exploring two models for named entity recognition. The first is based on a statistical model of word clustering (Baker and McCallum, 1998) which is trained on pre-classified word lists from Swissprot and other databases. We supplemented this with short word lists to identify the class from a term&apos;s final noun if it existed in a head final position. In</context>
</contexts>
<marker>Sekimizu, Park, Tsujii, 1998</marker>
<rawString>T. Sekimizu, H. Park, and J. Tsujii. 1998. Identifying the interaction between genes and gene products based on frequently seen verbs in medline abstracts. In Genome Informatics. Unviersal Academy Press, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
</authors>
<title>Designing a (finite-state) parsing grammar.</title>
<date>1996</date>
<booktitle>Finite-State Language Processing.</booktitle>
<editor>In E. Roche and Y. Schabes, editors,</editor>
<publisher>A Bradford Book, The MIT Press.</publisher>
<contexts>
<context position="2508" citStr="Voutilainen, 1996" startWordPosition="370" endWordPosition="371">ese are outlined below and then the interface which integrates them is described. 1.1 Terminology identification and classification Through discussions with domain experts, we have identified several classes of useful entities such as the names of proteins and genes. The reliable identification and acquisition of such class members is one of our key goals so that terminology databases can be automatically extended. We should not however underestimate the difficulty of this task as the naming conventions in this field are very loose. In our initial experiments we used the ENGCG shallow parser (Voutilainen, 1996) to identify noun phrases and classify them as proteins (Sekimizu et al., 1998) according to their cooccurrence with a set of verbs. Due to the difficulties caused by inconsistent naming of terms, we have decided to use multiple sources of evidence for classifying terminology. Currently we have extended our approach and are exploring two models for named entity recognition. The first is based on a statistical model of word clustering (Baker and McCallum, 1998) which is trained on pre-classified word lists from Swissprot and other databases. We supplemented this with short word lists to identif</context>
</contexts>
<marker>Voutilainen, 1996</marker>
<rawString>A. Voutilainen. 1996. Designing a (finite-state) parsing grammar. In E. Roche and Y. Schabes, editors, Finite-State Language Processing. A Bradford Book, The MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>