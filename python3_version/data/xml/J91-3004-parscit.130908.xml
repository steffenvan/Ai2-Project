<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999490333333333">
Computation of the Probability of
Initial Substring Generation by Stochastic
Context-Free Grammars
</title>
<author confidence="0.9864585">
Frederick Jelinek.
John D. Lafferty.
</author>
<bodyText confidence="0.985527222222222">
IBM T. J. Watson Research Center
Speech recognition language models are based on probabilities P(Wk+1 = V I IV1V-72) • • • 3 Wk)
that the next word Wk+i will be any particular word v of the vocabulary, given that the
word sequence w1,w2, . . , wk is hypothesized to have been uttered in the past. If probabilistic
context-free grammars are to be used as the basis of the language model, it will be necessary
to compute the probability that successive application of the grammar rewrite rules (beginning
with the sentence start symbol s) produces a word string whose initial substring is an arbitrary
sequence w1,w2, . . . ,wk+1. In this paper we describe a new algorithm that achieves the required
computation in at most a constant times k3-steps.
</bodyText>
<sectionHeader confidence="0.990287" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.843317565217391">
The purpose of this article is to develop an algorithm for computing the probability
that a stochastic context-free grammar (SCFG) (that is, a grammar whose production
rules have attached to them a probability of being used) generates an arbitrary initial
substring of terminals. Thus, we treat the same problem recently considered by Wright
and Wrigley (1989) from the point of view of LR grammars.
Probabilistic methods have been shown most effective in automatic speech recog-
nition. Recognition (actually transcription) of natural unrestricted speech requires a
&amp;quot;language model&amp;quot; that attaches probabilities to the production of all possible strings
of words (Bahl et al. 1983). Consequently, if we believe that word generation can be
modeled by context-free grammars, and if we want to base speech recognition (or
handwriting recognition, optical character recogition, etc.) on such models, then it
will become necessary to embed them into a probabilistic framework.
In speech recognition we are presented with words one at a time, in sequence, and
so we would like to calculate the probability P(s w2 wk ...) that an arbitrary
string w1w2 wk is the initial substring of a sentence generated by the given SCFG.I
* P.O. Box 218, Yorktown Heights, NY 10598
1 In fact, in speech recognition (Bahl et al. 1983) we are presented with a hypothesized past text (the
history) w1 w2 • • • wk and are interested in computing, for any arbitrary word v, the conditioned
probability P(Wk+i --= v I w1w2...wk) that the next word uttered will be v given the hypothesized past
wz • • • Wk. Assuming that successive sentences s are independent of each other (a rather dubious
assumption justifiable only by a lack of adequate understanding of how one sentence influences
another), we may as well take the view that wl is the first word of the current sentence and that wk is
not the last. Then
</bodyText>
<equation confidence="0.9776065">
P(s W1W2 • • •WkV • • •)
P(Wk+1 = V I W1W2 • • •Wk) =P(s _+ w1W2 • • •Wk • • •)
</equation>
<note confidence="0.580589333333333">
Hence our interest in the calculation of P(s ...).
C) 1991 Association for Computational Linguistics
Computational Linguistics Volume 17, Number 3
</note>
<sectionHeader confidence="0.556681" genericHeader="categories and subject descriptors">
2. Definition of Stochastic Context-Free Grammars
</sectionHeader>
<bodyText confidence="0.99998375">
We will now define stochastic context free grammars (SCFGs) and establish some
notation. We will use script symbols for sets, lowercase letters for elements of the
sets or specific string items, and capitals for variables. We start with a vocabulary
V =-- {vi , v2, . ,VN} whose elements, words vt, are the terminal symbols of the lan-
guage. We next list a set of nonterminals g = {gi = SI g21 • • • gA/1} whose elements
gl are grammatical phrase markers. They include the distinguished phrase marker s,
the sentence &amp;quot;start&amp;quot; symbol. The purpose of our grammar is to generate sentences
wi w2 wn of varying length n. The generation is accomplished by use of production
rules, belonging to a set R., that rewrite individual phrase markers as sequences of
phrase markers or words. For simplicity of manipulation but without loss of general-
ity, we will limit the productions to the Chomsky Normal Form (CNF). That is, only
the following types of productions will be allowed:
</bodyText>
<equation confidence="0.7812605">
1. H --&gt; GiG2
2. T V (1)
</equation>
<bodyText confidence="0.98509775">
The grammar is stochastic, because to each rule there is assigned a probability of its
use. Let H be any nonterminal, and let #(H) be the number of productions rewriting
H. The ith of these productions will then take place with probability P(i H). It is
assumed that for all i =-- 1,2, ... , #(H), P(i I H) is a strictly positive number and that
</bodyText>
<equation confidence="0.999519">
#(.14) (2)
E
P(i = 1
</equation>
<bodyText confidence="0.999333090909091">
It will be convenient to denote the probabilities P(i H) by the productions they
refer to, e.g., P(H G1 G2) or P(H —* V).
A context-free grammar is assumed to generate sentences from top to bottom,
starting with some rule s G1 G2 that rewrites the sentence symbol s and is used with
probability P(s G1G2). The generated nonterminals G1 and G2 are then rewritten,
and the rewriting process continues until no nonterminals remain to be rewritten, all
having been replaced by words through use of rewrite rules of type (1). The probability
of the entire process is equal to the product of the probabilities of the individual re-
write rules used.
We say that a SCFG is well defined in case it forms a language model; that is, the
total probability of strings of terminals generated by the grammar is equal to 1:
</bodyText>
<equation confidence="0.990746">
00
y: P(s wiw2 ...wn) =- 1
n=1 wiw2...wEV•
</equation>
<bodyText confidence="0.9943748">
A context-free grammar is said to be proper if starting from the distinguished nontermi-
nal s, the only nonterminals produced are those whose further rewriting can eventually
result in a string of terminals. In fact, condition (2) is necessary and sufficient for a
SCFG to be well defined if the underlying grammar is proper.2
The solutions to the following four problems are of interest.
</bodyText>
<footnote confidence="0.707797333333333">
2 The following simple algorithm determines whether or not a grammar may be made proper by the
elimination of rules.
Let S be the set of all nonterminals H such that a rule H V exists for some nonterminal V.
</footnote>
<page confidence="0.992717">
316
</page>
<note confidence="0.406371">
Jelinek and Lafferty Probability of Initial Substring Generation
</note>
<listItem confidence="0.994797666666667">
1. What is the probability P(s w1w2... wn) that the grammar, beginning
with the start nonterminal s, generates a given word string (sentence)
w2 • • • wn, w, E V?
</listItem>
<bodyText confidence="0.920991875">
The desired probability is computed by the Inside Algorithm (Baker 1979), which is a
modification of the well-known CYK parsing algorithm (Younger 1967; Graham et al.
1980).
2. What is the most probable parse of a given word string w1 w2. Wk? That
is, which sequence of rewrite rules resulting in WI W2. wk is such that
the product of its probabilities is maximal?
This parse is computed by the Viterbi Algorithm (Jelinek 1985), which uses the same
chart as the CYK algorithm.
</bodyText>
<listItem confidence="0.767493666666667">
3. What is the probability P(s w2 wn ...) that the grammar,
beginning with the start nonterminal s, generates a word string
(sentence) whose initial substring is wi wn?
</listItem>
<bodyText confidence="0.821525">
The algorithm providing the answer to this question is developed in the present paper.
</bodyText>
<listItem confidence="0.852971">
4. Given the set of rules specifying a context-free grammar, how should the
probabilities of their use be determined?
</listItem>
<bodyText confidence="0.9997476">
An answer to this question requires a criterion by which to judge it. The maxi-
mum likelihood criterion is as follows: given a &amp;quot;training corpus&amp;quot; WT (that is, a set of
sentences), determine the production probabilities so as to maximize the probability
that the grammar generated WT. The Inside—Outside Algorithm (Baker 1979) extracts
probabilities that locally (i.e., not necessarily globally) maximize the likelihood of WT.
</bodyText>
<subsectionHeader confidence="0.452893">
3. Development of the Left-to-Right Inside (LRI) Algorithm
</subsectionHeader>
<bodyText confidence="0.999126125">
In this section we will develop the Left-to-Right (LRI) Algorithm, which will allow us
to calculate the desired probabilities P(s W1 W2 Wk .). In order to present the LRI
Algorithm, we will introduce some notation that will simplify the appearance of the
following formulas. Let P(H(i,j)) denote the probability P(H ...w]) that starting
with the nonterminal H, successive application of grammar rules has produced the
sequence w,wi+i That is, if the SCFG production process is represented by the
usual tree diagram, then P(H(i,j)) is the sum of the probabilities of all trees whose
root is H and whose leaves are w,, wi±i • • • , zvi•
</bodyText>
<listItem confidence="0.901655">
1. If S = g, the grammar is proper. Stop.
</listItem>
<bodyText confidence="0.605648">
Else if S g, then find the set A of all nonterminals H not belonging to S that rewrite as H G1 G2
with G1 and G2 belonging to S.
</bodyText>
<listItem confidence="0.963530666666667">
2. If A is not empty, include the set A in S and go to I.
3. If s E 5, eliminate from g all nonterminals not belonging to S and purge all rules involving
nonterminals not belonging to S. The resulting grammar is proper.
</listItem>
<bodyText confidence="0.471085">
Else if s S, the grammar cannot be made proper by purging.
</bodyText>
<page confidence="0.970748">
317
</page>
<note confidence="0.258274">
Computational Linguistics Volume 17, Number 3
</note>
<equation confidence="0.875819333333333">
Next, let P(H &lt; i,j) denote the sum of the probabilities of all trees with root node
H resulting in word strings whose initial substring is w1w1±1 wj. Thus
P(H &lt; = P(1(i4)) +12 P(H wi • • • wixi)
X1
+ Ep(H -+ w,...w,x,x2)+ • • •
X1X2
</equation>
<bodyText confidence="0.998616166666667">
Note that the first sum in (3) is over all possible words xl, the second is over all
possible word pairs x1x2, and the third sum (the general term) is over all possible word
n-tuples x1x2 xi,. Using the notation (3), the desired probability P(s -4 WiW2 • • • Wk . • .)
is denoted by P(s &lt; 1,k).
In what follows we will need P. (H G), the sum of the probabilities of all the
rules H G1 G2 whose first righthand side element is G1 = G. That is,
</bodyText>
<equation confidence="0.9501338">
PL.(H — G) = ENH, GG) (4)
G2
Next we define the quantity
QL(H G) = PL(H G) + PL(H iii)PL(Ai –+G)
+ E PL(H AopL(Ai A2)PL(A2 G) + • •
A1 A2
+ E PL(H AopL(Ai A2) ...pL(Ak G) + • • •
A1, ..,Ak
- E P(H =*-&gt; Ga) (5)
a
</equation>
<bodyText confidence="0.999604166666667">
which is the sum of probabilities of all trees with root node H that produce G as the
leftmost (first) nonterminal. Note that the last displayed (general) term accounts for
all trees whose leftmost leaf has depth k. Note further that the above sum converges
since we assume that our underlying grammar is proper, and that rule probabilities
are non-zero.
We are now ready to compute
</bodyText>
<equation confidence="0.99640825">
P(H&lt; wi) + PL(1-1 G)P(G w,)
+E E pal/ A•opL(Ai G)P(G &apos;W)
G
+E EPL(H AoPL(Al A2)paA2 G)P(G
G A1,A2
+...
+E E pLGH AopL(Ai A2)...pL(Ak G)P(G w,)
G A1,...,Ak
</equation>
<page confidence="0.779259">
318
</page>
<bodyText confidence="0.4794115">
Jelinek and Lafferty Probability of Initial Substring Generation
Thus, using definition (5) we get
</bodyText>
<equation confidence="0.97738775">
P(H &lt;&lt; i) = P(H —+ w)+ E QL(H G)P(G --4 w,) (6)
To compute P(H &lt; i, i + n) for n &gt; 0, we will need to define
(2L(H GiG2) = P(H G1G2) WI/ A)P(A GG) (7)
A
</equation>
<bodyText confidence="0.662350333333333">
which can be seen to be the sum of probabilities of all trees with root node H whose
last leftmost production results in leaves G1 and G2. To compute P(H &lt; i, i + n) we
will rely on the strict CN form of the grammar. Obviously,
</bodyText>
<equation confidence="0.9989178">
P(H &lt; i + n) E P(H GiG2) [P(Gi (i, i))P (G2 &lt; + 1, i + n)
,G2
P(G1 i + 1)),P (G2 &lt; + 2, i + n) + • • -
+ P (Gi (1, i + n 1))P (G2 &lt; + n, i + n)
+ P(Gi &lt; 1,1 + n)] (8)
</equation>
<bodyText confidence="0.99305675">
since to generate the initial substring w,w,+1 • • • wi+n, some rule H G1G2 must first
be applied and then the first part of the substring must be generated from G1 and its
remaining part (and perhaps more!) from G2.
Defining the function
</bodyText>
<equation confidence="0.999024">
R(Gi, G2) = [P(Gi(i, OP (G2 &lt; + 1, i + n) + P(Gi (1, + 1))P (G2 &lt; + 2,i + n) + • • •
+P (GI (1, i + n — 1))P (G2 &lt; i + n, i + n)] (9)
we can next rearrange (8) as follows:
P(H &lt;&lt; i + n) E P(H- G1G2)R(G1, G2)
,G2
Ai)P(Ai &lt; n) (10)
</equation>
<bodyText confidence="0.9933605">
where we took advantage of the definition (4) and denoted the variable in the last
sum by Ai instead of by G1.
</bodyText>
<equation confidence="0.9480145">
Renaming H in (10) as A1, and A1 as A2, we get
P(Al &lt; + n) =E poi GiG2)R(Gi, G2)
Gi,G2
+ EpL(Ai A2)P(A2 &lt; + n)
</equation>
<page confidence="0.7972265">
A2
319
</page>
<bodyText confidence="0.201228">
Computational Linguistics Volume 17, Number 3
</bodyText>
<equation confidence="0.964413">
Substituting (11) into (10) and collecting and factoring out common terms, we get
P(H &lt; i + n) = E [p(H G [G2) + PL(H iii)P (Ai -4 GiG2)] R(Gi G2)
,G2 A1
+ E pL(H AopL(Ai A2)p(A2 + n) (12)
Ai ,A2
Next, renaming A1 in (11) as A2, and A2 as A3, and substituting the result into (12),
we get
P(H &lt; + n) =E [p(H G1G2) + E pijH G1G2)
G1, G2
+ E pL(H AopL(Ai A2)/302 GiG2) R(Gi, G2) (13)
,A2
+ E PL(H Ai)PL (Ai A2)PL (A2 -4 A3)P(A3 &lt; i + n)
A1 ,A2 ,A3
The pattern is now clear. Since
E PL (II Ai)P L (Ai A2) • • • PL(Ak-i Ak)P(Ak &lt; + n)
, ,Ak
</equation>
<bodyText confidence="0.9185415">
tends to 0 as k grows without limit, then using definition (7) and successive re-
substitutions, we get the final formula
</bodyText>
<equation confidence="0.989186333333333">
P(H &lt;1,1+n) =E (20-1 G1G2).R(G1G2) (14)
G ,G2
11
=
L., caw G1G2) + inP(G2 « i+j,i+ n)
• G2 j=1
</equation>
<bodyText confidence="0.9879185">
where the last equality follows from (9), the definition of R(Gi, G2).
We can now notice that formula (14) is very similar to the well-known formula
</bodyText>
<equation confidence="0.983177">
P(H(i, i + n)) G1G2) P(Gi (i , i +j — 1))P(G2(i +1, i + n)) (15)
,G,
</equation>
<bodyText confidence="0.998005875">
that allows an iterative calculation of the (inside) probabilities P(H(i, i +n)) ((15) serves
as the basis for the Inside Algorithm (Baker 1979)). There are two differences between
(14) and (15): instead of the rule probability P(H G1 G2) in (15), we have in (14)
the sum-of-tree-probability function QL (H G1G2) (defined in (7)), and instead of the
simple span generation probability P(G2(i + j, i + n)) in (15), we have in (14) the initial
substring generation probability P(G2 &lt; i + j, i + n) (defined in (3)). It follows that
once we determine how to calculate the values of QL(H zz G1G2) (this is discussed in
the next section), we will be able to compute iteratively all the other quantities (that
</bodyText>
<page confidence="0.992922">
320
</page>
<bodyText confidence="0.805341">
Jelinek and Lafferty Probability of Initial Substring Generation
is, P(H j) and P(H(i,j))). In fact, it follows from (14) that to calculate P(s &lt;&lt; 1, k)
one proceeds as follows:
</bodyText>
<listItem confidence="0.990134">
1. Calculate probabilities P (G(i, i + n)) for i = 1,2, ... , k — 1,
n = 0, 1,2, ... , k — i — 1, iteratively by formula (15).
2. Calculate probabilities P(H &lt; k, k) by formula (6).
3. Calculate probabilities
</listItem>
<equation confidence="0.990113">
P(H &lt; k — 1, k) QL (H GiG2)P(Gi (k 1, k — 1))P (G2 &lt; k, k)
Gi,G2
4. Calculate probabilities
P(H &lt; k —2, k) = QL(H G1G2)
G2
2
E P(Gi(k— 2, k + j — 3))P (G2 &lt; k + j — 2,k)
=1
k. Calculate probabilities
k-2
P(H &lt; 2, k) = E (H Gi G2) Ep(G,(2,1 -Fmp(G2 « 2 j,
G2 =1
k +1. Calculate the probability
k-1
P(s &lt; 1,k) = E QL(s G1G2) EP(Gi1,1))P(G2 + 1, k)
G1, G2 1=1
</equation>
<listItem confidence="0.344234">
4. Determination of the Functions QL(H G1G2) and P(H &lt; i, i)
</listItem>
<bodyText confidence="0.9843505">
Let us first observe that if w, = v then P(H &lt; I, i) = P(H v . . .) which, consistent
with previous notation (5), we denote by QL(H = v). We then get from (6)
</bodyText>
<equation confidence="0.548686">
(16)
</equation>
<bodyText confidence="0.996443714285714">
It follows from (16) and (7) that to calculate the desired quantities P(H &lt; I, i) and
QL(H z G1 G2) we must first determine the left corner probability sums QL(H G).
We will use matrix algebra to compute them.
Let PL and QL denote the square matrices (their dimension is equal to the number
of nonterminals) whose elements in the Hth row and Gth column are PL(H G)
(defined in (4)) and QL (H G), respectively. Then equation (5) can be rewritten in
matrix form as
</bodyText>
<equation confidence="0.960322">
QL = PL + Pi, + + _Pt + (17)
</equation>
<page confidence="0.989537">
321
</page>
<note confidence="0.55869">
Computational Linguistics Volume 17, Number 3
</note>
<bodyText confidence="0.938810333333333">
where Pt denotes i-fold multiplication of the matrix PL with itself. Post-multiplying
both sides of (17) by the matrix PL, subtracting the resulting equation from (17), and
cancelling terms, we get
QL - QLPL (18)
Finally, denoting by I the diagonal unit matrix of the same dimension as PL, we
get from (18) the desired solution
</bodyText>
<equation confidence="0.537691">
QL = PL [I - FL] -1 (19)
</equation>
<bodyText confidence="0.997623">
where [I - PL]-1 denotes the inverse of the matrix [I - Pd.
Equation (16) can also be stated in matrix form. Denoting by Pw and Qw the
rectangular matrices with elements P(H w) and QL (H --&gt; w) in the Hth row and wth
column, respectively, we get from (16) that
</bodyText>
<equation confidence="0.717511">
Qw 11+ QL]Pw (20)
</equation>
<sectionHeader confidence="0.95615" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.997246125">
While the LRI algorithm together with formulas (19) and (20) constitutes the solution
to the stated problem, its practicality is limited to grammars whose total number
of nonterminals is sufficiently limited so as to allow the calculation of the inverse
- Piti•
The algorithm itself has exactly twice the complexity of the Inside Algorithm
computing P(H(i,i + n)) by formula (15), and is thus of order n3. In fact, once all the
probabilities required for the computation of P(s &lt; 1, k) are computed, to get the next
probability of interest, P(s &lt; 1,k + 1), one needs to compute the following quantities:
</bodyText>
<listItem confidence="0.932396666666667">
1. The probabilities P(G(i,k)) for i = k, k - 1, . . . , 1, in that order.
2. The probabilities P(H &lt; i,k + 1) for i = k + 1,k, . , 2, in that order.
3. The probability P(s &lt; 1, k +1).
</listItem>
<bodyText confidence="0.963354555555556">
Let us finally recall that the language model of speech recognition provides to the
recognizer the probability P(Wk = V I WiW2 ...Wk_i) for all possible words v, and that
we therefore must be able to compute the probability P(s -&gt; w1w2 ...wk_iv ...) for all
N words v of the vocabulary. Fortunately, this does not mean carrying out the LRI
algorithm N times for each word position k, but only M times, where M is the number
of nonterminals of the grammar.
In fact, a simple modification of the algorithm allows one to compute the proba-
bilities of P(s -&gt; w1w2 ...wk_i g, ..) where g, is an element of the set of nonterminals
g = {gi = s, g2, • .. ,gm}. This may be done, for example, by setting
</bodyText>
<equation confidence="0.8753305">
{
P(H&lt;k,k) = 1 if H g,
(21)
0 otherwise
</equation>
<bodyText confidence="0.9767195">
in the algorithm of Section 3. Our desired LRI probabilities can then be computed by
the formula
</bodyText>
<equation confidence="0.457068">
P(s wiwz .. • wk-iv • • .) QL(g, v)P(s wiwz • • .) (22)
</equation>
<page confidence="0.945948">
1=1
</page>
<bodyText confidence="0.9957985">
This modification is particularly practical when the size of the vocabulary greatly
exceeds the number of nonterminals in the grammar.
</bodyText>
<page confidence="0.991021">
322
</page>
<note confidence="0.781051">
Jelinek and Lafferty Probability of Initial Substring Generation
</note>
<sectionHeader confidence="0.724648" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999490185185185">
Bahl, L. R.; Jelinek, F.; and Mercer, R. L.
(1983). &amp;quot;A maximum likelihood approach
to continuous speech recognition.&amp;quot; IEEE
Transactions on Pattern Analysis and
Machine Intelligence, Vol PAMI-5, No 2,
1798-1790.
Baker, J. K. (1979). &amp;quot;Trainable grammars for
speech recognition.&amp;quot; Proceedings, Spring
Conference of the Acoustical Society of
America, Boston, MA, 547-550.
Graham, S. L.; Harrison, M. A.; and Ruzzo,
W. L. (1980). &amp;quot;An improved context-free
recognizer,&amp;quot; ACM Transactions on
Programming Languages and Systems, Vol 2,
No 3,415-462.
Jelinek, F. (1985). &amp;quot;Markov source modeling
of text generation.&amp;quot; In The Impact of
Processing Techniques on Communications,
edited by J. K. Skwirzinski. Dordrecht:
Nijhoff.
Wright, J. H.; and Wrigley, E. N. (1989).
&amp;quot;Probabilistic LR parsing for speech
recognition.&amp;quot; International Workshop on
Parsing Technologies. 105-114.
Younger, D. H. (1967). &amp;quot;Recognition and
parsing of context free languages in time
N3,&amp;quot; Information and Control 10,1980-208.
</reference>
<page confidence="0.999385">
323
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.479898">
<title confidence="0.996642">Computation of the Probability of Initial Substring Generation by Stochastic Context-Free Grammars</title>
<author confidence="0.9894165">John D Lafferty</author>
<affiliation confidence="0.997397">IBM T. J. Watson Research Center</affiliation>
<abstract confidence="0.931541">recognition language models are based on probabilities P(Wk+1 • • • the next word be any particular word v of the vocabulary, given that the sequence . . , wk is hypothesized to have been uttered in the past. If probabilistic context-free grammars are to be used as the basis of the language model, it will be necessary to compute the probability that successive application of the grammar rewrite rules (beginning with the sentence start symbol s) produces a word string whose initial substring is an arbitrary . . . In this paper we describe a new algorithm that achieves the required in at most a constant times</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.&amp;quot;</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<pages>1798--1790</pages>
<contexts>
<context position="1579" citStr="Bahl et al. 1983" startWordPosition="249" endWordPosition="252">puting the probability that a stochastic context-free grammar (SCFG) (that is, a grammar whose production rules have attached to them a probability of being used) generates an arbitrary initial substring of terminals. Thus, we treat the same problem recently considered by Wright and Wrigley (1989) from the point of view of LR grammars. Probabilistic methods have been shown most effective in automatic speech recognition. Recognition (actually transcription) of natural unrestricted speech requires a &amp;quot;language model&amp;quot; that attaches probabilities to the production of all possible strings of words (Bahl et al. 1983). Consequently, if we believe that word generation can be modeled by context-free grammars, and if we want to base speech recognition (or handwriting recognition, optical character recogition, etc.) on such models, then it will become necessary to embed them into a probabilistic framework. In speech recognition we are presented with words one at a time, in sequence, and so we would like to calculate the probability P(s w2 wk ...) that an arbitrary string w1w2 wk is the initial substring of a sentence generated by the given SCFG.I * P.O. Box 218, Yorktown Heights, NY 10598 1 In fact, in speech </context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Bahl, L. R.; Jelinek, F.; and Mercer, R. L. (1983). &amp;quot;A maximum likelihood approach to continuous speech recognition.&amp;quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol PAMI-5, No 2, 1798-1790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.&amp;quot;</title>
<date>1979</date>
<booktitle>Proceedings, Spring Conference of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Boston, MA,</location>
<contexts>
<context position="6168" citStr="Baker 1979" startWordPosition="1069" endWordPosition="1070">f the underlying grammar is proper.2 The solutions to the following four problems are of interest. 2 The following simple algorithm determines whether or not a grammar may be made proper by the elimination of rules. Let S be the set of all nonterminals H such that a rule H V exists for some nonterminal V. 316 Jelinek and Lafferty Probability of Initial Substring Generation 1. What is the probability P(s w1w2... wn) that the grammar, beginning with the start nonterminal s, generates a given word string (sentence) w2 • • • wn, w, E V? The desired probability is computed by the Inside Algorithm (Baker 1979), which is a modification of the well-known CYK parsing algorithm (Younger 1967; Graham et al. 1980). 2. What is the most probable parse of a given word string w1 w2. Wk? That is, which sequence of rewrite rules resulting in WI W2. wk is such that the product of its probabilities is maximal? This parse is computed by the Viterbi Algorithm (Jelinek 1985), which uses the same chart as the CYK algorithm. 3. What is the probability P(s w2 wn ...) that the grammar, beginning with the start nonterminal s, generates a word string (sentence) whose initial substring is wi wn? The algorithm providing th</context>
<context position="12501" citStr="Baker 1979" startWordPosition="2335" endWordPosition="2336"> • • PL(Ak-i Ak)P(Ak &lt; + n) , ,Ak tends to 0 as k grows without limit, then using definition (7) and successive resubstitutions, we get the final formula P(H &lt;1,1+n) =E (20-1 G1G2).R(G1G2) (14) G ,G2 11 = L., caw G1G2) + inP(G2 « i+j,i+ n) • G2 j=1 where the last equality follows from (9), the definition of R(Gi, G2). We can now notice that formula (14) is very similar to the well-known formula P(H(i, i + n)) G1G2) P(Gi (i , i +j — 1))P(G2(i +1, i + n)) (15) ,G, that allows an iterative calculation of the (inside) probabilities P(H(i, i +n)) ((15) serves as the basis for the Inside Algorithm (Baker 1979)). There are two differences between (14) and (15): instead of the rule probability P(H G1 G2) in (15), we have in (14) the sum-of-tree-probability function QL (H G1G2) (defined in (7)), and instead of the simple span generation probability P(G2(i + j, i + n)) in (15), we have in (14) the initial substring generation probability P(G2 &lt; i + j, i + n) (defined in (3)). It follows that once we determine how to calculate the values of QL(H zz G1G2) (this is discussed in the next section), we will be able to compute iteratively all the other quantities (that 320 Jelinek and Lafferty Probability of </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, J. K. (1979). &amp;quot;Trainable grammars for speech recognition.&amp;quot; Proceedings, Spring Conference of the Acoustical Society of America, Boston, MA, 547-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Graham</author>
<author>M A Harrison</author>
<author>W L Ruzzo</author>
</authors>
<title>An improved context-free recognizer,&amp;quot;</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<volume>2</volume>
<pages>3--415</pages>
<contexts>
<context position="6268" citStr="Graham et al. 1980" startWordPosition="1083" endWordPosition="1086">erest. 2 The following simple algorithm determines whether or not a grammar may be made proper by the elimination of rules. Let S be the set of all nonterminals H such that a rule H V exists for some nonterminal V. 316 Jelinek and Lafferty Probability of Initial Substring Generation 1. What is the probability P(s w1w2... wn) that the grammar, beginning with the start nonterminal s, generates a given word string (sentence) w2 • • • wn, w, E V? The desired probability is computed by the Inside Algorithm (Baker 1979), which is a modification of the well-known CYK parsing algorithm (Younger 1967; Graham et al. 1980). 2. What is the most probable parse of a given word string w1 w2. Wk? That is, which sequence of rewrite rules resulting in WI W2. wk is such that the product of its probabilities is maximal? This parse is computed by the Viterbi Algorithm (Jelinek 1985), which uses the same chart as the CYK algorithm. 3. What is the probability P(s w2 wn ...) that the grammar, beginning with the start nonterminal s, generates a word string (sentence) whose initial substring is wi wn? The algorithm providing the answer to this question is developed in the present paper. 4. Given the set of rules specifying a </context>
</contexts>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>Graham, S. L.; Harrison, M. A.; and Ruzzo, W. L. (1980). &amp;quot;An improved context-free recognizer,&amp;quot; ACM Transactions on Programming Languages and Systems, Vol 2, No 3,415-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Markov source modeling of text generation.&amp;quot;</title>
<date>1985</date>
<booktitle>In The Impact of Processing Techniques on Communications,</booktitle>
<location>Dordrecht: Nijhoff.</location>
<note>edited by</note>
<contexts>
<context position="6523" citStr="Jelinek 1985" startWordPosition="1132" endWordPosition="1133">l Substring Generation 1. What is the probability P(s w1w2... wn) that the grammar, beginning with the start nonterminal s, generates a given word string (sentence) w2 • • • wn, w, E V? The desired probability is computed by the Inside Algorithm (Baker 1979), which is a modification of the well-known CYK parsing algorithm (Younger 1967; Graham et al. 1980). 2. What is the most probable parse of a given word string w1 w2. Wk? That is, which sequence of rewrite rules resulting in WI W2. wk is such that the product of its probabilities is maximal? This parse is computed by the Viterbi Algorithm (Jelinek 1985), which uses the same chart as the CYK algorithm. 3. What is the probability P(s w2 wn ...) that the grammar, beginning with the start nonterminal s, generates a word string (sentence) whose initial substring is wi wn? The algorithm providing the answer to this question is developed in the present paper. 4. Given the set of rules specifying a context-free grammar, how should the probabilities of their use be determined? An answer to this question requires a criterion by which to judge it. The maximum likelihood criterion is as follows: given a &amp;quot;training corpus&amp;quot; WT (that is, a set of sentences)</context>
</contexts>
<marker>Jelinek, 1985</marker>
<rawString>Jelinek, F. (1985). &amp;quot;Markov source modeling of text generation.&amp;quot; In The Impact of Processing Techniques on Communications, edited by J. K. Skwirzinski. Dordrecht: Nijhoff.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Wright</author>
<author>E N Wrigley</author>
</authors>
<title>Probabilistic LR parsing for speech recognition.&amp;quot;</title>
<date>1989</date>
<booktitle>International Workshop on Parsing Technologies.</booktitle>
<pages>105--114</pages>
<contexts>
<context position="1260" citStr="Wright and Wrigley (1989)" startWordPosition="202" endWordPosition="205">ing with the sentence start symbol s) produces a word string whose initial substring is an arbitrary sequence w1,w2, . . . ,wk+1. In this paper we describe a new algorithm that achieves the required computation in at most a constant times k3-steps. 1. Introduction The purpose of this article is to develop an algorithm for computing the probability that a stochastic context-free grammar (SCFG) (that is, a grammar whose production rules have attached to them a probability of being used) generates an arbitrary initial substring of terminals. Thus, we treat the same problem recently considered by Wright and Wrigley (1989) from the point of view of LR grammars. Probabilistic methods have been shown most effective in automatic speech recognition. Recognition (actually transcription) of natural unrestricted speech requires a &amp;quot;language model&amp;quot; that attaches probabilities to the production of all possible strings of words (Bahl et al. 1983). Consequently, if we believe that word generation can be modeled by context-free grammars, and if we want to base speech recognition (or handwriting recognition, optical character recogition, etc.) on such models, then it will become necessary to embed them into a probabilistic f</context>
</contexts>
<marker>Wright, Wrigley, 1989</marker>
<rawString>Wright, J. H.; and Wrigley, E. N. (1989). &amp;quot;Probabilistic LR parsing for speech recognition.&amp;quot; International Workshop on Parsing Technologies. 105-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context free languages in time N3,&amp;quot;</title>
<date>1967</date>
<journal>Information and Control</journal>
<pages>10--1980</pages>
<contexts>
<context position="6247" citStr="Younger 1967" startWordPosition="1081" endWordPosition="1082">ems are of interest. 2 The following simple algorithm determines whether or not a grammar may be made proper by the elimination of rules. Let S be the set of all nonterminals H such that a rule H V exists for some nonterminal V. 316 Jelinek and Lafferty Probability of Initial Substring Generation 1. What is the probability P(s w1w2... wn) that the grammar, beginning with the start nonterminal s, generates a given word string (sentence) w2 • • • wn, w, E V? The desired probability is computed by the Inside Algorithm (Baker 1979), which is a modification of the well-known CYK parsing algorithm (Younger 1967; Graham et al. 1980). 2. What is the most probable parse of a given word string w1 w2. Wk? That is, which sequence of rewrite rules resulting in WI W2. wk is such that the product of its probabilities is maximal? This parse is computed by the Viterbi Algorithm (Jelinek 1985), which uses the same chart as the CYK algorithm. 3. What is the probability P(s w2 wn ...) that the grammar, beginning with the start nonterminal s, generates a word string (sentence) whose initial substring is wi wn? The algorithm providing the answer to this question is developed in the present paper. 4. Given the set o</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Younger, D. H. (1967). &amp;quot;Recognition and parsing of context free languages in time N3,&amp;quot; Information and Control 10,1980-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>