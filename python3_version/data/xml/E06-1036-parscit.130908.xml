<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.974344">
Recognizing Textual Parallelisms with edit distance and similarity degree
</title>
<author confidence="0.600814">
Marie Gu´egan and Nicolas Hernandez
</author>
<note confidence="0.508999">
LIMSI-CNRS
Universit´e de Paris-Sud, France
</note>
<email confidence="0.987383">
guegan@aist.enst.fr  |hernandez@limsi.fr
</email>
<sectionHeader confidence="0.998477" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999084">
Detection of discourse structure is crucial
in many text-based applications. This pa-
per presents an original framework for de-
scribing textual parallelism which allows
us to generalize various discourse phe-
nomena and to propose a unique method
to recognize them. With this prospect, we
discuss several methods in order to iden-
tify the most appropriate one for the prob-
lem, and evaluate them based on a manu-
ally annotated corpus.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999858893617022">
Detection of discourse structure is crucial in many
text-based applications such as Information Re-
trieval, Question-Answering, Text Browsing, etc.
Thanks to a discourse structure one can precisely
point out an information, provide it a local context,
situate it globally, link it to others.
The context of our research is to improve au-
tomatic discourse analysis. A key feature of the
most popular discourse theories (RST (Mann and
Thompson, 1987), SDRT (Asher, 1993), etc.) is
the distinction between two sorts of discourse re-
lations or rhetorical functions: the subordinating
and the coordinating relations (some parts of a
text play a subordinate role relative to other parts,
while some others have equal importance).
In this paper, we focus our attention on a dis-
course feature we assume supporting coordination
relations, namely the Textual Parallelism. Based
on psycholinguistics studies (Dubey et al., 2005),
our intuition is that similarities concerning the sur-
face, the content and the structure of textual units
can be a way for authors to explicit their intention
to consider these units with the same rhetorical im-
portance.
Parallelism can be encountered in many specific
discourse structures such as continuity in infor-
mation structure (Kruijff-Korbayov´a and Kruijff,
1996), frame structures (Charolles, 1997), VP el-
lipses (Hobbs and Kehler, 1997), headings (Sum-
mers, 1998), enumerations (Luc et al., 1999), etc.
These phenomena are usually treated mostly inde-
pendently within individual systems with ad-hoc
resource developments.
In this work, we argue that, depending on de-
scription granularity we can proceed, computing
syntagmatic (succession axis of linguistic units)
and paradigmatic (substitution axis) similarities
between units can allow us to generically handle
such discourse structural phenomena. Section 2
introduces the discourse parallelism phenomenon.
Section 3 develops three methods we implemented
to detect it: a similarity degree measure, a string
editing distance (Wagner and Fischer, 1974) and a
tree editing distance1 (Zhang and Shasha, 1989).
Section 4 discusses and evaluates these methods
and their relevance. The final section reviews re-
lated work.
</bodyText>
<sectionHeader confidence="0.981627" genericHeader="method">
2 Textual parallelism
</sectionHeader>
<bodyText confidence="0.999585272727273">
Our notion of parallelism is based on similarities
between syntagmatic and paradigmatic represen-
tations of (constituents of) textual units. These
similarities concern various dimensions from shal-
low to deeper description: layout, typography,
morphology, lexicon, syntax, and semantics. This
account is not limited to the semantic dimension
as defined by (Hobbs and Kehler, 1997) who con-
sider text fragments as parallel if the same predi-
cate can be inferred from them with coreferential
or similar pairs of arguments.
</bodyText>
<footnote confidence="0.9290485">
1For all measures, elementary units considered are syn-
tactic tags and word tokens.
</footnote>
<page confidence="0.996417">
281
</page>
<bodyText confidence="0.999985272727273">
We observe parallelism at various structural lev-
els of text: among heading structures, VP ellipses
and others, enumerations of noun phrases in a
sentence, enumerations with or without markers
such as frame introducers (e.g. “In France, ...In
Italy, ... ”) or typographical and layout markers.
The underlying assumption is that parallelism be-
tween some textual units accounts for a rhetorical
coordination relation. It means that these units can
be regarded as equally important.
By describing textual units in a two-tier frame-
work composed of a paradigmatic level and syn-
tagmatic level, we argue that, depending on the
description granularity we consider (potentially at
the character level for item numbering), we can
detect a wide variety of parallelism phenomena.
Among parallelism properties, we note that the
parallelism of a given number of textual units is
based on the parallelism of their constituents. We
also note that certain semantic classes of con-
stituents, such as item numbering, are more effec-
tive in marking parallelism than others.
</bodyText>
<subsectionHeader confidence="0.997901">
2.1 An example of parallelism
</subsectionHeader>
<bodyText confidence="0.8800171">
The following example is extracted from our cor-
pus (see section 4.1). In this case, we have an enu-
meration without explicit markers.
For the purposes of chaining, each type of link
between WordNet synsets is assigned a direction
of up, down, or horizontal.
Upward links correspond to generalization: for
example, an upward link from apple to fruit indi-
cates that fruit is more general than apple.
Downward links correspond to specialization:
for example, a link from fruit to apple would have
a downward direction.
Horizontal links are very specific specializations.
The parallelism pattern of the first two items is de-
scribed as follows:
[JJ + suff =ward] links correspond to [NN + suff
= alization] : for example, X link from Y to Z .
This pattern indicates that several item con-
stituents can be concerned by parallelism and that
similarities can be observed at the typographic,
lexical and syntactic description levels. Tokens
(words or punctuation marks) having identical
shallow descriptions are written in italics. The
X, Y and Z variables stand for matching any non-
parallel text areas between contiguous parallel tex-
tual units. Some words are parallel based on
their syntactic category (“JJ” / adjectives, “NN” /
nouns) or suffix specifications (“suff” attribute).
The third item is similar to the first two items but
with a simpler pattern:
</bodyText>
<subsectionHeader confidence="0.569568">
JJ links U [NN + suff =alization] W .
</subsectionHeader>
<bodyText confidence="0.980159">
Parallelism is distinguished by these types of sim-
ilarities between sentences.
</bodyText>
<sectionHeader confidence="0.997791" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999686">
Three methods were used in this study. Given a
pair of sentences, they all produce a score of sim-
ilarity between these sentences. We first present
the preprocessing to be performed on the texts.
</bodyText>
<subsectionHeader confidence="0.998919">
3.1 Prior processing applied on the texts
</subsectionHeader>
<bodyText confidence="0.999652833333333">
The texts were automatically cut into sentences.
The first two steps hereinafter have been applied
for all the methods. The last third was not applied
for the tree editing distance (see 3.3). Punctua-
tion marks and syntactic labels were henceforward
considered as words.
</bodyText>
<listItem confidence="0.99560125">
1. Text homogenization: lemmatization together
with a semantic standardization. Lexical chains
are built using WordNet relations, then words are
replaced by their most representative synonym:
</listItem>
<subsectionHeader confidence="0.825255">
Horizontal links are specific specializations.
</subsectionHeader>
<bodyText confidence="0.763439">
horizontal connection be specific specialization.
</bodyText>
<listItem confidence="0.940436">
2. Syntactic analysis by (Charniak, 1997)’s parser:
( S1 ( S ( NP ( JJ Horizontal ) (NNS links) ( VP
( AUX are) ( NP ( ADJP ( JJ specific) ( NNS
specializations ) ( SENT.)))))))
3. Syntactic structure flattening:
</listItem>
<sectionHeader confidence="0.557227" genericHeader="method">
S1 S NP JJ Horizontal NNS links VP AUX are
NP ADJP JJ specific NNS specializations SENT.
</sectionHeader>
<subsectionHeader confidence="0.999547">
3.2 Wagner &amp; Fischer’s string edit distance
</subsectionHeader>
<bodyText confidence="0.99929425">
This method is based on Wagner &amp; Fischer’s
string edit distance algorithm (Wagner and Fis-
cher, 1974), applied to sentences viewed as strings
of words. It computes a sentence edit distance, us-
ing edit operations on these elementary entities.
The idea is to use edit operations to transform
sentence S1 into S2. Similarly to (Wagner and Fis-
cher, 1974), we considered three edit operations:
</bodyText>
<listItem confidence="0.987754333333333">
1. replacing word x E S1 by y E S2: (x —* y)
2. deleting word x E S1: (x —* A)
3. inserting word y E S2 into S1: (A —* y)
</listItem>
<bodyText confidence="0.7006325">
By definition, the cost of a sequence of edit op-
erations is the sum of the costs2 of the elementary
</bodyText>
<footnote confidence="0.940786">
2We used unitary costs in this study
</footnote>
<page confidence="0.99565">
282
</page>
<bodyText confidence="0.937018833333333">
operations, and the distance between S1 and S2 is
the cost of the least cost transformation of S1 into
S2. Wagner &amp; Fischer’s method provides a simple
and effective way (O( S1 S2 )) to compute it. To
reduce size effects, we normalized by |S1|+|S2|
2 .
</bodyText>
<subsectionHeader confidence="0.999274">
3.3 Zhang &amp; Shasha’s algorithm
</subsectionHeader>
<bodyText confidence="0.996084428571429">
Zhang &amp; Shasha’s method (Zhang and Shasha,
1989; Dulucq and Tichit, 2003) generalizes Wag-
ner &amp; Fischer’s edit distance to trees: given two
trees T1 and T2, it computes the least-cost se-
quence of edit operations that transforms T1 into
T2. Elementary operations have unitary costs and
apply to nodes (labels and words in the syntactic
trees). These operations are depicted below: sub-
stitution of node c by node g (top figure), inser-
tion of node d (middle fig.), and deletion of node
d (bottom fig.), each read from left to right.
Tree edit distance d(T1, T2) is determined after
a series of intermediate calculations involving spe-
cial subtrees of T1 and T2, rooted in keyroots.
</bodyText>
<subsectionHeader confidence="0.817984">
3.3.1 Keyroots, special subtrees and forests
</subsectionHeader>
<bodyText confidence="0.99992325">
Given a certain node x, L(x) denotes its left-
most leaf descendant. L is an equivalence rela-
tion over nodes and keyroots (KR) are by definition
the equivalence relation representatives of high-
est postfix index. Special subtrees (SST) are the
subtrees rooted in these keyroots. Consider a tree
T postfix indexed (left figure hereinafter) and its
three SSTs (right figure).
</bodyText>
<equation confidence="0.992124333333333">
SST(k1) rooted in k1 is denoted:
T [L(k1), L(k1) + 1, ... , k1]. E.g: SST(3) =
T[1, 2,3] is the subtree containing nodes a, b, d.
</equation>
<bodyText confidence="0.980968">
A forest of SST(k1) is defined as:
T[L(k1), L(k1) + 1,... , x], where x is a
node of SST(k1). E.g: SST(3) has 3 forests :
T[1] (node a), T[1, 2] (nodes a and b) and itself.
Forests are ordered sequences of subtrees.
</bodyText>
<subsectionHeader confidence="0.916201">
3.3.2 An idea of how it works
</subsectionHeader>
<bodyText confidence="0.999956868421052">
The algorithm computes the distance between all
pairs of SSTs taken in T1 and T2, rooted in
increasingly-indexed keyroots. In the end, the last
SSTs being the full trees, we have d(T1, T2).
In the main routine, an N1 x N2 array called
TREEDIST is progressively filled with values
TREEDIST(i, j) equal to the distance between the
subtree rooted in T1’s ith node and the subtree
rooted in T2’s jth node. The bottom right-hand
cell of TREEDIST is therefore equal to d(T1, T2).
Each step of the algorithm determines the edit
distance between two SSTs rooted in keyroots
(k1, k2) E (T1 x T2). An array FDIST is ini-
tialized for this step and contains as many lines
and columns as the two given SSTs have nodes.
The array is progressively filled with the distances
between increasing forests of these SSTs, simi-
larly to Wagner &amp; Fischer’s method. The bot-
tom right-hand value of FDIST contains the dis-
tance between the SSTs, which is then stored in
TREEDIST in the appropriate cell. Calculations
in FDIST and TREEDIST rely on the double re-
currence formula depicted below:
The first formula is used to compute the dis-
tance between two forests (a white one and a black
one), each of which is composed of several trees.
The small circles stand for the nodes of highest
postfix index. Distance between two forests is de-
fined as the minimum cost operation between three
possibilities: replacing the rightmost white tree by
the rightmost black tree, deleting the white node,
or inserting the black node.
The second formula is analogous to the first one,
in the special case where the forests are reduced to
a single tree. The distance is defined as the mini-
mum cost operation between: replacing the white
node with the black node, deleting the white node,
or inserting the black node.
</bodyText>
<page confidence="0.989685">
283
</page>
<bodyText confidence="0.999937125">
It is important to notice that the first formula
takes the left context of the considered subtrees
into account3: ancestor and left sibling orders are
preserved. It is not possible to replace the white
node with the black node directly, the whole sub-
tree rooted in the white node has to be replaced.
The good thing is, the cost of this operation has
already been computed and stored in TREEDIST.
Let’s see why all the computations required at a
given step of the recurrence formula have already
been calculated. Let two SSTs of T1 and T2 be
rooted in pos1 and pos2. Considering the symme-
try of the problem, let’s only consider what hap-
pens with T1. When filling FDIST(pos1, pos2),
all nodes belonging to SST(pos1) are run through,
according to increasing postfix indexes. Consider
</bodyText>
<equation confidence="0.855536">
x E T [L(pos1), ... , pos1]:
</equation>
<bodyText confidence="0.842862866666667">
If L(x) = L(pos1), then x belongs to the left-
most branch of T [L(pos1), ... , pos1] and forest
T [L(pos1), ... , x] is reduced to a single tree. By
construction, all FDIST(T [L(pos1), ... , y], −) for
y ≤ x − 1 have already been computed. If things
are the same for the current node in SST(pos2),
then TREEDIST(T[L(pos1), ... , x], −) can be
calculated directly, using the appropriate FDIST
values previously computed.
If L(x) =� L(pos1), then x does not belong
to the leftmost branch of T [L(pos1), ... , pos1]
and therefore x has a non-empty left context
T [L(pos1), ... , L(x) −1]. Let’s see why comput-
ing FDIST(T [L(pos1), ... , x], −) requires values
which have been previously obtained.
</bodyText>
<listItem confidence="0.9983606">
• If x is a keyroot, since the algorithm
runs through keyroots by increasing order,
TREEDIST(T[L(x), ... , x], −) has already
been computed.
• If x is not a keyroot, then there exists a node
</listItem>
<bodyText confidence="0.575162777777778">
z such that : x &lt; z &lt; pos1, z is a keyroot
and L(z) = L(x). Therefore x belongs to
the leftmost branch of T [L(z), ... , z], which
means TREEDIST(T[L(z), ... , x], −) has
already been computed.
Complexity for this algorithm is :
O(|T1 |x |T2 |x min(p(T1), f(T1)) x min(p(T2), f(T2)))
where d(Ti) is the depth Ti and f(Ti) is the num-
ber of terminal nodes of Ti.
</bodyText>
<footnote confidence="0.669921">
3The 2nd formula does too, since left context is empty.
</footnote>
<subsectionHeader confidence="0.956203">
3.4 Our proposal: a degree of similarity
</subsectionHeader>
<bodyText confidence="0.999752782608696">
This final method computes a degree of similar-
ity between two sentences, considered as lists of
syntactic (labels) and lexical (words) constituents.
Because some constituents are more likely to in-
dicate parallelism than others (e.g: the list item
marker is more pertinent than the determiner “a”),
a crescent weight function p(x) E [0, 1] w.r.t.
pertinence is assigned to all lexical and syntac-
tic constituents x. A set of special subsentences
is then generated: the greatest common divisor of
S1 and S2, gcd(S1, S2), is defined as the longest
list of words common to S1 and S2. Then for
each sentence Si, the set of special subsentences
is computed using the words of gcd(S1, S2) ac-
cording to their order of appearance in Si. For
example, if S1 = cabcad and S2 = acbae,
gcd(S1, S2) = {c, a, b, a}. The set of subsen-
tences for S1 is {caba, abca} and the set for S2 is
reduced to {acba}. Note that any generated sub-
sentence is exactly the size of gcd(S1, S2).
For any two subsentences s1 and s2, we define
a degree of similarity D(s1, s2), inspired from
string edit distances:
</bodyText>
<equation confidence="0.99484525">
nn (dmax − d(xi) /
x p(xi)
dmax
i=1
</equation>
<bodyText confidence="0.966500666666667">
I n size of all subsentences
xi ith constituent of s1
dmax max possible dist. between any xi E s1 and its
parallel constituent in s2, i.e. dmax = n − 1
d(xi) distance between current constituent xi
in s1 and its parallel constituent in s2
p(xi) parallelism weight of xi
The further a constituent from s1 is from its
symmetric occurrence in s2, the more similar
the compared subsentences are. Eventually, the
degree of similarity between sentences S1 and S2
is defined as:
</bodyText>
<equation confidence="0.847073">
2
D(S1, S2) = |S1 |+ |S2 |x max
s2)
s1,s
Example
</equation>
<bodyText confidence="0.996153833333333">
Consider S1 = cabcad and S2 = acbae, along
with their subsentences s1 = caba and si = abca
for S1, and s2 = acba for S2. The degrees of
parallelism between s1 and s2, and between si
and s2 are computed. The mapping between the
parallel constituents is shown below.
</bodyText>
<equation confidence="0.945919">
D(s1, s2) =
</equation>
<page confidence="0.50487">
284
</page>
<figure confidence="0.316504">
3 − d(xi) × p(xi)
«
3
= 2/3p(c)+2/3p(a)+p(b)+p(a)
Assume p(b) = p(c) = 21 and p(a) = 1. Then
D(s1, s2) = 2.5 and, similarly D(si, s2) ^_ 2.67.
Therefore the normalized degree of parallelism is
D(S1, S2) = 2
5+6 x 2.67, which is about 0.48.
</figure>
<sectionHeader confidence="0.990286" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999676">
This section describes the methodology employed
to evaluate performances. Then, after a prelimi-
nary study of our corpus, results are presented suc-
cessively for each method. Finally, the behavior of
the methods is analyzed at sentence level.
</bodyText>
<subsectionHeader confidence="0.887703">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999763463414635">
Our parallelism detection is an unsupervised clus-
tering application: given a set of pairs of sen-
tences, it automatically classifies them into the
class of the parallelisms and the remainders
class. Pairs were extracted from 5 scientific ar-
ticles written in English, each containing about
200 sentences: Green (ACL’98), Kan (Kan et
al. WVLC’98), Mitkov (Coling-ACL’98), Oakes
(IRSG’99) and Sand (Sanderson et al. SIGIR’99).
The idea was to compute for each pair a paral-
lelism score indicating the similarity between the
sentences. Then the choice of a threshold deter-
mined which pairs showed a score high enough to
be classified as parallel.
Evaluation was based on a manual annotation
we proceeded over the texts. In order to reduce
computational complexity, we only considered the
parallelism occurring between consecutive sen-
tences. For each sentence, we indicated the index
of its parallel sentence. We assumed transitivity of
parallelism: if S1//S2 and S2//S3, then S1//S3.
It was thus considered sufficient to indicate the in-
dex of S1 for S2 and the index of S2 for S3 to
account for a parallelism between S1, S2 and S3.
We annotated pairs of sentences where textual
parallelism led us to rhetorically coordinate them.
The decision was sometimes hard to make. Yet
we annotated it each time to get more data and to
study the behavior of the methods on these exam-
ples, possibly penalizing our applications. In the
end, 103 pairs were annotated.
We used the notions of precision (correctness)
and recall (completeness). Because efforts in im-
proving one often result in degrading the other,
the F-measure (harmonic mean) combines them
into a unique parameter, which simplifies compar-
isons of results. Let P be the set of the annotated
parallelisms and Q the set of the pairs automati-
cally classified in the parallelisms after the use of
a threshold. Then the associated precision p, recall
r and F-measure f are defined as:
</bodyText>
<equation confidence="0.9080295">
__ |P n Q |__ |P n Q |__ 2
p = IQ |r IP |f 1/p + 1/q
</equation>
<bodyText confidence="0.999044555555555">
As we said, the unique task of the implemented
methods was to assign parallelism scores to pairs
of sentences, which are collected in a list. We
manually applied various thresholds to the list
and computed their corresponding F-measure. We
kept as a performance indicator the best F-measure
found. This was performed for each method and
on each text, as well as on the texts all gathered
together.
</bodyText>
<subsectionHeader confidence="0.981961">
4.2 Preliminary corpus study
</subsectionHeader>
<bodyText confidence="0.9994362">
This paragraph underlines some of the character-
istics of the corpus, in particular the distribution of
the annotated parallelisms in the texts for adjacent
sentences. The following table gives the percent-
age of parallelisms for each text:
</bodyText>
<table confidence="0.995060571428572">
Parallelisms Nb of pairs
Green 39 (14.4 %) 270
Kan 12 (6 %) 200
Mitkov 13 (8.4 %) 168
Oakes 22 (13.7 %) 161
Sand 17 (7.7 %) 239
All gathered 103 (9.9 %) 1038
</table>
<bodyText confidence="0.999112333333333">
Green and Oakes show significantly more paral-
lelisms than the other texts. Therefore, if we con-
sider a lazy method that would put all pairs in the
class of parallelisms, Green and Oakes will yield
a priori better results. Precision is indeed directly
related to the percentage of parallelisms in the text.
In this case, it is exactly this percentage, and it
gives us a minimum value of the F-measure our
methods should at least reach:
</bodyText>
<table confidence="0.999540142857143">
Precision Recall F-measure
Green 14.4 100 25.1
Kan 6 100 11.3
Mitkov 8.4 100 15.5
Oakes 13.7 100 24.1
Sand 7.7 100 14.3
All 9.9 100 18.0
</table>
<subsectionHeader confidence="0.69793">
4.3 A baseline: counting words in common
</subsectionHeader>
<bodyText confidence="0.99887">
We first present the results of a very simple and
thus very fast method. This baseline counts the
</bodyText>
<equation confidence="0.9417114">
For example:
D(s1, s2) =
„
X4
i=1
</equation>
<page confidence="0.994074">
285
</page>
<bodyText confidence="0.9908532">
words sentences S1 and S2 have in common, and
normalizes the result by |S1|+|S2|
2 in order to re-
duce size effects. No syntactic analysis nor lexical
homogenization was performed on the texts.
Results for this method are summarized in the fol-
lowing table. The last column shows the loss (%)
in F-measure after applying a generic threshold
(the optimal threshold found when all texts are
gathered together) on each text.
</bodyText>
<table confidence="0.998874857142857">
F-meas. Prec. Recall Thres. Loss
Green 45 34 67 0.4 2
Kan 24 40 17 0.9 10
Mitkov 22 13 77 0.0 8
Oakes 45 78 32 0.8 7
Sand 23 17 35 0.5 1
All 30 23 42 0.5 -
</table>
<bodyText confidence="0.999443909090909">
We first note that results are twice as good as
with the lazy approach, with Green and Oakes
far above the rest. Yet this is not sufficient for a
real application. Furthermore, the optimal thresh-
old is very different from one text to another,
which makes the learning of a generic threshold
able to detect parallelisms for any text impossible.
The only advantage here is the simplicity of the
method: no prior treatment was performed on the
texts before the search, and the counting itself was
very fast.
</bodyText>
<subsectionHeader confidence="0.998327">
4.4 String edit distance
</subsectionHeader>
<bodyText confidence="0.991947">
We present the results for the 1&amp;quot; method below:
</bodyText>
<table confidence="0.998044428571428">
F-meas. Prec. Recall Thres. Loss
Green 52 79 38 0.69 0
Kan 44 67 33 0.64 2
Mitkov 38 50 31 0.69 0
Oakes 82 94 73 0.68 0
Sand 47 54 42 0.72 9
All 54 73 43 0.69 -
</table>
<bodyText confidence="0.998352272727273">
Green and Oakes still yield the best results, but
the other texts have almost doubled theirs. Results
for Oakes are especially good: an F-measure of
82% guaranties high precision and recall.
In addition, the use of a generic threshold on
each text had little influence on the value of the
F-measure. The greatest loss is for Sand and only
corresponds to the adjunction of four pairs of sen-
tences in the class of parallelisms. The selection of
a unique generic threshold to predict parallelisms
should therefore be possible.
</bodyText>
<subsectionHeader confidence="0.984656">
4.5 Tree edit distance
</subsectionHeader>
<bodyText confidence="0.992808">
The algorithm was applied using unitary edit
costs. Since it did not seem natural to establish
mappings between different levels of the sentence,
edit operations between two constituents of dif-
ferent nature (e.g: substitution of a lexical by a
syntactic element) were forbidden by a prohibitive
cost (1000). However, this banning only improved
the results shyly, unfortunately.
</bodyText>
<table confidence="0.999273714285714">
F-meas. Prec. Recall Thres. Loss
Green 46 92 31 0.72 3
Kan 44 67 33 0.75 0
Mitkov 43 40 46 0.87 11
Oakes 81 100 68 0.73 0
Sand 52 100 35 0.73 2
All 51 73 39 0.75 -
</table>
<bodyText confidence="0.9983634">
As illustrated in the table above, results are
comparable to those previously found. We note an
especially good F-measure for Sand: 52%, against
47% for the string edit distance. Optimal thresh-
olds were quite similar from one text to another.
</bodyText>
<subsectionHeader confidence="0.996983">
4.6 Degree of similarity
</subsectionHeader>
<bodyText confidence="0.9996504375">
Because of the high complexity of this method, a
heuristic was applied. The generation of the sub-
sentences is indeed in H C�i
�i, kz being the number
of occurrences of the constituent xz in gcd, and
nz the number of xz in the sentence. We chose
to limit the generation to a fixed amount of sub-
sentences. The constituents that have a great Cn
bring too much complexity: we chose to eliminate
their (nz − kz) last occurrences and to keep their
kz first occurrences only to generate subsequences.
An experiment was conducted in order to
determine the maximum amount of subsentences
that could be generated in a reasonable amount of
time without significant performance loss and 30
was a sufficient number. In another experiment,
different parallelism weights were assigned to
lexical constituents and syntactic labels. The aim
was to understand their relative importance for
parallelisms detection. Results show that lexical
constituents have a significant role, but conclu-
sions are more difficult to draw for syntactic
labels. It was decided that, from now on, the lex-
ical weight should be given the maximum value, 1.
Finally, we assigned different weights to the
syntactic labels. Weights were chosen after count-
ing the occurrences of the labels in the corpus. In
fact, we counted for each label the percentage of
occurrences that appeared in the gcd of the paral-
lelisms with respect to those appearing in the gcd
of the other pairs. Percentages were then rescaled
from 0 to 1, in order to emphasize differences
</bodyText>
<page confidence="0.994631">
286
</page>
<bodyText confidence="0.9882485">
between labels. The obtained parallelism values
measured the role of the labels in the detection of
parallelism. Results for this experiment appear in
the table below.
</bodyText>
<table confidence="0.997698142857143">
F-meas. Prec. Recall Thres. Loss
Green 55 59 51 0.329 2
Kan 47 80 33 0.354 5
Mitkov 35 40 31 0.355 0
Oakes 76 80 73 0.324 4
Sand 29 20 59 0.271 0
All 50 59 43 0.335 -
</table>
<bodyText confidence="0.983302666666667">
The optimal F-measures were comparable to
those obtained in 4.4 and the corresponding
thresholds were similar from one text to another.
This section showed how the three proposed
methods outperformed the baseline. Each of them
yielded comparable results.
The next section presents the results at sentence
level, together with a comparison of these three
methods.
</bodyText>
<subsectionHeader confidence="0.998641">
4.7 Analysis at sentence level
</subsectionHeader>
<bodyText confidence="0.983283215384616">
The different methods often agreed but sometimes
reacted quite differently.
Well retrieved parallelisms
Some parallelisms were found by each method
with no difficulty: they were given a high degree
of parallelism by each method. Typically, such
sentences presented a strong lexical and syntactic
similarity, as in the example in section 2.
Parallelisms hard to find
Other parallelisms received very low scores
from each method. This happened when the an-
notated parallelism was lexically and syntactically
poor and needed either contextual information or
external semantic knowledge to find keywords
(e.g: “first”, “second”, ... ), paraphrases or pat-
terns (e.g: “X:Y” in the following example (Kan)):
Rear: a paragraph in which a link just stopped
occurring the paragraph before.
No link: any remaining paragraphs.
Different methods, different results
Eventually, we present some parallelisms that
obtained very different scores, depending on the
method.
First, it seems that a different ordering of the
parallel constituents in the sentences alter the per-
formances of the edit distance algorithms (3.2;
3.3). The following example (Green) received a
low score with both methods:
When we consider AnsV as our dependent vari-
able, the model for the High Web group is still
not significant, and there is still a high probabil-
ity that the coefficient of LI is 0.
For our Low Web group, who followed signif-
icantly more intra-article links than the High
Web group, the model that results is significant
and has the following equation: &lt;EQN/&gt;.
This is due to the fact that both algorithms do not
allow the inversion of two constituents and thus
are unable to find all the links from the first sen-
tence to the other. The parallelism measure is ro-
bust to inversion.
Sometimes, the syntactic parser gave different
analyses for the same expression, which made
mapping between the sentences containing this ex-
pression more difficult, especially for the tree edit
distance. The syntactic structure has less impor-
tance for the other methods, which are thus more
insensitive to an incorrect analysis.
Finally, the parallelism measure seems more
adapted to a diffuse distribution of the parallel
constituents in the sentences, whereas edit dis-
tances seem more appropriate when parallel con-
stituents are concentrated in a certain part of the
sentences, in similar syntactic structures. The fol-
lowing example (Green) obtained very high scores
with the edit distances only:
Strong relations are also said to exist between
words that have synsets connected by a single
horizontal link or words that have synsets con-
nected by a single IS-A or INCLUDES relation.
A regular relation is said to exist between two
words when there is at least one allowable path
between a synset containing the first word and a
synset containing the second word in the Word-
Net database.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999736545454545">
Experimental work in psycholinguistics has
shown the importance of the parallelism effect in
human language processing. Due to some kind
of priming (syntactic, phonetic, lexical, etc.), the
comprehension and the production of a parallel ut-
terance is made faster (Dubey et al., 2005).
So far, most of the works were led in order to
acquire resources and to build systems to retrieve
specific parallelism phenomena. In the field of in-
formation structure theories, (Kruijff-Korbayov´a
and Kruijff, 1996) implemented an ad-hoc system
</bodyText>
<page confidence="0.985867">
287
</page>
<bodyText confidence="0.999950382352941">
to identify thematic continuity (lexical relation be-
tween the subject parts of consecutive sentences).
(Luc et al., 1999) described and classified markers
(lexical clues, layout and typography) occurring in
enumeration structures. (Summers, 1998) also de-
scribed the markers required for retrieving head-
ing structures. (Charolles, 1997) was involved in
the description of frame introducers.
Integration of specialized resources dedicated
to parallelism detection could be an improvement
to our approach. Let us not forget that our fi-
nal aim remains the detection of discourse struc-
tures. Parallelism should be considered as an ad-
ditional feature which among other discourse fea-
tures (e.g. connectors).
Regarding the use of parallelism, (Hernandez
and Grau, 2005) proposed an algorithm to parse
the discourse structure and to select pairs of sen-
tences to compare.
Confronted to the problem of determining tex-
tual entailment4 (the fact that the meaning of
one expression can be inferred from another)
(Kouylekov and Magnini, 2005) applied the
(Zhang and Shasha, 1989)’s algorithm on the de-
pendency trees of pairs of sentences (they did not
consider syntactic tags as nodes but only words).
They encountered problems similar to ours due to
pre-treatment limits. Indeed, the syntactic parser
sometimes represents in a different way occur-
rences of similar expressions, making it harder to
apply edit transformations. A drawback concern-
ing the tree-edit distance approach is that it is not
able to observe the whole tree, but only the subtree
of the processed node.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.962587695652174">
Textual parallelism plays an important role among
discourse features when detecting discourse struc-
tures. So far, only occurrences of this phenomenon
have been treated individually and often in an ad-
hoc manner. Our contribution is a unifying frame-
work which can be used for automatic processing
with much less specific knowledge than dedicated
techniques.
In addition, we discussed and evaluated several
methods to retrieve them generically. We showed
that simple methods such as (Wagner and Fis-
cher, 1974) can compete with more complex ap-
proaches, such as our degree of similarity and the
4Compared to entailment, the parallelism relation is bi-
directional and not restricted to semantic similarities.
(Zhang and Shasha, 1989)’s algorithm.
Among future works, it seems that variations
such as the editing cost of transformation for edit
distance methods and the weight of parallel units
(depending their semantic and syntactic charac-
teristics) can be implemented to enhance perfor-
mances. Combining methods also seems an inter-
esting track to follow.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999762928571428">
Nicholas Asher. 1993. Reference to abstract objects in
discourse. Kluwer, Dordrecht.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI.
M. Charolles. 1997. L’encadrement du discours -
univers, champs, domaines et espaces. Cahier de
recherche linguistique, 6.
Amit Dubey, Patrick Sturt, and Frank Keller. 2005.
Parallelism in coordination as an instance of syntac-
tic priming: Evidence from corpus-based modeling.
In HLTC and CEMNLP, Vancouver.
S. Dulucq and L. Tichit. 2003. RNA Secondary
Structure Comparison: Exact Analysis of the Zhang-
Shasha Tree Edit Algorithm. Theoretical Computer
Science, 306(1-3):471–484.
N. Hernandez and B. Grau. 2005. D´etection au-
tomatique de structures fines du discours. In TALN,
France.
J. R. Hobbs and A. Kehler. 1997. A theory of paral-
lelism and the case of vp ellipsis. In ACL.
M. Kouylekov and B. Magnini. 2005. Recognizing
Textual Entailment with Tree Edit Distance Algo-
rithms. PASCAL Challenges on RTE.
I. Kruijff-Korbayov´a and G.-J. M. Kruijff. 1996. Iden-
tification of topic-focus chains. In DAARC, vol-
ume 8, pages 165–179. University of Lancaster, UK.
C. Luc, M. Mojahid, J. Virbel, Cl. Garcia-Debanc, and
M.-P. P´ery-Woodley. 1999. A linguistic approach
to some parameters of layout: A study of enumera-
tions. In AAAI, North Falmouth, Massachusets.
W. C. Mann and S. A. Thompson. 1987. Rhetori-
cal structure theory: A theory of text organisation.
Technical report isi/rs-87-190.
K. M. Summers. 1998. Automatic Discovery of Logi-
cal Document Structure. Ph.D. thesis, U. of Cornell.
R.A. Wagner and M.J. Fischer. 1974. The String-to-
String Correction Problem. Journal of the ACM,
21(1):168–173.
K. Zhang and D. Shasha. 1989. Simple fast algo-
rithms for the editing distance between trees and
related problems. SIAM Journal on Computing,
18(6):1245–1262.
</reference>
<page confidence="0.997111">
288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.475449">
<title confidence="0.99936">Recognizing Textual Parallelisms with edit distance and similarity degree</title>
<author confidence="0.999976">Marie Gu´egan</author>
<author confidence="0.999976">Nicolas Hernandez</author>
<affiliation confidence="0.7516235">LIMSI-CNRS Universit´e de Paris-Sud, France</affiliation>
<email confidence="0.975856">guegan@aist.enst.fr|hernandez@limsi.fr</email>
<abstract confidence="0.997071333333333">Detection of discourse structure is crucial in many text-based applications. This paper presents an original framework for describing textual parallelism which allows us to generalize various discourse phenomena and to propose a unique method to recognize them. With this prospect, we discuss several methods in order to identify the most appropriate one for the problem, and evaluate them based on a manually annotated corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
</authors>
<title>Reference to abstract objects in discourse.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1112" citStr="Asher, 1993" startWordPosition="164" endWordPosition="165">ral methods in order to identify the most appropriate one for the problem, and evaluate them based on a manually annotated corpus. 1 Introduction Detection of discourse structure is crucial in many text-based applications such as Information Retrieval, Question-Answering, Text Browsing, etc. Thanks to a discourse structure one can precisely point out an information, provide it a local context, situate it globally, link it to others. The context of our research is to improve automatic discourse analysis. A key feature of the most popular discourse theories (RST (Mann and Thompson, 1987), SDRT (Asher, 1993), etc.) is the distinction between two sorts of discourse relations or rhetorical functions: the subordinating and the coordinating relations (some parts of a text play a subordinate role relative to other parts, while some others have equal importance). In this paper, we focus our attention on a discourse feature we assume supporting coordination relations, namely the Textual Parallelism. Based on psycholinguistics studies (Dubey et al., 2005), our intuition is that similarities concerning the surface, the content and the structure of textual units can be a way for authors to explicit their i</context>
</contexts>
<marker>Asher, 1993</marker>
<rawString>Nicholas Asher. 1993. Reference to abstract objects in discourse. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a contextfree grammar and word statistics.</title>
<date>1997</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="6856" citStr="Charniak, 1997" startWordPosition="1049" endWordPosition="1050">d on the texts The texts were automatically cut into sentences. The first two steps hereinafter have been applied for all the methods. The last third was not applied for the tree editing distance (see 3.3). Punctuation marks and syntactic labels were henceforward considered as words. 1. Text homogenization: lemmatization together with a semantic standardization. Lexical chains are built using WordNet relations, then words are replaced by their most representative synonym: Horizontal links are specific specializations. horizontal connection be specific specialization. 2. Syntactic analysis by (Charniak, 1997)’s parser: ( S1 ( S ( NP ( JJ Horizontal ) (NNS links) ( VP ( AUX are) ( NP ( ADJP ( JJ specific) ( NNS specializations ) ( SENT.))))))) 3. Syntactic structure flattening: S1 S NP JJ Horizontal NNS links VP AUX are NP ADJP JJ specific NNS specializations SENT. 3.2 Wagner &amp; Fischer’s string edit distance This method is based on Wagner &amp; Fischer’s string edit distance algorithm (Wagner and Fischer, 1974), applied to sentences viewed as strings of words. It computes a sentence edit distance, using edit operations on these elementary entities. The idea is to use edit operations to transform senten</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical parsing with a contextfree grammar and word statistics. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Charolles</author>
</authors>
<title>L’encadrement du discours -univers, champs, domaines et espaces. Cahier de recherche linguistique,</title>
<date>1997</date>
<pages>6</pages>
<contexts>
<context position="1969" citStr="Charolles, 1997" startWordPosition="293" endWordPosition="294"> importance). In this paper, we focus our attention on a discourse feature we assume supporting coordination relations, namely the Textual Parallelism. Based on psycholinguistics studies (Dubey et al., 2005), our intuition is that similarities concerning the surface, the content and the structure of textual units can be a way for authors to explicit their intention to consider these units with the same rhetorical importance. Parallelism can be encountered in many specific discourse structures such as continuity in information structure (Kruijff-Korbayov´a and Kruijff, 1996), frame structures (Charolles, 1997), VP ellipses (Hobbs and Kehler, 1997), headings (Summers, 1998), enumerations (Luc et al., 1999), etc. These phenomena are usually treated mostly independently within individual systems with ad-hoc resource developments. In this work, we argue that, depending on description granularity we can proceed, computing syntagmatic (succession axis of linguistic units) and paradigmatic (substitution axis) similarities between units can allow us to generically handle such discourse structural phenomena. Section 2 introduces the discourse parallelism phenomenon. Section 3 develops three methods we imple</context>
<context position="28110" citStr="Charolles, 1997" startWordPosition="4765" endWordPosition="4766">e faster (Dubey et al., 2005). So far, most of the works were led in order to acquire resources and to build systems to retrieve specific parallelism phenomena. In the field of information structure theories, (Kruijff-Korbayov´a and Kruijff, 1996) implemented an ad-hoc system 287 to identify thematic continuity (lexical relation between the subject parts of consecutive sentences). (Luc et al., 1999) described and classified markers (lexical clues, layout and typography) occurring in enumeration structures. (Summers, 1998) also described the markers required for retrieving heading structures. (Charolles, 1997) was involved in the description of frame introducers. Integration of specialized resources dedicated to parallelism detection could be an improvement to our approach. Let us not forget that our final aim remains the detection of discourse structures. Parallelism should be considered as an additional feature which among other discourse features (e.g. connectors). Regarding the use of parallelism, (Hernandez and Grau, 2005) proposed an algorithm to parse the discourse structure and to select pairs of sentences to compare. Confronted to the problem of determining textual entailment4 (the fact th</context>
</contexts>
<marker>Charolles, 1997</marker>
<rawString>M. Charolles. 1997. L’encadrement du discours -univers, champs, domaines et espaces. Cahier de recherche linguistique, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
<author>Patrick Sturt</author>
<author>Frank Keller</author>
</authors>
<title>Parallelism in coordination as an instance of syntactic priming: Evidence from corpus-based modeling.</title>
<date>2005</date>
<booktitle>In HLTC and CEMNLP,</booktitle>
<location>Vancouver.</location>
<contexts>
<context position="1560" citStr="Dubey et al., 2005" startWordPosition="230" endWordPosition="233">context of our research is to improve automatic discourse analysis. A key feature of the most popular discourse theories (RST (Mann and Thompson, 1987), SDRT (Asher, 1993), etc.) is the distinction between two sorts of discourse relations or rhetorical functions: the subordinating and the coordinating relations (some parts of a text play a subordinate role relative to other parts, while some others have equal importance). In this paper, we focus our attention on a discourse feature we assume supporting coordination relations, namely the Textual Parallelism. Based on psycholinguistics studies (Dubey et al., 2005), our intuition is that similarities concerning the surface, the content and the structure of textual units can be a way for authors to explicit their intention to consider these units with the same rhetorical importance. Parallelism can be encountered in many specific discourse structures such as continuity in information structure (Kruijff-Korbayov´a and Kruijff, 1996), frame structures (Charolles, 1997), VP ellipses (Hobbs and Kehler, 1997), headings (Summers, 1998), enumerations (Luc et al., 1999), etc. These phenomena are usually treated mostly independently within individual systems with</context>
<context position="27523" citStr="Dubey et al., 2005" startWordPosition="4678" endWordPosition="4681">ets connected by a single horizontal link or words that have synsets connected by a single IS-A or INCLUDES relation. A regular relation is said to exist between two words when there is at least one allowable path between a synset containing the first word and a synset containing the second word in the WordNet database. 5 Related work Experimental work in psycholinguistics has shown the importance of the parallelism effect in human language processing. Due to some kind of priming (syntactic, phonetic, lexical, etc.), the comprehension and the production of a parallel utterance is made faster (Dubey et al., 2005). So far, most of the works were led in order to acquire resources and to build systems to retrieve specific parallelism phenomena. In the field of information structure theories, (Kruijff-Korbayov´a and Kruijff, 1996) implemented an ad-hoc system 287 to identify thematic continuity (lexical relation between the subject parts of consecutive sentences). (Luc et al., 1999) described and classified markers (lexical clues, layout and typography) occurring in enumeration structures. (Summers, 1998) also described the markers required for retrieving heading structures. (Charolles, 1997) was involved</context>
</contexts>
<marker>Dubey, Sturt, Keller, 2005</marker>
<rawString>Amit Dubey, Patrick Sturt, and Frank Keller. 2005. Parallelism in coordination as an instance of syntactic priming: Evidence from corpus-based modeling. In HLTC and CEMNLP, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dulucq</author>
<author>L Tichit</author>
</authors>
<title>RNA Secondary Structure Comparison: Exact Analysis of the ZhangShasha Tree Edit Algorithm.</title>
<date>2003</date>
<journal>Theoretical Computer Science,</journal>
<pages>306--1</pages>
<contexts>
<context position="8168" citStr="Dulucq and Tichit, 2003" startWordPosition="1295" endWordPosition="1298">ons: 1. replacing word x E S1 by y E S2: (x —* y) 2. deleting word x E S1: (x —* A) 3. inserting word y E S2 into S1: (A —* y) By definition, the cost of a sequence of edit operations is the sum of the costs2 of the elementary 2We used unitary costs in this study 282 operations, and the distance between S1 and S2 is the cost of the least cost transformation of S1 into S2. Wagner &amp; Fischer’s method provides a simple and effective way (O( S1 S2 )) to compute it. To reduce size effects, we normalized by |S1|+|S2| 2 . 3.3 Zhang &amp; Shasha’s algorithm Zhang &amp; Shasha’s method (Zhang and Shasha, 1989; Dulucq and Tichit, 2003) generalizes Wagner &amp; Fischer’s edit distance to trees: given two trees T1 and T2, it computes the least-cost sequence of edit operations that transforms T1 into T2. Elementary operations have unitary costs and apply to nodes (labels and words in the syntactic trees). These operations are depicted below: substitution of node c by node g (top figure), insertion of node d (middle fig.), and deletion of node d (bottom fig.), each read from left to right. Tree edit distance d(T1, T2) is determined after a series of intermediate calculations involving special subtrees of T1 and T2, rooted in keyroo</context>
</contexts>
<marker>Dulucq, Tichit, 2003</marker>
<rawString>S. Dulucq and L. Tichit. 2003. RNA Secondary Structure Comparison: Exact Analysis of the ZhangShasha Tree Edit Algorithm. Theoretical Computer Science, 306(1-3):471–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Hernandez</author>
<author>B Grau</author>
</authors>
<title>D´etection automatique de structures fines du discours.</title>
<date>2005</date>
<booktitle>In TALN,</booktitle>
<contexts>
<context position="28536" citStr="Hernandez and Grau, 2005" startWordPosition="4828" endWordPosition="4831">classified markers (lexical clues, layout and typography) occurring in enumeration structures. (Summers, 1998) also described the markers required for retrieving heading structures. (Charolles, 1997) was involved in the description of frame introducers. Integration of specialized resources dedicated to parallelism detection could be an improvement to our approach. Let us not forget that our final aim remains the detection of discourse structures. Parallelism should be considered as an additional feature which among other discourse features (e.g. connectors). Regarding the use of parallelism, (Hernandez and Grau, 2005) proposed an algorithm to parse the discourse structure and to select pairs of sentences to compare. Confronted to the problem of determining textual entailment4 (the fact that the meaning of one expression can be inferred from another) (Kouylekov and Magnini, 2005) applied the (Zhang and Shasha, 1989)’s algorithm on the dependency trees of pairs of sentences (they did not consider syntactic tags as nodes but only words). They encountered problems similar to ours due to pre-treatment limits. Indeed, the syntactic parser sometimes represents in a different way occurrences of similar expressions</context>
</contexts>
<marker>Hernandez, Grau, 2005</marker>
<rawString>N. Hernandez and B. Grau. 2005. D´etection automatique de structures fines du discours. In TALN, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
<author>A Kehler</author>
</authors>
<title>A theory of parallelism and the case of vp ellipsis.</title>
<date>1997</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2007" citStr="Hobbs and Kehler, 1997" startWordPosition="298" endWordPosition="301"> focus our attention on a discourse feature we assume supporting coordination relations, namely the Textual Parallelism. Based on psycholinguistics studies (Dubey et al., 2005), our intuition is that similarities concerning the surface, the content and the structure of textual units can be a way for authors to explicit their intention to consider these units with the same rhetorical importance. Parallelism can be encountered in many specific discourse structures such as continuity in information structure (Kruijff-Korbayov´a and Kruijff, 1996), frame structures (Charolles, 1997), VP ellipses (Hobbs and Kehler, 1997), headings (Summers, 1998), enumerations (Luc et al., 1999), etc. These phenomena are usually treated mostly independently within individual systems with ad-hoc resource developments. In this work, we argue that, depending on description granularity we can proceed, computing syntagmatic (succession axis of linguistic units) and paradigmatic (substitution axis) similarities between units can allow us to generically handle such discourse structural phenomena. Section 2 introduces the discourse parallelism phenomenon. Section 3 develops three methods we implemented to detect it: a similarity degr</context>
<context position="3236" citStr="Hobbs and Kehler, 1997" startWordPosition="472" endWordPosition="475">sure, a string editing distance (Wagner and Fischer, 1974) and a tree editing distance1 (Zhang and Shasha, 1989). Section 4 discusses and evaluates these methods and their relevance. The final section reviews related work. 2 Textual parallelism Our notion of parallelism is based on similarities between syntagmatic and paradigmatic representations of (constituents of) textual units. These similarities concern various dimensions from shallow to deeper description: layout, typography, morphology, lexicon, syntax, and semantics. This account is not limited to the semantic dimension as defined by (Hobbs and Kehler, 1997) who consider text fragments as parallel if the same predicate can be inferred from them with coreferential or similar pairs of arguments. 1For all measures, elementary units considered are syntactic tags and word tokens. 281 We observe parallelism at various structural levels of text: among heading structures, VP ellipses and others, enumerations of noun phrases in a sentence, enumerations with or without markers such as frame introducers (e.g. “In France, ...In Italy, ... ”) or typographical and layout markers. The underlying assumption is that parallelism between some textual units accounts</context>
</contexts>
<marker>Hobbs, Kehler, 1997</marker>
<rawString>J. R. Hobbs and A. Kehler. 1997. A theory of parallelism and the case of vp ellipsis. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Recognizing Textual Entailment with Tree Edit Distance Algorithms.</title>
<date>2005</date>
<journal>PASCAL Challenges on RTE.</journal>
<contexts>
<context position="28802" citStr="Kouylekov and Magnini, 2005" startWordPosition="4871" endWordPosition="4874">on of specialized resources dedicated to parallelism detection could be an improvement to our approach. Let us not forget that our final aim remains the detection of discourse structures. Parallelism should be considered as an additional feature which among other discourse features (e.g. connectors). Regarding the use of parallelism, (Hernandez and Grau, 2005) proposed an algorithm to parse the discourse structure and to select pairs of sentences to compare. Confronted to the problem of determining textual entailment4 (the fact that the meaning of one expression can be inferred from another) (Kouylekov and Magnini, 2005) applied the (Zhang and Shasha, 1989)’s algorithm on the dependency trees of pairs of sentences (they did not consider syntactic tags as nodes but only words). They encountered problems similar to ours due to pre-treatment limits. Indeed, the syntactic parser sometimes represents in a different way occurrences of similar expressions, making it harder to apply edit transformations. A drawback concerning the tree-edit distance approach is that it is not able to observe the whole tree, but only the subtree of the processed node. 6 Conclusion Textual parallelism plays an important role among disco</context>
</contexts>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>M. Kouylekov and B. Magnini. 2005. Recognizing Textual Entailment with Tree Edit Distance Algorithms. PASCAL Challenges on RTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kruijff-Korbayov´a</author>
<author>G-J M Kruijff</author>
</authors>
<title>Identification of topic-focus chains.</title>
<date>1996</date>
<booktitle>In DAARC,</booktitle>
<volume>8</volume>
<pages>165--179</pages>
<institution>University of Lancaster, UK.</institution>
<marker>Kruijff-Korbayov´a, Kruijff, 1996</marker>
<rawString>I. Kruijff-Korbayov´a and G.-J. M. Kruijff. 1996. Identification of topic-focus chains. In DAARC, volume 8, pages 165–179. University of Lancaster, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Garcia-Debanc</author>
<author>M-P P´ery-Woodley</author>
</authors>
<title>A linguistic approach to some parameters of layout: A study of enumerations.</title>
<date>1999</date>
<booktitle>In AAAI,</booktitle>
<location>North Falmouth, Massachusets.</location>
<marker>Garcia-Debanc, P´ery-Woodley, 1999</marker>
<rawString>C. Luc, M. Mojahid, J. Virbel, Cl. Garcia-Debanc, and M.-P. P´ery-Woodley. 1999. A linguistic approach to some parameters of layout: A study of enumerations. In AAAI, North Falmouth, Massachusets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical structure theory: A theory of text organisation.</title>
<date>1987</date>
<tech>Technical report</tech>
<pages>87--190</pages>
<contexts>
<context position="1092" citStr="Mann and Thompson, 1987" startWordPosition="159" endWordPosition="162">h this prospect, we discuss several methods in order to identify the most appropriate one for the problem, and evaluate them based on a manually annotated corpus. 1 Introduction Detection of discourse structure is crucial in many text-based applications such as Information Retrieval, Question-Answering, Text Browsing, etc. Thanks to a discourse structure one can precisely point out an information, provide it a local context, situate it globally, link it to others. The context of our research is to improve automatic discourse analysis. A key feature of the most popular discourse theories (RST (Mann and Thompson, 1987), SDRT (Asher, 1993), etc.) is the distinction between two sorts of discourse relations or rhetorical functions: the subordinating and the coordinating relations (some parts of a text play a subordinate role relative to other parts, while some others have equal importance). In this paper, we focus our attention on a discourse feature we assume supporting coordination relations, namely the Textual Parallelism. Based on psycholinguistics studies (Dubey et al., 2005), our intuition is that similarities concerning the surface, the content and the structure of textual units can be a way for authors</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>W. C. Mann and S. A. Thompson. 1987. Rhetorical structure theory: A theory of text organisation. Technical report isi/rs-87-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Summers</author>
</authors>
<date>1998</date>
<booktitle>Automatic Discovery of Logical Document Structure. Ph.D. thesis,</booktitle>
<institution>U. of Cornell.</institution>
<contexts>
<context position="2033" citStr="Summers, 1998" startWordPosition="303" endWordPosition="305"> feature we assume supporting coordination relations, namely the Textual Parallelism. Based on psycholinguistics studies (Dubey et al., 2005), our intuition is that similarities concerning the surface, the content and the structure of textual units can be a way for authors to explicit their intention to consider these units with the same rhetorical importance. Parallelism can be encountered in many specific discourse structures such as continuity in information structure (Kruijff-Korbayov´a and Kruijff, 1996), frame structures (Charolles, 1997), VP ellipses (Hobbs and Kehler, 1997), headings (Summers, 1998), enumerations (Luc et al., 1999), etc. These phenomena are usually treated mostly independently within individual systems with ad-hoc resource developments. In this work, we argue that, depending on description granularity we can proceed, computing syntagmatic (succession axis of linguistic units) and paradigmatic (substitution axis) similarities between units can allow us to generically handle such discourse structural phenomena. Section 2 introduces the discourse parallelism phenomenon. Section 3 develops three methods we implemented to detect it: a similarity degree measure, a string editi</context>
<context position="28021" citStr="Summers, 1998" startWordPosition="4752" endWordPosition="4753">ic, lexical, etc.), the comprehension and the production of a parallel utterance is made faster (Dubey et al., 2005). So far, most of the works were led in order to acquire resources and to build systems to retrieve specific parallelism phenomena. In the field of information structure theories, (Kruijff-Korbayov´a and Kruijff, 1996) implemented an ad-hoc system 287 to identify thematic continuity (lexical relation between the subject parts of consecutive sentences). (Luc et al., 1999) described and classified markers (lexical clues, layout and typography) occurring in enumeration structures. (Summers, 1998) also described the markers required for retrieving heading structures. (Charolles, 1997) was involved in the description of frame introducers. Integration of specialized resources dedicated to parallelism detection could be an improvement to our approach. Let us not forget that our final aim remains the detection of discourse structures. Parallelism should be considered as an additional feature which among other discourse features (e.g. connectors). Regarding the use of parallelism, (Hernandez and Grau, 2005) proposed an algorithm to parse the discourse structure and to select pairs of senten</context>
</contexts>
<marker>Summers, 1998</marker>
<rawString>K. M. Summers. 1998. Automatic Discovery of Logical Document Structure. Ph.D. thesis, U. of Cornell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Wagner</author>
<author>M J Fischer</author>
</authors>
<title>The String-toString Correction Problem.</title>
<date>1974</date>
<journal>Journal of the ACM,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="2671" citStr="Wagner and Fischer, 1974" startWordPosition="390" endWordPosition="393">s (Luc et al., 1999), etc. These phenomena are usually treated mostly independently within individual systems with ad-hoc resource developments. In this work, we argue that, depending on description granularity we can proceed, computing syntagmatic (succession axis of linguistic units) and paradigmatic (substitution axis) similarities between units can allow us to generically handle such discourse structural phenomena. Section 2 introduces the discourse parallelism phenomenon. Section 3 develops three methods we implemented to detect it: a similarity degree measure, a string editing distance (Wagner and Fischer, 1974) and a tree editing distance1 (Zhang and Shasha, 1989). Section 4 discusses and evaluates these methods and their relevance. The final section reviews related work. 2 Textual parallelism Our notion of parallelism is based on similarities between syntagmatic and paradigmatic representations of (constituents of) textual units. These similarities concern various dimensions from shallow to deeper description: layout, typography, morphology, lexicon, syntax, and semantics. This account is not limited to the semantic dimension as defined by (Hobbs and Kehler, 1997) who consider text fragments as par</context>
<context position="7261" citStr="Wagner and Fischer, 1974" startWordPosition="1122" endWordPosition="1126"> WordNet relations, then words are replaced by their most representative synonym: Horizontal links are specific specializations. horizontal connection be specific specialization. 2. Syntactic analysis by (Charniak, 1997)’s parser: ( S1 ( S ( NP ( JJ Horizontal ) (NNS links) ( VP ( AUX are) ( NP ( ADJP ( JJ specific) ( NNS specializations ) ( SENT.))))))) 3. Syntactic structure flattening: S1 S NP JJ Horizontal NNS links VP AUX are NP ADJP JJ specific NNS specializations SENT. 3.2 Wagner &amp; Fischer’s string edit distance This method is based on Wagner &amp; Fischer’s string edit distance algorithm (Wagner and Fischer, 1974), applied to sentences viewed as strings of words. It computes a sentence edit distance, using edit operations on these elementary entities. The idea is to use edit operations to transform sentence S1 into S2. Similarly to (Wagner and Fischer, 1974), we considered three edit operations: 1. replacing word x E S1 by y E S2: (x —* y) 2. deleting word x E S1: (x —* A) 3. inserting word y E S2 into S1: (A —* y) By definition, the cost of a sequence of edit operations is the sum of the costs2 of the elementary 2We used unitary costs in this study 282 operations, and the distance between S1 and S2 is</context>
</contexts>
<marker>Wagner, Fischer, 1974</marker>
<rawString>R.A. Wagner and M.J. Fischer. 1974. The String-toString Correction Problem. Journal of the ACM, 21(1):168–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
<author>D Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAM Journal on Computing,</journal>
<volume>18</volume>
<issue>6</issue>
<contexts>
<context position="2725" citStr="Zhang and Shasha, 1989" startWordPosition="399" endWordPosition="402">reated mostly independently within individual systems with ad-hoc resource developments. In this work, we argue that, depending on description granularity we can proceed, computing syntagmatic (succession axis of linguistic units) and paradigmatic (substitution axis) similarities between units can allow us to generically handle such discourse structural phenomena. Section 2 introduces the discourse parallelism phenomenon. Section 3 develops three methods we implemented to detect it: a similarity degree measure, a string editing distance (Wagner and Fischer, 1974) and a tree editing distance1 (Zhang and Shasha, 1989). Section 4 discusses and evaluates these methods and their relevance. The final section reviews related work. 2 Textual parallelism Our notion of parallelism is based on similarities between syntagmatic and paradigmatic representations of (constituents of) textual units. These similarities concern various dimensions from shallow to deeper description: layout, typography, morphology, lexicon, syntax, and semantics. This account is not limited to the semantic dimension as defined by (Hobbs and Kehler, 1997) who consider text fragments as parallel if the same predicate can be inferred from them </context>
<context position="8142" citStr="Zhang and Shasha, 1989" startWordPosition="1291" endWordPosition="1294">dered three edit operations: 1. replacing word x E S1 by y E S2: (x —* y) 2. deleting word x E S1: (x —* A) 3. inserting word y E S2 into S1: (A —* y) By definition, the cost of a sequence of edit operations is the sum of the costs2 of the elementary 2We used unitary costs in this study 282 operations, and the distance between S1 and S2 is the cost of the least cost transformation of S1 into S2. Wagner &amp; Fischer’s method provides a simple and effective way (O( S1 S2 )) to compute it. To reduce size effects, we normalized by |S1|+|S2| 2 . 3.3 Zhang &amp; Shasha’s algorithm Zhang &amp; Shasha’s method (Zhang and Shasha, 1989; Dulucq and Tichit, 2003) generalizes Wagner &amp; Fischer’s edit distance to trees: given two trees T1 and T2, it computes the least-cost sequence of edit operations that transforms T1 into T2. Elementary operations have unitary costs and apply to nodes (labels and words in the syntactic trees). These operations are depicted below: substitution of node c by node g (top figure), insertion of node d (middle fig.), and deletion of node d (bottom fig.), each read from left to right. Tree edit distance d(T1, T2) is determined after a series of intermediate calculations involving special subtrees of T</context>
<context position="28839" citStr="Zhang and Shasha, 1989" startWordPosition="4877" endWordPosition="4880">arallelism detection could be an improvement to our approach. Let us not forget that our final aim remains the detection of discourse structures. Parallelism should be considered as an additional feature which among other discourse features (e.g. connectors). Regarding the use of parallelism, (Hernandez and Grau, 2005) proposed an algorithm to parse the discourse structure and to select pairs of sentences to compare. Confronted to the problem of determining textual entailment4 (the fact that the meaning of one expression can be inferred from another) (Kouylekov and Magnini, 2005) applied the (Zhang and Shasha, 1989)’s algorithm on the dependency trees of pairs of sentences (they did not consider syntactic tags as nodes but only words). They encountered problems similar to ours due to pre-treatment limits. Indeed, the syntactic parser sometimes represents in a different way occurrences of similar expressions, making it harder to apply edit transformations. A drawback concerning the tree-edit distance approach is that it is not able to observe the whole tree, but only the subtree of the processed node. 6 Conclusion Textual parallelism plays an important role among discourse features when detecting discours</context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>K. Zhang and D. Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM Journal on Computing, 18(6):1245–1262.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>