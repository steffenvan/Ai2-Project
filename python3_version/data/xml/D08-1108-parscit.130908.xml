<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.842409">
Mining and Modeling Relations between
Formal and Informal Chinese Phrases from Web Corpora
</title>
<author confidence="0.992644">
Zhifei Li and David Yarowsky
</author>
<affiliation confidence="0.9124855">
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
</affiliation>
<email confidence="0.991128">
zhifei.work@gmail.com and yarowsky@cs.jhu.edu
</email>
<sectionHeader confidence="0.996367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968111111111">
We present a novel method for discovering
and modeling the relationship between in-
formal Chinese expressions (including collo-
quialisms and instant-messaging slang) and
their formal equivalents. Specifically, we pro-
posed a bootstrapping procedure to identify
a list of candidate informal phrases in web
corpora. Given an informal phrase, we re-
trieve contextual instances from the web us-
ing a search engine, generate hypotheses of
formal equivalents via this data, and rank
the hypotheses using a conditional log-linear
model. In the log-linear model, we incorpo-
rate as feature functions both rule-based intu-
itions and data co-occurrence phenomena (ei-
ther as an explicit or indirect definition, or
through formal/informal usages occurring in
free variation in a discourse). We test our
system on manually collected test examples,
and find that the (formal-informal) relation-
ship discovery and extraction process using
our method achieves an average 1-best preci-
sion of 62%. Given the ubiquity of informal
conversational style on the internet, this work
has clear applications for text normalization
in text-processing systems including machine
translation aspiring to broad coverage.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999708666666667">
Informal text (e.g., newsgroups, online chat, blogs,
etc.) is the majority of all text appearing on the Inter-
net. Informal text tends to have very different style
from formal text (e.g., newswire, magazine, etc.).
In particular, they are different in vocabulary, syn-
tactic structure, semantic interpretation, discourse
</bodyText>
<sectionHeader confidence="0.706653" genericHeader="introduction">
Formal Informal
</sectionHeader>
<equation confidence="0.5167475">
99 (BaiBai)[bye-bye] 88 (BaBa)
4PA (XiHuan)[like] 4J (XiFan)[gruel]
�� (GeGe)[elder brother] GG
TWA (GeMi)[fans] tft (FenSi)[a food]
</equation>
<tableCaption confidence="0.742740333333333">
Table 1: Example Chinese Formal-informal Relations.
The PinYin pronunciation is in parentheses and an op-
tional literal gloss is in brackets.
</tableCaption>
<bodyText confidence="0.999062631578947">
structure, and so on. On the other hand, certain re-
lations exist between the informal and formal text,
and informal text often has a viable formal equiva-
lent. Table 1 shows several naturally occurring ex-
amples of informal expressions in Chinese, and Ta-
ble 2 provides a more detailed inventory and charac-
terization of this phenomena1. The first example of
informal phrase “88” is used very often in Chinese
on-line chat when a person wants to say “bye-bye”
to the other person. This can be explained as fol-
lows. In Chinese, the standard equivalent to “bye-
bye” is “99” whose PinYin is “BaiBai”. Coin-
cidentally, the PinYin of “88” is “BaBa”. Because
“BaBa” and “BaiBai” are near homophones, people
often use “88” to represent “99”, either for input
convenience or just for fun. The other relations in
Table 1 are formed due to similar processes as will
be described later.
Due to the often substantial divergence between
</bodyText>
<footnote confidence="0.997706666666667">
1For clarity, we represent Chinese words in the format: Chi-
nese characters (optional PinYin equivalent in parentheses and
optional English gloss in brackets).
</footnote>
<page confidence="0.89922">
1031
</page>
<note confidence="0.8841635">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1031–1040,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999961644444444">
informal and formal text, a text-processing system
trained on formal text does not typically work well
on informal genres. For example, in a machine
translation system (Koehn et al., 2007), if the bilin-
gual training data does not contain the word “�
1” (the second example in Table 1), it leaves the
word untranslated. On the other hand, if the word
“M1” does appear in the training data but it has
only a translation “gruel” as that is the meaning in
the formal text, the translation system may wrongly
translate “�1” into “gruel” for the informal text
where the word “M 1” is more likely to mean
“like”. Therefore, as a text-normalization step, it
is desirable to transform the informal text into its
standard formal equivalent before feeding it into a
general-purpose text-processing system. Unfortu-
nately, there are many processes for generating in-
formal expressions in common use today. Such
transformations are highly flexible/diverse, and new
phrases are invented on the Internet every day due to
major news events, popular movies, TV shows, ra-
dio talks, political activities, and so on. Therefore,
it is of great interest to have a data-driven method
that can automatically find the relations between in-
formal and formal expressions.
In this paper, we present a novel method for dis-
covering and modeling the relationship between in-
formal Chinese expressions found in web corpora
and their formal equivalents. Specifically, we im-
plement a bootstrapping procedure to identify a
list of candidate informal phrases. Given an indi-
vidual informal phrase, we retrieve contextual in-
stances from the web using a search engine (in this
case, www.baidu.com), generate hypotheses of for-
mal equivalents via this data, and rank the hypothe-
ses using a conditional log-linear model. In the log-
linear model, we incorporate as feature functions
both rule-based intuitions and data co-occurrence
phenomena (either as an explicit or indirect defini-
tion, or through formal/informal usages occurring in
free variation in a discourse). We test our system on
manually collected test examples2, and find that the
(formal-informal) relationship discovery and extrac-
tion process using our method achieves an average
precision of more than 60%. This work has applica-
</bodyText>
<footnote confidence="0.9787095">
2The training and test examples are freely available at
http://www.cs.jhu.edu/∼zfli.
</footnote>
<bodyText confidence="0.999190142857143">
tions for text normalization in many general-purpose
text-processing tasks, e.g., machine translation.
To the best of our knowledge, our work is the
first published machine-learning approach to pro-
ductively model the broad types of relationships be-
tween informal and formal expressions in Chinese
using web corpora.
</bodyText>
<sectionHeader confidence="0.8266145" genericHeader="method">
2 Formal to Informal: Phenomena and
Examples
</sectionHeader>
<bodyText confidence="0.999750217391304">
In this section, we describe the phenomena and pro-
vide examples of the relations between formal and
informal expressions in Chinese (we refer to the
relation as formal-informal phrases hereafter, even
in the case of single-word expressions). We man-
ually collected 908 formal-informal relations, and
classified these relations into four categories. We
collected these pairs by investigating multiple web-
pages where the formal-informal relations are man-
ually compiled, and then merged these seed relations
and removed duplicates. In this way, the 908 exam-
ples should give good coverage on the typical cat-
egories in the formal-informal relations. Also, the
distribution of the categories found in the 908 exam-
ples should be representative of the actual distribu-
tion of the formal-informal relations occurring in the
real text. Table 2 presents these categories and ex-
amples in each category. In the last column, the table
also shows the relative frequency of each category,
computed based on the 908 examples. Recall that
we represent Chinese words in the format: Chinese
characters (optional PinYin equivalent in parenthe-
ses and optional English gloss in brackets).
</bodyText>
<subsectionHeader confidence="0.980244">
2.1 Homophone
</subsectionHeader>
<bodyText confidence="0.998782166666667">
In general, a homophone is a word that is pro-
nounced the same as another word but differs in
meaning and/or written-form. Here, we use the word
“homophone” in a loose way. In particular, we re-
fer an informal phrase as a homophone of a formal
phrase if its pronunciation is the same or similar to
the formal phrase. In the three examples belonging
to the homophone category in Table 2, the first ex-
ample is a true homophone, while the other two are
loose homophones. The third example represents a
major sub-class where the informal phrase is a num-
ber (e.g., 88).
</bodyText>
<page confidence="0.985542">
1032
</page>
<table confidence="0.99974325">
Category Formal Informal %
Homophone {fit (BanZhu) [system administrator] f&apos;`.T (BanZhu) [bamboo] 4.2
SPA (XiHuan)[like] 4, (XiFan)[gruel] 4.4
99 (BaiBai)[bye-bye] 88 (BaBa) 21
Abbreviation XQ29K (MeiGuoJunDui)[american army] X29 (MeiJun)[american army] 3.8
Acronym ¸¸ (GeGe)[elder brother] GG 12.3
EŠŒ (NuPengYou)[girl friend] GF 7.2
Transliteration 1A (GeMi)[fans] Nft (FenSi)[a Chinese food] 2.3
igfigf (XieXie)[thank you] 3Q (SanQiu)
Others *i�fflNft (XiLaLiFenSi)[fans of Hilary] 4, (XiFan)[gruel]
AL, (AoBaMaFenSi)[fans of Obama] UN (OuFen)[a food] 44.8
0-39 (ChaoQiang)[super strong] JP_-TjÜM (ZouZhaoGongXu)
</table>
<tableCaption confidence="0.99944">
Table 2: Chinese Formal-informal Relations: Categories and Examples. Literal glosses in brackets.
</tableCaption>
<bodyText confidence="0.999810166666667">
For illustrative purposes, we can present the
transformation path showing how the informal
phrase is obtained from the formal phrase. In par-
ticular, the transformation path for this category is
“Formal —* PinYin —* Informal (similar or same
PinYin as the formal phrase)”.
</bodyText>
<subsectionHeader confidence="0.999605">
2.2 Abbreviation and Acronym
</subsectionHeader>
<bodyText confidence="0.9999870625">
A Chinese abbreviation of a formal phrase is ob-
tained by selecting one or more characters from this
formal phrase, and the selected characters can be at
any position in the formal phrase (Li and Yarowsky,
2008; Lee, 2005; Yin, 1999). In comparison, an
acronym is a special form of abbreviation, where
only the first character of each word in the formal
phrase is selected to form the informal phrase. Table
2 presents three examples belonging to this category.
While the first example is an abbreviation, and the
other two examples are acronyms.
The transformation path for the second exam-
ple is “Formal —* PinYin —* Acronym”, and the
transformation path for the third example is “For-
mal —* English —* Acronym”. Clearly, they differ in
whether PinYin or English is used as a bridge.
</bodyText>
<subsectionHeader confidence="0.993439">
2.3 Transliteration
</subsectionHeader>
<bodyText confidence="0.9993578">
A transliteration is transcribing a word or text writ-
ten in one writing system into another writing sys-
tem. Table 2 presents examples belonging to this
category. In the first example, the Chinese infor-
mal phrase “Nft (FenSi)[a Chinese food]” can be
thought as a transliteration of the English phase
“fans” as the pronunciation of “fans” is quite sim-
ilar to the PinYin “FenSi”.
The transformation path for this category is “For-
mal —* English —* Chinese Transliteration”.
</bodyText>
<subsectionHeader confidence="0.98463">
2.4 Others
</subsectionHeader>
<bodyText confidence="0.999993">
Due to the inherently informal and flexible nature of
expressions in informal genre, the formation of an
informal phrase can be very complex or ad-hoc. For
example, an informal phrase can be generated by ap-
plying the above transformation rules jointly. More
importantly, many relations cannot be described us-
ing a simple set of rules. Table 2 presents three such
examples, where the first two examples are gener-
ated by applying rules jointly and the third example
is created by decomposing the Chinese characters in
the formal form. The statistics collected from the
904 examples tells us that about 45% of the relations
belonging to this category. This motivates us to use
a data-driven method to automatically discover the
relations between informal and formal phrases.
</bodyText>
<sectionHeader confidence="0.996005" genericHeader="method">
3 Data Co-occurrence
</sectionHeader>
<bodyText confidence="0.99738">
In natural language, related words tend to appear to-
gether (i.e., co-occurrence). For example, Bill Gates
</bodyText>
<page confidence="0.949386">
1033
</page>
<bodyText confidence="0.999926">
tends to appear together with Microsoft more of-
ten than expected by chance. Such co-occurrence
may imply the existence of a relationship, and is ex-
ploited in formal-informal relation discovery under
different conditions.
</bodyText>
<subsectionHeader confidence="0.999648">
3.1 Data Co-occurrence in Definitions
</subsectionHeader>
<bodyText confidence="0.999995578947368">
In general, for many informal phrases in popular use,
there is likely to be an explicit definition somewhere
that provides or paraphrases its meaning for an unfa-
miliar audience. People have created dedicated def-
inition web-pages to explain the relations between
formal and informal phrases. For example, the first
example in Table 3 is commonly explained in many
dedicated definition web-pages on the Internet. On
the other hand, in some formal text (e.g., research
papers), people tend to define the informal phrase
before it is used frequently in the later part of the
text. The second example of Table 3 illustrates this
phenomena. Clearly, the definition text normally
contains salient patterns. For example, the first ex-
ample follows the “informal4formaln, ‡” defi-
nition pattern, while the second example follows the
pattern “formal (informal)”. This gives us a reliable
way to seed and bootstrap a list of informal phrases
as will be discussed in Section 4.1.
</bodyText>
<sectionHeader confidence="0.591238" genericHeader="method">
Relation Definition Text
</sectionHeader>
<equation confidence="0.64408275">
(EŠA, GF) GF4EŠAn, ‡o
(-Ž¥���, *An, yAffl-Ž¥
-¥) ��� (-¥)�fflyy
�����
</equation>
<tableCaption confidence="0.989043">
Table 3: Data Co-occurrence in Definitions
</tableCaption>
<subsectionHeader confidence="0.999692">
3.2 Data Co-occurrence in Online Chat
</subsectionHeader>
<bodyText confidence="0.9739845">
Informal phrases appear in online chat very often for
input convenience or just for fun. Since different
people may have different ways or traditions to ex-
press semantically-equivalent phrases, one may find
many nearby data co-occurrence examples in chat
text. For example, in Table 4, after a series of mes-
sage exchanges, person A wants to end the conver-
sation and types “3/43/4” (meaning “bye-bye”), per-
son B later includes the same semantic content, but
in a different (more or less formal) expression (e.g.
“88”).
...
</bodyText>
<table confidence="0.581246333333333">
Person A: S å, ·�S TfJ T
Person A: 3/43/4
Person B: 88
</table>
<tableCaption confidence="0.983971">
Table 4: Data Co-occurrence in Online Chat for Relation
(3/43/4, 88) meaning “bye-bye”
</tableCaption>
<subsectionHeader confidence="0.998656">
3.3 Data Co-occurrence in News Articles
</subsectionHeader>
<bodyText confidence="0.999907928571429">
For some formal-informal relations, since both of
the informal and formal phrases have been used in
public very often and people are normally aware
of these relations, an author may use the informal
and formal phrases interchangeably without bother-
ing to explain the relations. This is particularly true
in news articles for some well-known relations. Ta-
ble 5 shows an example, where the abbreviation “lC
£Ì” (meaning “winter olympics”) appears in the
title and its full-form “lC�£jLÌ” appears in the
text of the same document. In general, the relative
distance between an informal phrase and its formal
phrase varies. For example, they may appear in the
same sentence, or in neighboring sentences.
</bodyText>
<figure confidence="0.3322636">
Title ô*Rí1r&amp;quot;±�
Text q(iE,-9TVfA]
÷)%20“lC*£jLÌn,)Fô*R
-TAA~-10F1t8,,&apos;-7(E��£f*
š�����*y)F�3n,Á�o
</figure>
<tableCaption confidence="0.9523725">
Table 5: Data Co-occurrence in News Article for Relation
(lC�£jLÌ,lC£Ì) meaning “winter olympics”
</tableCaption>
<sectionHeader confidence="0.8244805" genericHeader="method">
4 Mining Relations between Informal and
Formal Phrases from Web
</sectionHeader>
<bodyText confidence="0.999424727272727">
In this section, we describe an approach that auto-
matically discovers the relation between a formal
phrase and an informal phrase from web corpora.
Specifically, we propose a bootstrapping procedure
to identify a list of candidate informal phrases.
Given a target informal phrase, we retrieve a large
set of of instances in context from the Web, generate
candidate hypotheses (i.e, candidate formal phrases)
from the data, and rank the hypotheses by using a
conditional log-linear model. The log-linear model
is very flexible to incorporate both the rule- and data-
</bodyText>
<page confidence="0.984494">
1034
</page>
<bodyText confidence="0.994234">
driven intuitions (described in Sections 2 and 3, re-
spectively) into the model as feature functions.
</bodyText>
<subsectionHeader confidence="0.997071">
4.1 Identifying Informal Phrases
</subsectionHeader>
<bodyText confidence="0.999988233333333">
Before finding the formal phrase corresponding to
an informal phrase, we first need to identify infor-
mal phrases of interest. For example, one can collect
informal phrases manually. However, this is too ex-
pensive as new relations between informal and for-
mal phrases emerge every day on the Internet. Alter-
natively, one can employ a large amount of formal
text (e.g., newswire) and informal text (e.g., Inter-
net blogs) to derive such a list as follows. Specifi-
cally, from the informal corpus we can extract those
phrases whose frequency in the informal corpus is
significantly different from that in the formal cor-
pus. However, such a list may be quite noisy, i.e.,
many of them are not informal phrases at all.
An alternative approach to extracting the infor-
mal phrases is to use a bootstrapping algorithm (e.g.,
Yarowsky (1995)). Specifically, we first manually
collect a small set of example relations. Then, using
these relations as a seed set, we extract the text pat-
terns (e.g., the definition pattern showing how the
informal and formal phrases co-occur in the data as
discussed in Section 3.1). With these patterns, we
identify many more new relations from the data and
augment them into the seed set. The procedure it-
erates. Using such an approach, we should be able
to extract a large list of formal-informal relations.
Clearly, the list extracted in this way may be quite
noisy, and thus it is important to exploit both the
data- and rule-driven intuitions to rank these rela-
tions properly.
</bodyText>
<subsectionHeader confidence="0.998316">
4.2 Retrieving Data from Web
</subsectionHeader>
<bodyText confidence="0.999964333333333">
Given an informal phrase, we retrieve training data
from the web on the fly. Specifically, we first use
a search engine to identify a set of hyper-links that
point to web pages containing contexts relevant to
the informal phrase, and then follow the hyper-links
to download the web pages. The input to the search
engine is a text query. One can simply use the infor-
mal phrase as a query. However, this may lead to a
set of pages that have nothing to do with the infor-
mal phrase. For example, if we search the informal
phrase “88” (the third example in Table 2) using the
well-known Chinese search engine www.baidu.com,
none of the top-10 pages are related to the infor-
mal phrase “88”. To avoid this situation, one can
use a search engine that is dedicated to informal text
search (e.g., blogsearch.baidu.com). Alternatively,
one can use the general-purpose search engine but
expanding the query with domain information. For
example, for the informal phrase “88”, we can use
a query “88 MW fE”, where “ is�” means
internet language.
</bodyText>
<subsectionHeader confidence="0.999211">
4.3 Generating Candidate Hypotheses
</subsectionHeader>
<bodyText confidence="0.999803272727273">
Given an informal phrase, we generate a set of hy-
potheses which are candidate formal phrases corre-
sponding to the informal phrase. We considered two
general approaches to the generation of hypotheses.
Rule-driven Hypothesis Generation: One can
use the rules described in Section 2 to generate a
set of hypotheses. However, with this approach, one
may generate an exponential number of hypotheses.
For example, assuming the number of English words
starting with a given letter is O(|V ), we can generate
O(IVIn) hypotheses given an acronym containing n
letters. Another problem with this approach is that
a relation between an informal phrase and a formal
phrase may not be explained by a specific rule. In
fact, as shown in the last row of Table 2, such rela-
tions consist of 44.8% of all corpus instances.
Data-driven Hypothesis Generation: With data
retrieved from the Web, we can generate hypotheses
by enumerating the frequent n-grams co-occurring
with the informal phrase within certain distance.
This exploits the data co-occurrence phenomena de-
scribed in Section 3, that is, the formal phrase tends
to co-occur with the informal phrase nearby in the
data, for the multiple reasons described above. This
can deal with the cases where the relation between
an informal phrase and a formal phrase cannot be
explained by a rule. However, it also suffers from
the over-generation problem as in the rule-driven ap-
proach.
In this paper, we use the data-driven method to
generate hypotheses, and rank the hypotheses using
a conditional log-linear model that incorporates both
the rule and data intuitions as feature functions.
</bodyText>
<page confidence="0.985792">
1035
</page>
<subsectionHeader confidence="0.9961475">
4.4 Ranking Hypotheses: Conditional
Log-linear Model
</subsectionHeader>
<bodyText confidence="0.999972333333333">
Log-linear models are known for flexible incorpora-
tion of features into the model. Each feature func-
tion reflects a hint/intuition that can be used to rank
the hypotheses. In this subsection, we develop a
conditional log-linear model that incorporates both
the rule and data intuitions as feature functions.
</bodyText>
<subsectionHeader confidence="0.935728">
4.4.1 Conditional Log-linear Model
</subsectionHeader>
<bodyText confidence="0.9999905">
Given an informal phrase (say x) and a candidate
formal phrase (say y), the model assigns the pair a
score (say s(x, y)), which will be used to rank the
hypothesis y. The score s(x, y) is a linear combina-
tion of the feature scores (say -bz(x, y)) over a set of
feature functions indexed by i. Formally,
</bodyText>
<equation confidence="0.998855">
K
s(x, y) = 4&apos;z(x, y) x az (1)
z=1
</equation>
<bodyText confidence="0.9997235">
where K is the number of feature functions defined
and az is the weight assigned to the i-th feature func-
tion (i.e., 4bz). To learn the weight vector a, we first
define a probability measure,
</bodyText>
<equation confidence="0.9971565">
Pa(y|x) = 1 es(x,y) (2)
Z(x, a)
</equation>
<bodyText confidence="0.999776666666667">
where Z(x, a) is a normalization constant. Now, we
define the regularized log-likelihood (LLR) of the
training data (i.e, a set of pairs of (x, y)), as follows,
</bodyText>
<equation confidence="0.997782">
log Pa(yj|xj) − ||��||2
2σ2 (3)
</equation>
<bodyText confidence="0.553439">
where N is the number of training examples, and the
</bodyText>
<equation confidence="0.867219428571429">
regularization term ||��||2
2σ2 is a Gaussian prior with a
variance Q2 (Roark et al., 2007). The optimal weight
vector a∗ is obtained by maximizing the regularized
log-likelihood (LLR), that is,
a∗ = arg max LLR(a) (4)
a
</equation>
<bodyText confidence="0.999608222222222">
To maximize the above function, we use a limited-
memory variable method (Benson and More, 2002)
that is implemented in the TAO package (Benson et
al., 2002) and has been shown to be very effective in
various natural language processing tasks (Malouf,
2002).
During test time, the following decision rule is
normally used to predict the optimal formal phrase
y∗ for a given informal phrase x,
</bodyText>
<equation confidence="0.966687">
y∗ = arg max s(x, y). (5)
y
</equation>
<subsectionHeader confidence="0.786614">
4.4.2 Feature Functions
</subsectionHeader>
<bodyText confidence="0.999645125">
As mentioned before, we incorporate both the
rule- and data-driven intuitions as feature functions
in the log-linear model.
Rule-driven feature functions: Clearly, if a pair
(x, y) matches the rule patterns described in Table 2,
the pair has a high possibility to be a true formal-
informal relation. To reflect this intuition, we de-
velop several feature functions as follows.
</bodyText>
<listItem confidence="0.976578923076923">
• LD-PinYin(x, y): the Levenshtein distance on
PinYin of x and y. The distance between
two PinYin characters is weighted based on
the similarity of pronunciation, for example,
the weight w(l, n) is smaller than the weight
w(a, z).
• LEN-PinYin(x, y): the difference in the num-
ber of PinYin characters between x and y.
• Is-PinYin-Acronym(x, y): is x a PinYin
acronym of y? For example,
Is-PinYin-Acronym(GG, p p)=1,
Is-PinYin-Acronym(GG, RA4)=0.
• Is-CN-Abbreviation(x, y): is x a Chinese ab-
</listItem>
<bodyText confidence="0.898595428571429">
breviation of y? For example,
Is-CN-Abbreviation()Z29,XQ29F,k)=1,
Is-CN-Abbreviation()V29,rPQ29K)=0.
Data-driven feature functions: As described in
Section 3, the informal and formal phrases tends to
co-occur in the data. Here, we develop several fea-
ture functions to reflect this intuition.
</bodyText>
<listItem confidence="0.992274818181818">
• n-gram co-occurrence relative frequency: we
collect the n-grams that occur in the data within
a window of the occurrence of the informal
phrase, and compute their relative frequency
as feature values. Since different orders of
grams will have quite different statistics, we
define 7 features in this category: 1-gram, 2-
gram, 3-gram, 4-gram, 5-gram, 6to10-gram,
and 11to15-gram. Note that the order n of a
n-gram is in terms of number of Chinese char-
acters instead of words.
</listItem>
<equation confidence="0.983332666666667">
N
LLR(a) =
j=1
</equation>
<page confidence="0.947391">
1036
</page>
<listItem confidence="0.973589714285714">
• Features on a definition pattern: we have dis-
cussed definition patterns in Section 3.1. For
each definition pattern, we can define a feature
function saying that if the co-occurrence of x
and y satisfies the definition pattern, the feature
value is one, otherwise is zero.
• Features on the number of relevant web-pages:
another interesting feature function can be de-
fined as follows. For each candidate relation
(x, y), we use the pair as a query to search the
web, and treat the number of pages returned by
the search engine as a feature value.3 However,
these features are quite expensive as millions of
queries may need to be served.
</listItem>
<sectionHeader confidence="0.987618" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.99987952">
Recall that in Section 2 we categorize the formal-
informal relations based on the manually collected
relations. In this section, we use a subset of them for
training and testing. In particular, we use 252 exam-
ples to train the log-linear model that is described
in Section 4, and use 249 examples as test data to
compute the precision.4
Table 6 shows the weights5 learned for the var-
ious feature functions described in Section 4.4.
Clearly, different feature functions get quite differ-
ent weights. This is intuitive as the feature functions
may differ in the scale of the feature values or in
their importance in ranking the hypotheses. In fact,
this shows the importance of using the log-linear
model to learn the optimal weights in a principled
and automatic manner, instead of manually tuning
the weights in an ad-hoc way.
Tables 7-9 show the precision results for different
categories as described in Section 2, using the rule-
driven, data-driven, or both rule and data-driven fea-
tures, respectively. In the tables, the precision corre-
sponding to the “top-N” is computed in the follow-
ing way: if the true hypothesis is among the top-N
hypotheses ranked by the model, we tag the classi-
fication as correct, otherwise as wrong. Clearly, the
</bodyText>
<footnote confidence="0.998308333333333">
3Note that the number of pages relevant to a query can be
easily obtained as most search engines return this number.
4Again, the training and test examples are freely available at
http://www.cs.jhu.edu/∼zfli.
5Note that we do not use the features on definition patterns
and on the number of relevant web pages, for efficiency.
</footnote>
<figure confidence="0.915631333333333">
Category Feature Weight
LD-PinYin 0.800
Len-PinYin 0.781
Is-PinYin-Acronym 7.594
Is-CN-Abbreviation 7.464
1-gram 14.506
2-gram 108.193
3-gram 82.975
Data-driven 4-gram 66.872
5-gram 42.258
6to10-gram 21.229
11to15-gram 0.985
</figure>
<tableCaption confidence="0.961955">
Table 6: Optimal Weights in the Log-linear Model
</tableCaption>
<bodyText confidence="0.999782866666667">
larger the N is, the higher the precision is. Comput-
ing the top-N precision (instead of just computing
the usual top-1 precision) is meaningful especially
when we consider our relation extractor as an inter-
mediate step in an end-to-end text-processing sys-
tem (e.g., machine translation) since the final deci-
sion can be delayed to later stage based on more ev-
idence. In general, our model gets quite respectably
high precision for such a task (e.g., more than 60%
for top-1 and more than 85% for top-100) when us-
ing both data and rule-driven features, as shown in
Table 9. Moreover, the data-driven features are more
helpful than the rule-driven features (e.g, 25.3% ab-
solute improvement in 1-best precision), while the
combination of these features does boost the perfor-
mance of any individual feature set (e.g., 10.4% ab-
solute improvement in 1-best precision over the case
using data-driven features only).
We also carried out experiments (see Table 10)
in the bootstrapping procedure described in Section
4.1. In particular, we start from a seed set having
130 relations. We identify the frequent patterns from
the data retrieved from the web for these seed exam-
ples. Then, we use these patterns to identify many
more new possible formal-informal relations. After
the first iteration, we select the top 3000 pairs of re-
lations matched by the patterns. The recall of a man-
ually collected test set (having 750 pairs) on these
3000 pairs is around 30%, which is quite promising
given the highly noisy data.
</bodyText>
<table confidence="0.906739">
Rule-driven
1037
Category Top-1 Precision (%) Top-100
Top-10 Top-50
Homophone Same PinYin 31.6 47.4 68.4 73.7
Similar PinYin 15.0 35.0 45.0 50.0
Number 31.6 64.2 84.2 90.5
Abbreviation Chinese abbreviation 11.8 35.3 41.2 41.2
Acronym PinYin Acronym 39.3 82.1 91.1 92.9
English Acronym 3.1 6.3 9.4 28.1
Transliteration 10.0 20.0 20.0 20.0
Average 26.1 53.4 66.3 72.3
</table>
<tableCaption confidence="0.97597">
Table 7: Rule-driven Features only: Precision on Chinese Formal-informal Relation Extraction
</tableCaption>
<table confidence="0.9999559">
Category Top-1 Precision (%) Top-100
Top-10 Top-50
Homophone Same PinYin 52.6 73.7 73.7 78.9
Similar PinYin 45.0 65.0 75.0 75.0
Number 66.3 86.3 94.7 96.8
Abbreviation Chinese abbreviation 0.0 23.5 47.1 47.1
Acronym PinYin Acronym 58.9 78.6 85.7 87.5
English Acronym 25.0 46.9 68.6 68.8
Transliteration 50.0 50.0 50.0 50.0
Average 51.4 71.1 81.1 82.7
</table>
<tableCaption confidence="0.957384">
Table 8: Data-driven Features only: Precision on Chinese Formal-informal Relation Extraction
</tableCaption>
<table confidence="0.999964">
Category Top-1 Precision (%) Top-100
Top-10 Top-50
Homophone Same PinYin 63.2 73.7 84.2 84.2
Similar PinYin 40.0 60.0 70.0 80.0
Number 81.1 91.6 95.8 96.8
Abbreviation Chinese abbreviation 11.8 41.2 52.9 52.9
Acronym PinYin Acronym 82.1 94.6 96.4 96.4
English Acronym 21.9 46.9 56.3 59.4
Transliteration 20.0 40.0 50.0 50.0
Average 61.8 77.1 83.1 84.7
</table>
<tableCaption confidence="0.993233">
Table 9: Both Data and Rule-drive Features: Precision on Chinese Formal-informal Relation Extraction
</tableCaption>
<page confidence="0.623117">
1038
</page>
<table confidence="0.99933475">
Size of seed set 130
Size of candidate set 3000
Size of test set 750
Recall 30%
</table>
<tableCaption confidence="0.960433">
Table 10: Recall of Test Set on a Candidate Set Extracted
by a Bootstrapping Procedure
</tableCaption>
<sectionHeader confidence="0.999858" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999611">
Automatically extracting the relations between full-
form Chinese phrases and their abbreviations is an
interesting and important task for many NLP appli-
cations (e.g., machine translation, information re-
trieval, etc.). Recently, Chang and Lai (2004), Lee
(2005), Chang and Teng (2006), Li and Yarowsky
(2008) have investigated this task. Specifically,
Chang and Lai (2004) describes a hidden markov
model (HMM) to model the relationship between
a full-form phrase and its abbreviation, by treat-
ing the abbreviation as the observation and the full-
form words as states in the model. Using a set
of manually-created full-abbreviation relations as
training data, they report experimental results on
a recognition task (i.e., given an abbreviation, the
task is to obtain its full-form, or the vice versa).
Chang and Teng (2006) extends the work in Chang
and Lai (2004) to automatically extract the relations
between full-form phrases and their abbreviations,
where both the full-form phrase and its abbrevia-
tion are not given. Clearly, the method in (Chang
and Lai, 2004; Chang and Teng, 2006) is super-
vised because it requires the full-abbreviation rela-
tions as training data. Li and Yarowsky (2008) pro-
pose an unsupervised method to extract the relations
between full-form phrases and their abbreviations.
They exploit the data co-occurrence phenomena in
the newswire text, as we have done in this paper.
Moreover, they augment and improve a statistical
machine translation by incorporating the extracted
relations into the baseline translation system.
Other interesting work that addresses a similar
task as ours includes the work on homophones (e.g.,
Lee and Chen (1997)), abbreviations with their defi-
nitions (e.g., Park and Byrd (2001)), abbreviations
and acronyms in the medical domain (Pakhomov,
2002), and transliteration (e.g., (Knight and Graehl,
1998; Virga and Khudanpur, 2003; Li et al., 2004;
Wu and Chang, 2007)).
While all the above work deals with the rela-
tions occurring within the formal text, we consider
the formal-informal relations that occur across both
formal and informal text, and we extract the rela-
tions from the web corpora, instead from just formal
text. Moreover, our method is semi-supervised in
the sense that the weights of the feature functions
are tuned in a supervised log-linear model using a
small number of seed relations while the generation
and ranking of the hypotheses are unsupervised by
exploiting the data co-occurrence phenomena.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999990153846154">
In this paper, we have first presented a taxonomy of
the formal-informal relations occurring in Chinese
text. We have then proposed a novel method for
discovering and modeling the relationship between
informal Chinese expressions (including colloqui-
alisms and instant-messaging slang) and their formal
equivalents. Specifically, we have proposed a boot-
strapping procedure to identify a list of candidate
informal phrases in web corpora. Given an infor-
mal phrase, we retrieved contextual instances from
the web using a search engine, generated hypothe-
ses of formal equivalents via this data, and ranked
the hypotheses using a conditional log-linear model.
In the log-linear model, we incorporated as feature
functions both rule-based intuitions and data co-
occurrence phenomena (either as an explicit or in-
direct definition, or through formal/informal usages
occurring in free variation in a discourse). We tested
our system on manually collected test examples,
and found that the (formal-informal) relationship
discovery and extraction process using our method
achieves an average 1-best precision of 62%. Given
the ubiquity of informal conversational style on the
internet, this work has clear applications for text nor-
malization in text-processing systems including ma-
chine translation aspiring to broad coverage.
</bodyText>
<sectionHeader confidence="0.99882" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9416976">
We would like to thank Yi Su, Sanjeev Khudanpur,
and the anonymous reviewers for their helpful com-
ments. This work was partially supported by the De-
fense Advanced Research Projects Agency’s GALE
program via Contract No¯ HR0011-06-2-0001.
</bodyText>
<page confidence="0.996072">
1039
</page>
<sectionHeader confidence="0.993848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883414285714">
S. J. Benson, L. C. McInnes, J. J. More, and J. Sarich.
2002. Tao users manual, Technical Report ANL/MCS-
TM-242-Revision 1.4, Argonne National Laboratory.
S. J. Benson and J. J. More. 2002. A limited memory vari-
able metric method for bound constrained minimiza-
tion. preprint ANL/ACSP909-0901, Argonne National
Laboratory.
Jing-Shin Chang and Yu-Tso Lai. 2004. A preliminary
study on probabilistic models for Chinese abbrevia-
tions. In Proceedings of the 3rd SIGHAN Workshop
on Chinese Language Processing, Barcelona, Spain
(2004),pages 9-16.
Jing-Shin Chang and Wei-Lun Teng. 2006. Mining
Atomic Chinese Abbreviation Pairs: A Probabilistic
Model for Single Character Word Recovery. In Pro-
ceedings of the 5rd SIGHAN Workshop on Chinese
Language Processing, Sydney, Australia (2006), pages
17-24.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration. Computational Linguistics, 24(4):599-
612.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan,Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
strantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings ofACL, Demonstration Session, pages 177-180.
H.W.D Lee. 2005. A study of automatic expansion of
Chinese abbreviations. MA Thesis, The University of
Hong Kong.
Yue-Shi Lee and Hsin-Hsi Chen. 1997. Applying Repair
Processing in Chinese Homophone Disambiguation.
In Proceedings of the Fifth Conference on Applied Nat-
ural Language Processing, pages 57-63.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source
channel model for machine transliteration. In Proceed-
ings ofACL 2004, pages 159-166.
Zhifei Li and David Yarowsky. 2008. Unsupervised
Translation Induction for Chinese Abbreviations using
Monolingual Corpora. In Proceedings of ACL 2008,
pages 425-433.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
CoNLL 2002, pages 49-55.
Serguei Pakhomov. 2002. Semi-Supervised Maximum
Entropy Based Approach to Acronym and Abbrevia-
tion Normalization in Medical Texts. In Proceedings
ofACL 2002, pages 160-167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of EMNLP 2001, pages 126-133.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373-392.
Paola Virga and Sanjeev Khudanpur. 2003. Transliter-
ation of Proper Names in Cross lingual Information
Retrieval. In Proceedings of the ACL 2003 Workshop
on Multilingual and Mixed-language Named Entity
Recognition.
Jian-Cheng Wu and Jason S. Chang. 2007. Learning to
Find English to Chinese Transliterations on the Web.
In Proceedings of EMNLP-CoNLL 2007, pages 996-
1004.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
ofACL 1995, pages 189-196.
Z.P. Yin. 1999. Methodologies and principles of Chi-
nese abbreviation formation. In Language Teaching
and Study, No.2 (1999) 73-82.
</reference>
<page confidence="0.986913">
1040
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882697">
<title confidence="0.981641">Mining and Modeling Relations between Formal and Informal Chinese Phrases from Web Corpora</title>
<author confidence="0.992656">Li</author>
<affiliation confidence="0.9988">Department of Computer Science and Center for Language and Speech</affiliation>
<address confidence="0.945389">Johns Hopkins University, Baltimore, MD 21218,</address>
<abstract confidence="0.999085357142857">We present a novel method for discovering and modeling the relationship between informal Chinese expressions (including colloquialisms and instant-messaging slang) and their formal equivalents. Specifically, we proposed a bootstrapping procedure to identify a list of candidate informal phrases in web corpora. Given an informal phrase, we retrieve contextual instances from the web using a search engine, generate hypotheses of formal equivalents via this data, and rank the hypotheses using a conditional log-linear model. In the log-linear model, we incorporate as feature functions both rule-based intuand co-occurrence (either as an explicit or indirect definition, or through formal/informal usages occurring in free variation in a discourse). We test our system on manually collected test examples, find that the relationship discovery and extraction process using our method achieves an average 1-best precision of 62%. Given the ubiquity of informal conversational style on the internet, this work has clear applications for text normalization in text-processing systems including machine translation aspiring to broad coverage.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S J Benson</author>
<author>L C McInnes</author>
<author>J J More</author>
<author>J Sarich</author>
</authors>
<title>Tao users manual,</title>
<date>2002</date>
<tech>Technical Report ANL/MCSTM-242-Revision 1.4,</tech>
<institution>Argonne National Laboratory.</institution>
<contexts>
<context position="20583" citStr="Benson et al., 2002" startWordPosition="3276" endWordPosition="3279">here Z(x, a) is a normalization constant. Now, we define the regularized log-likelihood (LLR) of the training data (i.e, a set of pairs of (x, y)), as follows, log Pa(yj|xj) − ||��||2 2σ2 (3) where N is the number of training examples, and the regularization term ||��||2 2σ2 is a Gaussian prior with a variance Q2 (Roark et al., 2007). The optimal weight vector a∗ is obtained by maximizing the regularized log-likelihood (LLR), that is, a∗ = arg max LLR(a) (4) a To maximize the above function, we use a limitedmemory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). During test time, the following decision rule is normally used to predict the optimal formal phrase y∗ for a given informal phrase x, y∗ = arg max s(x, y). (5) y 4.4.2 Feature Functions As mentioned before, we incorporate both the rule- and data-driven intuitions as feature functions in the log-linear model. Rule-driven feature functions: Clearly, if a pair (x, y) matches the rule patterns described in Table 2, the pair has a high possibility to be a true formalinformal relation. To reflect th</context>
</contexts>
<marker>Benson, McInnes, More, Sarich, 2002</marker>
<rawString>S. J. Benson, L. C. McInnes, J. J. More, and J. Sarich. 2002. Tao users manual, Technical Report ANL/MCSTM-242-Revision 1.4, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Benson</author>
<author>J J More</author>
</authors>
<title>A limited memory variable metric method for bound constrained minimization.</title>
<date>2002</date>
<tech>preprint ANL/ACSP909-0901,</tech>
<institution>Argonne National Laboratory.</institution>
<contexts>
<context position="20522" citStr="Benson and More, 2002" startWordPosition="3265" endWordPosition="3268">define a probability measure, Pa(y|x) = 1 es(x,y) (2) Z(x, a) where Z(x, a) is a normalization constant. Now, we define the regularized log-likelihood (LLR) of the training data (i.e, a set of pairs of (x, y)), as follows, log Pa(yj|xj) − ||��||2 2σ2 (3) where N is the number of training examples, and the regularization term ||��||2 2σ2 is a Gaussian prior with a variance Q2 (Roark et al., 2007). The optimal weight vector a∗ is obtained by maximizing the regularized log-likelihood (LLR), that is, a∗ = arg max LLR(a) (4) a To maximize the above function, we use a limitedmemory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). During test time, the following decision rule is normally used to predict the optimal formal phrase y∗ for a given informal phrase x, y∗ = arg max s(x, y). (5) y 4.4.2 Feature Functions As mentioned before, we incorporate both the rule- and data-driven intuitions as feature functions in the log-linear model. Rule-driven feature functions: Clearly, if a pair (x, y) matches the rule patterns described in Table 2, the pair has a high po</context>
</contexts>
<marker>Benson, More, 2002</marker>
<rawString>S. J. Benson and J. J. More. 2002. A limited memory variable metric method for bound constrained minimization. preprint ANL/ACSP909-0901, Argonne National Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing-Shin Chang</author>
<author>Yu-Tso Lai</author>
</authors>
<title>A preliminary study on probabilistic models for Chinese abbreviations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>9--16</pages>
<location>Barcelona,</location>
<contexts>
<context position="28327" citStr="Chang and Lai (2004)" startWordPosition="4520" endWordPosition="4523"> 21.9 46.9 56.3 59.4 Transliteration 20.0 40.0 50.0 50.0 Average 61.8 77.1 83.1 84.7 Table 9: Both Data and Rule-drive Features: Precision on Chinese Formal-informal Relation Extraction 1038 Size of seed set 130 Size of candidate set 3000 Size of test set 750 Recall 30% Table 10: Recall of Test Set on a Candidate Set Extracted by a Bootstrapping Procedure 6 Related Work Automatically extracting the relations between fullform Chinese phrases and their abbreviations is an interesting and important task for many NLP applications (e.g., machine translation, information retrieval, etc.). Recently, Chang and Lai (2004), Lee (2005), Chang and Teng (2006), Li and Yarowsky (2008) have investigated this task. Specifically, Chang and Lai (2004) describes a hidden markov model (HMM) to model the relationship between a full-form phrase and its abbreviation, by treating the abbreviation as the observation and the fullform words as states in the model. Using a set of manually-created full-abbreviation relations as training data, they report experimental results on a recognition task (i.e., given an abbreviation, the task is to obtain its full-form, or the vice versa). Chang and Teng (2006) extends the work in Chang </context>
</contexts>
<marker>Chang, Lai, 2004</marker>
<rawString>Jing-Shin Chang and Yu-Tso Lai. 2004. A preliminary study on probabilistic models for Chinese abbreviations. In Proceedings of the 3rd SIGHAN Workshop on Chinese Language Processing, Barcelona, Spain (2004),pages 9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing-Shin Chang</author>
<author>Wei-Lun Teng</author>
</authors>
<title>Mining Atomic Chinese Abbreviation Pairs: A Probabilistic Model for Single Character Word Recovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5rd SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>17--24</pages>
<location>Sydney, Australia</location>
<contexts>
<context position="28362" citStr="Chang and Teng (2006)" startWordPosition="4526" endWordPosition="4529">on 20.0 40.0 50.0 50.0 Average 61.8 77.1 83.1 84.7 Table 9: Both Data and Rule-drive Features: Precision on Chinese Formal-informal Relation Extraction 1038 Size of seed set 130 Size of candidate set 3000 Size of test set 750 Recall 30% Table 10: Recall of Test Set on a Candidate Set Extracted by a Bootstrapping Procedure 6 Related Work Automatically extracting the relations between fullform Chinese phrases and their abbreviations is an interesting and important task for many NLP applications (e.g., machine translation, information retrieval, etc.). Recently, Chang and Lai (2004), Lee (2005), Chang and Teng (2006), Li and Yarowsky (2008) have investigated this task. Specifically, Chang and Lai (2004) describes a hidden markov model (HMM) to model the relationship between a full-form phrase and its abbreviation, by treating the abbreviation as the observation and the fullform words as states in the model. Using a set of manually-created full-abbreviation relations as training data, they report experimental results on a recognition task (i.e., given an abbreviation, the task is to obtain its full-form, or the vice versa). Chang and Teng (2006) extends the work in Chang and Lai (2004) to automatically ext</context>
</contexts>
<marker>Chang, Teng, 2006</marker>
<rawString>Jing-Shin Chang and Wei-Lun Teng. 2006. Mining Atomic Chinese Abbreviation Pairs: A Probabilistic Model for Single Character Word Recovery. In Proceedings of the 5rd SIGHAN Workshop on Chinese Language Processing, Sydney, Australia (2006), pages 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine Transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="29935" citStr="Knight and Graehl, 1998" startWordPosition="4768" endWordPosition="4771">act the relations between full-form phrases and their abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small number of seed relations while the generation and ranking of the hypotheses are unsupervised by exploiting the data co-occu</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration. Computational Linguistics, 24(4):599-612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL, Demonstration Session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and</location>
<contexts>
<context position="3557" citStr="Koehn et al., 2007" startWordPosition="532" endWordPosition="535">ar processes as will be described later. Due to the often substantial divergence between 1For clarity, we represent Chinese words in the format: Chinese characters (optional PinYin equivalent in parentheses and optional English gloss in brackets). 1031 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1031–1040, Honolulu, October 2008. c�2008 Association for Computational Linguistics informal and formal text, a text-processing system trained on formal text does not typically work well on informal genres. For example, in a machine translation system (Koehn et al., 2007), if the bilingual training data does not contain the word “� 1” (the second example in Table 1), it leaves the word untranslated. On the other hand, if the word “M1” does appear in the training data but it has only a translation “gruel” as that is the meaning in the formal text, the translation system may wrongly translate “�1” into “gruel” for the informal text where the word “M 1” is more likely to mean “like”. Therefore, as a text-normalization step, it is desirable to transform the informal text into its standard formal equivalent before feeding it into a general-purpose text-processing s</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofACL, Demonstration Session, pages 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W D Lee</author>
</authors>
<title>A study of automatic expansion of Chinese abbreviations.</title>
<date>2005</date>
<tech>MA Thesis,</tech>
<institution>The University of Hong Kong.</institution>
<contexts>
<context position="9042" citStr="Lee, 2005" startWordPosition="1385" endWordPosition="1386">hinese Formal-informal Relations: Categories and Examples. Literal glosses in brackets. For illustrative purposes, we can present the transformation path showing how the informal phrase is obtained from the formal phrase. In particular, the transformation path for this category is “Formal —* PinYin —* Informal (similar or same PinYin as the formal phrase)”. 2.2 Abbreviation and Acronym A Chinese abbreviation of a formal phrase is obtained by selecting one or more characters from this formal phrase, and the selected characters can be at any position in the formal phrase (Li and Yarowsky, 2008; Lee, 2005; Yin, 1999). In comparison, an acronym is a special form of abbreviation, where only the first character of each word in the formal phrase is selected to form the informal phrase. Table 2 presents three examples belonging to this category. While the first example is an abbreviation, and the other two examples are acronyms. The transformation path for the second example is “Formal —* PinYin —* Acronym”, and the transformation path for the third example is “Formal —* English —* Acronym”. Clearly, they differ in whether PinYin or English is used as a bridge. 2.3 Transliteration A transliteration</context>
<context position="28339" citStr="Lee (2005)" startWordPosition="4524" endWordPosition="4525">ransliteration 20.0 40.0 50.0 50.0 Average 61.8 77.1 83.1 84.7 Table 9: Both Data and Rule-drive Features: Precision on Chinese Formal-informal Relation Extraction 1038 Size of seed set 130 Size of candidate set 3000 Size of test set 750 Recall 30% Table 10: Recall of Test Set on a Candidate Set Extracted by a Bootstrapping Procedure 6 Related Work Automatically extracting the relations between fullform Chinese phrases and their abbreviations is an interesting and important task for many NLP applications (e.g., machine translation, information retrieval, etc.). Recently, Chang and Lai (2004), Lee (2005), Chang and Teng (2006), Li and Yarowsky (2008) have investigated this task. Specifically, Chang and Lai (2004) describes a hidden markov model (HMM) to model the relationship between a full-form phrase and its abbreviation, by treating the abbreviation as the observation and the fullform words as states in the model. Using a set of manually-created full-abbreviation relations as training data, they report experimental results on a recognition task (i.e., given an abbreviation, the task is to obtain its full-form, or the vice versa). Chang and Teng (2006) extends the work in Chang and Lai (200</context>
</contexts>
<marker>Lee, 2005</marker>
<rawString>H.W.D Lee. 2005. A study of automatic expansion of Chinese abbreviations. MA Thesis, The University of Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue-Shi Lee</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Applying Repair Processing in Chinese Homophone Disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>57--63</pages>
<contexts>
<context position="29747" citStr="Lee and Chen (1997)" startWordPosition="4742" endWordPosition="4745">g and Lai, 2004; Chang and Teng, 2006) is supervised because it requires the full-abbreviation relations as training data. Li and Yarowsky (2008) propose an unsupervised method to extract the relations between full-form phrases and their abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature fu</context>
</contexts>
<marker>Lee, Chen, 1997</marker>
<rawString>Yue-Shi Lee and Hsin-Hsi Chen. 1997. Applying Repair Processing in Chinese Homophone Disambiguation. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 57-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL 2004,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="29979" citStr="Li et al., 2004" startWordPosition="4776" endWordPosition="4779">r abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small number of seed relations while the generation and ranking of the hypotheses are unsupervised by exploiting the data co-occurrence phenomena. 7 Conclusions In this pape</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source channel model for machine transliteration. In Proceedings ofACL 2004, pages 159-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Translation Induction for Chinese Abbreviations using Monolingual Corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>425--433</pages>
<contexts>
<context position="9031" citStr="Li and Yarowsky, 2008" startWordPosition="1381" endWordPosition="1384">uZhaoGongXu) Table 2: Chinese Formal-informal Relations: Categories and Examples. Literal glosses in brackets. For illustrative purposes, we can present the transformation path showing how the informal phrase is obtained from the formal phrase. In particular, the transformation path for this category is “Formal —* PinYin —* Informal (similar or same PinYin as the formal phrase)”. 2.2 Abbreviation and Acronym A Chinese abbreviation of a formal phrase is obtained by selecting one or more characters from this formal phrase, and the selected characters can be at any position in the formal phrase (Li and Yarowsky, 2008; Lee, 2005; Yin, 1999). In comparison, an acronym is a special form of abbreviation, where only the first character of each word in the formal phrase is selected to form the informal phrase. Table 2 presents three examples belonging to this category. While the first example is an abbreviation, and the other two examples are acronyms. The transformation path for the second example is “Formal —* PinYin —* Acronym”, and the transformation path for the third example is “Formal —* English —* Acronym”. Clearly, they differ in whether PinYin or English is used as a bridge. 2.3 Transliteration A tran</context>
<context position="28386" citStr="Li and Yarowsky (2008)" startWordPosition="4530" endWordPosition="4533">Average 61.8 77.1 83.1 84.7 Table 9: Both Data and Rule-drive Features: Precision on Chinese Formal-informal Relation Extraction 1038 Size of seed set 130 Size of candidate set 3000 Size of test set 750 Recall 30% Table 10: Recall of Test Set on a Candidate Set Extracted by a Bootstrapping Procedure 6 Related Work Automatically extracting the relations between fullform Chinese phrases and their abbreviations is an interesting and important task for many NLP applications (e.g., machine translation, information retrieval, etc.). Recently, Chang and Lai (2004), Lee (2005), Chang and Teng (2006), Li and Yarowsky (2008) have investigated this task. Specifically, Chang and Lai (2004) describes a hidden markov model (HMM) to model the relationship between a full-form phrase and its abbreviation, by treating the abbreviation as the observation and the fullform words as states in the model. Using a set of manually-created full-abbreviation relations as training data, they report experimental results on a recognition task (i.e., given an abbreviation, the task is to obtain its full-form, or the vice versa). Chang and Teng (2006) extends the work in Chang and Lai (2004) to automatically extract the relations betwe</context>
</contexts>
<marker>Li, Yarowsky, 2008</marker>
<rawString>Zhifei Li and David Yarowsky. 2008. Unsupervised Translation Induction for Chinese Abbreviations using Monolingual Corpora. In Proceedings of ACL 2008, pages 425-433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>49--55</pages>
<contexts>
<context position="20683" citStr="Malouf, 2002" startWordPosition="3294" endWordPosition="3295">ng data (i.e, a set of pairs of (x, y)), as follows, log Pa(yj|xj) − ||��||2 2σ2 (3) where N is the number of training examples, and the regularization term ||��||2 2σ2 is a Gaussian prior with a variance Q2 (Roark et al., 2007). The optimal weight vector a∗ is obtained by maximizing the regularized log-likelihood (LLR), that is, a∗ = arg max LLR(a) (4) a To maximize the above function, we use a limitedmemory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). During test time, the following decision rule is normally used to predict the optimal formal phrase y∗ for a given informal phrase x, y∗ = arg max s(x, y). (5) y 4.4.2 Feature Functions As mentioned before, we incorporate both the rule- and data-driven intuitions as feature functions in the log-linear model. Rule-driven feature functions: Clearly, if a pair (x, y) matches the rule patterns described in Table 2, the pair has a high possibility to be a true formalinformal relation. To reflect this intuition, we develop several feature functions as follows. • LD-PinYin(x, y): the Levenshtein di</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>R. Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of CoNLL 2002, pages 49-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serguei Pakhomov</author>
</authors>
<title>Semi-Supervised Maximum Entropy Based Approach to Acronym and Abbreviation Normalization in Medical Texts.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL 2002,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="29882" citStr="Pakhomov, 2002" startWordPosition="4763" endWordPosition="4764">(2008) propose an unsupervised method to extract the relations between full-form phrases and their abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small number of seed relations while the generation and ranking of the hypot</context>
</contexts>
<marker>Pakhomov, 2002</marker>
<rawString>Serguei Pakhomov. 2002. Semi-Supervised Maximum Entropy Based Approach to Acronym and Abbreviation Normalization in Medical Texts. In Proceedings ofACL 2002, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youngja Park</author>
<author>Roy J Byrd</author>
</authors>
<title>Hybrid text mining for finding abbreviations and their definitions.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>126--133</pages>
<contexts>
<context position="29814" citStr="Park and Byrd (2001)" startWordPosition="4752" endWordPosition="4755">quires the full-abbreviation relations as training data. Li and Yarowsky (2008) propose an unsupervised method to extract the relations between full-form phrases and their abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small nu</context>
</contexts>
<marker>Park, Byrd, 2001</marker>
<rawString>Youngja Park and Roy J. Byrd. 2001. Hybrid text mining for finding abbreviations and their definitions. In Proceedings of EMNLP 2001, pages 126-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<date>2007</date>
<booktitle>Discriminative n-gram language modeling. Computer Speech and Language,</booktitle>
<pages>21--2</pages>
<contexts>
<context position="20298" citStr="Roark et al., 2007" startWordPosition="3227" endWordPosition="3230">indexed by i. Formally, K s(x, y) = 4&apos;z(x, y) x az (1) z=1 where K is the number of feature functions defined and az is the weight assigned to the i-th feature function (i.e., 4bz). To learn the weight vector a, we first define a probability measure, Pa(y|x) = 1 es(x,y) (2) Z(x, a) where Z(x, a) is a normalization constant. Now, we define the regularized log-likelihood (LLR) of the training data (i.e, a set of pairs of (x, y)), as follows, log Pa(yj|xj) − ||��||2 2σ2 (3) where N is the number of training examples, and the regularization term ||��||2 2σ2 is a Gaussian prior with a variance Q2 (Roark et al., 2007). The optimal weight vector a∗ is obtained by maximizing the regularized log-likelihood (LLR), that is, a∗ = arg max LLR(a) (4) a To maximize the above function, we use a limitedmemory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). During test time, the following decision rule is normally used to predict the optimal formal phrase y∗ for a given informal phrase x, y∗ = arg max s(x, y). (5) y 4.4.2 Feature Functions As mentioned before, we inc</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer Speech and Language, 21(2):373-392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Virga</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Transliteration of Proper Names in Cross lingual Information Retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multilingual and Mixed-language Named Entity Recognition.</booktitle>
<contexts>
<context position="29962" citStr="Virga and Khudanpur, 2003" startWordPosition="4772" endWordPosition="4775"> full-form phrases and their abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small number of seed relations while the generation and ranking of the hypotheses are unsupervised by exploiting the data co-occurrence phenomena. 7 Conclus</context>
</contexts>
<marker>Virga, Khudanpur, 2003</marker>
<rawString>Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of Proper Names in Cross lingual Information Retrieval. In Proceedings of the ACL 2003 Workshop on Multilingual and Mixed-language Named Entity Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Cheng Wu</author>
<author>Jason S Chang</author>
</authors>
<title>Learning to Find English to Chinese Transliterations on the Web. In</title>
<date>2007</date>
<booktitle>Proceedings of EMNLP-CoNLL</booktitle>
<pages>996--1004</pages>
<contexts>
<context position="30000" citStr="Wu and Chang, 2007" startWordPosition="4780" endWordPosition="4783">They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small number of seed relations while the generation and ranking of the hypotheses are unsupervised by exploiting the data co-occurrence phenomena. 7 Conclusions In this paper, we have first pres</context>
</contexts>
<marker>Wu, Chang, 2007</marker>
<rawString>Jian-Cheng Wu and Jason S. Chang. 2007. Learning to Find English to Chinese Transliterations on the Web. In Proceedings of EMNLP-CoNLL 2007, pages 996-1004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>189--196</pages>
<contexts>
<context position="15605" citStr="Yarowsky (1995)" startWordPosition="2436" endWordPosition="2437">as new relations between informal and formal phrases emerge every day on the Internet. Alternatively, one can employ a large amount of formal text (e.g., newswire) and informal text (e.g., Internet blogs) to derive such a list as follows. Specifically, from the informal corpus we can extract those phrases whose frequency in the informal corpus is significantly different from that in the formal corpus. However, such a list may be quite noisy, i.e., many of them are not informal phrases at all. An alternative approach to extracting the informal phrases is to use a bootstrapping algorithm (e.g., Yarowsky (1995)). Specifically, we first manually collect a small set of example relations. Then, using these relations as a seed set, we extract the text patterns (e.g., the definition pattern showing how the informal and formal phrases co-occur in the data as discussed in Section 3.1). With these patterns, we identify many more new relations from the data and augment them into the seed set. The procedure iterates. Using such an approach, we should be able to extract a large list of formal-informal relations. Clearly, the list extracted in this way may be quite noisy, and thus it is important to exploit bot</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings ofACL 1995, pages 189-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z P Yin</author>
</authors>
<title>Methodologies and principles of Chinese abbreviation formation.</title>
<date>1999</date>
<booktitle>In Language Teaching and Study, No.2</booktitle>
<pages>73--82</pages>
<contexts>
<context position="9054" citStr="Yin, 1999" startWordPosition="1387" endWordPosition="1388">al-informal Relations: Categories and Examples. Literal glosses in brackets. For illustrative purposes, we can present the transformation path showing how the informal phrase is obtained from the formal phrase. In particular, the transformation path for this category is “Formal —* PinYin —* Informal (similar or same PinYin as the formal phrase)”. 2.2 Abbreviation and Acronym A Chinese abbreviation of a formal phrase is obtained by selecting one or more characters from this formal phrase, and the selected characters can be at any position in the formal phrase (Li and Yarowsky, 2008; Lee, 2005; Yin, 1999). In comparison, an acronym is a special form of abbreviation, where only the first character of each word in the formal phrase is selected to form the informal phrase. Table 2 presents three examples belonging to this category. While the first example is an abbreviation, and the other two examples are acronyms. The transformation path for the second example is “Formal —* PinYin —* Acronym”, and the transformation path for the third example is “Formal —* English —* Acronym”. Clearly, they differ in whether PinYin or English is used as a bridge. 2.3 Transliteration A transliteration is transcri</context>
</contexts>
<marker>Yin, 1999</marker>
<rawString>Z.P. Yin. 1999. Methodologies and principles of Chinese abbreviation formation. In Language Teaching and Study, No.2 (1999) 73-82.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>