<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006845">
<title confidence="0.665574">
HIDDEN UNDERSTANDING MODELS
OF NATURAL LANGUAGE
</title>
<author confidence="0.97019">
Scott Miller
</author>
<affiliation confidence="0.994625">
College of Computer Science
Northeastern University
</affiliation>
<address confidence="0.88034">
Boston, MA 02115
</address>
<email confidence="0.999203">
millers@ccs.neu.edu
</email>
<author confidence="0.833889">
Robert Bobrow, Robert Ingria,
Richard Schwartz
</author>
<affiliation confidence="0.56791">
BBN Systems and Technologies
</affiliation>
<address confidence="0.952463">
70 Fawcett St.,
Cambridge, MA 02138
</address>
<email confidence="0.99287">
rusty, ingria, schwartz@BBN.COM
</email>
<sectionHeader confidence="0.993794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999705">
We describe and evaluate hidden understanding models, a
statistical learning approach to natural language
understanding. Given a string of words, hidden
understanding models determine the most likely meaning for
the string. We discuss 1) the problem of representing
meaning in this framework, 2) the structure of the statistical
model, 3) the process of training the model, and 4) the
process of understanding using the model. Finally, we give
experimental results, including results on an ARPA
evaluation.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999900304347826">
Hidden understanding models are an innovative class of
statistical mechanisms that, given a string of words,
determines the most likely meaning for the string. The
overall approach represents a substantial departure from
traditional techniques by replacing hand-crafted grammars
and rules with statistical models that are automatically
learned from examples. Hidden understanding models were
primarily motivated by techniques that have been extremely
successful in speech recognition, especially hidden Markov
models [Baum, 72]. Related techniques have previously
been applied to the problem of identifying concept
sequences within a sentence [Pieraccini et al., 91]. In
addition, the approach contains elements of other natural
language processing techniques including semantic
grammars [Waltz, 78; Hendrix, 781, augmented transition
networks (ATM) [Woods, 70], probabilistic parsing
[Fujisaki et al., 89; Chitrao and Grishman, 90; Seneff, 92],
and automatic grammar induction [Pereira and Schabes, 92].
Hidden understanding models are capable of learning a
variety of meaning representations, ranging from simple
domain-specific representations, to ones at a level of detail
and sophistication comparable to current natural language
systems. In fact, a hidden understanding model can be used
to produce a representation with essentially the same
information content as the semantic graph used by the
Delphi system [Bobrow et al., 90], a general purpose NLP
system, which utilizes a modified Definite Clause Grammar
formalism. This fact made it possible to interface a hidden
understanding system to the discourse processing and data-
base retrieval components of Delphi to produce a complete
&amp;quot;end to end&amp;quot; system. This hybrid system participated in the
1993 ATIS natural language evaluation. Although only four
months old, the scores achieved by the combined system
were quite respectable.
Because of differences between language understanding and
speech recognition, significant changes are required in the
hidden Markov model methodology. Unlike speech, where
each phoneme results in a local sequence of spectra, the
relation between the meaning of a sentence and the sequence
of words is not a simple linear sequential model. Language
is inherently nested, with subgroups of concepts within
other concepts.
A statistical system for understanding language must take
this and other differences into account in its overall design.
In principle, we have the following requirements for a
hidden understanding system:
</bodyText>
<listItem confidence="0.993868">
• A notational system for expressing meanings.
• A statistical model that is capable of representing
meanings and the association between meanings and
words.
• An automatic training program which, given pairs of
meanings and word sequences, can estimate the
parameters of a statistical model.
• An understanding program that can search the
statistical model to find the most likely meaning given
a word sequence.
</listItem>
<figure confidence="0.5484505">
statistical
model
sentences understanding Z
program nira.
</figure>
<figureCaption confidence="0.988791">
Figure 1. The Main Components of a Hidden
Understanding System.
</figureCaption>
<figure confidence="0.9427148">
sentences
training
program
meaning
expressions
</figure>
<page confidence="0.994387">
25
</page>
<bodyText confidence="0.999947">
Below, we describe solutions for each of these requirements,
and describe the relationship of these solutions to other work
in stochastic grammars and probabilistic parsing. Finally,
we will report on initial experiments with hidden
understanding models.
</bodyText>
<sectionHeader confidence="0.969817" genericHeader="introduction">
2 Expressing Meanings
</sectionHeader>
<bodyText confidence="0.99994925">
One of the key requirements for a hidden understanding
model is that the meaning representation must be both
precise and appropriate for automatic learning techniques.
Specifically, we require a meaning representation that is:
</bodyText>
<listItem confidence="0.999633545454546">
• Expressive. It must be able to express meanings over
the entire range of utterances that are likely to occur in
an application.
• Annotatable. It must be possible to produce accurate
annotations for a sufficiently large corpus with an
acceptable level of human effort.
• Trainable. It must be possible to estimate the model
parameters from a reasonable number of training
examples.
• Tractable. There must be a computationally tractable
algorithm capable of searching the meaning space.
</listItem>
<bodyText confidence="0.980395555555556">
In order to facilitate annotation of a training corpus, meaning
expressions should be as simple as possible. Frame based
representations, such as the example shown in figure 2, have
the advantage that they are relatively simple to understand.
A difficulty with this style of representation is that the
frames do not align directly to the words of the sentences. In
particular, a meaning frame contains few explicit clues as to
how the words of a sentence imply the structural
characteristics of the frame. Tree structured meaning
representations, discussed in the next section, have the
advantage that they can be fully aligned to the words of a
sentence. The cost is that these tree structured
representations are more detailed than their frame based
counterparts, thereby requiring greater annotation effort.
Fortunately, the techniques developed for tree structured
representations can be extended to simpler frame
representations as well.
SHOW:
</bodyText>
<figure confidence="0.933494454545454">
FLIGHTS:
TIME:
PART-OF-DAY: morning
ORIGIN:
CITY: Boston
DEST:
CITY: San Francisco
DATE:
DAY-OF-WEEK: Tuesday
Please show me morning flights from Boston to San
Francisco on Tuesday.
</figure>
<figureCaption confidence="0.999779">
Figure 2. A Frame Based Meaning Representation.
</figureCaption>
<subsectionHeader confidence="0.996623">
2.1 Tree Structured Meaning Representations
</subsectionHeader>
<bodyText confidence="0.99957745">
The central characteristic of a tree structured representation
is that individual concepts appear as nodes in a tree, with
component concepts appearing as nodes attached directly
below them. For example, the concept of a flight in the
ATIS domain has component concepts including airline,
flight number, origin, and destination. These could then
form part of the representation for the phrase: United flight
203 from Dallas to Atlanta. The use of a hierarchical
representation is one characteristic that distinguishes hidden
understanding models from earlier work in which meaning
is represented by a linear sequence of concepts [Pieraccini et
al., 91].
A requirement for tree structured representations is that the
order of the component concepts must match the order of the
words they correspond to. Thus, the representation of the
phrase flight 203 to Atlanta from Dallas on United includes
the same nodes as the earlier example, but in a different
order. For both examples, however, the interpretation is
identical.
At the leaves of a meaning tree are the words of the
</bodyText>
<figureCaption confidence="0.995855">
Figure 3. A Tree Structured Meaning Representation.
</figureCaption>
<page confidence="0.964555">
26
</page>
<bodyText confidence="0.9986273">
sentence. We distinguish between nodes that appear above
other nodes, and those that appear directly above the words.
These will be referred to as nonterminal nodes and terminal
nodes respectively, forming two disjoint sets. No node has
both words and other nodes appearing directly below it.
Figure 3 shows an example of a typical meaning tree. In this
example, the flight node represents the abstract concept of a
flight, which is a structured entity that may contain an
origin, a destination, and other component concepts.
Appearing directly above the word &amp;quot;flight&amp;quot; is a terminal
node, which we call a flight indicator. This name is chosen
to distinguish it from the flight node, and also because the
word flight, in some sense, indicates the presence of a flight
concept. Similarly, there are airline indicators, origin
indicators, and destination indicators.
One view of these tree structured representations is that they
are parse trees produced according to a semantic grammar.
In this view, the dominance relations of the grammar are
predetermined by the annotation schema, while the
precedence relations are learned from the training examples.
</bodyText>
<subsectionHeader confidence="0.998701">
2.2 Alternative Tree Representations
</subsectionHeader>
<bodyText confidence="0.99775025">
Tree structured meaning expressions can range in
complexity from simple special purpose sublanguage
representations to the structural equivalent of detailed
syntactic parse trees. The possibilities are limited only by
two fundamental requirements: (1) semantic concepts must
be hierarchically nested within a tree structure, and (2) the
sets of terminal and nonterminal nodes must remain
disjoint. Both of these requirements can be satisfied by
trees possessing most of the structural characteristics of
conventional syntactic parse trees. Since our objective is to
model meaning, the nodes must still be labeled to reflect
semantic categories. However, additional and augmented
labels may be introduced to reflect syntactic categories as
well.
Representations of this form contain significantly more
internal structure than specialized sublanguage models.
This can be seen in the example in figure 4. The specialized
sublanguage representation requires only seven nodes, while
a full syntactically motivated analysis requires fifteen. The
additional nodes are used to distinguish what is being shown
to whom, to reflect the fact that the stopover phrase is part
of a relative clause, and to determine the internal structure
of the relative clause.
One interesting characteristic of these more elaborate trees
is their similarity to those produced by classical,
linguistically motivated, natural language systems. Thus, a
hidden understanding model can serve to replace the part-of-
speech tagger, parser, and semantic interpreter of a classical
system. Instead of writing grammar and semantic
interpretation rules by hand, the training program
automatically constructs a statistical model from examples
of meaning trees.
Regardless of the details of the tree structure and labels, the
components comprising a hidden understanding system
remain unchanged. The only difference is in how the system
is trained.
</bodyText>
<figureCaption confidence="0.99126">
Figure 4. A Specialized Sublanguage Analysis and a Full Syntactic Analysis.
</figureCaption>
<page confidence="0.981193">
27
</page>
<subsectionHeader confidence="0.998733">
2.3 Frame Based Representations
</subsectionHeader>
<bodyText confidence="0.999938606060606">
One way to think of a frame based meaning is as a partially
specified tree in which some words are not accounted for.
Nevertheless, a frame representation is a complete meaning
representation in the sense that it fully specifies the concepts
and structure comprising the meaning. In terms of a tree
structured representation, the set of nontermisial nodes is
fully specified, while some of the terminal nodes may be
omitted.
The missing terminal nodes are said to be hidden, in the
sense that every word is required to align to some terminal
node, but the alignment is not necessarily given by the
meaning frame. These hidden nodes must later be aligned
as part of the training process. The general idea is to assign
a small number of free terminal nodes (typically one or two)
beneath every nontenninal node. These are then free to align
to any unassigned words, provided that the overall tree
structure is not violated. An EM algorithm (Estimate-
Maximize) is used to organize the unassigned terminal
nodes into classes that correspond to individual words and
phrases, and that bind to particular abstract concepts.
Figure 5 shows the complete meaning tree with hidden
nodes corresponding to the frame in figure 2.
If we consider tree structured meaning expressions as parse
trees which are generated according to some incompletely
specified grammar, then the problem of aligning the hidden
nodes can be considered as a grammar induction problem.
In this way, the problem of aligning the hidden nodes given
only a partially specified set of trees is analogous to the
problem of fully parsing a training corpus given only a
partial bracketing. The difference is that while a partial
bracketing determines constituent boundaries that cannot be
crossed, a partially specified tree determines structure that
must be preserved.
</bodyText>
<sectionHeader confidence="0.991992" genericHeader="method">
3 The Statistical Model
</sectionHeader>
<bodyText confidence="0.999493666666667">
One central characteristic of hidden understanding models is
that they are generative. From this viewpoint, language is
produced by a two component statistical process. The first
component chooses the meaning to be expressed, effectively
deciding &amp;quot;what to say&amp;quot;. The second component selects word
sequences to express that meaning, effectively deciding
&amp;quot;how to say it&amp;quot;. The first phase is referred to as the semantic
language model, and can be thought of as a stochastic
process that produces meaning expressions selected from a
universe of meanings. The second phase is referred to as the
lexical realization model, and can be thought of as a
stochastic process that generates words once a meaning is
given.
By analogy with hidden Markov models, we refer to the
combination of these two models as a hidden understanding
model. The word &amp;quot;hidden&amp;quot; refers to the fact that only words
can be observed. The internal states of each of the two
models are unseen and must be inferred from the words.
The problem of language understanding, then, is to recover
the most likely meaning structure given a sequence of
words. More formally, understanding a word sequence W is
accomplished by searching among all possible meanings for
some meaning M such that P(M I W) is maximized. By
Bayes Rule, P(M ITO can be rewritten as:
</bodyText>
<equation confidence="0.9988275">
P(M1W) = P(WIM)P(M)
P(W)
</equation>
<bodyText confidence="0.9982489">
Now, since P(W) does not depend on M, maximizing
P(M I W) is equivalent to maximizing the product P(W I M)
P(M). However, P(M I W) is simply our lexical realization
model, and P(M) is simply our semantic language model.
Thus, by searching a combination of these models it is
possible to find the maximum likelihood meaning M given
word sequence W. Considering the statistical model as a
stochastic grammar, the problem of determining Al given W
is analogous to the problem of finding the most likely
derivation for W ac,cording to that grammar.
</bodyText>
<figureCaption confidence="0.995809">
Figure 5. A Tree Structure Corresponding to a Frame Representation.
</figureCaption>
<page confidence="0.988444">
28
</page>
<subsectionHeader confidence="0.989948">
3.1 Semantic Language Model
</subsectionHeader>
<bodyText confidence="0.999988224137931">
For tree structured meaning representations, individual
nonterminal nodes determine particular abstract semantic
concepts. In the semantic language model, each abstract
concept corresponds to a probabilistic state transition
network. All such networks are then combined into a single
probabilistic recursive transition network, forming the
entire semantic language model.
The network corresponding to a particular abstract concept
consists of states for each of its component concepts,
together with two extra states that define the entry and exit
points. Every component concept is fully connected to every
other component concept, with additional paths leading from
the entry state to each component concept, and from each
component concept to the exit state. Figure 6 shows a
sample network corresponding to the flight concept. Of
course, there are many more flight component concepts in
the ATIS domain than actually appear in this example.
Associated with each arc is a probability value, in a similar
fashion to the TINA system [Seneff, 92]. These
probabilities have the form P(State„ I State„_1„ Context),
which is the probability of a taking transition from one state
to another within a particular context. Thus, the arc from
origin to dest has probability P(dest I origin, flight),
meaning the probability of entering dest from origin within
the context of the flight network. Presumably, this
probability is relatively high, since people usually mention
the destination of a flight directly after mentioning its origin.
Conversely, P(origin I dest, flight) is probably low because
people don&apos;t usually express concepts in that order. Thus,
while all paths through the state space are possible, some
have much higher probabilities than others.
Within a concept network, component concept states exist
for both nonterminal concepts, such as origin, as well as
terminal concepts, such as flight indicator. Arrows pointing
into nonterminal states indicate entries into other networks,
while arrows pointing away indicate exits out of those
networks. Terminal states correspond to networks as well,
although these are determined by the lexical realization
model and have a different internal structure. Thus, every
meaning tree directly corresponds directly to some particular
path through the state space. Figure 7 shows a meaning tree
and its corresponding path through state space.
Viewed as a grammar, the semantic language model is
expressed directly as a collection of networks rather than as
a collection of production rules. These networks represent
grammatical constraints in a somewhat different fashion
than do grammars based on production rules, In this model,
constituents may appear beneath nonterminal nodes in any
arbitrary order, while preferences for some orderings are
determined through the use of probabilities. By contrast,
most grammars limit the ordering of constituents to an
explicit set which is specified by the grammar rules. The
approach taken in the TINA system eliminates many
ordering constraints while retaining the local state transition
constraints determined by its grammar. We believe that an
unconstrained ordering of constraints increases parsing
robustness, while the preferences determined by the arc
probabilities help minimize overgeneration.
</bodyText>
<subsectionHeader confidence="0.99791">
3.2 Lexical Realization Model
</subsectionHeader>
<bodyText confidence="0.998562">
Just as nonterminal tree nodes correspond to networks in the
semantic language model, terminal nodes correspond to
networks in the lexical realization model. The difference is
that semantic language networks specify transition
</bodyText>
<figureCaption confidence="0.982648">
Figure 6. A Partial Network Corresponding to the ATIS Flight Concept.
</figureCaption>
<page confidence="0.996021">
29
</page>
<bodyText confidence="0.99993125">
probabilities between states, while lexical realization
networks specify transition probabilities between words.
Lexical realization probabilities have the form
P(word. I word„! , context), which is the probability of
taking a transition from one word to another given a
particular context. Thus, P(show I please, show-indicator) is
the probability that the word show follows the word please
within the context of a show indicator phrase. In addition,
there are two pseudo-words, *begin* and *end*, which
indicate the beginning and ending of phrases. Thus, we
have probabilities such as P(please I *begin*,
show-indicator), which is the probability that please is the
first word of a show indicator phrase, and
P(*end* I me, show-indicator) , which is the probability of
exiting a show indicator phrase given that the previous word
was me.
</bodyText>
<sectionHeader confidence="0.996574" genericHeader="method">
4 The Understanding Component
</sectionHeader>
<bodyText confidence="0.999821555555556">
As we have seen, understanding a word string W requires
fmding a meaning M such that the probability P(W f Al)
P(M) is maximized. Since, the semantic language model
and the lexical realization model are both probabilistic
networks, P(W I M) P(M) is the probability of a particular
path through the combined network. Thus, the problem of
understanding is to find the highest probability path among
all possible paths, where the probability of a path is the
product of all the transition probabilities along that path.
</bodyText>
<equation confidence="0.7387">
rP(state.istate „_,,context) if t in Semantic Language Modell
tePathLP(word,,Iword.,,contect) if t in Lexical Realization Model
</equation>
<bodyText confidence="0.99997837037037">
Thus far, we have discussed the need to search among all
meanings for one with a maximal probability. In fact, if it
were necessary to search every path through the combined
network individually, the algorithm would require
exponential time with respect to sentence length.
Fortunately, this can be drastically reduced by combining the
probability computation of common subpaths through
dynamic programming. In particular, because our meaning
representation aligns to the words, the search can be
efficiently performed using the well-known Viterbi [Viterbi,
67] algorithm.
Since our underlying model is a recursive transition
network, the states for the Viterbi search must be allocated
dynamically as the search proceeds. In addition, it is
necessary to prune very low probability paths in order to
keep the computation tractable. We have developed an
elegant algorithm that integrates state allocation, Viterbi
search, and pruning all within a single traversal of a tree-
like data structure. In this algorithm, each of the set of
currently active states is represented as a node in a tree.
New nodes are added to the tree as the computation pushes
into new subnetworks that are not currently active. Stored at
each node is the probability of the most likely path reaching
that state, together with a backpointer sufficient to recreate
the path later if needed. Whenever the probability of all
states in a subtree falls below the threshold specified by the
beam width, the entire subtree is pruned away.
</bodyText>
<sectionHeader confidence="0.984214" genericHeader="method">
5 The Training Component
</sectionHeader>
<bodyText confidence="0.999425571428572">
In order to train the statistical model, we must estimate
transition probabilities for the semantic language model and
lexical realization model. In the case of fully specified
meaning trees, each meaning tree can be straightforwardly
converted into a path through state space. Then, by counting
occurrence and transition frequencies along those paths, it is
possible to form simple estimates of the transition
probabilities. Let C(state,„ context,) denote the number of
times statem has occurred in context„ and let C(state„
state., context,) denote the number of times that this
condition has led to a transition to state state„ Similarly,
define counts C(word,„, context!) and C(word„ I word„„
context!). Then, a direct estimate of the probabilities is
given by:
</bodyText>
<figure confidence="0.8175505">
P(Path)= fi
Show flights to Atlanta
</figure>
<figureCaption confidence="0.999435">
Figure 7. A Meaning Tree and its Corresponding Path Through State Space.
</figureCaption>
<page confidence="0.9754">
30
</page>
<bodyText confidence="0.801729">
and
</bodyText>
<equation confidence="0.8136705">
C(wordniwordm,context)
C(wordm,context)
</equation>
<bodyText confidence="0.999924409090909">
In order to obtain robust estimates, these simple estimates
are smoothed with backed-off estimates [Good, 53], using
techniques similar to those used in speech recognition [Katz,
87; Placeway et aL, 93]. Thus, P(state„ I state,„, context) is
smoothed with P(staten context), and P(wordn word,,,,
context) is smoothed with P(word„ I context). Robustness is
further increased through word classes. For example,
Boston and San Francisco are both members of the class of
cities.
In the case of frame based representations, it is not always
possible to construct an exact path through the state space
corresponding to a meaning representation. Nevertheless,
since frames are treated as partially specified trees, most of
the path can be reconstructed, with some portions
undetermined. Then, the partial path can be used to
constrain a gradient descent search, called the forward-
backward algorithm [Baum, 72] for estimating the model
parameters. This algorithm is an iterative procedure for
adjusting the model parameters so as to increase the
likelihood of generating the training data, and is an instance
of the well-known class called EM (Estimate-Maximize)
algorithms.
</bodyText>
<sectionHeader confidence="0.998392" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999865090909091">
We have implemented a hidden understanding system and
performed a variety of experiments. In addition, we
participated in the 1993 ARPA ATIS NL evaluation.
One experiment involved a 1000 sentence ATIS corpus,
annotated according to a simple specialized sublanguage
model. The annotation effort was split between two
annotators, one of whom was a system developer, while the
other was not. To annotate the training data, we used a
bootstrapping process in which only the first 100 sentences
were annotated strictly by hand.
Thereafter, we worked in cycles of
</bodyText>
<listItem confidence="0.9993236">
1. Running the training program using all available
annotated data.
2. Running the understanding component to annotate new
sentences.
3. Hand correcting the new annotations.
</listItem>
<bodyText confidence="0.99985275">
Annotating in this way, we found that a single annotator
could produce 200 sentences per day. We then extracted the
first 100 sentences as a test set, and trained the system on
the remaining 900 sentences. The results were as follows:
</bodyText>
<listItem confidence="0.999123333333333">
• 61% matched exactly.
• 21% had correct meanings, but did not match exactly.
• 28% had the wrong meaning.
</listItem>
<bodyText confidence="0.999909545454545">
Another experiment involved a 6000 sentence ATIS corpus,
annotated according to a more sophisticated meaning model.
In this experiment, the Delphi system automatically
produced the annotation by printing out its own internal
representation for each sentence, converted into a more
readable form. In order to maintain high quality
annotations, we used only sentences for which Delphi
produced a complete parse, and for which it also retrieved a
correct answer from the database. We then removed 300
sentences as a test set, and trained the system on the
remaining 5700. The results were as follows:
</bodyText>
<listItem confidence="0.999262333333333">
• 85% matched exactly.
• 8% had correct meanings, but did not match exactly.
• 7% had the wrong meaning.
</listItem>
<bodyText confidence="0.9999682">
For the ARPA evaluation, we coupled our hidden
understanding system to the discourse and backend
components of the Delphi. Using the entire 6000 sentence
corpus described above as training data, the system
produced a score of 26% simple error on the ATIS NL
evaluation. By examining the errors, we have reached the
conclusion that nearly half are due to simple programming
issues, especially in the interface between Delphi and the
hidden understanding system. In fact, the interface was still
incomplete at the time of the evaluation.
We have just begun a series of experiments using frame
based annotations, and are continuing to refine our
techniques. In a preliminary test involving a small corpus of
588 ATIS sentences, the system correctly aligned the hidden
states for over 95% of the sentences in the corpus.
</bodyText>
<sectionHeader confidence="0.996836" genericHeader="method">
7 Limitations
</sectionHeader>
<bodyText confidence="0.999985333333333">
Several limitations to our current approach are worth noting.
In a small number of cases, linguistic movement phenomena
make it difficult to align the words of a sentence to any tree
structured meaning expression without introducing
crossings. In most cases, we have been able to work around
this problem by introducing minor changes in our annotation
such that the tree structure is maintained. A second
limitation, due to the local nature of the model, is an
inability to handle nonlocal phenomena such as coreference.
Finally, in some cases the meaning of a sentence depends
strongly upon the discourse state, which is beyond the scope
of the current model.
</bodyText>
<sectionHeader confidence="0.999008" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.99992675">
We have demonstrated the possibility of automatically
learning semantic representations directly from a training
corpus through the application of statistical techniques.
Empirical results, including the results of an ARPA
</bodyText>
<equation confidence="0.857315333333333">
P(statenIstatem,context)= C(statenIstatem,context)
C(statem,context)
P(word nlwordm,context)=
</equation>
<page confidence="0.998655">
31
</page>
<bodyText confidence="0.999916222222222">
evaluation, indicate that these techniques are capable of
relatively high levels of performance.
While hidden understanding models are based primarily on
the concepts of hidden Markov models, we have also shown
their relationship to other work in stochastic grammars and
probabilistic parsing.
Finally, we have noted some limitations to our current
approach. We view each of these limitations as opportunities
for further research and exploration.
</bodyText>
<sectionHeader confidence="0.99817" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999209">
The work reported here was supported in part by the
Defense Advanced Research Projects Agency under ARPA
Contract No. N00014-92-C-0035. The views and
conclusions contained in this document are those of the
authors and should not be interpreted as necessarily
representing the official policies, either expressed or
implied, of the Defense Advanced Research Projects Agency
or the United States Government.
</bodyText>
<sectionHeader confidence="0.998743" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999910423076923">
1. E. Baum, &amp;quot;An Inequality and Associated Maximization
Technique in Statistical Estimation of Probabilistic
Functions of Markov Processes,&amp;quot; Inequalities 3:1-8,
1972
2. Bobrow, R. Ingria, and D. Stallard, &amp;quot;Syntactic and
Semantic Knowledge in the DELPHI Unification
Grammar,&amp;quot; Proceedings, Speech and Natural Language
Workshop, pp. 230-236, June 1990
3. Chitrao, and R. Grishman, &amp;quot;Statistical Parsing of
Messages,&amp;quot; Proceedings, Speech and Natural
Language Workshop, pp. 263-276, Morgan Kaufmann
Publishers, June 1990
4. Fujisaki, F. Jelinek, J. Cocke, E. Black, T. Nishino, &amp;quot;A
Probabilistic Parsing Method for Sentence
Disambiguation,&amp;quot; International Parsing Workshop, pp.
85-90, 1989
5. Good, &amp;quot;The Population Frequencies of Species and the
Estimation of Population Parameters,&amp;quot; Biometrika 40,
pp.237-264, 1953
6. G.G Hendrix, &amp;quot;Semantic Aspects of Translation,&amp;quot;
Understanding Spoken Language, pp. 193-226, New
York, Elsevier, 1978
7. Katz, &amp;quot;Estimation of Probabilities from Sparse Data for
the Language Model Component of a Speech
Recognizer,&amp;quot; IEEE Transactions on Acoustics, Speech,
and Signal Processing, Vol. ASSP-35, pp. 400-401,
1987
8. Pereira and Y. Schabes, &amp;quot;Inside-Outside Reestimation
from Partially Bracketed Corpora,&amp;quot; Proceedings of the
30th Annual Meeting of the Association for
Computational Linguistics, pp.128-135, Newark,
Delaware, 1992
9. R. Pieraccini, E. Levin, and C. Lee, Stochastic
Representation of Conceptual Structure in the ATIS
Task DARPA Speech and Natural Language Workshop,
pp. 121-124, Feb. 1991.
10. Placeway, R. Schwartz, P. Fung, L. Nguyen, &amp;quot;The
Estimation of Powerful Language Models from Small
and Large Corpora,&amp;quot; IEEE ICASSP, II:33-36
11. Seneff, &amp;quot;TINA: A Natural Language System for Spoken
Language Applications,&amp;quot; Computational Linguistics
Vol. 18, Number 1, pp. 61-86, March 1992
12. J. Viterbi, &amp;quot;Error Bounds for Convolutional Codes and
an Asympotically Optimum Decoding Algorithm,&amp;quot;
IEEE Transactions on Information Theory TT-13(2):260-
269, April 1967
13. D.L Waltz, &amp;quot;An English Language Question Answering
System for a Large Relational Database,&amp;quot;
Communications of the ACM 21(7):526-39, 1978.
14. W.A Woods, &amp;quot;Transition Network Grammars for
Natural Language Analysis,&amp;quot; Communications of the
ACM 13(10):591-606, 1970
</reference>
<page confidence="0.9993">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.821561">
<title confidence="0.9796775">HIDDEN UNDERSTANDING MODELS OF NATURAL LANGUAGE</title>
<author confidence="0.998957">Scott Miller</author>
<affiliation confidence="0.9998635">College of Computer Science Northeastern University</affiliation>
<address confidence="0.999675">Boston, MA 02115</address>
<email confidence="0.999871">millers@ccs.neu.edu</email>
<author confidence="0.9996635">Robert Bobrow</author>
<author confidence="0.9996635">Robert Ingria</author>
<author confidence="0.9996635">Richard Schwartz</author>
<affiliation confidence="0.992836">BBN Systems and Technologies</affiliation>
<address confidence="0.9943415">70 Fawcett St., Cambridge, MA 02138</address>
<email confidence="0.992339">rusty,ingria,schwartz@BBN.COM</email>
<abstract confidence="0.988952818181818">We describe and evaluate hidden understanding models, a statistical learning approach to natural language understanding. Given a string of words, hidden understanding models determine the most likely meaning for the string. We discuss 1) the problem of representing meaning in this framework, 2) the structure of the statistical model, 3) the process of training the model, and 4) the process of understanding using the model. Finally, we give experimental results, including results on an ARPA evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>E Baum</author>
</authors>
<title>An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of Markov Processes,&amp;quot;</title>
<tech>Inequalities 3:1-8,</tech>
<contexts>
<context position="8709" citStr="(1)" startWordPosition="1295" endWordPosition="1295">d destination indicators. One view of these tree structured representations is that they are parse trees produced according to a semantic grammar. In this view, the dominance relations of the grammar are predetermined by the annotation schema, while the precedence relations are learned from the training examples. 2.2 Alternative Tree Representations Tree structured meaning expressions can range in complexity from simple special purpose sublanguage representations to the structural equivalent of detailed syntactic parse trees. The possibilities are limited only by two fundamental requirements: (1) semantic concepts must be hierarchically nested within a tree structure, and (2) the sets of terminal and nonterminal nodes must remain disjoint. Both of these requirements can be satisfied by trees possessing most of the structural characteristics of conventional syntactic parse trees. Since our objective is to model meaning, the nodes must still be labeled to reflect semantic categories. However, additional and augmented labels may be introduced to reflect syntactic categories as well. Representations of this form contain significantly more internal structure than specialized sublanguage mo</context>
</contexts>
<marker>1.</marker>
<rawString>E. Baum, &amp;quot;An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of Markov Processes,&amp;quot; Inequalities 3:1-8,</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ingria Bobrow</author>
<author>D Stallard</author>
</authors>
<title>Syntactic and Semantic Knowledge in the DELPHI Unification Grammar,&amp;quot;</title>
<date>1990</date>
<booktitle>Proceedings, Speech and Natural Language Workshop,</booktitle>
<pages>230--236</pages>
<contexts>
<context position="8790" citStr="(2)" startWordPosition="1307" endWordPosition="1307">at they are parse trees produced according to a semantic grammar. In this view, the dominance relations of the grammar are predetermined by the annotation schema, while the precedence relations are learned from the training examples. 2.2 Alternative Tree Representations Tree structured meaning expressions can range in complexity from simple special purpose sublanguage representations to the structural equivalent of detailed syntactic parse trees. The possibilities are limited only by two fundamental requirements: (1) semantic concepts must be hierarchically nested within a tree structure, and (2) the sets of terminal and nonterminal nodes must remain disjoint. Both of these requirements can be satisfied by trees possessing most of the structural characteristics of conventional syntactic parse trees. Since our objective is to model meaning, the nodes must still be labeled to reflect semantic categories. However, additional and augmented labels may be introduced to reflect syntactic categories as well. Representations of this form contain significantly more internal structure than specialized sublanguage models. This can be seen in the example in figure 4. The specialized sublanguage re</context>
</contexts>
<marker>2.</marker>
<rawString>Bobrow, R. Ingria, and D. Stallard, &amp;quot;Syntactic and Semantic Knowledge in the DELPHI Unification Grammar,&amp;quot; Proceedings, Speech and Natural Language Workshop, pp. 230-236, June 1990</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chitrao</author>
<author>R Grishman</author>
</authors>
<title>Statistical Parsing of Messages,&amp;quot;</title>
<date>1990</date>
<booktitle>Proceedings, Speech and Natural Language Workshop,</booktitle>
<pages>263--276</pages>
<publisher>Morgan Kaufmann Publishers,</publisher>
<marker>3.</marker>
<rawString>Chitrao, and R. Grishman, &amp;quot;Statistical Parsing of Messages,&amp;quot; Proceedings, Speech and Natural Language Workshop, pp. 263-276, Morgan Kaufmann Publishers, June 1990</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek Fujisaki</author>
<author>J Cocke</author>
<author>E Black</author>
<author>T Nishino</author>
</authors>
<title>A Probabilistic Parsing Method for Sentence Disambiguation,&amp;quot; International Parsing Workshop,</title>
<date>1989</date>
<pages>85--90</pages>
<marker>4.</marker>
<rawString>Fujisaki, F. Jelinek, J. Cocke, E. Black, T. Nishino, &amp;quot;A Probabilistic Parsing Method for Sentence Disambiguation,&amp;quot; International Parsing Workshop, pp. 85-90, 1989</rawString>
</citation>
<citation valid="true">
<authors>
<author>Good</author>
</authors>
<title>The Population Frequencies of Species and the Estimation of Population Parameters,&amp;quot;</title>
<date>1953</date>
<journal>Biometrika</journal>
<volume>40</volume>
<pages>237--264</pages>
<marker>5.</marker>
<rawString>Good, &amp;quot;The Population Frequencies of Species and the Estimation of Population Parameters,&amp;quot; Biometrika 40, pp.237-264, 1953</rawString>
</citation>
<citation valid="true">
<authors>
<author>G G Hendrix</author>
</authors>
<title>Semantic Aspects of Translation,&amp;quot; Understanding Spoken Language,</title>
<date>1978</date>
<pages>193--226</pages>
<location>New York, Elsevier,</location>
<marker>6.</marker>
<rawString>G.G Hendrix, &amp;quot;Semantic Aspects of Translation,&amp;quot; Understanding Spoken Language, pp. 193-226, New York, Elsevier, 1978</rawString>
</citation>
<citation valid="false">
<authors>
<author>Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer,&amp;quot;</title>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>35</volume>
<pages>400--401</pages>
<marker>7.</marker>
<rawString>Katz, &amp;quot;Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer,&amp;quot; IEEE Transactions on Acoustics, Speech, and Signal Processing, Vol. ASSP-35, pp. 400-401,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pereira</author>
<author>Y Schabes</author>
</authors>
<title>Inside-Outside Reestimation from Partially Bracketed Corpora,&amp;quot;</title>
<date>1992</date>
<booktitle>Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<location>Newark, Delaware,</location>
<marker>8.</marker>
<rawString>Pereira and Y. Schabes, &amp;quot;Inside-Outside Reestimation from Partially Bracketed Corpora,&amp;quot; Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pp.128-135, Newark, Delaware, 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pieraccini</author>
<author>E Levin</author>
<author>C Lee</author>
</authors>
<date>1991</date>
<booktitle>Stochastic Representation of Conceptual Structure in the ATIS Task DARPA Speech and Natural Language Workshop,</booktitle>
<pages>121--124</pages>
<marker>9.</marker>
<rawString>R. Pieraccini, E. Levin, and C. Lee, Stochastic Representation of Conceptual Structure in the ATIS Task DARPA Speech and Natural Language Workshop, pp. 121-124, Feb. 1991.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Schwartz Placeway</author>
<author>P Fung</author>
<author>L Nguyen</author>
</authors>
<title>The Estimation of Powerful Language Models from Small and Large Corpora,&amp;quot;</title>
<journal>IEEE ICASSP,</journal>
<pages>33--36</pages>
<marker>10.</marker>
<rawString>Placeway, R. Schwartz, P. Fung, L. Nguyen, &amp;quot;The Estimation of Powerful Language Models from Small and Large Corpora,&amp;quot; IEEE ICASSP, II:33-36</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seneff</author>
</authors>
<title>TINA: A Natural Language System for Spoken Language Applications,&amp;quot;</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<volume>18</volume>
<pages>61--86</pages>
<marker>11.</marker>
<rawString>Seneff, &amp;quot;TINA: A Natural Language System for Spoken Language Applications,&amp;quot; Computational Linguistics Vol. 18, Number 1, pp. 61-86, March 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Viterbi</author>
</authors>
<title>Error Bounds for Convolutional Codes and an Asympotically Optimum Decoding Algorithm,&amp;quot;</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory</journal>
<pages>13--2</pages>
<marker>12.</marker>
<rawString>J. Viterbi, &amp;quot;Error Bounds for Convolutional Codes and an Asympotically Optimum Decoding Algorithm,&amp;quot; IEEE Transactions on Information Theory TT-13(2):260-269, April 1967</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Waltz</author>
</authors>
<title>An English Language Question Answering System for a Large Relational Database,&amp;quot;</title>
<date>1978</date>
<journal>Communications of the ACM</journal>
<pages>21--7</pages>
<marker>13.</marker>
<rawString>D.L Waltz, &amp;quot;An English Language Question Answering System for a Large Relational Database,&amp;quot; Communications of the ACM 21(7):526-39, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Transition Network Grammars for Natural Language Analysis,&amp;quot;</title>
<date>1970</date>
<journal>Communications of the ACM</journal>
<pages>13--10</pages>
<marker>14.</marker>
<rawString>W.A Woods, &amp;quot;Transition Network Grammars for Natural Language Analysis,&amp;quot; Communications of the ACM 13(10):591-606, 1970</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>