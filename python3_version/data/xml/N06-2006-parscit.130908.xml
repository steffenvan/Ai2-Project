<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.079703">
<title confidence="0.991776">
Class Model Adaptation for Speech Summarisation
</title>
<author confidence="0.999131">
Pierre Chatain, Edward W.D. Whittaker, Joanna Mrozinski and Sadaoki Furui
</author>
<affiliation confidence="0.999718">
Dept. of Computer Science
Tokyo Institute of Technology
</affiliation>
<address confidence="0.961362">
2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552, Japan
</address>
<email confidence="0.975083">
{pierre, edw, mrozinsk, furui}@furui.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.995356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925214285714">
The performance of automatic speech
summarisation has been improved in pre-
vious experiments by using linguistic
model adaptation. We extend such adapta-
tion to the use of class models, whose ro-
bustness further improves summarisation
performance on a wider variety of objec-
tive evaluation metrics such as ROUGE-2
and ROUGE-SU4 used in the text sum-
marisation literature. Summaries made
from automatic speech recogniser tran-
scriptions benefit from relative improve-
ments ranging from 6.0% to 22.2% on all
investigated metrics.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998435625">
Techniques for automatically summarising written
text have been actively investigated in the field of
natural language processing, and more recently new
techniques have been developed for speech sum-
marisation (Kikuchi et al., 2003). However it is
still very hard to obtain good quality summaries.
Moreover, recognition accuracy is still around 30%
on spontaneous speech tasks, in contrast to speech
read from text such as broadcast news. Spontaneous
speech is characterised by disfluencies, repetitions,
repairs, and fillers, all of which make recognition
and consequently speech summarisation more diffi-
cult (Zechner, 2002). In a previous study (Chatain
et al., 2006), linguistic model (LiM) adaptation us-
ing different types of word models has proved use-
ful in order to improve summary quality. However
</bodyText>
<page confidence="0.985884">
21
</page>
<bodyText confidence="0.999944555555556">
sparsity of the data available for adaptation makes it
difficult to obtain reliable estimates of word n-gram
probabilities. In speech recognition, class models
are often used in such cases to improve model ro-
bustness. In this paper we extend the work previ-
ously done on adapting the linguistic model of the
speech summariser by investigating class models.
We also use a wider variety of objective evaluation
metrics to corroborate results.
</bodyText>
<sectionHeader confidence="0.948625" genericHeader="method">
2 Summarisation Method
</sectionHeader>
<bodyText confidence="0.9999385">
The summarisation system used in this paper is es-
sentially the same as the one described in (Kikuchi
et al., 2003), which involves a two step summarisa-
tion process, consisting of sentence extraction and
sentence compaction. Practically, only the sentence
extraction part was used in this paper, as prelimi-
nary experiments showed that compaction had little
impact on results for the data used in this study.
Important sentences are first extracted accord-
ing to the following score for each sentence
</bodyText>
<equation confidence="0.8108515">
W = w1, w2, ..., wn, obtained from the automatic
speech recognition output:
{αCC(wi)+αII(wi)+αLL(wi)},
(1)
</equation>
<bodyText confidence="0.980277857142857">
where N is the number of words in the sentence
W, and C(wi), I(wi) and L(wi) are the confidence
score, the significance score and the linguistic score
of word wi, respectively. αC, αI and αL are the
respective weighting factors of those scores, deter-
mined experimentally.
For each word from the automatic speech recogni-
</bodyText>
<equation confidence="0.9958964">
N
1
5(W) =
�
N i�1
</equation>
<note confidence="0.525462">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 21–24,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999857529411765">
tion transcription, a logarithmic value of its posterior
probability, the ratio of a word hypothesis probabil-
ity to that of all other hypotheses, is calculated using
a word graph obtained from the speech recogniser
and used as a confidence score.
For the significance score, the frequencies of oc-
currence of 115k words were found using the WSJ
and the Brown corpora.
In the experiments in this paper we modified the
linguistic component to use combinations of dif-
ferent linguistic models. The linguistic component
gives the linguistic likelihood of word strings in
the sentence. Starting with a baseline LiM (LiMB)
we perform LiM adaptation by linearly interpolat-
ing the baseline model with other component mod-
els trained on different data. The probability of a
given n-gram sequence then becomes:
</bodyText>
<equation confidence="0.999922">
P(wi|wi−n+1..wi−1) = A1P1(wi|wi−n+1..wi−1)
+... + AnPn(wi|wi−n+1..wi−1), (2)
</equation>
<bodyText confidence="0.999369666666667">
where Ek Ak = 1 and Ak and Pk are the weight and
the probability assigned by model k.
In the case of a two-sided class-based model,
</bodyText>
<equation confidence="0.999303">
Pk(wi|wi−n+1..wi−1) = Pk(wi|C(wi)) �
Pk(C(wi)|C(wi−n+1)..C(wi−1)), (3)
</equation>
<bodyText confidence="0.986425083333334">
where Pk(wi|C(wi)) is the probability of the
word wi belonging to a given class C, and
Pk(C(wi)|C(wi−n+1)..C(wi−1)) the probability of
a certain word class C(wi) to appear after a history
of word classes, C(wi−n+1), ..., C(wi−1).
Different types of component LiM are built, com-
ing from different sources of data, either as word
or class models. The LiMB and component LiMs
are then combined for adaptation using linear inter-
polation as in Equation (2). The linguistic score is
then computed using this modified probability as in
Equation (4):
</bodyText>
<equation confidence="0.985756">
L(wi) = log P(wi|wi−n+1..wi−1). (4)
</equation>
<sectionHeader confidence="0.998861" genericHeader="method">
3 Evaluation Criteria
</sectionHeader>
<subsectionHeader confidence="0.999968">
3.1 Summarisation Accuracy
</subsectionHeader>
<bodyText confidence="0.9998005">
To automatically evaluate the summarised speeches,
correctly transcribed talks were manually sum-
marised, and used as the correct targets for evalua-
tion. Variations of manual summarisation results are
merged into a word network, which is considered to
approximately express all possible correct summari-
sations covering subjective variations. The word ac-
curacy of automatic summarisation is calculated as
the summarisation accuracy (SumACCY) using the
word network (Hori et al., 2003):
</bodyText>
<equation confidence="0.506813">
Accuracy = (Len−Sub−Ins−Del)/Len*100[%],
</equation>
<bodyText confidence="0.987305">
(5)
where Sub is the number of substitution errors, Ins
is the number of insertion errors, Del is the number
of deletion errors, and Len is the number of words
in the most similar word string in the network.
</bodyText>
<subsectionHeader confidence="0.957545">
3.2 ROUGE
</subsectionHeader>
<bodyText confidence="0.999753333333333">
Version 1.5.5 of the ROUGE scoring algorithm
(Lin, 2004) is also used for evaluating results.
ROUGE F-measure scores are given for ROUGE-
2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4
(skip-bigram), using the model average (average
score across all references) metric.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999775833333333">
Experiments were performed on spontaneous
speech, using 9 talks taken from the Translanguage
English Database (TED) corpus (Lamel et al., 1994;
Wolfel and Burger, 2005), each transcribed and
manually summarised by nine different humans for
both 10% and 30% summarization ratios. Speech
recognition transcriptions (ASR) were obtained for
each talk, with an average word error rate of 33.3%.
A corpus consisting of around ten years of con-
ference proceedings (17.8M words) on the subject
of speech and signal processing is used to generate
the LiMB and word classes using the clustering al-
gorithm in (Ney et al., 1994).
Different types of component LiM are built and
combined for adaptation as described in Section 2.
The first type of component linguistic models are
built on the small corpus of hand-made summaries
described above, made for the same summarisation
ratio as the one we are generating. For each talk
the hand-made summaries of the other eight talks
(i.e. 72 summaries) were used as the LiM training
corpus. This type of LiM is expected to help gener-
ate automatic summaries in the same style as those
made manually.
</bodyText>
<page confidence="0.996097">
22
</page>
<table confidence="0.9998413">
Baseline Adapted
SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4
10% Random 34.4 0.104 0.055 0.142 - - - -
Word 63.1 0.186 0.130 0.227 67.8 0.193 0.140 0.228
Class 65.1 0.195 0.131 0.226 72.6 0.210 0.143 0.234
Mixed 63.6 0.186 0.128 0.218 71.8 0.211 0.139 0.231
30% Random 71.2 0.294 0.198 0.331 - - - -
Word 81.6 0.365 0.271 0.395 83.3 0.365 0.270 0.392
Class 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442
Mixed 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442
</table>
<tableCaption confidence="0.999959">
Table 1: TRS baseline and adapted results.
</tableCaption>
<bodyText confidence="0.99998954">
The second type of component linguistic models
are built from the papers in the conference proceed-
ings for the talk we want to summarise. This type
of LiM, used for topic adaptation, is investigated be-
cause key words and important sentences that appear
in the associated paper are expected to have a high
information value and should be selected during the
summarisation process.
Three sets of experiments were made: in the first
experiment (referred to as Word), LiMB and both
component models are word models, as introduced
in (Chatain et al., 2006). For the second one (Class),
both LiMB and the component models are class
models built using exactly the same data as the word
models. For the third experiment (Mixed), the LiMB
is an interpolation of class and word models, while
the component LiMs are class models.
To optimise use of the available data, a rotating
form of cross-validation (Duda and Hart, 1973) is
used: all talks but one are used for development, the
remaining talk being used for testing. Summaries
from the development talks are generated automati-
cally by the system using different sets of parameters
and the LiMB. These summaries are evaluated and
the set of parameters which maximises the develop-
ment score for the LiMB is selected for the remain-
ing talk. The purpose of the development phase is
to choose the most effective combination of weights
αC, αI and αL. The summary generated for each
talk using its set of optimised parameters is then
evaluated using the same metric, which gives us our
baseline for this talk. Using the same parameters as
those that were selected for the baseline, we gener-
ate summaries for the lectures in the development set
for different LiM interpolation weights Ak. Values
between 0 and 1 in steps of 0.1, were investigated
for the latter, and an optimal set of Ak is selected.
Using these interpolation weights, as well as the set
of parameters determined for the baseline, we gen-
erate a summary of the test talk, which is evaluated
using the same evaluation metric, giving us our fi-
nal adapted result for this talk. Averaging those re-
sults over the test set (i.e. all talks) gives us our final
adapted result.
This process is repeated for all evaluation metrics,
and all three experiments (Word, Class, and Mixed).
Lower bound results are given by random sum-
marisation (Random) i.e. randomly extracting sen-
tences and words, without use of the scores present
in Equation (1) for appropriate summarisation ratios.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999021">
5.1 TRS Results
</subsectionHeader>
<bodyText confidence="0.999929230769231">
Initial experiments were made on the human tran-
scriptions (TRS), and results are given in Table 1.
Experiments on word models (Word) show relative
improvements in terms of SumACCY of 7.5% and
2.1% for the 10% and 30% summarisation ratios, re-
spectively. ROUGE metrics, however, do not show
any significant improvement.
Using class models (Class and Mixed), for all
ROUGE metrics, relative improvements range from
3.5% to 13.4% for the 10% summarisation ratio, and
from 8.6% to 16.5% on the 30% summarisation ra-
tio. For SumACCY, relative improvements between
11.5% to 12.9% are observed.
</bodyText>
<subsectionHeader confidence="0.998228">
5.2 ASR Results
</subsectionHeader>
<bodyText confidence="0.987206">
ASR results for each experiment are given in Ta-
ble 2 for appropriate summarisation ratios. As for
</bodyText>
<page confidence="0.992089">
23
</page>
<table confidence="0.9998585">
Baseline Adapted
SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4
10% Random 33.9 0.095 0.042 0.140 - - - -
Word 48.6 0.143 0.064 0.182 49.8 0.129 0.060 0.173
Class 50.0 0.133 0.063 0.170 55.1 0.156 0.077 0.193
Mixed 48.5 0.134 0.068 0.176 56.2 0.142 0.077 0.191
30% Random 56.1 0.230 0.124 0.283 - - - -
Word 66.7 0.265 0.157 0.314 68.7 0.271 0.161 0.328
Class 66.1 0.277 0.165 0.324 71.1 0.300 0.180 0.348
Mixed 64.9 0.268 0.160 0.312 70.5 0.304 0.192 0.351
</table>
<tableCaption confidence="0.999624">
Table 2: ASR baseline and adapted results.
</tableCaption>
<bodyText confidence="0.9996165">
the TRS, LiM adaptation showed improvements in
terms of SumACCY, but ROUGE metrics do not cor-
roborate those results for the 10% summarisation ra-
tio. Using class models, for all ROUGE metrics, rel-
ative improvements range from 6.0% to 22.2% and
from 7.4% to 20.0% for the 10% and 30% summari-
sation ratios, respectively. SumACCY relative im-
provements range from 7.6% to 15.9%.
</bodyText>
<sectionHeader confidence="0.999753" genericHeader="evaluation">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999754375">
Compared to previous experiments using only word
models, improvements obtained using class models
are larger and more significant for both ROUGE and
SumACCY metrics. This can be explained by the
fact that the data we are performing adaptation on
is very sparse, and that the nine talks used in these
experiments are quite different from each other, es-
pecially since the speakers also vary in style. Class
models are more robust to this spontaneous speech
aspect than word models, since they generalise bet-
ter to unseen word sequences.
There is little difference between the Class and
Mixed results, since the development phase assigned
most weight to the class model component in the
Mixed experiment, making the results quite similar
to those of the Class experiment.
</bodyText>
<sectionHeader confidence="0.998942" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999164">
In this paper we have investigated linguistic model
adaptation using different sources of data for an au-
tomatic speech summarisation system. Class mod-
els have proved to be much more robust than word
models for this process, and relative improvements
ranging from 6.0% to 22.2% were obtained on a va-
riety of evaluation metrics on summaries generated
from automatic speech recogniser transcriptions.
Acknowledgements: The authors would like to
thank M. W¨olfel for the recogniser transcriptions
and C. Hori for her work on two stage summarisa-
tion and gathering the TED corpus data. This work
is supported by the 21st Century COE Programme.
</bodyText>
<sectionHeader confidence="0.999112" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996625">
P. Chatain, E.W.D. Whittaker, J. Mrozinski, and S. Fu-
rui. 2006. Topic and Stylistic Adaptation for Speech
Summarization. Proc. ICASSP, Toulouse, France.
R. Duda and P. Hart. 1973. Pattern Classification and
Scene Analysis. Wiley, New York.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation
Method for Automatic Speech Summarization. Proc.
Eurospeech, Geneva, Switzerland, 4:2825–2828.
T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic
Speech Summarization based on Sentence Extraction
and Compaction. Proc. ICASSP, Hong Kong, China,
1:236–239.
L. Lamel, F. Schiel, A. Fourcin, J. Mariani, and H. Till-
mann. 1994. The Translanguage English Database
(TED). Proc. ICSLP, Yokohama, Japan, 4:1795–1798.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic
Evaluation of Summaries. Proc. WAS, Barcelona,
Spain.
H. Ney, U. Essen, and R. Kneser. 1994. On Structur-
ing Probabilistic Dependences in Stochastic Language
Modelling. Computer Speech and Language, (8):1–
38.
M. Wolfel and S. Burger. 2005. The ISL Baseline Lec-
ture Transcription System for the TED Corpus. Tech-
nical report, Karlsruhe University.
K. Zechner. 2002. Summarization of Spoken Language-
Challenges, Methods, and Prospects. Speech Technol-
ogy Expert eZine, Issue.6.
</reference>
<page confidence="0.999175">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867775">
<title confidence="0.999353">Class Model Adaptation for Speech Summarisation</title>
<author confidence="0.990938">Pierre Chatain</author>
<author confidence="0.990938">Edward W D Whittaker</author>
<author confidence="0.990938">Joanna Mrozinski</author>
<author confidence="0.990938">Sadaoki</author>
<affiliation confidence="0.9975755">Dept. of Computer Tokyo Institute of</affiliation>
<address confidence="0.924346">2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552,</address>
<email confidence="0.966918">edw,mrozinsk,</email>
<abstract confidence="0.998502733333333">The performance of automatic speech summarisation has been improved in previous experiments by using linguistic model adaptation. We extend such adaptation to the use of class models, whose robustness further improves summarisation performance on a wider variety of objective evaluation metrics such as ROUGE-2 and ROUGE-SU4 used in the text summarisation literature. Summaries made from automatic speech recogniser transcriptions benefit from relative improvements ranging from 6.0% to 22.2% on all investigated metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Chatain</author>
<author>E W D Whittaker</author>
<author>J Mrozinski</author>
<author>S Furui</author>
</authors>
<title>Topic and Stylistic Adaptation for Speech Summarization.</title>
<date>2006</date>
<booktitle>Proc. ICASSP,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="1495" citStr="Chatain et al., 2006" startWordPosition="210" endWordPosition="213">n text have been actively investigated in the field of natural language processing, and more recently new techniques have been developed for speech summarisation (Kikuchi et al., 2003). However it is still very hard to obtain good quality summaries. Moreover, recognition accuracy is still around 30% on spontaneous speech tasks, in contrast to speech read from text such as broadcast news. Spontaneous speech is characterised by disfluencies, repetitions, repairs, and fillers, all of which make recognition and consequently speech summarisation more difficult (Zechner, 2002). In a previous study (Chatain et al., 2006), linguistic model (LiM) adaptation using different types of word models has proved useful in order to improve summary quality. However 21 sparsity of the data available for adaptation makes it difficult to obtain reliable estimates of word n-gram probabilities. In speech recognition, class models are often used in such cases to improve model robustness. In this paper we extend the work previously done on adapting the linguistic model of the speech summariser by investigating class models. We also use a wider variety of objective evaluation metrics to corroborate results. 2 Summarisation Metho</context>
<context position="8137" citStr="Chatain et al., 2006" startWordPosition="1279" endWordPosition="1282">407 92.9 0.415 0.325 0.442 Table 1: TRS baseline and adapted results. The second type of component linguistic models are built from the papers in the conference proceedings for the talk we want to summarise. This type of LiM, used for topic adaptation, is investigated because key words and important sentences that appear in the associated paper are expected to have a high information value and should be selected during the summarisation process. Three sets of experiments were made: in the first experiment (referred to as Word), LiMB and both component models are word models, as introduced in (Chatain et al., 2006). For the second one (Class), both LiMB and the component models are class models built using exactly the same data as the word models. For the third experiment (Mixed), the LiMB is an interpolation of class and word models, while the component LiMs are class models. To optimise use of the available data, a rotating form of cross-validation (Duda and Hart, 1973) is used: all talks but one are used for development, the remaining talk being used for testing. Summaries from the development talks are generated automatically by the system using different sets of parameters and the LiMB. These summa</context>
</contexts>
<marker>Chatain, Whittaker, Mrozinski, Furui, 2006</marker>
<rawString>P. Chatain, E.W.D. Whittaker, J. Mrozinski, and S. Furui. 2006. Topic and Stylistic Adaptation for Speech Summarization. Proc. ICASSP, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Duda</author>
<author>P Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="8501" citStr="Duda and Hart, 1973" startWordPosition="1341" endWordPosition="1344">ve a high information value and should be selected during the summarisation process. Three sets of experiments were made: in the first experiment (referred to as Word), LiMB and both component models are word models, as introduced in (Chatain et al., 2006). For the second one (Class), both LiMB and the component models are class models built using exactly the same data as the word models. For the third experiment (Mixed), the LiMB is an interpolation of class and word models, while the component LiMs are class models. To optimise use of the available data, a rotating form of cross-validation (Duda and Hart, 1973) is used: all talks but one are used for development, the remaining talk being used for testing. Summaries from the development talks are generated automatically by the system using different sets of parameters and the LiMB. These summaries are evaluated and the set of parameters which maximises the development score for the LiMB is selected for the remaining talk. The purpose of the development phase is to choose the most effective combination of weights αC, αI and αL. The summary generated for each talk using its set of optimised parameters is then evaluated using the same metric, which give</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. Duda and P. Hart. 1973. Pattern Classification and Scene Analysis. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hori</author>
<author>T Hori</author>
<author>S Furui</author>
</authors>
<title>Evaluation Method for Automatic Speech Summarization.</title>
<date>2003</date>
<booktitle>Proc. Eurospeech,</booktitle>
<pages>4--2825</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5409" citStr="Hori et al., 2003" startWordPosition="824" endWordPosition="827">sing this modified probability as in Equation (4): L(wi) = log P(wi|wi−n+1..wi−1). (4) 3 Evaluation Criteria 3.1 Summarisation Accuracy To automatically evaluate the summarised speeches, correctly transcribed talks were manually summarised, and used as the correct targets for evaluation. Variations of manual summarisation results are merged into a word network, which is considered to approximately express all possible correct summarisations covering subjective variations. The word accuracy of automatic summarisation is calculated as the summarisation accuracy (SumACCY) using the word network (Hori et al., 2003): Accuracy = (Len−Sub−Ins−Del)/Len*100[%], (5) where Sub is the number of substitution errors, Ins is the number of insertion errors, Del is the number of deletion errors, and Len is the number of words in the most similar word string in the network. 3.2 ROUGE Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. ROUGE F-measure scores are given for ROUGE2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4 (skip-bigram), using the model average (average score across all references) metric. 4 Experimental Setup Experiments were performed on spontaneous speech, us</context>
</contexts>
<marker>Hori, Hori, Furui, 2003</marker>
<rawString>C. Hori, T. Hori, and S. Furui. 2003. Evaluation Method for Automatic Speech Summarization. Proc. Eurospeech, Geneva, Switzerland, 4:2825–2828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kikuchi</author>
<author>S Furui</author>
<author>C Hori</author>
</authors>
<title>Automatic Speech Summarization based on Sentence Extraction and Compaction.</title>
<date>2003</date>
<booktitle>Proc. ICASSP,</booktitle>
<location>Hong Kong, China,</location>
<contexts>
<context position="1058" citStr="Kikuchi et al., 2003" startWordPosition="145" endWordPosition="148">aptation to the use of class models, whose robustness further improves summarisation performance on a wider variety of objective evaluation metrics such as ROUGE-2 and ROUGE-SU4 used in the text summarisation literature. Summaries made from automatic speech recogniser transcriptions benefit from relative improvements ranging from 6.0% to 22.2% on all investigated metrics. 1 Introduction Techniques for automatically summarising written text have been actively investigated in the field of natural language processing, and more recently new techniques have been developed for speech summarisation (Kikuchi et al., 2003). However it is still very hard to obtain good quality summaries. Moreover, recognition accuracy is still around 30% on spontaneous speech tasks, in contrast to speech read from text such as broadcast news. Spontaneous speech is characterised by disfluencies, repetitions, repairs, and fillers, all of which make recognition and consequently speech summarisation more difficult (Zechner, 2002). In a previous study (Chatain et al., 2006), linguistic model (LiM) adaptation using different types of word models has proved useful in order to improve summary quality. However 21 sparsity of the data ava</context>
</contexts>
<marker>Kikuchi, Furui, Hori, 2003</marker>
<rawString>T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic Speech Summarization based on Sentence Extraction and Compaction. Proc. ICASSP, Hong Kong, China, 1:236–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>F Schiel</author>
<author>A Fourcin</author>
<author>J Mariani</author>
<author>H Tillmann</author>
</authors>
<date>1994</date>
<booktitle>The Translanguage English Database (TED). Proc. ICSLP,</booktitle>
<pages>4--1795</pages>
<location>Yokohama, Japan,</location>
<contexts>
<context position="6099" citStr="Lamel et al., 1994" startWordPosition="931" endWordPosition="934">f substitution errors, Ins is the number of insertion errors, Del is the number of deletion errors, and Len is the number of words in the most similar word string in the network. 3.2 ROUGE Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. ROUGE F-measure scores are given for ROUGE2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4 (skip-bigram), using the model average (average score across all references) metric. 4 Experimental Setup Experiments were performed on spontaneous speech, using 9 talks taken from the Translanguage English Database (TED) corpus (Lamel et al., 1994; Wolfel and Burger, 2005), each transcribed and manually summarised by nine different humans for both 10% and 30% summarization ratios. Speech recognition transcriptions (ASR) were obtained for each talk, with an average word error rate of 33.3%. A corpus consisting of around ten years of conference proceedings (17.8M words) on the subject of speech and signal processing is used to generate the LiMB and word classes using the clustering algorithm in (Ney et al., 1994). Different types of component LiM are built and combined for adaptation as described in Section 2. The first type of component</context>
</contexts>
<marker>Lamel, Schiel, Fourcin, Mariani, Tillmann, 1994</marker>
<rawString>L. Lamel, F. Schiel, A. Fourcin, J. Mariani, and H. Tillmann. 1994. The Translanguage English Database (TED). Proc. ICSLP, Yokohama, Japan, 4:1795–1798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>Proc. WAS,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="5726" citStr="Lin, 2004" startWordPosition="879" endWordPosition="880">esults are merged into a word network, which is considered to approximately express all possible correct summarisations covering subjective variations. The word accuracy of automatic summarisation is calculated as the summarisation accuracy (SumACCY) using the word network (Hori et al., 2003): Accuracy = (Len−Sub−Ins−Del)/Len*100[%], (5) where Sub is the number of substitution errors, Ins is the number of insertion errors, Del is the number of deletion errors, and Len is the number of words in the most similar word string in the network. 3.2 ROUGE Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. ROUGE F-measure scores are given for ROUGE2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4 (skip-bigram), using the model average (average score across all references) metric. 4 Experimental Setup Experiments were performed on spontaneous speech, using 9 talks taken from the Translanguage English Database (TED) corpus (Lamel et al., 1994; Wolfel and Burger, 2005), each transcribed and manually summarised by nine different humans for both 10% and 30% summarization ratios. Speech recognition transcriptions (ASR) were obtained for each talk, with an average word </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: a Package for Automatic Evaluation of Summaries. Proc. WAS, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>U Essen</author>
<author>R Kneser</author>
</authors>
<title>On Structuring Probabilistic Dependences in Stochastic Language Modelling.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<volume>8</volume>
<pages>38</pages>
<contexts>
<context position="6572" citStr="Ney et al., 1994" startWordPosition="1008" endWordPosition="1011">p Experiments were performed on spontaneous speech, using 9 talks taken from the Translanguage English Database (TED) corpus (Lamel et al., 1994; Wolfel and Burger, 2005), each transcribed and manually summarised by nine different humans for both 10% and 30% summarization ratios. Speech recognition transcriptions (ASR) were obtained for each talk, with an average word error rate of 33.3%. A corpus consisting of around ten years of conference proceedings (17.8M words) on the subject of speech and signal processing is used to generate the LiMB and word classes using the clustering algorithm in (Ney et al., 1994). Different types of component LiM are built and combined for adaptation as described in Section 2. The first type of component linguistic models are built on the small corpus of hand-made summaries described above, made for the same summarisation ratio as the one we are generating. For each talk the hand-made summaries of the other eight talks (i.e. 72 summaries) were used as the LiM training corpus. This type of LiM is expected to help generate automatic summaries in the same style as those made manually. 22 Baseline Adapted SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4 10% Random 34.4 0.104 0</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>H. Ney, U. Essen, and R. Kneser. 1994. On Structuring Probabilistic Dependences in Stochastic Language Modelling. Computer Speech and Language, (8):1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wolfel</author>
<author>S Burger</author>
</authors>
<title>The ISL Baseline Lecture Transcription System for the TED Corpus.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Karlsruhe University.</institution>
<contexts>
<context position="6125" citStr="Wolfel and Burger, 2005" startWordPosition="935" endWordPosition="938">s, Ins is the number of insertion errors, Del is the number of deletion errors, and Len is the number of words in the most similar word string in the network. 3.2 ROUGE Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. ROUGE F-measure scores are given for ROUGE2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4 (skip-bigram), using the model average (average score across all references) metric. 4 Experimental Setup Experiments were performed on spontaneous speech, using 9 talks taken from the Translanguage English Database (TED) corpus (Lamel et al., 1994; Wolfel and Burger, 2005), each transcribed and manually summarised by nine different humans for both 10% and 30% summarization ratios. Speech recognition transcriptions (ASR) were obtained for each talk, with an average word error rate of 33.3%. A corpus consisting of around ten years of conference proceedings (17.8M words) on the subject of speech and signal processing is used to generate the LiMB and word classes using the clustering algorithm in (Ney et al., 1994). Different types of component LiM are built and combined for adaptation as described in Section 2. The first type of component linguistic models are bui</context>
</contexts>
<marker>Wolfel, Burger, 2005</marker>
<rawString>M. Wolfel and S. Burger. 2005. The ISL Baseline Lecture Transcription System for the TED Corpus. Technical report, Karlsruhe University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Summarization of Spoken LanguageChallenges, Methods, and Prospects. Speech Technology Expert</title>
<date>2002</date>
<location>eZine, Issue.6.</location>
<contexts>
<context position="1451" citStr="Zechner, 2002" startWordPosition="204" endWordPosition="205"> for automatically summarising written text have been actively investigated in the field of natural language processing, and more recently new techniques have been developed for speech summarisation (Kikuchi et al., 2003). However it is still very hard to obtain good quality summaries. Moreover, recognition accuracy is still around 30% on spontaneous speech tasks, in contrast to speech read from text such as broadcast news. Spontaneous speech is characterised by disfluencies, repetitions, repairs, and fillers, all of which make recognition and consequently speech summarisation more difficult (Zechner, 2002). In a previous study (Chatain et al., 2006), linguistic model (LiM) adaptation using different types of word models has proved useful in order to improve summary quality. However 21 sparsity of the data available for adaptation makes it difficult to obtain reliable estimates of word n-gram probabilities. In speech recognition, class models are often used in such cases to improve model robustness. In this paper we extend the work previously done on adapting the linguistic model of the speech summariser by investigating class models. We also use a wider variety of objective evaluation metrics t</context>
</contexts>
<marker>Zechner, 2002</marker>
<rawString>K. Zechner. 2002. Summarization of Spoken LanguageChallenges, Methods, and Prospects. Speech Technology Expert eZine, Issue.6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>