<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018446">
<title confidence="0.991521">
Unsupervised Morphology Rivals Supervised Morphology for Arabic MT
</title>
<author confidence="0.936789">
David Stallard Jacob Devlin
Michael Kayser
</author>
<affiliation confidence="0.429079">
BBN Technologies
</affiliation>
<email confidence="0.99006">
{stallard,jdevlin,rzbib}@bbn.com
</email>
<author confidence="0.87077">
Yoong Keok Lee Regina Barzilay
</author>
<affiliation confidence="0.9213865">
CSAIL
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.998794">
{yklee,regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996399">
If unsupervised morphological analyzers
could approach the effectiveness of super-
vised ones, they would be a very attractive
choice for improving MT performance on
low-resource inflected languages. In this
paper, we compare performance gains for
state-of-the-art supervised vs. unsupervised
morphological analyzers, using a state-of-the-
art Arabic-to-English MT system. We apply
maximum marginal decoding to the unsu-
pervised analyzer, and show that this yields
the best published segmentation accuracy
for Arabic, while also making segmentation
output more stable. Our approach gives
an 18% relative BLEU gain for Levantine
dialectal Arabic. Furthermore, it gives higher
gains for Modern Standard Arabic (MSA), as
measured on NIST MT-08, than does MADA
(Habash and Rambow, 2005), a leading
supervised MSA segmenter.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917244444444">
If unsupervised morphological segmenters could ap-
proach the effectiveness of supervised ones, they
would be a very attractive choice for improving ma-
chine translation (MT) performance in low-resource
inflected languages. An example of particular cur-
rent interest is Arabic, whose various colloquial di-
alects are sufficiently different from Modern Stan-
dard Arabic (MSA) in lexicon, orthography, and
morphology, as to be low-resource languages them-
selves. An additional advantage of Arabic for study
is the availability of high-quality supervised seg-
menters for MSA, such as MADA (Habash and
Rambow, 2005), for performance comparison. The
MT gain for supervised MSA segmenters on dialect
establishes a lower bound, which the unsupervised
segmenter must exceed if it is to be useful for dialect.
And comparing the gain for supervised and unsuper-
vised segmenters on MSA tells us how useful the
unsupervised segmenter is, relative to the ideal case
in which a supervised segmenter is available.
In this paper, we show that an unsupervised seg-
menter can in fact rival or surpass supervised MSA
segmenters on MSA itself, while at the same time
providing superior performance on dialect. Specifi-
cally, we compare the state-of-the-art morphological
analyzer of Lee et al. (2011) with two leading super-
vised analyzers for MSA, MADA and Sakhr1, each
serving as an alternative preprocessor for a state-of-
the-art statistical MT system (Shen et al., 2008). We
measure MSA performance on NIST MT-08 (NIST,
2010), and dialect performance on a Levantine di-
alect web corpus (Zbib et al., 2012b).
To improve performance, we apply maximum
marginal decoding (Johnson and Goldwater, 2009)
(MM) to combine multiple runs of the Lee seg-
menter, and show that this dramatically reduces the
variance and noise in the segmenter output, while
yielding an improved segmentation accuracy that
exceeds the best published scores for unsupervised
segmentation on Arabic Treebank (Naradowsky and
Toutanova, 2011). We also show that it yields MT-
08 BLEU scores that are higher than those obtained
with MADA, a leading supervised MSA segmenter.
For Levantine, the segmenter increases BLEU score
by 18% over the unsegmented baseline.
</bodyText>
<footnote confidence="0.991482">
1http://www.sakhr.com/Default.aspx
</footnote>
<page confidence="0.945504">
322
</page>
<note confidence="0.833939">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322–327,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.998889" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999832568181818">
Machine translation systems that process highly in-
flected languages often incorporate morphological
analysis. Some of these approaches rely on mor-
phological analysis for pre- and post-processing,
while others modify the core of a translation system
to incorporate morphological information (Habash,
2008; Luong et al., 2010; Nakov and Ng, 2011). For
instance, factored translation Models (Koehn and
Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis
and Koehn, 2008) parametrize translation probabili-
ties as factors encoding morphological features.
The approach we have taken in this paper is
an instance of a segmented MT model, which di-
vides the input into morphemes and uses the de-
rived morphemes as a unit of translation (Sadat and
Habash, 2006; Badr et al., 2008; Clifton and Sarkar,
2011). This is a mainstream architecture that has
been shown to be effective when translating from a
morphologically rich language.
A number of recent approaches have explored
the use of unsupervised morphological analyzers
for MT (Virpioja et al., 2007; Creutz and Lagus,
2007; Clifton and Sarkar, 2011; Mermer and Akın,
2010; Mermer and Saraclar, 2011). Virpioja et al.
(2007) apply the unsupervised morphological seg-
menter Morfessor (Creutz and Lagus, 2007), and
apply an existing MT system at the level of mor-
phemes. The system does not outperform the word
baseline partially due to the insufficient accuracy of
the automatic morphological analyzer.
The work of Mermer and Akın (2010) and Mer-
mer and Saraclar (2011) attempts to integrate mor-
phology and MT more closely than we do, by in-
corporating bilingual alignment probabilities into a
Gibbs-sampled version of Morfessor for Turkish-to-
English MT. However, the bilingual strategy shows
no gain over the monolingual version, and nei-
ther version is competitive for MT with a super-
vised Turkish morphological segmenter (Oflazer,
1993). By contrast, the unsupervised analyzer we
report on here yields MSA-to-English MT perfor-
mance that equals or exceed the performance ob-
tained with a leading supervised MSA segmenter,
MADA (Habash and Rambow, 2005).
</bodyText>
<sectionHeader confidence="0.478724" genericHeader="method">
3 Review of Lee Unsupervised Segmenter
</sectionHeader>
<bodyText confidence="0.999942857142857">
The segmenter of Lee et al. (2011) is a probabilis-
tic model operating at word-type level. It is di-
vided into four sub-model levels. Model 1 prefers
small affix lexicons, and assumes that morphemes
are drawn independently. Model 2 generates a la-
tent POS tag for each word type, conditioning the
word’s affixes on the tag, thereby encouraging com-
patible affixes to be generated together. Model 3
incorporates token-level contextual information, by
generating word tokens with a type-level Hidden
Markov Model (HMM). Finally, Model 4 models
morphosyntactic agreement with a transition proba-
bility distribution, encouraging adjacent tokens with
the same endings to also have the same final suffix.
</bodyText>
<sectionHeader confidence="0.939623" genericHeader="method">
4 Applying Maximum Marginal Decoding
</sectionHeader>
<subsectionHeader confidence="0.801286">
to Reduce Variance and Noise
</subsectionHeader>
<bodyText confidence="0.999951827586207">
Maximum marginal decoding (Johnson and Gold-
water, 2009) (MM) is a technique which assigns
to each latent variable the value with the high-
est marginal probability, thereby maximizing the
expected number of correct assignments (Rabiner,
1989). Johnson and Goldwater (2009) extend MM
to Gibbs sampling by drawing a set of N indepen-
dent Gibbs samples, and selecting for each word the
most frequent segmentation found in them. They
found that MM improved segmentation accuracy
over the mean, consistent with its maximization cri-
terion. However, for our setting, we find that MM
provides several other crucial advantages as well.
First, MM dramatically reduces the output vari-
ance of Gibbs sampling (GS). Table 1 documents the
severity of this variance for the MT-08 lexicon, as
measured by the average exact-match accuracy and
segmentation F-measure between different runs. It
shows that on average, 13% of the word tokens, and
25% of the word types, are segmented differently
from run to run, which obviously makes the input to
MT highly unstable. By contrast the “MM” column
of Table 1 shows that two different runs of MM, each
derived by combining separate sets of 25 GS runs,
agree on the segmentations of over 95% of the word
token – a dramatic improvement in stability.
Second, MM reduces noise from the spurious af-
fixes that the unsupervised segmenter induces for
large lexicons. As Table 2 shows, the segmenter
</bodyText>
<page confidence="0.997451">
323
</page>
<table confidence="0.999477">
Decoding Level Rec Prec F1 Acc
Gibbs Type 82.9 83.2 83.1 74.5
Token 87.5 89.1 88.3 86.7
MM Type 95.9 95.8 95.9 93.9
Token 97.3 94.0 95.6 95.1
</table>
<tableCaption confidence="0.9017654">
Table 1: Comparison of agreement in outputs between
25 runs of Gibbs sampling vs. 2 runs of MM on the
full MT-08 data set. We give the average segmentation
recall, precision, F1-measure, and exact-match accuracy
between outputs, at word-type and word-token levels.
</tableCaption>
<table confidence="0.999821833333333">
ATB MT-08
GS GS MM Morf
Unique prefixes 17 130 93 287
Unique suffixes 41 261 216 241
Top-95 prefixes 7 7 6 6
Top-95 suffixes 14 26 19 19
</table>
<tableCaption confidence="0.982045">
Table 2: Affix statistics of unsupervised segmenters. For
</tableCaption>
<bodyText confidence="0.98216996">
the ATB lexicon, we show statistics for the Lee seg-
menter with regular Gibbs sampling (GS). For the MT-
08 lexicon, we also show the output of the Lee segmenter
with maximum marginal decoding (MM). In addition, we
show statistics for Morfessor.
induces 130 prefixes and 261 suffixes for MT-08
(statistics for Morfessor are similar). This phe-
nomenon is fundamental to Bayesian nonparamet-
ric models, which expand indefinitely to fit the data
they are given (Wasserman, 2006). But MM helps
to alleviate it, reducing unique prefixes and suffixes
for MT-08 by 28% and 21%, respectively. It also re-
duces the number of unique prefixes/suffixes which
account for 95% of the prefix/suffix tokens (Top-95).
Finally, we find that in our setting, MM increases
accuracy not just over the mean, but over even the
best-scoring of the runs. As shown in Table 3, MM
increases segmentation F-measure from 86.2% to
88.2%. This exceeds the best published results on
ATB (Naradowsky and Toutanova, 2011).
These results suggest that MM may be worth con-
sidering for other GS applications, not only for the
accuracy improvements pointed out by Johnson and
Goldwater (2009), but also for its potential to pro-
vide more stable and less noisy results.
</bodyText>
<table confidence="0.9856842">
Model Mean Min Max MM
M1 80.1 79.0 81.5 81.8
M2 81.4 80.2 83.0 82.0
M3 81.4 80.1 82.8 83.2
M4 86.2 85.4 87.2 88.2
</table>
<tableCaption confidence="0.961712166666667">
Table 3: Segmentation F-scores on ATB dataset for Lee
segmenter, shown for each Model level M1–M4 on the
Arabic segmentation dataset used by (Poon et al., 2009):
We give the mean, minimum, and maximum F-scores for
25 independent runs of Gibbs sampling, together with the
F-score from running MM over that same set of runs.
</tableCaption>
<sectionHeader confidence="0.990894" genericHeader="method">
5 MT Evaluation
</sectionHeader>
<subsectionHeader confidence="0.965048">
5.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.996197129032258">
MT System. Our experiments were performed
using a state-of-the-art, hierarchical string-to-
dependency-tree MT system, described in Shen et
al. (2008).
Morphological Analyzers. We compare the Lee
segmenter with the supervised MSA segmenter
MADA, using its “D3” scheme. We also compare
with Sakhr, an intensively-engineered, supervised
MSA segmenter which applies multiple NLP tech-
nologies to the segmentation problem, and which
has given the best results for our MT system in pre-
vious work (Zbib et al., 2012a). We also compare
with Morfessor.
MT experiments. We apply the appropriate seg-
menter to split words into morphemes, which we
then treat as words for alignment and decoding. Fol-
lowing Lee et al. (2011), we segment the test and
training sets jointly, estimating separate translation
models for each segmenter/dataset combination.
Training and Test Corpora. Our “Full MSA” cor-
pus is the NIST MT-08 Constrained Data Track Ara-
bic training corpus (35M total, 336K unique words);
our “Small MSA” corpus is a 1.3M-word subset.
Both are tested on the MT-08 evaluation set. For
dialect, we use a Levantine dialectal Arabic cor-
pus collected from the web with 1.5M total, 160K
unique words and 18K words held-out for test (Zbib
et al., 2012b)
Performance Metrics. We evaluate MT with BLEU
score. To calculate statistical significance, we use
the boot-strap resampling method of Koehn (2004).
</bodyText>
<page confidence="0.997547">
324
</page>
<subsectionHeader confidence="0.817396">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999896043478261">
Table 4 summarizes the BLEU scores obtained from
using various segmenters, for three training/test sets:
Full MSA, Small MSA, and Levantine dialect.
As expected, Sakhr gives the best results for
MSA. Morfessor underperforms the other seg-
menters, perhaps because of its lower accuracy on
Arabic, as reported by Poon et al. (2009). The
Lee segmenter gives the best results for Levantine,
inducing valid Levantine affixes (e.g “hAl+” for
MSA’s “h*A-Al+”, English “this-the”) and yielding
an 18% relative gain over the unsegmented baseline.
What is more surprising is that the Lee segmenter
compares favorably with the supervised MSA seg-
menters on MSA itself. In particular, the Lee seg-
menter with MM yields higher BLEU scores than
does MADA, a leading supervised segmenter, while
preserving almost the same performance as GS on
dialect. On Small MSA, it recoups 93% of even
Sakhr’s gain.
By contrast, the Lee segmenter recoups only 79%
of Sakhr’s gain on Full MSA. This might result from
the phenomenon alluded to in Section 4, where addi-
tional data sometimes degrades performance for un-
supervised analyzers. However, the Lee segmenter’s
gain on Levantine (18%) is higher than its gain on
Small MSA (13%), even though Levantine has more
data (1.5M vs. 1.3M words). This might be be-
cause dialect, being less standardized, has more or-
thographic and morphological variability, which un-
supervised segmentation helps to resolve.
These experiments also show that while Model 4
gives the best F-score, Model 3 gives the best MT
scores. Comparison of Model 3 and 4 segmentations
shows that Model 4 induces a much larger num-
ber of inflectional suffixes, especially the feminine
singular suffix “-p”, which accounts for a plurality
(16%) of the differences by token. While such suf-
fixes improve F-measure on the segmentation refer-
ences, they do not correspond to any English lexical
unit, and thus do not help alignment.
An interesting question is how much performance
might be gained from a supervised segmenter that
was as intensively engineered for dialect as Sakhr
was for MSA. Assuming a gain ratio of 0.93, similar
to Small MSA, the estimated BLEU score would be
20.38, for a relative gain of just 5% over the unsuper-
</bodyText>
<table confidence="0.9996526">
System Small Full Lev
MSA MSA Dial
Unsegmented 38.69 43.45 17.10
Sakhr 43.99 46.51 19.60
MADA 43.23 45.64 19.29
Morfessor 42.07 44.71 18.38
M1 43.12 44.80 19.70
M2 43.16 45.45 20.15+
Lee GS
M3 43.07 44.82 19.97
M4 42.93 45.06 19.55
M1 43.53 45.14 19.75
Lee MM M2 43.45 45.29 19.75
M3 43.64+ 45.84 20.09
M4 43.56 45.16 19.93
</table>
<tableCaption confidence="0.996431">
Table 4: BLEU scores for all experiments. Full MSA is
</tableCaption>
<bodyText confidence="0.991832222222222">
the the full MT-08 corpus, Small MSA is a 1.3M-word
subset, Lev Dial our Levantine dataset. For each of these,
the highest Lee segmenter score is in bold, with “+” if
statistically significant vs. MADA at the 95% confidence
level or higher. The highest overall score is in bold italic.
vised segmenter. Given the large engineering effort
that would be required to achieve this gain, the un-
supervised segmenter may be a more cost-effective
choice for dialectal Arabic.
</bodyText>
<sectionHeader confidence="0.996153" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999948555555556">
We compare unsupervised vs. supervised morpho-
logical segmentation for Arabic-to-English machine
translation. We add maximum marginal decoding
to the unsupervised segmenter, and show that it
surpasses the state-of-the-art segmentation perfor-
mance, purges the segmenter of noise and variabil-
ity, yields BLEU scores on MSA competitive with
those from supervised segmenters, and gives an 18%
relative BLEU gain on Levantine dialectal Arabic.
</bodyText>
<sectionHeader confidence="0.974873" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999279777777778">
This material is based upon work supported by
DARPA under Contract Nos. HR0011-12-C00014
and HR0011-12-C00015, and by ONR MURI Con-
tract No. W911NF-10-1-0533. Any opinions, find-
ings and conclusions or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the views of the US government.
We thank Rabih Zbib for his help with interpreting
Levantine Arabic segmentation output.
</bodyText>
<page confidence="0.998755">
325
</page>
<sectionHeader confidence="0.933153" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992160971428571">
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proceedings ofACL-08: HLT.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic statistical machine
translation. In Proceedings of ACL-08: HLT, Short
Papers.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process., 4:3:1–
3:34, February.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in Arabic-English statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, Short Papers.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparametric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009Annual Conference of the NorthAmeri-
can Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings ofEMNLP-CoNLL, pages
868–876.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004.
Yoong Keok Lee, Aria Haghighi, and Regina Barzi-
lay. 2011. Modeling syntactic context improves
morphological segmentation. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation
for machine translation of morphologically rich lan-
guages. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing.
Cos¸kun Mermer and Ahmet Afs¸ın Akın. 2010. Unsuper-
vised search for the optimal segmentation for statisti-
cal machine translation. In Proceedings of the ACL
2010 Student Research Workshop, pages 31–36, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Cos¸kun Mermer and Murat Saraclar. 2011. Unsuper-
vised Turkish morphological segmentation for statis-
tical machine translation. In Workshop on Machine
Translation and Morphologically-rich languages, Jan-
uary.
Preslav Nakov and Hwee Tou Ng. 2011. Trans-
lating from morphologically complex languages: A
paraphrase-based approach. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich hidden semi-Markov models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
NIST. 2010. NIST 2008 Open Machine Translation
(Open MT) Evaluation. http://www.ldc.
upenn.edu/Catalog/catalogEntry.jsp?
catalogId=LDC2010T21/.
Kemal Oflazer. 1993. Two-level description of Turkish
morphology. In Proceedings of the Sixth Conference
of the European Chapter of the Association for Com-
putational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. In Proceedings of the IEEE, pages 257–
286.
Fatiha Sadat and Nizar Habash. 2006. Combination
of Arabic preprocessing schemes for statistical ma-
chine translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT.
Sami Virpioja, Jaakko J. V¨ayrynen, Mathias Creutz, and
Markus Sadeniemi. 2007. Morphology-aware statisti-
cal machine translation based on morphs induced in an
unsupervised manner. In Proceedings of the Machine
Translation Summit XI.
Larry Wasserman. 2006. All of Nonparametric Statistics.
Springer.
</reference>
<page confidence="0.98832">
326
</page>
<reference confidence="0.999398647058824">
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In Proceedings of EACL.
Rabih Zbib, Michael Kayser, Spyros Matsoukas, John
Makhoul, Hazem Nader, Hamdy Soliman, and Rami
Safadi. 2012a. Methods for integrating rule-based and
statistical systems for Arabic to English machine trans-
lation. Machine Translation, 26(1-2):67–83.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012b. Machine translation of Arabic dialects. In
NAACL 2012: Proceedings of the 2012 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Montreal, Quebec, Canada, June. Association for
Computational Linguistics.
</reference>
<page confidence="0.998418">
327
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.401964">
<title confidence="0.999078">Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</title>
<author confidence="0.9545895">David Stallard Jacob Michael</author>
<affiliation confidence="0.560007">BBN Technologies</affiliation>
<author confidence="0.944271">Yoong Keok Lee Regina Barzilay</author>
<affiliation confidence="0.948571">CSAIL Massachusetts Institute of Technology</affiliation>
<abstract confidence="0.996362380952381">If unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving MT performance on low-resource inflected languages. In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system. We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable. Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic. Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005), a leading segmenter.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
<author>Philipp Koehn</author>
</authors>
<title>Enriching morphologically poor languages for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08:</booktitle>
<publisher>HLT.</publisher>
<contexts>
<context position="4009" citStr="Avramidis and Koehn, 2008" startWordPosition="581" endWordPosition="584">ssociation for Computational Linguistics, pages 322–327, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2 Related Work Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007</context>
</contexts>
<marker>Avramidis, Koehn, 2008</marker>
<rawString>Eleftherios Avramidis and Philipp Koehn. 2008. Enriching morphologically poor languages for statistical machine translation. In Proceedings ofACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ibrahim Badr</author>
<author>Rabih Zbib</author>
<author>James Glass</author>
</authors>
<title>Segmentation for English-to-Arabic statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers.</booktitle>
<contexts>
<context position="4310" citStr="Badr et al., 2008" startWordPosition="632" endWordPosition="635">ological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007; Clifton and Sarkar, 2011; Mermer and Akın, 2010; Mermer and Saraclar, 2011). Virpioja et al. (2007) apply the unsupervised morphological segmenter Morfessor (Creutz and Lagus, 2007), and apply an existing MT system at the level of morphemes. The system does not outperform the word baseline partiall</context>
</contexts>
<marker>Badr, Zbib, Glass, 2008</marker>
<rawString>Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Segmentation for English-to-Arabic statistical machine translation. In Proceedings of ACL-08: HLT, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Clifton</author>
<author>Anoop Sarkar</author>
</authors>
<title>Combining morpheme-based machine translation with postprocessing morpheme prediction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="4337" citStr="Clifton and Sarkar, 2011" startWordPosition="636" endWordPosition="639">or pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007; Clifton and Sarkar, 2011; Mermer and Akın, 2010; Mermer and Saraclar, 2011). Virpioja et al. (2007) apply the unsupervised morphological segmenter Morfessor (Creutz and Lagus, 2007), and apply an existing MT system at the level of morphemes. The system does not outperform the word baseline partially due to the insufficient a</context>
</contexts>
<marker>Clifton, Sarkar, 2011</marker>
<rawString>Ann Clifton and Anoop Sarkar. 2011. Combining morpheme-based machine translation with postprocessing morpheme prediction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>4</volume>
<pages>3--34</pages>
<contexts>
<context position="4609" citStr="Creutz and Lagus, 2007" startWordPosition="678" endWordPosition="681">midis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007; Clifton and Sarkar, 2011; Mermer and Akın, 2010; Mermer and Saraclar, 2011). Virpioja et al. (2007) apply the unsupervised morphological segmenter Morfessor (Creutz and Lagus, 2007), and apply an existing MT system at the level of morphemes. The system does not outperform the word baseline partially due to the insufficient accuracy of the automatic morphological analyzer. The work of Mermer and Akın (2010) and Mermer and Saraclar (2011) attempts to integrate morphology and MT more closely than we do, by incorporating bilingual alignment probabilities into a Gibbs-sampled version of Morfessor</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Trans. Speech Lang. Process., 4:3:1– 3:34, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1050" citStr="Habash and Rambow, 2005" startWordPosition="136" endWordPosition="139">ormance on low-resource inflected languages. In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system. We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable. Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic. Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005), a leading supervised MSA segmenter. 1 Introduction If unsupervised morphological segmenters could approach the effectiveness of supervised ones, they would be a very attractive choice for improving machine translation (MT) performance in low-resource inflected languages. An example of particular current interest is Arabic, whose various colloquial dialects are sufficiently different from Modern Standard Arabic (MSA) in lexicon, orthography, and morphology, as to be low-resource languages themselves. An additional advantage of Arabic for study is the availability of high-quality supervised se</context>
<context position="5633" citStr="Habash and Rambow, 2005" startWordPosition="838" endWordPosition="841">Akın (2010) and Mermer and Saraclar (2011) attempts to integrate morphology and MT more closely than we do, by incorporating bilingual alignment probabilities into a Gibbs-sampled version of Morfessor for Turkish-toEnglish MT. However, the bilingual strategy shows no gain over the monolingual version, and neither version is competitive for MT with a supervised Turkish morphological segmenter (Oflazer, 1993). By contrast, the unsupervised analyzer we report on here yields MSA-to-English MT performance that equals or exceed the performance obtained with a leading supervised MSA segmenter, MADA (Habash and Rambow, 2005). 3 Review of Lee Unsupervised Segmenter The segmenter of Lee et al. (2011) is a probabilistic model operating at word-type level. It is divided into four sub-model levels. Model 1 prefers small affix lexicons, and assumes that morphemes are drawn independently. Model 2 generates a latent POS tag for each word type, conditioning the word’s affixes on the tag, thereby encouraging compatible affixes to be generated together. Model 3 incorporates token-level contextual information, by generating word tokens with a type-level Hidden Markov Model (HMM). Finally, Model 4 models morphosyntactic agree</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers.</booktitle>
<contexts>
<context position="3848" citStr="Habash, 2008" startWordPosition="558" endWordPosition="559">r increases BLEU score by 18% over the unsegmented baseline. 1http://www.sakhr.com/Default.aspx 322 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322–327, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2 Related Work Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically</context>
</contexts>
<marker>Habash, 2008</marker>
<rawString>Nizar Habash. 2008. Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation. In Proceedings of ACL-08: HLT, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparametric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2769" citStr="Johnson and Goldwater, 2009" startWordPosition="402" endWordPosition="405">enter can in fact rival or surpass supervised MSA segmenters on MSA itself, while at the same time providing superior performance on dialect. Specifically, we compare the state-of-the-art morphological analyzer of Lee et al. (2011) with two leading supervised analyzers for MSA, MADA and Sakhr1, each serving as an alternative preprocessor for a state-ofthe-art statistical MT system (Shen et al., 2008). We measure MSA performance on NIST MT-08 (NIST, 2010), and dialect performance on a Levantine dialect web corpus (Zbib et al., 2012b). To improve performance, we apply maximum marginal decoding (Johnson and Goldwater, 2009) (MM) to combine multiple runs of the Lee segmenter, and show that this dramatically reduces the variance and noise in the segmenter output, while yielding an improved segmentation accuracy that exceeds the best published scores for unsupervised segmentation on Arabic Treebank (Naradowsky and Toutanova, 2011). We also show that it yields MT08 BLEU scores that are higher than those obtained with MADA, a leading supervised MSA segmenter. For Levantine, the segmenter increases BLEU score by 18% over the unsegmented baseline. 1http://www.sakhr.com/Default.aspx 322 Proceedings of the 50th Annual Me</context>
<context position="6489" citStr="Johnson and Goldwater, 2009" startWordPosition="969" endWordPosition="973">morphemes are drawn independently. Model 2 generates a latent POS tag for each word type, conditioning the word’s affixes on the tag, thereby encouraging compatible affixes to be generated together. Model 3 incorporates token-level contextual information, by generating word tokens with a type-level Hidden Markov Model (HMM). Finally, Model 4 models morphosyntactic agreement with a transition probability distribution, encouraging adjacent tokens with the same endings to also have the same final suffix. 4 Applying Maximum Marginal Decoding to Reduce Variance and Noise Maximum marginal decoding (Johnson and Goldwater, 2009) (MM) is a technique which assigns to each latent variable the value with the highest marginal probability, thereby maximizing the expected number of correct assignments (Rabiner, 1989). Johnson and Goldwater (2009) extend MM to Gibbs sampling by drawing a set of N independent Gibbs samples, and selecting for each word the most frequent segmentation found in them. They found that MM improved segmentation accuracy over the mean, consistent with its maximization criterion. However, for our setting, we find that MM provides several other crucial advantages as well. First, MM dramatically reduces </context>
<context position="9600" citStr="Johnson and Goldwater (2009)" startWordPosition="1491" endWordPosition="1494">prefixes and suffixes for MT-08 by 28% and 21%, respectively. It also reduces the number of unique prefixes/suffixes which account for 95% of the prefix/suffix tokens (Top-95). Finally, we find that in our setting, MM increases accuracy not just over the mean, but over even the best-scoring of the runs. As shown in Table 3, MM increases segmentation F-measure from 86.2% to 88.2%. This exceeds the best published results on ATB (Naradowsky and Toutanova, 2011). These results suggest that MM may be worth considering for other GS applications, not only for the accuracy improvements pointed out by Johnson and Goldwater (2009), but also for its potential to provide more stable and less noisy results. Model Mean Min Max MM M1 80.1 79.0 81.5 81.8 M2 81.4 80.2 83.0 82.0 M3 81.4 80.1 82.8 83.2 M4 86.2 85.4 87.2 88.2 Table 3: Segmentation F-scores on ATB dataset for Lee segmenter, shown for each Model level M1–M4 on the Arabic segmentation dataset used by (Poon et al., 2009): We give the mean, minimum, and maximum F-scores for 25 independent runs of Gibbs sampling, together with the F-score from running MM over that same set of runs. 5 MT Evaluation 5.1 Experimental Design MT System. Our experiments were performed using</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparametric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL,</booktitle>
<pages>868--876</pages>
<contexts>
<context position="3955" citStr="Koehn and Hoang, 2007" startWordPosition="573" endWordPosition="576">2 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322–327, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2 Related Work Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings ofEMNLP-CoNLL, pages 868–876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="11539" citStr="Koehn (2004)" startWordPosition="1811" endWordPosition="1812">te translation models for each segmenter/dataset combination. Training and Test Corpora. Our “Full MSA” corpus is the NIST MT-08 Constrained Data Track Arabic training corpus (35M total, 336K unique words); our “Small MSA” corpus is a 1.3M-word subset. Both are tested on the MT-08 evaluation set. For dialect, we use a Levantine dialectal Arabic corpus collected from the web with 1.5M total, 160K unique words and 18K words held-out for test (Zbib et al., 2012b) Performance Metrics. We evaluate MT with BLEU score. To calculate statistical significance, we use the boot-strap resampling method of Koehn (2004). 324 5.2 Results and Discussion Table 4 summarizes the BLEU scores obtained from using various segmenters, for three training/test sets: Full MSA, Small MSA, and Levantine dialect. As expected, Sakhr gives the best results for MSA. Morfessor underperforms the other segmenters, perhaps because of its lower accuracy on Arabic, as reported by Poon et al. (2009). The Lee segmenter gives the best results for Levantine, inducing valid Levantine affixes (e.g “hAl+” for MSA’s “h*A-Al+”, English “this-the”) and yielding an 18% relative gain over the unsegmented baseline. What is more surprising is tha</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Modeling syntactic context improves morphological segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="2372" citStr="Lee et al. (2011)" startWordPosition="339" endWordPosition="342">ervised MSA segmenters on dialect establishes a lower bound, which the unsupervised segmenter must exceed if it is to be useful for dialect. And comparing the gain for supervised and unsupervised segmenters on MSA tells us how useful the unsupervised segmenter is, relative to the ideal case in which a supervised segmenter is available. In this paper, we show that an unsupervised segmenter can in fact rival or surpass supervised MSA segmenters on MSA itself, while at the same time providing superior performance on dialect. Specifically, we compare the state-of-the-art morphological analyzer of Lee et al. (2011) with two leading supervised analyzers for MSA, MADA and Sakhr1, each serving as an alternative preprocessor for a state-ofthe-art statistical MT system (Shen et al., 2008). We measure MSA performance on NIST MT-08 (NIST, 2010), and dialect performance on a Levantine dialect web corpus (Zbib et al., 2012b). To improve performance, we apply maximum marginal decoding (Johnson and Goldwater, 2009) (MM) to combine multiple runs of the Lee segmenter, and show that this dramatically reduces the variance and noise in the segmenter output, while yielding an improved segmentation accuracy that exceeds </context>
<context position="5708" citStr="Lee et al. (2011)" startWordPosition="851" endWordPosition="854"> more closely than we do, by incorporating bilingual alignment probabilities into a Gibbs-sampled version of Morfessor for Turkish-toEnglish MT. However, the bilingual strategy shows no gain over the monolingual version, and neither version is competitive for MT with a supervised Turkish morphological segmenter (Oflazer, 1993). By contrast, the unsupervised analyzer we report on here yields MSA-to-English MT performance that equals or exceed the performance obtained with a leading supervised MSA segmenter, MADA (Habash and Rambow, 2005). 3 Review of Lee Unsupervised Segmenter The segmenter of Lee et al. (2011) is a probabilistic model operating at word-type level. It is divided into four sub-model levels. Model 1 prefers small affix lexicons, and assumes that morphemes are drawn independently. Model 2 generates a latent POS tag for each word type, conditioning the word’s affixes on the tag, thereby encouraging compatible affixes to be generated together. Model 3 incorporates token-level contextual information, by generating word tokens with a type-level Hidden Markov Model (HMM). Finally, Model 4 models morphosyntactic agreement with a transition probability distribution, encouraging adjacent token</context>
<context position="10861" citStr="Lee et al. (2011)" startWordPosition="1701" endWordPosition="1704">ependency-tree MT system, described in Shen et al. (2008). Morphological Analyzers. We compare the Lee segmenter with the supervised MSA segmenter MADA, using its “D3” scheme. We also compare with Sakhr, an intensively-engineered, supervised MSA segmenter which applies multiple NLP technologies to the segmentation problem, and which has given the best results for our MT system in previous work (Zbib et al., 2012a). We also compare with Morfessor. MT experiments. We apply the appropriate segmenter to split words into morphemes, which we then treat as words for alignment and decoding. Following Lee et al. (2011), we segment the test and training sets jointly, estimating separate translation models for each segmenter/dataset combination. Training and Test Corpora. Our “Full MSA” corpus is the NIST MT-08 Constrained Data Track Arabic training corpus (35M total, 336K unique words); our “Small MSA” corpus is a 1.3M-word subset. Both are tested on the MT-08 evaluation set. For dialect, we use a Levantine dialectal Arabic corpus collected from the web with 1.5M total, 160K unique words and 18K words held-out for test (Zbib et al., 2012b) Performance Metrics. We evaluate MT with BLEU score. To calculate sta</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2011</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2011. Modeling syntactic context improves morphological segmentation. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Preslav Nakov</author>
<author>Min-Yen Kan</author>
</authors>
<title>A hybrid morpheme-word representation for machine translation of morphologically rich languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3868" citStr="Luong et al., 2010" startWordPosition="560" endWordPosition="563">EU score by 18% over the unsegmented baseline. 1http://www.sakhr.com/Default.aspx 322 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322–327, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2 Related Work Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A nu</context>
</contexts>
<marker>Luong, Nakov, Kan, 2010</marker>
<rawString>Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan. 2010. A hybrid morpheme-word representation for machine translation of morphologically rich languages. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cos¸kun Mermer</author>
<author>Ahmet Afs¸ın Akın</author>
</authors>
<title>Unsupervised search for the optimal segmentation for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Student Research Workshop,</booktitle>
<pages>31--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="4658" citStr="Mermer and Akın, 2010" startWordPosition="686" endWordPosition="689">babilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007; Clifton and Sarkar, 2011; Mermer and Akın, 2010; Mermer and Saraclar, 2011). Virpioja et al. (2007) apply the unsupervised morphological segmenter Morfessor (Creutz and Lagus, 2007), and apply an existing MT system at the level of morphemes. The system does not outperform the word baseline partially due to the insufficient accuracy of the automatic morphological analyzer. The work of Mermer and Akın (2010) and Mermer and Saraclar (2011) attempts to integrate morphology and MT more closely than we do, by incorporating bilingual alignment probabilities into a Gibbs-sampled version of Morfessor for Turkish-toEnglish MT. However, the bilingual</context>
</contexts>
<marker>Mermer, Akın, 2010</marker>
<rawString>Cos¸kun Mermer and Ahmet Afs¸ın Akın. 2010. Unsupervised search for the optimal segmentation for statistical machine translation. In Proceedings of the ACL 2010 Student Research Workshop, pages 31–36, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cos¸kun Mermer</author>
<author>Murat Saraclar</author>
</authors>
<title>Unsupervised Turkish morphological segmentation for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Workshop on Machine Translation and Morphologically-rich languages,</booktitle>
<contexts>
<context position="4686" citStr="Mermer and Saraclar, 2011" startWordPosition="690" endWordPosition="693">ncoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007; Clifton and Sarkar, 2011; Mermer and Akın, 2010; Mermer and Saraclar, 2011). Virpioja et al. (2007) apply the unsupervised morphological segmenter Morfessor (Creutz and Lagus, 2007), and apply an existing MT system at the level of morphemes. The system does not outperform the word baseline partially due to the insufficient accuracy of the automatic morphological analyzer. The work of Mermer and Akın (2010) and Mermer and Saraclar (2011) attempts to integrate morphology and MT more closely than we do, by incorporating bilingual alignment probabilities into a Gibbs-sampled version of Morfessor for Turkish-toEnglish MT. However, the bilingual strategy shows no gain over</context>
</contexts>
<marker>Mermer, Saraclar, 2011</marker>
<rawString>Cos¸kun Mermer and Murat Saraclar. 2011. Unsupervised Turkish morphological segmentation for statistical machine translation. In Workshop on Machine Translation and Morphologically-rich languages, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Translating from morphologically complex languages: A paraphrase-based approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="3889" citStr="Nakov and Ng, 2011" startWordPosition="564" endWordPosition="567"> the unsegmented baseline. 1http://www.sakhr.com/Default.aspx 322 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322–327, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2 Related Work Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approa</context>
</contexts>
<marker>Nakov, Ng, 2011</marker>
<rawString>Preslav Nakov and Hwee Tou Ng. 2011. Translating from morphologically complex languages: A paraphrase-based approach. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Naradowsky</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised bilingual morpheme segmentation and alignment with context-rich hidden semi-Markov models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="3079" citStr="Naradowsky and Toutanova, 2011" startWordPosition="448" endWordPosition="451">g as an alternative preprocessor for a state-ofthe-art statistical MT system (Shen et al., 2008). We measure MSA performance on NIST MT-08 (NIST, 2010), and dialect performance on a Levantine dialect web corpus (Zbib et al., 2012b). To improve performance, we apply maximum marginal decoding (Johnson and Goldwater, 2009) (MM) to combine multiple runs of the Lee segmenter, and show that this dramatically reduces the variance and noise in the segmenter output, while yielding an improved segmentation accuracy that exceeds the best published scores for unsupervised segmentation on Arabic Treebank (Naradowsky and Toutanova, 2011). We also show that it yields MT08 BLEU scores that are higher than those obtained with MADA, a leading supervised MSA segmenter. For Levantine, the segmenter increases BLEU score by 18% over the unsegmented baseline. 1http://www.sakhr.com/Default.aspx 322 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322–327, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2 Related Work Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches</context>
<context position="9434" citStr="Naradowsky and Toutanova, 2011" startWordPosition="1464" endWordPosition="1467"> fundamental to Bayesian nonparametric models, which expand indefinitely to fit the data they are given (Wasserman, 2006). But MM helps to alleviate it, reducing unique prefixes and suffixes for MT-08 by 28% and 21%, respectively. It also reduces the number of unique prefixes/suffixes which account for 95% of the prefix/suffix tokens (Top-95). Finally, we find that in our setting, MM increases accuracy not just over the mean, but over even the best-scoring of the runs. As shown in Table 3, MM increases segmentation F-measure from 86.2% to 88.2%. This exceeds the best published results on ATB (Naradowsky and Toutanova, 2011). These results suggest that MM may be worth considering for other GS applications, not only for the accuracy improvements pointed out by Johnson and Goldwater (2009), but also for its potential to provide more stable and less noisy results. Model Mean Min Max MM M1 80.1 79.0 81.5 81.8 M2 81.4 80.2 83.0 82.0 M3 81.4 80.1 82.8 83.2 M4 86.2 85.4 87.2 88.2 Table 3: Segmentation F-scores on ATB dataset for Lee segmenter, shown for each Model level M1–M4 on the Arabic segmentation dataset used by (Poon et al., 2009): We give the mean, minimum, and maximum F-scores for 25 independent runs of Gibbs s</context>
</contexts>
<marker>Naradowsky, Toutanova, 2011</marker>
<rawString>Jason Naradowsky and Kristina Toutanova. 2011. Unsupervised bilingual morpheme segmentation and alignment with context-rich hidden semi-Markov models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>NIST</title>
<date>2010</date>
<booktitle>Open Machine Translation (Open MT) Evaluation. http://www.ldc. upenn.edu/Catalog/catalogEntry.jsp? catalogId=LDC2010T21/.</booktitle>
<contexts>
<context position="2599" citStr="NIST, 2010" startWordPosition="378" endWordPosition="379"> the unsupervised segmenter is, relative to the ideal case in which a supervised segmenter is available. In this paper, we show that an unsupervised segmenter can in fact rival or surpass supervised MSA segmenters on MSA itself, while at the same time providing superior performance on dialect. Specifically, we compare the state-of-the-art morphological analyzer of Lee et al. (2011) with two leading supervised analyzers for MSA, MADA and Sakhr1, each serving as an alternative preprocessor for a state-ofthe-art statistical MT system (Shen et al., 2008). We measure MSA performance on NIST MT-08 (NIST, 2010), and dialect performance on a Levantine dialect web corpus (Zbib et al., 2012b). To improve performance, we apply maximum marginal decoding (Johnson and Goldwater, 2009) (MM) to combine multiple runs of the Lee segmenter, and show that this dramatically reduces the variance and noise in the segmenter output, while yielding an improved segmentation accuracy that exceeds the best published scores for unsupervised segmentation on Arabic Treebank (Naradowsky and Toutanova, 2011). We also show that it yields MT08 BLEU scores that are higher than those obtained with MADA, a leading supervised MSA s</context>
</contexts>
<marker>NIST, 2010</marker>
<rawString>NIST. 2010. NIST 2008 Open Machine Translation (Open MT) Evaluation. http://www.ldc. upenn.edu/Catalog/catalogEntry.jsp? catalogId=LDC2010T21/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Two-level description of Turkish morphology.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5419" citStr="Oflazer, 1993" startWordPosition="807" endWordPosition="808">n existing MT system at the level of morphemes. The system does not outperform the word baseline partially due to the insufficient accuracy of the automatic morphological analyzer. The work of Mermer and Akın (2010) and Mermer and Saraclar (2011) attempts to integrate morphology and MT more closely than we do, by incorporating bilingual alignment probabilities into a Gibbs-sampled version of Morfessor for Turkish-toEnglish MT. However, the bilingual strategy shows no gain over the monolingual version, and neither version is competitive for MT with a supervised Turkish morphological segmenter (Oflazer, 1993). By contrast, the unsupervised analyzer we report on here yields MSA-to-English MT performance that equals or exceed the performance obtained with a leading supervised MSA segmenter, MADA (Habash and Rambow, 2005). 3 Review of Lee Unsupervised Segmenter The segmenter of Lee et al. (2011) is a probabilistic model operating at word-type level. It is divided into four sub-model levels. Model 1 prefers small affix lexicons, and assumes that morphemes are drawn independently. Model 2 generates a latent POS tag for each word type, conditioning the word’s affixes on the tag, thereby encouraging comp</context>
</contexts>
<marker>Oflazer, 1993</marker>
<rawString>Kemal Oflazer. 1993. Two-level description of Turkish morphology. In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9950" citStr="Poon et al., 2009" startWordPosition="1557" endWordPosition="1560">e from 86.2% to 88.2%. This exceeds the best published results on ATB (Naradowsky and Toutanova, 2011). These results suggest that MM may be worth considering for other GS applications, not only for the accuracy improvements pointed out by Johnson and Goldwater (2009), but also for its potential to provide more stable and less noisy results. Model Mean Min Max MM M1 80.1 79.0 81.5 81.8 M2 81.4 80.2 83.0 82.0 M3 81.4 80.1 82.8 83.2 M4 86.2 85.4 87.2 88.2 Table 3: Segmentation F-scores on ATB dataset for Lee segmenter, shown for each Model level M1–M4 on the Arabic segmentation dataset used by (Poon et al., 2009): We give the mean, minimum, and maximum F-scores for 25 independent runs of Gibbs sampling, together with the F-score from running MM over that same set of runs. 5 MT Evaluation 5.1 Experimental Design MT System. Our experiments were performed using a state-of-the-art, hierarchical string-todependency-tree MT system, described in Shen et al. (2008). Morphological Analyzers. We compare the Lee segmenter with the supervised MSA segmenter MADA, using its “D3” scheme. We also compare with Sakhr, an intensively-engineered, supervised MSA segmenter which applies multiple NLP technologies to the seg</context>
<context position="11900" citStr="Poon et al. (2009)" startWordPosition="1866" endWordPosition="1869">ollected from the web with 1.5M total, 160K unique words and 18K words held-out for test (Zbib et al., 2012b) Performance Metrics. We evaluate MT with BLEU score. To calculate statistical significance, we use the boot-strap resampling method of Koehn (2004). 324 5.2 Results and Discussion Table 4 summarizes the BLEU scores obtained from using various segmenters, for three training/test sets: Full MSA, Small MSA, and Levantine dialect. As expected, Sakhr gives the best results for MSA. Morfessor underperforms the other segmenters, perhaps because of its lower accuracy on Arabic, as reported by Poon et al. (2009). The Lee segmenter gives the best results for Levantine, inducing valid Levantine affixes (e.g “hAl+” for MSA’s “h*A-Al+”, English “this-the”) and yielding an 18% relative gain over the unsegmented baseline. What is more surprising is that the Lee segmenter compares favorably with the supervised MSA segmenters on MSA itself. In particular, the Lee segmenter with MM yields higher BLEU scores than does MADA, a leading supervised segmenter, while preserving almost the same performance as GS on dialect. On Small MSA, it recoups 93% of even Sakhr’s gain. By contrast, the Lee segmenter recoups only</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="6674" citStr="Rabiner, 1989" startWordPosition="1000" endWordPosition="1001">Model 3 incorporates token-level contextual information, by generating word tokens with a type-level Hidden Markov Model (HMM). Finally, Model 4 models morphosyntactic agreement with a transition probability distribution, encouraging adjacent tokens with the same endings to also have the same final suffix. 4 Applying Maximum Marginal Decoding to Reduce Variance and Noise Maximum marginal decoding (Johnson and Goldwater, 2009) (MM) is a technique which assigns to each latent variable the value with the highest marginal probability, thereby maximizing the expected number of correct assignments (Rabiner, 1989). Johnson and Goldwater (2009) extend MM to Gibbs sampling by drawing a set of N independent Gibbs samples, and selecting for each word the most frequent segmentation found in them. They found that MM improved segmentation accuracy over the mean, consistent with its maximization criterion. However, for our setting, we find that MM provides several other crucial advantages as well. First, MM dramatically reduces the output variance of Gibbs sampling (GS). Table 1 documents the severity of this variance for the MT-08 lexicon, as measured by the average exact-match accuracy and segmentation F-mea</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. In Proceedings of the IEEE, pages 257– 286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fatiha Sadat</author>
<author>Nizar Habash</author>
</authors>
<title>Combination of Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4291" citStr="Sadat and Habash, 2006" startWordPosition="628" endWordPosition="631">approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007; Clifton and Sarkar, 2011; Mermer and Akın, 2010; Mermer and Saraclar, 2011). Virpioja et al. (2007) apply the unsupervised morphological segmenter Morfessor (Creutz and Lagus, 2007), and apply an existing MT system at the level of morphemes. The system does not outperform the wor</context>
</contexts>
<marker>Sadat, Habash, 2006</marker>
<rawString>Fatiha Sadat and Nizar Habash. 2006. Combination of Arabic preprocessing schemes for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="2544" citStr="Shen et al., 2008" startWordPosition="367" endWordPosition="370">ervised and unsupervised segmenters on MSA tells us how useful the unsupervised segmenter is, relative to the ideal case in which a supervised segmenter is available. In this paper, we show that an unsupervised segmenter can in fact rival or surpass supervised MSA segmenters on MSA itself, while at the same time providing superior performance on dialect. Specifically, we compare the state-of-the-art morphological analyzer of Lee et al. (2011) with two leading supervised analyzers for MSA, MADA and Sakhr1, each serving as an alternative preprocessor for a state-ofthe-art statistical MT system (Shen et al., 2008). We measure MSA performance on NIST MT-08 (NIST, 2010), and dialect performance on a Levantine dialect web corpus (Zbib et al., 2012b). To improve performance, we apply maximum marginal decoding (Johnson and Goldwater, 2009) (MM) to combine multiple runs of the Lee segmenter, and show that this dramatically reduces the variance and noise in the segmenter output, while yielding an improved segmentation accuracy that exceeds the best published scores for unsupervised segmentation on Arabic Treebank (Naradowsky and Toutanova, 2011). We also show that it yields MT08 BLEU scores that are higher th</context>
<context position="10301" citStr="Shen et al. (2008)" startWordPosition="1611" endWordPosition="1614">ean Min Max MM M1 80.1 79.0 81.5 81.8 M2 81.4 80.2 83.0 82.0 M3 81.4 80.1 82.8 83.2 M4 86.2 85.4 87.2 88.2 Table 3: Segmentation F-scores on ATB dataset for Lee segmenter, shown for each Model level M1–M4 on the Arabic segmentation dataset used by (Poon et al., 2009): We give the mean, minimum, and maximum F-scores for 25 independent runs of Gibbs sampling, together with the F-score from running MM over that same set of runs. 5 MT Evaluation 5.1 Experimental Design MT System. Our experiments were performed using a state-of-the-art, hierarchical string-todependency-tree MT system, described in Shen et al. (2008). Morphological Analyzers. We compare the Lee segmenter with the supervised MSA segmenter MADA, using its “D3” scheme. We also compare with Sakhr, an intensively-engineered, supervised MSA segmenter which applies multiple NLP technologies to the segmentation problem, and which has given the best results for our MT system in previous work (Zbib et al., 2012a). We also compare with Morfessor. MT experiments. We apply the appropriate segmenter to split words into morphemes, which we then treat as words for alignment and decoding. Following Lee et al. (2011), we segment the test and training sets </context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Jaakko J V¨ayrynen</author>
<author>Mathias Creutz</author>
<author>Markus Sadeniemi</author>
</authors>
<title>Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner.</title>
<date>2007</date>
<booktitle>In Proceedings of the Machine Translation</booktitle>
<location>Summit XI.</location>
<marker>Virpioja, V¨ayrynen, Creutz, Sadeniemi, 2007</marker>
<rawString>Sami Virpioja, Jaakko J. V¨ayrynen, Mathias Creutz, and Markus Sadeniemi. 2007. Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner. In Proceedings of the Machine Translation Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Wasserman</author>
</authors>
<title>All of Nonparametric Statistics.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="8924" citStr="Wasserman, 2006" startWordPosition="1382" endWordPosition="1383"> 287 Unique suffixes 41 261 216 241 Top-95 prefixes 7 7 6 6 Top-95 suffixes 14 26 19 19 Table 2: Affix statistics of unsupervised segmenters. For the ATB lexicon, we show statistics for the Lee segmenter with regular Gibbs sampling (GS). For the MT08 lexicon, we also show the output of the Lee segmenter with maximum marginal decoding (MM). In addition, we show statistics for Morfessor. induces 130 prefixes and 261 suffixes for MT-08 (statistics for Morfessor are similar). This phenomenon is fundamental to Bayesian nonparametric models, which expand indefinitely to fit the data they are given (Wasserman, 2006). But MM helps to alleviate it, reducing unique prefixes and suffixes for MT-08 by 28% and 21%, respectively. It also reduces the number of unique prefixes/suffixes which account for 95% of the prefix/suffix tokens (Top-95). Finally, we find that in our setting, MM increases accuracy not just over the mean, but over even the best-scoring of the runs. As shown in Table 3, MM increases segmentation F-measure from 86.2% to 88.2%. This exceeds the best published results on ATB (Naradowsky and Toutanova, 2011). These results suggest that MM may be worth considering for other GS applications, not on</context>
</contexts>
<marker>Wasserman, 2006</marker>
<rawString>Larry Wasserman. 2006. All of Nonparametric Statistics. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mei Yang</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Phrase-based backoff models for machine translation of highly inflected languages.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="3981" citStr="Yang and Kirchhoff, 2006" startWordPosition="577" endWordPosition="580">th Annual Meeting of the Association for Computational Linguistics, pages 322–327, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics 2 Related Work Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., </context>
</contexts>
<marker>Yang, Kirchhoff, 2006</marker>
<rawString>Mei Yang and Katrin Kirchhoff. 2006. Phrase-based backoff models for machine translation of highly inflected languages. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabih Zbib</author>
<author>Michael Kayser</author>
<author>Spyros Matsoukas</author>
<author>John Makhoul</author>
<author>Hazem Nader</author>
<author>Hamdy Soliman</author>
<author>Rami Safadi</author>
</authors>
<title>Methods for integrating rule-based and statistical systems for Arabic to English machine translation.</title>
<date>2012</date>
<journal>Machine Translation,</journal>
<pages>26--1</pages>
<contexts>
<context position="2677" citStr="Zbib et al., 2012" startWordPosition="390" endWordPosition="393">pervised segmenter is available. In this paper, we show that an unsupervised segmenter can in fact rival or surpass supervised MSA segmenters on MSA itself, while at the same time providing superior performance on dialect. Specifically, we compare the state-of-the-art morphological analyzer of Lee et al. (2011) with two leading supervised analyzers for MSA, MADA and Sakhr1, each serving as an alternative preprocessor for a state-ofthe-art statistical MT system (Shen et al., 2008). We measure MSA performance on NIST MT-08 (NIST, 2010), and dialect performance on a Levantine dialect web corpus (Zbib et al., 2012b). To improve performance, we apply maximum marginal decoding (Johnson and Goldwater, 2009) (MM) to combine multiple runs of the Lee segmenter, and show that this dramatically reduces the variance and noise in the segmenter output, while yielding an improved segmentation accuracy that exceeds the best published scores for unsupervised segmentation on Arabic Treebank (Naradowsky and Toutanova, 2011). We also show that it yields MT08 BLEU scores that are higher than those obtained with MADA, a leading supervised MSA segmenter. For Levantine, the segmenter increases BLEU score by 18% over the un</context>
<context position="10659" citStr="Zbib et al., 2012" startWordPosition="1667" endWordPosition="1670"> together with the F-score from running MM over that same set of runs. 5 MT Evaluation 5.1 Experimental Design MT System. Our experiments were performed using a state-of-the-art, hierarchical string-todependency-tree MT system, described in Shen et al. (2008). Morphological Analyzers. We compare the Lee segmenter with the supervised MSA segmenter MADA, using its “D3” scheme. We also compare with Sakhr, an intensively-engineered, supervised MSA segmenter which applies multiple NLP technologies to the segmentation problem, and which has given the best results for our MT system in previous work (Zbib et al., 2012a). We also compare with Morfessor. MT experiments. We apply the appropriate segmenter to split words into morphemes, which we then treat as words for alignment and decoding. Following Lee et al. (2011), we segment the test and training sets jointly, estimating separate translation models for each segmenter/dataset combination. Training and Test Corpora. Our “Full MSA” corpus is the NIST MT-08 Constrained Data Track Arabic training corpus (35M total, 336K unique words); our “Small MSA” corpus is a 1.3M-word subset. Both are tested on the MT-08 evaluation set. For dialect, we use a Levantine di</context>
</contexts>
<marker>Zbib, Kayser, Matsoukas, Makhoul, Nader, Soliman, Safadi, 2012</marker>
<rawString>Rabih Zbib, Michael Kayser, Spyros Matsoukas, John Makhoul, Hazem Nader, Hamdy Soliman, and Rami Safadi. 2012a. Methods for integrating rule-based and statistical systems for Arabic to English machine translation. Machine Translation, 26(1-2):67–83.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rabih Zbib</author>
<author>Erika Malchiodi</author>
<author>Jacob Devlin</author>
<author>David Stallard</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Machine translation of Arabic dialects.</title>
<date>2012</date>
<booktitle>In NAACL 2012: Proceedings of the 2012 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="2677" citStr="Zbib et al., 2012" startWordPosition="390" endWordPosition="393">pervised segmenter is available. In this paper, we show that an unsupervised segmenter can in fact rival or surpass supervised MSA segmenters on MSA itself, while at the same time providing superior performance on dialect. Specifically, we compare the state-of-the-art morphological analyzer of Lee et al. (2011) with two leading supervised analyzers for MSA, MADA and Sakhr1, each serving as an alternative preprocessor for a state-ofthe-art statistical MT system (Shen et al., 2008). We measure MSA performance on NIST MT-08 (NIST, 2010), and dialect performance on a Levantine dialect web corpus (Zbib et al., 2012b). To improve performance, we apply maximum marginal decoding (Johnson and Goldwater, 2009) (MM) to combine multiple runs of the Lee segmenter, and show that this dramatically reduces the variance and noise in the segmenter output, while yielding an improved segmentation accuracy that exceeds the best published scores for unsupervised segmentation on Arabic Treebank (Naradowsky and Toutanova, 2011). We also show that it yields MT08 BLEU scores that are higher than those obtained with MADA, a leading supervised MSA segmenter. For Levantine, the segmenter increases BLEU score by 18% over the un</context>
<context position="10659" citStr="Zbib et al., 2012" startWordPosition="1667" endWordPosition="1670"> together with the F-score from running MM over that same set of runs. 5 MT Evaluation 5.1 Experimental Design MT System. Our experiments were performed using a state-of-the-art, hierarchical string-todependency-tree MT system, described in Shen et al. (2008). Morphological Analyzers. We compare the Lee segmenter with the supervised MSA segmenter MADA, using its “D3” scheme. We also compare with Sakhr, an intensively-engineered, supervised MSA segmenter which applies multiple NLP technologies to the segmentation problem, and which has given the best results for our MT system in previous work (Zbib et al., 2012a). We also compare with Morfessor. MT experiments. We apply the appropriate segmenter to split words into morphemes, which we then treat as words for alignment and decoding. Following Lee et al. (2011), we segment the test and training sets jointly, estimating separate translation models for each segmenter/dataset combination. Training and Test Corpora. Our “Full MSA” corpus is the NIST MT-08 Constrained Data Track Arabic training corpus (35M total, 336K unique words); our “Small MSA” corpus is a 1.3M-word subset. Both are tested on the MT-08 evaluation set. For dialect, we use a Levantine di</context>
</contexts>
<marker>Zbib, Malchiodi, Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, Callison-Burch, 2012</marker>
<rawString>Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Makhoul, Omar F. Zaidan, and Chris Callison-Burch. 2012b. Machine translation of Arabic dialects. In NAACL 2012: Proceedings of the 2012 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Montreal, Quebec, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>