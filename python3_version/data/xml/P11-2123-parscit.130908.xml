<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015401">
<title confidence="0.988325">
Improving Dependency Parsing with Semantic Classes
</title>
<author confidence="0.94571">
Eneko Agirre*, Kepa Bengoetxea*, Koldo Gojenola*, Joakim Nivre+
</author>
<affiliation confidence="0.694976">
* Department of Computer Languages and Systems, University of the Basque Country
UPV/EHU
+ Department of Linguistics and Philosophy, Uppsala University
</affiliation>
<email confidence="0.987015">
{e.agirre, kepa.bengoetxea, koldo.gojenola}@ehu.es joakim.nivre@lingfil.uu.se
</email>
<sectionHeader confidence="0.997277" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960384615385">
This paper presents the introduction of
WordNet semantic classes in a dependency
parser, obtaining improvements on the full
Penn Treebank for the first time. We tried
different combinations of some basic se-
mantic classes and word sense disambigua-
tion algorithms. Our experiments show that
selecting the adequate combination of se-
mantic features on development data is key
for success. Given the basic nature of the
semantic classes and word sense disam-
biguation algorithms used, we think there is
ample room for future improvements.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997556">
Using semantic information to improve parsing
performance has been an interesting research ave-
nue since the early days of NLP, and several re-
search works have tried to test the intuition that
semantics should help parsing, as can be exempli-
fied by the classical PP attachment experiments
(Ratnaparkhi, 1994). Although there have been
some significant results (see Section 2), this issue
continues to be elusive. In principle, dependency
parsing offers good prospects for experimenting
with word-to-word-semantic relationships.
We present a set of experiments using semantic
classes in dependency parsing of the Penn Tree-
bank (PTB). We extend the tests made in Agirre et
al. (2008), who used different types of semantic
information, obtaining significant improvements in
two constituency parsers, showing how semantic
information helps in constituency parsing.
As our baseline parser, we use MaltParser
(Nivre, 2006). We will evaluate the parser on both
the full PTB (Marcus et al. 1993) and on a sense-
annotated subset of the Brown Corpus portion of
PTB, in order to investigate the upper bound per-
formance of the models given gold-standard sense
information, as in Agirre et al. (2008).
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998482090909091">
Agirre et al. (2008) trained two state-of-the-art sta-
tistical parsers (Charniak, 2000; Bikel, 2004) on
semantically-enriched input, where content words
had been substituted with their semantic classes.
This was done trying to overcome the limitations
of lexicalized approaches to parsing (Magerman,
1995; Collins, 1996; Charniak, 1997; Collins,
2003), where related words, like scissors and knife
cannot be generalized. This simple method allowed
incorporating lexical semantic information into the
parser. They tested the parsers in both a full pars-
ing and a PP attachment context. The experiments
showed that semantic classes gave significant im-
provement relative to the baseline, demonstrating
that a simplistic approach to incorporating lexical
semantics into a parser significantly improves its
performance. This work presented the first results
over both WordNet and the Penn Treebank to show
that semantic processing helps parsing.
Collins (2000) tested a combined parsing/word
sense disambiguation model based in WordNet
which did not obtain improvements in parsing.
Koo et al. (2008) presented a semisupervised
method for training dependency parsers, using
word clusters derived from a large unannotated
corpus as features. They demonstrate the effective-
ness of the approach in a series of dependency
parsing experiments on PTB and the Prague De-
pendency Treebank, showing that the cluster-based
features yield substantial gains in performance
across a wide range of conditions. Suzuki et al.
(2009) also experiment with the same method
combined with semi-supervised learning.
</bodyText>
<page confidence="0.985132">
699
</page>
<note confidence="0.585636">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 699–703,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999778363636364">
Ciaramita and Attardi (2007) show that adding
semantic features extracted by a named entity tag-
ger (such as PERSON or MONEY) improves the
accuracy of a dependency parser, yielding a 5.8%
relative error reduction on the full PTB.
Candito and Seddah (2010) performed experi-
ments in statistical parsing of French, where termi-
nal forms were replaced by more general symbols,
particularly clusters of words obtained through
unsupervised clustering. The results showed that
word clusters had a positive effect.
Regarding dependency parsing of the English
PTB, currently Koo and Collins (2010) and Zhang
and Nivre (2011) hold the best results, with 93.0
and 92.9 unlabeled attachment score, respectively.
Both works used the Penn2Malt constituency-to-
dependency converter, while we will make use of
PennConverter (Johansson and Nugues, 2007).
Apart from these, there have been other attempts
to make use of semantic information in different
frameworks and languages, as in (Hektoen 1997;
Xiong et al. 2005; Fujita et al. 2007).
</bodyText>
<sectionHeader confidence="0.999017" genericHeader="method">
3 Experimental Framework
</sectionHeader>
<bodyText confidence="0.9999042">
In this section we will briefly describe the data-
driven parser used for the experiments (subsection
3.1), followed by the PTB-based datasets (subsec-
tion 3.2). Finally, we will describe the types of se-
mantic representation used in the experiments.
</bodyText>
<subsectionHeader confidence="0.994261">
3.1 MaltParser
</subsectionHeader>
<bodyText confidence="0.999951066666667">
MaltParser (Nivre et al. 2006) is a trainable de-
pendency parser that has been successfully applied
to typologically different languages and treebanks.
We will use one of its standard versions (version
1.4). The parser obtains deterministically a de-
pendency tree in linear-time in a single pass over
the input using two main data structures: a stack of
partially analyzed items and the remaining input
sequence. To determine the best action at each
step, the parser uses history-based feature models
and SVM classifiers. One of the main reasons for
using MaltParser for our experiments is that it eas-
ily allows the introduction of semantic informa-
tion, adding new features, and incorporating them
in the training model.
</bodyText>
<subsectionHeader confidence="0.996162">
3.2 Dataset
</subsectionHeader>
<bodyText confidence="0.999863032258064">
We used two different datasets: the full PTB and
the Semcor/PTB intersection (Agirre et al. 2008).
The full PTB allows for comparison with the state-
of-the-art, and we followed the usual train-test
split. The Semcor/PTB intersection contains both
gold-standard sense and parse tree annotations, and
allows to set an upper bound of the relative impact
of a given semantic representation on parsing. We
use the same train-test split of Agirre et al. (2008),
with a total of 8,669 sentences containing 151,928
words partitioned into 3 sets: 80% training, 10%
development and 10% test data. This dataset is
available on request to the research community.
We will evaluate the parser via Labeled Attach-
ment Score (LAS). We will use Bikel’s random-
ized parsing evaluation comparator to test the
statistical significance of the results using word
sense information, relative to the respective base-
line parser using only standard features.
We used PennConverter (Johansson and
Nugues, 2007) to convert constituent trees in the
Penn Treebank annotation style into dependency
trees. Although in general the results from parsing
Pennconverter’s output are lower than with other
conversions, Johansson and Nugues (2007) claim
that this conversion is better suited for semantic
processing, with a richer structure and a more fine-
grained set of dependency labels. For the experi-
ments, we used the best configuration for English
at the CoNLL 2007 Shared Task on Dependency
Parsing (Nivre et al., 2007) as our baseline.
</bodyText>
<subsectionHeader confidence="0.956289">
3.3 Semantic representation and disambigua-
tion methods
</subsectionHeader>
<bodyText confidence="0.999531315789474">
We will experiment with the range of semantic
representations used in Agirre et al. (2008), all of
which are based on WordNet 2.1. Words in Word-
Net (Fellbaum, 1998) are organized into sets of
synonyms, called synsets (SS). Each synset in turn
belongs to a unique semantic file (SF). There are a
total of 45 SFs (1 for adverbs, 3 for adjectives, 15
for verbs, and 26 for nouns), based on syntactic
and semantic categories. For example, noun se-
mantic files (SF_N) differentiate nouns denoting
acts or actions, and nouns denoting animals,
among others. We experiment with both full syn-
sets and SFs as instances of fine-grained and
coarse-grained semantic representation, respec-
tively. As an example of the difference in these
two representations, knife in its tool sense is in the
EDGE TOOL USED AS A CUTTING
INSTRUMENT singleton synset, and also in the
ARTIFACT SF along with thousands of other
</bodyText>
<page confidence="0.972202">
700
</page>
<bodyText confidence="0.99965934375">
words including cutter. Note that these are the two
extremes of semantic granularity in WordNet.
As a hybrid representation, we also tested the ef-
fect of merging words with their corresponding SF
(e.g. knife+ARTIFACT). This is a form of seman-
tic specialization rather than generalization, and
allows the parser to discriminate between the dif-
ferent senses of each word, but not generalize
across words. For each of these three semantic rep-
resentations, we experimented with using each of:
(1) all open-class POSs (nouns, verbs, adjectives
and adverbs), (2) nouns only, and (3) verbs only.
There are thus a total of 9 combinations of repre-
sentation type and target POS: SS (synset), SS_N
(noun synsets), SS_V (verb synsets), SF (semantic
file), SF_N (noun semantic files), SF_V (verb se-
mantic files), WSF (wordform+SF), WSF_N
(wordform+SF for nouns) and WSF_V (for verbs).
For a given semantic representation, we need
some form of WSD to determine the semantics of
each token occurrence of a target word. We ex-
perimented with three options: a) gold-standard
(GOLD) annotations from SemCor, which gives
the upper bound performance of the semantic rep-
resentation, b) first Sense (1ST), where all token
instances of a given word are tagged with their
most frequent sense in WordNet, and c) automatic
Sense Ranking (ASR) which uses the sense re-
turned by an unsupervised system based on an in-
dependent corpus (McCarthy et al. 2004). For the
full Penn Treebank experiments, we only had ac-
cess to the first sense, taken from Wordnet 1.7.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999944142857143">
In the following two subsections, we will first pre-
sent the results in the SemCor/PTB intersection,
with the option of using gold, 1st sense and auto-
matic sense information (subsection 4.1) and the
next subsection (4.2) will show the results on the
full PTB, using 1st sense information. All results
are shown as labelled attachment score (LAS).
</bodyText>
<subsectionHeader confidence="0.989354">
4.1 Semcor/PTB (GOLD/1ST/ASR)
</subsectionHeader>
<bodyText confidence="0.998504">
We conducted a series of experiments testing:
</bodyText>
<listItem confidence="0.977842">
• Each individual semantic feature, which
gives 9 possibilities, also testing different
learning configurations for each one.
• Combinations of semantic features, for in-
stance, SF+SS_N+WSF would combine the
</listItem>
<table confidence="0.99987775">
System LAS
Baseline 81.10
SS 81.18 +0.08
SS_N 81.40 +0.30
Gold SS_V *81.58 +0.48
SF **82.05 +0.95
SF_N 81.51 +0.41
SF_V 81.51 +0.41
WSF 81.51 +0.41
WSF_N 81.43 +0.33
WSF_V *81.51 +0.41
SF+SF_N+SF_V+SS+WSF_N *81.74 +0.64
SS 81.30 +0.20
SS_N *81.56 +0.46
ASR SS_V *81.49 +0.39
SF 81.00 -0.10
SF_N 80.97 -0.13
SF_V **81.66 +0.56
WSF 81.32 +0.22
WSF_N *81.62 +0.52
WSF_V **81.72 +0.62
SF_V+SS_V 81.41 +0.31
SS 81.40 +0.30
SS_N 81.39 +0.29
1ST SS_V *81.48 +0.38
SF *81.59 +0.49
SF_N 81.38 +0.28
SF_V *81.52 +0.42
WSF *81.57 +0.46
WSF_N 81.40 +0.30
WSF_V 81.42 +0.32
SF+SS_V+WSF_N **81.92 +0.81
</table>
<tableCaption confidence="0.81077675">
Table 1. Evaluation results on the test set for the
Semcor-Penn intersection. Individual semantic
features and best combination.
(**: statistically significant, p &lt; 0.005; *: p &lt; 0.05)
</tableCaption>
<bodyText confidence="0.996538230769231">
semantic file with noun synsets and word-
form+semantic file.
Although there were hundreds of combinations,
we took the best combination of semantic features
on the development set for the final test. For that
reason, the table only presents 10 results for each
disambiguation method, 9 for the individual fea-
tures and one for the best combination.
Table 1 presents the results obtained for each of
the disambiguation methods (gold standard sense
information, 1st sense, and automatic sense rank-
ing) and individual semantic feature. In all cases
except two, the use of semantic classes is benefi-
</bodyText>
<page confidence="0.996537">
701
</page>
<table confidence="0.988443090909091">
System LAS
SS *86.53 +0.26
SS_N 86.33 +0.06
1ST SS_V *86.48 +0.21
SF **86.63 +0.36
SF_N *86.56 +0.29
SF_V 86.34 +0.07
WSF *86.50 +0.23
WSF_N 86.25 -0.02
WSF_V *86.51 +0.24
SF+SS_V+WSF_N *86.60 +0.33
</table>
<tableCaption confidence="0.997924">
Table 1. Evaluation results (LAS) on the test
</tableCaption>
<bodyText confidence="0.950271136363636">
set for the full PTB. Individual features and
best combination.
(**: statistically, p &lt; 0.005; *: p &lt; 0.05)
cial albeit small. Regarding individual features, the
SF feature using GOLD senses gives the best im-
provement. However, GOLD does not seem to
clearly improve over 1ST and ASR on the rest of
the features. Comparing the automatically obtained
classes, 1ST and ASR, there is no evident clue
about one of them being superior to the other.
Regarding the best combination as selected in
the training data, each WSD method yields a dif-
ferent combination, with best results for 1ST. The
improvement is statistically significant for both
1ST and GOLD. In general, the results in Table 1
do not show any winning feature across all WSD
algorithms. The best results are obtained when us-
ing the first sense heuristic, but the difference is
not statistically significant. This shows that perfect
WSD is not needed to obtain improvements, but it
also shows that we reached the upperbound of our
generalization and learning method.
</bodyText>
<subsectionHeader confidence="0.999837">
4.2 Penn Treebank and 1st sense
</subsectionHeader>
<bodyText confidence="0.999962708333333">
We only had 1st sense information available for
the full PTB. We tested MaltParser on the best
configuration obtained for the reduced Sem-
cor/PTB on the full treebank, taking sections 2-21
for training and section 23 for the final test. Table
2 presents the results, showing that several of the
individual features and the best combination give
significant improvements. To our knowledge, this
is the first time that WordNet semantic classes help
to obtain improvements on the full Penn Treebank.
It is interesting to mention that, although not
shown on the tables, using lemmatization to assign
semantic classes to wordforms gave a slight in-
crease for all the tests (0.1 absolute point approxi-
mately), as it helped to avoid data sparseness. We
applied Schmid’s (1994) TreeTagger. This can be
seen as an argument in favour of performing mor-
phological analysis, an aspect that is many times
neglected when processing morphologically poor
languages as English.
We also did some preliminary experiments us-
ing Koo et al.’s (2008) word clusters, both inde-
pendently and also combined with the WordNet-
based features, without noticeable improvements.
</bodyText>
<sectionHeader confidence="0.998476" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.998687666666667">
We tested the inclusion of several types of seman-
tic information, in the form of WordNet semantic
classes in a dependency parser, showing that:
</bodyText>
<listItem confidence="0.990537">
• Semantic information gives an improvement
on a transition-based deterministic depend-
ency parsing.
• Feature combinations give an improvement
over using a single feature. Agirre et al.
(2008) used a simple method of substituting
</listItem>
<bodyText confidence="0.97005588">
wordforms with semantic information,
which only allowed using a single semantic
feature. MaltParser allows the combination
of several semantic features together with
other features such as wordform, lemma or
part of speech. Although tables 1 and 2 only
show the best combination for each type of
semantic information, this can be appreci-
ated on GOLD and 1ST in Table 1. Due to
space reasons, we only have showed the best
combination, but we can say that in general
combining features gives significant in-
creases over using a single semantic feature.
• The present work presents a statistically sig-
nificant improvement for the full treebank
using WordNet-based semantic information
for the first time. Our results extend those of
Agirre et al. (2008), which showed im-
provements on a subset of the PTB.
Given the basic nature of the semantic classes
and WSD algorithms, we think there is room for
future improvements, incorporating new kinds of
semantic information, such as WordNet base con-
cepts, Wikipedia concepts, or similarity measures.
Baseline 86.27
</bodyText>
<page confidence="0.994053">
702
</page>
<sectionHeader confidence="0.996192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999793052631579">
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and PP attachment perform-
ance with sense information. In Proceedings of ACL-
08: HLT, pages 317–325, Columbus, Ohio.
Daniel M. Bikel. 2004. Intricacies of Collins’ parsing
model. Computational Linguistics, 30(4):479–511.
Candito, M. and D. Seddah. 2010. Parsing word clus-
ters. In Proceedings of the NAACL HLT 2010 First
Workshop on Statistical Parsing of Morphologically-
Rich Language, Los Angeles, USA.
M. Ciaramita and G. Attardi. 2007. Dependency Parsing
with Second-Order Feature Maps and Annotated Se-
mantic Information, In Proceedings of the 10th In-
ternational Conference on Parsing Technology.
Eugene Charniak. 1997. Statistical parsing with a con-
text-free grammar and word statistics. In Proc. of the
15th Annual Conference on Artificial Intelligence
(AAAI-97), pages 598–603, Stanford, USA.
Eugene Charniak. 2000. A maximum entropy-based
parser. In Proc. of the 1st Annual Meeting of the
North American Chapter of Association for Compu-
tational Linguistics (NAACL2000), Seattle, USA.
Michael J. Collins. 1996. A new statistical parser based
on lexical dependencies. In Proc. of the 34th Annual
Meeting of the ACL, pages 184–91, USA.
Michael Collins. 2000. A Statistical Model for Parsing
and Word-Sense Disambiguation. Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589–637.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge.
Sanae Fujita, Francis Bond, Stephan Oepen, and Taka-
aki Tanaka. 2007. Exploiting semantic information
for HPSG parse selection. In Proc. of the ACL 2007
Workshop on Deep Linguistic Processing.
Richard Johansson and Pierre Nugues. 2007. Extended
Constituent-to-dependency Conversion for English.
In Proceedings of NODALIDA 2007, Tartu, Estonia.
Erik Hektoen. 1997. Probabilistic parse selection based
on semantic cooccurrences. In Proc. of the 5th Inter-
national Workshop on Parsing Technologies.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08, pages 595–603, USA.
Terry Koo, and Michael Collins. 2008. Efficient Third-
order Dependency Parsers. In Proceedings of ACL-
2010, pages 1–11, Uppsala, Sweden.
Shari Landes, Claudia Leacock, and Randee I. Tengi.
1998. Building semantic concordances. In Christiane
Fellbaum, editor, WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, USA.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. of the 33rd Annual
Meeting of the ACL, pages 276–83, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computational
Linguistics, 19(2):313–30.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in
untagged text. In Proc. of the 42nd Annual Meeting
of the ACL, pages 280–7, Barcelona, Spain.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Text, Speech and Language Technology series,
Springer. 2006, XI, ISBN: 978-1-4020-4888-3.
Joakim Nivre, Johan Hall, Sandra Kübler, Ryan
McDonald, Jens Nilsson, Sebastian Riedel and
Deniz Yuret. 2007b. The CoNLL 2007 Shared Task
on Dependency Parsing. Proceedings of EMNLP-
CoNLL. Prague, Czech Republic.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In HLT ’94: Proceedings of the
Workshop on Human Language Technology, USA.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing. September 1994
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Mi-
chael Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for De-
pendency Parsing. In Proceedings of EMNLP, pages
551–560. Association for Computational Linguistics.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese
Treebank with semantic knowledge. In Proc. of the
2nd International Joint Conference on Natural Lan-
guage Processing (IJCNLP-05), Korea.
Yue Zhang, and Joakim Nivre. 2011. Transition-Based
Parsing with Rich Non-Local Features. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics.
</reference>
<page confidence="0.998994">
703
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.376006">
<title confidence="0.999956">Improving Dependency Parsing with Semantic Classes</title>
<author confidence="0.8835225">Kepa Koldo Joakim of Computer Languages</author>
<author confidence="0.8835225">University of the Basque Systems</author>
<affiliation confidence="0.997214">of Linguistics and Philosophy, Uppsala University</affiliation>
<email confidence="0.821599">e.agirre@ehu.esjoakim.nivre@lingfil.uu.se</email>
<email confidence="0.821599">kepa.bengoetxea@ehu.esjoakim.nivre@lingfil.uu.se</email>
<email confidence="0.821599">koldo.gojenola@ehu.esjoakim.nivre@lingfil.uu.se</email>
<abstract confidence="0.583486"></abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Timothy Baldwin</author>
<author>David Martinez</author>
</authors>
<title>Improving parsing and PP attachment performance with sense information.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>317--325</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1582" citStr="Agirre et al. (2008)" startWordPosition="226" endWordPosition="229">s been an interesting research avenue since the early days of NLP, and several research works have tried to test the intuition that semantics should help parsing, as can be exemplified by the classical PP attachment experiments (Ratnaparkhi, 1994). Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in Agirre et al. (2008), who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. (2008). 2 Related Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charnia</context>
<context position="6029" citStr="Agirre et al. 2008" startWordPosition="897" endWordPosition="900">e parser obtains deterministically a dependency tree in linear-time in a single pass over the input using two main data structures: a stack of partially analyzed items and the remaining input sequence. To determine the best action at each step, the parser uses history-based feature models and SVM classifiers. One of the main reasons for using MaltParser for our experiments is that it easily allows the introduction of semantic information, adding new features, and incorporating them in the training model. 3.2 Dataset We used two different datasets: the full PTB and the Semcor/PTB intersection (Agirre et al. 2008). The full PTB allows for comparison with the stateof-the-art, and we followed the usual train-test split. The Semcor/PTB intersection contains both gold-standard sense and parse tree annotations, and allows to set an upper bound of the relative impact of a given semantic representation on parsing. We use the same train-test split of Agirre et al. (2008), with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data. This dataset is available on request to the research community. We will evaluate the parser via Labeled Attachm</context>
<context position="7579" citStr="Agirre et al. (2008)" startWordPosition="1139" endWordPosition="1142">ank annotation style into dependency trees. Although in general the results from parsing Pennconverter’s output are lower than with other conversions, Johansson and Nugues (2007) claim that this conversion is better suited for semantic processing, with a richer structure and a more finegrained set of dependency labels. For the experiments, we used the best configuration for English at the CoNLL 2007 Shared Task on Dependency Parsing (Nivre et al., 2007) as our baseline. 3.3 Semantic representation and disambiguation methods We will experiment with the range of semantic representations used in Agirre et al. (2008), all of which are based on WordNet 2.1. Words in WordNet (Fellbaum, 1998) are organized into sets of synonyms, called synsets (SS). Each synset in turn belongs to a unique semantic file (SF). There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories. For example, noun semantic files (SF_N) differentiate nouns denoting acts or actions, and nouns denoting animals, among others. We experiment with both full synsets and SFs as instances of fine-grained and coarse-grained semantic representation, respectively. As an ex</context>
<context position="14719" citStr="Agirre et al. (2008)" startWordPosition="2300" endWordPosition="2303">at is many times neglected when processing morphologically poor languages as English. We also did some preliminary experiments using Koo et al.’s (2008) word clusters, both independently and also combined with the WordNetbased features, without noticeable improvements. 5 Conclusions We tested the inclusion of several types of semantic information, in the form of WordNet semantic classes in a dependency parser, showing that: • Semantic information gives an improvement on a transition-based deterministic dependency parsing. • Feature combinations give an improvement over using a single feature. Agirre et al. (2008) used a simple method of substituting wordforms with semantic information, which only allowed using a single semantic feature. MaltParser allows the combination of several semantic features together with other features such as wordform, lemma or part of speech. Although tables 1 and 2 only show the best combination for each type of semantic information, this can be appreciated on GOLD and 1ST in Table 1. Due to space reasons, we only have showed the best combination, but we can say that in general combining features gives significant increases over using a single semantic feature. • The presen</context>
</contexts>
<marker>Agirre, Baldwin, Martinez, 2008</marker>
<rawString>Eneko Agirre, Timothy Baldwin, and David Martinez. 2008. Improving parsing and PP attachment performance with sense information. In Proceedings of ACL08: HLT, pages 317–325, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="2203" citStr="Bikel, 2004" startWordPosition="325" endWordPosition="326">d different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. (2008). 2 Related Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating th</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>D Seddah</author>
</authors>
<title>Parsing word clusters.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of MorphologicallyRich Language,</booktitle>
<location>Los Angeles, USA.</location>
<contexts>
<context position="4148" citStr="Candito and Seddah (2010)" startWordPosition="605" endWordPosition="608">substantial gains in performance across a wide range of conditions. Suzuki et al. (2009) also experiment with the same method combined with semi-supervised learning. 699 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 699–703, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Ciaramita and Attardi (2007) show that adding semantic features extracted by a named entity tagger (such as PERSON or MONEY) improves the accuracy of a dependency parser, yielding a 5.8% relative error reduction on the full PTB. Candito and Seddah (2010) performed experiments in statistical parsing of French, where terminal forms were replaced by more general symbols, particularly clusters of words obtained through unsupervised clustering. The results showed that word clusters had a positive effect. Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. Both works used the Penn2Malt constituency-todependency converter, while we will make use of PennConverter (Johansson and Nugues, 2007). Apart from these, t</context>
</contexts>
<marker>Candito, Seddah, 2010</marker>
<rawString>Candito, M. and D. Seddah. 2010. Parsing word clusters. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of MorphologicallyRich Language, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>G Attardi</author>
</authors>
<title>Dependency Parsing with Second-Order Feature Maps and Annotated Semantic Information,</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technology.</booktitle>
<contexts>
<context position="3922" citStr="Ciaramita and Attardi (2007)" startWordPosition="567" endWordPosition="570">om a large unannotated corpus as features. They demonstrate the effectiveness of the approach in a series of dependency parsing experiments on PTB and the Prague Dependency Treebank, showing that the cluster-based features yield substantial gains in performance across a wide range of conditions. Suzuki et al. (2009) also experiment with the same method combined with semi-supervised learning. 699 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 699–703, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Ciaramita and Attardi (2007) show that adding semantic features extracted by a named entity tagger (such as PERSON or MONEY) improves the accuracy of a dependency parser, yielding a 5.8% relative error reduction on the full PTB. Candito and Seddah (2010) performed experiments in statistical parsing of French, where terminal forms were replaced by more general symbols, particularly clusters of words obtained through unsupervised clustering. The results showed that word clusters had a positive effect. Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best </context>
</contexts>
<marker>Ciaramita, Attardi, 2007</marker>
<rawString>M. Ciaramita and G. Attardi. 2007. Dependency Parsing with Second-Order Feature Maps and Annotated Semantic Information, In Proceedings of the 10th International Conference on Parsing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. of the 15th Annual Conference on Artificial Intelligence (AAAI-97),</booktitle>
<pages>598--603</pages>
<location>Stanford, USA.</location>
<contexts>
<context position="2438" citStr="Charniak, 1997" startWordPosition="357" endWordPosition="358">ll evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. (2008). 2 Related Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps pars</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proc. of the 15th Annual Conference on Artificial Intelligence (AAAI-97), pages 598–603, Stanford, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropy-based parser.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st Annual Meeting of the North American Chapter of Association for Computational Linguistics (NAACL2000),</booktitle>
<location>Seattle, USA.</location>
<contexts>
<context position="2189" citStr="Charniak, 2000" startWordPosition="323" endWordPosition="324"> (2008), who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. (2008). 2 Related Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, de</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropy-based parser. In Proc. of the 1st Annual Meeting of the North American Chapter of Association for Computational Linguistics (NAACL2000), Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
</authors>
<title>A new statistical parser based on lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the ACL,</booktitle>
<pages>184--91</pages>
<publisher>USA.</publisher>
<contexts>
<context position="2422" citStr="Collins, 1996" startWordPosition="355" endWordPosition="356">e, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. (2008). 2 Related Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic proce</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael J. Collins. 1996. A new statistical parser based on lexical dependencies. In Proc. of the 34th Annual Meeting of the ACL, pages 184–91, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A Statistical Model for Parsing and Word-Sense Disambiguation.</title>
<date>2000</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="3057" citStr="Collins (2000)" startWordPosition="447" endWordPosition="448">lins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing. Collins (2000) tested a combined parsing/word sense disambiguation model based in WordNet which did not obtain improvements in parsing. Koo et al. (2008) presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features. They demonstrate the effectiveness of the approach in a series of dependency parsing experiments on PTB and the Prague Dependency Treebank, showing that the cluster-based features yield substantial gains in performance across a wide range of conditions. Suzuki et al. (2009) also experiment with the same method combined</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. A Statistical Model for Parsing and Word-Sense Disambiguation. Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="2454" citStr="Collins, 2003" startWordPosition="359" endWordPosition="360">parser on both the full PTB (Marcus et al. 1993) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. (2008). 2 Related Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing. Collins (20</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanae Fujita</author>
<author>Francis Bond</author>
<author>Stephan Oepen</author>
<author>Takaaki Tanaka</author>
</authors>
<title>Exploiting semantic information for HPSG parse selection.</title>
<date>2007</date>
<booktitle>In Proc. of the ACL 2007 Workshop on Deep Linguistic Processing.</booktitle>
<contexts>
<context position="4912" citStr="Fujita et al. 2007" startWordPosition="720" endWordPosition="723">ds obtained through unsupervised clustering. The results showed that word clusters had a positive effect. Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. Both works used the Penn2Malt constituency-todependency converter, while we will make use of PennConverter (Johansson and Nugues, 2007). Apart from these, there have been other attempts to make use of semantic information in different frameworks and languages, as in (Hektoen 1997; Xiong et al. 2005; Fujita et al. 2007). 3 Experimental Framework In this section we will briefly describe the datadriven parser used for the experiments (subsection 3.1), followed by the PTB-based datasets (subsection 3.2). Finally, we will describe the types of semantic representation used in the experiments. 3.1 MaltParser MaltParser (Nivre et al. 2006) is a trainable dependency parser that has been successfully applied to typologically different languages and treebanks. We will use one of its standard versions (version 1.4). The parser obtains deterministically a dependency tree in linear-time in a single pass over the input us</context>
</contexts>
<marker>Fujita, Bond, Oepen, Tanaka, 2007</marker>
<rawString>Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki Tanaka. 2007. Exploiting semantic information for HPSG parse selection. In Proc. of the ACL 2007 Workshop on Deep Linguistic Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended Constituent-to-dependency Conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA 2007,</booktitle>
<location>Tartu, Estonia.</location>
<contexts>
<context position="4727" citStr="Johansson and Nugues, 2007" startWordPosition="689" endWordPosition="692">ction on the full PTB. Candito and Seddah (2010) performed experiments in statistical parsing of French, where terminal forms were replaced by more general symbols, particularly clusters of words obtained through unsupervised clustering. The results showed that word clusters had a positive effect. Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. Both works used the Penn2Malt constituency-todependency converter, while we will make use of PennConverter (Johansson and Nugues, 2007). Apart from these, there have been other attempts to make use of semantic information in different frameworks and languages, as in (Hektoen 1997; Xiong et al. 2005; Fujita et al. 2007). 3 Experimental Framework In this section we will briefly describe the datadriven parser used for the experiments (subsection 3.1), followed by the PTB-based datasets (subsection 3.2). Finally, we will describe the types of semantic representation used in the experiments. 3.1 MaltParser MaltParser (Nivre et al. 2006) is a trainable dependency parser that has been successfully applied to typologically different </context>
<context position="6912" citStr="Johansson and Nugues, 2007" startWordPosition="1035" endWordPosition="1038">a given semantic representation on parsing. We use the same train-test split of Agirre et al. (2008), with a total of 8,669 sentences containing 151,928 words partitioned into 3 sets: 80% training, 10% development and 10% test data. This dataset is available on request to the research community. We will evaluate the parser via Labeled Attachment Score (LAS). We will use Bikel’s randomized parsing evaluation comparator to test the statistical significance of the results using word sense information, relative to the respective baseline parser using only standard features. We used PennConverter (Johansson and Nugues, 2007) to convert constituent trees in the Penn Treebank annotation style into dependency trees. Although in general the results from parsing Pennconverter’s output are lower than with other conversions, Johansson and Nugues (2007) claim that this conversion is better suited for semantic processing, with a richer structure and a more finegrained set of dependency labels. For the experiments, we used the best configuration for English at the CoNLL 2007 Shared Task on Dependency Parsing (Nivre et al., 2007) as our baseline. 3.3 Semantic representation and disambiguation methods We will experiment with</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended Constituent-to-dependency Conversion for English. In Proceedings of NODALIDA 2007, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Hektoen</author>
</authors>
<title>Probabilistic parse selection based on semantic cooccurrences.</title>
<date>1997</date>
<booktitle>In Proc. of the 5th International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="4872" citStr="Hektoen 1997" startWordPosition="714" endWordPosition="715">ols, particularly clusters of words obtained through unsupervised clustering. The results showed that word clusters had a positive effect. Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. Both works used the Penn2Malt constituency-todependency converter, while we will make use of PennConverter (Johansson and Nugues, 2007). Apart from these, there have been other attempts to make use of semantic information in different frameworks and languages, as in (Hektoen 1997; Xiong et al. 2005; Fujita et al. 2007). 3 Experimental Framework In this section we will briefly describe the datadriven parser used for the experiments (subsection 3.1), followed by the PTB-based datasets (subsection 3.2). Finally, we will describe the types of semantic representation used in the experiments. 3.1 MaltParser MaltParser (Nivre et al. 2006) is a trainable dependency parser that has been successfully applied to typologically different languages and treebanks. We will use one of its standard versions (version 1.4). The parser obtains deterministically a dependency tree in linear</context>
</contexts>
<marker>Hektoen, 1997</marker>
<rawString>Erik Hektoen. 1997. Probabilistic parse selection based on semantic cooccurrences. In Proc. of the 5th International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08,</booktitle>
<pages>595--603</pages>
<publisher>USA.</publisher>
<contexts>
<context position="3196" citStr="Koo et al. (2008)" startWordPosition="466" endWordPosition="469">ic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that semantic processing helps parsing. Collins (2000) tested a combined parsing/word sense disambiguation model based in WordNet which did not obtain improvements in parsing. Koo et al. (2008) presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features. They demonstrate the effectiveness of the approach in a series of dependency parsing experiments on PTB and the Prague Dependency Treebank, showing that the cluster-based features yield substantial gains in performance across a wide range of conditions. Suzuki et al. (2009) also experiment with the same method combined with semi-supervised learning. 699 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pa</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL-08, pages 595–603, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient Thirdorder Dependency Parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL2010,</booktitle>
<pages>1--11</pages>
<location>Uppsala,</location>
<marker>Koo, Collins, 2008</marker>
<rawString>Terry Koo, and Michael Collins. 2008. Efficient Thirdorder Dependency Parsers. In Proceedings of ACL2010, pages 1–11, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Randee I Tengi</author>
</authors>
<title>Building semantic concordances.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, USA.</location>
<marker>Landes, Leacock, Tengi, 1998</marker>
<rawString>Shari Landes, Claudia Leacock, and Randee I. Tengi. 1998. Building semantic concordances. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database. MIT Press, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proc. of the 33rd Annual Meeting of the ACL,</booktitle>
<pages>276--83</pages>
<publisher>USA.</publisher>
<contexts>
<context position="2407" citStr="Magerman, 1995" startWordPosition="353" endWordPosition="354">MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. (2008). 2 Related Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scissors and knife cannot be generalized. This simple method allowed incorporating lexical semantic information into the parser. They tested the parsers in both a full parsing and a PP attachment context. The experiments showed that semantic classes gave significant improvement relative to the baseline, demonstrating that a simplistic approach to incorporating lexical semantics into a parser significantly improves its performance. This work presented the first results over both WordNet and the Penn Treebank to show that</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proc. of the 33rd Annual Meeting of the ACL, pages 276–83, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1888" citStr="Marcus et al. 1993" startWordPosition="271" endWordPosition="274">tion 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in Agirre et al. (2008), who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. (2008). 2 Related Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins, 1996; Charniak, 1997; Collins, 2003), where related words, like scisso</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>280--7</pages>
<location>Barcelona,</location>
<contexts>
<context position="9811" citStr="McCarthy et al. 2004" startWordPosition="1508" endWordPosition="1511">(wordform+SF), WSF_N (wordform+SF for nouns) and WSF_V (for verbs). For a given semantic representation, we need some form of WSD to determine the semantics of each token occurrence of a target word. We experimented with three options: a) gold-standard (GOLD) annotations from SemCor, which gives the upper bound performance of the semantic representation, b) first Sense (1ST), where all token instances of a given word are tagged with their most frequent sense in WordNet, and c) automatic Sense Ranking (ASR) which uses the sense returned by an unsupervised system based on an independent corpus (McCarthy et al. 2004). For the full Penn Treebank experiments, we only had access to the first sense, taken from Wordnet 1.7. 4 Results In the following two subsections, we will first present the results in the SemCor/PTB intersection, with the option of using gold, 1st sense and automatic sense information (subsection 4.1) and the next subsection (4.2) will show the results on the full PTB, using 1st sense information. All results are shown as labelled attachment score (LAS). 4.1 Semcor/PTB (GOLD/1ST/ASR) We conducted a series of experiments testing: • Each individual semantic feature, which gives 9 possibilities</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant senses in untagged text. In Proc. of the 42nd Annual Meeting of the ACL, pages 280–7, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing. Text, Speech and Language Technology series,</title>
<date>2006</date>
<pages>978--1</pages>
<publisher>Springer.</publisher>
<location>XI, ISBN:</location>
<contexts>
<context position="1817" citStr="Nivre, 2006" startWordPosition="259" endWordPosition="260">994). Although there have been some significant results (see Section 2), this issue continues to be elusive. In principle, dependency parsing offers good prospects for experimenting with word-to-word-semantic relationships. We present a set of experiments using semantic classes in dependency parsing of the Penn Treebank (PTB). We extend the tests made in Agirre et al. (2008), who used different types of semantic information, obtaining significant improvements in two constituency parsers, showing how semantic information helps in constituency parsing. As our baseline parser, we use MaltParser (Nivre, 2006). We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a senseannotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. (2008). 2 Related Work Agirre et al. (2008) trained two state-of-the-art statistical parsers (Charniak, 2000; Bikel, 2004) on semantically-enriched input, where content words had been substituted with their semantic classes. This was done trying to overcome the limitations of lexicalized approaches to parsing (Magerman, 1995; Collins,</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Inductive Dependency Parsing. Text, Speech and Language Technology series, Springer. 2006, XI, ISBN: 978-1-4020-4888-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra Kübler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>Shared Task on Dependency Parsing. Proceedings of EMNLPCoNLL.</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7416" citStr="Nivre et al., 2007" startWordPosition="1114" endWordPosition="1117"> to the respective baseline parser using only standard features. We used PennConverter (Johansson and Nugues, 2007) to convert constituent trees in the Penn Treebank annotation style into dependency trees. Although in general the results from parsing Pennconverter’s output are lower than with other conversions, Johansson and Nugues (2007) claim that this conversion is better suited for semantic processing, with a richer structure and a more finegrained set of dependency labels. For the experiments, we used the best configuration for English at the CoNLL 2007 Shared Task on Dependency Parsing (Nivre et al., 2007) as our baseline. 3.3 Semantic representation and disambiguation methods We will experiment with the range of semantic representations used in Agirre et al. (2008), all of which are based on WordNet 2.1. Words in WordNet (Fellbaum, 1998) are organized into sets of synonyms, called synsets (SS). Each synset in turn belongs to a unique semantic file (SF). There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories. For example, noun semantic files (SF_N) differentiate nouns denoting acts or actions, and nouns denoting </context>
</contexts>
<marker>Nivre, Hall, Kübler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra Kübler, Ryan McDonald, Jens Nilsson, Sebastian Riedel and Deniz Yuret. 2007b. The CoNLL 2007 Shared Task on Dependency Parsing. Proceedings of EMNLPCoNLL. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Jeff Reynar</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>In HLT ’94: Proceedings of the Workshop on Human Language Technology,</booktitle>
<location>USA.</location>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In HLT ’94: Proceedings of the Workshop on Human Language Technology, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing.</booktitle>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of International Conference on New Methods in Language Processing. September 1994</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An Empirical Study of Semisupervised Structured Conditional Models for Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>551--560</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3611" citStr="Suzuki et al. (2009)" startWordPosition="528" endWordPosition="531">ank to show that semantic processing helps parsing. Collins (2000) tested a combined parsing/word sense disambiguation model based in WordNet which did not obtain improvements in parsing. Koo et al. (2008) presented a semisupervised method for training dependency parsers, using word clusters derived from a large unannotated corpus as features. They demonstrate the effectiveness of the approach in a series of dependency parsing experiments on PTB and the Prague Dependency Treebank, showing that the cluster-based features yield substantial gains in performance across a wide range of conditions. Suzuki et al. (2009) also experiment with the same method combined with semi-supervised learning. 699 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 699–703, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Ciaramita and Attardi (2007) show that adding semantic features extracted by a named entity tagger (such as PERSON or MONEY) improves the accuracy of a dependency parser, yielding a 5.8% relative error reduction on the full PTB. Candito and Seddah (2010) performed experiments in statistical parsing of French, where </context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An Empirical Study of Semisupervised Structured Conditional Models for Dependency Parsing. In Proceedings of EMNLP, pages 551–560. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
<author>Yueliang Qian</author>
</authors>
<title>Parsing the Penn Chinese Treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proc. of the 2nd International Joint Conference on Natural Language Processing (IJCNLP-05),</booktitle>
<contexts>
<context position="4891" citStr="Xiong et al. 2005" startWordPosition="716" endWordPosition="719">rly clusters of words obtained through unsupervised clustering. The results showed that word clusters had a positive effect. Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. Both works used the Penn2Malt constituency-todependency converter, while we will make use of PennConverter (Johansson and Nugues, 2007). Apart from these, there have been other attempts to make use of semantic information in different frameworks and languages, as in (Hektoen 1997; Xiong et al. 2005; Fujita et al. 2007). 3 Experimental Framework In this section we will briefly describe the datadriven parser used for the experiments (subsection 3.1), followed by the PTB-based datasets (subsection 3.2). Finally, we will describe the types of semantic representation used in the experiments. 3.1 MaltParser MaltParser (Nivre et al. 2006) is a trainable dependency parser that has been successfully applied to typologically different languages and treebanks. We will use one of its standard versions (version 1.4). The parser obtains deterministically a dependency tree in linear-time in a single p</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the Penn Chinese Treebank with semantic knowledge. In Proc. of the 2nd International Joint Conference on Natural Language Processing (IJCNLP-05), Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-Based Parsing with Rich Non-Local Features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4507" citStr="Zhang and Nivre (2011)" startWordPosition="658" endWordPosition="661">guistics Ciaramita and Attardi (2007) show that adding semantic features extracted by a named entity tagger (such as PERSON or MONEY) improves the accuracy of a dependency parser, yielding a 5.8% relative error reduction on the full PTB. Candito and Seddah (2010) performed experiments in statistical parsing of French, where terminal forms were replaced by more general symbols, particularly clusters of words obtained through unsupervised clustering. The results showed that word clusters had a positive effect. Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. Both works used the Penn2Malt constituency-todependency converter, while we will make use of PennConverter (Johansson and Nugues, 2007). Apart from these, there have been other attempts to make use of semantic information in different frameworks and languages, as in (Hektoen 1997; Xiong et al. 2005; Fujita et al. 2007). 3 Experimental Framework In this section we will briefly describe the datadriven parser used for the experiments (subsection 3.1), followed by the PTB-based datasets (subsection 3.2). Finally, </context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang, and Joakim Nivre. 2011. Transition-Based Parsing with Rich Non-Local Features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>