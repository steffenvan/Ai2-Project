<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002896">
<title confidence="0.996404">
Cascaded Classifiers for Confidence-Based Chemical Named Entity
Recognition
</title>
<author confidence="0.99574">
Peter Corbett
</author>
<affiliation confidence="0.997958333333333">
Unilever Centre For
Molecular Science Informatics
Chemical Laboratory University Of Cambridge
</affiliation>
<address confidence="0.95642">
C132 1EW, UK
</address>
<email confidence="0.997112">
ptc24@cam.ac.uk
</email>
<author confidence="0.976709">
Ann Copestake
</author>
<affiliation confidence="0.967554">
Computer Laboratory
University Of Cambridge
</affiliation>
<address confidence="0.936215">
C133 0FD, UK
</address>
<email confidence="0.998463">
aac10@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99608375">
Chemical named entities represent an impor-
tant facet of biomedical text. We have de-
veloped a system to use character-based n-
grams, Maximum Entropy Markov Models
and rescoring to recognise chemical names
and other such entities, and to make confi-
dence estimates for the extracted entities. An
adjustable threshold allows the system to be
tuned to high precision or high recall. At a
threshold set for balanced precision and recall,
we were able to extract named entities at an
F score of 80.7% from chemistry papers and
83.2% from PubMed abstracts. Furthermore,
we were able to achieve 57.6% and 60.3% re-
call at 95% precision, and 58.9% and 49.1%
precision at 90% recall. These results show
that chemical named entities can be extracted
with good performance, and that the proper-
ties of the extraction can be tuned to suit the
demands of the task.
</bodyText>
<sectionHeader confidence="0.999124" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990425">
Systems for the recognition of biomedical named
entities have traditionally worked on a ‘first-best’
approach, where all of the entities recognised have
equal status, and precision and recall are given
roughly equal importance. This does not reflect that
fact that precision is of greater importance for some
applications, and recall is the key for others. Fur-
thermore, knowing the confidence1 with which the
</bodyText>
<footnote confidence="0.8851">
1In this paper, we use “confidence” to refer to a system’s
estimate of the probability that a potential named entity is a cor-
rect named entity.
</footnote>
<bodyText confidence="0.999540941176471">
system has assigned the named entities is likely to
be useful in a range of different applications.
Named entities of relevance to biomedical sci-
ence include not only genes and proteins but also
other chemical substances which can be of inter-
est as drugs, metabolites, nutrients, enzyme cofac-
tors, experimental reagents and in many other roles.
We have recently investigated the issue of chemical
named entities (Corbett et al., 2007), by compiling a
set of manual annotation guidelines, demonstrating
93% interannotator agreement and manually anno-
tating a set of 42 chemistry papers. In this paper we
demonstrate a named entity recogniser that assigns
a confidence score to each named entity, allowing it
to be tuned for high precision or recall.
Our review of the methods of chemical named
entity recognition showed a consistent theme: the
use of character-based n-Grams to identify chemi-
cal names via their constituent substrings (Wilbur et
al., 1999; Vasserman, 2004; Townsend et al., 2005).
This can be a powerful technique, due to systematic
and semisystematic chemical names and additional
conventions in drug names. However this technique
does not cover all aspects of chemical nomenclature.
Much current named entity work uses approaches
which combine the structured prediction abilities
of HMMs and their derivatives with techniques
which enable the use of large, diverse feature sets
such as maximum entropy (also known as logis-
tic regression). Maximum Entropy Markov Mod-
els, (MEMMs) (McCallum et al., 2000) provide a
relatively simple framework for this. MEMMs do
have a theoretical weakness, namely the “label bias”
problem (Lafferty et al., 2001), which has been ad-
</bodyText>
<page confidence="0.984758">
54
</page>
<note confidence="0.780329">
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 54–62,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999808072916667">
dressed with the development of Conditional Ran-
dom Fields (CRFs). CRFs are now a mainstay of
the field, being used in a high proportion of entries
in the latest BioCreative evaluation (Krallinger and
Hirschman, 2007). However, despite the label bias
problem, MEMMs still attract interest due to practi-
cal advantages such as shorter training cycles.
The framework of HMMs and their successors of-
fers three modes of operation; first-best, n-best and
confidence-based. In first-best NER, the Viterbi al-
gorithm is used to identify a single sequence of la-
bels for the target sentence. In n-best operation,
the n best sequences for the sentence are identi-
fied, along with their probabilities, for example by
coupling the Viterbi algorithm with A* search. In
confidence-based operation, potential entities (with
a probability above a threshold) are identified di-
rectly, without directly seeking a single optimal la-
belling for the entire sentence. This is done by
examining the probability of the label transitions
within the entity, and the forward and backward
probabilities at the start and end of the entity. This
mode has been termed the Constrained Forward-
Backward algorithm (Culotta and McCallum, 2004).
Where a single unambiguous non-overlapping la-
belling is required, it can be obtained by identify-
ing cases where the entities overlap, and discarding
those with lower probabilities.
Confidence-based extraction has two main advan-
tages. First, it enables the balance between precision
and recall to be controlled by varying the probability
threshold. Second, confidence-based NER avoids
over-commitment in systems where it is used as a
preprocessor, since multiple overlapping options can
be used as input to later components.
The optimum balance between recall and preci-
sion depends on the application of the NER and on
the other components in the system. High precision
is useful in search even when recall is low when
there is a large degree of redundancy in the informa-
tion in the original documents. High precision NER
may also be useful in contexts such as the extraction
of seed terms for clustering algorithms. Balanced
precision/recall is often appropriate for search, al-
though in principle it is desirable to be able to shift
the balance if there are too many/too few results.
Balanced precision/recall is also generally assumed
for use in strictly pipelined systems, when a single
set of consistent NER results is to be passed on to
subsequent processing. Contexts where high recall
is appropriate include those where a search is being
carried out where there is little redundancy (cf Car-
penter 2007) or where the NER system is being used
with other components which can filter the results.
One use of our NER system is within a language
processing architecture (Copestake et al., 2006) that
systematically allows for ambiguity by treating the
input/output of each component as a lattice (repre-
sented in terms of standoff annotation on an orig-
inal XML document). This system exploits rela-
tively deep parsing, which is not fully robust to NER
errors but which can exploit complex syntactic in-
formation to select between candidate NER results.
NER preprocessing is especially important in the
context of chemistry terms which utilise punctuation
characters (e.g., ‘2,4-dinitrotoluene’, ‘2,4- and 2,6-
dinitrotoluene’) since failure to identify these will
lead to tokenisation errors in the parser. Such errors
frequently cause complete parse failure, or highly
inaccurate analyses. In our approach, the NER re-
sults contribute edges to a lattice which can (option-
ally) be treated as tokens by the parser. The NER
results may compete with analyses provided by the
main parser lexicon. In this context, some NER er-
rors are unimportant: e.g., the parser is not sensitive
to all the distinctions between types of named entity.
In other cases, the parser will filter the NER results.
Hence it makes sense to emphasise recall over pre-
cision. We also hypothesise that we will be able to
incorporate the NER confidence scores as features
in the parse ranking model.
Another example of the use of high-recall NER in
an integrated system is shown in the editing work-
flows used by the Royal Society of Chemistry in
their Project Prospect system (Batchelor and Cor-
bett, 2007), where chemical named entity recogni-
tion is used to produce semantically-enriched jour-
nal articles. In this situation, high recall is desirable,
as false positives can be removed in two ways; by
removing entities where a chemical structure cannot
be assigned, and by having them checked by a tech-
nical editor. False negatives are harder to correct.
The use of confidence-based recognition has been
demonstrated with CRFs in the domain of contact
details (Culotta and McCallum, 2004), and using
HMMs in the domain of gene annotation (Carpen-
</bodyText>
<page confidence="0.997455">
55
</page>
<bodyText confidence="0.999938225806452">
ter, 2007). In the latter case, the LingPipe toolkit
was used in the BioCreative 2 evaluation without
significant adaptation. Although only 54% preci-
sion was achieved at 60% recall (the best systems
were achieving precision and recall scores in the
high eighties), the system was capable of 99.99%
recall with 7% precision, and 95% recall with 18%
precision, indicating that very high recall could be
obtained in this difficult domain.
Another potential use of confidence-based NER
is the potential to rescore named entities. In this
approach, the NER system is run, generating a set
of named entities. Information obtained about these
entities throughout the document (or corpus) that
they occur in can then be used in further classi-
fiers. We are not aware of examples of rescoring
being applied to confidence-based NER, but there
are precedents using other modes of operations. For
example, Krishnan and Manning (2006) describe a
system where a first-best CRF is used to analyse a
corpus, the results of which are then used to gener-
ate additional features to use in a second first-best
CRF. Similarly, Yoshida and Tsujii (2007) use an n-
best MEMM to generate multiple analyses for a sen-
tence, and re-rank the analyses based on information
extracted from neighbouring sentences.
Therefore, to explore the potential of these tech-
niques, we have produced a chemical NER system
that uses a MEMM for confidence-based extraction
of named entities, with an emphasis on the use of
character-level n-Grams, and a rescoring system.
</bodyText>
<sectionHeader confidence="0.971151" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999971242424242">
Previously, we have produced a set of annotation
guidelines for chemical named entities, and used
them to annotate a set of 42 chemistry papers (Cor-
bett et al., 2007). Inter-annotator agreement was
tested on 14 of these, and found to be 93%. The an-
notation guidelines specified five classes of named
entity, which are detailed in Table 1. The annotation
was performed on untokenised text.
To test the applicability of the method to a
different corpus, we retrieved 500 PubMed ab-
stracts and titles, and annotated them using the
same methods. The abstracts were acquired us-
ing the query metabolism[Mesh] AND drug
AND hasabstract. This produced a diverse set
of abstracts spanning a wide range of subject ar-
eas, but which contain a higher proportion of rele-
vant terms than PubMed overall. 445 out of 500 ab-
stracts contained at least one named entity, whereas
249 contained at least ten. Notably, the ASE class
was more common in the PubMed corpus than in
the chemistry papers, reflecting the important of en-
zymes to biological and medical topics.
In this study, we have left out the named entity
type CPR, as it is rare (&lt;1%) and causes difficulties
with tokenisation. This entity type covers cases such
as the “1,3-” in “1,3-disubstituted”, and as such re-
quires the “1,3-” to be a separate token or token se-
quence. However, we have found that recognition
of the other four classes is improved if words such
as “1,3-disubstituted” are kept together as single to-
kens. Therefore it makes sense to treat the recogni-
tion of CPR as an essentially separate problem - a
problem that will not be addressed here.
</bodyText>
<table confidence="0.998230666666667">
Type Description Example nCh nPM
CM compound citric acid 6865 4494
RN reaction methylation 288 401
CJ adjective pyrazolic 60 87
ASE enzyme demethylase 31 181
CPR prefix 1,3- 53 21
</table>
<tableCaption confidence="0.935438">
Table 1: Named Entity types. nCh = number in Chem-
istry corpus, nPm = number in PubMed corpus.
</tableCaption>
<sectionHeader confidence="0.996454" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999930666666667">
Our system is quite complex, and as such we have
made the source code available (see below). The fol-
lowing gives an outline of the system:
</bodyText>
<subsectionHeader confidence="0.998616">
3.1 External Resources
</subsectionHeader>
<bodyText confidence="0.999973428571429">
Chemical names were extracted from the chem-
ical ontology ChEBI (Degtyarenko et al., 2008),
and a standard English word list was taken from
/usr/share/dict/words on a Linux system2.
A list of chemical element names and symbols was
also compiled. To overcome the shortage of enti-
ties of type ASE, a list of words from enzyme names
</bodyText>
<footnote confidence="0.5289545">
2This dictionary was chosen as it contains inflectional forms
of English words. Our system does not perform stemming,
partly because suffixes are often good cues as to whether a word
is chemical or not.
</footnote>
<page confidence="0.989549">
56
</page>
<bodyText confidence="0.999526333333333">
ending in ‘-ase’ was extracted from the Gene Ontol-
ogy (GO), and hand sorted into words of type ASE,
and words not of type ASE.
</bodyText>
<subsectionHeader confidence="0.999887">
3.2 Overview of operation
</subsectionHeader>
<bodyText confidence="0.999992860465116">
The text is tokenised before processing; this is
done using the tokeniser described in our previous
work (Corbett et al., 2007), which is adapted to
chemical text.
Our system uses three groups of classifiers to
recognise chemical names. The first classifier—the
‘preclassifier’—uses character-level n-grams to esti-
mate the probabilities of whether tokens are chemi-
cal or not. The output of this classification is com-
bined with information from the suffix of the word,
and is used to provide features for the MEMM.
The second group of classifiers constitute the
MEMM proper. Named entities are represented us-
ing an BIO-encoding, and methods analogous to
other confidence-based taggers (Culotta and McCal-
lum, 2004; Carpenter, 2007) are used to estimate
the conditional probability of tag sequences corre-
sponding to named entities. The result of this is
a list of potential named entities, with start posi-
tions, end positions, types and probabilities, where
all of the probabilities are above a threshold value.
A small set of hand-written filtering rules is used to
remove obvious absurdities, such as named entities
ending in the word “the”, and simple violations of
the annotation guidelines, such as named entities of
type ASE that contain whitespace. These filtering
rules make very little difference at recall values up
to about 80%—however, we have found that they are
useful for improving precision at very high recall.
The third group of classifiers—one per entity
type—implement a rescoring system. After all of
the potential entities from a document have been
generated, a set of features is generated for each en-
tity. These features are derived from the probabili-
ties of other entities that share the same text string
as the entity, from probabilities of potential syn-
onyms found via acronym matching and other pro-
cesses, and most importantly, from the pre-rescoring
probability of the entities themselves. In essence,
the rescoring process performs Bayesian reasoning
by adjusting the raw probabilities from the previ-
ous stage up or down based on nonlocal information
within the document.
</bodyText>
<subsectionHeader confidence="0.998659">
3.3 Overview of training
</subsectionHeader>
<bodyText confidence="0.999951032258064">
A form of training conceptually similar to cross-
validation is used to train the three layers of clas-
sifiers. To train the overall system, the set of docu-
ments used for training is split into three. Two thirds
are used to train a MEMM, which is then used to
generate training data for the rescorer using the held-
out last third. This process is repeated another two
times, holding out a different third of the training
data each time. Finally, the rescorer is trained using
all of the training data generated by this procedure,
and the final version of the MEMM is generated us-
ing all of the training data. This procedure ensures
that both the MEMM and the rescorer are able to
make use of all of the training data, and also that
the rescorer is trained to work with the output of a
MEMM that has not been trained on the documents
that it is to rescore.
A similar procedure is used when training the
MEMM itself. The available set of documents to use
as training data is divided into half. One half is used
to train the preclassifier and build its associated dic-
tionaries, which are then used to generate features
for the MEMM on the other half of the data. The
roles of each half are then reversed, and the same
process is applied. Finally, the MEMM is trained
using all of the generated features, and a new pre-
classifier is trained using all of the available training
data.
It should be noted that the dictionaries extracted
during the training of the preclassifier are also used
directly in the MEMM.
</bodyText>
<subsectionHeader confidence="0.99347">
3.4 The character n-gram based preclassifier
</subsectionHeader>
<bodyText confidence="0.999541846153846">
During the training of the preclassifier, sets of to-
kens are extracted from the hand-annotated train-
ing data. A heuristic is used to classify these
into ‘word tokens’—those that match the regex
.*[a-z][a-z].*, and ‘nonword tokens’—those
that do not (this class includes many acronyms and
chemical formulae). The n-gram analysis is only
performed upon ‘word tokens’.
The token sets that are compiled are chemi-
cal word tokens (those that only appear inside
named entities), nonchemical word tokens (those
that do not appear in entities), chemical nonword
tokens, nonchemical nonword tokens and ambigu-
</bodyText>
<page confidence="0.992914">
57
</page>
<bodyText confidence="0.999964285714286">
ous tokens—those that occur both inside and out-
side of named entities. A few other minor sets are
collected to deal with tokens related to such proper
noun-containing entities as ‘Diels–Alder reaction’.
Some of this data is combined with external dic-
tionaries to train the preclassifier, which works us-
ing 4-grams of characters and modified Kneser-Ney
smoothing, as described by Townsend et al. (2005).
The set of ‘chemical word tokens’ is used as a set of
positive examples, along with tokens extracted from
ChEBI, a list of element names and symbols, and
the ASE tokens extracted from the GO. The negative
examples used are the extracted ‘nonchemical word
tokens’, the non-ASE tokens from the GO and to-
kens taken from the English dictionary—except for
those that were listed as positive examples. This gets
around the problem that the English dictionary con-
tains the names of all of the elements and a number
of simple compounds such as ‘ethanol’.
During operation, n-gram analysis is used to cal-
culate a score for each word token, of the form:
</bodyText>
<equation confidence="0.99029">
ln(P(token|chem)) − ln(P (token|nonchem))
</equation>
<bodyText confidence="0.999970833333333">
If this score is above zero, the preclassifier clas-
sifies the token as chemical and gives it a tentative
type, based on its suffix. This can be considered to
be a “first draft” of its named entity type. For exam-
ple tokens ending in “-ation” are given the type RN,
whereas those ending in “-ene” are given type CM.
</bodyText>
<subsectionHeader confidence="0.794672">
3.5 The MEMM
</subsectionHeader>
<bodyText confidence="0.9998966">
The MEMM is a first-order MEMM, in that it has a
separate maximum-entropy model for each possible
preceeding tag. No information about the tag se-
quence was included directly in the feature set. We
use the OpenNLP MaxEnt classifier3 for maximum-
entropy classification.
The feature set for the MEMM is divided into
three types of features; type 1 (which apply to the
token itself), type 2 (which can apply to the token it-
self, the previous token and the next token) and type
3 (which can act as type 2 features, and which can
also form bigrams with other type 3 features).
An example type 1 feature would be 4G=ceti,
indicating that the 4-gram ceti had been found
in the token. An example type 2 feature would be
</bodyText>
<footnote confidence="0.766742">
3http://maxent.sourceforge.net/
</footnote>
<bodyText confidence="0.999911727272727">
c-1:w=in, indicating that the previous token was
‘in’. An example bigram constructed from type 3
features would be bg:0:1:ct=CJ w=acid, in-
dicating that the preclassifier had classified the token
as being of type CJ, and having a score above zero,
and that the next token was ‘acid’.
Type 1 features include 1, 2, 3 and 4-grams of
characters found within the token, whether the to-
ken appeared in any of the word lists, and features to
represent the probability and type given by the pre-
classifier for that token. Type 2 features include the
token itself with any terminal letter ‘s’ removed, the
token converted to lowercase (if it matched the regex
.*[a-z][a-z].*), and a three-character suffix
taken from the token. The token itself was usually
used as a type 2 feature, unless it unless it was short
(less than four characters), or had been found to be
an ambiguous token during preclassifier training, in
which case it was type 3. Other type 3 features in-
clude a word shape feature, and tentative type of the
token if the preclassifier had classed it as chemical.
A few other features were used to cover a few spe-
cial cases, and were found to yield a slight improve-
ment during development.
After generating the features, a feature selection
based on log-likelihood ratios is used to remove the
least informative features, with a threshold set to re-
move about half of them. This was found during
development to have only a very small beneficial ef-
fect on the performance of the classifier, but it did
make training faster and produced smaller models.
This largely removed rare features which were only
found on a few non-chemical tokens.
</bodyText>
<subsectionHeader confidence="0.998457">
3.6 The rescorer
</subsectionHeader>
<bodyText confidence="0.9999322">
The rescoring system works by constructing four
maximum entropy classifiers, one for each entity
type. The output of these classifiers is a probabil-
ity of whether or not a potential named entity really
is a correct named entity. The generation of features
is done on a per-document basis.
The key features in the rescorer represent the
probability of the potential entity as estimated by
the MEMM. The raw probability p is converted to
the logit score
</bodyText>
<equation confidence="0.944504">
l = ln(p) − ln(1 − p)
</equation>
<bodyText confidence="0.979159">
This mirrors the way probabilities are represented
</bodyText>
<page confidence="0.995997">
58
</page>
<bodyText confidence="0.999836342857143">
within maximum entropy (aka logistic regression)
classifiers. If l is positive, int(min(15.0, l) * 50)
instances 4 of the feature conf+ are generated, and
a corresponding technique is used if l is negative.
Before generating further features, it is necessary
to find entities that are ‘blocked’—entities that over-
lap with other entities of higher confidence. For ex-
ample, consider “ethyl acetate”, which might give
rise to the named entity “ethyl acetate” with 98%
confidence, and also “ethyl” with 1% confidence and
“acetate” with 1% confidence. In this case, “ethyl”
and “acetate” would be blocked by “ethyl acetate”.
Further features are generated by collecting to-
gether all of the unblocked5 potential entities of a
type that share the same string, calculating the max-
imum and average probability, and calculating the
difference between the p and those quantities.
Some acronym and abbreviation handling is also
performed. The system looks for named entities that
are surrounded by brackets. For each of these, a list
of features is generated that is then given to every
other entity of the same string. If there is a potential
entity to the left of the bracketed potential abbre-
viation, then features are generated to represent the
probability of that potential entity, and how well the
string form of that entity matches the potential ab-
breviation. If no potential entity is found to match
with, then features are generated to represent how
well the potential abbreviation matches the tokens
to the left of it. By this method, the rescorer can
gather information about whether a potential abbre-
viation stands for a named entity, something other
than a named entity—or whether it is not an abbre-
viation at all, and use that information to help score
all occurrences of that abbreviation in the document.
</bodyText>
<sectionHeader confidence="0.996727" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.987890368421053">
The systems were evaluated by 3-fold cross-
validation methodology, whereby the data was split
into three equal folds (in the case of the chemistry
4We found that 15.0 was a good threshold by experimenta-
tion on development data: papers annotated during trial runs of
the annotation process.
5Doing this without regards for blocking causes problems.
In a document containing both “ethyl acetate” and “ethyl
group”, it would be detrimental to allow the low confidence
for the “ethyl” in “ethyl acetate” to lower the confidence of the
“ethyl” in “ethyl group”.
papers, each fold consists of one paper per journal.
For the PubMed abstracts, each fold consists of one
third of the total abstracts). For each fold, the system
was trained on the other two folds and then evaluated
on that fold, and the results were pooled.
The direct output from the system is a list of
putative named entities with start positions, end
positions, types and confidence scores. This list
was sorted in order of confidence—most confident
first—and each entity was classified as a true posi-
tive or a false positive according to whether an ex-
act match (start position, end position and type all
matched perfectly) could be found in the annotated
corpus. Also, the number of entities in the annotated
corpus was recorded.
Precision/recall curves were plotted from these
lists by selecting the first n elements, and calculat-
ing precision and recall taking all of the elements in
this sublist as true or false positives, and all the enti-
ties in the corpus that were not in the sublist as false
negatives. The value of n was gradually increased,
recording the scores at each point. The area under
the curve (treating precision as zero at recall values
higher than the highest reported) was used to calcu-
late mean average precision (MAP). Finally, F were
generated by selecting all of the entities with a con-
fidence score of 0.3 or higher.
</bodyText>
<table confidence="0.996584666666667">
Full System
No Rescorer
No Preclassifier
No n−Grams
Customised LingPipe HMM
Pure LingPipe HMM
</table>
<figure confidence="0.486279">
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</figure>
<figureCaption confidence="0.996622">
Figure 1: Evaluation on chemistry papers.
</figureCaption>
<bodyText confidence="0.842382">
The results of this evaluation on the corpus of
</bodyText>
<figure confidence="0.3855735">
Precision
0.0 0.2 0.4 0.6 0.8 1.0
</figure>
<page confidence="0.996027">
59
</page>
<bodyText confidence="0.999990347826087">
chemistry papers is show in Figure 1. The full sys-
tem achieves 57.6% recall at 95% precision, 58.9%
precision at 90% recall, and 78.7% precision and
82.9% recall (F = 80.7%) at a confidence threshold
of 0.3. Also shown are the results of successively
eliminating parts of the system. “No Rescorer” re-
moves the rescorer. In “No Preclassifier”, the pre-
classifier is disabled, and all of the dictionaries ex-
tracted during the training of the preclassifier are
also disabled. Finally, in “No n-Grams”, the 1-, 2-
, 3- and 4-grams used directly by the MEMM are
also disabled, showing the results of using a sys-
tem where no character-level n-grams are used at all.
These modifications apply successively—for exam-
ple, in the “No n-Grams” case the rescorer and pre-
classifier are also disabled. These results validate the
the cascade of classifiers, and underline the impor-
tance of character-level n-grams in chemical NER.
We also show comparisons to an HMM-based
approach, based on LingPipe 3.4.0.6 This is es-
sentially the same system as described by Corbett
et al. (2007), but operating in a confidence-based
mode. The HMMs used make use of character-level
n-Grams, but do not allow the use of the rich fea-
ture set used by the MEMM. The line “Customised
LingPipe HMM” shows the system using the cus-
tom tokenisation and ChEBI-derived dictionary used
in the MEMM system, whereas the “Pure LingPipe
HMM” shows the system used with the default to-
keniser and no external dictionaries. In the region
where precision is roughly equal to recall (mimick-
ing the operation of a first-best system), the fact that
the MEMM-based system outperforms an HMM is
no surprise. However, it is gratifying that a clear
advantage can be seen throughout the whole recall
range studied (0-97%), indicating that the training
processes for the MEMM are not excessively at-
tuned to the first-best decision boundary. This in-
creased accuracy comes at a price in the speed of
development, training and execution.
It is notable that we were not able to achieve ex-
tremes of recall at tolerable levels of precision us-
ing any of the systems, whereas it was possible for
LingPipe to achieve 99.99% recall at 7% precision in
the BioCreative 2006 evaluation. There are a num-
ber of potential reasons for this. The first is that the
</bodyText>
<footnote confidence="0.874951">
6http://alias-i.com/lingpipe/
</footnote>
<bodyText confidence="0.998508470588235">
tokeniser used in all systems apart from the “Pure
LingPipe HMM” system tries in general to make
as few token boundaries as possible; this leads to
some cases where the boundaries of the entities to
be recognised in the test paper occur in the middle
of tokens, thus making those entities unrecognisable
whatever the threshold. However this does not ap-
pear to be the whole problem. Other factors that may
have had an influence include the more generous
method of evaluation at BioCreative 2006, (where
several allowable alternatives were given for diffi-
cult named entities), and the greater quantity and di-
versity (sentences selected from a large number of
different texts, rather than a relatively small number
of whole full papers) of training data. Finally, there
might be some important difference between chem-
ical names and gene names.
</bodyText>
<figure confidence="0.871866">
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</figure>
<figureCaption confidence="0.999812">
Figure 2: Evaluation on PubMed abstracts.
</figureCaption>
<bodyText confidence="0.995095818181818">
Figure 2 shows the results of running the sys-
tem on the set of annotated PubMed abstracts. The
full system achieves 60.3% recall at 95% precision,
49.1% precision at 90% recall, and 85.0% preci-
sion and 81.6% recall (F = 83.2%) at a confidence
threshold of 0.3. In PubMed abstracts, it is common
to define ad-hoc abbreviations for chemicals within
an abstract (e.g., the abstract might say ‘dexametha-
sone (DEX)’, and then use ‘DEX’ and not ‘dexam-
ethasone’ throughout the rest of the abstract). The
rescorer provides a good place to resolve these ab-
</bodyText>
<figure confidence="0.939082625">
Full System
No Rescorer
No Preclassifier
No n−Grams
Customised LingPipe HMM
Pure LingPipe HMM
Precision
0.0 0.2 0.4 0.6 0.8 1.0
</figure>
<page confidence="0.975305">
60
</page>
<bodyText confidence="0.999885428571429">
breviations, and thus has a much larger effect than
in the case of chemistry papers where these ad hoc
abbreviations are less common. It is also notable
that the maximum recall is lower in this case. One
system—the “Pure LingPipe HMM”, which uses a
different, more aggressive tokeniser from the other
systems—has a clear advantage in terms of maxi-
mum recall, showing that overcautious tokenisation
limits the recall of the other systems.
In some cases the systems studied behave
strangely, having “spikes” of lowered precision at
very low recall, indicating that the systems can occa-
sionally be overconfident, and assign very high con-
fidence scores to incorrect named entities.
</bodyText>
<table confidence="0.999394928571429">
Corpus System MAP F
Chemistry Full 87.1% 80.8%
Chemistry No Rescorer 86.8% 81.0%
Chemistry No Preclassifier 82.7% 74.8%
Chemistry No n-Grams 79.2% 72.2%
Chemistry Custom LingPipe 75.9% 74.6%
Chemistry Pure LingPipe 66.9% 63.2%
Chemistry No Overlaps 82.9% 80.8%
PubMed Full 86.1% 83.2%
PubMed No Rescorer 83.3% 79.1%
PubMed No Preclassifier 81.4% 73.4%
PubMed No n-Grams 77.6% 70.6%
PubMed Custom LingPipe 78.6% 75.6%
PubMed Pure LingPipe 71.9% 66.1%
</table>
<tableCaption confidence="0.986199">
Table 2: F scores (at confidence threshold of 0.3) and
Mean Average Precision (MAP) values for Figs. 1-3.
</tableCaption>
<bodyText confidence="0.999916666666667">
Neither corpus contains enough data for the re-
sults to reach a plateau—using additional training
data is likely to give improvements in performance.
The “No Overlaps” line in Figure 3 shows the ef-
fect of removing “blocked” named entities (as de-
fined in section 3.6) prior to rescoring. This sim-
ulates a situation where an unambiguous inline an-
notation is required—for example a situation where
a paper is displayed with the named entities being
highlighted. This condition makes little difference
at low to medium recall, but it sets an effective max-
imum recall of 90%. The remaining 10% of cases
presumably consist of situations where the recog-
niser is finding an entity in the right part of the text,
but making boundary or type errors.
</bodyText>
<figure confidence="0.9456655">
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</figure>
<figureCaption confidence="0.9884355">
Figure 3: Evaluation on chemistry papers, showing ef-
fects of disallowing overlapping entities.
</figureCaption>
<sectionHeader confidence="0.99749" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9995085">
We have demonstrated that MEMMs can be adapted
to recognise chemical named entities, and that the
balance between precision and recall can be tuned
effectively, at least in the range of 0 - 95% recall.
The MEMM system is available as part of the OS-
CAR3 chemical named entity recognition system. 7
</bodyText>
<sectionHeader confidence="0.996014" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996595">
PTC thanks Peter Murray-Rust for supervision. We
thank the Royal Society of Chemistry for provid-
ing the papers, and the EPSRC (EP/C010035/1) for
funding. We thank the reviewers for their helpful
suggestions and regret that we did not have the time
or space to address all of the issues raised.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996981875">
Colin Batchelor and Peter Corbett. 2007. Semantic en-
richment of journal articles using chemical named en-
tity recognition Proceedings of the ACL 2007 Demo
and Poster Sessions, pp 45-48. Prague, Czech Repub-
lic.
Bob Carpenter. 2007. LingPipe for 99.99% Recall of
Gene Mentions Proceedings of the Second BioCre-
ative Challenge Evaluation Workshop, 307-309.
</reference>
<footnote confidence="0.549042">
7https://sourceforge.net/projects/oscar3-chem
</footnote>
<figure confidence="0.9838535">
Full System
No Overlaps
Precision
0.0 0.2 0.4 0.6 0.8 1.0
</figure>
<page confidence="0.980803">
61
</page>
<reference confidence="0.999774587301588">
Ann Copestake, Peter Corbett, Peter Murray-Rust, C. J.
Rupp, Advaith Siddharthan, Simone Teufel and Ben
Waldron. 2006. An Architecture for Language Tech-
nology for Processing Scientific Texts. Proceedings of
the 4th UK E-Science All Hands Meeting, Nottingham,
UK.
Peter Corbett, Colin Batchelor and Simone Teufel. 2007.
Annotation of Chemical Named Entities BioNLP
2007: Biological, translational, and clinical language
processing, pp 57-64. Prague, Czech Republic.
Aron Culotta and Andrew McCallum 2004. Confidence
Estimation for Information Extraction Proceedings of
Human Language Technology Conference and North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pp 109-112. Boston,
MA.
Kirill Degtyarenko, Paula de Matos, Marcus Ennis, Janna
Hastings, Martin Zbinden, Alan McNaught, Rafael Al-
cantara, Michael Darsow, Mickael Guedj and Michael
Ashburner. 2008. ChEBI: a database and ontology for
chemical entities of biological interest. Nucleic Acids
Res, Vol. 36, Database issue D344-D350.
The Gene Ontology Consortium 2000. Gene Ontology:
tool for the unification of biology. Nature Genetics,
Vol. 25, 26-29.
Martin Krallinger and Lynette Hirschman, editors. 2007.
Proceedings of the Second BioCreative Challenge
Evaluation Workshop.
Vijay Krishnan and Christopher D. Manning. 2006. An
Effective Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, 1121-1128.
Sindey, Australia.
John Lafferty, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of the Eighteenth International Conference
on Machine Learning, 282-289.
Andrew McCallum, Dayne Freitag and Fernando Pereira.
2000. Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation Proceedings of
the Seventeenth International Conference on Machine
Learning, 591-598. San Fransisco, CA.
Joe A. Townsend, Ann Copestake, Peter Murray-Rust, Si-
mone H. Teufel and Christopher A. Waudby. 2005.
Language Technology for Processing Chemistry Pub-
lications Proceedings of the fourth UK e-Science All
Hands Meeting, 247-253. Nottingham, UK.
Alexander Vasserman 2004 Identifying Chemical
Names in Biomedial Text: An Investigation of the
Substring Co-occurence Based Approaches Proceed-
ings of the Student Research Workshop at HLT-NAACL
W. John Wilbur, George F. Hazard, Jr., Guy Divita,
James G. Mork, Alan R. Aronson and Allen C.
Browne. 1999 Analysis of Biomedical Text for Chem-
ical Names: A Comparison of Three Methods Proc.
AMIA Symp. 176-180.
Kazuhiro Yoshida and Jun’ichi Tsujii. 2007. Reranking
for Biomedical Named-Entity Recognition BioNLP
2007: Biological, translational, and clinical language
processing, pp 57-64. Prague, Czech Republic.
</reference>
<page confidence="0.999185">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.397174">
<title confidence="0.997263">Cascaded Classifiers for Confidence-Based Chemical Named Entity Recognition</title>
<author confidence="0.990696">Peter</author>
<affiliation confidence="0.995744">Unilever Centre Molecular Science Chemical Laboratory University Of</affiliation>
<address confidence="0.685938">C132 1EW,</address>
<email confidence="0.921626">ptc24@cam.ac.uk</email>
<author confidence="0.873327">Ann</author>
<affiliation confidence="0.985726">Computer University Of</affiliation>
<address confidence="0.723359">C133 0FD,</address>
<email confidence="0.990742">aac10@cl.cam.ac.uk</email>
<abstract confidence="0.99940719047619">Chemical named entities represent an important facet of biomedical text. We have developed a system to use character-based ngrams, Maximum Entropy Markov Models and rescoring to recognise chemical names and other such entities, and to make confidence estimates for the extracted entities. An adjustable threshold allows the system to be tuned to high precision or high recall. At a threshold set for balanced precision and recall, we were able to extract named entities at an of 80.7% from chemistry papers and 83.2% from PubMed abstracts. Furthermore, we were able to achieve 57.6% and 60.3% recall at 95% precision, and 58.9% and 49.1% precision at 90% recall. These results show that chemical named entities can be extracted with good performance, and that the properties of the extraction can be tuned to suit the demands of the task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Batchelor</author>
<author>Peter Corbett</author>
</authors>
<title>Semantic enrichment of journal articles using chemical named entity recognition</title>
<date>2007</date>
<booktitle>Proceedings of the ACL 2007 Demo and Poster Sessions,</booktitle>
<pages>45--48</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7845" citStr="Batchelor and Corbett, 2007" startWordPosition="1238" endWordPosition="1242">th analyses provided by the main parser lexicon. In this context, some NER errors are unimportant: e.g., the parser is not sensitive to all the distinctions between types of named entity. In other cases, the parser will filter the NER results. Hence it makes sense to emphasise recall over precision. We also hypothesise that we will be able to incorporate the NER confidence scores as features in the parse ranking model. Another example of the use of high-recall NER in an integrated system is shown in the editing workflows used by the Royal Society of Chemistry in their Project Prospect system (Batchelor and Corbett, 2007), where chemical named entity recognition is used to produce semantically-enriched journal articles. In this situation, high recall is desirable, as false positives can be removed in two ways; by removing entities where a chemical structure cannot be assigned, and by having them checked by a technical editor. False negatives are harder to correct. The use of confidence-based recognition has been demonstrated with CRFs in the domain of contact details (Culotta and McCallum, 2004), and using HMMs in the domain of gene annotation (Carpen55 ter, 2007). In the latter case, the LingPipe toolkit was </context>
</contexts>
<marker>Batchelor, Corbett, 2007</marker>
<rawString>Colin Batchelor and Peter Corbett. 2007. Semantic enrichment of journal articles using chemical named entity recognition Proceedings of the ACL 2007 Demo and Poster Sessions, pp 45-48. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>LingPipe for 99.99% Recall of Gene Mentions</title>
<date>2007</date>
<booktitle>Proceedings of the Second BioCreative Challenge Evaluation Workshop,</booktitle>
<pages>307--309</pages>
<contexts>
<context position="6185" citStr="Carpenter 2007" startWordPosition="969" endWordPosition="971">cuments. High precision NER may also be useful in contexts such as the extraction of seed terms for clustering algorithms. Balanced precision/recall is often appropriate for search, although in principle it is desirable to be able to shift the balance if there are too many/too few results. Balanced precision/recall is also generally assumed for use in strictly pipelined systems, when a single set of consistent NER results is to be passed on to subsequent processing. Contexts where high recall is appropriate include those where a search is being carried out where there is little redundancy (cf Carpenter 2007) or where the NER system is being used with other components which can filter the results. One use of our NER system is within a language processing architecture (Copestake et al., 2006) that systematically allows for ambiguity by treating the input/output of each component as a lattice (represented in terms of standoff annotation on an original XML document). This system exploits relatively deep parsing, which is not fully robust to NER errors but which can exploit complex syntactic information to select between candidate NER results. NER preprocessing is especially important in the context o</context>
<context position="13384" citStr="Carpenter, 2007" startWordPosition="2166" endWordPosition="2167"> et al., 2007), which is adapted to chemical text. Our system uses three groups of classifiers to recognise chemical names. The first classifier—the ‘preclassifier’—uses character-level n-grams to estimate the probabilities of whether tokens are chemical or not. The output of this classification is combined with information from the suffix of the word, and is used to provide features for the MEMM. The second group of classifiers constitute the MEMM proper. Named entities are represented using an BIO-encoding, and methods analogous to other confidence-based taggers (Culotta and McCallum, 2004; Carpenter, 2007) are used to estimate the conditional probability of tag sequences corresponding to named entities. The result of this is a list of potential named entities, with start positions, end positions, types and probabilities, where all of the probabilities are above a threshold value. A small set of hand-written filtering rules is used to remove obvious absurdities, such as named entities ending in the word “the”, and simple violations of the annotation guidelines, such as named entities of type ASE that contain whitespace. These filtering rules make very little difference at recall values up to abo</context>
</contexts>
<marker>Carpenter, 2007</marker>
<rawString>Bob Carpenter. 2007. LingPipe for 99.99% Recall of Gene Mentions Proceedings of the Second BioCreative Challenge Evaluation Workshop, 307-309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Peter Corbett</author>
<author>Peter Murray-Rust</author>
<author>C J Rupp</author>
<author>Advaith Siddharthan</author>
<author>Simone Teufel</author>
<author>Ben Waldron</author>
</authors>
<title>An Architecture for Language Technology for Processing Scientific Texts.</title>
<date>2006</date>
<booktitle>Proceedings of the 4th UK E-Science All Hands Meeting,</booktitle>
<location>Nottingham, UK.</location>
<contexts>
<context position="6371" citStr="Copestake et al., 2006" startWordPosition="1000" endWordPosition="1003">ch, although in principle it is desirable to be able to shift the balance if there are too many/too few results. Balanced precision/recall is also generally assumed for use in strictly pipelined systems, when a single set of consistent NER results is to be passed on to subsequent processing. Contexts where high recall is appropriate include those where a search is being carried out where there is little redundancy (cf Carpenter 2007) or where the NER system is being used with other components which can filter the results. One use of our NER system is within a language processing architecture (Copestake et al., 2006) that systematically allows for ambiguity by treating the input/output of each component as a lattice (represented in terms of standoff annotation on an original XML document). This system exploits relatively deep parsing, which is not fully robust to NER errors but which can exploit complex syntactic information to select between candidate NER results. NER preprocessing is especially important in the context of chemistry terms which utilise punctuation characters (e.g., ‘2,4-dinitrotoluene’, ‘2,4- and 2,6- dinitrotoluene’) since failure to identify these will lead to tokenisation errors in th</context>
</contexts>
<marker>Copestake, Corbett, Murray-Rust, Rupp, Siddharthan, Teufel, Waldron, 2006</marker>
<rawString>Ann Copestake, Peter Corbett, Peter Murray-Rust, C. J. Rupp, Advaith Siddharthan, Simone Teufel and Ben Waldron. 2006. An Architecture for Language Technology for Processing Scientific Texts. Proceedings of the 4th UK E-Science All Hands Meeting, Nottingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Corbett</author>
<author>Colin Batchelor</author>
<author>Simone Teufel</author>
</authors>
<title>Biological, translational, and clinical language processing,</title>
<date>2007</date>
<journal>Annotation of Chemical Named Entities BioNLP</journal>
<pages>57--64</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2161" citStr="Corbett et al., 2007" startWordPosition="338" endWordPosition="341">re, knowing the confidence1 with which the 1In this paper, we use “confidence” to refer to a system’s estimate of the probability that a potential named entity is a correct named entity. system has assigned the named entities is likely to be useful in a range of different applications. Named entities of relevance to biomedical science include not only genes and proteins but also other chemical substances which can be of interest as drugs, metabolites, nutrients, enzyme cofactors, experimental reagents and in many other roles. We have recently investigated the issue of chemical named entities (Corbett et al., 2007), by compiling a set of manual annotation guidelines, demonstrating 93% interannotator agreement and manually annotating a set of 42 chemistry papers. In this paper we demonstrate a named entity recogniser that assigns a confidence score to each named entity, allowing it to be tuned for high precision or recall. Our review of the methods of chemical named entity recognition showed a consistent theme: the use of character-based n-Grams to identify chemical names via their constituent substrings (Wilbur et al., 1999; Vasserman, 2004; Townsend et al., 2005). This can be a powerful technique, due </context>
<context position="10086" citStr="Corbett et al., 2007" startWordPosition="1603" endWordPosition="1607">first-best CRF. Similarly, Yoshida and Tsujii (2007) use an nbest MEMM to generate multiple analyses for a sentence, and re-rank the analyses based on information extracted from neighbouring sentences. Therefore, to explore the potential of these techniques, we have produced a chemical NER system that uses a MEMM for confidence-based extraction of named entities, with an emphasis on the use of character-level n-Grams, and a rescoring system. 2 Corpus Previously, we have produced a set of annotation guidelines for chemical named entities, and used them to annotate a set of 42 chemistry papers (Corbett et al., 2007). Inter-annotator agreement was tested on 14 of these, and found to be 93%. The annotation guidelines specified five classes of named entity, which are detailed in Table 1. The annotation was performed on untokenised text. To test the applicability of the method to a different corpus, we retrieved 500 PubMed abstracts and titles, and annotated them using the same methods. The abstracts were acquired using the query metabolism[Mesh] AND drug AND hasabstract. This produced a diverse set of abstracts spanning a wide range of subject areas, but which contain a higher proportion of relevant terms t</context>
<context position="12782" citStr="Corbett et al., 2007" startWordPosition="2072" endWordPosition="2075">mical element names and symbols was also compiled. To overcome the shortage of entities of type ASE, a list of words from enzyme names 2This dictionary was chosen as it contains inflectional forms of English words. Our system does not perform stemming, partly because suffixes are often good cues as to whether a word is chemical or not. 56 ending in ‘-ase’ was extracted from the Gene Ontology (GO), and hand sorted into words of type ASE, and words not of type ASE. 3.2 Overview of operation The text is tokenised before processing; this is done using the tokeniser described in our previous work (Corbett et al., 2007), which is adapted to chemical text. Our system uses three groups of classifiers to recognise chemical names. The first classifier—the ‘preclassifier’—uses character-level n-grams to estimate the probabilities of whether tokens are chemical or not. The output of this classification is combined with information from the suffix of the word, and is used to provide features for the MEMM. The second group of classifiers constitute the MEMM proper. Named entities are represented using an BIO-encoding, and methods analogous to other confidence-based taggers (Culotta and McCallum, 2004; Carpenter, 200</context>
<context position="26314" citStr="Corbett et al. (2007)" startWordPosition="4348" endWordPosition="4351">lassifier are also disabled. Finally, in “No n-Grams”, the 1-, 2- , 3- and 4-grams used directly by the MEMM are also disabled, showing the results of using a system where no character-level n-grams are used at all. These modifications apply successively—for example, in the “No n-Grams” case the rescorer and preclassifier are also disabled. These results validate the the cascade of classifiers, and underline the importance of character-level n-grams in chemical NER. We also show comparisons to an HMM-based approach, based on LingPipe 3.4.0.6 This is essentially the same system as described by Corbett et al. (2007), but operating in a confidence-based mode. The HMMs used make use of character-level n-Grams, but do not allow the use of the rich feature set used by the MEMM. The line “Customised LingPipe HMM” shows the system using the custom tokenisation and ChEBI-derived dictionary used in the MEMM system, whereas the “Pure LingPipe HMM” shows the system used with the default tokeniser and no external dictionaries. In the region where precision is roughly equal to recall (mimicking the operation of a first-best system), the fact that the MEMM-based system outperforms an HMM is no surprise. However, it i</context>
</contexts>
<marker>Corbett, Batchelor, Teufel, 2007</marker>
<rawString>Peter Corbett, Colin Batchelor and Simone Teufel. 2007. Annotation of Chemical Named Entities BioNLP 2007: Biological, translational, and clinical language processing, pp 57-64. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Confidence Estimation for Information Extraction</title>
<date>2004</date>
<booktitle>Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>109--112</pages>
<location>Boston, MA.</location>
<contexts>
<context position="4775" citStr="Culotta and McCallum, 2004" startWordPosition="742" endWordPosition="745">ence. In n-best operation, the n best sequences for the sentence are identified, along with their probabilities, for example by coupling the Viterbi algorithm with A* search. In confidence-based operation, potential entities (with a probability above a threshold) are identified directly, without directly seeking a single optimal labelling for the entire sentence. This is done by examining the probability of the label transitions within the entity, and the forward and backward probabilities at the start and end of the entity. This mode has been termed the Constrained ForwardBackward algorithm (Culotta and McCallum, 2004). Where a single unambiguous non-overlapping labelling is required, it can be obtained by identifying cases where the entities overlap, and discarding those with lower probabilities. Confidence-based extraction has two main advantages. First, it enables the balance between precision and recall to be controlled by varying the probability threshold. Second, confidence-based NER avoids over-commitment in systems where it is used as a preprocessor, since multiple overlapping options can be used as input to later components. The optimum balance between recall and precision depends on the applicatio</context>
<context position="8328" citStr="Culotta and McCallum, 2004" startWordPosition="1315" endWordPosition="1318">d system is shown in the editing workflows used by the Royal Society of Chemistry in their Project Prospect system (Batchelor and Corbett, 2007), where chemical named entity recognition is used to produce semantically-enriched journal articles. In this situation, high recall is desirable, as false positives can be removed in two ways; by removing entities where a chemical structure cannot be assigned, and by having them checked by a technical editor. False negatives are harder to correct. The use of confidence-based recognition has been demonstrated with CRFs in the domain of contact details (Culotta and McCallum, 2004), and using HMMs in the domain of gene annotation (Carpen55 ter, 2007). In the latter case, the LingPipe toolkit was used in the BioCreative 2 evaluation without significant adaptation. Although only 54% precision was achieved at 60% recall (the best systems were achieving precision and recall scores in the high eighties), the system was capable of 99.99% recall with 7% precision, and 95% recall with 18% precision, indicating that very high recall could be obtained in this difficult domain. Another potential use of confidence-based NER is the potential to rescore named entities. In this approa</context>
<context position="13366" citStr="Culotta and McCallum, 2004" startWordPosition="2161" endWordPosition="2165">n our previous work (Corbett et al., 2007), which is adapted to chemical text. Our system uses three groups of classifiers to recognise chemical names. The first classifier—the ‘preclassifier’—uses character-level n-grams to estimate the probabilities of whether tokens are chemical or not. The output of this classification is combined with information from the suffix of the word, and is used to provide features for the MEMM. The second group of classifiers constitute the MEMM proper. Named entities are represented using an BIO-encoding, and methods analogous to other confidence-based taggers (Culotta and McCallum, 2004; Carpenter, 2007) are used to estimate the conditional probability of tag sequences corresponding to named entities. The result of this is a list of potential named entities, with start positions, end positions, types and probabilities, where all of the probabilities are above a threshold value. A small set of hand-written filtering rules is used to remove obvious absurdities, such as named entities ending in the word “the”, and simple violations of the annotation guidelines, such as named entities of type ASE that contain whitespace. These filtering rules make very little difference at recal</context>
</contexts>
<marker>Culotta, McCallum, 2004</marker>
<rawString>Aron Culotta and Andrew McCallum 2004. Confidence Estimation for Information Extraction Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pp 109-112. Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kirill Degtyarenko</author>
<author>Paula de Matos</author>
<author>Marcus Ennis</author>
<author>Janna Hastings</author>
<author>Martin Zbinden</author>
<author>Alan McNaught</author>
<author>Rafael Alcantara</author>
<author>Michael Darsow</author>
</authors>
<title>Mickael Guedj and Michael Ashburner.</title>
<date>2008</date>
<journal>Nucleic Acids Res,</journal>
<volume>36</volume>
<note>Database issue D344-D350.</note>
<marker>Degtyarenko, de Matos, Ennis, Hastings, Zbinden, McNaught, Alcantara, Darsow, 2008</marker>
<rawString>Kirill Degtyarenko, Paula de Matos, Marcus Ennis, Janna Hastings, Martin Zbinden, Alan McNaught, Rafael Alcantara, Michael Darsow, Mickael Guedj and Michael Ashburner. 2008. ChEBI: a database and ontology for chemical entities of biological interest. Nucleic Acids Res, Vol. 36, Database issue D344-D350.</rawString>
</citation>
<citation valid="false">
<title>The Gene Ontology Consortium 2000. Gene Ontology: tool for the unification of biology.</title>
<journal>Nature Genetics,</journal>
<volume>25</volume>
<pages>26--29</pages>
<marker></marker>
<rawString>The Gene Ontology Consortium 2000. Gene Ontology: tool for the unification of biology. Nature Genetics, Vol. 25, 26-29.</rawString>
</citation>
<citation valid="true">
<date>2007</date>
<booktitle>Proceedings of the Second BioCreative Challenge Evaluation Workshop.</booktitle>
<editor>Martin Krallinger and Lynette Hirschman, editors.</editor>
<contexts>
<context position="9517" citStr="(2007)" startWordPosition="1513" endWordPosition="1513">s. In this approach, the NER system is run, generating a set of named entities. Information obtained about these entities throughout the document (or corpus) that they occur in can then be used in further classifiers. We are not aware of examples of rescoring being applied to confidence-based NER, but there are precedents using other modes of operations. For example, Krishnan and Manning (2006) describe a system where a first-best CRF is used to analyse a corpus, the results of which are then used to generate additional features to use in a second first-best CRF. Similarly, Yoshida and Tsujii (2007) use an nbest MEMM to generate multiple analyses for a sentence, and re-rank the analyses based on information extracted from neighbouring sentences. Therefore, to explore the potential of these techniques, we have produced a chemical NER system that uses a MEMM for confidence-based extraction of named entities, with an emphasis on the use of character-level n-Grams, and a rescoring system. 2 Corpus Previously, we have produced a set of annotation guidelines for chemical named entities, and used them to annotate a set of 42 chemistry papers (Corbett et al., 2007). Inter-annotator agreement was</context>
<context position="26314" citStr="(2007)" startWordPosition="4351" endWordPosition="4351">lso disabled. Finally, in “No n-Grams”, the 1-, 2- , 3- and 4-grams used directly by the MEMM are also disabled, showing the results of using a system where no character-level n-grams are used at all. These modifications apply successively—for example, in the “No n-Grams” case the rescorer and preclassifier are also disabled. These results validate the the cascade of classifiers, and underline the importance of character-level n-grams in chemical NER. We also show comparisons to an HMM-based approach, based on LingPipe 3.4.0.6 This is essentially the same system as described by Corbett et al. (2007), but operating in a confidence-based mode. The HMMs used make use of character-level n-Grams, but do not allow the use of the rich feature set used by the MEMM. The line “Customised LingPipe HMM” shows the system using the custom tokenisation and ChEBI-derived dictionary used in the MEMM system, whereas the “Pure LingPipe HMM” shows the system used with the default tokeniser and no external dictionaries. In the region where precision is roughly equal to recall (mimicking the operation of a first-best system), the fact that the MEMM-based system outperforms an HMM is no surprise. However, it i</context>
</contexts>
<marker>2007</marker>
<rawString>Martin Krallinger and Lynette Hirschman, editors. 2007. Proceedings of the Second BioCreative Challenge Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay Krishnan</author>
<author>Christopher D Manning</author>
</authors>
<title>An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition.</title>
<date>2006</date>
<booktitle>Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1121--1128</pages>
<publisher>Sindey, Australia.</publisher>
<contexts>
<context position="9308" citStr="Krishnan and Manning (2006)" startWordPosition="1473" endWordPosition="1476">.99% recall with 7% precision, and 95% recall with 18% precision, indicating that very high recall could be obtained in this difficult domain. Another potential use of confidence-based NER is the potential to rescore named entities. In this approach, the NER system is run, generating a set of named entities. Information obtained about these entities throughout the document (or corpus) that they occur in can then be used in further classifiers. We are not aware of examples of rescoring being applied to confidence-based NER, but there are precedents using other modes of operations. For example, Krishnan and Manning (2006) describe a system where a first-best CRF is used to analyse a corpus, the results of which are then used to generate additional features to use in a second first-best CRF. Similarly, Yoshida and Tsujii (2007) use an nbest MEMM to generate multiple analyses for a sentence, and re-rank the analyses based on information extracted from neighbouring sentences. Therefore, to explore the potential of these techniques, we have produced a chemical NER system that uses a MEMM for confidence-based extraction of named entities, with an emphasis on the use of character-level n-Grams, and a rescoring syste</context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>Vijay Krishnan and Christopher D. Manning. 2006. An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, 1121-1128. Sindey, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="3387" citStr="Lafferty et al., 2001" startWordPosition="527" endWordPosition="530">ystematic and semisystematic chemical names and additional conventions in drug names. However this technique does not cover all aspects of chemical nomenclature. Much current named entity work uses approaches which combine the structured prediction abilities of HMMs and their derivatives with techniques which enable the use of large, diverse feature sets such as maximum entropy (also known as logistic regression). Maximum Entropy Markov Models, (MEMMs) (McCallum et al., 2000) provide a relatively simple framework for this. MEMMs do have a theoretical weakness, namely the “label bias” problem (Lafferty et al., 2001), which has been ad54 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 54–62, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics dressed with the development of Conditional Random Fields (CRFs). CRFs are now a mainstay of the field, being used in a high proportion of entries in the latest BioCreative evaluation (Krallinger and Hirschman, 2007). However, despite the label bias problem, MEMMs still attract interest due to practical advantages such as shorter training cycles. The framework of HMMs and their successors offers three modes of</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the Eighteenth International Conference on Machine Learning, 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum Entropy Markov Models for Information Extraction and Segmentation</title>
<date>2000</date>
<booktitle>Proceedings of the Seventeenth International Conference on Machine Learning,</booktitle>
<pages>591--598</pages>
<location>San Fransisco, CA.</location>
<contexts>
<context position="3245" citStr="McCallum et al., 2000" startWordPosition="505" endWordPosition="508">mes via their constituent substrings (Wilbur et al., 1999; Vasserman, 2004; Townsend et al., 2005). This can be a powerful technique, due to systematic and semisystematic chemical names and additional conventions in drug names. However this technique does not cover all aspects of chemical nomenclature. Much current named entity work uses approaches which combine the structured prediction abilities of HMMs and their derivatives with techniques which enable the use of large, diverse feature sets such as maximum entropy (also known as logistic regression). Maximum Entropy Markov Models, (MEMMs) (McCallum et al., 2000) provide a relatively simple framework for this. MEMMs do have a theoretical weakness, namely the “label bias” problem (Lafferty et al., 2001), which has been ad54 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 54–62, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics dressed with the development of Conditional Random Fields (CRFs). CRFs are now a mainstay of the field, being used in a high proportion of entries in the latest BioCreative evaluation (Krallinger and Hirschman, 2007). However, despite the label bias problem, MEMMs still </context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag and Fernando Pereira. 2000. Maximum Entropy Markov Models for Information Extraction and Segmentation Proceedings of the Seventeenth International Conference on Machine Learning, 591-598. San Fransisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joe A Townsend</author>
<author>Ann Copestake</author>
<author>Peter Murray-Rust</author>
<author>Simone H Teufel</author>
<author>Christopher A Waudby</author>
</authors>
<date>2005</date>
<booktitle>Language Technology for Processing Chemistry Publications Proceedings of the fourth UK e-Science All Hands Meeting,</booktitle>
<pages>247--253</pages>
<location>Nottingham, UK.</location>
<contexts>
<context position="2721" citStr="Townsend et al., 2005" startWordPosition="426" endWordPosition="429">ed the issue of chemical named entities (Corbett et al., 2007), by compiling a set of manual annotation guidelines, demonstrating 93% interannotator agreement and manually annotating a set of 42 chemistry papers. In this paper we demonstrate a named entity recogniser that assigns a confidence score to each named entity, allowing it to be tuned for high precision or recall. Our review of the methods of chemical named entity recognition showed a consistent theme: the use of character-based n-Grams to identify chemical names via their constituent substrings (Wilbur et al., 1999; Vasserman, 2004; Townsend et al., 2005). This can be a powerful technique, due to systematic and semisystematic chemical names and additional conventions in drug names. However this technique does not cover all aspects of chemical nomenclature. Much current named entity work uses approaches which combine the structured prediction abilities of HMMs and their derivatives with techniques which enable the use of large, diverse feature sets such as maximum entropy (also known as logistic regression). Maximum Entropy Markov Models, (MEMMs) (McCallum et al., 2000) provide a relatively simple framework for this. MEMMs do have a theoretical</context>
<context position="17328" citStr="Townsend et al. (2005)" startWordPosition="2823" endWordPosition="2826"> that are compiled are chemical word tokens (those that only appear inside named entities), nonchemical word tokens (those that do not appear in entities), chemical nonword tokens, nonchemical nonword tokens and ambigu57 ous tokens—those that occur both inside and outside of named entities. A few other minor sets are collected to deal with tokens related to such proper noun-containing entities as ‘Diels–Alder reaction’. Some of this data is combined with external dictionaries to train the preclassifier, which works using 4-grams of characters and modified Kneser-Ney smoothing, as described by Townsend et al. (2005). The set of ‘chemical word tokens’ is used as a set of positive examples, along with tokens extracted from ChEBI, a list of element names and symbols, and the ASE tokens extracted from the GO. The negative examples used are the extracted ‘nonchemical word tokens’, the non-ASE tokens from the GO and tokens taken from the English dictionary—except for those that were listed as positive examples. This gets around the problem that the English dictionary contains the names of all of the elements and a number of simple compounds such as ‘ethanol’. During operation, n-gram analysis is used to calcul</context>
</contexts>
<marker>Townsend, Copestake, Murray-Rust, Teufel, Waudby, 2005</marker>
<rawString>Joe A. Townsend, Ann Copestake, Peter Murray-Rust, Simone H. Teufel and Christopher A. Waudby. 2005. Language Technology for Processing Chemistry Publications Proceedings of the fourth UK e-Science All Hands Meeting, 247-253. Nottingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Vasserman</author>
</authors>
<title>Identifying Chemical Names in Biomedial Text: An Investigation of the Substring Co-occurence Based Approaches</title>
<date>2004</date>
<booktitle>Proceedings of the Student Research Workshop at HLT-NAACL</booktitle>
<contexts>
<context position="2697" citStr="Vasserman, 2004" startWordPosition="424" endWordPosition="425">cently investigated the issue of chemical named entities (Corbett et al., 2007), by compiling a set of manual annotation guidelines, demonstrating 93% interannotator agreement and manually annotating a set of 42 chemistry papers. In this paper we demonstrate a named entity recogniser that assigns a confidence score to each named entity, allowing it to be tuned for high precision or recall. Our review of the methods of chemical named entity recognition showed a consistent theme: the use of character-based n-Grams to identify chemical names via their constituent substrings (Wilbur et al., 1999; Vasserman, 2004; Townsend et al., 2005). This can be a powerful technique, due to systematic and semisystematic chemical names and additional conventions in drug names. However this technique does not cover all aspects of chemical nomenclature. Much current named entity work uses approaches which combine the structured prediction abilities of HMMs and their derivatives with techniques which enable the use of large, diverse feature sets such as maximum entropy (also known as logistic regression). Maximum Entropy Markov Models, (MEMMs) (McCallum et al., 2000) provide a relatively simple framework for this. MEM</context>
</contexts>
<marker>Vasserman, 2004</marker>
<rawString>Alexander Vasserman 2004 Identifying Chemical Names in Biomedial Text: An Investigation of the Substring Co-occurence Based Approaches Proceedings of the Student Research Workshop at HLT-NAACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John Wilbur</author>
<author>George F Hazard</author>
<author>Guy Divita</author>
<author>James G Mork</author>
<author>Alan R Aronson</author>
<author>Allen C Browne</author>
</authors>
<title>Analysis of Biomedical Text for Chemical Names: A Comparison of Three Methods</title>
<date>1999</date>
<booktitle>Proc. AMIA Symp.</booktitle>
<pages>176--180</pages>
<contexts>
<context position="2680" citStr="Wilbur et al., 1999" startWordPosition="420" endWordPosition="423">her roles. We have recently investigated the issue of chemical named entities (Corbett et al., 2007), by compiling a set of manual annotation guidelines, demonstrating 93% interannotator agreement and manually annotating a set of 42 chemistry papers. In this paper we demonstrate a named entity recogniser that assigns a confidence score to each named entity, allowing it to be tuned for high precision or recall. Our review of the methods of chemical named entity recognition showed a consistent theme: the use of character-based n-Grams to identify chemical names via their constituent substrings (Wilbur et al., 1999; Vasserman, 2004; Townsend et al., 2005). This can be a powerful technique, due to systematic and semisystematic chemical names and additional conventions in drug names. However this technique does not cover all aspects of chemical nomenclature. Much current named entity work uses approaches which combine the structured prediction abilities of HMMs and their derivatives with techniques which enable the use of large, diverse feature sets such as maximum entropy (also known as logistic regression). Maximum Entropy Markov Models, (MEMMs) (McCallum et al., 2000) provide a relatively simple framew</context>
</contexts>
<marker>Wilbur, Hazard, Divita, Mork, Aronson, Browne, 1999</marker>
<rawString>W. John Wilbur, George F. Hazard, Jr., Guy Divita, James G. Mork, Alan R. Aronson and Allen C. Browne. 1999 Analysis of Biomedical Text for Chemical Names: A Comparison of Three Methods Proc. AMIA Symp. 176-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuhiro Yoshida</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Reranking for Biomedical Named-Entity Recognition BioNLP</title>
<date>2007</date>
<pages>57--64</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9517" citStr="Yoshida and Tsujii (2007)" startWordPosition="1510" endWordPosition="1513">score named entities. In this approach, the NER system is run, generating a set of named entities. Information obtained about these entities throughout the document (or corpus) that they occur in can then be used in further classifiers. We are not aware of examples of rescoring being applied to confidence-based NER, but there are precedents using other modes of operations. For example, Krishnan and Manning (2006) describe a system where a first-best CRF is used to analyse a corpus, the results of which are then used to generate additional features to use in a second first-best CRF. Similarly, Yoshida and Tsujii (2007) use an nbest MEMM to generate multiple analyses for a sentence, and re-rank the analyses based on information extracted from neighbouring sentences. Therefore, to explore the potential of these techniques, we have produced a chemical NER system that uses a MEMM for confidence-based extraction of named entities, with an emphasis on the use of character-level n-Grams, and a rescoring system. 2 Corpus Previously, we have produced a set of annotation guidelines for chemical named entities, and used them to annotate a set of 42 chemistry papers (Corbett et al., 2007). Inter-annotator agreement was</context>
</contexts>
<marker>Yoshida, Tsujii, 2007</marker>
<rawString>Kazuhiro Yoshida and Jun’ichi Tsujii. 2007. Reranking for Biomedical Named-Entity Recognition BioNLP 2007: Biological, translational, and clinical language processing, pp 57-64. Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>