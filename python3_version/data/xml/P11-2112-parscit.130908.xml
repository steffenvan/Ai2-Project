<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019885">
<title confidence="0.9987555">
Learning Condensed Feature Representations from Large Unsupervised
Data Sets for Supervised Learning
</title>
<author confidence="0.908118">
Jun Suzuki, Hideki Isozaki, and Masaaki Nagata
</author>
<affiliation confidence="0.729486">
NTT Communication Science Laboratories, NTT Corp.
</affiliation>
<address confidence="0.800355">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
</address>
<email confidence="0.998413">
{suzuki.jun, isozaki.hideki, nagata.masaaki}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.9939" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999794666666667">
This paper proposes a novel approach for ef-
fectively utilizing unsupervised data in addi-
tion to supervised data for supervised learn-
ing. We use unsupervised data to gener-
ate informative ‘condensed feature represen-
tations’ from the original feature set used in
supervised NLP systems. The main con-
tribution of our method is that it can of-
fer dense and low-dimensional feature spaces
for NLP tasks while maintaining the state-of-
the-art performance provided by the recently
developed high-performance semi-supervised
learning technique. Our method matches the
results of current state-of-the-art systems with
very few features, i.e., F-score 90.72 with
344 features for CoNLL-2003 NER data, and
UAS 93.55 with 12.5K features for depen-
dency parsing data derived from PTB-III.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988583333333">
In the last decade, supervised learning has become
a standard way to train the models of many natural
language processing (NLP) systems. One simple but
powerful approach for further enhancing the perfor-
mance is to utilize a large amount of unsupervised
data to supplement supervised data. Specifically,
an approach that involves incorporating ‘clustering-
based word representations (CWR)’ induced from
unsupervised data as additional features of super-
vised learning has demonstrated substantial perfor-
mance gains over state-of-the-art supervised learn-
ing systems in typical NLP tasks, such as named en-
tity recognition (Lin and Wu, 2009; Turian et al.,
2010) and dependency parsing (Koo et al., 2008).
We refer to this approach as the iCWR approach,
The iCWR approach has become popular for en-
hancement because of its simplicity and generality.
The goal of this paper is to provide yet another
simple and general framework, like the iCWR ap-
proach, to enhance existing state-of-the-art super-
vised NLP systems. The differences between the
iCWR approach and our method are as follows; sup-
pose F is the original feature set used in supervised
learning, C is the CWR feature set, and H is the new
feature set generated by our method. Then, with the
iCWR approach, C is induced independently from
F, and used in addition to F in supervised learning,
i.e., F U C. In contrast, in our method H is directly
induced from F with the help of an existing model
already trained by supervised learning with F, and
used in place of F in supervised learning.
The largest contribution of our method is that
it offers an architecture that can drastically reduce
the number of features, i.e., from 10M features
in F to less than 1K features in H by construct-
ing ‘condensed feature representations (COFER)’,
which is a new and very unique property that can-
not be matched by previous semi-supervised learn-
ing methods including the iCWR approach. One
noteworthy feature of our method is that there is no
need to handle sparse and high-dimensional feature
spaces often used in many supervised NLP systems,
which is one of the main causes of the data sparse-
ness problem often encountered when we learn the
model with a supervised leaning algorithm. As a
result, NLP systems that are both compact and high-
performance can be built by retraining the model
with the obtained condensed feature set H.
</bodyText>
<sectionHeader confidence="0.987744" genericHeader="method">
2 Condensed Feature Representations
</sectionHeader>
<bodyText confidence="0.976241333333333">
Let us first define the condensed feature set H. In
this paper, we call the feature set generally used in
supervised learning, F, the original feature set. Let
N and M represent the numbers of features in F and
H, respectively. We assume M &lt;N, and generally
M «N. A condensed feature hm EH is charac-
</bodyText>
<page confidence="0.995517">
636
</page>
<note confidence="0.8929155">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 636–641,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.992451">
Figure 1: Outline of our method to construct a condensed
feature set.
</figureCaption>
<equation confidence="0.9988585">
�r(x) = X r(x, y)/|Y(x)|.
yEY(x)
VD� (fn) = X fn(x, Y)(r(x, Y) − r(x))
xED
V D � (fn) = − X X fn(x, y)(r(x, y) − r(x))
xED yEY(x)\Y
</equation>
<figureCaption confidence="0.990716">
Figure 2: Notations used in this paper.
</figureCaption>
<bodyText confidence="0.999642545454546">
terized as a set of features in F, that is, hm = Sm
where Sm ⊆ F. We assume that each original fea-
ture fn ∈F maps, at most, to one condensed feature
hm. This assumption prevents two condensed fea-
tures from containing the same original feature, and
some original features from not being mapped to any
condensed feature. Namely, Sm ∩ Sm, =∅ for all m
and m&apos;, where m6=m&apos;, and ∪Mm=1 Sm ⊆F hold.
The value of each condensed feature is calcu-
lated by summing the values of the original fea-
tures assigned to it. Formally, let X and Y repre-
sent the sets of all possible inputs and outputs of
a target task, respectively. Let x ∈ X be an in-
put, and y ∈ Y(x) be an output, where Y(x) ⊆ Y
represents the set of possible outputs given x. We
write the n-th feature function of the original fea-
tures, whose value is determined by x and y, as
fn(x, y), where n ∈ {1, ... , N}. Similarly, we
write the m-th feature function of the condensed fea-
tures as hm(x, y), where m∈{1, ... , M}. We state
that the value of hm(x, y) is calculated as follows:
hm(x ,y)=∑fnESm fn(x,y).
</bodyText>
<sectionHeader confidence="0.96385" genericHeader="method">
3 Learning COFERs
</sectionHeader>
<bodyText confidence="0.999893">
The remaining part of our method consists of the
way to map the original features into the condensed
features. For this purpose, we define the feature po-
tency, which is evaluated by employing an existing
supervised model with unsupervised data sets. Fig-
ure 1 shows a brief sketch of the process to construct
the condensed features described in this section.
</bodyText>
<subsectionHeader confidence="0.999463">
3.1 Self-taught-style feature potency estimation
</subsectionHeader>
<bodyText confidence="0.998885">
We assume that we have a model trained by super-
vised learning, which we call the ‘base supervised
model’, and the original feature set F that is used
in the base supervised model. We consider a case
where the base supervised model is a (log-)linear
model, and use the following equation to select the
best output y� given x:
</bodyText>
<equation confidence="0.9998135">
y� = arg max N
yEY(x) ∑n=1 wnfn(x,y), (1)
</equation>
<bodyText confidence="0.982103147058824">
where wn is a model parameter (or weight) of fn.
Linear models are currently the most widely-used
models and are employed in many NLP systems.
To simplify the explanation, we define function
r(x, y), where r(x, y) returns 1 if y = y� is obtained
from the base supervised model given x, and 0 oth-
erwise. Let r(x) represent the average of r(x, y) in
x (see Figure 2 for details). We also define VD� (fn)
and VD− (fn) as shown in Figure 2 where D repre-
sents the unsupervised data set. VD� (fn) measures
the positive correlation with the best output y� given
by the base supervised model since this is the sum-
mation of all the (weighted) feature values used in
the estimation of the one best output y� over all x in
the unsupervised data D. Similarly, VD− (fn) mea-
sures the negative correlation with y. Next, we de-
fine
(VD(fn) as (the feature potency of fn: VD (fn) =
VD (fn) − VD (fn).
An intuitive explanation of VD(fn) is as follows;
if |VD(fn) |is large, the distribution of fn has either
a large positive or negative correlation with the best
output y� given by the base supervised model. This
implies that fn is an informative and potent feature
in the model. Then, the distribution of fn has very
small (or no) correlation to determine y� if |VD(fn)|
is zero or near zero. In this case, fn can be evaluated
as an uninformative feature in the model. From this
perspective, we treat VD(fn) as a measure of feature
potency in terms of the base supervised model.
The essence of this idea, evaluating features
against each other on a certain model, is widely
used in the context of semi-supervised learning,
i.e., (Ando and Zhang, 2005; Suzuki and Isozaki,
</bodyText>
<figure confidence="0.984152921052632">
Feature potency
F~ f
Section 3.2: Feature potency discounting
Features mapped into this area will be zeroed
by the effect of C
0
0
Section 3.3: Feature potency quantization
Feature potency
Feu f
Potencies are multiplied by a positive constant δ
( Integer Space N )
Features mapped into zero are discarded and
never mapped into any condensed features
Feature potency
F∗n f
3 4
0 1 2
-1
-2
Section 3.4: Condensed feature construction
u. 11
Condensed feature set
9
3/e
φ
(Quantized feature potency)
M (e.g., M=1K)
1/e
1/e
The potencies are also utilized as
an (M+1)-th condensed feature
-2/e
N (e.g., N=100M)
Original feature set
Section 3.1: Feature potency estimation
-C
C
</figure>
<bodyText confidence="0.6137705">
Each condensed feature is represented as a set
of features in the original feature set F.
</bodyText>
<equation confidence="0.985698666666667">
XRn=
xED
X X
r(x, y)fn(x, y), An=
yEY(x)
X
�r(x)
fn(x, y)
xED yEY(x)
</equation>
<page confidence="0.961371">
637
</page>
<bodyText confidence="0.999686111111111">
2008; Druck and McCallum, 2010). Our method
is rough and a much simpler framework for imple-
menting this fundamental idea of semi-supervised
learning developed for NLP tasks. We create a
simple framework to achieve improved flexibility,
extendability, and applicability. In fact, we apply
the framework by incorporating a feature merging
and elimination architecture to obtain effective con-
densed feature sets for supervised learning.
</bodyText>
<subsectionHeader confidence="0.999878">
3.2 Feature potency discounting
</subsectionHeader>
<bodyText confidence="0.999789">
To discount low potency values, we redefine feature
potency as V0D(fn) instead of VD(fn) as follows:
</bodyText>
<equation confidence="0.993127">
V0D(fn) = { log [Rn+C]−log[An] if Rn−An&lt;−C
0 if − C &lt;Rn−An&lt;C
log [Rn−C]−log[An] if C &lt;Rn−An
</equation>
<bodyText confidence="0.999400111111111">
where Rn and An are defined in Figure 2. Note
that VD(fn) = VD + (fn) − VD � (fn) = Rn − An.
The difference from VD(fn) is that we cast it in the
log-domain and introduce a non-negative constant
C. The introduction of C is inspired by the L1-
regularization technique used in supervised learning
algorithms such as (Duchi and Singer, 2009; Tsu-
ruoka et al., 2009). C controls how much we dis-
count VD(fn) toward zero, and is given by the user.
</bodyText>
<subsectionHeader confidence="0.999289">
3.3 Feature potency quantization
</subsectionHeader>
<bodyText confidence="0.999062888888889">
We define VD(fn) as VD(fn) = F6V0D(fn)1 if
V0D(fn)&gt; 0 and VD(fn) = L6V0D(fn)J otherwise,
where 6 is a positive user-specified constant. Note
that VD(fn) always becomes an integer, that is,
V*D(fn) E N where N = {..., −2,−1,0,1,2, ...}.
This calculation can be seen as mapping each fea-
ture into a discrete (integer) space with respect to
V0D(fn). 6 controls the range of V0D(fn) mapping
into the same integer.
</bodyText>
<subsectionHeader confidence="0.994328">
3.4 Condensed feature construction
</subsectionHeader>
<bodyText confidence="0.999943542857143">
Suppose we have M different quantized feature po-
tency values in V � D(fn) for all n, which we rewrite
as {um}M m=1. Then, we define 5m as a set of fn
whose quantized feature potency value is um. As
described in Section 2, we define the m-th con-
densed feature hm(x, y) as the summation of all
the original features fn assigned to 5m. That is,
hm(x, y) = E fn,Sr fn(x, y). This feature fusion
process is intuitive since it is acceptable if features
with the same (similar) feature potency are given the
same weight by supervised learning since they have
the same potency with regard to determining y. 6
determines the number of condensed features to be
made; the number of condensed features becomes
large if 6 is large. Obviously, the upper bound of
the number of condensed features is the number of
original features.
To exclude possibly unnecessary original features
from the condensed features, we discard feature fn
for all n if un = 0. This is reasonable since, as de-
scribed in Section 3.1, a feature has small (or no)
effect in achieving the best output decision in the
base supervised model if its potency is near 0. C in-
troduced in Section 3.2 mainly influences how many
original features are discarded.
Additionally, we also utilize the ‘quantized’ fea-
ture potency values themselves as a new feature.
The reason behind is that they are also very infor-
mative for supervised learning. Their use is impor-
tant to further boost the performance gain offered
by our method. For this purpose, we define 0(x, y)
as 0(x, y) = EMm=1(um/6)hm(x, y). We then
use 0(x, y) as the (M + 1)-th feature of our con-
densed feature set. As a result, the condensed fea-
ture set obtained with our method is represented as
</bodyText>
<equation confidence="0.572838">
R = {h1(x, y),. . . , hM(x, y), 0(x, y)}.
</equation>
<bodyText confidence="0.999449181818182">
Note that the calculation cost of 0(x, y) is negli-
gible. We can calculate the linear discriminant func-
tion 9(x, y) as: 9(x, y) = EMm=1 wmhm(x, y) +
wM+10(x, y) = EMm=1 w�mhm(x, y), where w&apos; =
(wm + wM+1um/6). We emphasize that once
{wm}M+1
m=1 are determined by supervised learning,
we can calculate w&apos;m in a preliminary step before
the test phase. Thus, our method also takes the form
of a linear model. The number of features for our
method is essentially M even if we add 0.
</bodyText>
<subsectionHeader confidence="0.986471">
3.5 Application to Structured Prediction Tasks
</subsectionHeader>
<bodyText confidence="0.999966666666667">
We modify our method to better suit structured pre-
diction problems in terms of calculation cost. For a
structured prediction problem, it is usual to decom-
pose or factorize output structure y into a set of lo-
cal sub-structures z to reduce the calculation cost
and to cope with the sparsity of the output space
Y. This factorization can be accomplished by re-
stricting features that are extracted only from the in-
formation within decomposed local sub-structure z
</bodyText>
<page confidence="0.994743">
638
</page>
<bodyText confidence="0.999429545454545">
and given input x. We write z ∈ y when the lo-
cal sub-structure z is a part of output y, assuming
that output y is constructed by a set of local sub-
structures. Then formally, the n-th feature is written
as fn(x, z), and fn(x, y) = ∑zcy fn(x, z) holds.
Similarly, we introduce r(x, z), where r(x, z) = 1
if z ∈ y, and r(x, z) = 0 otherwise, namely z ∈� y.
We define Z(x) as the set of all local sub-
structures possibly generated for all y in Y(x).
Z(x) can be enumerated easily, unless we use typi-
cal first- or second-order factorization models by the
restriction of efficient decoding algorithms, which is
the typical case for many NLP tasks such as named
entity recognition and dependency parsing.
Finally, we replace all Y(x) with Z(x), and use
fn(x, z) and r(x, z) instead of fn(x, y) and r(x, y),
respectively, in Rn and An. When we use these sub-
stitutions, there is no need to incorporate an efficient
algorithm such as dynamic programming into our
method. This means that our feature potency esti-
mation can be applied to the structured prediction
problem at low cost.
</bodyText>
<subsectionHeader confidence="0.992256">
3.6 Efficient feature potency computation
</subsectionHeader>
<bodyText confidence="0.9999729">
Our feature potency estimation described in Section
3.1 to 3.3 is highly suitable for implementation in
the MapReduce framework (Dean and Ghemawat,
2008), which is a modern distributed parallel com-
puting framework. This is because Rn and An can
be calculated by the summation of a data-wise cal-
culation (map phase), and VD(fn) can be calculated
independently by each feature (reduce phase). We
emphasize that our feature potency estimation can
be performed in a ‘single’ map-reduce process.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999528">
We conducted experiments on two different NLP
tasks, namely NER and dependency parsing. To fa-
cilitate comparisons with the performance of previ-
ous methods, we adopted the experimental settings
used to examine high-performance semi-supervised
NLP systems; i.e., NER (Ando and Zhang, 2005;
Suzuki and Isozaki, 2008) and dependency pars-
ing (Koo et al., 2008; Chen et al., 2009; Suzuki
et al., 2009). For the supervised datasets, we used
CoNLL’03 (Tjong Kim Sang and De Meulder, 2003)
shared task data for NER, and the Penn Treebank III
(PTB) corpus (Marcus et al., 1994) for dependency
parsing. We prepared a total of 3.72 billion token
text data as unsupervised data following the instruc-
tions given in (Suzuki et al., 2009).
</bodyText>
<subsectionHeader confidence="0.666432">
4.1 Comparative Methods
</subsectionHeader>
<bodyText confidence="0.999968342105263">
We mainly compare the effectiveness of COFER
with that of CWR derived by the Brown algorithm.
The iCWR approach yields the state-of-the-art re-
sults with both dependency parsing data derived
from PTB-III (Koo et al., 2008), and the CoNLL’03
shared task data (Turian et al., 2010). By compar-
ing COFER with iCWR we can clarify its effective-
ness in terms of providing better features for super-
vised learning. We use the term active features to
refer to features whose corresponding model param-
eter is non-zero after supervised learning. It is well-
known that we can discard non-active features from
the trained model without any loss after finishing su-
pervised learning. Finally, we compared the perfor-
mance in terms of the number of active features in
the model given by supervised learning. We note
here that the number of active features for COFER
is the number of features hm if w&apos;&apos; m = 0, which is
not wm = 0 for a fair comparison.
Unlike COFER, iCWR does not have any archi-
tecture to winnow the original feature set used in
supervised learning. For a fair comparison, we
prepared L1-regularized supervised learning algo-
rithms, which try to reduce the non-zero parameters
in a model. Specifically, we utilized L1-regularized
CRF (L1CRF) optimized by OWL-QN (Andrew
and Gao, 2007) for NER, and the online struc-
tured output learning version of FOBOS (Duchi
and Singer, 2009; Tsuruoka et al., 2009) with L1-
regularization (ostL1FOBOS) for dependency pars-
ing. In addition, we also examined L2 regular-
ized CRF (Lafferty et al., 2001) optimized by L-
BFGS (Liu and Nocedal, 1989) (L2CRF) for NER,
and the online structured output learning version of
the Passive-Aggressive algorithm (ostPA) (Cram-
mer et al., 2006) for dependency parsing to illus-
trate the baseline performance regardless of the ac-
tive feature number.
</bodyText>
<subsectionHeader confidence="0.941849">
4.2 Settings for COFER
</subsectionHeader>
<bodyText confidence="0.997477">
We utilized baseline supervised learning mod-
els as the base supervised models of COFER.
</bodyText>
<page confidence="0.994176">
639
</page>
<figure confidence="0.997481">
(a) NER (F-score) (b) dep. parsing (UAS)
</figure>
<figureCaption confidence="0.999688">
Figure 3: Performance vs. size of active features in the
trained model on the development sets
</figureCaption>
<bodyText confidence="0.999793">
In addition, we also report the results when we
treat iCWR as COFER’s base supervised mod-
els (iCWR+COFER). This is a very natural and
straightforward approach to combining these two.
We generally handle several different types of fea-
tures such as words, part-of-speech tags, word sur-
face forms, and their combinations. Suppose we
have K different feature types, which are often de-
fined by feature templates, i.e., (Suzuki and Isozaki,
2008; Lin and Wu, 2009). In our experiments, we re-
strict the merging of features during the condensed
feature construction process if and only if the fea-
tures are the same feature type. As a result, COFER
essentially consists of K different condensed feature
sets. The numbers of feature types K were 79 and 30
for our NER and dependency parsing experiments,
respectively. We note that this kind of feature par-
tition by their types is widely used in the context of
semi-supervised learning (Ando and Zhang, 2005;
Suzuki and Isozaki, 2008).
</bodyText>
<subsectionHeader confidence="0.987046">
4.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999808384615385">
Figure 3 displays the performance on the develop-
ment set with respect to the number of active fea-
tures in the trained models given by each supervised
learning algorithm. In both NER and dependency
parsing experiments, COFER significantly outper-
formed iCWR. Moreover, COFER was surprisingly
robust in relation to the number of active features
in the model. These results reveal that COFER pro-
vides effective feature sets for certain NLP tasks.
We summarize the noteworthy results in Figure 3,
and also the performance of recent top-line systems
for NER and dependency parsing in Table 1. Over-
all, COFER matches the results of top-line semi-
</bodyText>
<table confidence="0.998254041666667">
NER system dev. test #.USD #.AF
Sup.L1CRF 90.40 85.08 0 0.57M
iCWR: L1CRF 93.33 89.99 3,720M 0.62M
COFER: L1CRF (S = 1e + 00) 93.42 88.81 3,720M 359
(S = 1e + 04) 93.60 89.22 3,720M 2.46M
iCWR+COFER: (S = 1e + 00) 94.39 90.72 3,720M 344
L1CRF (S = 1e + 04) 94.91 91.02 3,720M 5.94M
(Ando and Zhang, 2005) 93.15 89.31 27M N/A
(Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A
(Ratinov and Roth, 2009) 93.50 90.57 N/A N/A
(Turian et al., 2010) 93.95 90.36 37M N/A
(Lin and Wu, 2009) N/A 90.90 700,000M N/A
Dependency parser dev. test #.USD #.AF
ostL1FOBOS 93.15 92.82 0 6.80M
iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M
COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K
(S = 1e + 05) 93.91 93.71 3,720M 3.23M
iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K
ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M
(Koo and Collins, 2010) 93.49 93.04 0 N/A
(Martins et al., 2010) N/A 93.26 0 55.25M
(Koo et al., 2008) 93.30 93.16 43M N/A
(Chen et al., 2009) N/A 93.16 43M N/A
(Suzuki et al., 2009) 94.13 93.79 3,720M N/A
</table>
<tableCaption confidence="0.97471">
Table 1: Comparison with previous top-line systems on
test data. (#.USD: unsupervised data size. #.AF: the size
of active features in the trained model.)
</tableCaption>
<bodyText confidence="0.999670125">
supervised learning systems even though it uses far
fewer active features.
In addition, the combination of iCWR+COFER
significantly outperformed the current best results
by achieving a 0.12 point gain from 90.90 to 91.02
for NER, and a 0.43 point gain from 93.79 to 94.22
for dependency parsing, with only 5.94M and 5.77M
features, respectively.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998824">
This paper introduced the idea of condensed feature
representations (COFER) as a simple and general
framework that can enhance the performance of ex-
isting supervised NLP systems. We also proposed
a method that efficiently constructs condensed fea-
ture sets through discrete feature potency estima-
tion over unsupervised data. We demonstrated that
COFER based on our feature potency estimation can
offer informative dense and low-dimensional feature
spaces for supervised learning, which is theoreti-
cally preferable to the sparse and high-dimensional
feature spaces often used in many NLP tasks. Exist-
ing NLP systems can be made more compact with
higher performance by retraining their models with
our condensed features.
</bodyText>
<figure confidence="0.927758038461538">
1.0E+01 1.0E+03 1.0E+05 1.0E+07 1.0E+09
# of active features [log-scale]
1.E+02 1.E+04 1.E+06 1.E+08
# of active features [log-scale]
96.0
95.0
proposed
8=1e+018=1e+02 8=1e+04
e=1e+03
e=1e+05
proposed
e=1e+01
94.0
94.0
e=1e+00
Unlabeled Attachment Score
93.0
92.0
8=1e+00
F-score
92.0
90.0
91.0
88.0
90.0
86.0
</figure>
<table confidence="0.986236375">
iCWR+COFER: L2CRF iCWR+COFER: L1CRF
COFER: L2CRF COFER: L1CRF
iCWR: L2CRF iCWR: L1CRF
Sup.L2CRF Sup.L1CRF
iCWR+COFER: ostPA iCWR+COFER: ostL1FOBOS
COFER: ostPA COFER: ostL1FOBOS
iCWR: ostPA iCWR: ostL1FOBOS
Sup.ostPA Sup.ostL1FOBOS
</table>
<page confidence="0.996067">
640
</page>
<sectionHeader confidence="0.982584" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999151172043011">
Rie Kubota Ando and Tong Zhang. 2005. A High-
Performance Semi-Supervised Learning Method for
Text Chunking. In Proceedings of 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 1–9.
Galen Andrew and Jianfeng Gao. 2007. Scalable
Training of L1-regularized Log-linear Models. In
Zoubin Ghahramani, editor, Proceedings of the 24th
Annual International Conference on Machine Learn-
ing (ICML 2007), pages 33–40. Omnipress.
Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving Dependency
Parsing with Subtrees from Auto-Parsed Data. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 570–579.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning
Research, 7:551–585.
Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
Simplified Data Processing on Large Clusters. Com-
mun. ACM, 51(1):107–113.
Gregory Druck and Andrew McCallum. 2010. High-
Performance Semi-Supervised Learning using Dis-
criminatively Constrained Generative Models. In Pro-
ceedings of the International Conference on Machine
Learning (ICML 2010), pages 319–326.
John Duchi and Yoram Singer. 2009. Efficient On-
line and Batch Learning Using Forward Backward
Splitting. Journal of Machine Learning Research,
10:2899–2934.
Terry Koo and Michael Collins. 2010. Efficient Third-
Order Dependency Parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1–11.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In Pro-
ceedings of ACL-08: HLT, pages 595–603.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of the International Conference on Ma-
chine Learning (ICML 2001), pages 282–289.
Dekang Lin and Xiaoyun Wu. 2009. Phrase Cluster-
ing for Discriminative Learning. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1030–1038.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimization.
Math. Programming, Ser. B, 45(3):503–528.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo Parsers: Depen-
dency Parsing by Approximate Variational Inference.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34–
44.
Lev Ratinov and Dan Roth. 2009. Design Challenges
and Misconceptions in Named Entity Recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL-2009),
pages 147–155.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
Sequential Labeling and Segmentation Using Giga-
Word Scale Unlabeled Data. In Proceedings of ACL-
08: HLT, pages 665–673.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for Depen-
dency Parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 551–560.
Erik Tjong Kim Sang and Fien De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proceed-
ings of CoNLL-2003, pages 142–147.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic Gradient Descent Training
for L1-regularized Log-linear Models with Cumula-
tive Penalty. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 477–485.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 384–394.
</reference>
<page confidence="0.998333">
641
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.767248">
<title confidence="0.9918725">Learning Condensed Feature Representations from Large Data Sets for Supervised Learning</title>
<author confidence="0.975728">Jun Suzuki</author>
<author confidence="0.975728">Hideki Isozaki</author>
<author confidence="0.975728">Masaaki</author>
<affiliation confidence="0.980549">NTT Communication Science Laboratories, NTT</affiliation>
<address confidence="0.954838">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237</address>
<email confidence="0.98556">isozaki.hideki,</email>
<abstract confidence="0.990672789473684">This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. We use unsupervised data to generate informative ‘condensed feature representations’ from the original feature set used in supervised NLP systems. The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique. Our method matches the results of current state-of-the-art systems with few features, F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A HighPerformance Semi-Supervised Learning Method for Text Chunking.</title>
<date>2005</date>
<booktitle>In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="7745" citStr="Ando and Zhang, 2005" startWordPosition="1323" endWordPosition="1326">tive correlation with the best output y� given by the base supervised model. This implies that fn is an informative and potent feature in the model. Then, the distribution of fn has very small (or no) correlation to determine y� if |VD(fn)| is zero or near zero. In this case, fn can be evaluated as an uninformative feature in the model. From this perspective, we treat VD(fn) as a measure of feature potency in terms of the base supervised model. The essence of this idea, evaluating features against each other on a certain model, is widely used in the context of semi-supervised learning, i.e., (Ando and Zhang, 2005; Suzuki and Isozaki, Feature potency F~ f Section 3.2: Feature potency discounting Features mapped into this area will be zeroed by the effect of C 0 0 Section 3.3: Feature potency quantization Feature potency Feu f Potencies are multiplied by a positive constant δ ( Integer Space N ) Features mapped into zero are discarded and never mapped into any condensed features Feature potency F∗n f 3 4 0 1 2 -1 -2 Section 3.4: Condensed feature construction u. 11 Condensed feature set 9 3/e φ (Quantized feature potency) M (e.g., M=1K) 1/e 1/e The potencies are also utilized as an (M+1)-th condensed fe</context>
<context position="14819" citStr="Ando and Zhang, 2005" startWordPosition="2554" endWordPosition="2557">n distributed parallel computing framework. This is because Rn and An can be calculated by the summation of a data-wise calculation (map phase), and VD(fn) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-ar</context>
<context position="18288" citStr="Ando and Zhang, 2005" startWordPosition="3134" endWordPosition="3137"> we have K different feature types, which are often defined by feature templates, i.e., (Suzuki and Isozaki, 2008; Lin and Wu, 2009). In our experiments, we restrict the merging of features during the condensed feature construction process if and only if the features are the same feature type. As a result, COFER essentially consists of K different condensed feature sets. The numbers of feature types K were 79 and 30 for our NER and dependency parsing experiments, respectively. We note that this kind of feature partition by their types is widely used in the context of semi-supervised learning (Ando and Zhang, 2005; Suzuki and Isozaki, 2008). 4.3 Results and Discussion Figure 3 displays the performance on the development set with respect to the number of active features in the trained models given by each supervised learning algorithm. In both NER and dependency parsing experiments, COFER significantly outperformed iCWR. Moreover, COFER was surprisingly robust in relation to the number of active features in the model. These results reveal that COFER provides effective feature sets for certain NLP tasks. We summarize the noteworthy results in Figure 3, and also the performance of recent top-line systems </context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A HighPerformance Semi-Supervised Learning Method for Text Chunking. In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable Training of L1-regularized Log-linear Models.</title>
<date>2007</date>
<booktitle>In Zoubin Ghahramani, editor, Proceedings of the 24th Annual International Conference on Machine Learning (ICML 2007),</booktitle>
<pages>33--40</pages>
<publisher>Omnipress.</publisher>
<contexts>
<context position="16560" citStr="Andrew and Gao, 2007" startWordPosition="2849" endWordPosition="2852">arning. Finally, we compared the performance in terms of the number of active features in the model given by supervised learning. We note here that the number of active features for COFER is the number of features hm if w&apos;&apos; m = 0, which is not wm = 0 for a fair comparison. Unlike COFER, iCWR does not have any architecture to winnow the original feature set used in supervised learning. For a fair comparison, we prepared L1-regularized supervised learning algorithms, which try to reduce the non-zero parameters in a model. Specifically, we utilized L1-regularized CRF (L1CRF) optimized by OWL-QN (Andrew and Gao, 2007) for NER, and the online structured output learning version of FOBOS (Duchi and Singer, 2009; Tsuruoka et al., 2009) with L1- regularization (ostL1FOBOS) for dependency parsing. In addition, we also examined L2 regularized CRF (Lafferty et al., 2001) optimized by LBFGS (Liu and Nocedal, 1989) (L2CRF) for NER, and the online structured output learning version of the Passive-Aggressive algorithm (ostPA) (Crammer et al., 2006) for dependency parsing to illustrate the baseline performance regardless of the active feature number. 4.2 Settings for COFER We utilized baseline supervised learning model</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable Training of L1-regularized Log-linear Models. In Zoubin Ghahramani, editor, Proceedings of the 24th Annual International Conference on Machine Learning (ICML 2007), pages 33–40. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving Dependency Parsing with Subtrees from Auto-Parsed Data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>570--579</pages>
<contexts>
<context position="14906" citStr="Chen et al., 2009" startWordPosition="2570" endWordPosition="2573"> the summation of a data-wise calculation (map phase), and VD(fn) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dependency parsing data derived from PTB-III (Koo et al., 2008), an</context>
<context position="19936" citStr="Chen et al., 2009" startWordPosition="3427" endWordPosition="3430"> Isozaki, 2008) 94.48 89.92 1,000M N/A (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al., 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev. test #.USD #.AF ostL1FOBOS 93.15 92.82 0 6.80M iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K (S = 1e + 05) 93.91 93.71 3,720M 3.23M iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M (Koo and Collins, 2010) 93.49 93.04 0 N/A (Martins et al., 2010) N/A 93.26 0 55.25M (Koo et al., 2008) 93.30 93.16 43M N/A (Chen et al., 2009) N/A 93.16 43M N/A (Suzuki et al., 2009) 94.13 93.79 3,720M N/A Table 1: Comparison with previous top-line systems on test data. (#.USD: unsupervised data size. #.AF: the size of active features in the trained model.) supervised learning systems even though it uses far fewer active features. In addition, the combination of iCWR+COFER significantly outperformed the current best results by achieving a 0.12 point gain from 90.90 to 91.02 for NER, and a 0.43 point gain from 93.79 to 94.22 for dependency parsing, with only 5.94M and 5.77M features, respectively. 5 Conclusion This paper introduced t</context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving Dependency Parsing with Subtrees from Auto-Parsed Data. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online PassiveAggressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="16987" citStr="Crammer et al., 2006" startWordPosition="2917" endWordPosition="2921">egularized supervised learning algorithms, which try to reduce the non-zero parameters in a model. Specifically, we utilized L1-regularized CRF (L1CRF) optimized by OWL-QN (Andrew and Gao, 2007) for NER, and the online structured output learning version of FOBOS (Duchi and Singer, 2009; Tsuruoka et al., 2009) with L1- regularization (ostL1FOBOS) for dependency parsing. In addition, we also examined L2 regularized CRF (Lafferty et al., 2001) optimized by LBFGS (Liu and Nocedal, 1989) (L2CRF) for NER, and the online structured output learning version of the Passive-Aggressive algorithm (ostPA) (Crammer et al., 2006) for dependency parsing to illustrate the baseline performance regardless of the active feature number. 4.2 Settings for COFER We utilized baseline supervised learning models as the base supervised models of COFER. 639 (a) NER (F-score) (b) dep. parsing (UAS) Figure 3: Performance vs. size of active features in the trained model on the development sets In addition, we also report the results when we treat iCWR as COFER’s base supervised models (iCWR+COFER). This is a very natural and straightforward approach to combining these two. We generally handle several different types of features such a</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online PassiveAggressive Algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<date>2008</date>
<booktitle>MapReduce: Simplified Data Processing on Large Clusters. Commun. ACM,</booktitle>
<pages>51--1</pages>
<contexts>
<context position="14181" citStr="Dean and Ghemawat, 2008" startWordPosition="2456" endWordPosition="2459">h as named entity recognition and dependency parsing. Finally, we replace all Y(x) with Z(x), and use fn(x, z) and r(x, z) instead of fn(x, y) and r(x, y), respectively, in Rn and An. When we use these substitutions, there is no need to incorporate an efficient algorithm such as dynamic programming into our method. This means that our feature potency estimation can be applied to the structured prediction problem at low cost. 3.6 Efficient feature potency computation Our feature potency estimation described in Section 3.1 to 3.3 is highly suitable for implementation in the MapReduce framework (Dean and Ghemawat, 2008), which is a modern distributed parallel computing framework. This is because Rn and An can be calculated by the summation of a data-wise calculation (map phase), and VD(fn) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP sy</context>
</contexts>
<marker>Dean, Ghemawat, 2008</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: Simplified Data Processing on Large Clusters. Commun. ACM, 51(1):107–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Andrew McCallum</author>
</authors>
<title>HighPerformance Semi-Supervised Learning using Discriminatively Constrained Generative Models.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML</booktitle>
<pages>319--326</pages>
<contexts>
<context position="8633" citStr="Druck and McCallum, 2010" startWordPosition="1478" endWordPosition="1481">t δ ( Integer Space N ) Features mapped into zero are discarded and never mapped into any condensed features Feature potency F∗n f 3 4 0 1 2 -1 -2 Section 3.4: Condensed feature construction u. 11 Condensed feature set 9 3/e φ (Quantized feature potency) M (e.g., M=1K) 1/e 1/e The potencies are also utilized as an (M+1)-th condensed feature -2/e N (e.g., N=100M) Original feature set Section 3.1: Feature potency estimation -C C Each condensed feature is represented as a set of features in the original feature set F. XRn= xED X X r(x, y)fn(x, y), An= yEY(x) X �r(x) fn(x, y) xED yEY(x) 637 2008; Druck and McCallum, 2010). Our method is rough and a much simpler framework for implementing this fundamental idea of semi-supervised learning developed for NLP tasks. We create a simple framework to achieve improved flexibility, extendability, and applicability. In fact, we apply the framework by incorporating a feature merging and elimination architecture to obtain effective condensed feature sets for supervised learning. 3.2 Feature potency discounting To discount low potency values, we redefine feature potency as V0D(fn) instead of VD(fn) as follows: V0D(fn) = { log [Rn+C]−log[An] if Rn−An&lt;−C 0 if − C &lt;Rn−An&lt;C log</context>
</contexts>
<marker>Druck, McCallum, 2010</marker>
<rawString>Gregory Druck and Andrew McCallum. 2010. HighPerformance Semi-Supervised Learning using Discriminatively Constrained Generative Models. In Proceedings of the International Conference on Machine Learning (ICML 2010), pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Yoram Singer</author>
</authors>
<title>Efficient Online and Batch Learning Using Forward Backward Splitting.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--2899</pages>
<contexts>
<context position="9599" citStr="Duchi and Singer, 2009" startWordPosition="1637" endWordPosition="1640"> effective condensed feature sets for supervised learning. 3.2 Feature potency discounting To discount low potency values, we redefine feature potency as V0D(fn) instead of VD(fn) as follows: V0D(fn) = { log [Rn+C]−log[An] if Rn−An&lt;−C 0 if − C &lt;Rn−An&lt;C log [Rn−C]−log[An] if C &lt;Rn−An where Rn and An are defined in Figure 2. Note that VD(fn) = VD + (fn) − VD � (fn) = Rn − An. The difference from VD(fn) is that we cast it in the log-domain and introduce a non-negative constant C. The introduction of C is inspired by the L1- regularization technique used in supervised learning algorithms such as (Duchi and Singer, 2009; Tsuruoka et al., 2009). C controls how much we discount VD(fn) toward zero, and is given by the user. 3.3 Feature potency quantization We define VD(fn) as VD(fn) = F6V0D(fn)1 if V0D(fn)&gt; 0 and VD(fn) = L6V0D(fn)J otherwise, where 6 is a positive user-specified constant. Note that VD(fn) always becomes an integer, that is, V*D(fn) E N where N = {..., −2,−1,0,1,2, ...}. This calculation can be seen as mapping each feature into a discrete (integer) space with respect to V0D(fn). 6 controls the range of V0D(fn) mapping into the same integer. 3.4 Condensed feature construction Suppose we have M d</context>
<context position="16652" citStr="Duchi and Singer, 2009" startWordPosition="2865" endWordPosition="2868">he model given by supervised learning. We note here that the number of active features for COFER is the number of features hm if w&apos;&apos; m = 0, which is not wm = 0 for a fair comparison. Unlike COFER, iCWR does not have any architecture to winnow the original feature set used in supervised learning. For a fair comparison, we prepared L1-regularized supervised learning algorithms, which try to reduce the non-zero parameters in a model. Specifically, we utilized L1-regularized CRF (L1CRF) optimized by OWL-QN (Andrew and Gao, 2007) for NER, and the online structured output learning version of FOBOS (Duchi and Singer, 2009; Tsuruoka et al., 2009) with L1- regularization (ostL1FOBOS) for dependency parsing. In addition, we also examined L2 regularized CRF (Lafferty et al., 2001) optimized by LBFGS (Liu and Nocedal, 1989) (L2CRF) for NER, and the online structured output learning version of the Passive-Aggressive algorithm (ostPA) (Crammer et al., 2006) for dependency parsing to illustrate the baseline performance regardless of the active feature number. 4.2 Settings for COFER We utilized baseline supervised learning models as the base supervised models of COFER. 639 (a) NER (F-score) (b) dep. parsing (UAS) Figur</context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>John Duchi and Yoram Singer. 2009. Efficient Online and Batch Learning Using Forward Backward Splitting. Journal of Machine Learning Research, 10:2899–2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient ThirdOrder Dependency Parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="19817" citStr="Koo and Collins, 2010" startWordPosition="3403" endWordPosition="3406"> 94.39 90.72 3,720M 344 L1CRF (S = 1e + 04) 94.91 91.02 3,720M 5.94M (Ando and Zhang, 2005) 93.15 89.31 27M N/A (Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al., 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev. test #.USD #.AF ostL1FOBOS 93.15 92.82 0 6.80M iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K (S = 1e + 05) 93.91 93.71 3,720M 3.23M iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M (Koo and Collins, 2010) 93.49 93.04 0 N/A (Martins et al., 2010) N/A 93.26 0 55.25M (Koo et al., 2008) 93.30 93.16 43M N/A (Chen et al., 2009) N/A 93.16 43M N/A (Suzuki et al., 2009) 94.13 93.79 3,720M N/A Table 1: Comparison with previous top-line systems on test data. (#.USD: unsupervised data size. #.AF: the size of active features in the trained model.) supervised learning systems even though it uses far fewer active features. In addition, the combination of iCWR+COFER significantly outperformed the current best results by achieving a 0.12 point gain from 90.90 to 91.02 for NER, and a 0.43 point gain from 93.79 </context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient ThirdOrder Dependency Parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="1811" citStr="Koo et al., 2008" startWordPosition="256" endWordPosition="259">dels of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, with the iCWR approach, C is induced independently from F, and us</context>
<context position="14887" citStr="Koo et al., 2008" startWordPosition="2566" endWordPosition="2569">n be calculated by the summation of a data-wise calculation (map phase), and VD(fn) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dependency parsing data derived from PTB-III (Ko</context>
<context position="19896" citStr="Koo et al., 2008" startWordPosition="3419" endWordPosition="3422">, 2005) 93.15 89.31 27M N/A (Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al., 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev. test #.USD #.AF ostL1FOBOS 93.15 92.82 0 6.80M iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K (S = 1e + 05) 93.91 93.71 3,720M 3.23M iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M (Koo and Collins, 2010) 93.49 93.04 0 N/A (Martins et al., 2010) N/A 93.26 0 55.25M (Koo et al., 2008) 93.30 93.16 43M N/A (Chen et al., 2009) N/A 93.16 43M N/A (Suzuki et al., 2009) 94.13 93.79 3,720M N/A Table 1: Comparison with previous top-line systems on test data. (#.USD: unsupervised data size. #.AF: the size of active features in the trained model.) supervised learning systems even though it uses far fewer active features. In addition, the combination of iCWR+COFER significantly outperformed the current best results by achieving a 0.12 point gain from 90.90 to 91.02 for NER, and a 0.43 point gain from 93.79 to 94.22 for dependency parsing, with only 5.94M and 5.77M features, respective</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proceedings of ACL-08: HLT, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML</booktitle>
<pages>282--289</pages>
<contexts>
<context position="16810" citStr="Lafferty et al., 2001" startWordPosition="2890" endWordPosition="2893">0 for a fair comparison. Unlike COFER, iCWR does not have any architecture to winnow the original feature set used in supervised learning. For a fair comparison, we prepared L1-regularized supervised learning algorithms, which try to reduce the non-zero parameters in a model. Specifically, we utilized L1-regularized CRF (L1CRF) optimized by OWL-QN (Andrew and Gao, 2007) for NER, and the online structured output learning version of FOBOS (Duchi and Singer, 2009; Tsuruoka et al., 2009) with L1- regularization (ostL1FOBOS) for dependency parsing. In addition, we also examined L2 regularized CRF (Lafferty et al., 2001) optimized by LBFGS (Liu and Nocedal, 1989) (L2CRF) for NER, and the online structured output learning version of the Passive-Aggressive algorithm (ostPA) (Crammer et al., 2006) for dependency parsing to illustrate the baseline performance regardless of the active feature number. 4.2 Settings for COFER We utilized baseline supervised learning models as the base supervised models of COFER. 639 (a) NER (F-score) (b) dep. parsing (UAS) Figure 3: Performance vs. size of active features in the trained model on the development sets In addition, we also report the results when we treat iCWR as COFER’</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the International Conference on Machine Learning (ICML 2001), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrase Clustering for Discriminative Learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1030--1038</pages>
<contexts>
<context position="1747" citStr="Lin and Wu, 2009" startWordPosition="245" endWordPosition="248">, supervised learning has become a standard way to train the models of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, w</context>
<context position="17800" citStr="Lin and Wu, 2009" startWordPosition="3051" endWordPosition="3054">d models of COFER. 639 (a) NER (F-score) (b) dep. parsing (UAS) Figure 3: Performance vs. size of active features in the trained model on the development sets In addition, we also report the results when we treat iCWR as COFER’s base supervised models (iCWR+COFER). This is a very natural and straightforward approach to combining these two. We generally handle several different types of features such as words, part-of-speech tags, word surface forms, and their combinations. Suppose we have K different feature types, which are often defined by feature templates, i.e., (Suzuki and Isozaki, 2008; Lin and Wu, 2009). In our experiments, we restrict the merging of features during the condensed feature construction process if and only if the features are the same feature type. As a result, COFER essentially consists of K different condensed feature sets. The numbers of feature types K were 79 and 30 for our NER and dependency parsing experiments, respectively. We note that this kind of feature partition by their types is widely used in the context of semi-supervised learning (Ando and Zhang, 2005; Suzuki and Isozaki, 2008). 4.3 Results and Discussion Figure 3 displays the performance on the development set</context>
<context position="19462" citStr="Lin and Wu, 2009" startWordPosition="3339" endWordPosition="3342">the performance of recent top-line systems for NER and dependency parsing in Table 1. Overall, COFER matches the results of top-line semiNER system dev. test #.USD #.AF Sup.L1CRF 90.40 85.08 0 0.57M iCWR: L1CRF 93.33 89.99 3,720M 0.62M COFER: L1CRF (S = 1e + 00) 93.42 88.81 3,720M 359 (S = 1e + 04) 93.60 89.22 3,720M 2.46M iCWR+COFER: (S = 1e + 00) 94.39 90.72 3,720M 344 L1CRF (S = 1e + 04) 94.91 91.02 3,720M 5.94M (Ando and Zhang, 2005) 93.15 89.31 27M N/A (Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al., 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev. test #.USD #.AF ostL1FOBOS 93.15 92.82 0 6.80M iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K (S = 1e + 05) 93.91 93.71 3,720M 3.23M iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M (Koo and Collins, 2010) 93.49 93.04 0 N/A (Martins et al., 2010) N/A 93.26 0 55.25M (Koo et al., 2008) 93.30 93.16 43M N/A (Chen et al., 2009) N/A 93.16 43M N/A (Suzuki et al., 2009) 94.13 93.79 3,720M N/A Table 1: Comparison with previous top-line systems on test dat</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrase Clustering for Discriminative Learning. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1030–1038.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the Limited Memory BFGS Method for Large Scale Optimization.</title>
<date>1989</date>
<journal>Math. Programming, Ser. B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="16853" citStr="Liu and Nocedal, 1989" startWordPosition="2898" endWordPosition="2901"> does not have any architecture to winnow the original feature set used in supervised learning. For a fair comparison, we prepared L1-regularized supervised learning algorithms, which try to reduce the non-zero parameters in a model. Specifically, we utilized L1-regularized CRF (L1CRF) optimized by OWL-QN (Andrew and Gao, 2007) for NER, and the online structured output learning version of FOBOS (Duchi and Singer, 2009; Tsuruoka et al., 2009) with L1- regularization (ostL1FOBOS) for dependency parsing. In addition, we also examined L2 regularized CRF (Lafferty et al., 2001) optimized by LBFGS (Liu and Nocedal, 1989) (L2CRF) for NER, and the online structured output learning version of the Passive-Aggressive algorithm (ostPA) (Crammer et al., 2006) for dependency parsing to illustrate the baseline performance regardless of the active feature number. 4.2 Settings for COFER We utilized baseline supervised learning models as the base supervised models of COFER. 639 (a) NER (F-score) (b) dep. parsing (UAS) Figure 3: Performance vs. size of active features in the trained model on the development sets In addition, we also report the results when we treat iCWR as COFER’s base supervised models (iCWR+COFER). This</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the Limited Memory BFGS Method for Large Scale Optimization. Math. Programming, Ser. B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="15100" citStr="Marcus et al., 1994" startWordPosition="2604" endWordPosition="2607">rmed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dependency parsing data derived from PTB-III (Koo et al., 2008), and the CoNLL’03 shared task data (Turian et al., 2010). By comparing COFER with iCWR we can clarify its effectiveness in terms of providing better features for supervised learning. We use the ter</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
<author>Pedro Aguiar</author>
<author>Mario Figueiredo</author>
</authors>
<title>Turbo Parsers: Dependency Parsing by Approximate Variational Inference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>34--44</pages>
<contexts>
<context position="19858" citStr="Martins et al., 2010" startWordPosition="3411" endWordPosition="3414">) 94.91 91.02 3,720M 5.94M (Ando and Zhang, 2005) 93.15 89.31 27M N/A (Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al., 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev. test #.USD #.AF ostL1FOBOS 93.15 92.82 0 6.80M iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K (S = 1e + 05) 93.91 93.71 3,720M 3.23M iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M (Koo and Collins, 2010) 93.49 93.04 0 N/A (Martins et al., 2010) N/A 93.26 0 55.25M (Koo et al., 2008) 93.30 93.16 43M N/A (Chen et al., 2009) N/A 93.16 43M N/A (Suzuki et al., 2009) 94.13 93.79 3,720M N/A Table 1: Comparison with previous top-line systems on test data. (#.USD: unsupervised data size. #.AF: the size of active features in the trained model.) supervised learning systems even though it uses far fewer active features. In addition, the combination of iCWR+COFER significantly outperformed the current best results by achieving a 0.12 point gain from 90.90 to 91.02 for NER, and a 0.43 point gain from 93.79 to 94.22 for dependency parsing, with onl</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo Parsers: Dependency Parsing by Approximate Variational Inference. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34– 44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design Challenges and Misconceptions in Named Entity Recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<pages>147--155</pages>
<contexts>
<context position="19381" citStr="Ratinov and Roth, 2009" startWordPosition="3323" endWordPosition="3326"> sets for certain NLP tasks. We summarize the noteworthy results in Figure 3, and also the performance of recent top-line systems for NER and dependency parsing in Table 1. Overall, COFER matches the results of top-line semiNER system dev. test #.USD #.AF Sup.L1CRF 90.40 85.08 0 0.57M iCWR: L1CRF 93.33 89.99 3,720M 0.62M COFER: L1CRF (S = 1e + 00) 93.42 88.81 3,720M 359 (S = 1e + 04) 93.60 89.22 3,720M 2.46M iCWR+COFER: (S = 1e + 00) 94.39 90.72 3,720M 344 L1CRF (S = 1e + 04) 94.91 91.02 3,720M 5.94M (Ando and Zhang, 2005) 93.15 89.31 27M N/A (Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al., 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev. test #.USD #.AF ostL1FOBOS 93.15 92.82 0 6.80M iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K (S = 1e + 05) 93.91 93.71 3,720M 3.23M iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M (Koo and Collins, 2010) 93.49 93.04 0 N/A (Martins et al., 2010) N/A 93.26 0 55.25M (Koo et al., 2008) 93.30 93.16 43M N/A (Chen et al., 2009) N/A 93.16 43M N/A (Suzuki et al., 2009) 94.1</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design Challenges and Misconceptions in Named Entity Recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised Sequential Labeling and Segmentation Using GigaWord Scale Unlabeled Data.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>665--673</pages>
<contexts>
<context position="14846" citStr="Suzuki and Isozaki, 2008" startWordPosition="2558" endWordPosition="2561"> computing framework. This is because Rn and An can be calculated by the summation of a data-wise calculation (map phase), and VD(fn) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both depende</context>
<context position="17781" citStr="Suzuki and Isozaki, 2008" startWordPosition="3047" endWordPosition="3050">dels as the base supervised models of COFER. 639 (a) NER (F-score) (b) dep. parsing (UAS) Figure 3: Performance vs. size of active features in the trained model on the development sets In addition, we also report the results when we treat iCWR as COFER’s base supervised models (iCWR+COFER). This is a very natural and straightforward approach to combining these two. We generally handle several different types of features such as words, part-of-speech tags, word surface forms, and their combinations. Suppose we have K different feature types, which are often defined by feature templates, i.e., (Suzuki and Isozaki, 2008; Lin and Wu, 2009). In our experiments, we restrict the merging of features during the condensed feature construction process if and only if the features are the same feature type. As a result, COFER essentially consists of K different condensed feature sets. The numbers of feature types K were 79 and 30 for our NER and dependency parsing experiments, respectively. We note that this kind of feature partition by their types is widely used in the context of semi-supervised learning (Ando and Zhang, 2005; Suzuki and Isozaki, 2008). 4.3 Results and Discussion Figure 3 displays the performance on </context>
<context position="19333" citStr="Suzuki and Isozaki, 2008" startWordPosition="3315" endWordPosition="3318">sults reveal that COFER provides effective feature sets for certain NLP tasks. We summarize the noteworthy results in Figure 3, and also the performance of recent top-line systems for NER and dependency parsing in Table 1. Overall, COFER matches the results of top-line semiNER system dev. test #.USD #.AF Sup.L1CRF 90.40 85.08 0 0.57M iCWR: L1CRF 93.33 89.99 3,720M 0.62M COFER: L1CRF (S = 1e + 00) 93.42 88.81 3,720M 359 (S = 1e + 04) 93.60 89.22 3,720M 2.46M iCWR+COFER: (S = 1e + 00) 94.39 90.72 3,720M 344 L1CRF (S = 1e + 04) 94.91 91.02 3,720M 5.94M (Ando and Zhang, 2005) 93.15 89.31 27M N/A (Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al., 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev. test #.USD #.AF ostL1FOBOS 93.15 92.82 0 6.80M iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K (S = 1e + 05) 93.91 93.71 3,720M 3.23M iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M (Koo and Collins, 2010) 93.49 93.04 0 N/A (Martins et al., 2010) N/A 93.26 0 55.25M (Koo et al., 2008) 93.30 93.16 43M N/A (Chen et al., 20</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised Sequential Labeling and Segmentation Using GigaWord Scale Unlabeled Data. In Proceedings of ACL08: HLT, pages 665–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An Empirical Study of Semisupervised Structured Conditional Models for Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="14928" citStr="Suzuki et al., 2009" startWordPosition="2574" endWordPosition="2577"> data-wise calculation (map phase), and VD(fn) can be calculated independently by each feature (reduce phase). We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. 4 Experiments We conducted experiments on two different NLP tasks, namely NER and dependency parsing. To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009). For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dependency parsing data derived from PTB-III (Koo et al., 2008), and the CoNLL’03 shared </context>
<context position="19976" citStr="Suzuki et al., 2009" startWordPosition="3435" endWordPosition="3438"> (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al., 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev. test #.USD #.AF ostL1FOBOS 93.15 92.82 0 6.80M iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K (S = 1e + 05) 93.91 93.71 3,720M 3.23M iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M (Koo and Collins, 2010) 93.49 93.04 0 N/A (Martins et al., 2010) N/A 93.26 0 55.25M (Koo et al., 2008) 93.30 93.16 43M N/A (Chen et al., 2009) N/A 93.16 43M N/A (Suzuki et al., 2009) 94.13 93.79 3,720M N/A Table 1: Comparison with previous top-line systems on test data. (#.USD: unsupervised data size. #.AF: the size of active features in the trained model.) supervised learning systems even though it uses far fewer active features. In addition, the combination of iCWR+COFER significantly outperformed the current best results by achieving a 0.12 point gain from 90.90 to 91.02 for NER, and a 0.43 point gain from 93.79 to 94.22 for dependency parsing, with only 5.94M and 5.77M features, respectively. 5 Conclusion This paper introduced the idea of condensed feature representat</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An Empirical Study of Semisupervised Structured Conditional Models for Dependency Parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>142--147</pages>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Erik Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition. In Proceedings of CoNLL-2003, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>477--485</pages>
<contexts>
<context position="9623" citStr="Tsuruoka et al., 2009" startWordPosition="1641" endWordPosition="1645">ture sets for supervised learning. 3.2 Feature potency discounting To discount low potency values, we redefine feature potency as V0D(fn) instead of VD(fn) as follows: V0D(fn) = { log [Rn+C]−log[An] if Rn−An&lt;−C 0 if − C &lt;Rn−An&lt;C log [Rn−C]−log[An] if C &lt;Rn−An where Rn and An are defined in Figure 2. Note that VD(fn) = VD + (fn) − VD � (fn) = Rn − An. The difference from VD(fn) is that we cast it in the log-domain and introduce a non-negative constant C. The introduction of C is inspired by the L1- regularization technique used in supervised learning algorithms such as (Duchi and Singer, 2009; Tsuruoka et al., 2009). C controls how much we discount VD(fn) toward zero, and is given by the user. 3.3 Feature potency quantization We define VD(fn) as VD(fn) = F6V0D(fn)1 if V0D(fn)&gt; 0 and VD(fn) = L6V0D(fn)J otherwise, where 6 is a positive user-specified constant. Note that VD(fn) always becomes an integer, that is, V*D(fn) E N where N = {..., −2,−1,0,1,2, ...}. This calculation can be seen as mapping each feature into a discrete (integer) space with respect to V0D(fn). 6 controls the range of V0D(fn) mapping into the same integer. 3.4 Condensed feature construction Suppose we have M different quantized featu</context>
<context position="16676" citStr="Tsuruoka et al., 2009" startWordPosition="2869" endWordPosition="2872">ised learning. We note here that the number of active features for COFER is the number of features hm if w&apos;&apos; m = 0, which is not wm = 0 for a fair comparison. Unlike COFER, iCWR does not have any architecture to winnow the original feature set used in supervised learning. For a fair comparison, we prepared L1-regularized supervised learning algorithms, which try to reduce the non-zero parameters in a model. Specifically, we utilized L1-regularized CRF (L1CRF) optimized by OWL-QN (Andrew and Gao, 2007) for NER, and the online structured output learning version of FOBOS (Duchi and Singer, 2009; Tsuruoka et al., 2009) with L1- regularization (ostL1FOBOS) for dependency parsing. In addition, we also examined L2 regularized CRF (Lafferty et al., 2001) optimized by LBFGS (Liu and Nocedal, 1989) (L2CRF) for NER, and the online structured output learning version of the Passive-Aggressive algorithm (ostPA) (Crammer et al., 2006) for dependency parsing to illustrate the baseline performance regardless of the active feature number. 4.2 Settings for COFER We utilized baseline supervised learning models as the base supervised models of COFER. 639 (a) NER (F-score) (b) dep. parsing (UAS) Figure 3: Performance vs. siz</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 477–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-Supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="1769" citStr="Turian et al., 2010" startWordPosition="249" endWordPosition="252">ing has become a standard way to train the models of many natural language processing (NLP) systems. One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. Specifically, an approach that involves incorporating ‘clusteringbased word representations (CWR)’ induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008). We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality. The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems. The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method. Then, with the iCWR approach,</context>
<context position="15559" citStr="Turian et al., 2010" startWordPosition="2679" endWordPosition="2682">upervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III (PTB) corpus (Marcus et al., 1994) for dependency parsing. We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009). 4.1 Comparative Methods We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm. The iCWR approach yields the state-of-the-art results with both dependency parsing data derived from PTB-III (Koo et al., 2008), and the CoNLL’03 shared task data (Turian et al., 2010). By comparing COFER with iCWR we can clarify its effectiveness in terms of providing better features for supervised learning. We use the term active features to refer to features whose corresponding model parameter is non-zero after supervised learning. It is wellknown that we can discard non-active features from the trained model without any loss after finishing supervised learning. Finally, we compared the performance in terms of the number of active features in the model given by supervised learning. We note here that the number of active features for COFER is the number of features hm if </context>
<context position="19423" citStr="Turian et al., 2010" startWordPosition="3331" endWordPosition="3334"> noteworthy results in Figure 3, and also the performance of recent top-line systems for NER and dependency parsing in Table 1. Overall, COFER matches the results of top-line semiNER system dev. test #.USD #.AF Sup.L1CRF 90.40 85.08 0 0.57M iCWR: L1CRF 93.33 89.99 3,720M 0.62M COFER: L1CRF (S = 1e + 00) 93.42 88.81 3,720M 359 (S = 1e + 04) 93.60 89.22 3,720M 2.46M iCWR+COFER: (S = 1e + 00) 94.39 90.72 3,720M 344 L1CRF (S = 1e + 04) 94.91 91.02 3,720M 5.94M (Ando and Zhang, 2005) 93.15 89.31 27M N/A (Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A (Ratinov and Roth, 2009) 93.50 90.57 N/A N/A (Turian et al., 2010) 93.95 90.36 37M N/A (Lin and Wu, 2009) N/A 90.90 700,000M N/A Dependency parser dev. test #.USD #.AF ostL1FOBOS 93.15 92.82 0 6.80M iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M COFER:ostL1FOBOS (S = 1e + 03) 93.53 93.23 3,720M 20.7K (S = 1e + 05) 93.91 93.71 3,720M 3.23M iCWR+COFER: (S = 1e + 03) 93.93 93.55 3,720M 12.5K ostL1FOBOS (S = 1e + 05) 94.33 94.22 3,720M 5.77M (Koo and Collins, 2010) 93.49 93.04 0 N/A (Martins et al., 2010) N/A 93.26 0 55.25M (Koo et al., 2008) 93.30 93.16 43M N/A (Chen et al., 2009) N/A 93.16 43M N/A (Suzuki et al., 2009) 94.13 93.79 3,720M N/A Table 1: Comparison wit</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-Supervised Learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>