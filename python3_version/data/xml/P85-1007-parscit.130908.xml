<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.794798">
Speech Acts and Rationality
</title>
<note confidence="0.936537833333333">
Philip R. Cohen Hector J. Levesque
Artificial Intelligence Center Department of Computer Science
SRI International University of Toronto&apos;
and
Center for the Study of Language and Information
Stanford University
</note>
<sectionHeader confidence="0.980419" genericHeader="abstract">
1 Abstract
</sectionHeader>
<bodyText confidence="0.998157888888889">
This paper derives the basis of a theory of communication from
a formal theory of rational interaction. The major result is a
demonstration that illocutionary acts need not be primitive, and
need not be recognized. As a test case, we derive Searle&apos;s con-
ditions on requesting from principles of rationality coupled with
a (Iricean theory of imperatives. The theory is shown to dis-
tinguish insincere or nonserious imperatives from true requests.
I.:xtensions to indirect speech acts, and ramifications for natural
language systems are also briefly discussed.
</bodyText>
<sectionHeader confidence="0.998375" genericHeader="keywords">
2 Introduction
</sectionHeader>
<bodyText confidence="0.98038148">
The unifying theme of much corrent pragmatics and discourse
research is that the cohervner of dialogne is to be found in the
interaction of the conversant,: plans. That is, a speaker is re-
garded as planning his utterances to achieve his goals, which
at as involve influencing a hearer by the use of communicative
or -speech acts. On receiving an utterance realizing such an
action, the hearer attempts to infer the speaker&apos;s goal(s) and to
understand how the titterance furthers thern. The hearer then
adopts new goals (e.g.. to respond to a reqoest. to clarify the pre-
vious speaker&apos;s utterance or goal) and plans his Own it,
to achieve those. A conversation ensues.
This view of language as purposeful action has pervaded I &apos;om-
putational Linguistics research. anti has resulted in numerous
prototype systems [1, 2, 3. 5. f). 25, 271. However, the formal
foundations underlying these systems have been unspecified or
underspecified. In this state of affairs, one cannot characterize
what a system should (I() independently from what it does.
This paper begins to rectify this situation by presenting a
formalization of rational interaction. upon which is erected the
beginnings if a theory of communication and speech acts. Inter-
action is derived from principles of rational action for individual
agents, a.s well as principles of belief and goal adoption among
agents. The ba.sis of a theory of purposeful communication thus
_
&apos;Fellow of thr Canadian Institute for A,:vanceil Research.
</bodyText>
<footnote confidence="0.9476451">
This research was made possible &apos;it part hy a gift from the Systems Devil-
opment Foundation, and in part by support front the Defense Advanced
Research Projects Agency under Contract Ntio010.8.1-k-0078 with the
Naval Electronic Systems Command. The views and conclusions con-
tained in this document are those of the authors and should not be inter-
preted as representative of the official policies, either expressed or implied,
of the Defense Advanced Research Pmjects Agency or the United States
Governtnent. Much of this research was done when the second author
was employed at the Fairchild Camera and Instrument Corp.
emerges as a consequence of principles of action.
</footnote>
<subsectionHeader confidence="0.908356">
2.1 Speech Act Theory
</subsectionHeader>
<bodyText confidence="0.999502515151515">
Speech act theory was originally conceived as part of action the-
ory. Many of Austin&apos;s 1.11 insights about the nature of speech
acts, felicity conditions, anti modes of failure apply equally well
to non-communicative actions. Searle [26] repeatedly mentions
that many of the conditions lie attributes to various illocnt ion-
ary acts (such as requests anti questions) apply more generally
to non-communicative action. However, researchers have grad-
ually lost sight of their roots. In recent work 1281 illocutionary
acts are formalized. and a logic is proposed, in which properties
of IA&apos;s (e.g., -preparatory conditions&amp;quot; and -modes if achieve-
ment&apos;) are primitively stipulated, rather than derived from more
basic principles of action. We believe this approach misses sig-
nificant generalities. This paper shows how to derive properties
of illocutionary acts from principles of rationality, updating the
formalism of [10].
Work in Artificial Intelligence provided the first formal
grounding of speech act theory in terms of planning, and plan
recognition. (laminating in Perrault and Allen&apos;s 1221 theory of
indirect speech acts. Much of our research is inspired 1) their
analyses. Howes er, one major ingredient of their theory can be
shown to be redundant. [101 illocutionary acts. All the in-
ferential power of the recognition of their illocutionary acts was
already available in other -operators&apos;. Nevertheless, the natu-
ral language systems based on this approach [I, 51 always had
to recognize which illocutionary act was performed in order to
respond to a user&apos;s utterance. Since the illocutionary acts were
unnecessary for achieving their effects, so too was their recogni-
tion.
The stance that. illocutionary acts are not primitive, and need
not be recognized, is a lihs•rating one. Once taken, it becomes
apparent that many of the difficulties in applying speech act
theory to discourse, or to computer systems, stem from taking
these acts too seriously .— i.e., too primitively.
</bodyText>
<sectionHeader confidence="0.557494" genericHeader="introduction">
3 Form of the argument
</sectionHeader>
<footnote confidence="0.825757166666667">
We show that illocutionary acts need not be primitive by de-
riving Searle&apos;s conditions on requesting from an independently-
motivated theory of action. The realm of communicative action
is entered following Grice [13] — by postulating a correlation
between the utterance of a sentence with a certain syntactic fea-
ture (e.g., its dominant clause is an imperative) and a complex
</footnote>
<page confidence="0.999305">
49
</page>
<bodyText confidence="0.999931627906977">
propositional attitude expressing the speaker&apos;s goal. This atti-
tude becomes true as a result of uttering a sentence with that
feature. Because of certain general principles governing beliefs
and goals, other causal consequences of the speaker&apos;s having the
expressed goal can be derived. Such derivations will be &amp;quot;summa-
rized&amp;quot; as lemmas of the form &amp;quot;If (conditions) are true, then any
action making (antecedent) true also makes (consequent) true.&apos;
These lemmas will be used to characterize illocutionary acts,
though they are not themselves acts. For example, the lemma
called REQUEST will characterize a derivation that shows how
a hearer&apos;s knowing that the speaker has certain goals can cause
the hearer to act. The conditions licensing that chain will be col-
lected in the REQUEST lemma, and will be shown to subsume
those stipulated by Searle [261 as felicity conditions. However,
they have been derived here from first principles, and without
the need for a primitive action of requesting.
The benefits of this approach become clearer as other illocu-
tionary acts are derived. We have derived a characterization
of the speech act of informing, and have used it in deriving
the speech act of questioning. The latter derivation also allows
us to distinguish real questions from teacher/student questions,
and rhetorical questions. However, for brevity, the discussion of
these speech acts has been omitted.
Indirect speech acts can be handled within the framework.
although, again, we cannot present the analyses here. Briefly,
axioms similar to those of Perrault and Allen [221 can be sup-
plied enabling one to reason that an agent has a goal that q,
given that he also ha.s a goal p. When the p&apos;s and q&apos;s are them-
selves goals of the hearer (i.e.. the speaker is trying to get the
hearer to do something), then we can derive a set of lemmas for
indirect requests. Many of these indirect request lemmas corre-
spond to what have been called &apos;short-circuited&amp;quot; implicatures,
which, it was suggested [211 underlie the processing of utterances
of the form &apos;Can you do X?&amp;quot;. &apos;Do you know y?&amp;quot;, etc. Lemma
formation and lemma application thus provide a familiar model
of &amp;quot;hon-circuiting. Furthermore, this approach shows how one
ran use general purpose reasoning in concert with convention-
alized forms (e.g., how one can reason that Can you reach the
salt&amp;quot; is a request to pass the salt), a problem that has plagued
most, theories of speech acts.
The plan for the paper is to construct a formalism based on
a theory of action that is sufficient for characterizing a request.
Nlost of the work is in the theory of action, as it should he.
</bodyText>
<sectionHeader confidence="0.994099" genericHeader="method">
4 The Formalism
</sectionHeader>
<bodyText confidence="0.999825423076923">
To achieve these goals we need a carefully worked out (though
perhaps incomplete) theory of rational action and interaction.
Vie theory will be expressed in a logic whose model theory is
based (loosely) on a possible-worlds semantics. We shall propose
a logic with four primary modal operators -- BELief, OMB,
GOAL, and AFTER. With these, we shall characterize what
agents need to know to perform actions that are intended to
achieve their goals. The agents do so with ilw knowledge that
other agents operate similarly. Thus. agents have beliefs about
(e! her&apos;s gcals, and they have goals to influence others&apos; beliefs
and goals. The integration of these operators follows that of
Moore [201, who arialyzes how an agent&apos;s knowledge affects and
is affected by his actions, by meshing a possible-worlds model
of knowledge with a situation calculus model of action [181. By
adding GOAL, we can begin to talk about an agent&apos;s plans,
which can include his plans to influence the beliefs and goals of
others.
Intuitively, a model for these operators includes courses of
events (i.e., sequences of primitive acts) that characterize what
has happened. Courses of events (c.o.e.&apos;s) are paths through a
tree of possible future primitive acts, and after any primitive act
has occurred, one can recover the course of events that led up
to it. C.o.e.&apos;s can also be related to one another via accessiblity
relations that partake in the semantics of BEL and GOAL. Fur-
ther details of this semantics must await our forthcoming paper
[171.
As a general strategy, the formalism will be too strong. First,
we have the usual consequential closure problems that plague
possible-worlds models for belief. These, however, will be ac-
cepted for the time being. Second, the formalism will describe
agents as satisfying certain properties that might generally be
true, but for which there might be exceptions. Perhaps a process
of non-monotonic reasoning could smooth over the exceptions,
but we will not attempt to specify such reasoning here. Instead,
we assemble a set of basic principles and examine their conse-
quences for speech act use. Third, we are willing to live with the
difficulties of the situation calculus model of action — e.g., the
lack of a way to capture true parallelism, and the frame prob-
lem. Finally, the formalism should be regarded as a description
or specification of an agent, rather than one that any agent could
or should use.
Our approach will be to ground a theory of communication in
a theory of rational interaction, itself supported by a theory of
rational action, which is finally grounded in mental states. Ac-
cordingly, we first need to describe the, behavior of BEL, BMB.
GOAL and AFTER. Then, these operators will be combined
to describe how agents&apos; goals and plans influence their actions.
Then, we characterize how having beliefs about the beliefs and
goals of others can affect one&apos;s own beliefs and goals. Finally,
we characterize a request.
To be more specific, here are the primitives that will be used,
with a minimal explanation.
</bodyText>
<subsectionHeader confidence="0.985256">
4.1 Primitives
</subsectionHeader>
<bodyText confidence="0.914345333333333">
Assume p, q, ... are schema variables ranging over wffs, and
a, b . are schematic varia:Iles ranging over acts. Then the
following are w Ifs.
</bodyText>
<subsectionHeader confidence="0.7657455">
4.1.1 Miffs
(p v q)
</subsectionHeader>
<construct confidence="0.764435833333333">
(AFTER a p) - p is true in all courses of events that obtain from
act as happening&apos;&apos;, (if a denotes a halting act).
(DONE a) - The event denoted by a has just happened.
(ACT a x - Agent x is the only agent of act a
a b Art a preeetien act b in the current course of events.
p where p contains a free occurrence of variable z.
</construct>
<equation confidence="0.8156868">
x= y
True, raise
(BEL x p) - p follows from x&apos;s beliefs.
(GOAL x p) p fo:lows from x&apos;s goals.
(BMB x y p) p fellows from x&apos;s beliefs about what is mutually
</equation>
<subsectionHeader confidence="0.511264">
believed by x and y.
&apos;F•rir this paper, the only events that will be considered are primitive acts.
</subsectionHeader>
<bodyText confidence="0.7625845">
&apos;That is, p is true in all c.o es resulting from concatenating the current
with the c.o.e. denoted by a.
</bodyText>
<page confidence="0.992749">
50
</page>
<subsectionHeader confidence="0.684752">
4.1.2 Action Formation
</subsectionHeader>
<bodyText confidence="0.9452005">
If a, b, c, d range over sequences of primitive acts, and p is a
wff. then the following are complex act descriptions:
</bodyText>
<equation confidence="0.952988571428571">
a:b — sequential action
a 1 b — non-deterministic choice (a or b) action
p? — action of positively testing p.
(IF p a b) — conditional action dr!.! (p?;a) 1 (-0;b), as in dy-
namic logic.
(UNTIL p a) — iterative action dlb-f (—pia)•:--p? (again, as in
dynamic logic).
</equation>
<bodyText confidence="0.976928857142857">
The meta-symbol will prefix formulas that are theorems,
i.e.. that are derivable. Properties of the formal system that will
be assumed to hold will be termed Propositions. Propositions
will be both formulas that should always be valid, for our forth-
coming semantics, and rules of inference that should be sound.
No attempt to prove or validate these propositions here, but we
do so in [I71.
</bodyText>
<subsectionHeader confidence="0.999505">
4.2 Properties of Acts
</subsectionHeader>
<bodyText confidence="0.998778666666667">
We adopt the usual axioms characterizing how complex actions
behave under AFTER, as treated in a dynamic logic (e.g., 120))
namely,
</bodyText>
<subsectionHeader confidence="0.406583">
Proposition 1 Properties of romplez arts
</subsectionHeader>
<bodyText confidence="0.581777">
(AFTER a:h p) s (AFTER a (AFTER b p)).
(AFTER alb p) a (AFTER a p) A (AFTER b p).
(AFTER p? q) p A q.
</bodyText>
<sectionHeader confidence="0.392698" genericHeader="method">
AFTER and DONE will have the following additional proper-
ties:
</sectionHeader>
<bodyText confidence="0.88073219047619">
Proposition 2 V act (AFTER act (DONE x act)) 4
Proposition 3 Va ((DONE (AFTER a p)i:a) pl
Proposition 4 If- otD11 then
Nia (DONE n1;a) D (DONE 3?;a)
Proposition 5 p (DONE p?)
Proposition 8 (DONE [(p D q) A p(?) D (DONE q?)
Our treatment of acts requires that we deal somehow with the
&apos;frame problem&apos; [181. That is. we must characterize not only
what changes as a result of doing an action, but also what does
not change. To approach this problem, the following notation
will be convenient:
Definition 1 (PRESERVES a p) p D (AFTER a p)
Of course, all theorems are preserved.
Temporal concepts are introduced witl. DONE (for past hap-
penings) and O (read &apos;eventually&amp;quot;). To say that p was true at
some point in the past, we use 3a (DONE p?;a). is to be
regarded in the &apos;branching time&apos; sense [111, and will be defined
more rigorously in [171. Essentially, Op is true ilT for all infinite
extensions of any course of events there is a finite prefix satis-
fying p. Op and 0—p are jointly satisfiable. Since Op starts
&amp;quot;now&amp;quot;, the following property is also true,
</bodyText>
<footnote confidence="0.9474805">
4(AFTER t (DONE t)), where t is term denoting a primitive act
(or a sequence of primitive acts), is not always true since an act may
change the values of terms (e.g., an election changes the value of the term
(PRESIDENT U.S.))
</footnote>
<equation confidence="0.348851">
Proposition 7 p D Op
</equation>
<bodyText confidence="0.713284">
Also, we have the following rule of inference:
Proposition 8 If I- aJ then P(cf V p) D v p)
</bodyText>
<subsectionHeader confidence="0.999364">
4.3 The Attitudes
</subsectionHeader>
<bodyText confidence="0.972173">
Neither BEL, BMB, nor GOAL characterize what an agent
actively believes, mutually believes (with someone else), or has
as a goal, but rather what is implicit in his beliefs, mutual be-
liefs, and goals. 5 That is, these operators characterize what
the world would he like if the agent&apos;s beliefs and mutual beliefs
were true, and if his goals were made true. Importantly. we
do not include an operator for wanting, since desires need not
be consistent. We assume that once an agent ha_s sorted out
his possibly inconsistent desires in deciding what he wishes to
achieve, the worlds he will be striving for are consistent. con-
versely, recognition of an agent&apos;s plans need not. consider that
agent&apos;s possibly inconsistent desires. Furthermore, there is also
no explicit operator for intending. If an agent intends to bring
about p, the agent is usually regarded as also being able to bring
about p. fly using GOAL, we will be able to reason about the
end state the agent is aiming at separately from our reasoning
about his ability to achieve that state.
For simplicity, we assume the usual Hintikka axiom schemata
for BEL [151, and we introduce KNOW by definition:
Definition 2 (KNOW x p) dlt p A (BEL x p)
</bodyText>
<subsubsectionHeader confidence="0.346521">
4.3.1 Mutual Belief
</subsubsectionHeader>
<bodyText confidence="0.998264833333333">
Human communication depends crucially on what is mutually
believed [I, 6, 7, 9, 22, 23, 241. We do not use the standard
definitions, but employ (13M13 y x p), which stands for y&apos;s belief
that it is mutually believed between y and x that p. (13M13 y
x p) is true iff (BEL y [p A (BMB x y p)1). 6 BMB has the
following properties:
</bodyText>
<equation confidence="0.917377444444444">
Proposition 9 (BMB y x ;mg) (13MI3 y x p)
(BMB y x q)
Proposition 10 (BMB y x ppq) D
((BMB y x p) D (BMB y x q))
Proposition 11 !f- s D 3 then
t-(13MI3 y x n) D (BMB y x 3)
Also, we characterize mutual knowledge as:
Definition 3 (MK x y prig p A (BMB x y p)
(nmn y x p)&apos;
</equation>
<bodyText confidence="0.8775784">
&apos;For an exploration of the issues involved in explicit vs. implicit belief, see
)16).
&apos;Notice that (BMB y x p) A (BMB x y p).
&apos;This definition is not entirely correct, but is adequate for present
purposes.
</bodyText>
<page confidence="0.994196">
51
</page>
<table confidence="0.419231333333333">
4.3.2 Goals
For GOAL, we have the following properties:
Proposition 12 (GOAL x (GOAL x p)) 7 (GOAL x p)
If an agent thinks he has a goal, then he does.
Proposition 13 (BEL x (GOAL x p))E., (GOAL x p)
Proposition 14 (GOAL x p) A (GOAL x pDq)
(GOAL x q)s
The following two derived rules are also useful:
Proposition 15 If I- a D $ then
</table>
<equation confidence="0.74358275">
I-(COAL x a) D (GOAL x d)
Proposition 18 Ifl-aA$ D 7 then
1-(DMB y x (GOAL x a)) A (BMB y x (GOAL x i))
(BMB y x (GOAL x -f))
</equation>
<bodyText confidence="0.890313">
More properties of GOAL follow.
</bodyText>
<subsectionHeader confidence="0.997993">
4.4 Attitudes and Rational Action
</subsectionHeader>
<bodyText confidence="0.837869714285714">
Next, we must characterize how beliefs, goals, and actions are
related. The interaction of BEL and AFTER will be patterned
after Moore&apos;s analysis [201. In particular, we have:
Proposition 17 v x. act (AGT a x) D
(AFTER act (KNOW x (DONE act)))
Agents know what they have done. Moreover, they think certain
effects of their own actions are achieved:
</bodyText>
<construct confidence="0.782856">
Proposition 18 (BEL x (RESULT x a p)) D
(RESULT x a (BEL x p)). where
Definition 4 (RESULT x a p) 4-12 (AFTER a p) A
(AGT a x)
</construct>
<bodyText confidence="0.97935204">
The major addition we have made is GOAL. which interacts
tightly with the other operators.
r will say a rational agent only adopts goals that are achiev-
able, and accepts as &apos;desirable&amp;quot; those states of the world that
are inevitable. To characterize inevitabilities, we have
Definition 5 (ALWAYS p) la (AFTER a p)
This says that no matter what happens, p is true. Clearly, we
want
Proposition 19 then 1- (BEL, x (ALWAYS a))
That is, theorems are believed to be alweys true.
Another property we want is that no sequence of primitive
acts is forever ruled out from happening.
Proposition 20 I- Vs (ACT a) D —(ALWAYS —(DONE a)),
where (ACT a)`1-4! —(AFTER a --(DONE a))
One important variant of ALWAYS is (ALWAYS x p) (rel-
ative to an agent), which indicates that no matter what that
agent does, p is true. The definition of this version is:
Definition 8 (ALWAYS x p) Va (RESULT x a p)
A useful instance of ALWAYS is (ALWAYS pPq) in which no
matter what happens, p still implies q. We can now distinguish
between p D &apos;is being logically valid, its being true in all courses
of events, and its merely being true after some event happens.
&apos;Notice that if pDci is true (or even believed) but (GOAL x pDcf) is not
true, we should not reach this conclusion since some act could make it
false.
</bodyText>
<subsectionHeader confidence="0.58051">
4.4.1 Goals and Inevitabilities
</subsectionHeader>
<bodyText confidence="0.9968515">
What an agent believes to be inevitable is a goal (he accepts
what he cannot change).
</bodyText>
<equation confidence="0.783397">
Proposition 21 (BEL x (ALWAYS p)) D (GOAL x p)
</equation>
<bodyText confidence="0.998186">
and conversely (almost), agents do not adopt goals that they
believe to be impossible to achieve —
</bodyText>
<equation confidence="0.4677695">
Proposition 22 No futility — (GOAL x p)
—(BEL x (ALWAYS —p))
</equation>
<bodyText confidence="0.808739">
This gives the following useful lemma:
</bodyText>
<subsectionHeader confidence="0.662324">
Lemma 1 Inevitable Consequences
</subsectionHeader>
<bodyText confidence="0.9533859375">
(GOAL x p) A (BEL x (ALWAYS pDq )) D (GOAL x q)
Proof By Proposition 21, if an agent believes poq is always
true, he has it as a goal. Hence by Proposition 14, q follows
from his goals.
This lemma states that if one&apos;s goal is a c.o.e. in which p holds,
and if one thinks that no matter what happens, ppq, then one&apos;s
goal is a c.o.e. in which q holds. Two aspects of this property
are crucially important to its plausibility. First, one must keep
in mind the &apos;follows from&amp;quot; interpretation of our propositional
attitudes. Second, the key aspect of the connection between
p and q is that no one can achieve p without achieving q. If
someone could do so, then q need not be true in a c.o.e. that
satisfies the agent&apos;s goals.
Now, we have the following as a lemma that will be used in
the speech act derivations:
Lemma 2 Shared Recognition
</bodyText>
<equation confidence="0.967181666666667">
(BMB y x (GOAL x p)) A
(BME1 y x (BEL x (ALWAYS pDq)))
(IIMB y x (GOAL x q))
</equation>
<bodyText confidence="0.996828">
The proof is a straightforward application of Lemma 1 and
Propositions 9 and 10.
</bodyText>
<subsectionHeader confidence="0.797086">
4.4.2 Persistent goals
</subsectionHeader>
<bodyText confidence="0.934301">
In this formalism, we are attempting to capture a number of
properties of what might be called &amp;quot;intention&amp;quot; without postu-
lating a primitive concept for &apos;intend&amp;quot;. Instead, we will combine
acts, beliefs. goals, and a notion of commitment built out of more
primitive notions.
To eapture one grade of commitment than an agent might
have towards his goals, we define a persistent goal. P-GOAL,
to be one that the agent will not give up until he thinks it has
been satisfied, or until he thinks he cannot achieve it.
Now, in order to state constraints on c.o.e.&apos;s we define:
Definition 7 (PREREQ x p q)
Ve (RESULT x e q) D 3 a (a c) A (RESULT x a p)
This definition states that p is a prerequisite for x&apos;s achieving q
if all ways for x to bring about q result in a course of events in
which p has been true. Now, we are ready for persistent goals:
</bodyText>
<page confidence="0.864083">
52
</page>
<equation confidence="0.8459454">
Definition 8 (P-GOAL x p) tLf
(GOAL x p) A
(PREREQ x ((BEL x p) v
(BEL x (ALWAYS x
—(GOAL x p)(
</equation>
<bodyText confidence="0.9937216">
Persistent goals are ones the agent will replan to achieve if his
earlier attempts to achieve it fail to do so. Our definition does
not say that an agent must give up his goal when he thinks it is
satisfied. since goals of maintenance are allowed. All this says is
that somewhere along the way to giving up the persistent goal,
the agent had to think it was true (or belieVe it was impossible
for him to achieve).
Though an agent may be persistent, he may be foolishly so
because he has no competence to achieve his goals. We charac-
terize competence below.
</bodyText>
<subsectionHeader confidence="0.727144">
4.4.3 Competence
</subsectionHeader>
<bodyText confidence="0.9898825">
People are sometimes experts in certain fields, as well as in their
own bodily movements. For example. a competent electrician
will form correct plans to achieve world states in which &amp;quot;elec-
trical&amp;quot; states of affairs obtain. Most adults are competent in
achieving world states in which their teeth are brushed, etc.
We will say an agent is COMPETENT with respect to p if,
whenever he thinks p will true after some action happens, he is
correct:
</bodyText>
<table confidence="0.722156166666667">
Definition 9 (COMPETENT x p)
Va (BEL x (AFTER x p)) D (AFTER a p)
One property of competence we will want is:
Proposition 23 Vx. a (ACT x a) D
(ALWAYS (COMPETENT x (DONE x a))), where
Definition 10 (DONE x a) (DONE a) A (AGT a x)
</table>
<tableCaption confidence="0.287166">
That is. any person is always competent to do the acts of
</tableCaption>
<bodyText confidence="0.85948825">
which he is the agent. Of course, he is not always competent
to achieve any particular effect.
Finally, given all these properties we are ready to describe
rational agents.
</bodyText>
<subsectionHeader confidence="0.98953">
4.5 Rational Agents
</subsectionHeader>
<bodyText confidence="0.959106">
Below are properties of ideally rational agents who adopt per-
sistent goals.
First, agents are careful; they do not knowingly and deliber-
ately make their persistent goals impossible for them achieve.
</bodyText>
<construct confidence="0.451151">
Proposition 24 (DONE x act) D (DONE x p7;act), where
(P-GOAL x q) D —(BEL x (AFTER act
(ALWAYS x -p))) v
—(GOAL x (DONE x act))
</construct>
<bodyText confidence="0.937350521739131">
In other words, no deliberately shooting onesself in the foot.
Now, agents are cautious in adopting persistent goals, since
they must eventually come to some decision about their feasi-
bility. We require an agent to either come up with a &amp;quot;plan&apos; to
°Because of Proposition 2. all Proposition 23 says is that if a competent
agent. believes his own primitive act halts, it will.
&apos;Notice that it is crucial that p be true in the same world in which the
agent does act, hence the use of •is7;act&amp;quot;
achieve them — a belief of some act (or act sequence) that it
achieves the persistent goal — or to believe he cannot bring the
goal about. That is, agents do not adopt persistent goals they
could never give up. The next Proposition will characterize this
property of P-GOAL.
But, even with a correct plan and a persistent goal. there
is still the possibility that the competent agent never executes
the plan in the right circumstances — some other agent has
changed the circumstances, thereby making the plan incorrect.
If the agent is competent, then if he formulates another plan, it
will be correct for the new circumstances. But again, the world
could change out from under him. Now, just as with operating
systems, we want to say that the world is &apos;fair&apos; - the agent will
eventually get a chance to execute his plans. This property is
also characterized in the following Proposition:
</bodyText>
<construct confidence="0.523538">
Proposition 25 Fair Execution — The agent will eventually
form a plan and execute it, believing it achieves hos persistent
goal on circumstances he believes to be appropriate for its success.
</construct>
<equation confidence="0.99866">
V x (P-GOAL x q) D
043 act&apos; (DONE x 0:ace)] v
(BEL x (ALWAYS x
</equation>
<bodyText confidence="0.9777295">
where p (BEL x (RESULT x act&apos; q))
We now give a crucial theorem:
Theorem 1 Consequences of a persistent goal — If someone
has a persistent goal of bringing about p, and bringing about p Is
within his area of competence, then eventually either p becomes
true or he will believe there is nothing that can be done to achieve
</bodyText>
<equation confidence="0.935165666666667">
V y (P-GOAL y p) A (ALWAYS (COMPETENT y p)) D
(p v (BEL y (ALWAYS y —p)))
Proof sketch:
</equation>
<bodyText confidence="0.9490994">
Since the agent has a persistent goal. he eventually will either
find and execute a plan, or will believe there is nothing he can
do to achieve the goal. Since he is competent with respect to p,
the plans he forms will be correct. Since his plan act&apos; is correct,
and since any other plans he forms for bringing about p are also
correct, and since the world is &amp;quot;fair&apos;, eventually either the agent
executes his correct plan, making p true, or the agent comes to
believe he cannot achieve p. A more rigorous proof can be found
in the Appendix.
This theorem is a major cornerstone of the formalism, telling
us when we can conclude Op, given a plan and a goal. and is
used throughout the speech act analyses. If an agent who is not
COMPETENT with respect to p adopts p as a persistent goal,
we cannot conclude that eventually either p will be true (or the
agent will think he cannot bring it about), since the agent could
forever create incorrect plans. If the goal is not persistent, we
also cannot conclude Op since the agent could give it up without
achies,ing it.
The use of 0 opens the formalism to McDermott&apos;s -Little
Neil&amp;quot; paradox [191. In our context, the problem arises as
follows: First, since an agent has a persistent goal to achieve p,
IlLittle Nell is tied to the railroad tracks, and will be mashed by the next
train. Dudley Doright is planning to save her. McDermott claims that,
according to various Al theories of planning, he never will, even though
he always knows just what to do.
</bodyText>
<page confidence="0.995473">
53
</page>
<bodyText confidence="0.999927608695652">
and we assume here he is always competent with respect to p.
Op is true. But, when p is of the form Oq (e.g., 0(SAVED
LITTLE-NELL)), 00q is true, so Oq is true as well. Let us
assume the agent knows all this. Hence, by the definition of
P-GOAL, one might expect the agent to give up his persistent
goal that 0q, since it is already satisfied!
On the other hand, it would appear that Proposition 25 is
sufficient to prevent the agent from giving up his goal too soon,
since it states that the agent with a persistent goal must act on
it, and, moreover, the definition of P-GOAL does not require the
agent to give up his goal immediately. For persistent goals to
achieve 0q. within someone&apos;s scope of competence, one might
think the agent need &amp;quot;only&amp;quot; maintain Qq as a goal, and then
the other properties of rationality force the agent to perform a
primitive act.
Unfortunately, the properties given so far do not yet rule out
Little Nell&apos;s being mashed, and for two reasons. First, NIL
denotes a primitive act — the empty sequence. Hence, doing it
would satisfy Proposition 25, but the agent never does anything
substantive. Second, doing anything that does not affect g also
satisfies Proposition 25, since after doing the unrelated act. Oq
is still true. We need to say that the agent eventually acts on g!
To do so, we have the following property:
</bodyText>
<equation confidence="0.988631666666667">
Proposition 28 (P-GOAL y 0g) D
P-GOAL y g) V
(DEL y (ALWAYS y ^-41))1,
</equation>
<bodyText confidence="0.999802">
That is. eventually the agent will have the persistent goal that
g, and by Proposition 25. will act on it. If he eventually comes to
believe he cannot bring about g, he eventually comes to believe
he cannot bring about. eventually g as well, allowing him to give
up his persistent goal that eventually q.
</bodyText>
<subsectionHeader confidence="0.947457">
4.6 Rational Interaction
</subsectionHeader>
<bodyText confidence="0.999585">
This ends our discussion of single agents. We now need to char-
acterize rational interaction sufficiently to handle a simple re-
cit,-.!st. First, we discuss cooperative agents, and then the effects
of uttering sentences.
</bodyText>
<subsectionHeader confidence="0.724641">
4.8.1 Properties of Cooperative Agents
</subsectionHeader>
<bodyText confidence="0.999978384615385">
We describe agents as sincere, helpful, and more knowledgeable
than others about the truth of some state of affairs. Essentially,
these concepts capture (quite simplistic) constraints on influenc-
ing someone else&apos;s `whet&apos;s and goals, and on adopting the beliefs
and goals of someone else as one&apos;s own. More refined versions
are certainly desirable. Ultimately. we expect such properties of
cooperative agents, as embedded in a theory of rational inter-
action. to provide a formal description of the kinds of conver-
sational behavior (&apos;,rice [HI describes with his &apos;conversational
maxims&amp;quot;.
First, we will say an agent is SINCERE with respect to p if
whenever his goal is to get someone else CO 5tlieve p, his goal is
in fact to get, that person to know p.
</bodyText>
<equation confidence="0.71146">
Definition 11 (SINCERE x p)
(GOAL x (BEL y p)) (GOAL x (KNOW y p))
</equation>
<bodyText confidence="0.996269">
An agent is HELPFUL to another if he adopts as his own
persistent goal another agent&apos;s goal that he eventually do some-
thing (provided that potential goal does not conflict with his
own).
</bodyText>
<table confidence="0.553987">
Definition 12 (HELPFUL x y)
Ya (BEL x (GOAL y O(DONE y a))) A
--(GOAL x —(DONE x a)) D
(P-GOAL x (DONE x a))
</table>
<tableCaption confidence="0.463663">
Agent x thinks agent y is more EXPERT about the true of p
than x if he always adopts x&apos;s beliefs about p as his own.
Definition 13 (EXPERT y x p) (1-1&apos;
(BEL x (BEL y p)) D (BEL x p)
4.8.2 Uttering Sentences with Certain &amp;quot;Features&amp;quot;
</tableCaption>
<bodyText confidence="0.998397333333333">
Finally, we need to describe the effects of tittering sentences with
certain &apos;features&apos; 1141, such as mood. In particular, we need
to characterize the results of uttering imperative, interrogative,
and declarative sentences 12 Our descriptions of these effects
will be similar to Grices&apos;s 1131 and to Perrault and Allen&apos;s [221
&apos;surface speech acts&apos;. Many times, these sentence forms are not
used literally to perform the corresponding speech acts (requests,
questions, and assertions).
The following is used to characterize uttering an imperative:
</bodyText>
<equation confidence="0.922482777777778">
Proposition 27 imperative,:
x y (MK x y (ATTEND y a)) D
(RESULT x [IMPER x y &amp;quot;&apos;do y ace]
(DMB y x
(GOAL x
(BEL y
(GOAL x
(P-GOAL y (DONE y act)
WM/
</equation>
<bodyText confidence="0.999935555555556">
The act IIMPEH. speaker hearer &apos;pf stands for &apos;make p true&amp;quot;.
Proposition 27 stales that if it is mutually known that y is at-
tending to a, 13 then the result of tittering an imperative to y
to make it the case that y has done action act is that y thinks
it is mutually believed that the speaker&apos;s goal is that y should
think his goal is for y to form the persistent goal of doing set.
We also need to assert that IMPER preserves sincerity about
the speaker&apos;s rnals and helpfulness. These restrictions could be
loosened. hut maintaining them is simpler.
</bodyText>
<equation confidence="0.8499875">
Proposition 28 (PRESERVES (IMPER x y -(10 y act-1
(BMB y x (SINCERE y ((OAL y p))))
Proposition 29 (PRESERVES [IMPER x y -do y
(HELPFUL y xi)
</equation>
<bodyText confidence="0.848036285714286">
All t ;ricean &amp;quot;feature&amp;quot;-based theories of communication need
to account for cases in which a speaker uses an utterance with a
feature, but does not have the attitudes (e.g.. beliefs, and goals)
&amp;quot;However, we can only present the analysis of imperatives here.
&amp;quot;If it is not mutually known that y is attending, for example. if the speaker
is not speaking to an audience, then we do not say what the result of
uttering an imperative is.
</bodyText>
<page confidence="0.997022">
54
</page>
<bodyText confidence="0.998567142857143">
usually attributed to someone uttering sentences with that fea-
ture. Thus, the attribution of the attitudes needs to be context-
dependent. Specifically, proposition 28 needs to be weak enough
to prevent nonserious utterances such as &amp;quot;go jump in the lake&apos;
from being automatically interpreted as requests even though
the utterance is an imperative. On the other hand, the formula
must be strong enough that requests are derivable.
</bodyText>
<sectionHeader confidence="0.904257" genericHeader="method">
5 Deriving a Simple Request
</sectionHeader>
<bodyText confidence="0.98960825">
In making a request. the speaker is trying to get the hearer to do
an act. We will show how the speaker&apos;s uttering an imperative
to do the act leads to its eventually being done. What we need
to prove is this:
</bodyText>
<equation confidence="0.964634">
Theorem 2 Result of an Imperative —
(DONE ((MK x y (ATTEND y x)) A
(DMB y x
(SINCERE x
(GOAL x
(P-GOAL y (DONE y act)))))/t
(HELPFUL y x)l?;
(IMPER x y -do y act&amp;quot;[) D
0(DONE y act)
</equation>
<bodyText confidence="0.998622928571429">
We will give the major steps of the proof in Fignre I. and
point to their justifications. The full-fledged proofs are left to
the energetic reader. All formulas preceded by a • are supposed
be true just prior to performing the IMPER, are preserved by
it. and thus are implicitly conjoined to formulas 2 - 9. By their
placement in the proof, we indicate where they are necessary for
making the deductions.
Essentially, the proof proceeds as follows:
If it is mutually known that y is attending to x. and y thinks it
is mutually believed that the io-condit ions hold, then x&apos;s tittering
an imperative toy to do some action results in formula (2). Since
it is mutually believed x is sincere about his goals, then (1) it is
mutually believed his goal truly is that y form a persistent goal
to In the act. Since everyone is always competent to do acts of
which they are the agent. (4) it is mutually believed that the act
will eventually be done, or y will think it is forever impossible
to do. But since no halting act is forever impossible to do, it
is (5) mutually believed that x&apos;s goal is that y eventually do it.
&apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now,
since y is helpfully disposed towards x, and has no objections to
doing the act, (7) y takes it on as a persistent goal. Since he
is always competent about doing his own acts, (8) eventually it
will he done or he will think it impossible to do. Again, since it
is not forever impossible, (:)) he will eventually do it.
We have shown how the performing of an imperative to do
an act leads to the act&apos;s eventually being done. We wish to
create a number of lemmas from this proof (and others like it)
to characterize illocutionary acts.
</bodyText>
<sectionHeader confidence="0.985829" genericHeader="method">
8 Plans and Summaries
</sectionHeader>
<subsectionHeader confidence="0.997834">
6.1 Plans
</subsectionHeader>
<bodyText confidence="0.996924666666667">
A plan for agent &amp;quot;x&amp;quot; to achieve some goal &amp;quot;q&amp;quot; is an action term
&amp;quot;a&amp;quot; and two sequences of wffs &amp;quot;po&amp;quot;, &amp;quot;pt&amp;quot;• • • • &apos;AC and &apos;go&amp;quot;,
... &amp;quot;qk&amp;quot; where &amp;quot;ok&apos; is &amp;quot;q&apos; and satisfying
</bodyText>
<equation confidence="0.998143">
I. I- (I3EL x (po A pi A ... pk) D
(RESULT x a q, A pi A ... A pk )))
2. I- (BEL x (ALWAYS (pin ch_i) D q,)))
</equation>
<bodyText confidence="0.925162714285714">
In other words, given a state where &amp;quot;x&amp;quot; believes the &apos;pi&apos;, he
will believe that if he does &amp;quot;a&amp;quot; then &amp;quot;go&amp;quot; will hold and moreover.
given that the act preserves pi, and he believes his making
true in the presence of pi will also make &amp;quot;qi&amp;quot; true. Consequently,
a plan is a special kind of proof that
I- (BEL x Pk) D (RESULT x a q)))
and therefore, since
</bodyText>
<equation confidence="0.8098036">
(BEL x p) D (BEL x (BEL x p))
and
(BEL x (p D q)) D ((BEL x p) D (BEL x q)), are axioms of
belief, a plan is a proof that
(DEL x (p,A ...A Pk)) D (BEL x (RESULT x a q))
</equation>
<bodyText confidence="0.901746">
Among the corollaries to a plan are
</bodyText>
<equation confidence="0.83196775">
I- (DEL x ((P0 A ... A p,) D (RESULT x a q,))) In,. ..k
and
h (BEL x ( (pi A ... A pi) D (ALWAYS qi_, D (OD
i=1....k j=i,...k
</equation>
<bodyText confidence="0.999495777777778">
There are two main points to be made about these corollaries.
First of all, since they are theorems, the implications can be
taken to be believed by the agent &amp;quot;x&amp;quot; in every state. In this
sense, these wffs express general methods believed to achieve
certain effects provided the assumptions are satisfied. The sec-
ond point is that these corollaries are in precisely the form that
is required in a plan and therefore can be used as justification for
a step in a future plan in much the same. way a lemma becomes
a single step in the proof of a theorem.
</bodyText>
<subsectionHeader confidence="0.999449">
6.2 Summaries
</subsectionHeader>
<bodyText confidence="0.9569179">
We therefore propose a notation for describing many steps of
a plan as a single summarizing operator. A summary consists
of a name, a list of free variables, a distinguished free variable
called the agent of the summary (who will always be listed first),
an Effect which is a wff, a optional Body which is either an
action or a wff and Finally, an optional Gate which is a wff. The
understanding here is that summaries are associated with agent
and for an agent &amp;quot;x&amp;quot; to have summary &amp;quot;u&amp;quot;, then there are three
cases depending on the body of &amp;quot;u&amp;quot;:
I. If the Body of &amp;quot;u&apos; is a wff, then
</bodyText>
<footnote confidence="0.7613642">
1- (DEL x (ALWAYS (Gate A Body) D (Gate A Effect))) 15
2. If the Body of &amp;quot;u&amp;quot; is an action term, then
(BEL x (Gate D (RESULT agent Body (Gate A Effect))))
160f course, many actions change the truth of their preconditions. Handling
such actions and preconditions is straightforward.
</footnote>
<page confidence="0.994659">
55
</page>
<table confidence="0.998730863636364">
(DONE ((MK x y (ATTEND y X)) A
(•conditiona)17;
EIMPER x y &amp;quot;do y actl) Given
(BMB y x (GOAL x (BEL y (GOAL x
(P-GOAL y (DONE y act)))))) A P27, P3, P4, 1
.(BMB y x (SINCERE x
(GOAL x (P-GOAL y (DONE y act)))))
(BMB y x (GOAL x (P-GOAL y (DONE y act)))) A Pll, P12, 2
.(BMB y x (ALWAYS
(COMPETENT y (DONE y act))))
(BMB y x (GOAL x 01(DONE y act) v
(BEL y (ALWAYS —(DONE y act)))))) A Ti, P16, 3
.(BMB y x --(ALWAYS --(DONE y act)))
(BMB y x (GOAL x 0(DONE y act))) A P16, P20, PS. 4
(BEL y x (GOAL x 0(DONE y act))) A Def. BMB
(HELPFUL y x)
T. (P-GOAL y x (DONE y act)) A Def. of HELPFUL, MP
&apos;(ALWAYS (COMPETENT y (DONE y act)))
8. Or(DONE y act) v (BEL y (ALWAYS --(DONE y act)))I Ti
• --(ALWAYS —(DONE y act))
9. 0(DONE y act) P20, PS
Q.E.D.
</table>
<figureCaption confidence="0.988825">
Figure 1: Proof of Theorem 2 — An imperative to do an act results in its eventually being done. &amp;quot;
</figureCaption>
<subsectionHeader confidence="0.6901155">
One thing worth noting about summaries is that normally the
wffs used above
</subsectionHeader>
<bodyText confidence="0.887695875">
I- (DEL x (Gate D
will follow from the more general wff
I- Gale
However, this need not be the case and different agents could
have different summaries (even with the same name). Saying
that an agent has a summary is no more than a convenient
way of saying that the agent always believes an implication of a
certain kind.
</bodyText>
<subsectionHeader confidence="0.58467">
7 Summarization of a Request
</subsectionHeader>
<bodyText confidence="0.9025405">
The following is a summary named REQUEST that captures
steps 2 through steps 5 of the proof of Theorem 2.
</bodyText>
<figure confidence="0.945291636363636">
[REQUEST x y act):
Gate: (1) (BMB y x (SINCERE x (GOAL x
(P-GOAL y (DONE y act))))) A
(2) (OMB y x (ALWAYS
(COMPETENT y (DONE y act))))
(3) (BMB y x —(ALWAYS --(DONE y act)))
Body: (BMB y x
(GOAL x
(BEL y
(GOAL x (P-GOAL y (DONE y act))))))
Effect: (BMB y x (GOAL x 0(DONE y act)))
</figure>
<subsectionHeader confidence="0.5214745">
This summary allows us to conclude that any action preserv-
ing the Cate and making the Body true makes the Effect true.
</subsectionHeader>
<bodyText confidence="0.990237272727272">
Conditions (2) and (3) are theorems and hence are always pre-
served. Condition (1) was preserved by assumption.
Searle&apos;s conditions for requesting are captured by the above.
Specifically, his &amp;quot;propositional content&amp;quot; condition, which states
that one requests a future act, is present as the Effect because
of Theorem 2. Searle&apos;s first &amp;quot;preparatory&apos; condition — that the
hearer be able to do the requested act, and that the speaker
think so is satisfied by condition (2). Searle&apos;s second prepara-
tory condition — that it not be obvious that the hearer was
going to do the act anyway — is captured by our conditions on
persistence, which state when an agent can give up a persistent
goal, that is not one of maintenance, when it has been satisfied.
Grice&apos;s &apos;recognition of intent&apos; condition [12, 131 is satisfied
since the endpoint in the chain (step 9) is a goal. Hence, the
speaker&apos;s goal is to get the hearer to do the act by means, in
part, of the (mutual) recognition that the speaker&apos;s goal is to
get the hearer to do it. Thus, according to Grice, the speaker
has meant„,, that the hearer should do the act. Searle&apos;s revised
Gricean condition, that the hearer should &amp;quot;understand&amp;quot; the ;it-
eral meaning of the utterance, and what illocutionary act the
utterance &apos;counts as&amp;quot; are also satisfied, provided the summary
is mutually known. 16
</bodyText>
<subsectionHeader confidence="0.994259">
7.1 Nonserious Requests
</subsectionHeader>
<bodyText confidence="0.9211511">
Two questions now arise. First, is this not overly complicated?
The answer, perhaps surprisingly, is &amp;quot;No&amp;quot;. By applying this
REQUEST theorem, we can prove that the utterance of an im-
perative in the circumstances specified by the Gate results in
the Effect, which is as simple a propositional attitude as anyone
would propose for the effect of uttering an imperative — namely
that it is mutually believed that the speaker&apos;s goal is that the
hearer eventually do the act. The Body need never be considered
&amp;quot;The further elaboration of this point that it deserves is outside the scope
of this paper.
</bodyText>
<page confidence="0.982683">
56
</page>
<bodyText confidence="0.999095317073171">
unless one of the gating conditions fails.
Then, if the Body is rarely needed, when is the &apos;extra&amp;quot; em-
bedding (GOAL speaker (BEL hearer ...)) attitude of use?
The answer is that these embeddings are essential to preventing
nonserious or insincere imperatives from being interpreted un-
conditionally as requests. In demonstrating this, we will show
how Searle&apos;s &amp;quot;Sincerity&amp;quot; condition is captured by our SINCERE
predicate.
The formula (SINCERE speaker p) is false when the speaker
does something to get the hearer to believe he, the speaker, has
the goal of the hearer&apos;s believing p, when he in fact does not have
the goal of the hearer&apos;s knowing that p Let us see see how this
would be applied for &apos;Go jump in the lake&amp;quot;, uttered idiomati-
cally. Notice that it could be uttered and meant as a request,
and we should be able to capture the distinction between serious
and nonserious uses. In the case of uttering this imperative, the
content of SINCERE. p p = (GOAL speaker (P-GOAL hearer
(DONE hearer [JUMP-INTO Lake1())).
Assume that it is mutually known/believed that the lake is
frigidly cold (any other conditions leading to --(GOAL x p)
would do as well, e.g., that the hearer is wearing his best suit,
or that there is no lake around). So, by a reasonable axiom of
goal formation, no one has goals to achieve states of affairs that
are objectionable (assume what is *objectionable&amp;quot; involves a
weighing of alternatives). So, it is mutually known/believed that
–(GOAL speaker (DONE hearer PUMP-INTO Laken)), and
so the speaker does not believe he has such a goal. &amp;quot; The
consequent to the implication defining SINCERE is false, and
because the result of the imperative is a mutual belief that the
speaker&apos;s goal is that the hearer think he has the goal of the
hearer&apos;s jumping into the lake, the antecedent of the implica-
tion is true. Hence, the speaker is insincere or not serious, and
a request interpretation is blocked. le
In the case of there not being a lake around, the speaker&apos;s goal
cannot he that the hearer form the persistent goal of jumping
in some non-existent lake, since by the No Futility property, the
hearer will not adopt a goal if it is unachievable, and hence the
speaker will not form his goal to achieve the unachievable state of
affairs (that the hearer adopt a goal he cannot achieve). Hence,
since all this is mutually believed, using the same argument, the
speaker must be insincere.
</bodyText>
<sectionHeader confidence="0.994833" genericHeader="method">
8 Nonspecific requests
</sectionHeader>
<bodyText confidence="0.975399416666667">
The ability conditions for requests are particularly simple, since
as long as the hearer knows what action the speaker is referring
to. he can always do it. He cannot, however, always bring about
some goal world. An important variation of requesting is one in
which the speaker does not specify the act to be performed; he
merely expresses his goal that some p be made true. This will be
captured by the action LIMPER y for &amp;quot;make p true&amp;quot;. Here,
&amp;quot;The speaker&apos;s expressed goal is that the hearer form a persistent goal
to jump in the lake. But, by the Inevitable Consequence, lemma, given
that a c.o.e. satisfying the speaker&apos;s goal also has the hearer&apos;s eventually
jumping in (since the hearer knows what to do), the speaker&apos;s goal is also
a c.o.e. in which the hearer eventually jumps in. In the same way, the
speaker&apos;s goal would also be that the h eventually gets wet.
&amp;quot;However, we do not say what else might be derivable. The speaker&apos;s true
goals may have more to do with the manner of hi. action (e.g., tone of
voice), than with the content. All we have done is demonstrate formally
how a hearer could determine the utterance is not to be taken at face
value.
in planning this act, the speaker need only believe the hearer
thinks it is mutually believed that it is always the case that the
hearer will eventually find a plan to bring about p. Although we
cannot present the proof that performing an [IMPER x y &apos;pi
will make Op true, the following is the illocutionary summary
of that proof: [NONSPECIFIC-REQUEST x y p]:
</bodyText>
<table confidence="0.600168416666667">
(BMB y x (SINCERE x (GOAL x (BEL y
(GOAL x (P-GOAL y p)))))) A
(BMB y x (ALWAYS (COMPETENT y p)))
(8MI3 y x (ALWAYS
03 act&apos; (DONE y q?;act&apos;),
where q dg (BEL y (RESULT y act&apos; p))))
Body: (BMI3 y x
(GOAL x
(BEL y
(GOAL x (P-GOAL y p)))))
Effect: (I3MB y x
(GOAL x 00)
</table>
<bodyText confidence="0.972609333333333">
Since the speaker only asks the hearer to make p true, the
ability conditions are that the hearer think it is mutually be-
lieved that it is always true that eventually there will be some
act such that the hearer believes of it that it achieves p (or he
will believe it is impossible for him to achieve). The speaker
need not know what act the hearer might choose.
</bodyText>
<sectionHeader confidence="0.943416" genericHeader="method">
9 On summarization
</sectionHeader>
<bodyText confidence="0.984604612903226">
Just as mathematicians have the leeway to decide which proofs
are useful enough to he named as lemmas or theorems, so too
does the language user. linguist, computer system, and speech
act theoretician have great leeway in deciding which summaries
to name and form. Grounds for making such decisions range
from the existence of illocutionary verbs in a particular lan-
guage, to efficiency. However, summaries are flexible — they
allow for different languages and different agents to carve up
the same plans differently. IG Furthermore, a summary formed
for efficiency may not correspond to a verb in the language.
Philosophical considerations may enter into how much of a
plan to summarize for an illocutionary verb. For example, most
illocutionary acts are considered successful when the speaker has
communicated his intentions, not when the intended effect has
taken hold. This argues for labelling as Effects of summaries in-
tended to capture illocutionary acts only formulas that are of the
form (BMB hearer speaker (GOAL speaker p)), rather than
those of the form (BMB hearer speaker p) or (BEL hearer p),
where p is not a GOAL-dominated formula. Finally, summaries
may be formed as conversations progress.
The same ability to capture varying amounts of a chain of
inference will allow us to deal with multi-utterance or multi-
agent acts, such as, betting, complying, answering, etc., in which
there either needs to be more than one act (a successful bet
requires an offer and an acceptance), or one act is defined to
require the presence of another (complying makes sense only
in the presence of a previous directive). For example, where
REQUEST captured the chain of inference from step 2 to step
5, one called COMPLY could start at 5 and stop at step 9.
&amp;quot;Remember, summaries are actually beliefs of agents, and those beliefs
need not be shared.
</bodyText>
<page confidence="0.995489">
57
</page>
<bodyText confidence="0.998988">
Thus, the notion of characterizing illocutionary acts as lemma-
like summaries, i.e., as chains of inference subject to certain
conditions, buys us the ability to encapsulate distant inferences
at &amp;quot;one-shot&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.8598665">
9.1 Ramifications for Computational Models of
Language Use
</subsectionHeader>
<bodyText confidence="0.9999292">
The use of these summaries provides a way to prove that various
short-cuts that a system might take in deriving a speaker&apos;s goals
are correct. Furthermore, the ability to index summaries by
their Bodies or from the utterance types that could lead to their
application (e.g., for utterances of the form &apos;Can you do &lt;X&gt;&amp;quot;)
allows for fast retrieval of a lemma that is likely to result in goal
recognition. By an appropriate organization of summaries 151,
a system can attempt to apply the most comprehensive sum-
maries first, and if inapplicable, can fall back on less compre-
hensive ones, eventually relying on first principles of reasoning
about actions. Thus, the apparent difficulty of reasoning about
speaker-intent can be tamed for the &amp;quot;short-circuited&amp;quot; cases, but
more general-purpose reasoning can deployed when necessary.
However. the complexities of reasoning about others&apos; beliefs and
goals remains.
</bodyText>
<sectionHeader confidence="0.990817" genericHeader="method">
10 Extensions: Indirection
</sectionHeader>
<bodyText confidence="0.998820722222222">
Indirection will he modeled in this framework as the derivation of
propositions dealing with the speaker&apos;s goals that are not stated
as such by the initial propositional attitude. For example, if we
can conclude from (OMB y x (GOAL x (GOAL y p))) that
y x (GOAL x (GOAL y 0 q))), where p does not entail
q, then. &apos;loosely&amp;quot;. we will say an indirect request has been made
by x.
(iiven the properties of O. (GOAL x p) D (GOAL x Op) is
a theorem. (GOAL x p) and (GOAL x are mutually un-
satisfiable, boa (GOAL x Op) and (GOAL x 0—p) are jointly
satisfiable. For example, (GOAL BILL OHAVE BILL HAM-
MERI))) and (GOAL DILL O(HAVE JOHN HAMMER1))
could both be part of a description of Bill&apos;s plan for John to get
a hammer and give it to him. Such a plan could be triggered
by Bill&apos;s merely saying &amp;quot;Get the hammer&apos; in the right circum-
stances, such as when Bill is on a ladder plainly holding a nail.
2° A subsequent paper will demonstrate the conditions under
which such reasoning is sound.
</bodyText>
<sectionHeader confidence="0.717309" genericHeader="method">
11 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.988445204545455">
This paper has detnonstrated that all illocutionary acts need
not be primitive. At least some can he derived from more basic
principles of rational action, and an account of the propositional
attitudes affected by the uttering of sentences with declarative,
interrogative, and imperative moods. This account satisfies a
number of criteria for a good theory of illocutionary acts.
• Most elements of the theory are independently motivated.
The theory of rational action is motivated independently
from any notions of communication. Similarly, the proper-
ties of cooperative agents are also independent of commu-
nication.
&amp;quot;Notice that most theories of speech acts would treat the above utterance
as only a direct request. We do not.
• The characterization of the result of uttering sentences with
certain syntactic moods is justified by the results we derive
for illocutionary acts, as well as the results we cannot de-
rive (e.g., we cannot derive a request under conditions of
insincerity).
• Summaries need not correspond to illocutionary verbs in a
language. Different languages could capture different parts
of the same chain of reasoning, and an agent might have
formed a summary for purposes of efficiency, but that sum-
mary need not correspond to any other agent&apos;s summary.
• The rules of combination of illocutionary acts (character-
izing, for example, how multiple assertions could consti-
tute the performance of a request) are now reduced to rules
for combining propositional contents and attitudes. Thus,
multi-utterance illocutionary acts can be handled by accu-
mulating the speaker&apos;s goals expressed in multiple utter-
ances, to allow an illocutionary theorem to be applied.
• Multi-act utterances are also a natural outgrowth of this ap-
proach. There is no reason why one cannot apply multiple
illocutionary summaries to the result of uttering a sentence.
Those summaries, however, need not correspond to illocu-
tionary verbs.
• The theory is naturally extensible to indirection (to be ar-
gued for in another paper). to other illocutionary act, such
as questions, commands, informs, assertions, and to the act
of referring (81.
Finally, although illocutionary act recognition may be strictly
unnecessary, given the complexity of our proofs, it is likely to
be useful. Essentially. such recognition would ;moono to the
application of illocutionary summaries theorems to discover the
speaker&apos;s goal(s).
</bodyText>
<sectionHeader confidence="0.976448" genericHeader="method">
12 Acknowledgements
</sectionHeader>
<bodyText confidence="0.87630925">
We would like to thank Tom Blenko, Herb Clark, Michael
George If, David Israel, Bei) Moore, Geoff Nutiberg, Fernando
Pereira. Hay Perrault, Stan Rosenschein, Ivan Sag, and Moshe
Vardi for valuable discussions.
</bodyText>
<sectionHeader confidence="0.929489" genericHeader="conclusions">
13 References
</sectionHeader>
<reference confidence="0.999635666666667">
1. I&amp;quot;. A plan-based approiv.-11 to speech act. recognition.
Peport WI, Department of Computer Science.
University of Toronto, January. 1979.
2. Allen, .1. F., Frisch. A. txt. it&apos; Litman. D. .1. Alt( i01&apos;: The
Rochester dialogue system. Proceedings of theVii tw,ieul
Conference an Artificial intelligence, Pittsburgh. Pennsyl-
vania, l912. C1(-7O.
3. Appelt, D. Planning ,Vatural Language Utterances to Saasfy
Multiple Goals. Ph.D. Th., Stanford University, Stanford,
California, December 1981.
4. Austin, J. L. flow to do things with words. Oxford University
Prem, London, 1962.
</reference>
<page confidence="0.98336">
58
</page>
<reference confidence="0.999968746666667">
5. Brachman. R.. Bobrow, R., Cohen, P., Klovstad, J., Web-
ber, B. L., &amp; Woods, W. A. Research in natural language
understanding. Technical Report 4274, Bolt Beranek and
Newman Inc., August, 1979.
8. Bruce, B. C., St Newman, D. Interacting plans. Cognitive
Science I!, 3, 1978, pp. 195-233.
7. Clark. H. H., St Marshall, C. Definite reference and mutual
knowledge. In Elements of Discourse Understanding, Aca-
demic Press, Joshi, A. K., Sag, I. A., &amp; Webber, B., Eds.,
New York, 1981.
8. Cohen, P. R. The Pragmatics of Referring and the Modality
of Communication. Computational Linguistics 10, 2, 1984,
pp. 97-146.
9. Cohen, P. R. On Knowing what to Say: Planning Speech
Acts. Ph.D. Th., University of Toronto. Toronto, January
1978. Technical Report No. 118, Dept. of Computer Sci-
ence.
10. (ober&apos;. P. R.. Levesque, II. J. Speech Acts and the Recog-
nition of Shared Plans. Proc. of the Third lliennial Con-
ference, Canadian Society for Computational Studies of In-
telligence, Victoria. B. C., May, 1980, 263-271.
11. Emerson, E. A., and Ilalpern, J. Y. &apos;Sometimes&amp;quot; and Not
Never- Revisited: On Branching versus Linear Time. ACM
Symposium on Principles of Progransming Languages, 1983.
12. Grice, II. P. Meaning. Philosophical Review 66, 1957, pp.
377-38s.
13. Grier. II. P. Utterer&apos;s Meaning and Intentions. Philosophi-
cal Review 68, 2. 1969, pp. 147-177.
14. Crice, H. P. Logic and conversation. In Cole., P. and Mor-
gan, J. I,., Eds.,Syntax and Semantics: Speech Acts , Aca-
demic Press, New York,1975.
16. Halpern, J. Y., and Moses. Y. O. A Clinic to the Modal
Logics of Knowledge and Belief. Proc. of the Ninth Inter-
national Joint Conference on Artificial intelligence, 1.11 Al.
Los Angeles, (&apos;alif.. August, 1985.
Levesque, Hector, J. A logic of implicit and explicit belief.
Proceedings of the National Conference of the American As-
viation for Artificial Intelligence, Austin, Texas, 1984.
esque, Fl. J., &amp; Cohen, P. R. A Simplified Logic of In-
Leraction. In preparation
18. McCarthy..1...t: Hayes, P..1. Some Philosophical Problems
from the :;tandpoint of Artificial Inielligence. In Marhsne
inielligenre American Elsevier, B. Meltzer I). Michie.
Eds.. New York. 1969.
19. McDermott, D. A temporal logic for reasoning about pro-
cesses and plans. Cognitive Science 6, 2, 1982, pp. 101-155.
20. Moore, R. C. Reasoning about Knowledge and Action.
Technical Note 191, Artificial Intelligence Center, SRI In-
ternational, October, 1980.
21. Morgan, J. 1... Two types of convention in indirect speech
acts. In Syntax and Semantics, Volume 9: Pragmatics,
Academic Press, P. Cole, Ed., New York, 1978, 261-280.
22. Perrault, C. R.. St Allen, J. F. A Plan-Based Analysis of
Indirect Speech Acts. American Journal of Computational
Linguistics 6, 3, 1980, pp. 167-182.
23. Perrault, C. R., Se Cohen, P. R. It&apos;s for your own good:
A note on inaccurate reference. In Elements of Discourse
Understanding, Cambridge University Press, Joshi, A., Sag,
I., &amp; Webber, B., Eds., Cambridge, Mass., 1981.
24. Schiffer, S. Meaning. Oxford University Press, London.
1972.
25. Schmidt, D. F., Sridharan, N.S., &amp; Coodson, J. L. The
plan recognition problem: An intersection of artificial intel-
ligence and psychology. Artificial Intelligence 10, 1979, pp.
15-83.
28. Searle, J. R. Speech acts: An essay in the philosophy of
language. Cambridge University Press, Cambridge, 1969.
27. Sidner, C. L., Bates, M., Bobrow, R. J., Brachman, R. J.,
Cohen, P. It.. Israel, D. J., Webber, B. L., &amp; Woods, W. A.
Research in knowledge representation for natural language
understanding. Annual Report 1785, Bolt. Beranek and
Newman Inc., November, 1981.
28. Vanderveken, D. A Model-Theoretic. Semantics for Illocu-
tionary Force. Logique et Analyse. 26, 103-101, 1983, pp.
359-395.
</reference>
<page confidence="0.999409">
59
</page>
<sectionHeader confidence="0.950873" genericHeader="references">
13 Appendix
</sectionHeader>
<reference confidence="0.47343975">
Proof of Theorem 1:
First, we need a lemma:
Lemma 3 Va (DONE x ((BEL x (AFTER a p)) A (COMPETENT x p)(?;a) D p
Proof
</reference>
<table confidence="0.868859857142857">
1. Va (DONE x [(BEL x (AFTER a p)) A (COMPETENT x p)(?;a) Ass
•-■ (BEL x (AFTER x p)) A (COMPETENT x p) D (AFTER a p) Def. of
COMPETENT. MP
Va (DONE x (AFTER a p)?;a) 2, P4
P 3,P3
Va (DONE x ((BEL x (AFTER a p)) A (COMPETENT x M(?:a) D p Imp). Intr.
Q.E.D.
</table>
<reference confidence="0.95951975">
Theorem 1. Vy (P-GOAL y p) A (ALWAYS (COMPETENT y p)) D O(p v (BEL y (ALWAYS —p)))
Proof:
1. (P-GOAL y (DONE y act)) A (ALWAYS (COMPETENT y (DONE y act))) A.
2. 0(3a (DONE y [(BEL y (AFTER a p))I?;a) v (BEL y (ALWAYS —p))) I, P25, MP
3. O(p V (BEL y (ALWAYS —p))) L3, PS, 2
4. (P-GOAL y (DONE y act)) A (ALWAYS (COMPETENT y (DONE y act))) D Impl. Intr., 3
O(p v (BEL y (ALWAYS —p)))
Q.E.D.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.909133">
<title confidence="0.999635">Speech Acts and Rationality</title>
<author confidence="0.999999">Philip R Cohen Hector J Levesque</author>
<affiliation confidence="0.9988314">Artificial Intelligence Center of Computer University of Toronto&apos; SRI International and Center for the Study of Language and Information Stanford University</affiliation>
<abstract confidence="0.987169">1 Abstract This paper derives the basis of a theory of communication from a formal theory of rational interaction. The major result is a demonstration that illocutionary acts need not be primitive, and need not be recognized. As a test case, we derive Searle&apos;s conditions on requesting from principles of rationality coupled with a (Iricean theory of imperatives. The theory is shown to distinguish insincere or nonserious imperatives from true requests. I.:xtensions to indirect speech acts, and ramifications for natural language systems are also briefly discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I</author>
</authors>
<title>A plan-based approiv.-11 to speech act.</title>
<date>1979</date>
<tech>recognition. Peport WI,</tech>
<institution>Department of Computer Science. University of Toronto,</institution>
<contexts>
<context position="33286" citStr="(1)" startWordPosition="5892" endWordPosition="5892">ofs are left to the energetic reader. All formulas preceded by a • are supposed be true just prior to performing the IMPER, are preserved by it. and thus are implicitly conjoined to formulas 2 - 9. By their placement in the proof, we indicate where they are necessary for making the deductions. Essentially, the proof proceeds as follows: If it is mutually known that y is attending to x. and y thinks it is mutually believed that the io-condit ions hold, then x&apos;s tittering an imperative toy to do some action results in formula (2). Since it is mutually believed x is sincere about his goals, then (1) it is mutually believed his goal truly is that y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objections to doing the act, (7) y takes it on as a persistent goal. Since he i</context>
<context position="38111" citStr="(1)" startWordPosition="6863" endWordPosition="6863">ct results in its eventually being done. &amp;quot; One thing worth noting about summaries is that normally the wffs used above I- (DEL x (Gate D will follow from the more general wff I- Gale However, this need not be the case and different agents could have different summaries (even with the same name). Saying that an agent has a summary is no more than a convenient way of saying that the agent always believes an implication of a certain kind. 7 Summarization of a Request The following is a summary named REQUEST that captures steps 2 through steps 5 of the proof of Theorem 2. [REQUEST x y act): Gate: (1) (BMB y x (SINCERE x (GOAL x (P-GOAL y (DONE y act))))) A (2) (OMB y x (ALWAYS (COMPETENT y (DONE y act)))) (3) (BMB y x —(ALWAYS --(DONE y act))) Body: (BMB y x (GOAL x (BEL y (GOAL x (P-GOAL y (DONE y act)))))) Effect: (BMB y x (GOAL x 0(DONE y act))) This summary allows us to conclude that any action preserving the Cate and making the Body true makes the Effect true. Conditions (2) and (3) are theorems and hence are always preserved. Condition (1) was preserved by assumption. Searle&apos;s conditions for requesting are captured by the above. Specifically, his &amp;quot;propositional content&amp;quot; condition, w</context>
</contexts>
<marker>1.</marker>
<rawString>I&amp;quot;. A plan-based approiv.-11 to speech act. recognition. Peport WI, Department of Computer Science. University of Toronto, January. 1979.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A txt it&apos; Litman D</author>
</authors>
<title>Alt( i01&apos;: The Rochester dialogue system.</title>
<booktitle>Proceedings of theVii tw,ieul Conference an Artificial intelligence,</booktitle>
<location>Pittsburgh. Pennsylvania, l912. C1(-7O.</location>
<contexts>
<context position="33216" citStr="(2)" startWordPosition="5879" endWordPosition="5879">f in Fignre I. and point to their justifications. The full-fledged proofs are left to the energetic reader. All formulas preceded by a • are supposed be true just prior to performing the IMPER, are preserved by it. and thus are implicitly conjoined to formulas 2 - 9. By their placement in the proof, we indicate where they are necessary for making the deductions. Essentially, the proof proceeds as follows: If it is mutually known that y is attending to x. and y thinks it is mutually believed that the io-condit ions hold, then x&apos;s tittering an imperative toy to do some action results in formula (2). Since it is mutually believed x is sincere about his goals, then (1) it is mutually believed his goal truly is that y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objection</context>
<context position="38172" citStr="(2)" startWordPosition="6877" endWordPosition="6877">ting about summaries is that normally the wffs used above I- (DEL x (Gate D will follow from the more general wff I- Gale However, this need not be the case and different agents could have different summaries (even with the same name). Saying that an agent has a summary is no more than a convenient way of saying that the agent always believes an implication of a certain kind. 7 Summarization of a Request The following is a summary named REQUEST that captures steps 2 through steps 5 of the proof of Theorem 2. [REQUEST x y act): Gate: (1) (BMB y x (SINCERE x (GOAL x (P-GOAL y (DONE y act))))) A (2) (OMB y x (ALWAYS (COMPETENT y (DONE y act)))) (3) (BMB y x —(ALWAYS --(DONE y act))) Body: (BMB y x (GOAL x (BEL y (GOAL x (P-GOAL y (DONE y act)))))) Effect: (BMB y x (GOAL x 0(DONE y act))) This summary allows us to conclude that any action preserving the Cate and making the Body true makes the Effect true. Conditions (2) and (3) are theorems and hence are always preserved. Condition (1) was preserved by assumption. Searle&apos;s conditions for requesting are captured by the above. Specifically, his &amp;quot;propositional content&amp;quot; condition, which states that one requests a future act, is present as the</context>
</contexts>
<marker>2.</marker>
<rawString>Allen, .1. F., Frisch. A. txt. it&apos; Litman. D. .1. Alt( i01&apos;: The Rochester dialogue system. Proceedings of theVii tw,ieul Conference an Artificial intelligence, Pittsburgh. Pennsylvania, l912. C1(-7O.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Appelt</author>
</authors>
<title>Planning ,Vatural Language Utterances to Saasfy Multiple Goals.</title>
<date>1981</date>
<tech>Ph.D. Th.,</tech>
<institution>Stanford University,</institution>
<location>Stanford, California,</location>
<contexts>
<context position="38222" citStr="(3)" startWordPosition="6887" endWordPosition="6887">d above I- (DEL x (Gate D will follow from the more general wff I- Gale However, this need not be the case and different agents could have different summaries (even with the same name). Saying that an agent has a summary is no more than a convenient way of saying that the agent always believes an implication of a certain kind. 7 Summarization of a Request The following is a summary named REQUEST that captures steps 2 through steps 5 of the proof of Theorem 2. [REQUEST x y act): Gate: (1) (BMB y x (SINCERE x (GOAL x (P-GOAL y (DONE y act))))) A (2) (OMB y x (ALWAYS (COMPETENT y (DONE y act)))) (3) (BMB y x —(ALWAYS --(DONE y act))) Body: (BMB y x (GOAL x (BEL y (GOAL x (P-GOAL y (DONE y act)))))) Effect: (BMB y x (GOAL x 0(DONE y act))) This summary allows us to conclude that any action preserving the Cate and making the Body true makes the Effect true. Conditions (2) and (3) are theorems and hence are always preserved. Condition (1) was preserved by assumption. Searle&apos;s conditions for requesting are captured by the above. Specifically, his &amp;quot;propositional content&amp;quot; condition, which states that one requests a future act, is present as the Effect because of Theorem 2. Searle&apos;s first &amp;quot;prep</context>
</contexts>
<marker>3.</marker>
<rawString>Appelt, D. Planning ,Vatural Language Utterances to Saasfy Multiple Goals. Ph.D. Th., Stanford University, Stanford, California, December 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Austin</author>
</authors>
<title>flow to do things with words.</title>
<date>1962</date>
<location>Oxford University Prem, London,</location>
<contexts>
<context position="33452" citStr="(4)" startWordPosition="5925" endWordPosition="5925">ly conjoined to formulas 2 - 9. By their placement in the proof, we indicate where they are necessary for making the deductions. Essentially, the proof proceeds as follows: If it is mutually known that y is attending to x. and y thinks it is mutually believed that the io-condit ions hold, then x&apos;s tittering an imperative toy to do some action results in formula (2). Since it is mutually believed x is sincere about his goals, then (1) it is mutually believed his goal truly is that y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objections to doing the act, (7) y takes it on as a persistent goal. Since he is always competent about doing his own acts, (8) eventually it will he done or he will think it impossible to do. Again, since it is not forever impossible, (:)) he w</context>
</contexts>
<marker>4.</marker>
<rawString>Austin, J. L. flow to do things with words. Oxford University Prem, London, 1962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bobrow</author>
<author>R Cohen</author>
<author>P Klovstad</author>
<author>J Webber</author>
<author>B L</author>
<author>W A Woods</author>
</authors>
<title>Research in natural language understanding.</title>
<date>1979</date>
<tech>Technical Report 4274,</tech>
<institution>Bolt Beranek and Newman Inc.,</institution>
<contexts>
<context position="33626" citStr="(5)" startWordPosition="5959" endWordPosition="5959">f it is mutually known that y is attending to x. and y thinks it is mutually believed that the io-condit ions hold, then x&apos;s tittering an imperative toy to do some action results in formula (2). Since it is mutually believed x is sincere about his goals, then (1) it is mutually believed his goal truly is that y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objections to doing the act, (7) y takes it on as a persistent goal. Since he is always competent about doing his own acts, (8) eventually it will he done or he will think it impossible to do. Again, since it is not forever impossible, (:)) he will eventually do it. We have shown how the performing of an imperative to do an act leads to the act&apos;s eventually being done. We wish to create a number of lemmas from this </context>
</contexts>
<marker>5.</marker>
<rawString>Brachman. R.. Bobrow, R., Cohen, P., Klovstad, J., Webber, B. L., &amp; Woods, W. A. Research in natural language understanding. Technical Report 4274, Bolt Beranek and Newman Inc., August, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B C Bruce</author>
<author>St Newman</author>
<author>D</author>
</authors>
<title>Interacting plans.</title>
<date>1978</date>
<journal>Cognitive Science I!,</journal>
<volume>3</volume>
<pages>195--233</pages>
<contexts>
<context position="33934" citStr="(8)" startWordPosition="6019" endWordPosition="6019">at y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objections to doing the act, (7) y takes it on as a persistent goal. Since he is always competent about doing his own acts, (8) eventually it will he done or he will think it impossible to do. Again, since it is not forever impossible, (:)) he will eventually do it. We have shown how the performing of an imperative to do an act leads to the act&apos;s eventually being done. We wish to create a number of lemmas from this proof (and others like it) to characterize illocutionary acts. 8 Plans and Summaries 6.1 Plans A plan for agent &amp;quot;x&amp;quot; to achieve some goal &amp;quot;q&amp;quot; is an action term &amp;quot;a&amp;quot; and two sequences of wffs &amp;quot;po&amp;quot;, &amp;quot;pt&amp;quot;• • • • &apos;AC and &apos;go&amp;quot;, ... &amp;quot;qk&amp;quot; where &amp;quot;ok&apos; is &amp;quot;q&apos; and satisfying I. I- (I3EL x (po A pi A ... pk) D (RESULT x </context>
</contexts>
<marker>8.</marker>
<rawString>Bruce, B. C., St Newman, D. Interacting plans. Cognitive Science I!, 3, 1978, pp. 195-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H</author>
<author>St Marshall</author>
<author>C</author>
</authors>
<title>Definite reference and mutual knowledge.</title>
<date>1981</date>
<booktitle>In Elements of Discourse Understanding,</booktitle>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="33839" citStr="(7)" startWordPosition="6000" endWordPosition="6000">ly believed x is sincere about his goals, then (1) it is mutually believed his goal truly is that y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objections to doing the act, (7) y takes it on as a persistent goal. Since he is always competent about doing his own acts, (8) eventually it will he done or he will think it impossible to do. Again, since it is not forever impossible, (:)) he will eventually do it. We have shown how the performing of an imperative to do an act leads to the act&apos;s eventually being done. We wish to create a number of lemmas from this proof (and others like it) to characterize illocutionary acts. 8 Plans and Summaries 6.1 Plans A plan for agent &amp;quot;x&amp;quot; to achieve some goal &amp;quot;q&amp;quot; is an action term &amp;quot;a&amp;quot; and two sequences of wffs &amp;quot;po&amp;quot;, &amp;quot;pt&amp;quot;• • • • &apos;AC an</context>
</contexts>
<marker>7.</marker>
<rawString>Clark. H. H., St Marshall, C. Definite reference and mutual knowledge. In Elements of Discourse Understanding, Academic Press, Joshi, A. K., Sag, I. A., &amp; Webber, B., Eds., New York, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>The Pragmatics of Referring and the Modality of Communication.</title>
<date>1984</date>
<journal>Computational Linguistics</journal>
<volume>10</volume>
<pages>97--146</pages>
<contexts>
<context position="33934" citStr="(8)" startWordPosition="6019" endWordPosition="6019">at y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objections to doing the act, (7) y takes it on as a persistent goal. Since he is always competent about doing his own acts, (8) eventually it will he done or he will think it impossible to do. Again, since it is not forever impossible, (:)) he will eventually do it. We have shown how the performing of an imperative to do an act leads to the act&apos;s eventually being done. We wish to create a number of lemmas from this proof (and others like it) to characterize illocutionary acts. 8 Plans and Summaries 6.1 Plans A plan for agent &amp;quot;x&amp;quot; to achieve some goal &amp;quot;q&amp;quot; is an action term &amp;quot;a&amp;quot; and two sequences of wffs &amp;quot;po&amp;quot;, &amp;quot;pt&amp;quot;• • • • &apos;AC and &apos;go&amp;quot;, ... &amp;quot;qk&amp;quot; where &amp;quot;ok&apos; is &amp;quot;q&apos; and satisfying I. I- (I3EL x (po A pi A ... pk) D (RESULT x </context>
</contexts>
<marker>8.</marker>
<rawString>Cohen, P. R. The Pragmatics of Referring and the Modality of Communication. Computational Linguistics 10, 2, 1984, pp. 97-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>On Knowing what to Say: Planning Speech Acts.</title>
<date></date>
<tech>Ph.D. Th.,</tech>
<institution>University of Toronto.</institution>
<location>Toronto,</location>
<marker>9.</marker>
<rawString>Cohen, P. R. On Knowing what to Say: Planning Speech Acts. Ph.D. Th., University of Toronto. Toronto, January</rawString>
</citation>
<citation valid="false">
<tech>Technical Report No. 118,</tech>
<institution>Dept. of Computer Science.</institution>
<marker>1978.</marker>
<rawString>Technical Report No. 118, Dept. of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J</author>
</authors>
<title>Speech Acts and the Recognition of Shared Plans.</title>
<date>1980</date>
<booktitle>Proc. of the Third lliennial Conference, Canadian Society for Computational Studies of Intelligence,</booktitle>
<pages>263--271</pages>
<location>Victoria. B. C.,</location>
<contexts>
<context position="3916" citStr="[10]" startWordPosition="610" endWordPosition="610">llocnt ionary acts (such as requests anti questions) apply more generally to non-communicative action. However, researchers have gradually lost sight of their roots. In recent work 1281 illocutionary acts are formalized. and a logic is proposed, in which properties of IA&apos;s (e.g., -preparatory conditions&amp;quot; and -modes if achievement&apos;) are primitively stipulated, rather than derived from more basic principles of action. We believe this approach misses significant generalities. This paper shows how to derive properties of illocutionary acts from principles of rationality, updating the formalism of [10]. Work in Artificial Intelligence provided the first formal grounding of speech act theory in terms of planning, and plan recognition. (laminating in Perrault and Allen&apos;s 1221 theory of indirect speech acts. Much of our research is inspired 1) their analyses. Howes er, one major ingredient of their theory can be shown to be redundant. [101 illocutionary acts. All the inferential power of the recognition of their illocutionary acts was already available in other -operators&apos;. Nevertheless, the natural language systems based on this approach [I, 51 always had to recognize which illocutionary act </context>
</contexts>
<marker>10.</marker>
<rawString>(ober&apos;. P. R.. Levesque, II. J. Speech Acts and the Recognition of Shared Plans. Proc. of the Third lliennial Conference, Canadian Society for Computational Studies of Intelligence, Victoria. B. C., May, 1980, 263-271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Emerson</author>
<author>J Y Ilalpern</author>
</authors>
<title>Sometimes&amp;quot; and Not Never- Revisited: On Branching versus Linear Time.</title>
<date>1983</date>
<booktitle>ACM Symposium on Principles of Progransming Languages,</booktitle>
<marker>11.</marker>
<rawString>Emerson, E. A., and Ilalpern, J. Y. &apos;Sometimes&amp;quot; and Not Never- Revisited: On Branching versus Linear Time. ACM Symposium on Principles of Progransming Languages, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Meaning</author>
</authors>
<date>1957</date>
<journal>Philosophical Review</journal>
<volume>66</volume>
<pages>377--38</pages>
<marker>12.</marker>
<rawString>Grice, II. P. Meaning. Philosophical Review 66, 1957, pp. 377-38s.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P</author>
</authors>
<title>Utterer&apos;s Meaning and Intentions.</title>
<date>1969</date>
<journal>Philosophical Review</journal>
<volume>68</volume>
<pages>147--177</pages>
<contexts>
<context position="5218" citStr="[13]" startWordPosition="817" endWordPosition="817">sary for achieving their effects, so too was their recognition. The stance that. illocutionary acts are not primitive, and need not be recognized, is a lihs•rating one. Once taken, it becomes apparent that many of the difficulties in applying speech act theory to discourse, or to computer systems, stem from taking these acts too seriously .— i.e., too primitively. 3 Form of the argument We show that illocutionary acts need not be primitive by deriving Searle&apos;s conditions on requesting from an independentlymotivated theory of action. The realm of communicative action is entered following Grice [13] — by postulating a correlation between the utterance of a sentence with a certain syntactic feature (e.g., its dominant clause is an imperative) and a complex 49 propositional attitude expressing the speaker&apos;s goal. This attitude becomes true as a result of uttering a sentence with that feature. Because of certain general principles governing beliefs and goals, other causal consequences of the speaker&apos;s having the expressed goal can be derived. Such derivations will be &amp;quot;summarized&amp;quot; as lemmas of the form &amp;quot;If (conditions) are true, then any action making (antecedent) true also makes (consequent</context>
</contexts>
<marker>13.</marker>
<rawString>Grier. II. P. Utterer&apos;s Meaning and Intentions. Philosophical Review 68, 2. 1969, pp. 147-177.</rawString>
</citation>
<citation valid="false">
<authors>
<author>In Cole</author>
<author>P</author>
<author>J I Morgan</author>
</authors>
<title>Eds.,Syntax and Semantics: Speech Acts ,</title>
<publisher>Academic Press,</publisher>
<location>New York,1975.</location>
<marker>14.</marker>
<rawString>Crice, H. P. Logic and conversation. In Cole., P. and Morgan, J. I,., Eds.,Syntax and Semantics: Speech Acts , Academic Press, New York,1975.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Y O</author>
</authors>
<title>A Clinic to the Modal Logics of Knowledge and Belief.</title>
<date>1985</date>
<booktitle>Proc. of the Ninth International Joint Conference on Artificial intelligence, 1.11 Al.</booktitle>
<location>Los Angeles, (&apos;alif..</location>
<marker>16.</marker>
<rawString>Halpern, J. Y., and Moses. Y. O. A Clinic to the Modal Logics of Knowledge and Belief. Proc. of the Ninth International Joint Conference on Artificial intelligence, 1.11 Al. Los Angeles, (&apos;alif.. August, 1985. Levesque, Hector, J. A logic of implicit and explicit belief. Proceedings of the National Conference of the American Asviation for Artificial Intelligence, Austin, Texas, 1984. esque, Fl. J., &amp; Cohen, P. R. A Simplified Logic of InLeraction. In preparation</rawString>
</citation>
<citation valid="true">
<authors>
<author>McCarthy 1 t Hayes</author>
<author>P 1</author>
</authors>
<title>Some Philosophical Problems from the :;tandpoint of Artificial Inielligence.</title>
<date>1969</date>
<booktitle>In Marhsne inielligenre</booktitle>
<publisher>American Elsevier,</publisher>
<location>New York.</location>
<marker>18.</marker>
<rawString>McCarthy..1...t: Hayes, P..1. Some Philosophical Problems from the :;tandpoint of Artificial Inielligence. In Marhsne inielligenre American Elsevier, B. Meltzer I). Michie. Eds.. New York. 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDermott</author>
</authors>
<title>A temporal logic for reasoning about processes and plans.</title>
<date>1982</date>
<journal>Cognitive Science</journal>
<volume>6</volume>
<pages>101--155</pages>
<marker>19.</marker>
<rawString>McDermott, D. A temporal logic for reasoning about processes and plans. Cognitive Science 6, 2, 1982, pp. 101-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<title>Reasoning about Knowledge and Action.</title>
<date>1980</date>
<tech>Technical Note 191,</tech>
<institution>Artificial Intelligence Center, SRI International,</institution>
<marker>20.</marker>
<rawString>Moore, R. C. Reasoning about Knowledge and Action. Technical Note 191, Artificial Intelligence Center, SRI International, October, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morgan</author>
</authors>
<title>Two types of convention in indirect speech acts.</title>
<date>1978</date>
<booktitle>In Syntax and Semantics,</booktitle>
<volume>9</volume>
<pages>261--280</pages>
<publisher>Pragmatics, Academic Press,</publisher>
<location>New York,</location>
<marker>21.</marker>
<rawString>Morgan, J. 1... Two types of convention in indirect speech acts. In Syntax and Semantics, Volume 9: Pragmatics, Academic Press, P. Cole, Ed., New York, 1978, 261-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R St Allen Perrault</author>
<author>J F</author>
</authors>
<title>A Plan-Based Analysis of Indirect Speech Acts.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>6</volume>
<pages>167--182</pages>
<marker>22.</marker>
<rawString>Perrault, C. R.. St Allen, J. F. A Plan-Based Analysis of Indirect Speech Acts. American Journal of Computational Linguistics 6, 3, 1980, pp. 167-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Perrault</author>
<author>Se Cohen</author>
<author>P R</author>
</authors>
<title>It&apos;s for your own good: A note on inaccurate reference.</title>
<date>1981</date>
<booktitle>In Elements of Discourse Understanding,</booktitle>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, Mass.,</location>
<marker>23.</marker>
<rawString>Perrault, C. R., Se Cohen, P. R. It&apos;s for your own good: A note on inaccurate reference. In Elements of Discourse Understanding, Cambridge University Press, Joshi, A., Sag, I., &amp; Webber, B., Eds., Cambridge, Mass., 1981.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Meaning Schiffer</author>
</authors>
<publisher>Oxford University Press,</publisher>
<location>London.</location>
<marker>24.</marker>
<rawString>Schiffer, S. Meaning. Oxford University Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D F Schmidt</author>
<author>N S Sridharan</author>
<author>J L Coodson</author>
</authors>
<title>The plan recognition problem: An intersection of artificial intelligence and psychology.</title>
<date>1979</date>
<journal>Artificial Intelligence</journal>
<volume>10</volume>
<pages>15--83</pages>
<marker>25.</marker>
<rawString>Schmidt, D. F., Sridharan, N.S., &amp; Coodson, J. L. The plan recognition problem: An intersection of artificial intelligence and psychology. Artificial Intelligence 10, 1979, pp. 15-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>Speech acts: An essay in the philosophy of language.</title>
<date>1969</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<marker>28.</marker>
<rawString>Searle, J. R. Speech acts: An essay in the philosophy of language. Cambridge University Press, Cambridge, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
<author>M Bates</author>
<author>R J Bobrow</author>
<author>R J Brachman</author>
<author>P It Israel Cohen</author>
<author>D J</author>
<author>B L Webber</author>
<author>W A Woods</author>
</authors>
<title>Research in knowledge representation for natural language understanding.</title>
<date>1981</date>
<tech>Annual Report 1785,</tech>
<institution>Bolt. Beranek and Newman Inc.,</institution>
<marker>27.</marker>
<rawString>Sidner, C. L., Bates, M., Bobrow, R. J., Brachman, R. J., Cohen, P. It.. Israel, D. J., Webber, B. L., &amp; Woods, W. A. Research in knowledge representation for natural language understanding. Annual Report 1785, Bolt. Beranek and Newman Inc., November, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vanderveken</author>
</authors>
<title>A Model-Theoretic. Semantics for Illocutionary Force. Logique et Analyse.</title>
<date>1983</date>
<booktitle>Proof of Theorem 1: First, we need a lemma: Lemma 3 Va (DONE x ((BEL x (AFTER a p)) A (COMPETENT x p)(?;a) D p Proof Theorem 1. Vy (P-GOAL y p) A (ALWAYS (COMPETENT y p)) D O(p v (BEL y (ALWAYS —p))) Proof:</booktitle>
<volume>26</volume>
<pages>103--101</pages>
<marker>28.</marker>
<rawString>Vanderveken, D. A Model-Theoretic. Semantics for Illocutionary Force. Logique et Analyse. 26, 103-101, 1983, pp. 359-395. Proof of Theorem 1: First, we need a lemma: Lemma 3 Va (DONE x ((BEL x (AFTER a p)) A (COMPETENT x p)(?;a) D p Proof Theorem 1. Vy (P-GOAL y p) A (ALWAYS (COMPETENT y p)) D O(p v (BEL y (ALWAYS —p))) Proof:</rawString>
</citation>
<citation valid="false">
<journal>P-GOAL y (DONE y act)) A (ALWAYS (COMPETENT y (DONE y act))) A.</journal>
<contexts>
<context position="33286" citStr="(1)" startWordPosition="5892" endWordPosition="5892">ofs are left to the energetic reader. All formulas preceded by a • are supposed be true just prior to performing the IMPER, are preserved by it. and thus are implicitly conjoined to formulas 2 - 9. By their placement in the proof, we indicate where they are necessary for making the deductions. Essentially, the proof proceeds as follows: If it is mutually known that y is attending to x. and y thinks it is mutually believed that the io-condit ions hold, then x&apos;s tittering an imperative toy to do some action results in formula (2). Since it is mutually believed x is sincere about his goals, then (1) it is mutually believed his goal truly is that y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objections to doing the act, (7) y takes it on as a persistent goal. Since he i</context>
<context position="38111" citStr="(1)" startWordPosition="6863" endWordPosition="6863">ct results in its eventually being done. &amp;quot; One thing worth noting about summaries is that normally the wffs used above I- (DEL x (Gate D will follow from the more general wff I- Gale However, this need not be the case and different agents could have different summaries (even with the same name). Saying that an agent has a summary is no more than a convenient way of saying that the agent always believes an implication of a certain kind. 7 Summarization of a Request The following is a summary named REQUEST that captures steps 2 through steps 5 of the proof of Theorem 2. [REQUEST x y act): Gate: (1) (BMB y x (SINCERE x (GOAL x (P-GOAL y (DONE y act))))) A (2) (OMB y x (ALWAYS (COMPETENT y (DONE y act)))) (3) (BMB y x —(ALWAYS --(DONE y act))) Body: (BMB y x (GOAL x (BEL y (GOAL x (P-GOAL y (DONE y act)))))) Effect: (BMB y x (GOAL x 0(DONE y act))) This summary allows us to conclude that any action preserving the Cate and making the Body true makes the Effect true. Conditions (2) and (3) are theorems and hence are always preserved. Condition (1) was preserved by assumption. Searle&apos;s conditions for requesting are captured by the above. Specifically, his &amp;quot;propositional content&amp;quot; condition, w</context>
</contexts>
<marker>1.</marker>
<rawString>(P-GOAL y (DONE y act)) A (ALWAYS (COMPETENT y (DONE y act))) A.</rawString>
</citation>
<citation valid="false">
<booktitle>0(3a (DONE y [(BEL y (AFTER a p))I?;a) v (BEL y (ALWAYS —p))) I, P25, MP</booktitle>
<contexts>
<context position="33216" citStr="(2)" startWordPosition="5879" endWordPosition="5879">f in Fignre I. and point to their justifications. The full-fledged proofs are left to the energetic reader. All formulas preceded by a • are supposed be true just prior to performing the IMPER, are preserved by it. and thus are implicitly conjoined to formulas 2 - 9. By their placement in the proof, we indicate where they are necessary for making the deductions. Essentially, the proof proceeds as follows: If it is mutually known that y is attending to x. and y thinks it is mutually believed that the io-condit ions hold, then x&apos;s tittering an imperative toy to do some action results in formula (2). Since it is mutually believed x is sincere about his goals, then (1) it is mutually believed his goal truly is that y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objection</context>
<context position="38172" citStr="(2)" startWordPosition="6877" endWordPosition="6877">ting about summaries is that normally the wffs used above I- (DEL x (Gate D will follow from the more general wff I- Gale However, this need not be the case and different agents could have different summaries (even with the same name). Saying that an agent has a summary is no more than a convenient way of saying that the agent always believes an implication of a certain kind. 7 Summarization of a Request The following is a summary named REQUEST that captures steps 2 through steps 5 of the proof of Theorem 2. [REQUEST x y act): Gate: (1) (BMB y x (SINCERE x (GOAL x (P-GOAL y (DONE y act))))) A (2) (OMB y x (ALWAYS (COMPETENT y (DONE y act)))) (3) (BMB y x —(ALWAYS --(DONE y act))) Body: (BMB y x (GOAL x (BEL y (GOAL x (P-GOAL y (DONE y act)))))) Effect: (BMB y x (GOAL x 0(DONE y act))) This summary allows us to conclude that any action preserving the Cate and making the Body true makes the Effect true. Conditions (2) and (3) are theorems and hence are always preserved. Condition (1) was preserved by assumption. Searle&apos;s conditions for requesting are captured by the above. Specifically, his &amp;quot;propositional content&amp;quot; condition, which states that one requests a future act, is present as the</context>
</contexts>
<marker>2.</marker>
<rawString>0(3a (DONE y [(BEL y (AFTER a p))I?;a) v (BEL y (ALWAYS —p))) I, P25, MP</rawString>
</citation>
<citation valid="false">
<booktitle>O(p V (BEL y (ALWAYS —p))) L3, PS,</booktitle>
<volume>2</volume>
<contexts>
<context position="38222" citStr="(3)" startWordPosition="6887" endWordPosition="6887">d above I- (DEL x (Gate D will follow from the more general wff I- Gale However, this need not be the case and different agents could have different summaries (even with the same name). Saying that an agent has a summary is no more than a convenient way of saying that the agent always believes an implication of a certain kind. 7 Summarization of a Request The following is a summary named REQUEST that captures steps 2 through steps 5 of the proof of Theorem 2. [REQUEST x y act): Gate: (1) (BMB y x (SINCERE x (GOAL x (P-GOAL y (DONE y act))))) A (2) (OMB y x (ALWAYS (COMPETENT y (DONE y act)))) (3) (BMB y x —(ALWAYS --(DONE y act))) Body: (BMB y x (GOAL x (BEL y (GOAL x (P-GOAL y (DONE y act)))))) Effect: (BMB y x (GOAL x 0(DONE y act))) This summary allows us to conclude that any action preserving the Cate and making the Body true makes the Effect true. Conditions (2) and (3) are theorems and hence are always preserved. Condition (1) was preserved by assumption. Searle&apos;s conditions for requesting are captured by the above. Specifically, his &amp;quot;propositional content&amp;quot; condition, which states that one requests a future act, is present as the Effect because of Theorem 2. Searle&apos;s first &amp;quot;prep</context>
</contexts>
<marker>3.</marker>
<rawString>O(p V (BEL y (ALWAYS —p))) L3, PS, 2</rawString>
</citation>
<citation valid="false">
<booktitle>P-GOAL y (DONE y act)) A (ALWAYS (COMPETENT y (DONE y act))) D Impl. Intr., 3 O(p v (BEL y (ALWAYS —p))) Q.E.D.</booktitle>
<contexts>
<context position="33452" citStr="(4)" startWordPosition="5925" endWordPosition="5925">ly conjoined to formulas 2 - 9. By their placement in the proof, we indicate where they are necessary for making the deductions. Essentially, the proof proceeds as follows: If it is mutually known that y is attending to x. and y thinks it is mutually believed that the io-condit ions hold, then x&apos;s tittering an imperative toy to do some action results in formula (2). Since it is mutually believed x is sincere about his goals, then (1) it is mutually believed his goal truly is that y form a persistent goal to In the act. Since everyone is always competent to do acts of which they are the agent. (4) it is mutually believed that the act will eventually be done, or y will think it is forever impossible to do. But since no halting act is forever impossible to do, it is (5) mutually believed that x&apos;s goal is that y eventually do it. &apos;fence, (6) y thinks x&apos;s goal is that y eventually do the act. Now, since y is helpfully disposed towards x, and has no objections to doing the act, (7) y takes it on as a persistent goal. Since he is always competent about doing his own acts, (8) eventually it will he done or he will think it impossible to do. Again, since it is not forever impossible, (:)) he w</context>
</contexts>
<marker>4.</marker>
<rawString>(P-GOAL y (DONE y act)) A (ALWAYS (COMPETENT y (DONE y act))) D Impl. Intr., 3 O(p v (BEL y (ALWAYS —p))) Q.E.D.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>